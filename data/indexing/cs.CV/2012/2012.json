[{"id": "2012.00057", "submitter": "Ayush Jain", "authors": "Zhaoyuan Fang, Ayush Jain, Gabriel Sarch, Adam W. Harley, Katerina\n  Fragkiadaki", "title": "Move to See Better: Self-Improving Embodied Object Detection", "comments": "First three authors contributed equally. Project Page:\n  https://ayushjain1144.github.io/SeeingByMoving/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Passive methods for object detection and segmentation treat images of the\nsame scene as individual samples and do not exploit object permanence across\nmultiple views. Generalization to novel or difficult viewpoints thus requires\nadditional training with lots of annotations. In contrast, humans often\nrecognize objects by simply moving around, to get more informative viewpoints.\nIn this paper, we propose a method for improving object detection in testing\nenvironments, assuming nothing but an embodied agent with a pre-trained 2D\nobject detector. Our agent collects multi-view data, generates 2D and 3D\npseudo-labels, and fine-tunes its detector in a self-supervised manner.\nExperiments on both indoor and outdoor datasets show that (1) our method\nobtains high-quality 2D and 3D pseudo-labels from multi-view RGB-D data; (2)\nfine-tuning with these pseudo-labels improves the 2D detector significantly in\nthe test environment; (3) training a 3D detector with our pseudo-labels\noutperforms a prior self-supervised method by a large margin; (4) given weak\nsupervision, our method can generate better pseudo-labels for novel objects.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 19:16:51 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 08:09:11 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Fang", "Zhaoyuan", ""], ["Jain", "Ayush", ""], ["Sarch", "Gabriel", ""], ["Harley", "Adam W.", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "2012.00088", "submitter": "Qihao Liu", "authors": "Qihao Liu, Weichao Qiu, Weiyao Wang, Gregory D. Hager, Alan L. Yuille", "title": "Nothing But Geometric Constraints: A Model-Free Method for Articulated\n  Object Pose Estimation", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised vision-based system to estimate the joint\nconfigurations of the robot arm from a sequence of RGB or RGB-D images without\nknowing the model a priori, and then adapt it to the task of\ncategory-independent articulated object pose estimation. We combine a classical\ngeometric formulation with deep learning and extend the use of epipolar\nconstraint to multi-rigid-body systems to solve this task. Given a video\nsequence, the optical flow is estimated to get the pixel-wise dense\ncorrespondences. After that, the 6D pose is computed by a modified PnP\nalgorithm. The key idea is to leverage the geometric constraints and the\nconstraint between multiple frames. Furthermore, we build a synthetic dataset\nwith different kinds of robots and multi-joint articulated objects for the\nresearch of vision-based robot control and robotic vision. We demonstrate the\neffectiveness of our method on three benchmark datasets and show that our\nmethod achieves higher accuracy than the state-of-the-art supervised methods in\nestimating joint angles of robot arms and articulated objects.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 20:46:48 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Liu", "Qihao", ""], ["Qiu", "Weichao", ""], ["Wang", "Weiyao", ""], ["Hager", "Gregory D.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "2012.00119", "submitter": "Gongbo Liang", "authors": "Xin Xing, Gongbo Liang, Hunter Blanton, Muhammad Usman Rafique, Chris\n  Wang, Ai-Ling Lin, Nathan Jacobs", "title": "Dynamic Image for 3D MRI Image Alzheimer's Disease Classification", "comments": "Accepted to ECCV2020 Workshop on BioImage Computing", "journal-ref": null, "doi": "10.1007/978-3-030-66415-2_23", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to apply a 2D CNN architecture to 3D MRI image Alzheimer's disease\nclassification. Training a 3D convolutional neural network (CNN) is\ntime-consuming and computationally expensive. We make use of approximate rank\npooling to transform the 3D MRI image volume into a 2D image to use as input to\na 2D CNN. We show our proposed CNN model achieves $9.5\\%$ better Alzheimer's\ndisease classification accuracy than the baseline 3D models. We also show that\nour method allows for efficient training, requiring only 20% of the training\ntime compared to 3D CNN models. The code is available online:\nhttps://github.com/UkyVision/alzheimer-project.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 21:39:32 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Xing", "Xin", ""], ["Liang", "Gongbo", ""], ["Blanton", "Hunter", ""], ["Rafique", "Muhammad Usman", ""], ["Wang", "Chris", ""], ["Lin", "Ai-Ling", ""], ["Jacobs", "Nathan", ""]]}, {"id": "2012.00125", "submitter": "Sungbin Choi", "authors": "Sungbin Choi", "title": "Utilizing UNet for the future traffic map prediction task Traffic4cast\n  challenge 2020", "comments": "8 pages, 4 figures, NeurIPS 2020 Traffic4cast workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes our UNet based experiments on the Traffic4cast challenge\n2020. Similar to the Traffic4cast challenge 2019, the task is to predict\ntraffic flow volume, direction and speed on a high resolution map of three\nlarge cities worldwide. We mainly experimented with UNet based deep\nconvolutional networks with various compositions of densely connected\nconvolution layers, average pooling layers and max pooling layers. Three base\nUNet model types are tried and predictions are combined by averaging prediction\nscores or taking median value. Our method achieved best performance in this\nyears newly built challenge dataset.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 03:21:32 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Choi", "Sungbin", ""]]}, {"id": "2012.00139", "submitter": "Davis Gilton", "authors": "Davis Gilton, Gregory Ongie, Rebecca Willett", "title": "Model Adaptation for Inverse Problems in Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks have been applied successfully to a wide variety of\ninverse problems arising in computational imaging. These networks are typically\ntrained using a forward model that describes the measurement process to be\ninverted, which is often incorporated directly into the network itself.\nHowever, these approaches are sensitive to changes in the forward model: if at\ntest time the forward model varies (even slightly) from the one the network was\ntrained for, the reconstruction performance can degrade substantially. Given a\nnetwork trained to solve an initial inverse problem with a known forward model,\nwe propose two novel procedures that adapt the network to a change in the\nforward model, even without full knowledge of the change. Our approaches do not\nrequire access to more labeled data (i.e., ground truth images). We show these\nsimple model adaptation approaches achieve empirical success in a variety of\ninverse problems, including deblurring, super-resolution, and undersampled\nimage reconstruction in magnetic resonance imaging.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 22:19:59 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 20:38:44 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Gilton", "Davis", ""], ["Ongie", "Gregory", ""], ["Willett", "Rebecca", ""]]}, {"id": "2012.00144", "submitter": "Alireza Borjali", "authors": "Gergo Merkely, Alireza Borjali, Molly Zgoda, Evan M. Farina, Simon\n  Gortz, Orhun Muratoglu, Christian Lattermann, Kartik M. Varadarajan", "title": "Improved Diagnosis of Tibiofemoral Cartilage Defects on MRI Images Using\n  Deep Learning", "comments": null, "journal-ref": "https://doi.org/10.1016/j.jcjp.2021.100009", "doi": "10.1016/j.jcjp.2021.100009", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: MRI is the modality of choice for cartilage imaging; however, its\ndiagnostic performance is variable and significantly lower than the gold\nstandard diagnostic knee arthroscopy. In recent years, deep learning has been\nused to automatically interpret medical images to improve diagnostic accuracy\nand speed. Purpose: The primary purpose of this study was to evaluate whether\ndeep learning applied to the interpretation of knee MRI images can be utilized\nto identify cartilage defects accurately. Methods: We analyzed data from\npatients who underwent knee MRI evaluation and consequently had arthroscopic\nknee surgery (207 with cartilage defect, 90 without cartilage defect).\nPatients' arthroscopic findings were compared to preoperative MRI images to\nverify the presence or absence of isolated tibiofemoral cartilage defects. We\ndeveloped three convolutional neural networks (CNNs) to analyze the MRI images\nand implemented image-specific saliency maps to visualize the CNNs'\ndecision-making process. To compare the CNNs' performance against human\ninterpretation, the same test dataset images were provided to an experienced\northopaedic surgeon and an orthopaedic resident. Results: Saliency maps\ndemonstrated that the CNNs learned to focus on the clinically relevant areas of\nthe tibiofemoral articular cartilage on MRI images during the decision-making\nprocesses. One CNN achieved higher performance than the orthopaedic surgeon,\nwith two more accurate diagnoses made by the CNN. All the CNNs outperformed the\northopaedic resident. Conclusion: CNN can be used to enhance the diagnostic\nperformance of MRI in identifying isolated tibiofemoral cartilage defects and\nmay replace diagnostic knee arthroscopy in certain cases in the future.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 22:50:37 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Merkely", "Gergo", ""], ["Borjali", "Alireza", ""], ["Zgoda", "Molly", ""], ["Farina", "Evan M.", ""], ["Gortz", "Simon", ""], ["Muratoglu", "Orhun", ""], ["Lattermann", "Christian", ""], ["Varadarajan", "Kartik M.", ""]]}, {"id": "2012.00150", "submitter": "Hanchen Xie", "authors": "Hanchen Xie, Mohamed E. Hussein, Aram Galstyan, Wael Abd-Almageed", "title": "MUSCLE: Strengthening Semi-Supervised Learning Via Concurrent\n  Unsupervised Learning Using Mutual Information Maximization", "comments": "10 pages, 3 figures, Accepted to WACV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks are powerful, massively parameterized machine learning\nmodels that have been shown to perform well in supervised learning tasks.\nHowever, very large amounts of labeled data are usually needed to train deep\nneural networks. Several semi-supervised learning approaches have been proposed\nto train neural networks using smaller amounts of labeled data with a large\namount of unlabeled data. The performance of these semi-supervised methods\nsignificantly degrades as the size of labeled data decreases. We introduce\nMutual-information-based Unsupervised & Semi-supervised Concurrent LEarning\n(MUSCLE), a hybrid learning approach that uses mutual information to combine\nboth unsupervised and semi-supervised learning. MUSCLE can be used as a\nstand-alone training scheme for neural networks, and can also be incorporated\ninto other learning approaches. We show that the proposed hybrid model\noutperforms state of the art on several standard benchmarks, including\nCIFAR-10, CIFAR-100, and Mini-Imagenet. Furthermore, the performance gain\nconsistently increases with the reduction in the amount of labeled data, as\nwell as in the presence of bias. We also show that MUSCLE has the potential to\nboost the classification performance when used in the fine-tuning phase for a\nmodel pre-trained only on unlabeled data.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 23:01:04 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Xie", "Hanchen", ""], ["Hussein", "Mohamed E.", ""], ["Galstyan", "Aram", ""], ["Abd-Almageed", "Wael", ""]]}, {"id": "2012.00172", "submitter": "Maxwell Van Gelder", "authors": "Maxwell Van Gelder, Mitchell Wortsman, Kiana Ehsani", "title": "Deconstructing the Structure of Sparse Neural Networks", "comments": "6 pages, 4 figures, Accepted to ML-Retrospectives, Surveys &\n  Meta-Analyses @ NeurIPS 2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although sparse neural networks have been studied extensively, the focus has\nbeen primarily on accuracy. In this work, we focus instead on network\nstructure, and analyze three popular algorithms. We first measure performance\nwhen structure persists and weights are reset to a different random\ninitialization, thereby extending experiments in Deconstructing Lottery Tickets\n(Zhou et al., 2019). This experiment reveals that accuracy can be derived from\nstructure alone. Second, to measure structural robustness we investigate the\nsensitivity of sparse neural networks to further pruning after training,\nfinding a stark contrast between algorithms. Finally, for a recent dynamic\nsparsity algorithm we investigate how early in training the structure emerges.\nWe find that even after one epoch the structure is mostly determined, allowing\nus to propose a more efficient algorithm which does not require dense gradients\nthroughout training. In looking back at algorithms for sparse neural networks\nand analyzing their performance from a different lens, we uncover several\ninteresting properties and promising directions for future research.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 23:51:33 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Van Gelder", "Maxwell", ""], ["Wortsman", "Mitchell", ""], ["Ehsani", "Kiana", ""]]}, {"id": "2012.00179", "submitter": "Benjamin Choi", "authors": "Benjamin Choi, John Kamalu", "title": "Crowd-Sourced Road Quality Mapping in the Developing World", "comments": "Presented at NeurIPS 2020 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Road networks are among the most essential components of a country's\ninfrastructure. By facilitating the movement and exchange of goods, people, and\nideas, they support economic and cultural activity both within and across\nborders. Up-to-date mapping of the the geographical distribution of roads and\ntheir quality is essential in high-impact applications ranging from land use\nplanning to wilderness conservation. Mapping presents a particularly pressing\nchallenge in developing countries, where documentation is poor and\ndisproportionate amounts of road construction are expected to occur in the\ncoming decades. We present a new crowd-sourced approach capable of assessing\nroad quality and identify key challenges and opportunities in the\ntransferability of deep learning based methods across domains.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 00:10:36 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Choi", "Benjamin", ""], ["Kamalu", "John", ""]]}, {"id": "2012.00191", "submitter": "Aliaksei Petsiuk", "authors": "Aliaksei L. Petsiuk and Joshua M. Pearce", "title": "Open Source 3-D Filament Diameter Sensor for Recycling, Winding and\n  Additive Manufacturing Machines", "comments": "25 pages, 16 figures, 2 tables", "journal-ref": null, "doi": "10.1115/1.4050762", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  To overcome the challenge of upcycling plastic waste into 3-D printing\nfilament in the distributed recycling and additive manufacturing systems, this\nstudy designs, builds, tests and validates an open source 3-D filament diameter\nsensor for recycling and winding machines. The modular system for multi-axis\noptical control of the diameter of the recycled 3-D-printer filament makes it\npossible to analyze the surface structure of the processed filament, save the\nhistory of measurements along the entire length of the spool, as well as mark\ndefective areas. The sensor is developed as an independent module and\nintegrated into a recyclebot. The diameter sensor was tested on different kinds\nof polymers (ABS, PLA) different sources of plastic (recycled 3-D prints and\nvirgin plastic waste) and different colors including clear plastic. The results\nof the diameter measurements using the camera were compared with the manual\nmeasurements, and the measurements obtained with a one-dimensional digital\nlight caliper. The results found that the developed open source filament\nsensing method allows users to obtain significantly more information in\ncomparison with basic one-dimensional light sensors and using the received data\nnot only for more accurate diameter measurements, but also for a detailed\nanalysis of the recycled filament surface. The developed method ensures greater\navailability of plastics recycling technologies for the manufacturing community\nand stimulates the growth of composite materials creation. The presented system\ncan greatly enhance the user possibilities and serve as a starting point for a\ncomplete recycling control system that will regulate motor parameters to\nachieve the desired filament diameter with acceptable deviations and even\ncontrol the extrusion rate on a printer to recover from filament\nirregularities.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 00:54:47 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Petsiuk", "Aliaksei L.", ""], ["Pearce", "Joshua M.", ""]]}, {"id": "2012.00204", "submitter": "Peng Peng", "authors": "Peng Peng and Jiugen Wang", "title": "How to fine-tune deep neural networks in few-shot learning?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has been widely used in data-intensive applications. However,\ntraining a deep neural network often requires a large data set. When there is\nnot enough data available for training, the performance of deep learning models\nis even worse than that of shallow networks. It has been proved that few-shot\nlearning can generalize to new tasks with few training samples. Fine-tuning of\na deep model is simple and effective few-shot learning method. However, how to\nfine-tune deep learning models (fine-tune convolution layer or BN layer?) still\nlack deep investigation. Hence, we study how to fine-tune deep models through\nexperimental comparison in this paper. Furthermore, the weight of the models is\nanalyzed to verify the feasibility of the fine-tuning method.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 01:20:59 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Peng", "Peng", ""], ["Wang", "Jiugen", ""]]}, {"id": "2012.00212", "submitter": "Shuaicheng Liu Prof.", "authors": "Kunming Luo, Chuan Wang, Shuaicheng Liu, Haoqiang Fan, Jue Wang, Jian\n  Sun", "title": "UPFlow: Upsampling Pyramid for Unsupervised Optical Flow Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present an unsupervised learning approach for optical flow estimation by\nimproving the upsampling and learning of pyramid network. We design a\nself-guided upsample module to tackle the interpolation blur problem caused by\nbilinear upsampling between pyramid levels. Moreover, we propose a pyramid\ndistillation loss to add supervision for intermediate levels via distilling the\nfinest flow as pseudo labels. By integrating these two components together, our\nmethod achieves the best performance for unsupervised optical flow learning on\nmultiple leading benchmarks, including MPI-SIntel, KITTI 2012 and KITTI 2015.\nIn particular, we achieve EPE=1.4 on KITTI 2012 and F1=9.38% on KITTI 2015,\nwhich outperform the previous state-of-the-art methods by 22.2% and 15.7%,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 01:57:46 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 05:26:17 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Luo", "Kunming", ""], ["Wang", "Chuan", ""], ["Liu", "Shuaicheng", ""], ["Fan", "Haoqiang", ""], ["Wang", "Jue", ""], ["Sun", "Jian", ""]]}, {"id": "2012.00230", "submitter": "Cheng Lin", "authors": "Cheng Lin, Changjian Li, Yuan Liu, Nenglun Chen, Yi-King Choi, Wenping\n  Wang", "title": "Point2Skeleton: Learning Skeletal Representations from Point Clouds", "comments": "Accepted to CVPR2021 (oral). Project:\n  https://github.com/clinplayer/Point2Skeleton", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Point2Skeleton, an unsupervised method to learn skeletal\nrepresentations from point clouds. Existing skeletonization methods are limited\nto tubular shapes and the stringent requirement of watertight input, while our\nmethod aims to produce more generalized skeletal representations for complex\nstructures and handle point clouds. Our key idea is to use the insights of the\nmedial axis transform (MAT) to capture the intrinsic geometric and topological\nnatures of the original input points. We first predict a set of skeletal points\nby learning a geometric transformation, and then analyze the connectivity of\nthe skeletal points to form skeletal mesh structures. Extensive evaluations and\ncomparisons show our method has superior performance and robustness. The\nlearned skeletal representation will benefit several unsupervised tasks for\npoint clouds, such as surface reconstruction and segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 03:04:09 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 04:24:20 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Lin", "Cheng", ""], ["Li", "Changjian", ""], ["Liu", "Yuan", ""], ["Chen", "Nenglun", ""], ["Choi", "Yi-King", ""], ["Wang", "Wenping", ""]]}, {"id": "2012.00234", "submitter": "Xuesong Shi", "authors": "Dongjiang Li, Jinyu Miao, Xuesong Shi, Yuxin Tian, Qiwei Long, Tianyu\n  Cai, Ping Guo, Hongfei Yu, Wei Yang, Haosong Yue, Qi Wei, Fei Qiao", "title": "RaP-Net: A Region-wise and Point-wise Weighting Network to Extract\n  Robust Features for Indoor Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction plays an important role in visual localization. Unreliable\nfeatures on dynamic objects or repetitive regions will disturb robust feature\nmatching and thus, challenge indoor localization greatly. To conquer such an\nissue, we propose a novel network, RaP-Net, to simultaneously predict\nregion-wise invariability and point-wise reliability, and then extract features\nby considering both of them. We also introduce a new dataset, named\nOpenLORIS-Location, to train proposed network. The dataset contains 1553 indoor\nimages from 93 indoor locations. Various appearance changes between images of\nthe same location are included and they can help to learn the invariability in\ntypical indoor scenes. Experimental results show that the proposed RaP-Net\ntrained with the OpenLORIS-Location dataset achieves an excellent performance\nin the feature matching task and significantly outperforms state-of-the-arts\nfeature algorithms in indoor localization. The RaP-Net code and dataset are\navailable at https://github.com/ivipsourcecode/RaP-Net.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 03:12:09 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 14:41:33 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Li", "Dongjiang", ""], ["Miao", "Jinyu", ""], ["Shi", "Xuesong", ""], ["Tian", "Yuxin", ""], ["Long", "Qiwei", ""], ["Cai", "Tianyu", ""], ["Guo", "Ping", ""], ["Yu", "Hongfei", ""], ["Yang", "Wei", ""], ["Yue", "Haosong", ""], ["Wei", "Qi", ""], ["Qiao", "Fei", ""]]}, {"id": "2012.00238", "submitter": "Nithin Raghavan", "authors": "Nithin Raghavan, Punarjay Chakravarty, Shubham Shrivastava", "title": "Sim2Real for Self-Supervised Monocular Depth and Segmentation", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based learning methods for autonomous vehicle perception tasks require\nlarge quantities of labelled, real data in order to properly train without\noverfitting, which can often be incredibly costly. While leveraging the power\nof simulated data can potentially aid in mitigating these costs, networks\ntrained in the simulation domain usually fail to perform adequately when\napplied to images in the real domain. Recent advances in domain adaptation have\nindicated that a shared latent space assumption can help to bridge the gap\nbetween the simulation and real domains, allowing the transference of the\npredictive capabilities of a network from the simulation domain to the real\ndomain. We demonstrate that a twin VAE-based architecture with a shared latent\nspace and auxiliary decoders is able to bridge the sim2real gap without\nrequiring any paired, ground-truth data in the real domain. Using only paired,\nground-truth data in the simulation domain, this architecture has the potential\nto generate perception tasks such as depth and segmentation maps. We compare\nthis method to networks trained in a supervised manner to indicate the merit of\nthese results.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 03:25:02 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Raghavan", "Nithin", ""], ["Chakravarty", "Punarjay", ""], ["Shrivastava", "Shubham", ""]]}, {"id": "2012.00242", "submitter": "Weixuan Sun", "authors": "Weixuan Sun, Jing Zhang, Nick Barnes", "title": "3D Guided Weakly Supervised Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pixel-wise clean annotation is necessary for fully-supervised semantic\nsegmentation, which is laborious and expensive to obtain. In this paper, we\npropose a weakly supervised 2D semantic segmentation model by incorporating\nsparse bounding box labels with available 3D information, which is much easier\nto obtain with advanced sensors. We manually labeled a subset of the 2D-3D\nSemantics(2D-3D-S) dataset with bounding boxes, and introduce our 2D-3D\ninference module to generate accurate pixel-wise segment proposal masks. Guided\nby 3D information, we first generate a point cloud of objects and calculate\nobjectness probability score for each point. Then we project the point cloud\nwith objectness probabilities back to 2D images followed by a refinement step\nto obtain segment proposals, which are treated as pseudo labels to train a\nsemantic segmentation network. Our method works in a recursive manner to\ngradually refine the above-mentioned segment proposals. Extensive experimental\nresults on the 2D-3D-S dataset show that the proposed method can generate\naccurate segment proposals when bounding box labels are available on only a\nsmall subset of training images. Performance comparison with recent\nstate-of-the-art methods further illustrates the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 03:34:15 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Sun", "Weixuan", ""], ["Zhang", "Jing", ""], ["Barnes", "Nick", ""]]}, {"id": "2012.00253", "submitter": "Cheng Yan", "authors": "Cheng Yan, Xin Li, Guoqiang Li", "title": "A New Action Recognition Framework for Video Highlights Summarization in\n  Sporting Events", "comments": "18 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To date, machine learning for human action recognition in video has been\nwidely implemented in sports activities. Although some studies have been\nsuccessful in the past, precision is still the most significant concern. In\nthis study, we present a high-accuracy framework to automatically clip the\nsports video stream by using a three-level prediction algorithm based on two\nclassical open-source structures, i.e., YOLO-v3 and OpenPose. It is found that\nby using a modest amount of sports video training data, our methodology can\nperform sports activity highlights clipping accurately. Comparing with the\nprevious systems, our methodology shows some advantages in accuracy. This study\nmay serve as a new clipping system to extend the potential applications of the\nvideo summarization in sports field, as well as facilitates the development of\nmatch analysis system.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 04:14:40 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Yan", "Cheng", ""], ["Li", "Xin", ""], ["Li", "Guoqiang", ""]]}, {"id": "2012.00257", "submitter": "Andrew Shepley", "authors": "Andrew Shepley, Greg Falzon, Paul Kwan", "title": "Confluence: A Robust Non-IoU Alternative to Non-Maxima Suppression in\n  Object Detection", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Confluence is a novel non-Intersection over Union (IoU) alternative to\nNon-Maxima Suppression (NMS) in bounding box post-processing in object\ndetection. It overcomes the inherent limitations of IoU-based NMS variants to\nprovide a more stable, consistent predictor of bounding box clustering by using\na normalized Manhattan Distance inspired proximity metric to represent bounding\nbox clustering. Unlike Greedy and Soft NMS, it does not rely solely on\nclassification confidence scores to select optimal bounding boxes, instead\nselecting the box which is closest to every other box within a given cluster\nand removing highly confluent neighboring boxes. Confluence is experimentally\nvalidated on the MS COCO and CrowdHuman benchmarks, improving Average Precision\nby up to 2.3-3.8% and Average Recall by up to 5.3-7.2% when compared against\nde-facto standard and state of the art NMS variants. Quantitative results are\nsupported by extensive qualitative analysis and threshold sensitivity analysis\nexperiments support the conclusion that Confluence is more robust than NMS\nvariants. Confluence represents a paradigm shift in bounding box processing,\nwith potential to replace IoU in bounding box regression processes.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 04:22:01 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 03:29:54 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Shepley", "Andrew", ""], ["Falzon", "Greg", ""], ["Kwan", "Paul", ""]]}, {"id": "2012.00282", "submitter": "Sunhee Hwang", "authors": "Sunhee Hwang, Sungho Park, Dohyung Kim, Mirae Do, Hyeran Byun", "title": "FairFaceGAN: Fairness-aware Facial Image-to-Image Translation", "comments": "The 31st British Machine Vision Conference (BMVC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce FairFaceGAN, a fairness-aware facial\nImage-to-Image translation model, mitigating the problem of unwanted\ntranslation in protected attributes (e.g., gender, age, race) during facial\nattributes editing. Unlike existing models, FairFaceGAN learns fair\nrepresentations with two separate latents - one related to the target\nattributes to translate, and the other unrelated to them. This strategy enables\nFairFaceGAN to separate the information about protected attributes and that of\ntarget attributes. It also prevents unwanted translation in protected\nattributes while target attributes editing. To evaluate the degree of fairness,\nwe perform two types of experiments on CelebA dataset. First, we compare the\nfairness-aware classification performances when augmenting data by existing\nimage translation methods and FairFaceGAN respectively. Moreover, we propose a\nnew fairness metric, namely Frechet Protected Attribute Distance (FPAD), which\nmeasures how well protected attributes are preserved. Experimental results\ndemonstrate that FairFaceGAN shows consistent improvements in terms of fairness\nover the existing image translation models. Further, we also evaluate image\ntranslation performances, where FairFaceGAN shows competitive results, compared\nto those of existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 05:43:46 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 05:38:48 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Hwang", "Sunhee", ""], ["Park", "Sungho", ""], ["Kim", "Dohyung", ""], ["Do", "Mirae", ""], ["Byun", "Hyeran", ""]]}, {"id": "2012.00287", "submitter": "Takayuki Osakabe", "authors": "Takayuki Osakabe, Miki Tanaka, Yuma Kinoshita, Hitoshi Kiya", "title": "CycleGAN without checkerboard artifacts for counter-forensics of\n  fake-image detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel CycleGAN without checkerboard artifacts for\ncounter-forensics of fake-image detection. Recent rapid advances in image\nmanipulation tools and deep image synthesis techniques, such as Generative\nAdversarial Networks (GANs) have easily generated fake images, so detecting\nmanipulated images has become an urgent issue. Most state-of-the-art forgery\ndetection methods assume that images include checkerboard artifacts which are\ngenerated by using DNNs. Accordingly, we propose a novel CycleGAN without any\ncheckerboard artifacts for counter-forensics of fake-mage detection methods for\nthe first time, as an example of GANs without checkerboard artifacts.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 06:08:37 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Osakabe", "Takayuki", ""], ["Tanaka", "Miki", ""], ["Kinoshita", "Yuma", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "2012.00301", "submitter": "Liyuan Pan Miss", "authors": "Liyuan Pan, Shah Chowdhury, Richard Hartley, Miaomiao Liu, Hongguang\n  Zhang, and Hongdong Li", "title": "Dual Pixel Exploration: Simultaneous Depth Estimation and Image\n  Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dual-pixel (DP) hardware works by splitting each pixel in half and\ncreating an image pair in a single snapshot. Several works estimate\ndepth/inverse depth by treating the DP pair as a stereo pair. However,\ndual-pixel disparity only occurs in image regions with the defocus blur. The\nheavy defocus blur in DP pairs affects the performance of matching-based depth\nestimation approaches. Instead of removing the blur effect blindly, we study\nthe formation of the DP pair which links the blur and the depth information. In\nthis paper, we propose a mathematical DP model which can benefit depth\nestimation by the blur. These explorations motivate us to propose an end-to-end\nDDDNet (DP-based Depth and Deblur Network) to jointly estimate the depth and\nrestore the image. Moreover, we define a reblur loss, which reflects the\nrelationship of the DP image formation process with depth information, to\nregularise our depth estimate in training. To meet the requirement of a large\namount of data for learning, we propose the first DP image simulator which\nallows us to create datasets with DP pairs from any existing RGBD dataset. As a\nside contribution, we collect a real dataset for further research. Extensive\nexperimental evaluation on both synthetic and real datasets shows that our\napproach achieves competitive performance compared to state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 06:53:57 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Pan", "Liyuan", ""], ["Chowdhury", "Shah", ""], ["Hartley", "Richard", ""], ["Liu", "Miaomiao", ""], ["Zhang", "Hongguang", ""], ["Li", "Hongdong", ""]]}, {"id": "2012.00308", "submitter": "Tobias Schlagenhauf", "authors": "Tobias Schlagenhauf, Tim Brander, Juergen Fleischer", "title": "A Stitching Algorithm for Automated Surface Inspection of Rotationally\n  Symmetric Components", "comments": "9 pages, 13 figures", "journal-ref": "In: CIRP Journal of Manufacturing Science and Technology (35), S.\n  169-177 (2021)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper provides a novel approach to stitching surface images of\nrotationally symmetric parts. It presents a process pipeline that uses a\nfeature-based stitching approach to create a distortion-free and true-to-life\nimage from a video file. The developed process thus enables, for example,\ncondition monitoring without having to view many individual images. For\nvalidation purposes, this will be demonstrated in the paper using the concrete\nexample of a worn ball screw drive spindle. The developed algorithm aims at\nreproducing the functional principle of a line scan camera system, whereby the\nphysical measuring systems are replaced by a feature-based approach. For\nevaluation of the stitching algorithms, metrics are used, some of which have\nonly been developed in this work or have been supplemented by test procedures\nalready in use. The applicability of the developed algorithm is not only\nlimited to machine tool spindles. Instead, the developed method allows a\ngeneral approach to the surface inspection of various rotationally symmetric\ncomponents and can therefore be used in a variety of industrial applications.\nDeep-learning-based detection Algorithms can easily be implemented to generate\na complete pipeline for failure detection and condition monitoring on\nrotationally symmetric parts.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 07:03:45 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 12:20:11 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 16:26:53 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Schlagenhauf", "Tobias", ""], ["Brander", "Tim", ""], ["Fleischer", "Juergen", ""]]}, {"id": "2012.00313", "submitter": "Mengqi Guo", "authors": "Mengqi Guo, Yutong Bai, Zhishuai Zhang, Adam Kortylewski, Alan Yuille", "title": "Unsupervised Part Discovery via Feature Alignment", "comments": "10 pages, 9 figures, submitted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding objects in terms of their individual parts is important,\nbecause it enables a precise understanding of the objects' geometrical\nstructure, and enhances object recognition when the object is seen in a novel\npose or under partial occlusion. However, the manual annotation of parts in\nlarge scale datasets is time consuming and expensive. In this paper, we aim at\ndiscovering object parts in an unsupervised manner, i.e., without ground-truth\npart or keypoint annotations. Our approach builds on the intuition that objects\nof the same class in a similar pose should have their parts aligned at similar\nspatial locations. We exploit the property that neural network features are\nlargely invariant to nuisance variables and the main remaining source of\nvariations between images of the same object category is the object pose.\nSpecifically, given a training image, we find a set of similar images that show\ninstances of the same object category in the same pose, through an affine\nalignment of their corresponding feature maps. The average of the aligned\nfeature maps serves as pseudo ground-truth annotation for a supervised training\nof the deep network backbone. During inference, part detection is simple and\nfast, without any extra modules or overheads other than a feed-forward neural\nnetwork. Our experiments on several datasets from different domains verify the\neffectiveness of the proposed method. For example, we achieve 37.8 mAP on\nVehiclePart, which is at least 4.2 better than previous methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 07:25:00 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Guo", "Mengqi", ""], ["Bai", "Yutong", ""], ["Zhang", "Zhishuai", ""], ["Kortylewski", "Adam", ""], ["Yuille", "Alan", ""]]}, {"id": "2012.00317", "submitter": "Youngwan Lee", "authors": "Youngwan Lee, Hyung-Il Kim, Kimin Yun, Jinyoung Moon", "title": "Diverse Temporal Aggregation and Depthwise Spatiotemporal Factorization\n  for Efficient Video Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video classification researches that have recently attracted attention are\nthe fields of temporal modeling and 3D efficient architecture. However, the\ntemporal modeling methods are not efficient or the 3D efficient architecture is\nless interested in temporal modeling. For bridging the gap between them, we\npropose an efficient temporal modeling 3D architecture, called VoV3D, that\nconsists of a temporal one-shot aggregation (T-OSA) module and depthwise\nfactorized component, D(2+1)D. The T-OSA is devised to build a feature\nhierarchy by aggregating temporal features with different temporal receptive\nfields. Stacking this T-OSA enables the network itself to model short-range as\nwell as long-range temporal relationships across frames without any external\nmodules. Inspired by kernel factorization and channel factorization, we also\ndesign a depthwise spatiotemporal factorization module, named, D(2+1)D that\ndecomposes a 3D depthwise convolution into two spatial and temporal depthwise\nconvolutions for making our network more lightweight and efficient. By using\nthe proposed temporal modeling method (T-OSA), and the efficient factorized\ncomponent (D(2+1)D), we construct two types of VoV3D networks, VoV3D-M and\nVoV3D-L. Thanks to its efficiency and effectiveness of temporal modeling,\nVoV3D-L has 6x fewer model parameters and 16x less computation, surpassing a\nstate-of-the-art temporal modeling method on both Something-Something and\nKinetics-400. Furthermore, VoV3D shows better temporal modeling ability than a\nstate-of-the-art efficient 3D architecture, X3D having comparable model\ncapacity. We hope that VoV3D can serve as a baseline for efficient video\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 07:40:06 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 06:21:50 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 01:40:41 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Lee", "Youngwan", ""], ["Kim", "Hyung-Il", ""], ["Yun", "Kimin", ""], ["Moon", "Jinyoung", ""]]}, {"id": "2012.00318", "submitter": "Mingbao Lin", "authors": "Mingbao Lin, Rongrong Ji, Xiaoshuai Sun, Baochang Zhang, Feiyue Huang,\n  Yonghong Tian, Dacheng Tao", "title": "Fast Class-wise Updating for Online Hashing", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online image hashing has received increasing research attention recently,\nwhich processes large-scale data in a streaming fashion to update the hash\nfunctions on-the-fly. To this end, most existing works exploit this problem\nunder a supervised setting, i.e., using class labels to boost the hashing\nperformance, which suffers from the defects in both adaptivity and efficiency:\nFirst, large amounts of training batches are required to learn up-to-date hash\nfunctions, which leads to poor online adaptivity. Second, the training is\ntime-consuming, which contradicts with the core need of online learning. In\nthis paper, a novel supervised online hashing scheme, termed Fast Class-wise\nUpdating for Online Hashing (FCOH), is proposed to address the above two\nchallenges by introducing a novel and efficient inner product operation. To\nachieve fast online adaptivity, a class-wise updating method is developed to\ndecompose the binary code learning and alternatively renew the hash functions\nin a class-wise fashion, which well addresses the burden on large amounts of\ntraining batches. Quantitatively, such a decomposition further leads to at\nleast 75% storage saving. To further achieve online efficiency, we propose a\nsemi-relaxation optimization, which accelerates the online training by treating\ndifferent binary constraints independently. Without additional constraints and\nvariables, the time complexity is significantly reduced. Such a scheme is also\nquantitatively shown to well preserve past information during updating hashing\nfunctions. We have quantitatively demonstrated that the collective effort of\nclass-wise updating and semi-relaxation optimization provides a superior\nperformance comparing to various state-of-the-art methods, which is verified\nthrough extensive experiments on three widely-used datasets.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 07:41:54 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Lin", "Mingbao", ""], ["Ji", "Rongrong", ""], ["Sun", "Xiaoshuai", ""], ["Zhang", "Baochang", ""], ["Huang", "Feiyue", ""], ["Tian", "Yonghong", ""], ["Tao", "Dacheng", ""]]}, {"id": "2012.00321", "submitter": "Buru Chang", "authors": "Youngkyu Hong, Seungju Han, Kwanghee Choi, Seokjun Seo, Beomsu Kim,\n  Buru Chang", "title": "Disentangling Label Distribution for Long-tailed Visual Recognition", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current evaluation protocol of long-tailed visual recognition trains the\nclassification model on the long-tailed source label distribution and evaluates\nits performance on the uniform target label distribution. Such protocol has\nquestionable practicality since the target may also be long-tailed. Therefore,\nwe formulate long-tailed visual recognition as a label shift problem where the\ntarget and source label distributions are different. One of the significant\nhurdles in dealing with the label shift problem is the entanglement between the\nsource label distribution and the model prediction. In this paper, we focus on\ndisentangling the source label distribution from the model prediction. We first\nintroduce a simple but overlooked baseline method that matches the target label\ndistribution by post-processing the model prediction trained by the\ncross-entropy loss and the Softmax function. Although this method surpasses\nstate-of-the-art methods on benchmark datasets, it can be further improved by\ndirectly disentangling the source label distribution from the model prediction\nin the training phase. Thus, we propose a novel method, LAbel distribution\nDisEntangling (LADE) loss based on the optimal bound of Donsker-Varadhan\nrepresentation. LADE achieves state-of-the-art performance on benchmark\ndatasets such as CIFAR-100-LT, Places-LT, ImageNet-LT, and iNaturalist 2018.\nMoreover, LADE outperforms existing methods on various shifted target label\ndistributions, showing the general adaptability of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 07:56:53 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 15:22:19 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Hong", "Youngkyu", ""], ["Han", "Seungju", ""], ["Choi", "Kwanghee", ""], ["Seo", "Seokjun", ""], ["Kim", "Beomsu", ""], ["Chang", "Buru", ""]]}, {"id": "2012.00328", "submitter": "Camille Couprie", "authors": "Maxime Oquab, Pierre Stock, Oran Gafni, Daniel Haziza, Tao Xu, Peizhao\n  Zhang, Onur Celebi, Yana Hasson, Patrick Labatut, Bobo Bose-Kolanu, Thibault\n  Peyronel, Camille Couprie", "title": "Low Bandwidth Video-Chat Compression using Deep Generative Models", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To unlock video chat for hundreds of millions of people hindered by poor\nconnectivity or unaffordable data costs, we propose to authentically\nreconstruct faces on the receiver's device using facial landmarks extracted at\nthe sender's side and transmitted over the network. In this context, we discuss\nand evaluate the benefits and disadvantages of several deep adversarial\napproaches. In particular, we explore quality and bandwidth trade-offs for\napproaches based on static landmarks, dynamic landmarks or segmentation maps.\nWe design a mobile-compatible architecture based on the first order animation\nmodel of Siarohin et al. In addition, we leverage SPADE blocks to refine\nresults in important areas such as the eyes and lips. We compress the networks\ndown to about 3MB, allowing models to run in real time on iPhone 8 (CPU). This\napproach enables video calling at a few kbits per second, an order of magnitude\nlower than currently available alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 08:17:00 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Oquab", "Maxime", ""], ["Stock", "Pierre", ""], ["Gafni", "Oran", ""], ["Haziza", "Daniel", ""], ["Xu", "Tao", ""], ["Zhang", "Peizhao", ""], ["Celebi", "Onur", ""], ["Hasson", "Yana", ""], ["Labatut", "Patrick", ""], ["Bose-Kolanu", "Bobo", ""], ["Peyronel", "Thibault", ""], ["Couprie", "Camille", ""]]}, {"id": "2012.00332", "submitter": "Sedrick Scott Keh", "authors": "Sedrick Scott Keh", "title": "Semi-Supervised Noisy Student Pre-training on EfficientNet Architectures\n  for Plant Pathology Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, deep learning has vastly improved the identification and\ndiagnosis of various diseases in plants. In this report, we investigate the\nproblem of pathology classification using images of a single leaf. We explore\nthe use of standard benchmark models such as VGG16, ResNet101, and DenseNet 161\nto achieve a 0.945 score on the task. Furthermore, we explore the use of the\nnewer EfficientNet model, improving the accuracy to 0.962. Finally, we\nintroduce the state-of-the-art idea of semi-supervised Noisy Student training\nto the EfficientNet, resulting in significant improvements in both accuracy and\nconvergence rate. The final ensembled Noisy Student model performs very well on\nthe task, achieving a test score of 0.982.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 08:34:03 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Keh", "Sedrick Scott", ""]]}, {"id": "2012.00346", "submitter": "St\\'ephane Lathuili\\`ere", "authors": "Goluck Konuko, Giuseppe Valenzise, St\\'ephane Lathuili\\`ere", "title": "Ultra-low bitrate video conferencing using deep image animation", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work we propose a novel deep learning approach for ultra-low bitrate\nvideo compression for video conferencing applications. To address the\nshortcomings of current video compression paradigms when the available\nbandwidth is extremely limited, we adopt a model-based approach that employs\ndeep neural networks to encode motion information as keypoint displacement and\nreconstruct the video signal at the decoder side. The overall system is trained\nin an end-to-end fashion minimizing a reconstruction error on the encoder\noutput. Objective and subjective quality evaluation experiments demonstrate\nthat the proposed approach provides an average bitrate reduction for the same\nvisual quality of more than 80% compared to HEVC.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 09:06:34 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Konuko", "Goluck", ""], ["Valenzise", "Giuseppe", ""], ["Lathuili\u00e8re", "St\u00e9phane", ""]]}, {"id": "2012.00351", "submitter": "M\\'elodie Boillet", "authors": "M\\'elodie Boillet, Marie-Laurence Bonhomme, Dominique Stutzmann and\n  Christopher Kermorvant", "title": "HORAE: an annotated dataset of books of hours", "comments": null, "journal-ref": "HIP 5 (2019) 7-12", "doi": "10.1145/3352631.3352633", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce in this paper a new dataset of annotated pages from books of\nhours, a type of handwritten prayer books owned and used by rich lay people in\nthe late middle ages. The dataset was created for conducting historical\nresearch on the evolution of the religious mindset in Europe at this period\nsince the book of hours represent one of the major sources of information\nthanks both to their rich illustrations and the different types of religious\nsources they contain. We first describe how the corpus was collected and\nmanually annotated then present the evaluation of a state-of-the-art system for\ntext line detection and for zone detection and typing. The corpus is freely\navailable for research.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 09:25:38 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Boillet", "M\u00e9lodie", ""], ["Bonhomme", "Marie-Laurence", ""], ["Stutzmann", "Dominique", ""], ["Kermorvant", "Christopher", ""]]}, {"id": "2012.00353", "submitter": "Toru Saito", "authors": "Toru Saito, Toshimi Okubo, Naoki Takahashi", "title": "Robust and Accurate Object Velocity Detection by Stereo Camera for\n  Autonomous Driving", "comments": null, "journal-ref": "IEEE Intelligent Vehicles Symposium, 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although the number of camera-based sensors mounted on vehicles has recently\nincreased dramatically, robust and accurate object velocity detection is\ndifficult. Additionally, it is still common to use radar as a fusion system. We\nhave developed a method to accurately detect the velocity of object using a\ncamera, based on a large-scale dataset collected over 20 years by the\nautomotive manufacturer, SUBARU. The proposed method consists of three methods:\nan High Dynamic Range (HDR) detection method that fuses multiple stereo\ndisparity images, a fusion method that combines the results of monocular and\nstereo recognitions, and a new velocity calculation method. The evaluation was\ncarried out using measurement devices and a test course that can quantitatively\nreproduce severe environment by mounting the developed stereo camera on an\nactual vehicle.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 09:29:59 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Saito", "Toru", ""], ["Okubo", "Toshimi", ""], ["Takahashi", "Naoki", ""]]}, {"id": "2012.00362", "submitter": "Ashkan Khakzar", "authors": "Ashkan Khakzar, Soroosh Baselizadeh, Nassir Navab", "title": "Rethinking Positive Aggregation and Propagation of Gradients in\n  Gradient-based Saliency Methods", "comments": "ICML 2020 - Workshop on Human Interpretability in Machine Learning -\n  Spotlight paper - Video at http://whi2020.online/poster_40.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Saliency methods interpret the prediction of a neural network by showing the\nimportance of input elements for that prediction. A popular family of saliency\nmethods utilize gradient information. In this work, we empirically show that\ntwo approaches for handling the gradient information, namely positive\naggregation, and positive propagation, break these methods. Though these\nmethods reflect visually salient information in the input, they do not explain\nthe model prediction anymore as the generated saliency maps are insensitive to\nthe predicted output and are insensitive to model parameter randomization.\nSpecifically for methods that aggregate the gradients of a chosen layer such as\nGradCAM++ and FullGrad, exclusively aggregating positive gradients is\ndetrimental. We further support this by proposing several variants of\naggregation methods with positive handling of gradient information. For methods\nthat backpropagate gradient information such as LRP, RectGrad, and Guided\nBackpropagation, we show the destructive effect of exclusively propagating\npositive gradient information.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 09:38:54 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Khakzar", "Ashkan", ""], ["Baselizadeh", "Soroosh", ""], ["Navab", "Nassir", ""]]}, {"id": "2012.00364", "submitter": "Hanting Chen", "authors": "Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua\n  Liu, Siwei Ma, Chunjing Xu, Chao Xu, Wen Gao", "title": "Pre-Trained Image Processing Transformer", "comments": "To be appeared in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the computing power of modern hardware is increasing strongly, pre-trained\ndeep learning models (e.g., BERT, GPT-3) learned on large-scale datasets have\nshown their effectiveness over conventional methods. The big progress is mainly\ncontributed to the representation ability of transformer and its variant\narchitectures. In this paper, we study the low-level computer vision task\n(e.g., denoising, super-resolution and deraining) and develop a new pre-trained\nmodel, namely, image processing transformer (IPT). To maximally excavate the\ncapability of transformer, we present to utilize the well-known ImageNet\nbenchmark for generating a large amount of corrupted image pairs. The IPT model\nis trained on these images with multi-heads and multi-tails. In addition, the\ncontrastive learning is introduced for well adapting to different image\nprocessing tasks. The pre-trained model can therefore efficiently employed on\ndesired task after fine-tuning. With only one pre-trained model, IPT\noutperforms the current state-of-the-art methods on various low-level\nbenchmarks. Code is available at https://github.com/huawei-noah/Pretrained-IPT\nand https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/IPT\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 09:42:46 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 05:02:15 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 02:41:58 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Chen", "Hanting", ""], ["Wang", "Yunhe", ""], ["Guo", "Tianyu", ""], ["Xu", "Chang", ""], ["Deng", "Yiping", ""], ["Liu", "Zhenhua", ""], ["Ma", "Siwei", ""], ["Xu", "Chunjing", ""], ["Xu", "Chao", ""], ["Gao", "Wen", ""]]}, {"id": "2012.00417", "submitter": "Yuyang Zhao", "authors": "Yuyang Zhao, Zhun Zhong, Fengxiang Yang, Zhiming Luo, Yaojin Lin,\n  Shaozi Li, Nicu Sebe", "title": "Learning to Generalize Unseen Domains via Memory-based Multi-Source\n  Meta-Learning for Person Re-Identification", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in person re-identification (ReID) obtain impressive accuracy\nin the supervised and unsupervised learning settings. However, most of the\nexisting methods need to train a new model for a new domain by accessing data.\nDue to public privacy, the new domain data are not always accessible, leading\nto a limited applicability of these methods. In this paper, we study the\nproblem of multi-source domain generalization in ReID, which aims to learn a\nmodel that can perform well on unseen domains with only several labeled source\ndomains. To address this problem, we propose the Memory-based Multi-Source\nMeta-Learning (M$^3$L) framework to train a generalizable model for unseen\ndomains. Specifically, a meta-learning strategy is introduced to simulate the\ntrain-test process of domain generalization for learning more generalizable\nmodels. To overcome the unstable meta-optimization caused by the parametric\nclassifier, we propose a memory-based identification loss that is\nnon-parametric and harmonizes with meta-learning. We also present a meta batch\nnormalization layer (MetaBN) to diversify meta-test features, further\nestablishing the advantage of meta-learning. Experiments demonstrate that our\nM$^3$L can effectively enhance the generalization ability of the model for\nunseen domains and can outperform the state-of-the-art methods on four\nlarge-scale ReID datasets.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 11:38:16 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 11:14:23 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 09:21:53 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Zhao", "Yuyang", ""], ["Zhong", "Zhun", ""], ["Yang", "Fengxiang", ""], ["Luo", "Zhiming", ""], ["Lin", "Yaojin", ""], ["Li", "Shaozi", ""], ["Sebe", "Nicu", ""]]}, {"id": "2012.00424", "submitter": "Mengbiao Zhao", "authors": "Mengbiao Zhao, Wei Feng, Fei Yin, Xu-Yao Zhang, Cheng-Lin Liu", "title": "Weakly-Supervised Arbitrary-Shaped Text Detection with\n  Expectation-Maximization Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Arbitrary-shaped text detection is an important and challenging task in\ncomputer vision. Most existing methods require heavy data labeling efforts to\nproduce polygon-level text region labels for supervised training. In order to\nreduce the cost in data labeling, we study weakly-supervised arbitrary-shaped\ntext detection for combining various weak supervision forms (e.g., image-level\ntags, coarse, loose and tight bounding boxes), which are far easier for\nannotation. We propose an Expectation-Maximization (EM) based weakly-supervised\nlearning framework to train an accurate arbitrary-shaped text detector using\nonly a small amount of polygon-level annotated data combined with a large\namount of weakly annotated data. Meanwhile, we propose a contour-based\narbitrary-shaped text detector, which is suitable for incorporating\nweakly-supervised learning. Extensive experiments on three arbitrary-shaped\ntext benchmarks (CTW1500, Total-Text and ICDAR-ArT) show that (1) using only\n10% strongly annotated data and 90% weakly annotated data, our method yields\ncomparable performance to state-of-the-art methods, (2) with 100% strongly\nannotated data, our method outperforms existing methods on all three\nbenchmarks. We will make the weakly annotated datasets publicly available in\nthe future.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 11:45:39 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Zhao", "Mengbiao", ""], ["Feng", "Wei", ""], ["Yin", "Fei", ""], ["Zhang", "Xu-Yao", ""], ["Liu", "Cheng-Lin", ""]]}, {"id": "2012.00433", "submitter": "Yao Hu", "authors": "Yao Hu, Guohua Geng, Kang Li, Wei Zhou, Xingxing Hao, Xin Cao", "title": "SRG-Net: Unsupervised Segmentation for Terracotta Warrior Point Cloud\n  with 3D Pointwise CNN methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a seed-region-growing CNN(SRG-Net) for unsupervised\npart segmentation with 3D point clouds of terracotta warriors. Previous neural\nnetwork researches in 3D are mainly about supervised classification,\nclustering, unsupervised representation and reconstruction. There are few\nresearches focusing on unsupervised point cloud part segmentation. To address\nthese problems, we present a seed-region-growing CNN(SRG-Net) for unsupervised\npart segmentation with 3D point clouds of terracotta warriors. Firstly, we\npropose our customized seed region growing algorithm to coarsely segment the\npoint cloud. Then we present our supervised segmentation and unsupervised\nreconstruction networks to better understand the characteristics of 3D point\nclouds. Finally, we combine the SRG algorithm with our improved CNN using a\nrefinement method called SRG-Net to conduct the segmentation tasks on the\nterracotta warriors. Our proposed SRG-Net are evaluated on the terracotta\nwarriors data and the benchmark dataset of ShapeNet with measuring mean\nintersection over union(mIoU) and latency. The experimental results show that\nour SRG-Net outperforms the state-of-the-art methods. Our code is available at\nhttps://github.com/hyoau/SRG-Net.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 12:02:55 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Hu", "Yao", ""], ["Geng", "Guohua", ""], ["Li", "Kang", ""], ["Zhou", "Wei", ""], ["Hao", "Xingxing", ""], ["Cao", "Xin", ""]]}, {"id": "2012.00437", "submitter": "Peng Peng", "authors": "Peng Peng, Yong-Jie Li", "title": "A Unified Structure for Efficient RGB and RGB-D Salient Object Detection", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Salient object detection (SOD) has been well studied in recent years,\nespecially using deep neural networks. However, SOD with RGB and RGB-D images\nis usually treated as two different tasks with different network structures\nthat need to be designed specifically. In this paper, we proposed a unified and\nefficient structure with a cross-attention context extraction (CRACE) module to\naddress both tasks of SOD efficiently. The proposed CRACE module receives and\nappropriately fuses two (for RGB SOD) or three (for RGB-D SOD) inputs. The\nsimple unified feature pyramid network (FPN)-like structure with CRACE modules\nconveys and refines the results under the multi-level supervisions of saliency\nand boundaries. The proposed structure is simple yet effective; the rich\ncontext information of RGB and depth can be appropriately extracted and fused\nby the proposed structure efficiently. Experimental results show that our\nmethod outperforms other state-of-the-art methods in both RGB and RGB-D SOD\ntasks on various datasets and in terms of most metrics.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 12:12:03 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Peng", "Peng", ""], ["Li", "Yong-Jie", ""]]}, {"id": "2012.00451", "submitter": "Antoine Yang", "authors": "Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid", "title": "Just Ask: Learning to Answer Questions from Millions of Narrated Videos", "comments": "20 pages; 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent methods for visual question answering rely on large-scale annotated\ndatasets. Manual annotation of questions and answers for videos, however, is\ntedious, expensive and prevents scalability. In this work, we propose to avoid\nmanual annotation and generate a large-scale training dataset for video\nquestion answering making use of automatic cross-modal supervision. We leverage\na question generation transformer trained on text data and use it to generate\nquestion-answer pairs from transcribed video narrations. Given narrated videos,\nwe then automatically generate the HowToVQA69M dataset with 69M\nvideo-question-answer triplets. To handle the open vocabulary of diverse\nanswers in this dataset, we propose a training procedure based on a contrastive\nloss between a video-question multi-modal transformer and an answer\ntransformer. We introduce the zero-shot VideoQA task and show excellent\nresults, in particular for rare answers. Furthermore, we demonstrate our method\nto significantly outperform the state of the art on MSRVTT-QA, MSVD-QA,\nActivityNet-QA and How2QA. Finally, for a detailed evaluation we introduce a\nnew VideoQA dataset with reduced language biases and high-quality redundant\nmanual annotations. Our code and datasets will be made publicly available at\nhttps://antoyang.github.io/just-ask.html.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 12:59:20 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 14:33:37 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Yang", "Antoine", ""], ["Miech", "Antoine", ""], ["Sivic", "Josef", ""], ["Laptev", "Ivan", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2012.00452", "submitter": "Weizhe Liu", "authors": "Weizhe Liu, Mathieu Salzmann, Pascal Fua", "title": "Counting People by Estimating People Flows", "comments": "Extension of Our ECCV 2020 Paper: arXiv:1911.10782", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern methods for counting people in crowded scenes rely on deep networks to\nestimate people densities in individual images. As such, only very few take\nadvantage of temporal consistency in video sequences, and those that do only\nimpose weak smoothness constraints across consecutive frames. In this paper, we\nadvocate estimating people flows across image locations between consecutive\nimages and inferring the people densities from these flows instead of directly\nregressing them. This enables us to impose much stronger constraints encoding\nthe conservation of the number of people. As a result, it significantly boosts\nperformance without requiring a more complex architecture. Furthermore, it\nallows us to exploit the correlation between people flow and optical flow to\nfurther improve the results. We also show that leveraging people conservation\nconstraints in both a spatial and temporal manner makes it possible to train a\ndeep crowd counting model in an active learning setting with much fewer\nannotations. This significantly reduces the annotation cost while still leading\nto similar performance to the full supervision case.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 12:59:24 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Liu", "Weizhe", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "2012.00458", "submitter": "Yaqing Ding", "authors": "Yaqing Ding, Daniel Barath, Jian Yang, Hui Kong, Zuzana Kukelova", "title": "Globally Optimal Relative Pose Estimation with Gravity Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Smartphones, tablets and camera systems used, e.g., in cars and UAVs, are\ntypically equipped with IMUs (inertial measurement units) that can measure the\ngravity vector accurately. Using this additional information, the $y$-axes of\nthe cameras can be aligned, reducing their relative orientation to a single\ndegree-of-freedom. With this assumption, we propose a novel globally optimal\nsolver, minimizing the algebraic error in the least-squares sense, to estimate\nthe relative pose in the over-determined case. Based on the epipolar\nconstraint, we convert the optimization problem into solving two polynomials\nwith only two unknowns. Also, a fast solver is proposed using the first-order\napproximation of the rotation. The proposed solvers are compared with the\nstate-of-the-art ones on four real-world datasets with approx. 50000 image\npairs in total. Moreover, we collected a dataset, by a smartphone, consisting\nof 10933 image pairs, gravity directions, and ground truth 3D reconstructions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 13:09:59 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 02:38:55 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Ding", "Yaqing", ""], ["Barath", "Daniel", ""], ["Yang", "Jian", ""], ["Kong", "Hui", ""], ["Kukelova", "Zuzana", ""]]}, {"id": "2012.00465", "submitter": "Daniel Barath", "authors": "Yaqing Ding, Daniel Barath, Zuzana Kukelova", "title": "Minimal Solutions for Panoramic Stitching Given Gravity Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When capturing panoramas, people tend to align their cameras with the\nvertical axis, i.e., the direction of gravity. Moreover, modern devices, such\nas smartphones and tablets, are equipped with an IMU (Inertial Measurement\nUnit) that can measure the gravity vector accurately. Using this prior, the\ny-axes of the cameras can be aligned or assumed to be already aligned, reducing\ntheir relative orientation to 1-DOF (degree of freedom). Exploiting this\nassumption, we propose new minimal solutions to panoramic image stitching of\nimages taken by cameras with coinciding optical centers, i.e., undergoing pure\nrotation. We consider four practical camera configurations, assuming unknown\nfixed or varying focal length with or without radial distortion. The solvers\nare tested both on synthetic scenes and on more than 500k real image pairs from\nthe Sun360 dataset and from scenes captured by us using two smartphones\nequipped with IMUs. It is shown, that they outperform the state-of-the-art both\nin terms of accuracy and processing time.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 13:17:36 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Ding", "Yaqing", ""], ["Barath", "Daniel", ""], ["Kukelova", "Zuzana", ""]]}, {"id": "2012.00468", "submitter": "Benedetta Tondi", "authors": "Benedetta Tondi, Andrea Costranzo, Dequ Huang and Bin Li", "title": "Boosting CNN-based primary quantization matrix estimation of double JPEG\n  images via a classification-like architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating the primary quantization matrix of double JPEG compressed images\nis a problem of relevant importance in image forensics since it allows to infer\nimportant information about the past history of an image. In addition, the\ninconsistencies of the primary quantization matrices across different image\nregions can be used to localize splicing in double JPEG tampered images.\nTraditional model-based approaches work under specific assumptions on the\nrelationship between the first and second compression qualities and on the\nalignment of the JPEG grid. Recently, a deep learning-based estimator capable\nto work under a wide variety of conditions has been proposed, that outperforms\ntailored existing methods in most of the cases. The method is based on a\nConvolutional Neural Network (CNN) that is trained to solve the estimation as a\nstandard regression problem. By exploiting the integer nature of the\nquantization coefficients, in this paper, we propose a deep learning technique\nthat performs the estimation by resorting to a simil-classification\narchitecture. The CNN is trained with a loss function that takes into account\nboth the accuracy and the Mean Square Error (MSE) of the estimation. Results\nconfirm the superior performance of the proposed technique, compared to the\nstate-of-the art methods based on statistical analysis and, in particular, deep\nlearning regression. Moreover, the capability of the method to work under\ngeneral operative conditions, regarding the alignment of the second compression\ngrid with the one of first compression and the combinations of the JPEG\nqualities of former and second compression, is very relevant in practical\napplications, where these information are unknown a priori.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 13:20:11 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 19:54:31 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Tondi", "Benedetta", ""], ["Costranzo", "Andrea", ""], ["Huang", "Dequ", ""], ["Li", "Bin", ""]]}, {"id": "2012.00478", "submitter": "Dimas Mart\\'inez", "authors": "Victoria Hern\\'andez-Mederos, Dimas Mart\\'inez, Jorge\n  Estrada-Sarlabous and Valia Guerra-Ones", "title": "Farthest sampling segmentation of triangulated surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we introduce Farthest Sampling Segmentation (FSS), a new method\nfor segmentation of triangulated surfaces, which consists of two fundamental\nsteps: the computation of a submatrix $W^k$ of the affinity matrix $W$ and the\napplication of the k-means clustering algorithm to the rows of $W^k$. The\nsubmatrix $W^k$ is obtained computing the affinity between all triangles and\nonly a few special triangles: those which are farthest in the defined metric.\nThis is equivalent to select a sample of columns of $W$ without constructing it\ncompletely. The proposed method is computationally cheaper than other\nsegmentation algorithms, since it only calculates few columns of $W$ and it\ndoes not require the eigendecomposition of $W$ or of any submatrix of $W$.\n  We prove that the orthogonal projection of $W$ on the space generated by the\ncolumns of $W^k$ coincides with the orthogonal projection of $W$ on the space\ngenerated by the $k$ eigenvectors computed by Nystr\\\"om's method using the\ncolumns of $W^k$ as a sample of $W$. Further, it is shown that for increasing\nsize $k$, the proximity relationship among the rows of $W^k$ tends to\nfaithfully reflect the proximity among the corresponding rows of $W$.\n  The FSS method does not depend on parameters that must be tuned by hand and\nit is very flexible, since it can handle any metric to define the distance\nbetween triangles. Numerical experiments with several metrics and a large\nvariety of 3D triangular meshes show that the segmentations obtained computing\nless than the 10% of columns $W$ are as good as those obtained from clustering\nthe rows of the full matrix $W$.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 13:31:44 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Hern\u00e1ndez-Mederos", "Victoria", ""], ["Mart\u00ednez", "Dimas", ""], ["Estrada-Sarlabous", "Jorge", ""], ["Guerra-Ones", "Valia", ""]]}, {"id": "2012.00504", "submitter": "Boaz Lerner", "authors": "Boaz Lerner, Guy Shiran, Daphna Weinshall", "title": "Boosting the Performance of Semi-Supervised Learning with Unsupervised\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Semi-Supervised Learning (SSL) has shown much promise in leveraging\nunlabeled data while being provided with very few labels. In this paper, we\nshow that ignoring the labels altogether for whole epochs intermittently during\ntraining can significantly improve performance in the small sample regime. More\nspecifically, we propose to train a network on two tasks jointly. The primary\nclassification task is exposed to both the unlabeled and the scarcely annotated\ndata, whereas the secondary task seeks to cluster the data without any labels.\nAs opposed to hand-crafted pretext tasks frequently used in self-supervision,\nour clustering phase utilizes the same classification network and head in an\nattempt to relax the primary task and propagate the information from the labels\nwithout overfitting them. On top of that, the self-supervised technique of\nclassifying image rotations is incorporated during the unsupervised learning\nphase to stabilize training. We demonstrate our method's efficacy in boosting\nseveral state-of-the-art SSL algorithms, significantly improving their results\nand reducing running time in various standard semi-supervised benchmarks,\nincluding 92.6% accuracy on CIFAR-10 and 96.9% on SVHN, using only 4 labels per\nclass in each task. We also notably improve the results in the extreme cases of\n1,2 and 3 labels per class, and show that features learned by our model are\nmore meaningful for separating the data.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 14:19:14 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Lerner", "Boaz", ""], ["Shiran", "Guy", ""], ["Weinshall", "Daphna", ""]]}, {"id": "2012.00514", "submitter": "Amir Rasouli", "authors": "Amir Rasouli, Tiffany Yau, Mohsen Rohani and Jun Luo", "title": "Multi-Modal Hybrid Architecture for Pedestrian Action Prediction", "comments": "7 pages, 4 Figures, 3 tables, submitted to ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pedestrian behavior prediction is one of the major challenges for intelligent\ndriving systems in urban environments. Pedestrians often exhibit a wide range\nof behaviors and adequate interpretations of those depend on various sources of\ninformation such as pedestrian appearance, states of other road users, the\nenvironment layout, etc. To address this problem, we propose a novel\nmulti-modal prediction algorithm that incorporates different sources of\ninformation captured from the environment to predict future crossing actions of\npedestrians. The proposed model benefits from a hybrid learning architecture\nconsisting of feedforward and recurrent networks for analyzing visual features\nof the environment and dynamics of the scene. Using the existing 2D pedestrian\nbehavior benchmarks and a newly annotated 3D driving dataset, we show that our\nproposed model achieves state-of-the-art performance in pedestrian crossing\nprediction.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 15:17:58 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Rasouli", "Amir", ""], ["Yau", "Tiffany", ""], ["Rohani", "Mohsen", ""], ["Luo", "Jun", ""]]}, {"id": "2012.00517", "submitter": "Tuomo Sipola", "authors": "Joni Korpihalkola, Tuomo Sipola, Samir Puuska, Tero Kokkonen", "title": "One-Pixel Attack Deceives Computer-Assisted Diagnosis of Cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision and machine learning can be used to automate various tasks in\ncancer diagnostic and detection. If an attacker can manipulate the automated\nprocessing, the results can be devastating and in the worst case lead to wrong\ndiagnosis and treatment. In this research, the goal is to demonstrate the use\nof one-pixel attacks in a real-life scenario with a real pathology dataset,\nTUPAC16, which consists of digitized whole-slide images. We attack against the\nIBM CODAIT's MAX breast cancer detector using adversarial images. These\nadversarial examples are found using differential evolution to perform the\none-pixel modification to the images in the dataset. The results indicate that\na minor one-pixel modification of a whole slide image under analysis can affect\nthe diagnosis by reversing the automatic diagnosis result. The attack poses a\nthreat from the cyber security perspective: the one-pixel method can be used as\nan attack vector by a motivated attacker.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 14:27:28 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 09:42:34 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 12:06:48 GMT"}, {"version": "v4", "created": "Wed, 16 Jun 2021 12:59:02 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Korpihalkola", "Joni", ""], ["Sipola", "Tuomo", ""], ["Puuska", "Samir", ""], ["Kokkonen", "Tero", ""]]}, {"id": "2012.00558", "submitter": "Christian Cosgrove", "authors": "Christian Cosgrove, Adam Kortylewski, Chenglin Yang, Alan Yuille", "title": "Robustness Out of the Box: Compositional Representations Naturally\n  Defend Against Black-Box Patch Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patch-based adversarial attacks introduce a perceptible but localized change\nto the input that induces misclassification. While progress has been made in\ndefending against imperceptible attacks, it remains unclear how patch-based\nattacks can be resisted. In this work, we study two different approaches for\ndefending against black-box patch attacks. First, we show that adversarial\ntraining, which is successful against imperceptible attacks, has limited\neffectiveness against state-of-the-art location-optimized patch attacks.\nSecond, we find that compositional deep networks, which have part-based\nrepresentations that lead to innate robustness to natural occlusion, are robust\nto patch attacks on PASCAL3D+ and the German Traffic Sign Recognition\nBenchmark, without adversarial training. Moreover, the robustness of\ncompositional models outperforms that of adversarially trained standard models\nby a large margin. However, on GTSRB, we observe that they have problems\ndiscriminating between similar traffic signs with fine-grained differences. We\novercome this limitation by introducing part-based finetuning, which improves\nfine-grained recognition. By leveraging compositional representations, this is\nthe first work that defends against black-box patch attacks without expensive\nadversarial training. This defense is more robust than adversarial training and\nmore interpretable because it can locate and ignore adversarial patches.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 15:04:23 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Cosgrove", "Christian", ""], ["Kortylewski", "Adam", ""], ["Yang", "Chenglin", ""], ["Yuille", "Alan", ""]]}, {"id": "2012.00564", "submitter": "Andrea Romanoni", "authors": "Andrea Romanoni and Matteo Matteucci", "title": "Facetwise Mesh Refinement for Multi-View Stereo", "comments": "Accepted as Oral ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mesh refinement is a fundamental step for accurate Multi-View Stereo. It\nmodifies the geometry of an initial manifold mesh to minimize the photometric\nerror induced in a set of camera pairs. This initial mesh is usually the output\nof volumetric 3D reconstruction based on min-cut over Delaunay Triangulations.\nSuch methods produce a significant amount of non-manifold vertices, therefore\nthey require a vertex split step to explicitly repair them. In this paper, we\nextend this method to preemptively fix the non-manifold vertices by reasoning\ndirectly on the Delaunay Triangulation and avoid most vertex splits. The main\ncontribution of this paper addresses the problem of choosing the camera pairs\nadopted by the refinement process. We treat the problem as a mesh labeling\nprocess, where each label corresponds to a camera pair. Differently from the\nstate-of-the-art methods, which use each camera pair to refine all the visible\nparts of the mesh, we choose, for each facet, the best pair that enforces both\nthe overall visibility and coverage. The refinement step is applied for each\nfacet using only the camera pair selected. This facetwise refinement helps the\nprocess to be applied in the most evenly way possible.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 15:16:56 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Romanoni", "Andrea", ""], ["Matteucci", "Matteo", ""]]}, {"id": "2012.00567", "submitter": "Heng Yin", "authors": "Heng Yin, Hengwei Zhang, Jindong Wang and Ruiyu Dou", "title": "Boosting Adversarial Attacks on Neural Networks with Better Optimizer", "comments": null, "journal-ref": null, "doi": "10.1155/2021/9983309", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have outperformed humans in image recognition\ntasks, but they remain vulnerable to attacks from adversarial examples. Since\nthese data are crafted by adding imperceptible noise to normal images, their\nexistence poses potential security threats to deep learning systems.\nSophisticated adversarial examples with strong attack performance can also be\nused as a tool to evaluate the robustness of a model. However, the success rate\nof adversarial attacks can be further improved in black-box environments.\nTherefore, this study combines a modified Adam gradient descent algorithm with\nthe iterative gradient-based attack method. The proposed Adam Iterative Fast\nGradient Method is then used to improve the transferability of adversarial\nexamples. Extensive experiments on ImageNet showed that the proposed method\noffers a higher attack success rate than existing iterative methods. By\nextending our method, we achieved a state-of-the-art attack success rate of\n95.0% on defense models.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 15:18:19 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 12:40:43 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Yin", "Heng", ""], ["Zhang", "Hengwei", ""], ["Wang", "Jindong", ""], ["Dou", "Ruiyu", ""]]}, {"id": "2012.00573", "submitter": "Fei Ding", "authors": "Fei Ding, Yin Yang, Hongxin Hu, Venkat Krovi, Feng Luo", "title": "Multi-level Knowledge Distillation via Knowledge Alignment and\n  Correlation", "comments": "15 pages, 11 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge distillation (KD) has become an important technique for model\ncompression and knowledge transfer. In this work, we first perform a\ncomprehensive analysis of the knowledge transferred by different KD methods. We\ndemonstrate that traditional KD methods, which minimize the KL divergence of\nsoftmax outputs between networks, are related to the knowledge alignment of an\nindividual sample only. Meanwhile, recent contrastive learning-based KD methods\nmainly transfer relational knowledge between different samples, namely,\nknowledge correlation. While it is important to transfer the full knowledge\nfrom teacher to student, we introduce the Multi-level Knowledge Distillation\n(MLKD) by effectively considering both knowledge alignment and correlation.\nMLKD is task-agnostic and model-agnostic, and can easily transfer knowledge\nfrom supervised or self-supervised pretrained teachers. We show that MLKD can\nimprove the reliability and transferability of learned representations.\nExperiments demonstrate that MLKD outperforms other state-of-the-art methods on\na large number of experimental settings including different (a) pretraining\nstrategies (b) network architectures (c) datasets (d) tasks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 15:27:15 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 00:11:35 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Ding", "Fei", ""], ["Yang", "Yin", ""], ["Hu", "Hongxin", ""], ["Krovi", "Venkat", ""], ["Luo", "Feng", ""]]}, {"id": "2012.00595", "submitter": "Denys Rozumnyi", "authors": "Denys Rozumnyi, Martin R. Oswald, Vittorio Ferrari, Jiri Matas, Marc\n  Pollefeys", "title": "DeFMO: Deblurring and Shape Recovery of Fast Moving Objects", "comments": "CVPR 2021 camera-ready", "journal-ref": "2021 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR)", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Objects moving at high speed appear significantly blurred when captured with\ncameras. The blurry appearance is especially ambiguous when the object has\ncomplex shape or texture. In such cases, classical methods, or even humans, are\nunable to recover the object's appearance and motion. We propose a method that,\ngiven a single image with its estimated background, outputs the object's\nappearance and position in a series of sub-frames as if captured by a\nhigh-speed camera (i.e. temporal super-resolution). The proposed generative\nmodel embeds an image of the blurred object into a latent space representation,\ndisentangles the background, and renders the sharp appearance. Inspired by the\nimage formation model, we design novel self-supervised loss function terms that\nboost performance and show good generalization capabilities. The proposed DeFMO\nmethod is trained on a complex synthetic dataset, yet it performs well on\nreal-world data from several datasets. DeFMO outperforms the state of the art\nand generates high-quality temporal super-resolution frames.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 16:02:04 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 15:02:41 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 09:14:34 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Rozumnyi", "Denys", ""], ["Oswald", "Martin R.", ""], ["Ferrari", "Vittorio", ""], ["Matas", "Jiri", ""], ["Pollefeys", "Marc", ""]]}, {"id": "2012.00596", "submitter": "Zhengang Li", "authors": "Zhengang Li, Geng Yuan, Wei Niu, Pu Zhao, Yanyu Li, Yuxuan Cai, Xuan\n  Shen, Zheng Zhan, Zhenglun Kong, Qing Jin, Zhiyu Chen, Sijia Liu, Kaiyuan\n  Yang, Bin Ren, Yanzhi Wang, Xue Lin", "title": "NPAS: A Compiler-aware Framework of Unified Network Pruning and\n  Architecture Search for Beyond Real-Time Mobile Acceleration", "comments": "Accepted as an oral paper in the Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing demand to efficiently deploy DNNs on mobile edge devices,\nit becomes much more important to reduce unnecessary computation and increase\nthe execution speed. Prior methods towards this goal, including model\ncompression and network architecture search (NAS), are largely performed\nindependently and do not fully consider compiler-level optimizations which is a\nmust-do for mobile acceleration. In this work, we first propose (i) a general\ncategory of fine-grained structured pruning applicable to various DNN layers,\nand (ii) a comprehensive, compiler automatic code generation framework\nsupporting different DNNs and different pruning schemes, which bridge the gap\nof model compression and NAS. We further propose NPAS, a compiler-aware unified\nnetwork pruning, and architecture search. To deal with large search space, we\npropose a meta-modeling procedure based on reinforcement learning with fast\nevaluation and Bayesian optimization, ensuring the total number of training\nepochs comparable with representative NAS frameworks. Our framework achieves\n6.7ms, 5.9ms, 3.9ms ImageNet inference times with 78.2%, 75% (MobileNet-V3\nlevel), and 71% (MobileNet-V2 level) Top-1 accuracy respectively on an\noff-the-shelf mobile phone, consistently outperforming prior work.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 16:03:40 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 04:18:34 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 00:07:48 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Li", "Zhengang", ""], ["Yuan", "Geng", ""], ["Niu", "Wei", ""], ["Zhao", "Pu", ""], ["Li", "Yanyu", ""], ["Cai", "Yuxuan", ""], ["Shen", "Xuan", ""], ["Zhan", "Zheng", ""], ["Kong", "Zhenglun", ""], ["Jin", "Qing", ""], ["Chen", "Zhiyu", ""], ["Liu", "Sijia", ""], ["Yang", "Kaiyuan", ""], ["Ren", "Bin", ""], ["Wang", "Yanzhi", ""], ["Lin", "Xue", ""]]}, {"id": "2012.00606", "submitter": "Christof Kauba", "authors": "Christof Kauba, Luca Debiasi and Andreas Uhl", "title": "Enabling Fingerprint Presentation Attacks: Fake Fingerprint Fabrication\n  Techniques and Recognition Performance", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fake fingerprint representation pose a severe threat for fingerprint based\nauthentication systems. Despite advances in presentation attack detection\ntechnologies, which are often integrated directly into the fingerprint scanner\ndevices, many fingerprint scanners are still susceptible to presentation\nattacks using physical fake fingerprint representation. In this work we\nevaluate five different commercial-off-the-shelf fingerprint scanners based on\ndifferent sensing technologies, including optical, optical multispectral,\npassive capacitive, active capacitive and thermal regarding their\nsusceptibility to presentation attacks using fake fingerprint representations.\nSeveral different materials to create the fake representation are tested and\nevaluated, including wax, cast, latex, silicone, different types of glue,\nwindow colours, modelling clay, etc. The quantitative evaluation includes\nassessing the fingerprint quality of the samples captured from the fake\nrepresentations as well as comparison experiments where the achieved matching\nscores of the fake representations against the corresponding real fingerprints\nindicate the effectiveness of the fake representations. Our results confirmed\nthat all except one of the tested devices are susceptible to at least one\ntype/material of fake fingerprint representations.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 16:20:32 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Kauba", "Christof", ""], ["Debiasi", "Luca", ""], ["Uhl", "Andreas", ""]]}, {"id": "2012.00617", "submitter": "Ozan Ciga", "authors": "Ozan Ciga, Tony Xu, Sharon Nofech-Mozes, Shawna Noy, Fang-I Lu, Anne\n  L. Martel", "title": "Overcoming the limitations of patch-based learning to detect cancer in\n  whole slide images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole slide images (WSIs) pose unique challenges when training deep learning\nmodels. They are very large which makes it necessary to break each image down\ninto smaller patches for analysis, image features have to be extracted at\nmultiple scales in order to capture both detail and context, and extreme class\nimbalances may exist. Significant progress has been made in the analysis of\nthese images, thanks largely due to the availability of public annotated\ndatasets. We postulate, however, that even if a method scores well on a\nchallenge task, this success may not translate to good performance in a more\nclinically relevant workflow. Many datasets consist of image patches which may\nsuffer from data curation bias; other datasets are only labelled at the whole\nslide level and the lack of annotations across an image may mask erroneous\nlocal predictions so long as the final decision is correct. In this paper, we\noutline the differences between patch or slide-level classification versus\nmethods that need to localize or segment cancer accurately across the whole\nslide, and we experimentally verify that best practices differ in both cases.\nWe apply a binary cancer detection network on post neoadjuvant therapy breast\ncancer WSIs to find the tumor bed outlining the extent of cancer, a task which\nrequires sensitivity and precision across the whole slide. We extensively study\nmultiple design choices and their effects on the outcome, including\narchitectures and augmentations. Furthermore, we propose a negative data\nsampling strategy, which drastically reduces the false positive rate (7% on\nslide level) and improves each metric pertinent to our problem, with a 15%\nreduction in the error of tumor extent.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 16:37:18 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Ciga", "Ozan", ""], ["Xu", "Tony", ""], ["Nofech-Mozes", "Sharon", ""], ["Noy", "Shawna", ""], ["Lu", "Fang-I", ""], ["Martel", "Anne L.", ""]]}, {"id": "2012.00619", "submitter": "Yan Zhang", "authors": "Yan Zhang and Michael J. Black and Siyu Tang", "title": "We are More than Our Joints: Predicting how 3D Bodies Move", "comments": "camera ready, cvpr", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A key step towards understanding human behavior is the prediction of 3D human\nmotion. Successful solutions have many applications in human tracking, HCI, and\ngraphics. Most previous work focuses on predicting a time series of future 3D\njoint locations given a sequence 3D joints from the past. This Euclidean\nformulation generally works better than predicting pose in terms of joint\nrotations. Body joint locations, however, do not fully constrain 3D human pose,\nleaving degrees of freedom undefined, making it hard to animate a realistic\nhuman from only the joints. Note that the 3D joints can be viewed as a sparse\npoint cloud. Thus the problem of human motion prediction can be seen as point\ncloud prediction. With this observation, we instead predict a sparse set of\nlocations on the body surface that correspond to motion capture markers. Given\nsuch markers, we fit a parametric body model to recover the 3D shape and pose\nof the person. These sparse surface markers also carry detailed information\nabout human movement that is not present in the joints, increasing the\nnaturalness of the predicted motions. Using the AMASS dataset, we train MOJO,\nwhich is a novel variational autoencoder that generates motions from latent\nfrequencies. MOJO preserves the full temporal resolution of the input motion,\nand sampling from the latent frequencies explicitly introduces high-frequency\ncomponents into the generated motion. We note that motion prediction methods\naccumulate errors over time, resulting in joints or markers that diverge from\ntrue human bodies. To address this, we fit SMPL-X to the predictions at each\ntime step, projecting the solution back onto the space of valid bodies. These\nvalid markers are then propagated in time. Experiments show that our method\nproduces state-of-the-art results and realistic 3D body animations. The code\nfor research purposes is at https://yz-cnsdqz.github.io/MOJO/MOJO.html\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 16:41:04 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 13:04:34 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Zhang", "Yan", ""], ["Black", "Michael J.", ""], ["Tang", "Siyu", ""]]}, {"id": "2012.00630", "submitter": "Feixian Zhou", "authors": "Feixiang Zhou, Zheheng Jiang, Zhihua Liu, Fang Chen, Long Chen, Lei\n  Tong, Zhile Yang, Haikuan Wang, Minrui Fei, Ling Li and Huiyu Zhou", "title": "Structured Context Enhancement Network for Mouse Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated analysis of mouse behaviours is crucial for many applications in\nneuroscience. However, quantifying mouse behaviours from videos or images\nremains a challenging problem, where pose estimation plays an important role in\ndescribing mouse behaviours. Although deep learning based methods have made\npromising advances in human pose estimation, they cannot be directly applied to\npose estimation of mice due to different physiological natures. Particularly,\nsince mouse body is highly deformable, it is a challenge to accurately locate\ndifferent keypoints on the mouse body. In this paper, we propose a novel\nHourglass network based model, namely Graphical Model based Structured Context\nEnhancement Network (GM-SCENet) where two effective modules, i.e., Structured\nContext Mixer (SCM) and Cascaded Multi-Level Supervision (CMLS) are\nsubsequently implemented. SCM can adaptively learn and enhance the proposed\nstructured context information of each mouse part by a novel graphical model\nthat takes into account the motion difference between body parts. Then, the\nCMLS module is designed to jointly train the proposed SCM and the Hourglass\nnetwork by generating multi-level information, increasing the robustness of the\nwhole network.Using the multi-level prediction information from SCM and CMLS,\nwe develop an inference method to ensure the accuracy of the localisation\nresults. Finally, we evaluate our proposed approach against several\nbaselines...\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 16:50:19 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 15:10:01 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Zhou", "Feixiang", ""], ["Jiang", "Zheheng", ""], ["Liu", "Zhihua", ""], ["Chen", "Fang", ""], ["Chen", "Long", ""], ["Tong", "Lei", ""], ["Yang", "Zhile", ""], ["Wang", "Haikuan", ""], ["Fei", "Minrui", ""], ["Li", "Ling", ""], ["Zhou", "Huiyu", ""]]}, {"id": "2012.00641", "submitter": "Shiv Ram Dubey", "authors": "Shiv Ram Dubey", "title": "A Decade Survey of Content Based Image Retrieval using Deep Learning", "comments": "Published by IEEE Transactions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2021.3080920", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The content based image retrieval aims to find the similar images from a\nlarge scale dataset against a query image. Generally, the similarity between\nthe representative features of the query image and dataset images is used to\nrank the images for retrieval. In early days, various hand designed feature\ndescriptors have been investigated based on the visual cues such as color,\ntexture, shape, etc. that represent the images. However, the deep learning has\nemerged as a dominating alternative of hand-designed feature engineering from a\ndecade. It learns the features automatically from the data. This paper presents\na comprehensive survey of deep learning based developments in the past decade\nfor content based image retrieval. The categorization of existing\nstate-of-the-art methods from different perspectives is also performed for\ngreater understanding of the progress. The taxonomy used in this survey covers\ndifferent supervision, different networks, different descriptor type and\ndifferent retrieval type. A performance analysis is also performed using the\nstate-of-the-art methods. The insights are also presented for the benefit of\nthe researchers to observe the progress and to make the best choices. The\nsurvey presented in this paper will help in further research progress in image\nretrieval using deep learning.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 02:12:30 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 09:22:01 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Dubey", "Shiv Ram", ""]]}, {"id": "2012.00649", "submitter": "Yang Zhao", "authors": "Yang Zhao, Changyou Chen", "title": "Unpaired Image-to-Image Translation via Latent Energy Transport", "comments": "CVPR2021. Code: https://github.com/YangNaruto/latent-energy-transport", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation aims to preserve source contents while translating\nto discriminative target styles between two visual domains. Most works apply\nadversarial learning in the ambient image space, which could be computationally\nexpensive and challenging to train. In this paper, we propose to deploy an\nenergy-based model (EBM) in the latent space of a pretrained autoencoder for\nthis task. The pretrained autoencoder serves as both a latent code extractor\nand an image reconstruction worker. Our model, LETIT, is based on the\nassumption that two domains share the same latent space, where latent\nrepresentation is implicitly decomposed as a content code and a domain-specific\nstyle code. Instead of explicitly extracting the two codes and applying\nadaptive instance normalization to combine them, our latent EBM can implicitly\nlearn to transport the source style code to the target style code while\npreserving the content code, an advantage over existing image translation\nmethods. This simplified solution is also more efficient in the one-sided\nunpaired image translation setting. Qualitative and quantitative comparisons\ndemonstrate superior translation quality and faithfulness for content\npreservation. Our model is the first to be applicable to\n1024$\\times$1024-resolution unpaired image translation to the best of our\nknowledge.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 17:18:58 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 19:33:19 GMT"}, {"version": "v3", "created": "Sun, 23 May 2021 19:54:38 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zhao", "Yang", ""], ["Chen", "Changyou", ""]]}, {"id": "2012.00650", "submitter": "Ming Lu", "authors": "Ming Lu, Tong Chen, zhenyu Dai, Dong Wang, Dandan Ding, and Zhan Ma", "title": "Decoder-side Cross Resolution Synthesis for Video Compression\n  Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a decoder-side Cross Resolution Synthesis (CRS) module to\npursue better compression efficiency beyond the latest Versatile Video Coding\n(VVC), where we encode intra frames at original high resolution (HR), compress\ninter frames at a lower resolution (LR), and then super-resolve decoded LR\ninter frames with the help from preceding HR intra and neighboring LR inter\nframes. For a LR inter frame, a motion alignment and aggregation network (MAN)\nis devised to produce temporally aggregated motion representation (AMR) for the\nguarantee of temporal smoothness; Another texture compensation network (TCN)\ninputs decoded HR intra frame, re-sampled HR intra frame, and this LR inter\nframe to generate multiscale affinity map (MAM) and multiscale texture\nrepresentation (MTR) for better augmenting spatial details; Finally,\nsimilarity-driven fusion synthesizes AMR, MTR, MAM to upscale LR inter frame\nfor the removal of compression and resolution re-sampling noises. We enhance\nthe VVC using proposed CRS, showing averaged 8.76% and 11.93% Bj{\\o}ntegaard\nDelta Rate (BD-Rate) gains against the latest VVC anchor in Random Access (RA)\nand Low-delay P (LDP) settings respectively. In addition, experimental\ncomparisons to the state-of-the-art super-resolution (SR) based VVC enhancement\nmethods, and ablation studies are conducted to further report superior\nefficiency and generalization of proposed algorithm. All materials will be made\nto public at https://njuvision.github.io/CRS for reproducible research.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 17:23:53 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 06:16:32 GMT"}, {"version": "v3", "created": "Wed, 30 Jun 2021 05:20:27 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Lu", "Ming", ""], ["Chen", "Tong", ""], ["Dai", "zhenyu", ""], ["Wang", "Dong", ""], ["Ding", "Dandan", ""], ["Ma", "Zhan", ""]]}, {"id": "2012.00656", "submitter": "Perrine Paul-Gilloteaux", "authors": "Stephan Kunne (1), Guillaume Potier (1), Jean M\\'erot (1), Perrine\n  Paul-Gilloteaux (1 and 2) ((1) l'institut du thorax Nantes (2) MicroPICell\n  SFR Sante F. Bonamy)", "title": "Cross-modal registration using point clouds and graph-matching in the\n  context of correlative microscopies", "comments": "in Proceedings of iTWIST'20, Paper-ID: 22, Nantes, France, December,\n  2-4, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.TO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Correlative microscopy aims at combining two or more modalities to gain more\ninformation than the one provided by one modality on the same biological\nstructure. Registration is needed at different steps of correlative\nmicroscopies workflows. Biologists want to select the image content used for\nregistration not to introduce bias in the correlation of unknown structures.\nIntensity-based methods might not allow this selection and might be too slow\nwhen the images are very large. We propose an approach based on point clouds\ncreated from selected content by the biologist. These point clouds may be prone\nto big differences in densities but also missing parts and outliers. In this\npaper we present a method of registration for point clouds based on graph\nbuilding and graph matching, and compare the method to iterative closest point\nbased methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 17:27:00 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Kunne", "Stephan", "", "1 and 2"], ["Potier", "Guillaume", "", "1 and 2"], ["M\u00e9rot", "Jean", "", "1 and 2"], ["Paul-Gilloteaux", "Perrine", "", "1 and 2"]]}, {"id": "2012.00659", "submitter": "Raghav Puri", "authors": "Raghav Puri, Archit Gupta, Manas Sikri, Mohit Tiwari, Nitish Pathak,\n  Shivendra Goel", "title": "Emotion Detection using Image Processing in Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, user's emotion using its facial expressions will be detected.\nThese expressions can be derived from the live feed via system's camera or any\npre-exisiting image available in the memory. Emotions possessed by humans can\nbe recognized and has a vast scope of study in the computer vision industry\nupon which several researches have already been done. The work has been\nimplemented using Python (2.7, Open Source Computer Vision Library (OpenCV) and\nNumPy. The scanned image(testing dataset) is being compared to the training\ndataset and thus emotion is predicted. The objective of this paper is to\ndevelop a system which can analyze the image and predict the expression of the\nperson. The study proves that this procedure is workable and produces valid\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 17:34:35 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Puri", "Raghav", ""], ["Gupta", "Archit", ""], ["Sikri", "Manas", ""], ["Tiwari", "Mohit", ""], ["Pathak", "Nitish", ""], ["Goel", "Shivendra", ""]]}, {"id": "2012.00682", "submitter": "Minyoung Kim", "authors": "Minyoung Kim, Ricardo Guerrero, Vladimir Pavlovic", "title": "Learning Disentangled Latent Factors from Paired Data in Cross-Modal\n  Retrieval: An Implicit Identifiable VAE Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We deal with the problem of learning the underlying disentangled latent\nfactors that are shared between the paired bi-modal data in cross-modal\nretrieval. Our assumption is that the data in both modalities are complex,\nstructured, and high dimensional (e.g., image and text), for which the\nconventional deep auto-encoding latent variable models such as the Variational\nAutoencoder (VAE) often suffer from difficulty of accurate decoder training or\nrealistic synthesis. A suboptimally trained decoder can potentially harm the\nmodel's capability of identifying the true factors. In this paper we propose a\nnovel idea of the implicit decoder, which completely removes the ambient data\ndecoding module from a latent variable model, via implicit encoder inversion\nthat is achieved by Jacobian regularization of the low-dimensional embedding\nfunction. Motivated from the recent Identifiable VAE (IVAE) model, we modify it\nto incorporate the query modality data as conditioning auxiliary input, which\nallows us to prove that the true parameters of the model can be identified\nunder some regularity conditions. Tested on various datasets where the true\nfactors are fully/partially available, our model is shown to identify the\nfactors accurately, significantly outperforming conventional encoder-decoder\nlatent variable models. We also test our model on the Recipe1M, the large-scale\nfood image/recipe dataset, where the learned factors by our approach highly\ncoincide with the most pronounced food factors that are widely agreed on,\nincluding savoriness, wateriness, and greenness.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 17:47:50 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Kim", "Minyoung", ""], ["Guerrero", "Ricardo", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "2012.00720", "submitter": "Yanwei Li", "authors": "Yanwei Li, Hengshuang Zhao, Xiaojuan Qi, Liwei Wang, Zeming Li, Jian\n  Sun, Jiaya Jia", "title": "Fully Convolutional Networks for Panoptic Segmentation", "comments": "CVPR2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a conceptually simple, strong, and efficient\nframework for panoptic segmentation, called Panoptic FCN. Our approach aims to\nrepresent and predict foreground things and background stuff in a unified fully\nconvolutional pipeline. In particular, Panoptic FCN encodes each object\ninstance or stuff category into a specific kernel weight with the proposed\nkernel generator and produces the prediction by convolving the high-resolution\nfeature directly. With this approach, instance-aware and semantically\nconsistent properties for things and stuff can be respectively satisfied in a\nsimple generate-kernel-then-segment workflow. Without extra boxes for\nlocalization or instance separation, the proposed approach outperforms previous\nbox-based and -free models with high efficiency on COCO, Cityscapes, and\nMapillary Vistas datasets with single scale input. Our code is made publicly\navailable at https://github.com/Jia-Research-Lab/PanopticFCN.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 18:31:41 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 04:28:54 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Li", "Yanwei", ""], ["Zhao", "Hengshuang", ""], ["Qi", "Xiaojuan", ""], ["Wang", "Liwei", ""], ["Li", "Zeming", ""], ["Sun", "Jian", ""], ["Jia", "Jiaya", ""]]}, {"id": "2012.00726", "submitter": "Zachary Teed", "authors": "Zachary Teed and Jia Deng", "title": "RAFT-3D: Scene Flow using Rigid-Motion Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of scene flow: given a pair of stereo or RGB-D video\nframes, estimate pixelwise 3D motion. We introduce RAFT-3D, a new deep\narchitecture for scene flow. RAFT-3D is based on the RAFT model developed for\noptical flow but iteratively updates a dense field of pixelwise SE3 motion\ninstead of 2D motion. A key innovation of RAFT-3D is rigid-motion embeddings,\nwhich represent a soft grouping of pixels into rigid objects. Integral to\nrigid-motion embeddings is Dense-SE3, a differentiable layer that enforces\ngeometric consistency of the embeddings. Experiments show that RAFT-3D achieves\nstate-of-the-art performance. On FlyingThings3D, under the two-view evaluation,\nwe improved the best published accuracy (d < 0.05) from 34.3% to 83.7%. On\nKITTI, we achieve an error of 5.77, outperforming the best published method\n(6.31), despite using no object instance supervision. Code is available at\nhttps://github.com/princeton-vl/RAFT-3D.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 18:38:18 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 17:11:38 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Teed", "Zachary", ""], ["Deng", "Jia", ""]]}, {"id": "2012.00739", "submitter": "Kelvin C.K. Chan", "authors": "Kelvin C.K. Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, Chen Change Loy", "title": "GLEAN: Generative Latent Bank for Large-Factor Image Super-Resolution", "comments": "Tech report, 19 pages, 19 figures. A high-resolution version of this\n  paper can be found at https://ckkelvinchan.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that pre-trained Generative Adversarial Networks (GANs), e.g.,\nStyleGAN, can be used as a latent bank to improve the restoration quality of\nlarge-factor image super-resolution (SR). While most existing SR approaches\nattempt to generate realistic textures through learning with adversarial loss,\nour method, Generative LatEnt bANk (GLEAN), goes beyond existing practices by\ndirectly leveraging rich and diverse priors encapsulated in a pre-trained GAN.\nBut unlike prevalent GAN inversion methods that require expensive\nimage-specific optimization at runtime, our approach only needs a single\nforward pass to generate the upscaled image. GLEAN can be easily incorporated\nin a simple encoder-bank-decoder architecture with multi-resolution skip\nconnections. Switching the bank allows the method to deal with images from\ndiverse categories, e.g., cat, building, human face, and car. Images upscaled\nby GLEAN show clear improvements in terms of fidelity and texture faithfulness\nin comparison to existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 18:56:14 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Chan", "Kelvin C. K.", ""], ["Wang", "Xintao", ""], ["Xu", "Xiangyu", ""], ["Gu", "Jinwei", ""], ["Loy", "Chen Change", ""]]}, {"id": "2012.00744", "submitter": "Harry Wang", "authors": "Jinggang Zhuo, Ling Fan, Harry Jiannan Wang", "title": "A Framework and Dataset for Abstract Art Generation via CalligraphyGAN", "comments": "Accepted by NeurIPS 2020 Workshop on Machine Learning for Creativity\n  and Design, Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advancement of deep learning, artificial intelligence (AI) has made\nmany breakthroughs in recent years and achieved superhuman performance in\nvarious tasks such as object detection, reading comprehension, and video games.\nGenerative Modeling, such as various Generative Adversarial Networks (GAN)\nmodels, has been applied to generate paintings and music. Research in Natural\nLanguage Processing (NLP) also had a leap forward in 2018 since the release of\nthe pre-trained contextual neural language models such as BERT and recently\nreleased GPT3. Despite the exciting AI applications aforementioned, AI is still\nsignificantly lagging behind humans in creativity, which is often considered\nthe ultimate moonshot for AI. Our work is inspired by Chinese calligraphy,\nwhich is a unique form of visual art where the character itself is an aesthetic\npainting. We also draw inspirations from paintings of the Abstract\nExpressionist movement in the 1940s and 1950s, such as the work by American\npainter Franz Kline. In this paper, we present a creative framework based on\nConditional Generative Adversarial Networks and Contextual Neural Language\nModel to generate abstract artworks that have intrinsic meaning and aesthetic\nvalue, which is different from the existing work, such as image captioning and\ntext-to-image generation, where the texts are the descriptions of the images.\nIn addition, we have publicly released a Chinese calligraphy image dataset and\ndemonstrate our framework using a prototype system and a user study.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 16:24:20 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Zhuo", "Jinggang", ""], ["Fan", "Ling", ""], ["Wang", "Harry Jiannan", ""]]}, {"id": "2012.00759", "submitter": "Huiyu Wang", "authors": "Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, Liang-Chieh Chen", "title": "MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MaX-DeepLab, the first end-to-end model for panoptic segmentation.\nOur approach simplifies the current pipeline that depends heavily on surrogate\nsub-tasks and hand-designed components, such as box detection, non-maximum\nsuppression, thing-stuff merging, etc. Although these sub-tasks are tackled by\narea experts, they fail to comprehensively solve the target task. By contrast,\nour MaX-DeepLab directly predicts class-labeled masks with a mask transformer,\nand is trained with a panoptic quality inspired loss via bipartite matching.\nOur mask transformer employs a dual-path architecture that introduces a global\nmemory path in addition to a CNN path, allowing direct communication with any\nCNN layers. As a result, MaX-DeepLab shows a significant 7.1% PQ gain in the\nbox-free regime on the challenging COCO dataset, closing the gap between\nbox-based and box-free methods for the first time. A small variant of\nMaX-DeepLab improves 3.0% PQ over DETR with similar parameters and M-Adds.\nFurthermore, MaX-DeepLab, without test time augmentation, achieves new\nstate-of-the-art 51.3% PQ on COCO test-dev set. Code is available at\nhttps://github.com/google-research/deeplab2.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 19:00:00 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 21:57:15 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 21:16:19 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Wang", "Huiyu", ""], ["Zhu", "Yukun", ""], ["Adam", "Hartwig", ""], ["Yuille", "Alan", ""], ["Chen", "Liang-Chieh", ""]]}, {"id": "2012.00779", "submitter": "Mingjian Zhu", "authors": "Mingjian Zhu, Kai Han, Changbin Yu, Yunhe Wang", "title": "Dynamic Feature Pyramid Networks for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature pyramid network (FPN) is a critical component in modern object\ndetection frameworks. The performance gain in most of the existing FPN variants\nis mainly attributed to the increase of computational burden. An attempt to\nenhance the FPN is enriching the spatial information by expanding the receptive\nfields, which is promising to largely improve the detection accuracy. In this\npaper, we first investigate how expanding the receptive fields affect the\naccuracy and computational costs of FPN. We explore a baseline model called\ninception FPN in which each lateral connection contains convolution filters\nwith different kernel sizes. Moreover, we point out that not all objects need\nsuch a complicated calculation and propose a new dynamic FPN (DyFPN). The\noutput features of DyFPN will be calculated by using the adaptively selected\nbranch according to a dynamic gating operation. Therefore, the proposed method\ncan provide a more efficient dynamic inference for achieving a better trade-off\nbetween accuracy and computational cost. Extensive experiments conducted on\nMS-COCO benchmark demonstrate that the proposed DyFPN significantly improves\nperformance with the optimal allocation of computation resources. For instance,\nreplacing inception FPN with DyFPN reduces about 40% of its FLOPs while\nmaintaining similar high performance.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 19:03:55 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 14:05:04 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhu", "Mingjian", ""], ["Han", "Kai", ""], ["Yu", "Changbin", ""], ["Wang", "Yunhe", ""]]}, {"id": "2012.00781", "submitter": "Anirudh Tunga", "authors": "Anirudh Tunga, Sai Vidyaranya Nuthalapati, Juan Wachs", "title": "Pose-based Sign Language Recognition using GCN and BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sign language recognition (SLR) plays a crucial role in bridging the\ncommunication gap between the hearing and vocally impaired community and the\nrest of the society. Word-level sign language recognition (WSLR) is the first\nimportant step towards understanding and interpreting sign language. However,\nrecognizing signs from videos is a challenging task as the meaning of a word\ndepends on a combination of subtle body motions, hand configurations, and other\nmovements. Recent pose-based architectures for WSLR either model both the\nspatial and temporal dependencies among the poses in different frames\nsimultaneously or only model the temporal information without fully utilizing\nthe spatial information.\n  We tackle the problem of WSLR using a novel pose-based approach, which\ncaptures spatial and temporal information separately and performs late fusion.\nOur proposed architecture explicitly captures the spatial interactions in the\nvideo using a Graph Convolutional Network (GCN). The temporal dependencies\nbetween the frames are captured using Bidirectional Encoder Representations\nfrom Transformers (BERT). Experimental results on WLASL, a standard word-level\nsign language recognition dataset show that our model significantly outperforms\nthe state-of-the-art on pose-based methods by achieving an improvement in the\nprediction accuracy by up to 5%.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 19:10:50 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Tunga", "Anirudh", ""], ["Nuthalapati", "Sai Vidyaranya", ""], ["Wachs", "Juan", ""]]}, {"id": "2012.00802", "submitter": "George Yu", "authors": "Pranjal Awasthi, George Yu, Chun-Sung Ferng, Andrew Tomkins, Da-Cheng\n  Juan", "title": "Adversarial Robustness Across Representation Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial robustness corresponds to the susceptibility of deep neural\nnetworks to imperceptible perturbations made at test time. In the context of\nimage tasks, many algorithms have been proposed to make neural networks robust\nto adversarial perturbations made to the input pixels. These perturbations are\ntypically measured in an $\\ell_p$ norm. However, robustness often holds only\nfor the specific attack used for training. In this work we extend the above\nsetting to consider the problem of training of deep neural networks that can be\nmade simultaneously robust to perturbations applied in multiple natural\nrepresentation spaces. For the case of image data, examples include the\nstandard pixel representation as well as the representation in the discrete\ncosine transform~(DCT) basis. We design a theoretically sound algorithm with\nformal guarantees for the above problem. Furthermore, our guarantees also hold\nwhen the goal is to require robustness with respect to multiple $\\ell_p$ norm\nbased attacks. We then derive an efficient practical implementation and\ndemonstrate the effectiveness of our approach on standard datasets for image\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 19:55:58 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Yu", "George", ""], ["Ferng", "Chun-Sung", ""], ["Tomkins", "Andrew", ""], ["Juan", "Da-Cheng", ""]]}, {"id": "2012.00827", "submitter": "Rihuan Ke", "authors": "Rihuan Ke, Angelica Aviles-Rivero, Saurabh Pandey, Saikumar Reddy and\n  Carola-Bibiane Sch\\\"onlieb", "title": "A Three-Stage Self-Training Framework for Semi-Supervised Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation has been widely investigated in the community, in which\nthe state of the art techniques are based on supervised models. Those models\nhave reported unprecedented performance at the cost of requiring a large set of\nhigh quality segmentation masks. To obtain such annotations is highly expensive\nand time consuming, in particular, in semantic segmentation where pixel-level\nannotations are required. In this work, we address this problem by proposing a\nholistic solution framed as a three-stage self-training framework for\nsemi-supervised semantic segmentation. The key idea of our technique is the\nextraction of the pseudo-masks statistical information to decrease uncertainty\nin the predicted probability whilst enforcing segmentation consistency in a\nmulti-task fashion. We achieve this through a three-stage solution. Firstly, we\ntrain a segmentation network to produce rough pseudo-masks which predicted\nprobability is highly uncertain. Secondly, we then decrease the uncertainty of\nthe pseudo-masks using a multi-task model that enforces consistency whilst\nexploiting the rich statistical information of the data. We compare our\napproach with existing methods for semi-supervised semantic segmentation and\ndemonstrate its state-of-the-art performance with extensive experiments.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 21:00:27 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Ke", "Rihuan", ""], ["Aviles-Rivero", "Angelica", ""], ["Pandey", "Saurabh", ""], ["Reddy", "Saikumar", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "2012.00830", "submitter": "Taoufik Yeferny", "authors": "Achraf Ben Miled, Taoufik Yeferny, and Amira ben Rabeh", "title": "MRI Images Analysis Method for Early Stage Alzheimer's Disease Detection", "comments": null, "journal-ref": "IJCSNS International Journal of Computer Science and Network\n  Security, VOL.20 No.9, September 2020", "doi": "10.22937/IJCSNS.2020.20.09.26", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Alzheimer's disease is a neurogenerative disease that alters memories,\ncognitive functions leading to death. Early diagnosis of the disease, by\ndetection of the preliminary stage, called Mild Cognitive Impairment (MCI),\nremains a challenging issue. In this respect, we introduce, in this paper, a\npowerful classification architecture that implements the pre-trained network\nAlexNet to automatically extract the most prominent features from Magnetic\nResonance Imaging (MRI) images in order to detect the Alzheimer's disease at\nthe MCI stage. The proposed method is evaluated using a big database from OASIS\nDatabase Brain. Various sections of the brain: frontal, sagittal and axial were\nused. The proposed method achieved 96.83% accuracy by using 420 subjects: 210\nNormal and 210 MRI\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 12:36:36 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Miled", "Achraf Ben", ""], ["Yeferny", "Taoufik", ""], ["Rabeh", "Amira ben", ""]]}, {"id": "2012.00848", "submitter": "Qian Wang", "authors": "Qian Wang, Fanlin Meng, Toby P. Breckon", "title": "Data Augmentation with norm-VAE for Unsupervised Domain Adaptation", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the Unsupervised Domain Adaptation (UDA) problem in image\nclassification from a new perspective. In contrast to most existing works which\neither align the data distributions or learn domain-invariant features, we\ndirectly learn a unified classifier for both domains within a high-dimensional\nhomogeneous feature space without explicit domain adaptation. To this end, we\nemploy the effective Selective Pseudo-Labelling (SPL) techniques to take\nadvantage of the unlabelled samples in the target domain. Surprisingly, data\ndistribution discrepancy across the source and target domains can be well\nhandled by a computationally simple classifier (e.g., a shallow Multi-Layer\nPerceptron) trained in the original feature space. Besides, we propose a novel\ngenerative model norm-VAE to generate synthetic features for the target domain\nas a data augmentation strategy to enhance classifier training. Experimental\nresults on several benchmark datasets demonstrate the pseudo-labelling strategy\nitself can lead to comparable performance to many state-of-the-art methods\nwhilst the use of norm-VAE for feature augmentation can further improve the\nperformance in most cases. As a result, our proposed methods (i.e. naive-SPL\nand norm-VAE-SPL) can achieve new state-of-the-art performance with the average\naccuracy of 93.4% and 90.4% on Office-Caltech and ImageCLEF-DA datasets, and\ncomparable performance on Digits, Office31 and Office-Home datasets with the\naverage accuracy of 97.2%, 87.6% and 67.9% respectively.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 21:41:08 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Wang", "Qian", ""], ["Meng", "Fanlin", ""], ["Breckon", "Toby P.", ""]]}, {"id": "2012.00859", "submitter": "Morteza Ghahremani", "authors": "Morteza Ghahremani and Yonghuai Liu and Bernard Tiddeman", "title": "FFD: Fast Feature Detector", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, 2021", "doi": "10.1109/TIP.2020.3042057", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scale-invariance, good localization and robustness to noise and distortions\nare the main properties that a local feature detector should possess. Most\nexisting local feature detectors find excessive unstable feature points that\nincrease the number of keypoints to be matched and the computational time of\nthe matching step. In this paper, we show that robust and accurate keypoints\nexist in the specific scale-space domain. To this end, we first formulate the\nsuperimposition problem into a mathematical model and then derive a closed-form\nsolution for multiscale analysis. The model is formulated via\ndifference-of-Gaussian (DoG) kernels in the continuous scale-space domain, and\nit is proved that setting the scale-space pyramid's blurring ratio and\nsmoothness to 2 and 0.627, respectively, facilitates the detection of reliable\nkeypoints. For the applicability of the proposed model to discrete images, we\ndiscretize it using the undecimated wavelet transform and the cubic spline\nfunction. Theoretically, the complexity of our method is less than 5\\% of that\nof the popular baseline Scale Invariant Feature Transform (SIFT). Extensive\nexperimental results show the superiority of the proposed feature detector over\nthe existing representative hand-crafted and learning-based techniques in\naccuracy and computational time. The code and supplementary materials can be\nfound at~{\\url{https://github.com/mogvision/FFD}}.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 21:56:35 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Ghahremani", "Morteza", ""], ["Liu", "Yonghuai", ""], ["Tiddeman", "Bernard", ""]]}, {"id": "2012.00868", "submitter": "Srikar Appalaraju", "authors": "Srikar Appalaraju, Yi Zhu, Yusheng Xie, Istv\\'an Feh\\'erv\\'ari", "title": "Towards Good Practices in Self-supervised Representation Learning", "comments": null, "journal-ref": "Neural Information Processing Systems (NeurIPS Self-Supervision\n  Workshop 2020)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised representation learning has seen remarkable progress in the\nlast few years. More recently, contrastive instance learning has shown\nimpressive results compared to its supervised learning counterparts. However,\neven with the ever increased interest in contrastive instance learning, it is\nstill largely unclear why these methods work so well. In this paper, we aim to\nunravel some of the mysteries behind their success, which are the good\npractices. Through an extensive empirical analysis, we hope to not only provide\ninsights but also lay out a set of best practices that led to the success of\nrecent work in self-supervised representation learning.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 22:13:43 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Appalaraju", "Srikar", ""], ["Zhu", "Yi", ""], ["Xie", "Yusheng", ""], ["Feh\u00e9rv\u00e1ri", "Istv\u00e1n", ""]]}, {"id": "2012.00873", "submitter": "Stefanos Kollias", "authors": "Georgios Tsatiris, Kostas Karpouzis, Stefanos Kollias", "title": "A compact sequence encoding scheme for online human activity recognition\n  in HRI applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity recognition and analysis has always been one of the most\nactive areas of pattern recognition and machine intelligence, with applications\nin various fields, including but not limited to exertion games, surveillance,\nsports analytics and healthcare. Especially in Human-Robot Interaction, human\nactivity understanding plays a crucial role as household robotic assistants are\na trend of the near future. However, state-of-the-art infrastructures that can\nsupport complex machine intelligence tasks are not always available, and may\nnot be for the average consumer, as robotic hardware is expensive. In this\npaper we propose a novel action sequence encoding scheme which efficiently\ntransforms spatio-temporal action sequences into compact representations, using\nMahalanobis distance-based shape features and the Radon transform. This\nrepresentation can be used as input for a lightweight convolutional neural\nnetwork. Experiments show that the proposed pipeline, when based on\nstate-of-the-art human pose estimation techniques, can provide a robust\nend-to-end online action recognition scheme, deployable on hardware lacking\nextreme computing capabilities.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 22:33:09 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Tsatiris", "Georgios", ""], ["Karpouzis", "Kostas", ""], ["Kollias", "Stefanos", ""]]}, {"id": "2012.00888", "submitter": "Nicholas Sharp", "authors": "Nicholas Sharp, Souhaib Attaiki, Keenan Crane, Maks Ovsjanikov", "title": "DiffusionNet: Discretization Agnostic Learning on Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach to deep learning on 3D surfaces, based on the\ninsight that a simple diffusion layer is highly effective for spatial\ncommunication. The resulting networks automatically generalize across different\nsamplings and resolutions of a surface -- a basic property which is crucial for\npractical applications. Our networks can be discretized on various geometric\nrepresentations such as triangle meshes or point clouds, and can even be\ntrained on one representation then applied to another. We optimize the spatial\nsupport of diffusion as a continuous network parameter ranging from purely\nlocal to totally global, removing the burden of manually choosing neighborhood\nsizes. The only other ingredients in the method are a multi-layer perceptron\napplied independently at each point, and spatial gradient features to support\ndirectional filters. The resulting networks are simple, robust, and efficient.\nHere, we focus primarily on triangle mesh surfaces, and demonstrate\nstate-of-the-art results for a variety of tasks including surface\nclassification, segmentation, and non-rigid correspondence.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 23:24:22 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 19:28:43 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Sharp", "Nicholas", ""], ["Attaiki", "Souhaib", ""], ["Crane", "Keenan", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "2012.00899", "submitter": "Yiran Zhong", "authors": "Yiran Zhong, Charles Loop, Wonmin Byeon, Stan Birchfield, Yuchao Dai,\n  Kaihao Zhang, Alexey Kamenev, Thomas Breuel, Hongdong Li, Jan Kautz", "title": "Displacement-Invariant Cost Computation for Efficient Stereo Matching", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning-based methods have dominated stereo matching\nleaderboards by yielding unprecedented disparity accuracy, their inference time\nis typically slow, on the order of seconds for a pair of 540p images. The main\nreason is that the leading methods employ time-consuming 3D convolutions\napplied to a 4D feature volume. A common way to speed up the computation is to\ndownsample the feature volume, but this loses high-frequency details. To\novercome these challenges, we propose a \\emph{displacement-invariant cost\ncomputation module} to compute the matching costs without needing a 4D feature\nvolume. Rather, costs are computed by applying the same 2D convolution network\non each disparity-shifted feature map pair independently. Unlike previous 2D\nconvolution-based methods that simply perform context mapping between inputs\nand disparity maps, our proposed approach learns to match features between the\ntwo images. We also propose an entropy-based refinement strategy to refine the\ncomputed disparity map, which further improves speed by avoiding the need to\ncompute a second disparity map on the right image. Extensive experiments on\nstandard datasets (SceneFlow, KITTI, ETH3D, and Middlebury) demonstrate that\nour method achieves competitive accuracy with much less inference time. On\ntypical image sizes, our method processes over 100 FPS on a desktop GPU, making\nour method suitable for time-critical applications such as autonomous driving.\nWe also show that our approach generalizes well to unseen datasets,\noutperforming 4D-volumetric methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 23:58:16 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Zhong", "Yiran", ""], ["Loop", "Charles", ""], ["Byeon", "Wonmin", ""], ["Birchfield", "Stan", ""], ["Dai", "Yuchao", ""], ["Zhang", "Kaihao", ""], ["Kamenev", "Alexey", ""], ["Breuel", "Thomas", ""], ["Li", "Hongdong", ""], ["Kautz", "Jan", ""]]}, {"id": "2012.00904", "submitter": "Yang Zhao", "authors": "Yang Zhao, Chunyuan Li, Ping Yu, Changyou Chen", "title": "ReMP: Rectified Metric Propagation for Few-Shot Learning", "comments": "Tech Report, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning features the capability of generalizing from a few\nexamples. In this paper, we first identify that a discriminative feature space,\nnamely a rectified metric space, that is learned to maintain the metric\nconsistency from training to testing, is an essential component to the success\nof metric-based few-shot learning. Numerous analyses indicate that a simple\nmodification of the objective can yield substantial performance gains. The\nresulting approach, called rectified metric propagation (ReMP), further\noptimizes an attentive prototype propagation network, and applies a repulsive\nforce to make confident predictions. Extensive experiments demonstrate that the\nproposed ReMP is effective and efficient, and outperforms the state of the arts\non various standard few-shot learning datasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 00:07:53 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Zhao", "Yang", ""], ["Li", "Chunyuan", ""], ["Yu", "Ping", ""], ["Chen", "Changyou", ""]]}, {"id": "2012.00909", "submitter": "Yaguan Qian", "authors": "Yaguan Qian, Jiamin Wang, Bin Wang, Shaoning Zeng, Zhaoquan Gu,\n  Shouling Ji, and Wassim Swaileh", "title": "Visually Imperceptible Adversarial Patch Attacks on Digital Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The vulnerability of deep neural networks (DNNs) to adversarial examples has\nattracted more attention. Many algorithms have been proposed to craft powerful\nadversarial examples. However, most of these algorithms modified the global or\nlocal region of pixels without taking network explanations into account. Hence,\nthe perturbations are redundant, which are easily detected by human eyes. In\nthis paper, we propose a novel method to generate local region perturbations.\nThe main idea is to find a contributing feature region (CFR) of an image by\nsimulating the human attention mechanism and then add perturbations to CFR.\nFurthermore, a soft mask matrix is designed on the basis of an activation map\nto finely represent the contributions of each pixel in CFR. With this soft\nmask, we develop a new loss function with inverse temperature to search for\noptimal perturbations in CFR. Due to the network explanations, the\nperturbations added to CFR are more effective than those added to other\nregions. Extensive experiments conducted on CIFAR-10 and ILSVRC2012 demonstrate\nthe effectiveness of the proposed method, including attack success rate,\nimperceptibility, and transferability.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 00:47:56 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 00:30:04 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 06:14:28 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Qian", "Yaguan", ""], ["Wang", "Jiamin", ""], ["Wang", "Bin", ""], ["Zeng", "Shaoning", ""], ["Gu", "Zhaoquan", ""], ["Ji", "Shouling", ""], ["Swaileh", "Wassim", ""]]}, {"id": "2012.00914", "submitter": "Roderic Collins", "authors": "Kellie Corona (1), Katie Osterdahl (1), Roderic Collins (1), Anthony\n  Hoogs (1) ((1) Kitware, Inc.)", "title": "MEVA: A Large-Scale Multiview, Multimodal Video Dataset for Activity\n  Detection", "comments": "9 pages, 11 figures, to appear at WACV 2021. Dataset is available at\n  https://mevadata.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Multiview Extended Video with Activities (MEVA) dataset, a new\nand very-large-scale dataset for human activity recognition. Existing security\ndatasets either focus on activity counts by aggregating public video\ndisseminated due to its content, which typically excludes same-scene background\nvideo, or they achieve persistence by observing public areas and thus cannot\ncontrol for activity content. Our dataset is over 9300 hours of untrimmed,\ncontinuous video, scripted to include diverse, simultaneous activities, along\nwith spontaneous background activity. We have annotated 144 hours for 37\nactivity types, marking bounding boxes of actors and props. Our collection\nobserved approximately 100 actors performing scripted scenarios and spontaneous\nbackground activity over a three-week period at an access-controlled venue,\ncollecting in multiple modalities with overlapping and non-overlapping indoor\nand outdoor viewpoints. The resulting data includes video from 38 RGB and\nthermal IR cameras, 42 hours of UAV footage, as well as GPS locations for the\nactors. 122 hours of annotation are sequestered in support of the NIST Activity\nin Extended Video (ActEV) challenge; the other 22 hours of annotation and the\ncorresponding video are available on our website, along with an additional 306\nhours of ground camera data, 4.6 hours of UAV data, and 9.6 hours of GPS logs.\nAdditional derived data includes camera models geo-registering the outdoor\ncameras and a dense 3D point cloud model of the outdoor scene. The data was\ncollected with IRB oversight and approval and released under a CC-BY-4.0\nlicense.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 01:02:06 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Corona", "Kellie", "", "Kitware, Inc"], ["Osterdahl", "Katie", "", "Kitware, Inc"], ["Collins", "Roderic", "", "Kitware, Inc"], ["Hoogs", "Anthony", "", "Kitware, Inc"]]}, {"id": "2012.00924", "submitter": "Lixin Yang", "authors": "Lixin Yang, Xinyu Zhan, Kailin Li, Wenqiang Xu, Jiefeng Li, Cewu Lu", "title": "CPF: Learning a Contact Potential Field to Model the Hand-object\n  Interaction", "comments": "ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the hand-object (HO) interaction not only requires estimation of the\nHO pose, but also pays attention to the contact due to their interaction.\nSignificant progress has been made in estimating hand and object separately\nwith deep learning methods, simultaneous HO pose estimation and contact\nmodeling has not yet been fully explored. In this paper, we present an explicit\ncontact representation namely Contact Potential Field (CPF), and a\nlearning-fitting hybrid framework namely MIHO to Modeling the Interaction of\nHand and Object. In CPF, we treat each contacting HO vertex pair as a\nspring-mass system. Hence the whole system forms a potential field with minimal\nelastic energy at the grasp position. Extensive experiments on the two commonly\nused benchmarks have demonstrated that our method can achieve state-of-the-art\nin several reconstruction metrics, and allow us to produce more physically\nplausible HO pose even when the ground-truth exhibits severe interpenetration\nor disjointedness. Our code is available at https://github.com/lixiny/CPF.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 01:45:30 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 09:04:44 GMT"}, {"version": "v3", "created": "Mon, 26 Jul 2021 10:45:13 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Yang", "Lixin", ""], ["Zhan", "Xinyu", ""], ["Li", "Kailin", ""], ["Xu", "Wenqiang", ""], ["Li", "Jiefeng", ""], ["Lu", "Cewu", ""]]}, {"id": "2012.00925", "submitter": "Zhuowei Wang Mr", "authors": "Zhuowei Wang, Jing Jiang, Bo Han, Lei Feng, Bo An, Gang Niu, Guodong\n  Long", "title": "SemiNLL: A Framework of Noisy-Label Learning by Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning with noisy labels is a challenging task. Recent prominent\nmethods that build on a specific sample selection (SS) strategy and a specific\nsemi-supervised learning (SSL) model achieved state-of-the-art performance.\nIntuitively, better performance could be achieved if stronger SS strategies and\nSSL models are employed. Following this intuition, one might easily derive\nvarious effective noisy-label learning methods using different combinations of\nSS strategies and SSL models, which is, however, reinventing the wheel in\nessence. To prevent this problem, we propose SemiNLL, a versatile framework\nthat combines SS strategies and SSL models in an end-to-end manner. Our\nframework can absorb various SS strategies and SSL backbones, utilizing their\npower to achieve promising performance. We also instantiate our framework with\ndifferent combinations, which set the new state of the art on\nbenchmark-simulated and real-world datasets with noisy labels.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 01:49:47 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Wang", "Zhuowei", ""], ["Jiang", "Jing", ""], ["Han", "Bo", ""], ["Feng", "Lei", ""], ["An", "Bo", ""], ["Niu", "Gang", ""], ["Long", "Guodong", ""]]}, {"id": "2012.00926", "submitter": "Marco Monteiro", "authors": "Eric R. Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, Gordon\n  Wetzstein", "title": "pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware\n  Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have witnessed rapid progress on 3D-aware image synthesis, leveraging\nrecent advances in generative visual models and neural rendering. Existing\napproaches however fall short in two ways: first, they may lack an underlying\n3D representation or rely on view-inconsistent rendering, hence synthesizing\nimages that are not multi-view consistent; second, they often depend upon\nrepresentation network architectures that are not expressive enough, and their\nresults thus lack in image quality. We propose a novel generative model, named\nPeriodic Implicit Generative Adversarial Networks ($\\pi$-GAN or pi-GAN), for\nhigh-quality 3D-aware image synthesis. $\\pi$-GAN leverages neural\nrepresentations with periodic activation functions and volumetric rendering to\nrepresent scenes as view-consistent 3D representations with fine detail. The\nproposed approach obtains state-of-the-art results for 3D-aware image synthesis\nwith multiple real and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 01:57:46 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 23:18:10 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Chan", "Eric R.", ""], ["Monteiro", "Marco", ""], ["Kellnhofer", "Petr", ""], ["Wu", "Jiajun", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2012.00938", "submitter": "Hyungjun Kim", "authors": "Hyungjun Kim, Jihoon Park, Changhun Lee, Jae-Joon Kim", "title": "Improving Accuracy of Binary Neural Networks using Unbalanced Activation\n  Distribution", "comments": "CVPR 2021, 10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binarization of neural network models is considered as one of the promising\nmethods to deploy deep neural network models on resource-constrained\nenvironments such as mobile devices. However, Binary Neural Networks (BNNs)\ntend to suffer from severe accuracy degradation compared to the full-precision\ncounterpart model. Several techniques were proposed to improve the accuracy of\nBNNs. One of the approaches is to balance the distribution of binary\nactivations so that the amount of information in the binary activations becomes\nmaximum. Based on extensive analysis, in stark contrast to previous work, we\nargue that unbalanced activation distribution can actually improve the accuracy\nof BNNs. We also show that adjusting the threshold values of binary activation\nfunctions results in the unbalanced distribution of the binary activation,\nwhich increases the accuracy of BNN models. Experimental results show that the\naccuracy of previous BNN models (e.g. XNOR-Net and Bi-Real-Net) can be improved\nby simply shifting the threshold values of binary activation functions without\nrequiring any other modification.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 02:49:53 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 04:52:40 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Kim", "Hyungjun", ""], ["Park", "Jihoon", ""], ["Lee", "Changhun", ""], ["Kim", "Jae-Joon", ""]]}, {"id": "2012.00944", "submitter": "Zhebin Wu", "authors": "Zhebin Wu, Tianchi Liao, Chuan Chen, Cong Liu, Zibin Zheng, and\n  Xiongjun Zhang", "title": "Tensor Completion via Convolutional Sparse Coding Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tensor data often suffer from missing value problem due to the complex\nhigh-dimensional structure while acquiring them. To complete the missing\ninformation, lots of Low-Rank Tensor Completion (LRTC) methods have been\nproposed, most of which depend on the low-rank property of tensor data. In this\nway, the low-rank component of the original data could be recovered roughly.\nHowever, the shortcoming is that the detail information can not be fully\nrestored, no matter the Sum of the Nuclear Norm (SNN) nor the Tensor Nuclear\nNorm (TNN) based methods. On the contrary, in the field of signal processing,\nConvolutional Sparse Coding (CSC) can provide a good representation of the\nhigh-frequency component of the image, which is generally associated with the\ndetail component of the data. Nevertheless, CSC can not handle the\nlow-frequency component well. To this end, we propose two novel methods,\nLRTC-CSC-I and LRTC-CSC-II, which adopt CSC as a supplementary regularization\nfor LRTC to capture the high-frequency components. Therefore, the LRTC-CSC\nmethods can not only solve the missing value problem but also recover the\ndetails. Moreover, the regularizer CSC can be trained with small samples due to\nthe sparsity characteristic. Extensive experiments show the effectiveness of\nLRTC-CSC methods, and quantitative evaluation indicates that the performance of\nour models are superior to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 03:12:10 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 08:29:49 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Wu", "Zhebin", ""], ["Liao", "Tianchi", ""], ["Chen", "Chuan", ""], ["Liu", "Cong", ""], ["Zheng", "Zibin", ""], ["Zhang", "Xiongjun", ""]]}, {"id": "2012.00945", "submitter": "Yu Li", "authors": "Yu Li, Ming Liu, Yaling Yi, Qince Li, Dongwei Ren, Wangmeng Zuo", "title": "Two-Stage Single Image Reflection Removal with Reflection-Aware Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing undesired reflection from an image captured through a glass surface\nis a very challenging problem with many practical application scenarios. For\nimproving reflection removal, cascaded deep models have been usually adopted to\nestimate the transmission in a progressive manner. However, most existing\nmethods are still limited in exploiting the result in prior stage for guiding\ntransmission estimation. In this paper, we present a novel two-stage network\nwith reflection-aware guidance (RAGNet) for single image reflection removal\n(SIRR). To be specific, the reflection layer is firstly estimated due to that\nit generally is much simpler and is relatively easier to estimate.\nReflectionaware guidance (RAG) module is then elaborated for better exploiting\nthe estimated reflection in predicting transmission layer. By incorporating\nfeature maps from the estimated reflection and observation, RAG can be used (i)\nto mitigate the effect of reflection from the observation, and (ii) to generate\nmask in partial convolution for mitigating the effect of deviating from linear\ncombination hypothesis. A dedicated mask loss is further presented for\nreconciling the contributions of encoder and decoder features. Experiments on\nfive commonly used datasets demonstrate the quantitative and qualitative\nsuperiority of our RAGNet in comparison to the state-of-the-art SIRR methods.\nThe source code and pre-trained model are available at\nhttps://github.com/liyucs/RAGNet.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 03:14:57 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 07:53:31 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Li", "Yu", ""], ["Liu", "Ming", ""], ["Yi", "Yaling", ""], ["Li", "Qince", ""], ["Ren", "Dongwei", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "2012.00946", "submitter": "Qi Zhang", "authors": "Qi Zhang, Antoni B. Chan", "title": "Wide-Area Crowd Counting: Multi-View Fusion Networks for Counting in\n  Large Scenes", "comments": "29 pages, 13 figures, submitted to IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Crowd counting in single-view images has achieved outstanding performance on\nexisting counting datasets. However, single-view counting is not applicable to\nlarge and wide scenes (e.g., public parks, long subway platforms, or event\nspaces) because a single camera cannot capture the whole scene in adequate\ndetail for counting, e.g., when the scene is too large to fit into the\nfield-of-view of the camera, too long so that the resolution is too low on\nfaraway crowds, or when there are too many large objects that occlude large\nportions of the crowd. Therefore, to solve the wide-area counting task requires\nmultiple cameras with overlapping fields-of-view. In this paper, we propose a\ndeep neural network framework for multi-view crowd counting, which fuses\ninformation from multiple camera views to predict a scene-level density map on\nthe ground-plane of the 3D world. We consider three versions of the fusion\nframework: the late fusion model fuses camera-view density map; the naive early\nfusion model fuses camera-view feature maps; and the multi-view multi-scale\nearly fusion model ensures that features aligned to the same ground-plane point\nhave consistent scales. A rotation selection module further ensures consistent\nrotation alignment of the features. We test our 3 fusion models on 3 multi-view\ncounting datasets, PETS2009, DukeMTMC, and a newly collected multi-view\ncounting dataset containing a crowded street intersection. Our methods achieve\nstate-of-the-art results compared to other multi-view counting baselines.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 03:20:30 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Zhang", "Qi", ""], ["Chan", "Antoni B.", ""]]}, {"id": "2012.00953", "submitter": "Benjamin Smith", "authors": "Benjamin Smith", "title": "Ship Detection: Parameter Server Variant", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning ship detection in satellite optical imagery suffers from false\npositive occurrences with clouds, landmasses, and man-made objects that\ninterfere with correct classification of ships, typically limiting class\naccuracy scores to 88\\%. This work explores the tensions between customization\nstrategies, class accuracy rates, training times, and costs in cloud based\nsolutions. We demonstrate how a custom U-Net can achieve 92\\% class accuracy\nover a validation dataset and 68\\% over a target dataset with 90\\% confidence.\nWe also compare a single node architecture with a parameter server variant\nwhose workers act as a boosting mechanism. The parameter server variant\noutperforms class accuracy on the target dataset reaching 73\\% class accuracy\ncompared to the best single node approach. A comparative investigation on the\nsystematic performance of the single node and parameter server variant\narchitectures is discussed with support from empirical findings.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 03:39:24 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Smith", "Benjamin", ""]]}, {"id": "2012.00972", "submitter": "Guangming Wang", "authors": "Guangming Wang, Xinrui Wu, Zhe Liu, Hesheng Wang", "title": "PWCLO-Net: Deep LiDAR Odometry in 3D Point Clouds Using Hierarchical\n  Embedding Mask Optimization", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel 3D point cloud learning model for deep LiDAR odometry, named\nPWCLO-Net, using hierarchical embedding mask optimization is proposed in this\npaper. In this model, the Pyramid, Warping, and Cost volume (PWC) structure for\nthe LiDAR odometry task is built to refine the estimated pose in a\ncoarse-to-fine approach hierarchically. An attentive cost volume is built to\nassociate two point clouds and obtain embedding motion patterns. Then, a novel\ntrainable embedding mask is proposed to weigh the local motion patterns of all\npoints to regress the overall pose and filter outlier points. The estimated\ncurrent pose is used to warp the first point cloud to bridge the distance to\nthe second point cloud, and then the cost volume of the residual motion is\nbuilt. At the same time, the embedding mask is optimized hierarchically from\ncoarse to fine to obtain more accurate filtering information for pose\nrefinement. The trainable pose warp-refinement process is iteratively used to\nmake the pose estimation more robust for outliers. The superior performance and\neffectiveness of our LiDAR odometry model are demonstrated on KITTI odometry\ndataset. Our method outperforms all recent learning-based methods and\noutperforms the geometry-based approach, LOAM with mapping optimization, on\nmost sequences of KITTI odometry dataset.Our source codes will be released on\nhttps://github.com/IRMVLab/PWCLONet.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 05:23:41 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 05:20:05 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Wang", "Guangming", ""], ["Wu", "Xinrui", ""], ["Liu", "Zhe", ""], ["Wang", "Hesheng", ""]]}, {"id": "2012.00985", "submitter": "Won-Dong Jang", "authors": "Won-Dong Jang, Donglai Wei, Xingxuan Zhang, Brian Leahy, Helen Yang,\n  James Tompkin, Dalit Ben-Yosef, Daniel Needleman, and Hanspeter Pfister", "title": "Learning Vector Quantized Shape Code for Amodal Blastomere Instance\n  Segmentation", "comments": "9 pages including references, 9 figures, 5 tables, in preparation for\n  journal submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Blastomere instance segmentation is important for analyzing embryos'\nabnormality. To measure the accurate shapes and sizes of blastomeres, their\namodal segmentation is necessary. Amodal instance segmentation aims to recover\nthe complete silhouette of an object even when the object is not fully visible.\nFor each detected object, previous methods directly regress the target mask\nfrom input features. However, images of an object under different amounts of\nocclusion should have the same amodal mask output, which makes it harder to\ntrain the regression model. To alleviate the problem, we propose to classify\ninput features into intermediate shape codes and recover complete object shapes\nfrom them. First, we pre-train the Vector Quantized Variational Autoencoder\n(VQ-VAE) model to learn these discrete shape codes from ground truth amodal\nmasks. Then, we incorporate the VQ-VAE model into the amodal instance\nsegmentation pipeline with an additional refinement module. We also detect an\nocclusion map to integrate occlusion information with a backbone feature. As\nsuch, our network faithfully detects bounding boxes of amodal objects. On an\ninternal embryo cell image benchmark, the proposed method outperforms previous\nstate-of-the-art methods. To show generalizability, we show segmentation\nresults on the public KINS natural image benchmark. To examine the learned\nshape codes and model design choices, we perform ablation studies on a\nsynthetic dataset of simple overlaid shapes. Our method would enable accurate\nmeasurement of blastomeres in in vitro fertilization (IVF) clinics, which\npotentially can increase IVF success rate.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 06:17:28 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Jang", "Won-Dong", ""], ["Wei", "Donglai", ""], ["Zhang", "Xingxuan", ""], ["Leahy", "Brian", ""], ["Yang", "Helen", ""], ["Tompkin", "James", ""], ["Ben-Yosef", "Dalit", ""], ["Needleman", "Daniel", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "2012.00987", "submitter": "Yi Wei", "authors": "Yi Wei, Ziyi Wang, Yongming Rao, Jiwen Lu, Jie Zhou", "title": "PV-RAFT: Point-Voxel Correlation Fields for Scene Flow Estimation of\n  Point Clouds", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Point-Voxel Recurrent All-Pairs Field Transforms\n(PV-RAFT) method to estimate scene flow from point clouds. Since point clouds\nare irregular and unordered, it is challenging to efficiently extract features\nfrom all-pairs fields in the 3D space, where all-pairs correlations play\nimportant roles in scene flow estimation. To tackle this problem, we present\npoint-voxel correlation fields, which capture both local and long-range\ndependencies of point pairs. To capture point-based correlations, we adopt the\nK-Nearest Neighbors search that preserves fine-grained information in the local\nregion. By voxelizing point clouds in a multi-scale manner, we construct\npyramid correlation voxels to model long-range correspondences. Integrating\nthese two types of correlations, our PV-RAFT makes use of all-pairs relations\nto handle both small and large displacements. We evaluate the proposed method\non the FlyingThings3D and KITTI Scene Flow 2015 datasets. Experimental results\nshow that PV-RAFT outperforms state-of-the-art methods by remarkable margins.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 06:20:54 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 08:44:23 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Wei", "Yi", ""], ["Wang", "Ziyi", ""], ["Rao", "Yongming", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "2012.00996", "submitter": "Wenyu Sun", "authors": "Wenyu Sun, Jian Cao, Pengtao Xu, Xiangcheng Liu, Pu Li", "title": "An Once-for-All Budgeted Pruning Framework for ConvNets Considering\n  Input Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient once-for-all budgeted pruning framework (OFARPruning)\nto find many compact network structures close to winner tickets in the early\ntraining stage considering the effect of input resolution during the pruning\nprocess. In structure searching stage, we utilize cosine similarity to measure\nthe similarity of the pruning mask to get high-quality network structures with\nlow energy and time consumption. After structure searching stage, our proposed\nmethod randomly sample the compact structures with different pruning rates and\ninput resolution to achieve joint optimization. Ultimately, we can obtain a\ncohort of compact networks adaptive to various resolution to meet dynamic FLOPs\nconstraints on different edge devices with only once training. The experiments\nbased on image classification and object detection show that OFARPruning has a\nhigher accuracy than the once-for-all compression methods such as US-Net and\nMutualNet (1-2% better with less FLOPs), and achieve the same even higher\naccuracy as the conventional pruning methods (72.6% vs. 70.5% on MobileNetv2\nunder 170 MFLOPs) with much higher efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 07:09:12 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Sun", "Wenyu", ""], ["Cao", "Jian", ""], ["Xu", "Pengtao", ""], ["Liu", "Xiangcheng", ""], ["Li", "Pu", ""]]}, {"id": "2012.00999", "submitter": "Motoshi Abe", "authors": "Motoshi Abe, Junichi Miyao, and Takio Kurita", "title": "q-SNE: Visualizing Data using q-Gaussian Distributed Stochastic Neighbor\n  Embedding", "comments": "This paper is accepted ICPR2020. Code on Python is here\n  (https://github.com/i13abe/q-SNE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dimensionality reduction has been widely introduced to use the\nhigh-dimensional data for regression, classification, feature analysis, and\nvisualization. As the one technique of dimensionality reduction, a stochastic\nneighbor embedding (SNE) was introduced. The SNE leads powerful results to\nvisualize high-dimensional data by considering the similarity between the local\nGaussian distributions of high and low-dimensional space. To improve the SNE, a\nt-distributed stochastic neighbor embedding (t-SNE) was also introduced. To\nvisualize high-dimensional data, the t-SNE leads to more powerful and flexible\nvisualization on 2 or 3-dimensional mapping than the SNE by using a\nt-distribution as the distribution of low-dimensional data. Recently, Uniform\nmanifold approximation and projection (UMAP) is proposed as a dimensionality\nreduction technique. We present a novel technique called a q-Gaussian\ndistributed stochastic neighbor embedding (q-SNE). The q-SNE leads to more\npowerful and flexible visualization on 2 or 3-dimensional mapping than the\nt-SNE and the SNE by using a q-Gaussian distribution as the distribution of\nlow-dimensional data. The q-Gaussian distribution includes the Gaussian\ndistribution and the t-distribution as the special cases with q=1.0 and q=2.0.\nTherefore, the q-SNE can also express the t-SNE and the SNE by changing the\nparameter q, and this makes it possible to find the best visualization by\nchoosing the parameter q. We show the performance of q-SNE as visualization on\n2-dimensional mapping and classification by k-Nearest Neighbors (k-NN)\nclassifier in embedded space compared with SNE, t-SNE, and UMAP by using the\ndatasets MNIST, COIL-20, OlivettiFaces, FashionMNIST, and Glove.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 07:21:59 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Abe", "Motoshi", ""], ["Miyao", "Junichi", ""], ["Kurita", "Takio", ""]]}, {"id": "2012.01009", "submitter": "Doruk Pancaroglu", "authors": "Doruk Pancaroglu", "title": "Artist, Style And Year Classification Using Face Recognition And\n  Clustering With Convolutional Neural Networks", "comments": "14 pages, SIPO 2020", "journal-ref": null, "doi": "10.5121/csit.2020.101604", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artist, year and style classification of fine-art paintings are generally\nachieved using standard image classification methods, image segmentation, or\nmore recently, convolutional neural networks (CNNs). This works aims to use\nnewly developed face recognition methods such as FaceNet that use CNNs to\ncluster fine-art paintings using the extracted faces in the paintings, which\nare found abundantly. A dataset consisting of over 80,000 paintings from over\n1000 artists is chosen, and three separate face recognition and clustering\ntasks are performed. The produced clusters are analyzed by the file names of\nthe paintings and the clusters are named by their majority artist, year range,\nand style. The clusters are further analyzed and their performance metrics are\ncalculated. The study shows promising results as the artist, year, and styles\nare clustered with an accuracy of 58.8, 63.7, and 81.3 percent, while the\nclusters have an average purity of 63.1, 72.4, and 85.9 percent.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 08:01:27 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Pancaroglu", "Doruk", ""]]}, {"id": "2012.01030", "submitter": "Philipp Terh\\\"orst", "authors": "Philipp Terh\\\"orst, Daniel F\\\"ahrmann, Jan Niklas Kolf, Naser Damer,\n  Florian Kirchbuchner, and Arjan Kuijper", "title": "MAAD-Face: A Massively Annotated Attribute Dataset for Face Images", "comments": "Accepted in IEEE Transactions on Information Forensics and Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Soft-biometrics play an important role in face biometrics and related fields\nsince these might lead to biased performances, threatens the user's privacy, or\nare valuable for commercial aspects. Current face databases are specifically\nconstructed for the development of face recognition applications. Consequently,\nthese databases contain large amount of face images but lack in the number of\nattribute annotations and the overall annotation correctness. In this work, we\npropose MAADFace, a new face annotations database that is characterized by the\nlarge number of its high-quality attribute annotations. MAADFace is build on\nthe VGGFace2 database and thus, consists of 3.3M faces of over 9k individuals.\nUsing a novel annotation transfer-pipeline that allows an accurate\nlabel-transfer from multiple source-datasets to a target-dataset, MAAD-Face\nconsists of 123.9M attribute annotations of 47 different binary attributes.\nConsequently, it provides 15 and 137 times more attribute labels than CelebA\nand LFW. Our investigation on the annotation quality by three human evaluators\ndemonstrated the superiority of the MAAD-Face annotations over existing\ndatabases. Additionally, we make use of the large amount of high-quality\nannotations from MAAD-Face to study the viability of soft-biometrics for\nrecognition, providing insights about which attributes support genuine and\nimposter decisions. The MAAD-Face annotations dataset is publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 08:54:26 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 06:40:49 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Terh\u00f6rst", "Philipp", ""], ["F\u00e4hrmann", "Daniel", ""], ["Kolf", "Jan Niklas", ""], ["Damer", "Naser", ""], ["Kirchbuchner", "Florian", ""], ["Kuijper", "Arjan", ""]]}, {"id": "2012.01044", "submitter": "Sebastian Bullinger", "authors": "Sebastian Bullinger, Christoph Bodensteiner, Michael Arens", "title": "A Photogrammetry-based Framework to Facilitate Image-based Modeling and\n  Automatic Camera Tracking", "comments": null, "journal-ref": "In Proceedings of the 16th International Joint Conference on\n  Computer Vision, Imaging and Computer Graphics Theory and Applications -\n  Volume 1: GRAPP, 106-112, 2021", "doi": "10.5220/0010319801060112", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework that extends Blender to exploit Structure from Motion\n(SfM) and Multi-View Stereo (MVS) techniques for image-based modeling tasks\nsuch as sculpting or camera and motion tracking. Applying SfM allows us to\ndetermine camera motions without manually defining feature tracks or\ncalibrating the cameras used to capture the image data. With MVS we are able to\nautomatically compute dense scene models, which is not feasible with the\nbuilt-in tools of Blender. Currently, our framework supports several\nstate-of-the-art SfM and MVS pipelines. The modular system design enables us to\nintegrate further approaches without additional effort. The framework is\npublicly available as an open source software package.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 09:26:37 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Bullinger", "Sebastian", ""], ["Bodensteiner", "Christoph", ""], ["Arens", "Michael", ""]]}, {"id": "2012.01050", "submitter": "Tutian Tang", "authors": "Tutian Tang, Wenqiang Xu, Ruolin Ye, Lixin Yang, Cewu Lu", "title": "Learning Universal Shape Dictionary for Realtime Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel explicit shape representation for instance segmentation.\n  Based on how to model the object shape, current instance segmentation systems\ncan be divided into two categories, implicit and explicit models. The implicit\nmethods, which represent the object mask/contour by intractable network\nparameters, and produce it through pixel-wise classification, are predominant.\nHowever, the explicit methods, which parameterize the shape with simple and\nexplainable models, are less explored. Since the operations to generate the\nfinal shape are light-weighted, the explicit methods have a clear speed\nadvantage over implicit methods, which is crucial for real-world applications.\nThe proposed USD-Seg adopts a linear model, sparse coding with dictionary, for\nobject shapes.\n  First, it learns a dictionary from a large collection of shape datasets,\nmaking any shape being able to be decomposed into a linear combination through\nthe dictionary.\n  Hence the name \"Universal Shape Dictionary\".\n  Then it adds a simple shape vector regression head to ordinary object\ndetector, giving the detector segmentation ability with minimal overhead.\n  For quantitative evaluation, we use both average precision (AP) and the\nproposed Efficiency of AP (AP$_E$) metric, which intends to also measure the\ncomputational consumption of the framework to cater to the requirements of\nreal-world applications. We report experimental results on the challenging COCO\ndataset, in which our single model on a single Titan Xp GPU achieves 35.8 AP\nand 27.8 AP$_E$ at 65 fps with YOLOv4 as base detector, 34.1 AP and 28.6 AP$_E$\nat 12 fps with FCOS as base detector.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 09:44:49 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Tang", "Tutian", ""], ["Xu", "Wenqiang", ""], ["Ye", "Ruolin", ""], ["Yang", "Lixin", ""], ["Lu", "Cewu", ""]]}, {"id": "2012.01059", "submitter": "Quentin Paletta", "authors": "Quentin Paletta and Joan Lasenby", "title": "A Temporally Consistent Image-based Sun Tracking Algorithm for Solar\n  Energy Forecasting Applications", "comments": "Accepted as a workshop paper at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving irradiance forecasting is critical to further increase the share of\nsolar in the energy mix. On a short time scale, fish-eye cameras on the ground\nare used to capture cloud displacements causing the local variability of the\nelectricity production. As most of the solar radiation comes directly from the\nSun, current forecasting approaches use its position in the image as a\nreference to interpret the cloud cover dynamics. However, existing Sun tracking\nmethods rely on external data and a calibration of the camera, which requires\naccess to the device. To address these limitations, this study introduces an\nimage-based Sun tracking algorithm to localise the Sun in the image when it is\nvisible and interpolate its daily trajectory from past observations. We\nvalidate the method on a set of sky images collected over a year at SIRTA's\nlab. Experimental results show that the proposed method provides robust smooth\nSun trajectories with a mean absolute error below 1% of the image size.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 09:59:45 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 17:11:20 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Paletta", "Quentin", ""], ["Lasenby", "Joan", ""]]}, {"id": "2012.01096", "submitter": "Liu Liu", "authors": "Liu Liu, Hongdong Li, Haodong Yao and Ruyi Zha", "title": "PlueckerNet: Learn to Register 3D Line Reconstructions", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aligning two partially-overlapped 3D line reconstructions in Euclidean space\nis challenging, as we need to simultaneously solve correspondences and relative\npose between line reconstructions. This paper proposes a neural network based\nmethod and it has three modules connected in sequence: (i) a Multilayer\nPerceptron (MLP) based network takes Pluecker representations of lines as\ninputs, to extract discriminative line-wise features and matchabilities (how\nlikely each line is going to have a match), (ii) an Optimal Transport (OT)\nlayer takes two-view line-wise features and matchabilities as inputs to\nestimate a 2D joint probability matrix, with each item describes the matchness\nof a line pair, and (iii) line pairs with Top-K matching probabilities are fed\nto a 2-line minimal solver in a RANSAC framework to estimate a six\nDegree-of-Freedom (6-DoF) rigid transformation. Experiments on both indoor and\noutdoor datasets show that the registration (rotation and translation)\nprecision of our method outperforms baselines significantly.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 11:31:56 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Liu", "Liu", ""], ["Li", "Hongdong", ""], ["Yao", "Haodong", ""], ["Zha", "Ruyi", ""]]}, {"id": "2012.01110", "submitter": "Yiran Zhong", "authors": "Yiran Zhong, Yuchao Dai, Hongdong Li", "title": "Efficient Depth Completion Using Learned Bases", "comments": "This work was accomplished in 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new global geometry constraint for depth\ncompletion. By assuming depth maps often lay on low dimensional subspaces, a\ndense depth map can be approximated by a weighted sum of full-resolution\nprincipal depth bases. The principal components of depth fields can be learned\nfrom natural depth maps. The given sparse depth points are served as a data\nterm to constrain the weighting process. When the input depth points are too\nsparse, the recovered dense depth maps are often over smoothed. To address this\nissue, we add a colour-guided auto-regression model as another regularization\nterm. It assumes the reconstructed depth maps should share the same nonlocal\nsimilarity in the accompanying colour image. Our colour-guided PCA depth\ncompletion method has closed-form solutions, thus can be efficiently solved and\nis significantly more accurate than PCA only method. Extensive experiments on\nKITTI and Middlebury datasets demonstrate the superior performance of our\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 11:57:37 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Zhong", "Yiran", ""], ["Dai", "Yuchao", ""], ["Li", "Hongdong", ""]]}, {"id": "2012.01145", "submitter": "Jay Roberts", "authors": "Jay Roberts, Theodoros Tsiligkaridis", "title": "Ultrasound Diagnosis of COVID-19: Robustness and Explainability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diagnosis of COVID-19 at point of care is vital to the containment of the\nglobal pandemic. Point of care ultrasound (POCUS) provides rapid imagery of\nlungs to detect COVID-19 in patients in a repeatable and cost effective way.\nPrevious work has used public datasets of POCUS videos to train an AI model for\ndiagnosis that obtains high sensitivity. Due to the high stakes application we\npropose the use of robust and explainable techniques. We demonstrate\nexperimentally that robust models have more stable predictions and offer\nimproved interpretability. A framework of contrastive explanations based on\nadversarial perturbations is used to explain model predictions that aligns with\nhuman visual perception.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 20:26:39 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Roberts", "Jay", ""], ["Tsiligkaridis", "Theodoros", ""]]}, {"id": "2012.01158", "submitter": "Oran Gafni", "authors": "Oran Gafni, Oron Ashual, Lior Wolf", "title": "Single-Shot Freestyle Dance Reenactment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of motion transfer between a source dancer and a target person is a\nspecial case of the pose transfer problem, in which the target person changes\ntheir pose in accordance with the motions of the dancer.\n  In this work, we propose a novel method that can reanimate a single image by\narbitrary video sequences, unseen during training. The method combines three\nnetworks: (i) a segmentation-mapping network, (ii) a realistic frame-rendering\nnetwork, and (iii) a face refinement network. By separating this task into\nthree stages, we are able to attain a novel sequence of realistic frames,\ncapturing natural motion and appearance. Our method obtains significantly\nbetter visual quality than previous methods and is able to animate diverse body\ntypes and appearances, which are captured in challenging poses, as shown in the\nexperiments and supplementary video.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 12:57:43 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 14:11:57 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Gafni", "Oran", ""], ["Ashual", "Oron", ""], ["Wolf", "Lior", ""]]}, {"id": "2012.01170", "submitter": "Dominic Jack", "authors": "Dominic Jack, Frederic Maire, Simon Denman, Anders Eriksson", "title": "Sparse Convolutions on Continuous Domains for Point Cloud and Event\n  Stream Networks", "comments": "ACCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image convolutions have been a cornerstone of a great number of deep learning\nadvances in computer vision. The research community is yet to settle on an\nequivalent operator for sparse, unstructured continuous data like point clouds\nand event streams however. We present an elegant sparse matrix-based\ninterpretation of the convolution operator for these cases, which is consistent\nwith the mathematical definition of convolution and efficient during training.\nOn benchmark point cloud classification problems we demonstrate networks built\nwith these operations can train an order of magnitude or more faster than top\nexisting methods, whilst maintaining comparable accuracy and requiring a tiny\nfraction of the memory. We also apply our operator to event stream processing,\nachieving state-of-the-art results on multiple tasks with streams of hundreds\nof thousands of events.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 13:05:02 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Jack", "Dominic", ""], ["Maire", "Frederic", ""], ["Denman", "Simon", ""], ["Eriksson", "Anders", ""]]}, {"id": "2012.01172", "submitter": "Burak Yildiz", "authors": "Burak Yildiz, Hayley Hung, Jesse H. Krijthe, Cynthia C. S. Liem, Marco\n  Loog, Gosia Migut, Frans Oliehoek, Annibale Panichella, Przemyslaw Pawelczak,\n  Stjepan Picek, Mathijs de Weerdt, and Jan van Gemert", "title": "ReproducedPapers.org: Openly teaching and structuring machine learning\n  reproducibility", "comments": "Accepted to RRPR 2020: Third Workshop on Reproducible Research in\n  Pattern Recognition", "journal-ref": null, "doi": "10.1007/978-3-030-76423-4_1", "report-no": null, "categories": "cs.CY cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present ReproducedPapers.org: an open online repository for teaching and\nstructuring machine learning reproducibility. We evaluate doing a reproduction\nproject among students and the added value of an online reproduction repository\namong AI researchers. We use anonymous self-assessment surveys and obtained 144\nresponses. Results suggest that students who do a reproduction project place\nmore value on scientific reproductions and become more critical thinkers.\nStudents and AI researchers agree that our online reproduction repository is\nvaluable.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 11:19:45 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Yildiz", "Burak", ""], ["Hung", "Hayley", ""], ["Krijthe", "Jesse H.", ""], ["Liem", "Cynthia C. S.", ""], ["Loog", "Marco", ""], ["Migut", "Gosia", ""], ["Oliehoek", "Frans", ""], ["Panichella", "Annibale", ""], ["Pawelczak", "Przemyslaw", ""], ["Picek", "Stjepan", ""], ["de Weerdt", "Mathijs", ""], ["van Gemert", "Jan", ""]]}, {"id": "2012.01179", "submitter": "Mark Scanlon", "authors": "Felix Anda, Brett A. Becker, David Lillis, Nhien-An Le-Khac and Mark\n  Scanlon", "title": "Assessing the Influencing Factors on the Accuracy of Underage Facial Age\n  Estimation", "comments": null, "journal-ref": "The 6th IEEE International Conference on Cyber Security and\n  Protection of Digital Services (Cyber Security), Dublin, Ireland, June 2020", "doi": "10.1109/CyberSecurity49315.2020.9138851", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Swift response to the detection of endangered minors is an ongoing concern\nfor law enforcement. Many child-focused investigations hinge on digital\nevidence discovery and analysis. Automated age estimation techniques are needed\nto aid in these investigations to expedite this evidence discovery process, and\ndecrease investigator exposure to traumatic material. Automated techniques also\nshow promise in decreasing the overflowing backlog of evidence obtained from\nincreasing numbers of devices and online services. A lack of sufficient\ntraining data combined with natural human variance has been long hindering\naccurate automated age estimation -- especially for underage subjects. This\npaper presented a comprehensive evaluation of the performance of two cloud age\nestimation services (Amazon Web Service's Rekognition service and Microsoft\nAzure's Face API) against a dataset of over 21,800 underage subjects. The\nobjective of this work is to evaluate the influence that certain human\nbiometric factors, facial expressions, and image quality (i.e. blur, noise,\nexposure and resolution) have on the outcome of automated age estimation\nservices. A thorough evaluation allows us to identify the most influential\nfactors to be overcome in future age estimation systems.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 13:09:56 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Anda", "Felix", ""], ["Becker", "Brett A.", ""], ["Lillis", "David", ""], ["Le-Khac", "Nhien-An", ""], ["Scanlon", "Mark", ""]]}, {"id": "2012.01189", "submitter": "Dawid Rymarczyk", "authors": "Adriana Borowa, Dawid Rymarczyk, Dorota Ocho\\'nska, Monika\n  Brzychczy-W{\\l}och, Bartosz Zieli\\'nski", "title": "Classifying bacteria clones using attention-based deep multiple instance\n  learning interpreted by persistence homology", "comments": "Published at the International Joint Conferences on Neural Networks", "journal-ref": "978-0-7381-3366-9/21, 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we analyze if it is possible to distinguish between different\nclones of the same bacteria species (Klebsiella pneumoniae) based only on\nmicroscopic images. It is a challenging task, previously considered impossible\ndue to the high clones similarity. For this purpose, we apply a multi-step\nalgorithm with attention-based multiple instance learning. Except for obtaining\naccuracy at the level of 0.9, we introduce extensive interpretability based on\nCellProfiler and persistence homology, increasing the understandability and\ntrust in the model.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 13:20:39 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 11:27:58 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Borowa", "Adriana", ""], ["Rymarczyk", "Dawid", ""], ["Ocho\u0144ska", "Dorota", ""], ["Brzychczy-W\u0142och", "Monika", ""], ["Zieli\u0144ski", "Bartosz", ""]]}, {"id": "2012.01201", "submitter": "Chandan Gautam", "authors": "Sannidhi P Kumar, Chandan Gautam, Suresh Sundaram", "title": "Meta-Cognition-Based Simple And Effective Approach To Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many researchers have attempted to improve deep learning-based\nobject detection models, both in terms of accuracy and operational speeds.\nHowever, frequently, there is a trade-off between speed and accuracy of such\nmodels, which encumbers their use in practical applications such as autonomous\nnavigation. In this paper, we explore a meta-cognitive learning strategy for\nobject detection to improve generalization ability while at the same time\nmaintaining detection speed. The meta-cognitive method selectively samples the\nobject instances in the training dataset to reduce overfitting. We use YOLO v3\nTiny as a base model for the work and evaluate the performance using the MS\nCOCO dataset. The experimental results indicate an improvement in absolute\nprecision of 2.6% (minimum), and 4.4% (maximum), with no overhead to inference\ntime.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 13:36:51 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Kumar", "Sannidhi P", ""], ["Gautam", "Chandan", ""], ["Sundaram", "Suresh", ""]]}, {"id": "2012.01203", "submitter": "Marie-Julie Rakotosaona", "authors": "Marie-Julie Rakotosaona, Paul Guerrero, Noam Aigerman, Niloy Mitra,\n  Maks Ovsjanikov", "title": "Learning Delaunay Surface Elements for Mesh Reconstruction", "comments": null, "journal-ref": "CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for reconstructing triangle meshes from point clouds.\nExisting learning-based methods for mesh reconstruction mostly generate\ntriangles individually, making it hard to create manifold meshes. We leverage\nthe properties of 2D Delaunay triangulations to construct a mesh from manifold\nsurface elements. Our method first estimates local geodesic neighborhoods\naround each point. We then perform a 2D projection of these neighborhoods using\na learned logarithmic map. A Delaunay triangulation in this 2D domain is\nguaranteed to produce a manifold patch, which we call a Delaunay surface\nelement. We synchronize the local 2D projections of neighboring elements to\nmaximize the manifoldness of the reconstructed mesh. Our results show that we\nachieve better overall manifoldness of our reconstructed meshes than current\nmethods to reconstruct meshes with arbitrary topology. Our code, data and\npretrained models can be found online:\nhttps://github.com/mrakotosaon/dse-meshing\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 13:42:07 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 17:17:14 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Rakotosaona", "Marie-Julie", ""], ["Guerrero", "Paul", ""], ["Aigerman", "Noam", ""], ["Mitra", "Niloy", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "2012.01204", "submitter": "Francisco J. Castellanos", "authors": "Francisco J. Castellanos, Antonio-Javier Gallego, Jorge Calvo-Zaragoza", "title": "Unsupervised Neural Domain Adaptation for Document Image Binarization", "comments": null, "journal-ref": "Pattern Recognition, 2021, Vol. 119, p. 108099", "doi": "10.1016/j.patcog.2021.108099", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Binarization is a well-known image processing task, whose objective is to\nseparate the foreground of an image from the background. One of the many tasks\nfor which it is useful is that of preprocessing document images in order to\nidentify relevant information, such as text or symbols. The wide variety of\ndocument types, alphabets, and formats makes binarization challenging. There\nare multiple proposals with which to solve this problem, from classical\nmanually-adjusted methods, to more recent approaches based on machine learning.\nThe latter techniques require a large amount of training data in order to\nobtain good results; however, labeling a portion of each existing collection of\ndocuments is not feasible in practice. This is a common problem in supervised\nlearning, which can be addressed by using the so-called Domain Adaptation (DA)\ntechniques. These techniques take advantage of the knowledge learned in one\ndomain, for which labeled data are available, to apply it to other domains for\nwhich there are no labeled data. This paper proposes a method that combines\nneural networks and DA in order to carry out unsupervised document\nbinarization. However, when both the source and target domains are very\nsimilar, this adaptation could be detrimental. Our methodology, therefore,\nfirst measures the similarity between domains in an innovative manner in order\nto determine whether or not it is appropriate to apply the adaptation process.\nThe results reported in the experimentation, when evaluating up to 20 possible\ncombinations among five different domains, show that our proposal successfully\ndeals with the binarization of new document domains without the need for\nlabeled data.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 13:42:38 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 09:44:37 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Castellanos", "Francisco J.", ""], ["Gallego", "Antonio-Javier", ""], ["Calvo-Zaragoza", "Jorge", ""]]}, {"id": "2012.01211", "submitter": "Chaofeng Chen", "authors": "Chaofeng Chen, Dihong Gong, Hao Wang, Zhifeng Li, Kwan-Yee K. Wong", "title": "Learning Spatial Attention for Face Super-Resolution", "comments": "TIP 2020. Codes are available at\n  https://github.com/chaofengc/Face-SPARNet", "journal-ref": null, "doi": "10.1109/TIP.2020.3043093", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  General image super-resolution techniques have difficulties in recovering\ndetailed face structures when applying to low resolution face images. Recent\ndeep learning based methods tailored for face images have achieved improved\nperformance by jointly trained with additional task such as face parsing and\nlandmark prediction. However, multi-task learning requires extra manually\nlabeled data. Besides, most of the existing works can only generate relatively\nlow resolution face images (e.g., $128\\times128$), and their applications are\ntherefore limited. In this paper, we introduce a novel SPatial Attention\nResidual Network (SPARNet) built on our newly proposed Face Attention Units\n(FAUs) for face super-resolution. Specifically, we introduce a spatial\nattention mechanism to the vanilla residual blocks. This enables the\nconvolutional layers to adaptively bootstrap features related to the key face\nstructures and pay less attention to those less feature-rich regions. This\nmakes the training more effective and efficient as the key face structures only\naccount for a very small portion of the face image. Visualization of the\nattention maps shows that our spatial attention network can capture the key\nface structures well even for very low resolution faces (e.g., $16\\times16$).\nQuantitative comparisons on various kinds of metrics (including PSNR, SSIM,\nidentity similarity, and landmark detection) demonstrate the superiority of our\nmethod over current state-of-the-arts. We further extend SPARNet with\nmulti-scale discriminators, named as SPARNetHD, to produce high resolution\nresults (i.e., $512\\times512$). We show that SPARNetHD trained with synthetic\ndata cannot only produce high quality and high resolution outputs for\nsynthetically degraded face images, but also show good generalization ability\nto real world low quality face images.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 13:54:25 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 12:33:42 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Chen", "Chaofeng", ""], ["Gong", "Dihong", ""], ["Wang", "Hao", ""], ["Li", "Zhifeng", ""], ["Wong", "Kwan-Yee K.", ""]]}, {"id": "2012.01230", "submitter": "David Griffiths Mr", "authors": "David Griffiths, Jan Boehm, Tobias Ritschel", "title": "Curiosity-driven 3D Scene Structure from Single-image Self-supervision", "comments": "15 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous work has demonstrated learning isolated 3D objects (voxel grids,\npoint clouds, meshes, etc.) from 2D-only self-supervision. Here we set out to\nextend this to entire 3D scenes made out of multiple objects, including their\nlocation, orientation and type, and the scenes illumination. Once learned, we\ncan map arbitrary 2D images to 3D scene structure. We analyze why\nanalysis-by-synthesis-like losses for supervision of 3D scene structure using\ndifferentiable rendering is not practical, as it almost always gets stuck in\nlocal minima of visual ambiguities. This can be overcome by a novel form of\ntraining: we use an additional network to steer the optimization itself to\nexplore the full gamut of possible solutions \\ie to be curious, and hence, to\nresolve those ambiguities and find workable minima. The resulting system\nconverts 2D images of different virtual or real images into complete 3D scenes,\nlearned only from 2D images of those scenes.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 14:17:16 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 13:55:40 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Griffiths", "David", ""], ["Boehm", "Jan", ""], ["Ritschel", "Tobias", ""]]}, {"id": "2012.01241", "submitter": "Refik Soyak", "authors": "Refik Soyak, Ebru Navruz, Eda Ozgu Ersoy, Gastao Cruz, Claudia Prieto,\n  Andrew P. King, Devrim Unay, Ilkay Oksuz", "title": "Channel Attention Networks for Robust MR Fingerprinting Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Magnetic Resonance Fingerprinting (MRF) enables simultaneous mapping of\nmultiple tissue parameters such as T1 and T2 relaxation times. The working\nprinciple of MRF relies on varying acquisition parameters pseudo-randomly, so\nthat each tissue generates its unique signal evolution during scanning. Even\nthough MRF provides faster scanning, it has disadvantages such as erroneous and\nslow generation of the corresponding parametric maps, which needs to be\nimproved. Moreover, there is a need for explainable architectures for\nunderstanding the guiding signals to generate accurate parametric maps. In this\npaper, we addressed both of these shortcomings by proposing a novel neural\nnetwork architecture consisting of a channel-wise attention module and a fully\nconvolutional network. The proposed approach, evaluated over 3 simulated MRF\nsignals, reduces error in the reconstruction of tissue parameters by 8.88% for\nT1 and 75.44% for T2 with respect to state-of-the-art methods. Another\ncontribution of this study is a new channel selection method: attention-based\nchannel selection. Furthermore, the effect of patch size and temporal frames of\nMRF signal on channel reduction are analyzed by employing a channel-wise\nattention.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 14:34:40 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Soyak", "Refik", ""], ["Navruz", "Ebru", ""], ["Ersoy", "Eda Ozgu", ""], ["Cruz", "Gastao", ""], ["Prieto", "Claudia", ""], ["King", "Andrew P.", ""], ["Unay", "Devrim", ""], ["Oksuz", "Ilkay", ""]]}, {"id": "2012.01245", "submitter": "Fabian Schilling", "authors": "Fabian Schilling, Fabrizio Schiano, Dario Floreano", "title": "Vision-based Drone Flocking in Outdoor Environments", "comments": "8 pages, 8 figures, accepted for publication in the IEEE Robotics and\n  Automation Letters (RA-L) on February 2, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized deployment of drone swarms usually relies on inter-agent\ncommunication or visual markers that are mounted on the vehicles to simplify\ntheir mutual detection. This letter proposes a vision-based detection and\ntracking algorithm that enables groups of drones to navigate without\ncommunication or visual markers. We employ a convolutional neural network to\ndetect and localize nearby agents onboard the quadcopters in real-time. Rather\nthan manually labeling a dataset, we automatically annotate images to train the\nneural network using background subtraction by systematically flying a\nquadcopter in front of a static camera. We use a multi-agent state tracker to\nestimate the relative positions and velocities of nearby agents, which are\nsubsequently fed to a flocking algorithm for high-level control. The drones are\nequipped with multiple cameras to provide omnidirectional visual inputs. The\ncamera setup ensures the safety of the flock by avoiding blind spots regardless\nof the agent configuration. We evaluate the approach with a group of three real\nquadcopters that are controlled using the proposed vision-based flocking\nalgorithm. The results show that the drones can safely navigate in an outdoor\nenvironment despite substantial background clutter and difficult lighting\nconditions. The source code, image dataset, and trained detection model are\navailable at https://github.com/lis-epfl/vswarm.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 14:44:40 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 10:13:38 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Schilling", "Fabian", ""], ["Schiano", "Fabrizio", ""], ["Floreano", "Dario", ""]]}, {"id": "2012.01248", "submitter": "Micah Bowles", "authors": "Micah Bowles, Anna M. M. Scaife, Fiona Porter, Hongming Tang, David J.\n  Bastien", "title": "Attention-gating for improved radio galaxy classification", "comments": "18 pages, 16 figures, Published in MNRAS", "journal-ref": "MNRAS 501 (2021) 4579-4595", "doi": "10.1093/mnras/staa3946", "report-no": null, "categories": "astro-ph.GA astro-ph.IM cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we introduce attention as a state of the art mechanism for\nclassification of radio galaxies using convolutional neural networks. We\npresent an attention-based model that performs on par with previous classifiers\nwhile using more than 50% fewer parameters than the next smallest classic CNN\napplication in this field. We demonstrate quantitatively how the selection of\nnormalisation and aggregation methods used in attention-gating can affect the\noutput of individual models, and show that the resulting attention maps can be\nused to interpret the classification choices made by the model. We observe that\nthe salient regions identified by the our model align well with the regions an\nexpert human classifier would attend to make equivalent classifications. We\nshow that while the selection of normalisation and aggregation may only\nminimally affect the performance of individual models, it can significantly\naffect the interpretability of the respective attention maps and by selecting a\nmodel which aligns well with how astronomers classify radio sources by eye, a\nuser can employ the model in a more effective manner.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 14:49:53 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 13:09:41 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Bowles", "Micah", ""], ["Scaife", "Anna M. M.", ""], ["Porter", "Fiona", ""], ["Tang", "Hongming", ""], ["Bastien", "David J.", ""]]}, {"id": "2012.01250", "submitter": "Leticia Pinto-Alva", "authors": "Leticia Pinto-Alva, Ian K. Torres, Rosangel Garcia, Ziyan Yang,\n  Vicente Ordonez", "title": "Chair Segments: A Compact Benchmark for the Study of Object Segmentation", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, datasets and benchmarks have had an outsized influence on the\ndesign of novel algorithms. In this paper, we introduce ChairSegments, a novel\nand compact semi-synthetic dataset for object segmentation. We also show\nempirical findings in transfer learning that mirror recent findings for image\nclassification. We particularly show that models that are fine-tuned from a\npretrained set of weights lie in the same basin of the optimization landscape.\nChairSegments consists of a diverse set of prototypical images of chairs with\ntransparent backgrounds composited into a diverse array of backgrounds. We aim\nfor ChairSegments to be the equivalent of the CIFAR-10 dataset but for quickly\ndesigning and iterating over novel model architectures for segmentation. On\nChair Segments, a U-Net model can be trained to full convergence in only thirty\nminutes using a single GPU. Finally, while this dataset is semi-synthetic, it\ncan be a useful proxy for real data, leading to state-of-the-art accuracy on\nthe Object Discovery dataset when used as a source of pretraining.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 14:54:03 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Pinto-Alva", "Leticia", ""], ["Torres", "Ian K.", ""], ["Garcia", "Rosangel", ""], ["Yang", "Ziyan", ""], ["Ordonez", "Vicente", ""]]}, {"id": "2012.01271", "submitter": "Taewook Kim", "authors": "Taewook Kim and Yonghyun Kim", "title": "Suppressing Spoof-irrelevant Factors for Domain-agnostic Face\n  Anti-spoofing", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing aims to prevent false authentications of face recognition\nsystems by distinguishing whether an image is originated from a human face or a\nspoof medium. We propose a novel method called Doubly Adversarial Suppression\nNetwork (DASN) for domain-agnostic face anti-spoofing; DASN improves the\ngeneralization ability to unseen domains by learning to effectively suppress\nspoof-irrelevant factors (SiFs) (e.g., camera sensors, illuminations). To\nachieve our goal, we introduce two types of adversarial learning schemes. In\nthe first adversarial learning scheme, multiple SiFs are suppressed by\ndeploying multiple discrimination heads that are trained against an encoder. In\nthe second adversarial learning scheme, each of the discrimination heads is\nalso adversarially trained to suppress a spoof factor, and the group of the\nsecondary spoof classifier and the encoder aims to intensify the spoof factor\nby overcoming the suppression. We evaluate the proposed method on four public\nbenchmark datasets, and achieve remarkable evaluation results. The results\ndemonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 15:27:19 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Kim", "Taewook", ""], ["Kim", "Yonghyun", ""]]}, {"id": "2012.01295", "submitter": "Jing Su", "authors": "Jing Su, Chenghua Lin, Mian Zhou, Qingyun Dai, Haoyu Lv", "title": "Generating Descriptions for Sequential Images with Local-Object\n  Attention and Global Semantic Context Modelling", "comments": "Accepted by INLG 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an end-to-end CNN-LSTM model for generating\ndescriptions for sequential images with a local-object attention mechanism. To\ngenerate coherent descriptions, we capture global semantic context using a\nmulti-layer perceptron, which learns the dependencies between sequential\nimages. A paralleled LSTM network is exploited for decoding the sequence\ndescriptions. Experimental results show that our model outperforms the baseline\nacross three different evaluation metrics on the datasets published by\nMicrosoft.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 16:07:32 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Su", "Jing", ""], ["Lin", "Chenghua", ""], ["Zhou", "Mian", ""], ["Dai", "Qingyun", ""], ["Lv", "Haoyu", ""]]}, {"id": "2012.01311", "submitter": "Vladimir Iashin", "authors": "Vladimir Iashin, Francesca Palermo, G\\\"okhan Solak, Claudio Coppola", "title": "Top-1 CORSMAL Challenge 2020 Submission: Filling Mass Estimation Using\n  Multi-modal Observations of Human-robot Handovers", "comments": "Code: https://github.com/v-iashin/CORSMAL Docker:\n  https://hub.docker.com/r/iashin/corsmal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.RO cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human-robot object handover is a key skill for the future of human-robot\ncollaboration. CORSMAL 2020 Challenge focuses on the perception part of this\nproblem: the robot needs to estimate the filling mass of a container held by a\nhuman. Although there are powerful methods in image processing and audio\nprocessing individually, answering such a problem requires processing data from\nmultiple sensors together. The appearance of the container, the sound of the\nfilling, and the depth data provide essential information. We propose a\nmulti-modal method to predict three key indicators of the filling mass: filling\ntype, filling level, and container capacity. These indicators are then combined\nto estimate the filling mass of a container. Our method obtained Top-1 overall\nperformance among all submissions to CORSMAL 2020 Challenge on both public and\nprivate subsets while showing no evidence of overfitting. Our source code is\npublicly available: https://github.com/v-iashin/CORSMAL\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 16:31:03 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Iashin", "Vladimir", ""], ["Palermo", "Francesca", ""], ["Solak", "G\u00f6khan", ""], ["Coppola", "Claudio", ""]]}, {"id": "2012.01321", "submitter": "Korranat Naruenatthanaset", "authors": "Korranat Naruenatthanaset, Thanarat H. Chalidabhongse, Duangdao\n  Palasuwan, Nantheera Anantrasirichai, Attakorn Palasuwan", "title": "Red Blood Cell Segmentation with Overlapping Cell Separation and\n  Classification on Imbalanced Dataset", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automated red blood cell (RBC) classification on blood smear images helps\nhematologists to analyze RBC lab results in a reduced time and cost. However,\noverlapping cells can cause incorrect predicted results, and so they have to be\nseparated into multiple single RBCs before classifying. To classify multiple\nclasses with deep learning, imbalance problems are common in medical imaging\nbecause normal samples are always higher than rare disease samples. This paper\npresents a new method to segment and classify RBCs from blood smear images,\nspecifically to tackle cell overlapping and data imbalance problems. Focusing\non overlapping cell separation, our segmentation process first estimates\nellipses to represent RBCs. The method detects the concave points and then\nfinds the ellipses using directed ellipse fitting. The accuracy from 20 blood\nsmear images was 0.889. Classification requires balanced training datasets.\nHowever, some RBC types are rare. The imbalance ratio of this dataset was\n34.538 for 12 RBC classes from 20,875 individual RBC samples. The use of\nmachine learning for RBC classification with an imbalanced dataset is hence\nmore challenging than many other applications. We analyzed techniques to deal\nwith this problem. The best accuracy and F1-score were 0.921 and 0.8679,\nrespectively, using EfficientNet-B1 with augmentation. Experimental results\nshowed that the weight balancing technique with augmentation had the potential\nto deal with imbalance problems by improving the F1-score on minority classes,\nwhile data augmentation significantly improved the overall classification\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 16:49:51 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 06:29:54 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 14:47:50 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Naruenatthanaset", "Korranat", ""], ["Chalidabhongse", "Thanarat H.", ""], ["Palasuwan", "Duangdao", ""], ["Anantrasirichai", "Nantheera", ""], ["Palasuwan", "Attakorn", ""]]}, {"id": "2012.01338", "submitter": "Tobias Schlagenhauf", "authors": "Tobias Schlagenhauf, Faruk Yildirim, Benedikt Br\\\"uckner, J\\\"urgen\n  Fleischer", "title": "Siamese Basis Function Networks for Defect Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Defect classification on metallic surfaces is considered a critical issue\nsince substantial quantities of steel and other metals are processed by the\nmanufacturing industry on a daily basis. The authors propose a new approach\nwhere they introduce the usage of so called Siamese Kernels in a Basis Function\nNetwork to create the Siamese Basis Function Network (SBF-Network). The\nunderlying idea is to classify by comparison using similarity scores. This\nclassification is reinforced through efficient deep learning based feature\nextraction methods. First, a center image is assigned to each Siamese Kernel.\nThe Kernels are then trained to generate encodings in a way that enables them\nto distinguish their center from other images in the dataset. Using this\napproach the authors created some kind of class-awareness inside the Siamese\nKernels. To classify a given image, each Siamese Kernel generates a feature\nvector for its center as well as the given image. These vectors represent\nencodings of the respective images in a lower-dimensional space. The distance\nbetween each pair of encodings is then computed using the cosine distance\ntogether with radial basis functions. The distances are fed into a multilayer\nneural network to perform the classification. With this approach the authors\nachieved outstanding results on the state of the art NEU surface defect\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 17:16:42 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 08:51:13 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 06:34:17 GMT"}, {"version": "v4", "created": "Wed, 9 Dec 2020 07:48:48 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Schlagenhauf", "Tobias", ""], ["Yildirim", "Faruk", ""], ["Br\u00fcckner", "Benedikt", ""], ["Fleischer", "J\u00fcrgen", ""]]}, {"id": "2012.01345", "submitter": "Ricardo Guerrero", "authors": "Ricardo Guerrero, Hai Xuan Pham and Vladimir Pavlovic", "title": "Cross-modal Retrieval and Synthesis (X-MRS): Closing the modality gap in\n  shared subspace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computational food analysis (CFA), a broad set of methods that attempt to\nautomate food understanding, naturally requires analysis of multi-modal\nevidence of a particular food or dish, e.g. images, recipe text, preparation\nvideo, nutrition labels, etc. A key to making CFA possible is multi-modal\nshared subspace learning, which in turn can be used for cross-modal retrieval\nand/or synthesis, particularly, between food images and their corresponding\ntextual recipes. In this work we propose a simple yet novel architecture for\nshared subspace learning, which is used to tackle the food image-to-recipe\nretrieval problem. Our proposed method employs an effective transformer based\nmultilingual recipe encoder coupled with a traditional image embedding\narchitecture. Experimental analysis on the public Recipe1M dataset shows that\nthe subspace learned via the proposed method outperforms the current\nstate-of-the-arts (SoTA) in food retrieval by a large margin, obtaining\nrecall@1 of 0.64. Furthermore, in order to demonstrate the representational\npower of the learned subspace, we propose a generative food image synthesis\nmodel conditioned on the embeddings of recipes. Synthesized images can\neffectively reproduce the visual appearance of paired samples, achieving R@1 of\n0.68 in the image-to-recipe retrieval experiment, thus effectively capturing\nthe semantics of the textual recipe.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 17:27:00 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 22:49:07 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Guerrero", "Ricardo", ""], ["Pham", "Hai Xuan", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "2012.01362", "submitter": "Li Yang", "authors": "Li Yang, Adnan Siraj Rakin and Deliang Fan", "title": "$DA^3$: Deep Additive Attention Adaption for Memory-Efficient On-Device\n  Multi-Domain Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, one practical limitation of deep neural network (DNN) is its high\ndegree of specialization to a single task or domain (e.g., one visual domain).\nIt motivates researchers to develop algorithms that can adapt DNN model to\nmultiple domains sequentially, meanwhile still performing well on the past\ndomains, which is known as multi-domain learning. Almost all conventional\nmethods only focus on improving accuracy with minimal parameter update, while\nignoring high computing and memory cost during training, which makes it\nimpossible to deploy multi-domain learning into more and more widely used\nresource-limited edge devices, like mobile phone, IoT, embedded system, etc.\nDuring our study in multi-domain training, we observe that large memory used\nfor activation storage is the bottleneck that largely limits the training time\nand cost on edge devices. To reduce training memory usage, while keeping the\ndomain adaption accuracy performance, in this work, we propose Deep Additive\nAttention Adaption, a novel memory-efficient on-device multi-domain learning\nmethod, aiming to achieve domain adaption on memory-limited edge devices. To\nreduce the training memory consumption during on-device training, $DA^3$\nfreezes the weights of the pre-trained backbone model (i.e., no need to store\nactivation features during backward propagation). Furthermore, to improve the\nadaption accuracy performance, we propose to improve the model capacity by\nlearning a novel additive attention adaptor module, which is also designed to\navoid activation memory buffering for improving memory efficiency. We validate\n$DA^3$ on multiple datasets against state-of-the-art methods, which shows good\nimprovement in both accuracy and training time.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 18:03:18 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 20:13:15 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Yang", "Li", ""], ["Rakin", "Adnan Siraj", ""], ["Fan", "Deliang", ""]]}, {"id": "2012.01377", "submitter": "Mihai Dusmanu", "authors": "Mihai Dusmanu, Ondrej Miksik, Johannes L. Sch\\\"onberger, Marc\n  Pollefeys", "title": "Cross-Descriptor Visual Localization and Mapping", "comments": "15 pages, 11 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual localization and mapping is the key technology underlying the majority\nof Mixed Reality and robotics systems. Most state-of-the-art approaches rely on\nlocal features to establish correspondences between images. In this paper, we\npresent three novel scenarios for localization and mapping which require the\ncontinuous update of feature representations and the ability to match across\ndifferent feature types. While localization and mapping is a fundamental\ncomputer vision problem, the traditional setup treats it as a single-shot\nprocess using the same local image features throughout the evolution of a map.\nThis assumes the whole process is repeated from scratch whenever the underlying\nfeatures are changed. However, reiterating it is typically impossible in\npractice, because raw images are often not stored and re-building the maps\ncould lead to loss of the attached digital content. To overcome the limitations\nof current approaches, we present the first principled solution to\ncross-descriptor localization and mapping. Our data-driven approach is agnostic\nto the feature descriptor type, has low computational requirements, and scales\nlinearly with the number of description algorithms. Extensive experiments\ndemonstrate the effectiveness of our approach on state-of-the-art benchmarks\nfor a variety of handcrafted and learned features.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 18:19:51 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Dusmanu", "Mihai", ""], ["Miksik", "Ondrej", ""], ["Sch\u00f6nberger", "Johannes L.", ""], ["Pollefeys", "Marc", ""]]}, {"id": "2012.01386", "submitter": "Nikhil Kapoor", "authors": "Nikhil Kapoor, Chun Yuan, Jonas L\\\"ohdefink, Roland Zimmermann, Serin\n  Varghese, Fabian H\\\"uger, Nico Schmidt, Peter Schlicht, Tim Fingscheidt", "title": "A Self-Supervised Feature Map Augmentation (FMA) Loss and Combined\n  Augmentations Finetuning to Efficiently Improve the Robustness of CNNs", "comments": "Accepted at ACM CSCS 2020 (8 pages, 4 figures)", "journal-ref": null, "doi": "10.1145/3385958.3430477", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are often not robust to semantically-irrelevant changes\nin the input. In this work we address the issue of robustness of\nstate-of-the-art deep convolutional neural networks (CNNs) against commonly\noccurring distortions in the input such as photometric changes, or the addition\nof blur and noise. These changes in the input are often accounted for during\ntraining in the form of data augmentation. We have two major contributions:\nFirst, we propose a new regularization loss called feature-map augmentation\n(FMA) loss which can be used during finetuning to make a model robust to\nseveral distortions in the input. Second, we propose a new combined\naugmentations (CA) finetuning strategy, that results in a single model that is\nrobust to several augmentation types at the same time in a data-efficient\nmanner. We use the CA strategy to improve an existing state-of-the-art method\ncalled stability training (ST). Using CA, on an image classification task with\ndistorted images, we achieve an accuracy improvement of on average 8.94% with\nFMA and 8.86% with ST absolute on CIFAR-10 and 8.04% with FMA and 8.27% with ST\nabsolute on ImageNet, compared to 1.98% and 2.12%, respectively, with the well\nknown data augmentation method, while keeping the clean baseline performance.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 18:32:14 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Kapoor", "Nikhil", ""], ["Yuan", "Chun", ""], ["L\u00f6hdefink", "Jonas", ""], ["Zimmermann", "Roland", ""], ["Varghese", "Serin", ""], ["H\u00fcger", "Fabian", ""], ["Schmidt", "Nico", ""], ["Schlicht", "Peter", ""], ["Fingscheidt", "Tim", ""]]}, {"id": "2012.01392", "submitter": "Jonathan Jones", "authors": "Jonathan D. Jones, Cathryn Cortesa, Amy Shelton, Barbara Landau,\n  Sanjeev Khudanpur, and Gregory D. Hager", "title": "Fine-grained activity recognition for assembly videos", "comments": "8 pages, 6 figures. Submitted to RA-L/ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the task of recognizing assembly actions as a\nstructure (e.g. a piece of furniture or a toy block tower) is built up from a\nset of primitive objects. Recognizing the full range of assembly actions\nrequires perception at a level of spatial detail that has not been attempted in\nthe action recognition literature to date. We extend the fine-grained activity\nrecognition setting to address the task of assembly action recognition in its\nfull generality by unifying assembly actions and kinematic structures within a\nsingle framework. We use this framework to develop a general method for\nrecognizing assembly actions from observation sequences, along with observation\nfeatures that take advantage of a spatial assembly's special structure.\nFinally, we evaluate our method empirically on two application-driven data\nsources: (1) An IKEA furniture-assembly dataset, and (2) A block-building\ndataset. On the first, our system recognizes assembly actions with an average\nframewise accuracy of 70% and an average normalized edit distance of 10%. On\nthe second, which requires fine-grained geometric reasoning to distinguish\nbetween assemblies, our system attains an average normalized edit distance of\n23% -- a relative improvement of 69% over prior work.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 18:38:17 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Jones", "Jonathan D.", ""], ["Cortesa", "Cathryn", ""], ["Shelton", "Amy", ""], ["Landau", "Barbara", ""], ["Khudanpur", "Sanjeev", ""], ["Hager", "Gregory D.", ""]]}, {"id": "2012.01405", "submitter": "Long Zhao", "authors": "Long Zhao, Yuxiao Wang, Jiaping Zhao, Liangzhe Yuan, Jennifer J. Sun,\n  Florian Schroff, Hartwig Adam, Xi Peng, Dimitris Metaxas, Ting Liu", "title": "Learning View-Disentangled Human Pose Representation by Contrastive\n  Cross-View Mutual Information Maximization", "comments": "Accepted to CVPR 2021 (Oral presentation). Code is available at\n  https://github.com/google-research/google-research/tree/master/poem", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel representation learning method to disentangle\npose-dependent as well as view-dependent factors from 2D human poses. The\nmethod trains a network using cross-view mutual information maximization\n(CV-MIM) which maximizes mutual information of the same pose performed from\ndifferent viewpoints in a contrastive learning manner. We further propose two\nregularization terms to ensure disentanglement and smoothness of the learned\nrepresentations. The resulting pose representations can be used for cross-view\naction recognition. To evaluate the power of the learned representations, in\naddition to the conventional fully-supervised action recognition settings, we\nintroduce a novel task called single-shot cross-view action recognition. This\ntask trains models with actions from only one single viewpoint while models are\nevaluated on poses captured from all possible viewpoints. We evaluate the\nlearned representations on standard benchmarks for action recognition, and show\nthat (i) CV-MIM performs competitively compared with the state-of-the-art\nmodels in the fully-supervised scenarios; (ii) CV-MIM outperforms other\ncompeting methods by a large margin in the single-shot cross-view setting;\n(iii) and the learned representations can significantly boost the performance\nwhen reducing the amount of supervised training data. Our code is made publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/poem\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 18:55:35 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 04:05:44 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Zhao", "Long", ""], ["Wang", "Yuxiao", ""], ["Zhao", "Jiaping", ""], ["Yuan", "Liangzhe", ""], ["Sun", "Jennifer J.", ""], ["Schroff", "Florian", ""], ["Adam", "Hartwig", ""], ["Peng", "Xi", ""], ["Metaxas", "Dimitris", ""], ["Liu", "Ting", ""]]}, {"id": "2012.01411", "submitter": "Fangjinhua Wang", "authors": "Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Pablo Speciale,\n  Marc Pollefeys", "title": "PatchmatchNet: Learned Multi-View Patchmatch Stereo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PatchmatchNet, a novel and learnable cascade formulation of\nPatchmatch for high-resolution multi-view stereo. With high computation speed\nand low memory requirement, PatchmatchNet can process higher resolution imagery\nand is more suited to run on resource limited devices than competitors that\nemploy 3D cost volume regularization. For the first time we introduce an\niterative multi-scale Patchmatch in an end-to-end trainable architecture and\nimprove the Patchmatch core algorithm with a novel and learned adaptive\npropagation and evaluation scheme for each iteration. Extensive experiments\nshow a very competitive performance and generalization for our method on DTU,\nTanks & Temples and ETH3D, but at a significantly higher efficiency than all\nexisting top-performing models: at least two and a half times faster than\nstate-of-the-art methods with twice less memory usage.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 18:59:02 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Wang", "Fangjinhua", ""], ["Galliani", "Silvano", ""], ["Vogel", "Christoph", ""], ["Speciale", "Pablo", ""], ["Pollefeys", "Marc", ""]]}, {"id": "2012.01415", "submitter": "Fabio Cermelli", "authors": "Fabio Cermelli, Massimiliano Mancini, Yongqin Xian, Zeynep Akata,\n  Barbara Caputo", "title": "A Few Guidelines for Incremental Few-Shot Segmentation", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reducing the amount of supervision required by neural networks is especially\nimportant in the context of semantic segmentation, where collecting dense\npixel-level annotations is particularly expensive. In this paper, we address\nthis problem from a new perspective: Incremental Few-Shot Segmentation. In\nparticular, given a pretrained segmentation model and few images containing\nnovel classes, our goal is to learn to segment novel classes while retaining\nthe ability to segment previously seen ones. In this context, we discover,\nagainst all beliefs, that fine-tuning the whole architecture with these few\nimages is not only meaningful, but also very effective. We show how the main\nproblems of end-to-end training in this scenario are i) the drift of the\nbatch-normalization statistics toward novel classes that we can fix with batch\nrenormalization and ii) the forgetting of old classes, that we can fix with\nregularization strategies. We summarize our findings with five guidelines that\ntogether consistently lead to the state of the art on the COCO and Pascal-VOC\n2012 datasets, with different number of images per class and even with multiple\nlearning episodes.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 20:45:56 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Cermelli", "Fabio", ""], ["Mancini", "Massimiliano", ""], ["Xian", "Yongqin", ""], ["Akata", "Zeynep", ""], ["Caputo", "Barbara", ""]]}, {"id": "2012.01451", "submitter": "Aljaz Bozic", "authors": "Alja\\v{z} Bo\\v{z}i\\v{c}, Pablo Palafox, Michael Zollh\\\"ofer, Justus\n  Thies, Angela Dai, Matthias Nie{\\ss}ner", "title": "Neural Deformation Graphs for Globally-consistent Non-rigid\n  Reconstruction", "comments": "Video: https://youtu.be/vyq36eFkdWo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Neural Deformation Graphs for globally-consistent deformation\ntracking and 3D reconstruction of non-rigid objects. Specifically, we\nimplicitly model a deformation graph via a deep neural network. This neural\ndeformation graph does not rely on any object-specific structure and, thus, can\nbe applied to general non-rigid deformation tracking. Our method globally\noptimizes this neural graph on a given sequence of depth camera observations of\na non-rigidly moving object. Based on explicit viewpoint consistency as well as\ninter-frame graph and surface consistency constraints, the underlying network\nis trained in a self-supervised fashion. We additionally optimize for the\ngeometry of the object with an implicit deformable multi-MLP shape\nrepresentation. Our approach does not assume sequential input data, thus\nenabling robust tracking of fast motions or even temporally disconnected\nrecordings. Our experiments demonstrate that our Neural Deformation Graphs\noutperform state-of-the-art non-rigid reconstruction approaches both\nqualitatively and quantitatively, with 64% improved reconstruction and 62%\nimproved deformation tracking performance.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 19:00:13 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Bo\u017ei\u010d", "Alja\u017e", ""], ["Palafox", "Pablo", ""], ["Zollh\u00f6fer", "Michael", ""], ["Thies", "Justus", ""], ["Dai", "Angela", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2012.01461", "submitter": "Jihua Huang", "authors": "Jihua Huang, Amir Tamrakar", "title": "ACE-Net: Fine-Level Face Alignment through Anchors and Contours\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel facial Anchors and Contours Estimation framework, ACE-Net,\nfor fine-level face alignment tasks. ACE-Net predicts facial anchors and\ncontours that are richer than traditional facial landmarks and more accurate\nthan facial boundaries. In addition, it does not suffer from the ambiguities\nand inconsistencies in facial-landmarks definitions. We introduce a weakly\nsupervised loss enabling ACE-Net to learn from existing facial landmarks\ndatasets without the need for extra annotations. Synthetic data is also used\nduring training to bridge the density gap between landmarks annotation and true\nfacial contours. We evaluate ACE-Net on commonly used face alignment datasets\n300-W and HELEN, and show that ACE-Net achieves significantly higher fine-level\nface alignment accuracy than landmarks based models, without compromising its\nperformance at the landmarks level. The proposed ACE-Net framework does not\nrely on any specific network architecture and thus can be applied on top of\nexisting face alignment models for finer face alignment representation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 19:04:12 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Huang", "Jihua", ""], ["Tamrakar", "Amir", ""]]}, {"id": "2012.01468", "submitter": "Yuqi Ouyang", "authors": "Yuqi Ouyang, Victor Sanchez", "title": "Video Anomaly Detection by Estimating Likelihood of Representations", "comments": "Accepted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video anomaly detection is a challenging task not only because it involves\nsolving many sub-tasks such as motion representation, object localization and\naction recognition, but also because it is commonly considered as an\nunsupervised learning problem that involves detecting outliers. Traditionally,\nsolutions to this task have focused on the mapping between video frames and\ntheir low-dimensional features, while ignoring the spatial connections of those\nfeatures. Recent solutions focus on analyzing these spatial connections by\nusing hard clustering techniques, such as K-Means, or applying neural networks\nto map latent features to a general understanding, such as action attributes.\nIn order to solve video anomaly in the latent feature space, we propose a deep\nprobabilistic model to transfer this task into a density estimation problem\nwhere latent manifolds are generated by a deep denoising autoencoder and\nclustered by expectation maximization. Evaluations on several benchmarks\ndatasets show the strengths of our model, achieving outstanding performance on\nchallenging datasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 19:16:22 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Ouyang", "Yuqi", ""], ["Sanchez", "Victor", ""]]}, {"id": "2012.01469", "submitter": "Vikram V. Ramaswamy", "authors": "Vikram V. Ramaswamy, Sunnie S. Y. Kim and Olga Russakovsky", "title": "Fair Attribute Classification through Latent Space De-biasing", "comments": "Accepted to CVPR 2021, code can be found at\n  https://github.com/princetonvisualai/gan-debiasing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness in visual recognition is becoming a prominent and critical topic of\ndiscussion as recognition systems are deployed at scale in the real world.\nModels trained from data in which target labels are correlated with protected\nattributes (e.g., gender, race) are known to learn and exploit those\ncorrelations. In this work, we introduce a method for training accurate target\nclassifiers while mitigating biases that stem from these correlations. We use\nGANs to generate realistic-looking images, and perturb these images in the\nunderlying latent space to generate training data that is balanced for each\nprotected attribute. We augment the original dataset with this perturbed\ngenerated data, and empirically demonstrate that target classifiers trained on\nthe augmented dataset exhibit a number of both quantitative and qualitative\nbenefits. We conduct a thorough evaluation across multiple target labels and\nprotected attributes in the CelebA dataset, and provide an in-depth analysis\nand comparison to existing literature in the space.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 19:18:58 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 02:53:09 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2021 17:57:47 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Ramaswamy", "Vikram V.", ""], ["Kim", "Sunnie S. Y.", ""], ["Russakovsky", "Olga", ""]]}, {"id": "2012.01473", "submitter": "Tanvir Mahmud", "authors": "Tanvir Mahmud, Md Awsafur Rahman, Shaikh Anowarul Fattah, Sun-Yuan\n  Kung", "title": "CovSegNet: A Multi Encoder-Decoder Architecture for Improved Lesion\n  Segmentation of COVID-19 Chest CT Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic lung lesions segmentation of chest CT scans is considered a pivotal\nstage towards accurate diagnosis and severity measurement of COVID-19.\nTraditional U-shaped encoder-decoder architecture and its variants suffer from\ndiminutions of contextual information in pooling/upsampling operations with\nincreased semantic gaps among encoded and decoded feature maps as well as\ninstigate vanishing gradient problems for its sequential gradient propagation\nthat result in sub-optimal performance. Moreover, operating with 3D CT-volume\nposes further limitations due to the exponential increase of computational\ncomplexity making the optimization difficult. In this paper, an automated\nCOVID-19 lesion segmentation scheme is proposed utilizing a highly efficient\nneural network architecture, namely CovSegNet, to overcome these limitations.\nAdditionally, a two-phase training scheme is introduced where a deeper\n2D-network is employed for generating ROI-enhanced CT-volume followed by a\nshallower 3D-network for further enhancement with more contextual information\nwithout increasing computational burden. Along with the traditional vertical\nexpansion of Unet, we have introduced horizontal expansion with multi-stage\nencoder-decoder modules for achieving optimum performance. Additionally,\nmulti-scale feature maps are integrated into the scale transition process to\novercome the loss of contextual information. Moreover, a multi-scale fusion\nmodule is introduced with a pyramid fusion scheme to reduce the semantic gaps\nbetween subsequent encoder/decoder modules while facilitating the parallel\noptimization for efficient gradient propagation. Outstanding performances have\nbeen achieved in three publicly available datasets that largely outperform\nother state-of-the-art approaches. The proposed scheme can be easily extended\nfor achieving optimum segmentation performances in a wide variety of\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 19:26:35 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Mahmud", "Tanvir", ""], ["Rahman", "Md Awsafur", ""], ["Fattah", "Shaikh Anowarul", ""], ["Kung", "Sun-Yuan", ""]]}, {"id": "2012.01478", "submitter": "Jize Zhang", "authors": "Jize Zhang, Bhavya Kailkhura, T. Yong-Jin Han", "title": "Leveraging Uncertainty from Deep Learning for Trustworthy Materials\n  Discovery Workflows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.CV cs.LG physics.app-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we leverage predictive uncertainty of deep neural networks to\nanswer challenging questions material scientists usually encounter in machine\nlearning based materials applications workflows. First, we show that by\nleveraging predictive uncertainty, a user can determine the required training\ndata set size necessary to achieve a certain classification accuracy. Next, we\npropose uncertainty guided decision referral to detect and refrain from making\ndecisions on confusing samples. Finally, we show that predictive uncertainty\ncan also be used to detect out-of-distribution test samples. We find that this\nscheme is accurate enough to detect a wide range of real-world shifts in data,\ne.g., changes in the image acquisition conditions or changes in the synthesis\nconditions. Using microstructure information from scanning electron microscope\n(SEM) images as an example use case, we show that leveraging uncertainty-aware\ndeep learning can significantly improve the performance and dependability of\nclassification models.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 19:34:16 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 23:29:30 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Zhang", "Jize", ""], ["Kailkhura", "Bhavya", ""], ["Han", "T. Yong-Jin", ""]]}, {"id": "2012.01480", "submitter": "Yuhang Lu", "authors": "Yuhang Lu, Kang Zheng, Weijian Li, Yirui Wang, Adam P. Harrison,\n  Chihung Lin, Song Wang, Jing Xiao, Le Lu, Chang-Fu Kuo, Shun Miao", "title": "Contour Transformer Network for One-shot Segmentation of Anatomical\n  Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of anatomical structures is vital for medical image\nanalysis. The state-of-the-art accuracy is typically achieved by supervised\nlearning methods, where gathering the requisite expert-labeled image\nannotations in a scalable manner remains a main obstacle. Therefore,\nannotation-efficient methods that permit to produce accurate anatomical\nstructure segmentation are highly desirable. In this work, we present Contour\nTransformer Network (CTN), a one-shot anatomy segmentation method with a\nnaturally built-in human-in-the-loop mechanism. We formulate anatomy\nsegmentation as a contour evolution process and model the evolution behavior by\ngraph convolutional networks (GCNs). Training the CTN model requires only one\nlabeled image exemplar and leverages additional unlabeled data through newly\nintroduced loss functions that measure the global shape and appearance\nconsistency of contours. On segmentation tasks of four different anatomies, we\ndemonstrate that our one-shot learning method significantly outperforms\nnon-learning-based methods and performs competitively to the state-of-the-art\nfully supervised deep learning methods. With minimal human-in-the-loop editing\nfeedback, the segmentation performance can be further improved to surpass the\nfully supervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 19:42:18 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Lu", "Yuhang", ""], ["Zheng", "Kang", ""], ["Li", "Weijian", ""], ["Wang", "Yirui", ""], ["Harrison", "Adam P.", ""], ["Lin", "Chihung", ""], ["Wang", "Song", ""], ["Xiao", "Jing", ""], ["Lu", "Le", ""], ["Kuo", "Chang-Fu", ""], ["Miao", "Shun", ""]]}, {"id": "2012.01485", "submitter": "Rathziel Roncancio", "authors": "Rathziel Roncancio, Jupyoung Kim, Aly El Gamal and Jay P. Gore", "title": "Data-driven Analysis of Turbulent Flame Images", "comments": "AIAA Science and Technology Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Turbulent premixed flames are important for power generation using gas\nturbines. Improvements in characterization and understanding of turbulent\nflames continue particularly for transient events like ignition and extinction.\nPockets or islands of unburned material are features of turbulent flames during\nthese events. These features are directly linked to heat release rates and\nhydrocarbons emissions. Unburned material pockets in turbulent CH$_4$/air\npremixed flames with CO$_2$ addition were investigated using OH Planar\nLaser-Induced Fluorescence images. Convolutional Neural Networks (CNN) were\nused to classify images containing unburned pockets for three turbulent flames\nwith 0%, 5%, and 10% CO$_2$ addition. The CNN model was constructed using three\nconvolutional layers and two fully connected layers using dropout and weight\ndecay. The CNN model achieved accuracies of 91.72%, 89.35% and 85.80% for the\nthree flames, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 19:46:17 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Roncancio", "Rathziel", ""], ["Kim", "Jupyoung", ""], ["Gamal", "Aly El", ""], ["Gore", "Jay P.", ""]]}, {"id": "2012.01494", "submitter": "Minhas Kamal", "authors": "Minhas Kamal, Dr. Amin Ahsan Ali, Dr. Muhammad Asif Hossain Khan, Dr.\n  Mohammad Shoyaib", "title": "Braille to Text Translation for Bengali Language: A Geometric Approach", "comments": null, "journal-ref": "In Jahangirnagar University Journal of Information Technology\n  (JJIT), pp. 93-111, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Braille is the only system to visually impaired people for reading and\nwriting. However, general people cannot read Braille. So, teachers and\nrelatives find it hard to assist them with learning. Almost every major\nlanguage has software solutions for this translation purpose. However, in\nBengali there is an absence of this useful tool. Here, we propose Braille to\nText Translator, which takes image of these tactile alphabets, and translates\nthem to plain text. Image deterioration, scan-time page rotation, and braille\ndot deformation are the principal issues in this scheme. All of these\nchallenges are directly checked using special image processing and geometric\nstructure analysis. The technique yields 97.25% accuracy in recognizing Braille\ncharacters.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 19:57:29 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Kamal", "Minhas", ""], ["Ali", "Dr. Amin Ahsan", ""], ["Khan", "Dr. Muhammad Asif Hossain", ""], ["Shoyaib", "Dr. Mohammad", ""]]}, {"id": "2012.01506", "submitter": "Davis Wertheimer", "authors": "Davis Wertheimer, Luming Tang and Bharath Hariharan", "title": "Few-Shot Classification with Feature Map Reconstruction Networks", "comments": "Accepted to CVPR 2021. Updated to match most recent version. Code is\n  available at https://github.com/Tsingularity/FRN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we reformulate few-shot classification as a reconstruction\nproblem in latent space. The ability of the network to reconstruct a query\nfeature map from support features of a given class predicts membership of the\nquery in that class. We introduce a novel mechanism for few-shot classification\nby regressing directly from support features to query features in closed form,\nwithout introducing any new modules or large-scale learnable parameters. The\nresulting Feature Map Reconstruction Networks are both more performant and\ncomputationally efficient than previous approaches. We demonstrate consistent\nand substantial accuracy gains on four fine-grained benchmarks with varying\nneural architectures. Our model is also competitive on the non-fine-grained\nmini-ImageNet and tiered-ImageNet benchmarks with minimal bells and whistles.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 20:19:09 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 17:54:22 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Wertheimer", "Davis", ""], ["Tang", "Luming", ""], ["Hariharan", "Bharath", ""]]}, {"id": "2012.01511", "submitter": "Sina Honari", "authors": "Sina Honari, Victor Constantin, Helge Rhodin, Mathieu Salzmann, Pascal\n  Fua", "title": "Unsupervised Learning on Monocular Videos for 3D Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the presence of annotated data, deep human pose estimation networks yield\nimpressive performance. Nevertheless, annotating new data is extremely\ntime-consuming, particularly in real-world conditions. Here, we address this by\nleveraging contrastive self-supervised (CSS) learning to extract rich latent\nvectors from single-view videos. Instead of simply treating the latent features\nof nearby frames as positive pairs and those of temporally-distant ones as\nnegative pairs as in other CSS approaches, we explicitly disentangle each\nlatent vector into a time-variant component and a time-invariant one. We then\nshow that applying CSS only to the time-variant features, while also\nreconstructing the input and encouraging a gradual transition between nearby\nand away features, yields a rich latent space, well-suited for human pose\nestimation. Our approach outperforms other unsupervised single-view methods and\nmatches the performance of multi-view techniques.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 20:27:35 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 18:17:03 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Honari", "Sina", ""], ["Constantin", "Victor", ""], ["Rhodin", "Helge", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "2012.01526", "submitter": "Karttikeya Mangalam", "authors": "Karttikeya Mangalam, Yang An, Harshayu Girase, Jitendra Malik", "title": "From Goals, Waypoints & Paths To Long Term Human Trajectory Forecasting", "comments": "14 pages, 7 figures (including 2 GIFs)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human trajectory forecasting is an inherently multi-modal problem.\nUncertainty in future trajectories stems from two sources: (a) sources that are\nknown to the agent but unknown to the model, such as long term goals and\n(b)sources that are unknown to both the agent & the model, such as intent of\nother agents & irreducible randomness indecisions. We propose to factorize this\nuncertainty into its epistemic & aleatoric sources. We model the epistemic\nun-certainty through multimodality in long term goals and the aleatoric\nuncertainty through multimodality in waypoints& paths. To exemplify this\ndichotomy, we also propose a novel long term trajectory forecasting setting,\nwith prediction horizons upto a minute, an order of magnitude longer than prior\nworks. Finally, we presentY-net, a scene com-pliant trajectory forecasting\nnetwork that exploits the pro-posed epistemic & aleatoric structure for diverse\ntrajectory predictions across long prediction horizons.Y-net significantly\nimproves previous state-of-the-art performance on both (a) The well studied\nshort prediction horizon settings on the Stanford Drone & ETH/UCY datasets and\n(b) The proposed long prediction horizon setting on the re-purposed Stanford\nDrone & Intersection Drone datasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 21:01:29 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Mangalam", "Karttikeya", ""], ["An", "Yang", ""], ["Girase", "Harshayu", ""], ["Malik", "Jitendra", ""]]}, {"id": "2012.01541", "submitter": "Sobhan Soleymani", "authors": "Sobhan Soleymani, Baaria Chaudhary, Ali Dabouei, Jeremy Dawson, Nasser\n  M. Nasrabadi", "title": "Differential Morphed Face Detection Using Deep Siamese Networks", "comments": "MultiMedia FORensics in the WILD (MMForWILD 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although biometric facial recognition systems are fast becoming part of\nsecurity applications, these systems are still vulnerable to morphing attacks,\nin which a facial reference image can be verified as two or more separate\nidentities. In border control scenarios, a successful morphing attack allows\ntwo or more people to use the same passport to cross borders. In this paper, we\npropose a novel differential morph attack detection framework using a deep\nSiamese network. To the best of our knowledge, this is the first research work\nthat makes use of a Siamese network architecture for morph attack detection. We\ncompare our model with other classical and deep learning models using two\ndistinct morph datasets, VISAPP17 and MorGAN. We explore the embedding space\ngenerated by the contrastive loss using three decision making frameworks using\nEuclidean distance, feature difference and a support vector machine classifier,\nand feature concatenation and a support vector machine classifier.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 21:30:11 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 01:51:50 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Soleymani", "Sobhan", ""], ["Chaudhary", "Baaria", ""], ["Dabouei", "Ali", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "2012.01542", "submitter": "Sobhan Soleymani", "authors": "Sobhan Soleymani, Ali Dabouei, Fariborz Taherkhani, Jeremy Dawson,\n  Nasser M. Nasrabadi", "title": "Mutual Information Maximization on Disentangled Representations for\n  Differential Morph Detection", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a novel differential morph detection framework,\nutilizing landmark and appearance disentanglement. In our framework, the face\nimage is represented in the embedding domain using two disentangled but\ncomplementary representations. The network is trained by triplets of face\nimages, in which the intermediate image inherits the landmarks from one image\nand the appearance from the other image. This initially trained network is\nfurther trained for each dataset using contrastive representations. We\ndemonstrate that, by employing appearance and landmark disentanglement, the\nproposed framework can provide state-of-the-art differential morph detection\nperformance. This functionality is achieved by the using distances in landmark,\nappearance, and ID domains. The performance of the proposed framework is\nevaluated using three morph datasets generated with different methodologies.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 21:31:02 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Soleymani", "Sobhan", ""], ["Dabouei", "Ali", ""], ["Taherkhani", "Fariborz", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "2012.01558", "submitter": "Nikhil Kapoor", "authors": "Nikhil Kapoor, Andreas B\\\"ar, Serin Varghese, Jan David Schneider,\n  Fabian H\\\"uger, Peter Schlicht, Tim Fingscheidt", "title": "From a Fourier-Domain Perspective on Adversarial Examples to a Wiener\n  Filter Defense for Semantic Segmentation", "comments": "Accepted by The International Joint Conference on Neural Network\n  (IJCNN) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advancements, deep neural networks are not robust against\nadversarial perturbations. Many of the proposed adversarial defense approaches\nuse computationally expensive training mechanisms that do not scale to complex\nreal-world tasks such as semantic segmentation, and offer only marginal\nimprovements. In addition, fundamental questions on the nature of adversarial\nperturbations and their relation to the network architecture are largely\nunderstudied. In this work, we study the adversarial problem from a frequency\ndomain perspective. More specifically, we analyze discrete Fourier transform\n(DFT) spectra of several adversarial images and report two major findings:\nFirst, there exists a strong connection between a model architecture and the\nnature of adversarial perturbations that can be observed and addressed in the\nfrequency domain. Second, the observed frequency patterns are largely image-\nand attack-type independent, which is important for the practical impact of any\ndefense making use of such patterns. Motivated by these findings, we\nadditionally propose an adversarial defense method based on the well-known\nWiener filters that captures and suppresses adversarial frequencies in a\ndata-driven manner. Our proposed method not only generalizes across unseen\nattacks but also beats five existing state-of-the-art methods across two models\nin a variety of attack settings.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 22:06:04 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 15:44:10 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Kapoor", "Nikhil", ""], ["B\u00e4r", "Andreas", ""], ["Varghese", "Serin", ""], ["Schneider", "Jan David", ""], ["H\u00fcger", "Fabian", ""], ["Schlicht", "Peter", ""], ["Fingscheidt", "Tim", ""]]}, {"id": "2012.01591", "submitter": "Zhenzhen Weng", "authors": "Zhenzhen Weng, Serena Yeung", "title": "Holistic 3D Human and Scene Mesh Estimation from Single View Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3D world limits the human body pose and the human body pose conveys\ninformation about the surrounding objects. Indeed, from a single image of a\nperson placed in an indoor scene, we as humans are adept at resolving\nambiguities of the human pose and room layout through our knowledge of the\nphysical laws and prior perception of the plausible object and human poses.\nHowever, few computer vision models fully leverage this fact. In this work, we\npropose an end-to-end trainable model that perceives the 3D scene from a single\nRGB image, estimates the camera pose and the room layout, and reconstructs both\nhuman body and object meshes. By imposing a set of comprehensive and\nsophisticated losses on all aspects of the estimations, we show that our model\noutperforms existing human body mesh methods and indoor scene reconstruction\nmethods. To the best of our knowledge, this is the first model that outputs\nboth object and human predictions at the mesh level, and performs joint\noptimization on the scene and human poses.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 23:22:03 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 17:30:41 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Weng", "Zhenzhen", ""], ["Yeung", "Serena", ""]]}, {"id": "2012.01604", "submitter": "Vinu Joseph", "authors": "Vinu Joseph, Shoaib Ahmed Siddiqui, Aditya Bhaskara, Ganesh\n  Gopalakrishnan, Saurav Muralidharan, Michael Garland, Sheraz Ahmed, Andreas\n  Dengel", "title": "Going Beyond Classification Accuracy Metrics in Model Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise in edge-computing devices, there has been an increasing demand\nto deploy energy and resource-efficient models. A large body of research has\nbeen devoted to developing methods that can reduce the size of the model\nconsiderably without affecting the standard metrics such as top-1 accuracy.\nHowever, these pruning approaches tend to result in a significant mismatch in\nother metrics such as fairness across classes and explainability. To combat\nsuch misalignment, we propose a novel multi-part loss function inspired by the\nknowledge-distillation literature. Through extensive experiments, we\ndemonstrate the effectiveness of our approach across different compression\nalgorithms, architectures, tasks as well as datasets. In particular, we obtain\nup to $4.1\\times$ reduction in the number of prediction mismatches between the\ncompressed and reference models, and up to $5.7\\times$ in cases where the\nreference model makes the correct prediction; all while making no changes to\nthe compression algorithm, and minor modifications to the loss function.\nFurthermore, we demonstrate how inducing simple alignment between the\npredictions of the models naturally improves the alignment on other metrics\nincluding fairness and attributions. Our framework can thus serve as a simple\nplug-and-play component for compression algorithms in the future.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 00:00:41 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 20:10:09 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Joseph", "Vinu", ""], ["Siddiqui", "Shoaib Ahmed", ""], ["Bhaskara", "Aditya", ""], ["Gopalakrishnan", "Ganesh", ""], ["Muralidharan", "Saurav", ""], ["Garland", "Michael", ""], ["Ahmed", "Sheraz", ""], ["Dengel", "Andreas", ""]]}, {"id": "2012.01632", "submitter": "Sukjun Hwang", "authors": "Sukjun Hwang, Seoung Wug Oh, Seon Joo Kim", "title": "Single-shot Path Integrated Panoptic Segmentation", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoptic segmentation, which is a novel task of unifying instance\nsegmentation and semantic segmentation, has attracted a lot of attention\nlately. However, most of the previous methods are composed of multiple pathways\nwith each pathway specialized to a designated segmentation task. In this paper,\nwe propose to resolve panoptic segmentation in single-shot by integrating the\nexecution flows. With the integrated pathway, a unified feature map called\nPanoptic-Feature is generated, which includes the information of both things\nand stuffs. Panoptic-Feature becomes more sophisticated by auxiliary problems\nthat guide to cluster pixels that belong to the same instance and differentiate\nbetween objects of different classes. A collection of convolutional filters,\nwhere each filter represents either a thing or stuff, is applied to\nPanoptic-Feature at once, materializing the single-shot panoptic segmentation.\nTaking the advantages of both top-down and bottom-up approaches, our method,\nnamed SPINet, enjoys high efficiency and accuracy on major panoptic\nsegmentation benchmarks: COCO and Cityscapes.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 01:50:30 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 02:34:32 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Hwang", "Sukjun", ""], ["Oh", "Seoung Wug", ""], ["Kim", "Seon Joo", ""]]}, {"id": "2012.01634", "submitter": "Ankit Goyal", "authors": "Ankit Goyal, Kaiyu Yang, Dawei Yang, Jia Deng", "title": "Rel3D: A Minimally Contrastive Benchmark for Grounding Spatial Relations\n  in 3D", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding spatial relations (e.g., \"laptop on table\") in visual input is\nimportant for both humans and robots. Existing datasets are insufficient as\nthey lack large-scale, high-quality 3D ground truth information, which is\ncritical for learning spatial relations. In this paper, we fill this gap by\nconstructing Rel3D: the first large-scale, human-annotated dataset for\ngrounding spatial relations in 3D. Rel3D enables quantifying the effectiveness\nof 3D information in predicting spatial relations on large-scale human data.\nMoreover, we propose minimally contrastive data collection -- a novel\ncrowdsourcing method for reducing dataset bias. The 3D scenes in our dataset\ncome in minimally contrastive pairs: two scenes in a pair are almost identical,\nbut a spatial relation holds in one and fails in the other. We empirically\nvalidate that minimally contrastive examples can diagnose issues with current\nrelation detection models as well as lead to sample-efficient training. Code\nand data are available at https://github.com/princeton-vl/Rel3D.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 01:51:56 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Goyal", "Ankit", ""], ["Yang", "Kaiyu", ""], ["Yang", "Dawei", ""], ["Deng", "Jia", ""]]}, {"id": "2012.01641", "submitter": "Lei Zhang", "authors": "Lei Zhang, Fei Zhou, Wei Wei and Yanning Zhang", "title": "Meta-Generating Deep Attentive Metric for Few-shot Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to generate a task-aware base learner proves a promising direction\nto deal with few-shot learning (FSL) problem. Existing methods mainly focus on\ngenerating an embedding model utilized with a fixed metric (eg, cosine\ndistance) for nearest neighbour classification or directly generating a linear\nclassier. However, due to the limited discriminative capacity of such a simple\nmetric or classifier, these methods fail to generalize to challenging cases\nappropriately. To mitigate this problem, we present a novel deep metric\nmeta-generation method that turns to an orthogonal direction, ie, learning to\nadaptively generate a specific metric for a new FSL task based on the task\ndescription (eg, a few labelled samples). In this study, we structure the\nmetric using a three-layer deep attentive network that is flexible enough to\nproduce a discriminative metric for each task. Moreover, different from\nexisting methods that utilize an uni-modal weight distribution conditioned on\nlabelled samples for network generation, the proposed meta-learner establishes\na multi-modal weight distribution conditioned on cross-class sample pairs using\na tailored variational autoencoder, which can separately capture the specific\ninter-class discrepancy statistics for each class and jointly embed the\nstatistics for all classes into metric generation. By doing this, the generated\nmetric can be appropriately adapted to a new FSL task with pleasing\ngeneralization performance. To demonstrate this, we test the proposed method on\nfour benchmark FSL datasets and gain surprisingly obvious performance\nimprovement over state-of-the-art competitors, especially in the challenging\ncases, eg, improve the accuracy from 26.14% to 46.69% in the 20-way 1-shot task\non miniImageNet, while improve the accuracy from 45.2% to 68.72% in the 5-way\n1-shot task on FC100. Code is available: https://github.com/NWPUZhoufei/DAM.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:07:43 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Zhang", "Lei", ""], ["Zhou", "Fei", ""], ["Wei", "Wei", ""], ["Zhang", "Yanning", ""]]}, {"id": "2012.01642", "submitter": "Chris Thomas", "authors": "Christopher Thomas, Yale Song, Adriana Kovashka", "title": "Learning to Transfer Visual Effects from Videos to Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of animating images by transferring spatio-temporal\nvisual effects (such as melting) from a collection of videos. We tackle two\nprimary challenges in visual effect transfer: 1) how to capture the effect we\nwish to distill; and 2) how to ensure that only the effect, rather than content\nor artistic style, is transferred from the source videos to the input image. To\naddress the first challenge, we evaluate five loss functions; the most\npromising one encourages the generated animations to have similar optical flow\nand texture motions as the source videos. To address the second challenge, we\nonly allow our model to move existing image pixels from the previous frame,\nrather than predicting unconstrained pixel values. This forces any visual\neffects to occur using the input image's pixels, preventing unwanted artistic\nstyle or content from the source video from appearing in the output. We\nevaluate our method in objective and subjective settings, and show interesting\nqualitative results which demonstrate objects undergoing atypical\ntransformations, such as making a face melt or a deer bloom.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:10:14 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 17:32:50 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Thomas", "Christopher", ""], ["Song", "Yale", ""], ["Kovashka", "Adriana", ""]]}, {"id": "2012.01644", "submitter": "Joy Hsu", "authors": "Joy Hsu, Jeffrey Gu, Gong-Her Wu, Wah Chiu, Serena Yeung", "title": "Learning Hyperbolic Representations for Unsupervised 3D Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists a need for unsupervised 3D segmentation on complex volumetric\ndata, particularly when annotation ability is limited or discovery of new\ncategories is desired. Using the observation that much of 3D volumetric data is\ninnately hierarchical, we propose learning effective representations of 3D\npatches for unsupervised segmentation through a variational autoencoder (VAE)\nwith a hyperbolic latent space and a proposed gyroplane convolutional layer,\nwhich better models the underlying hierarchical structure within a 3D image. We\nalso introduce a hierarchical triplet loss and multi-scale patch sampling\nscheme to embed relationships across varying levels of granularity. We\ndemonstrate the effectiveness of our hyperbolic representations for\nunsupervised 3D segmentation on a hierarchical toy dataset, BraTS whole tumor\ndataset, and cryogenic electron microscopy data.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:15:31 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 23:28:46 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Hsu", "Joy", ""], ["Gu", "Jeffrey", ""], ["Wu", "Gong-Her", ""], ["Chiu", "Wah", ""], ["Yeung", "Serena", ""]]}, {"id": "2012.01654", "submitter": "Aishan Liu", "authors": "Aishan Liu, Shiyu Tang, Xianglong Liu, Xinyun Chen, Lei Huang,\n  Zhuozhuo Tu, Dawn Song, Dacheng Tao", "title": "Towards Defending Multiple Adversarial Perturbations via Gated Batch\n  Normalization", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is now extensive evidence demonstrating that deep neural networks are\nvulnerable to adversarial examples, motivating the development of defenses\nagainst adversarial attacks. However, existing adversarial defenses typically\nimprove model robustness against individual specific perturbation types. Some\nrecent methods improve model robustness against adversarial attacks in multiple\n$\\ell_p$ balls, but their performance against each perturbation type is still\nfar from satisfactory. To better understand this phenomenon, we propose the\n\\emph{multi-domain} hypothesis, stating that different types of adversarial\nperturbations are drawn from different domains. Guided by the multi-domain\nhypothesis, we propose \\emph{Gated Batch Normalization (GBN)}, a novel building\nblock for deep neural networks that improves robustness against multiple\nperturbation types. GBN consists of a gated sub-network and a multi-branch\nbatch normalization (BN) layer, where the gated sub-network separates different\nperturbation types, and each BN branch is in charge of a single perturbation\ntype and learns domain-specific statistics for input transformation. Then,\nfeatures from different branches are aligned as domain-invariant\nrepresentations for the subsequent layers. We perform extensive evaluations of\nour approach on MNIST, CIFAR-10, and Tiny-ImageNet, and demonstrate that GBN\noutperforms previous defense proposals against multiple perturbation types,\ni.e, $\\ell_1$, $\\ell_2$, and $\\ell_{\\infty}$ perturbations, by large margins of\n10-20\\%.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:26:01 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Liu", "Aishan", ""], ["Tang", "Shiyu", ""], ["Liu", "Xianglong", ""], ["Chen", "Xinyun", ""], ["Huang", "Lei", ""], ["Tu", "Zhuozhuo", ""], ["Song", "Dawn", ""], ["Tao", "Dacheng", ""]]}, {"id": "2012.01665", "submitter": "Liu Qing", "authors": "Qing Liu, Haotian Liu, Yixiong Liang", "title": "Dual-Branch Network with Dual-Sampling Modulated Dice Loss for Hard\n  Exudate Segmentation from Colour Fundus Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated segmentation of hard exudates in colour fundus images is a\nchallenge task due to issues of extreme class imbalance and enormous size\nvariation. This paper aims to tackle these issues and proposes a dual-branch\nnetwork with dual-sampling modulated Dice loss. It consists of two branches:\nlarge hard exudate biased learning branch and small hard exudate biased\nlearning branch. Both of them are responsible for their own duty separately.\nFurthermore, we propose a dual-sampling modulated Dice loss for the training\nsuch that our proposed dual-branch network is able to segment hard exudates in\ndifferent sizes. In detail, for the first branch, we use a uniform sampler to\nsample pixels from predicted segmentation mask for Dice loss calculation, which\nleads to this branch naturally be biased in favour of large hard exudates as\nDice loss generates larger cost on misidentification of large hard exudates\nthan small hard exudates. For the second branch, we use a re-balanced sampler\nto oversample hard exudate pixels and undersample background pixels for loss\ncalculation. In this way, cost on misidentification of small hard exudates is\nenlarged, which enforces the parameters in the second branch fit small hard\nexudates well. Considering that large hard exudates are much easier to be\ncorrectly identified than small hard exudates, we propose an easy-to-difficult\nlearning strategy by adaptively modulating the losses of two branches. We\nevaluate our proposed method on two public datasets and results demonstrate\nthat ours achieves state-of-the-art performances.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:37:13 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 06:59:31 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Liu", "Qing", ""], ["Liu", "Haotian", ""], ["Liang", "Yixiong", ""]]}, {"id": "2012.01674", "submitter": "Jindong Gu", "authors": "Jindong Gu and Volker Tresp", "title": "Interpretable Graph Capsule Networks for Object Recognition", "comments": null, "journal-ref": "The Thirty-Fifth AAAI Conference on Artificial Intelligence\n  (AAAI), 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule Networks, as alternatives to Convolutional Neural Networks, have been\nproposed to recognize objects from images. The current literature demonstrates\nmany advantages of CapsNets over CNNs. However, how to create explanations for\nindividual classifications of CapsNets has not been well explored. The widely\nused saliency methods are mainly proposed for explaining CNN-based\nclassifications; they create saliency map explanations by combining activation\nvalues and the corresponding gradients, e.g., Grad-CAM. These saliency methods\nrequire a specific architecture of the underlying classifiers and cannot be\ntrivially applied to CapsNets due to the iterative routing mechanism therein.\nTo overcome the lack of interpretability, we can either propose new post-hoc\ninterpretation methods for CapsNets or modifying the model to have build-in\nexplanations. In this work, we explore the latter. Specifically, we propose\ninterpretable Graph Capsule Networks (GraCapsNets), where we replace the\nrouting part with a multi-head attention-based Graph Pooling approach. In the\nproposed model, individual classification explanations can be created\neffectively and efficiently. Our model also demonstrates some unexpected\nbenefits, even though it replaces the fundamental part of CapsNets. Our\nGraCapsNets achieve better classification performance with fewer parameters and\nbetter adversarial robustness, when compared to CapsNets. Besides, GraCapsNets\nalso keep other advantages of CapsNets, namely, disentangled representations\nand affine transformation robustness.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 03:18:00 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 16:04:21 GMT"}, {"version": "v3", "created": "Sun, 7 Mar 2021 16:50:54 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Gu", "Jindong", ""], ["Tresp", "Volker", ""]]}, {"id": "2012.01690", "submitter": "Heming Yao", "authors": "Heming Yao, Ryan W. Stidham, Zijun Gao, Jonathan Gryak, Kayvan\n  Najarian", "title": "Motion-based Camera Localization System in Colonoscopy Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical colonoscopy is an essential diagnostic and prognostic tool for many\ngastrointestinal diseases, including cancer screening and staging, intestinal\nbleeding, diarrhea, abdominal symptom evaluation, and inflammatory bowel\ndisease assessment. Automated assessment of colonoscopy is of interest\nconsidering the subjectivity present in qualitative human interpretations of\ncolonoscopy findings. Localization of the camera is essential to interpreting\nthe meaning and context of findings for diseases evaluated by colonoscopy. In\nthis study, we propose a camera localization system to estimate the relative\nlocation of the camera and classify the colon into anatomical segments. The\ncamera localization system begins with non-informative frame detection and\nremoval. Then a self-training end-to-end convolutional neural network is built\nto estimate the camera motion, where several strategies are proposed to improve\nits robustness and generalization on endoscopic videos. Using the estimated\ncamera motion a camera trajectory can be derived and a relative location index\ncalculated. Based on the estimated location index, anatomical colon segment\nclassification is performed by constructing a colon template. The proposed\nmotion estimation algorithm was evaluated on an external dataset containing the\nground truth for camera pose. The experimental results show that the\nperformance of the proposed method is superior to other published methods. The\nrelative location index estimation and anatomical region classification were\nfurther validated using colonoscopy videos collected from routine clinical\npractice. This validation yielded an average accuracy in classification of\n0.754, which is substantially higher than the performances obtained using\nlocation indices built from other methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 03:57:12 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 02:59:32 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2021 20:09:22 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Yao", "Heming", ""], ["Stidham", "Ryan W.", ""], ["Gao", "Zijun", ""], ["Gryak", "Jonathan", ""], ["Najarian", "Kayvan", ""]]}, {"id": "2012.01693", "submitter": "Mohit Sharma", "authors": "Mohit Sharma, Oliver Kroemer", "title": "Relational Learning for Skill Preconditions", "comments": "CoRL'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To determine if a skill can be executed in any given environment, a robot\nneeds to learn the preconditions for the skill. As robots begin to operate in\ndynamic and unstructured environments, precondition models will need to\ngeneralize to variable number of objects with different shapes and sizes. In\nthis work, we focus on learning precondition models for manipulation skills in\nunconstrained environments. Our work is motivated by the intuition that many\ncomplex manipulation tasks, with multiple objects, can be simplified by\nfocusing on less complex pairwise object relations. We propose an\nobject-relation model that learns continuous representations for these pairwise\nobject relations. Our object-relation model is trained completely in\nsimulation, and once learned, is used by a separate precondition model to\npredict skill preconditions for real world tasks. We evaluate our precondition\nmodel on $3$ different manipulation tasks: sweeping, cutting, and unstacking.\nWe show that our approach leads to significant improvements in predicting\npreconditions for all 3 tasks, across objects of different shapes and sizes.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 04:13:49 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Sharma", "Mohit", ""], ["Kroemer", "Oliver", ""]]}, {"id": "2012.01699", "submitter": "Ryan Feng", "authors": "Ryan Feng, Wu-chi Feng, Atul Prakash", "title": "Essential Features: Reducing the Attack Surface of Adversarial\n  Perturbations with Robust Content-Aware Image Preprocessing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversaries are capable of adding perturbations to an image to fool machine\nlearning models into incorrect predictions. One approach to defending against\nsuch perturbations is to apply image preprocessing functions to remove the\neffects of the perturbation. Existing approaches tend to be designed\northogonally to the content of the image and can be beaten by adaptive attacks.\nWe propose a novel image preprocessing technique called Essential Features that\ntransforms the image into a robust feature space that preserves the main\ncontent of the image while significantly reducing the effects of the\nperturbations. Specifically, an adaptive blurring strategy that preserves the\nmain edge features of the original object along with a k-means color reduction\napproach is employed to simplify the image to its k most representative colors.\nThis approach significantly limits the attack surface for adversaries by\nlimiting the ability to adjust colors while preserving pertinent features of\nthe original image. We additionally design several adaptive attacks and find\nthat our approach remains more robust than previous baselines. On CIFAR-10 we\nachieve 64% robustness and 58.13% robustness on RESISC45, raising robustness by\nover 10% versus state-of-the-art adversarial training techniques against\nadaptive white-box and black-box attacks. The results suggest that strategies\nthat retain essential features in images by adaptive processing of the content\nhold promise as a complement to adversarial training for boosting robustness\nagainst adversarial inputs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 04:40:51 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Feng", "Ryan", ""], ["Feng", "Wu-chi", ""], ["Prakash", "Atul", ""]]}, {"id": "2012.01700", "submitter": "Seunghan Yang", "authors": "Seunghan Yang, Hyoungseob Park, Junyoung Byun, Changick Kim", "title": "Robust Federated Learning with Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a paradigm that enables local devices to jointly train\na server model while keeping the data decentralized and private. In federated\nlearning, since local data are collected by clients, it is hardly guaranteed\nthat the data are correctly annotated. Although a lot of studies have been\nconducted to train the networks robust to these noisy data in a centralized\nsetting, these algorithms still suffer from noisy labels in federated learning.\nCompared to the centralized setting, clients' data can have different noise\ndistributions due to variations in their labeling systems or background\nknowledge of users. As a result, local models form inconsistent decision\nboundaries and their weights severely diverge from each other, which are\nserious problems in federated learning. To solve these problems, we introduce a\nnovel federated learning scheme that the server cooperates with local models to\nmaintain consistent decision boundaries by interchanging class-wise centroids.\nThese centroids are central features of local data on each device, which are\naligned by the server every communication round. Updating local models with the\naligned centroids helps to form consistent decision boundaries among local\nmodels, although the noise distributions in clients' data are different from\neach other. To improve local model performance, we introduce a novel approach\nto select confident samples that are used for updating the model with given\nlabels. Furthermore, we propose a global-guided pseudo-labeling method to\nupdate labels of unconfident samples by exploiting the global model. Our\nexperimental results on the noisy CIFAR-10 dataset and the Clothing1M dataset\nshow that our approach is noticeably effective in federated learning with noisy\nlabels.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 04:46:07 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Yang", "Seunghan", ""], ["Park", "Hyoungseob", ""], ["Byun", "Junyoung", ""], ["Kim", "Changick", ""]]}, {"id": "2012.01714", "submitter": "David Lindell", "authors": "David B. Lindell, Julien N. P. Martel, Gordon Wetzstein", "title": "AutoInt: Automatic Integration for Fast Neural Volume Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical integration is a foundational technique in scientific computing and\nis at the core of many computer vision applications. Among these applications,\nneural volume rendering has recently been proposed as a new paradigm for view\nsynthesis, achieving photorealistic image quality. However, a fundamental\nobstacle to making these methods practical is the extreme computational and\nmemory requirements caused by the required volume integrations along the\nrendered rays during training and inference. Millions of rays, each requiring\nhundreds of forward passes through a neural network are needed to approximate\nthose integrations with Monte Carlo sampling. Here, we propose automatic\nintegration, a new framework for learning efficient, closed-form solutions to\nintegrals using coordinate-based neural networks. For training, we instantiate\nthe computational graph corresponding to the derivative of the network. The\ngraph is fitted to the signal to integrate. After optimization, we reassemble\nthe graph to obtain a network that represents the antiderivative. By the\nfundamental theorem of calculus, this enables the calculation of any definite\nintegral in two evaluations of the network. Applying this approach to neural\nrendering, we improve a tradeoff between rendering speed and image quality:\nimproving render times by greater than 10 times with a tradeoff of slightly\nreduced image quality.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 05:46:10 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 02:23:47 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Lindell", "David B.", ""], ["Martel", "Julien N. P.", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2012.01724", "submitter": "Ping-Yang Chen", "authors": "Ping-Yang Chen, Ming-Ching Chang, Jun-Wei Hsieh, Yong-Sheng Chen", "title": "Parallel Residual Bi-Fusion Feature Pyramid Network for Accurate\n  Single-Shot Object Detection", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Parallel Residual Bi-Fusion Feature Pyramid Network (PRB-FPN)\nfor fast and accurate single-shot object detection. Feature Pyramid (FP) is\nwidely used in recent visual detection, however the top-down pathway of FP\ncannot preserve accurate localization due to pooling shifting. The advantage of\nFP is weaken as deeper backbones with more layers are used. To address this\nissue, we propose a new parallel FP structure with bi-directional (top-down and\nbottom-up) fusion and associated improvements to retain high-quality features\nfor accurate localization. Our method is particularly suitable for detecting\nsmall objects. We provide the following design improvements: (1) A parallel\nbifusion FP structure with a Bottom-up Fusion Module (BFM) to detect both small\nand large objects at once with high accuracy. (2) A COncatenation and\nRE-organization (CORE) module provides a bottom-up pathway for feature fusion,\nwhich leads to the bi-directional fusion FP that can recover lost information\nfrom lower-layer feature maps. (3) The CORE feature is further purified to\nretain richer contextual information. Such purification is performed with CORE\nin a few iterations in both top-down and bottom-up pathways. (4) The adding of\na residual design to CORE leads to a new Re-CORE module that enables easy\ntraining and integration with a wide range of (deeper or lighter) backbones.\nThe proposed network achieves state-of-the-art performance on UAVDT17 and MS\nCOCO datasets.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 06:51:20 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Chen", "Ping-Yang", ""], ["Chang", "Ming-Ching", ""], ["Hsieh", "Jun-Wei", ""], ["Chen", "Yong-Sheng", ""]]}, {"id": "2012.01733", "submitter": "Jialiang Ma", "authors": "Jialiang Ma, Bin Chen", "title": "Dual Refinement Feature Pyramid Networks for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FPN is a common component used in object detectors, it supplements\nmulti-scale information by adjacent level features interpolation and summation.\nHowever, due to the existence of nonlinear operations and the convolutional\nlayers with different output dimensions, the relationship between different\nlevels is much more complex, the pixel-wise summation is not an efficient\napproach. In this paper, we first analyze the design defects from pixel level\nand feature map level. Then, we design a novel parameter-free feature pyramid\nnetworks named Dual Refinement Feature Pyramid Networks (DRFPN) for the\nproblems. Specifically, DRFPN consists of two modules: Spatial Refinement Block\n(SRB) and Channel Refinement Block (CRB). SRB learns the location and content\nof sampling points based on contextual information between adjacent levels. CRB\nlearns an adaptive channel merging method based on attention mechanism. Our\nproposed DRFPN can be easily plugged into existing FPN-based models. Without\nbells and whistles, for two-stage detectors, our model outperforms different\nFPN-based counterparts by 1.6 to 2.2 AP on the COCO detection benchmark, and\n1.5 to 1.9 AP on the COCO segmentation benchmark. For one-stage detectors,\nDRFPN improves anchor-based RetinaNet by 1.9 AP and anchor-free FCOS by 1.3 AP\nwhen using ResNet50 as backbone. Extensive experiments verifies the robustness\nand generalization ability of DRFPN. The code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 07:17:03 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 02:57:40 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Ma", "Jialiang", ""], ["Chen", "Bin", ""]]}, {"id": "2012.01740", "submitter": "Eli Shlizerman", "authors": "Jingyuan Li and Eli Shlizerman", "title": "Sparse Semi-Supervised Action Recognition with Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art methods for skeleton-based action recognition are\nsupervised and rely on labels. The reliance is limiting the performance due to\nthe challenges involved in annotation and mislabeled data. Unsupervised methods\nhave been introduced, however, they organize sequences into clusters and still\nrequire labels to associate clusters with actions. In this paper, we propose a\nnovel approach for skeleton-based action recognition, called SESAR, that\nconnects these approaches. SESAR leverages the information from both unlabeled\ndata and a handful of sequences actively selected for labeling, combining\nunsupervised training with sparsely supervised guidance. SESAR is composed of\ntwo main components, where the first component learns a latent representation\nfor unlabeled action sequences through an Encoder-Decoder RNN which\nreconstructs the sequences, and the second component performs active learning\nto select sequences to be labeled based on cluster and classification\nuncertainty. When the two components are simultaneously trained on\nskeleton-based action sequences, they correspond to a robust system for action\nrecognition with only a handful of labeled samples. We evaluate our system on\ncommon datasets with multiple sequences and actions, such as NW UCLA, NTU RGB+D\n60, and UWA3D. Our results outperform standalone skeleton-based supervised,\nunsupervised with cluster identification, and active-learning methods for\naction recognition when applied to sparse labeled samples, as low as 1% of the\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 07:48:31 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 20:28:54 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Li", "Jingyuan", ""], ["Shlizerman", "Eli", ""]]}, {"id": "2012.01743", "submitter": "Kumar Ashutosh", "authors": "Kumar Ashutosh, Saurabh Kumar, Subhasis Chaudhuri", "title": "3D-NVS: A 3D Supervision Approach for Next View Selection", "comments": "Submitted to CVPR-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a classification based approach for the next best view selection\nand show how we can plausibly obtain a supervisory signal for this task. The\nproposed approach is end-to-end trainable and aims to get the best possible 3D\nreconstruction quality with a pair of passively acquired 2D views. The proposed\nmodel consists of two stages: a classifier and a reconstructor network trained\njointly via the indirect 3D supervision from ground truth voxels. While\ntesting, the proposed method assumes no prior knowledge of the underlying 3D\nshape for selecting the next best view. We demonstrate the proposed method's\neffectiveness via detailed experiments on synthetic and real images and show\nhow it provides improved reconstruction quality than the existing state of the\nart 3D reconstruction and the next best view prediction techniques.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 07:50:16 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Ashutosh", "Kumar", ""], ["Kumar", "Saurabh", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "2012.01745", "submitter": "Jiangtao Nie", "authors": "Jiangtao Nie, Lei Zhang, Wei Wei, Zhiqiang Lang, Yanning Zhang", "title": "Unsupervised Alternating Optimization for Blind Hyperspectral Imagery\n  Super-resolution", "comments": "14 page, 13 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Despite the great success of deep model on Hyperspectral imagery (HSI)\nsuper-resolution(SR) for simulated data, most of them function unsatisfactory\nwhen applied to the real data, especially for unsupervised HSI SR methods. One\nof the main reason comes from the fact that the predefined degeneration models\n(e.g. blur in spatial domain) utilized by most HSI SR methods often exist great\ndiscrepancy with the real one, which results in these deep models overfit and\nultimately degrade their performance on real data. To well mitigate such a\nproblem, we explore the unsupervised blind HSI SR method. Specifically, we\ninvestigate how to effectively obtain the degeneration models in spatial and\nspectral domain, respectively, and makes them can well compatible with the\nfusion based SR reconstruction model. To this end, we first propose an\nalternating optimization based deep framework to estimate the degeneration\nmodels and reconstruct the latent image, with which the degeneration models\nestimation and HSI reconstruction can mutually promotes each other. Then, a\nmeta-learning based mechanism is further proposed to pre-train the network,\nwhich can effectively improve the speed and generalization ability adapting to\ndifferent complex degeneration. Experiments on three benchmark HSI SR datasets\nreport an excellent superiority of the proposed method on handling blind HSI\nfusion problem over other competing methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 07:52:32 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Nie", "Jiangtao", ""], ["Zhang", "Lei", ""], ["Wei", "Wei", ""], ["Lang", "Zhiqiang", ""], ["Zhang", "Yanning", ""]]}, {"id": "2012.01750", "submitter": "Sahil Singla", "authors": "Sahil Singla, Besmira Nushi, Shital Shah, Ece Kamar, Eric Horvitz", "title": "Understanding Failures of Deep Networks via Robust Feature Extraction", "comments": "Accepted at CVPR, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional evaluation metrics for learned models that report aggregate\nscores over a test set are insufficient for surfacing important and informative\npatterns of failure over features and instances. We introduce and study a\nmethod aimed at characterizing and explaining failures by identifying visual\nattributes whose presence or absence results in poor performance. In\ndistinction to previous work that relies upon crowdsourced labels for visual\nattributes, we leverage the representation of a separate robust model to\nextract interpretable features and then harness these features to identify\nfailure modes. We further propose a visualization method aimed at enabling\nhumans to understand the meaning encoded in such features and we test the\ncomprehensibility of the features. An evaluation of the methods on the ImageNet\ndataset demonstrates that: (i) the proposed workflow is effective for\ndiscovering important failure modes, (ii) the visualization techniques help\nhumans to understand the extracted features, and (iii) the extracted insights\ncan assist engineers with error analysis and debugging.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 08:33:29 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 12:44:02 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 07:15:51 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Singla", "Sahil", ""], ["Nushi", "Besmira", ""], ["Shah", "Shital", ""], ["Kamar", "Ece", ""], ["Horvitz", "Eric", ""]]}, {"id": "2012.01757", "submitter": "Khaled Saleh", "authors": "Khaled Saleh", "title": "Pedestrian Trajectory Prediction using Context-Augmented Transformer\n  Networks", "comments": "This work has been submitted to the 2021 IEEE International\n  Conference on Robotics and Automation (ICRA) with the Robotics and Automation\n  Letters (RA-L) option for possible publication. Copyright may be transferred\n  without notice, after which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Forecasting the trajectory of pedestrians in shared urban traffic\nenvironments is still considered one of the challenging problems facing the\ndevelopment of autonomous vehicles (AVs). In the literature, this problem is\noften tackled using recurrent neural networks (RNNs). Despite the powerful\ncapabilities of RNNs in capturing the temporal dependency in the pedestrians'\nmotion trajectories, they were argued to be challenged when dealing with longer\nsequential data. Thus, in this work, we are introducing a framework based on\nthe transformer networks that were shown recently to be more efficient and\noutperformed RNNs in many sequential-based tasks. We relied on a fusion of the\npast positional information, agent interactions information and scene physical\nsemantics information as an input to our framework in order to provide a robust\ntrajectory prediction of pedestrians. We have evaluated our framework on two\nreal-life datasets of pedestrians in shared urban traffic environments and it\nhas outperformed the compared baseline approaches in both short-term and\nlong-term prediction horizons.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 08:43:12 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Saleh", "Khaled", ""]]}, {"id": "2012.01768", "submitter": "Lars Schmarje", "authors": "Lars Schmarje and Johannes Br\\\"unger and Monty Santarossa and\n  Simon-Martin Schr\\\"oder and Rainer Kiko and Reinhard Koch", "title": "Beyond Cats and Dogs: Semi-supervised Classification of fuzzy labels\n  with overclustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-standing issue with deep learning is the need for large and\nconsistently labeled datasets. Although the current research in semi-supervised\nlearning can decrease the required amount of annotated data by a factor of 10\nor even more, this line of research still uses distinct classes like cats and\ndogs. However, in the real-world we often encounter problems where different\nexperts have different opinions, thus producing fuzzy labels. We propose a\nnovel framework for handling semi-supervised classifications of such fuzzy\nlabels. Our framework is based on the idea of overclustering to detect\nsubstructures in these fuzzy labels. We propose a novel loss to improve the\noverclustering capability of our framework and show on the common image\nclassification dataset STL-10 that it is faster and has better overclustering\nperformance than previous work. On a real-world plankton dataset, we illustrate\nthe benefit of overclustering for fuzzy labels and show that we beat previous\nstate-of-the-art semisupervised methods. Moreover, we acquire 5 to 10% more\nconsistent predictions of substructures.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 08:54:25 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Schmarje", "Lars", ""], ["Br\u00fcnger", "Johannes", ""], ["Santarossa", "Monty", ""], ["Schr\u00f6der", "Simon-Martin", ""], ["Kiko", "Rainer", ""], ["Koch", "Reinhard", ""]]}, {"id": "2012.01777", "submitter": "Duc Toan Bui", "authors": "Toan Duc Bui, Manh Nguyen, Ngan Le, Khoa Luu", "title": "Flow-based Deformation Guidance for Unpaired Multi-Contrast MRI\n  Image-to-Image Translation", "comments": "Medical Image Computing and Computer Assisted Interventions", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image synthesis from corrupted contrasts increases the diversity of\ndiagnostic information available for many neurological diseases. Recently the\nimage-to-image translation has experienced significant levels of interest\nwithin medical research, beginning with the successful use of the Generative\nAdversarial Network (GAN) to the introduction of cyclic constraint extended to\nmultiple domains. However, in current approaches, there is no guarantee that\nthe mapping between the two image domains would be unique or one-to-one. In\nthis paper, we introduce a novel approach to unpaired image-to-image\ntranslation based on the invertible architecture. The invertible property of\nthe flow-based architecture assures a cycle-consistency of image-to-image\ntranslation without additional loss functions. We utilize the temporal\ninformation between consecutive slices to provide more constraints to the\noptimization for transforming one domain to another in unpaired volumetric\nmedical images. To capture temporal structures in the medical images, we\nexplore the displacement between the consecutive slices using a deformation\nfield. In our approach, the deformation field is used as a guidance to keep the\ntranslated slides realistic and consistent across the translation. The\nexperimental results have shown that the synthesized images using our proposed\napproach are able to archive a competitive performance in terms of mean squared\nerror, peak signal-to-noise ratio, and structural similarity index when\ncompared with the existing deep learning-based methods on three standard\ndatasets, i.e. HCP, MRBrainS13, and Brats2019.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 09:10:22 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Bui", "Toan Duc", ""], ["Nguyen", "Manh", ""], ["Le", "Ngan", ""], ["Luu", "Khoa", ""]]}, {"id": "2012.01778", "submitter": "Konstantin Kobs", "authors": "Michael Fischer, Konstantin Kobs, Andreas Hotho", "title": "NICER: Aesthetic Image Enhancement with Humans in the Loop", "comments": "The code can be found at https://github.com/mr-Mojo/NICER", "journal-ref": "ACHI 2020, The Thirteenth International Conference on Advances in\n  Computer-Human Interactions; 2020; pages 357-362", "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully- or semi-automatic image enhancement software helps users to increase\nthe visual appeal of photos and does not require in-depth knowledge of manual\nimage editing. However, fully-automatic approaches usually enhance the image in\na black-box manner that does not give the user any control over the\noptimization process, possibly leading to edited images that do not\nsubjectively appeal to the user. Semi-automatic methods mostly allow for\ncontrolling which pre-defined editing step is taken, which restricts the users\nin their creativity and ability to make detailed adjustments, such as\nbrightness or contrast. We argue that incorporating user preferences by guiding\nan automated enhancement method simplifies image editing and increases the\nenhancement's focus on the user. This work thus proposes the Neural Image\nCorrection & Enhancement Routine (NICER), a neural network based approach to\nno-reference image enhancement in a fully-, semi-automatic or fully manual\nprocess that is interactive and user-centered. NICER iteratively adjusts image\nediting parameters in order to maximize an aesthetic score based on image style\nand content. Users can modify these parameters at any time and guide the\noptimization process towards a desired direction. This interactive workflow is\na novelty in the field of human-computer interaction for image enhancement\ntasks. In a user study, we show that NICER can improve image aesthetics without\nuser interaction and that allowing user interaction leads to diverse\nenhancement outcomes that are strongly preferred over the unedited image. We\nmake our code publicly available to facilitate further research in this\ndirection.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 09:14:10 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Fischer", "Michael", ""], ["Kobs", "Konstantin", ""], ["Hotho", "Andreas", ""]]}, {"id": "2012.01782", "submitter": "Zheng Yuan", "authors": "Zheng Yuan, Jie Zhang, Shiguang Shan, Xilin Chen", "title": "Attributes Aware Face Generation with Generative Adversarial Networks", "comments": "8 pages, 5 figures, 3 tables. Accepted by ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown remarkable success in face image generations.\nHowever, most of the existing methods only generate face images from random\nnoise, and cannot generate face images according to the specific attributes. In\nthis paper, we focus on the problem of face synthesis from attributes, which\naims at generating faces with specific characteristics corresponding to the\ngiven attributes. To this end, we propose a novel attributes aware face image\ngenerator method with generative adversarial networks called AFGAN.\nSpecifically, we firstly propose a two-path embedding layer and self-attention\nmechanism to convert binary attribute vector to rich attribute features. Then\nthree stacked generators generate $64 \\times 64$, $128 \\times 128$ and $256\n\\times 256$ resolution face images respectively by taking the attribute\nfeatures as input. In addition, an image-attribute matching loss is proposed to\nenhance the correlation between the generated images and input attributes.\nExtensive experiments on CelebA demonstrate the superiority of our AFGAN in\nterms of both qualitative and quantitative evaluations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 09:25:50 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Yuan", "Zheng", ""], ["Zhang", "Jie", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "2012.01784", "submitter": "John Cai", "authors": "John Cai, Bill Cai, Sheng Mei Shen", "title": "SB-MTL: Score-based Meta Transfer-Learning for Cross-Domain Few-Shot\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many deep learning methods have seen significant success in tackling\nthe problem of domain adaptation and few-shot learning separately, far fewer\nmethods are able to jointly tackle both problems in Cross-Domain Few-Shot\nLearning (CD-FSL). This problem is exacerbated under sharp domain shifts that\ntypify common computer vision applications. In this paper, we present a novel,\nflexible and effective method to address the CD-FSL problem. Our method, called\nScore-based Meta Transfer-Learning (SB-MTL), combines transfer-learning and\nmeta-learning by using a MAML-optimized feature encoder and a score-based Graph\nNeural Network. First, we have a feature encoder with specific layers designed\nto be fine-tuned. To do so, we apply a first-order MAML algorithm to find good\ninitializations. Second, instead of directly taking the classification scores\nafter fine-tuning, we interpret the scores as coordinates by mapping the\npre-softmax classification scores onto a metric space. Subsequently, we apply a\nGraph Neural Network to propagate label information from the support set to the\nquery set in our score-based metric space. We test our model on the Broader\nStudy of Cross-Domain Few-Shot Learning (BSCD-FSL) benchmark, which includes a\nrange of target domains with highly varying dissimilarity to the miniImagenet\nsource domain. We observe significant improvements in accuracy across 5, 20 and\n50 shot, and on the four target domains. In terms of average accuracy, our\nmodel outperforms previous transfer-learning methods by 5.93% and previous\nmeta-learning methods by 14.28%.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 09:29:35 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Cai", "John", ""], ["Cai", "Bill", ""], ["Shen", "Sheng Mei", ""]]}, {"id": "2012.01788", "submitter": "Yanmin Wu", "authors": "Yanmin Wu, Yunzhou Zhang, Delong Zhu, Xin Chen, Sonya Coleman, Wenkai\n  Sun, Xinggang Hu, Zhiqiang Deng", "title": "Object-Driven Active Mapping for More Accurate Object Pose Estimation\n  and Robotic Grasping", "comments": "7 pages, 11 figures. Project page:\n  https://yanmin-wu.github.io/project/active-mapping/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the first active object mapping framework for complex\nrobotic grasping tasks. The framework is built on an object SLAM system\nintegrated with a simultaneous multi-object pose estimation process. Aiming to\nreduce the observation uncertainty on target objects and increase their pose\nestimation accuracy, we also design an object-driven exploration strategy to\nguide the object mapping process. By combining the mapping module and the\nexploration strategy, an accurate object map that is compatible with robotic\ngrasping can be generated. Quantitative evaluations also show that the proposed\nframework has a very high mapping accuracy. Manipulation experiments, including\nobject grasping, object placement, and the augmented reality, significantly\ndemonstrate the effectiveness and advantages of our proposed framework.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 09:36:55 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 14:56:49 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Wu", "Yanmin", ""], ["Zhang", "Yunzhou", ""], ["Zhu", "Delong", ""], ["Chen", "Xin", ""], ["Coleman", "Sonya", ""], ["Sun", "Wenkai", ""], ["Hu", "Xinggang", ""], ["Deng", "Zhiqiang", ""]]}, {"id": "2012.01806", "submitter": "Tejas Gokhale", "authors": "Tejas Gokhale, Rushil Anirudh, Bhavya Kailkhura, Jayaraman J.\n  Thiagarajan, Chitta Baral, Yezhou Yang", "title": "Attribute-Guided Adversarial Training for Robustness to Natural\n  Perturbations", "comments": "AAAI 2021. Camera Ready version + Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  While existing work in robust deep learning has focused on small pixel-level\nnorm-based perturbations, this may not account for perturbations encountered in\nseveral real-world settings. In many such cases although test data might not be\navailable, broad specifications about the types of perturbations (such as an\nunknown degree of rotation) may be known. We consider a setup where robustness\nis expected over an unseen test domain that is not i.i.d. but deviates from the\ntraining domain. While this deviation may not be exactly known, its broad\ncharacterization is specified a priori, in terms of attributes. We propose an\nadversarial training approach which learns to generate new samples so as to\nmaximize exposure of the classifier to the attributes-space, without having\naccess to the data from the test domain. Our adversarial training solves a\nmin-max optimization problem, with the inner maximization generating\nadversarial perturbations, and the outer minimization finding model parameters\nby optimizing the loss on adversarial perturbations generated from the inner\nmaximization. We demonstrate the applicability of our approach on three types\nof naturally occurring perturbations -- object-related shifts, geometric\ntransformations, and common image corruptions. Our approach enables deep neural\nnetworks to be robust against a wide range of naturally occurring\nperturbations. We demonstrate the usefulness of the proposed approach by\nshowing the robustness gains of deep neural networks trained using our\nadversarial training on MNIST, CIFAR-10, and a new variant of the CLEVR\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 10:17:30 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 19:20:23 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 03:25:14 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Gokhale", "Tejas", ""], ["Anirudh", "Rushil", ""], ["Kailkhura", "Bhavya", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Baral", "Chitta", ""], ["Yang", "Yezhou", ""]]}, {"id": "2012.01821", "submitter": "Yanbin Liu", "authors": "Xiuli Bi, Yanbin Liu, Bin Xiao, Weisheng Li, Chi-Man Pun, Guoyin Wang,\n  and Xinbo Gao", "title": "D-Unet: A Dual-encoder U-Net for Image Splicing Forgery Detection and\n  Localization", "comments": "13 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many detection methods based on convolutional neural networks\n(CNNs) have been proposed for image splicing forgery detection. Most of these\ndetection methods focus on the local patches or local objects. In fact, image\nsplicing forgery detection is a global binary classification task that\ndistinguishes the tampered and non-tampered regions by image fingerprints.\nHowever, some specific image contents are hardly retained by CNN-based\ndetection networks, but if included, would improve the detection accuracy of\nthe networks. To resolve these issues, we propose a novel network called\ndual-encoder U-Net (D-Unet) for image splicing forgery detection, which employs\nan unfixed encoder and a fixed encoder. The unfixed encoder autonomously learns\nthe image fingerprints that differentiate between the tampered and non-tampered\nregions, whereas the fixed encoder intentionally provides the direction\ninformation that assists the learning and detection of the network. This\ndual-encoder is followed by a spatial pyramid global-feature extraction module\nthat expands the global insight of D-Unet for classifying the tampered and\nnon-tampered regions more accurately. In an experimental comparison study of\nD-Unet and state-of-the-art methods, D-Unet outperformed the other methods in\nimage-level and pixel-level detection, without requiring pre-training or\ntraining on a large number of forgery images. Moreover, it was stably robust to\ndifferent attacks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 10:54:02 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Bi", "Xiuli", ""], ["Liu", "Yanbin", ""], ["Xiao", "Bin", ""], ["Li", "Weisheng", ""], ["Pun", "Chi-Man", ""], ["Wang", "Guoyin", ""], ["Gao", "Xinbo", ""]]}, {"id": "2012.01829", "submitter": "Fengchao Xiong", "authors": "Fengchao Xiong, Jun Zhou, Jianfeng Lu, and Yuntao Qian", "title": "SMDS-Net: Model Guided Spectral-Spatial Network for Hyperspectral Image\n  Denoising", "comments": "11 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning (DL) based hyperspectral images (HSIs) denoising approaches\ndirectly learn the nonlinear mapping between observed noisy images and\nunderlying clean images. They normally do not consider the physical\ncharacteristics of HSIs, therefore making them lack of interpretability that is\nkey to understand their denoising mechanism.. In order to tackle this problem,\nwe introduce a novel model guided interpretable network for HSI denoising.\nSpecifically, fully considering the spatial redundancy, spectral low-rankness\nand spectral-spatial properties of HSIs, we first establish a subspace based\nmulti-dimensional sparse model. This model first projects the observed HSIs\ninto a low-dimensional orthogonal subspace, and then represents the projected\nimage with a multidimensional dictionary. After that, the model is unfolded\ninto an end-to-end network named SMDS-Net whose fundamental modules are\nseamlessly connected with the denoising procedure and optimization of the\nmodel. This makes SMDS-Net convey clear physical meanings, i.e., learning the\nlow-rankness and sparsity of HSIs. Finally, all key variables including\ndictionaries and thresholding parameters are obtained by the end-to-end\ntraining. Extensive experiments and comprehensive analysis confirm the\ndenoising ability and interpretability of our method against the\nstate-of-the-art HSI denoising methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 11:05:01 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Xiong", "Fengchao", ""], ["Zhou", "Jun", ""], ["Lu", "Jianfeng", ""], ["Qian", "Yuntao", ""]]}, {"id": "2012.01832", "submitter": "Hiya Roy", "authors": "Hiya Roy, Subhajit Chaudhury, Toshihiko Yamasaki, Tatsuaki Hashimoto", "title": "Image inpainting using frequency domain priors", "comments": null, "journal-ref": null, "doi": "10.1117/1.JEI.30.2.023016", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a novel image inpainting technique using frequency\ndomain information. Prior works on image inpainting predict the missing pixels\nby training neural networks using only the spatial domain information. However,\nthese methods still struggle to reconstruct high-frequency details for real\ncomplex scenes, leading to a discrepancy in color, boundary artifacts,\ndistorted patterns, and blurry textures. To alleviate these problems, we\ninvestigate if it is possible to obtain better performance by training the\nnetworks using frequency domain information (Discrete Fourier Transform) along\nwith the spatial domain information. To this end, we propose a frequency-based\ndeconvolution module that enables the network to learn the global context while\nselectively reconstructing the high-frequency components. We evaluate our\nproposed method on the publicly available datasets CelebA, Paris Streetview,\nand DTD texture dataset, and show that our method outperforms current\nstate-of-the-art image inpainting techniques both qualitatively and\nquantitatively.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 11:08:13 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Roy", "Hiya", ""], ["Chaudhury", "Subhajit", ""], ["Yamasaki", "Toshihiko", ""], ["Hashimoto", "Tatsuaki", ""]]}, {"id": "2012.01866", "submitter": "Tim Meinhardt", "authors": "Tim Meinhardt and Laura Leal-Taixe", "title": "Make One-Shot Video Object Segmentation Efficient Again", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video object segmentation (VOS) describes the task of segmenting a set of\nobjects in each frame of a video. In the semi-supervised setting, the first\nmask of each object is provided at test time. Following the one-shot principle,\nfine-tuning VOS methods train a segmentation model separately on each given\nobject mask. However, recently the VOS community has deemed such a test time\noptimization and its impact on the test runtime as unfeasible. To mitigate the\ninefficiencies of previous fine-tuning approaches, we present efficient\nOne-Shot Video Object Segmentation (e-OSVOS). In contrast to most VOS\napproaches, e-OSVOS decouples the object detection task and predicts only local\nsegmentation masks by applying a modified version of Mask R-CNN. The one-shot\ntest runtime and performance are optimized without a laborious and handcrafted\nhyperparameter search. To this end, we meta learn the model initialization and\nlearning rates for the test time optimization. To achieve optimal learning\nbehavior, we predict individual learning rates at a neuron level. Furthermore,\nwe apply an online adaptation to address the common performance degradation\nthroughout a sequence by continuously fine-tuning the model on previous mask\npredictions supported by a frame-to-frame bounding box propagation. e-OSVOS\nprovides state-of-the-art results on DAVIS 2016, DAVIS 2017, and YouTube-VOS\nfor one-shot fine-tuning methods while reducing the test runtime substantially.\n  Code is available at https://github.com/dvl-tum/e-osvos.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 12:21:23 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Meinhardt", "Tim", ""], ["Leal-Taixe", "Laura", ""]]}, {"id": "2012.01874", "submitter": "Jan Klopp", "authors": "Jan P. Klopp, Keng-Chi Liu, Liang-Gee Chen, Shao-Yi Chien", "title": "How to Exploit the Transferability of Learned Image Compression to\n  Conventional Codecs", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy image compression is often limited by the simplicity of the chosen loss\nmeasure. Recent research suggests that generative adversarial networks have the\nability to overcome this limitation and serve as a multi-modal loss, especially\nfor textures. Together with learned image compression, these two techniques can\nbe used to great effect when relaxing the commonly employed tight measures of\ndistortion. However, convolutional neural network based algorithms have a large\ncomputational footprint. Ideally, an existing conventional codec should stay in\nplace, which would ensure faster adoption and adhering to a balanced\ncomputational envelope.\n  As a possible avenue to this goal, in this work, we propose and investigate\nhow learned image coding can be used as a surrogate to optimize an image for\nencoding. The image is altered by a learned filter to optimise for a different\nperformance measure or a particular task. Extending this idea with a generative\nadversarial network, we show how entire textures are replaced by ones that are\nless costly to encode but preserve sense of detail.\n  Our approach can remodel a conventional codec to adjust for the MS-SSIM\ndistortion with over 20% rate improvement without any decoding overhead. On\ntask-aware image compression, we perform favourably against a similar but\ncodec-specific approach.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 12:34:51 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 03:44:13 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Klopp", "Jan P.", ""], ["Liu", "Keng-Chi", ""], ["Chen", "Liang-Gee", ""], ["Chien", "Shao-Yi", ""]]}, {"id": "2012.01879", "submitter": "Weisen Wang", "authors": "Weisen Wang, Xirong Li, Zhiyan Xu, Weihong Yu, Jianchun Zhao, Dayong\n  Ding, Youxin Chen", "title": "Learning Two-Stream CNN for Multi-Modal Age-related Macular Degeneration\n  Categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles automated categorization of Age-related Macular\nDegeneration (AMD), a common macular disease among people over 50. Previous\nresearch efforts mainly focus on AMD categorization with a single-modal input,\nlet it be a color fundus image or an OCT image. By contrast, we consider AMD\ncategorization given a multi-modal input, a direction that is clinically\nmeaningful yet mostly unexplored. Contrary to the prior art that takes a\ntraditional approach of feature extraction plus classifier training that cannot\nbe jointly optimized, we opt for end-to-end multi-modal Convolutional Neural\nNetworks (MM-CNN). Our MM-CNN is instantiated by a two-stream CNN, with\nspatially-invariant fusion to combine information from the fundus and OCT\nstreams. In order to visually interpret the contribution of the individual\nmodalities to the final prediction, we extend the class activation mapping\n(CAM) technique to the multi-modal scenario. For effective training of MM-CNN,\nwe develop two data augmentation methods. One is GAN-based fundus / OCT image\nsynthesis, with our novel use of CAMs as conditional input of a high-resolution\nimage-to-image translation GAN. The other method is Loose Pairing, which pairs\na fundus image and an OCT image on the basis of their classes instead of eye\nidentities. Experiments on a clinical dataset consisting of 1,099 color fundus\nimages and 1,290 OCT images acquired from 1,099 distinct eyes verify the\neffectiveness of the proposed solution for multi-modal AMD categorization.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 12:50:36 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Wang", "Weisen", ""], ["Li", "Xirong", ""], ["Xu", "Zhiyan", ""], ["Yu", "Weihong", ""], ["Zhao", "Jianchun", ""], ["Ding", "Dayong", ""], ["Chen", "Youxin", ""]]}, {"id": "2012.01884", "submitter": "Yuanman Li", "authors": "Rongqin Liang, Yuanman Li, Xia Li, yi tang, Jiantao Zhou, Wenbin Zou", "title": "Temporal Pyramid Network for Pedestrian Trajectory Prediction with\n  Multi-Supervision", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting human motion behavior in a crowd is important for many\napplications, ranging from the natural navigation of autonomous vehicles to\nintelligent security systems of video surveillance. All the previous works\nmodel and predict the trajectory with a single resolution, which is rather\ninefficient and difficult to simultaneously exploit the long-range information\n(e.g., the destination of the trajectory), and the short-range information\n(e.g., the walking direction and speed at a certain time) of the motion\nbehavior. In this paper, we propose a temporal pyramid network for pedestrian\ntrajectory prediction through a squeeze modulation and a dilation modulation.\nOur hierarchical framework builds a feature pyramid with increasingly richer\ntemporal information from top to bottom, which can better capture the motion\nbehavior at various tempos. Furthermore, we propose a coarse-to-fine fusion\nstrategy with multi-supervision. By progressively merging the top coarse\nfeatures of global context to the bottom fine features of rich local context,\nour method can fully exploit both the long-range and short-range information of\nthe trajectory. Experimental results on several benchmarks demonstrate the\nsuperiority of our method.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 13:02:59 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 02:11:49 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Liang", "Rongqin", ""], ["Li", "Yuanman", ""], ["Li", "Xia", ""], ["tang", "yi", ""], ["Zhou", "Jiantao", ""], ["Zou", "Wenbin", ""]]}, {"id": "2012.01909", "submitter": "Qunjie Zhou", "authors": "Qunjie Zhou, Torsten Sattler, Laura Leal-Taixe", "title": "Patch2Pix: Epipolar-Guided Pixel-Level Correspondences", "comments": "CVPR2021 Camera Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical matching pipeline used for visual localization typically\ninvolves three steps: (i) local feature detection and description, (ii) feature\nmatching, and (iii) outlier rejection. Recently emerged correspondence networks\npropose to perform those steps inside a single network but suffer from low\nmatching resolution due to the memory bottleneck. In this work, we propose a\nnew perspective to estimate correspondences in a detect-to-refine manner, where\nwe first predict patch-level match proposals and then refine them. We present\nPatch2Pix, a novel refinement network that refines match proposals by\nregressing pixel-level matches from the local regions defined by those\nproposals and jointly rejecting outlier matches with confidence scores.\nPatch2Pix is weakly supervised to learn correspondences that are consistent\nwith the epipolar geometry of an input image pair. We show that our refinement\nnetwork significantly improves the performance of correspondence networks on\nimage matching, homography estimation, and localization tasks. In addition, we\nshow that our learned refinement generalizes to fully-supervised methods\nwithout re-training, which leads us to state-of-the-art localization\nperformance. The code is available at https://github.com/GrumpyZhou/patch2pix.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 13:44:02 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 17:45:30 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 19:54:41 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhou", "Qunjie", ""], ["Sattler", "Torsten", ""], ["Leal-Taixe", "Laura", ""]]}, {"id": "2012.01918", "submitter": "Haijin Zeng", "authors": "Haijin Zeng", "title": "Multi-mode Core Tensor Factorization based Low-Rankness and Its\n  Applications to Tensor Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank tensor completion has been widely used in computer vision and\nmachine learning. This paper develops a kind of multi-modal core tensor\nfactorization (MCTF) method together with a tensor low-rankness measure and a\nbetter nonconvex relaxation form of it (NonMCTF). The proposed models encode\nlow-rank insights for general tensors provided by Tucker and T-SVD, and thus\nare expected to simultaneously model spectral low-rankness in multiple\norientations and accurately restore the data of intrinsic low-rank structure\nbased on few observed entries. Furthermore, we study the MCTF and NonMCTF\nregularization minimization problem, and design an effective BSUM algorithm to\nsolve them. This efficient solver can extend MCTF to various tasks, such as\ntensor completion. A series of experiments, including hyperspectral image\n(HSI), video and MRI completion, confirm the superior performance of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 13:57:00 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 07:13:54 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zeng", "Haijin", ""]]}, {"id": "2012.01940", "submitter": "Ben Sattelberg", "authors": "Ben Sattelberg, Renzo Cavalieri, Michael Kirby, Chris Peterson, Ross\n  Beveridge", "title": "Locally Linear Attributes of ReLU Neural Networks", "comments": "18 pages, 12 figures, 2 tables, submitted to SIMODS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A ReLU neural network determines/is a continuous piecewise linear map from an\ninput space to an output space. The weights in the neural network determine a\ndecomposition of the input space into convex polytopes and on each of these\npolytopes the network can be described by a single affine mapping. The\nstructure of the decomposition, together with the affine map attached to each\npolytope, can be analyzed to investigate the behavior of the associated neural\nnetwork.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 19:31:23 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Sattelberg", "Ben", ""], ["Cavalieri", "Renzo", ""], ["Kirby", "Michael", ""], ["Peterson", "Chris", ""], ["Beveridge", "Ross", ""]]}, {"id": "2012.01944", "submitter": "Miko{\\l}aj Ma{\\l}ki\\'nski", "authors": "Miko{\\l}aj Ma{\\l}ki\\'nski, Jacek Ma\\'ndziuk", "title": "Multi-Label Contrastive Learning for Abstract Visual Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a long time the ability to solve abstract reasoning tasks was considered\none of the hallmarks of human intelligence. Recent advances in application of\ndeep learning (DL) methods led, as in many other domains, to surpassing human\nabstract reasoning performance, specifically in the most popular type of such\nproblems - the Raven's Progressive Matrices (RPMs). While the efficacy of DL\nsystems is indeed impressive, the way they approach the RPMs is very different\nfrom that of humans. State-of-the-art systems solving RPMs rely on massive\npattern-based training and sometimes on exploiting biases in the dataset,\nwhereas humans concentrate on identification of the rules / concepts underlying\nthe RPM (or generally a visual reasoning task) to be solved. Motivated by this\ncognitive difference, this work aims at combining DL with human way of solving\nRPMs and getting the best of both worlds. Specifically, we cast the problem of\nsolving RPMs into multi-label classification framework where each RPM is viewed\nas a multi-label data point, with labels determined by the set of abstract\nrules underlying the RPM. For efficient training of the system we introduce a\ngeneralisation of the Noise Contrastive Estimation algorithm to the case of\nmulti-label samples. Furthermore, we propose a new sparse rule encoding scheme\nfor RPMs which, besides the new training algorithm, is the key factor\ncontributing to the state-of-the-art performance. The proposed approach is\nevaluated on two most popular benchmark datasets (Balanced-RAVEN and PGM) and\non both of them demonstrates an advantage over the current state-of-the-art\nresults. Contrary to applications of contrastive learning methods reported in\nother domains, the state-of-the-art performance reported in the paper is\nachieved with no need for large batch sizes or strong data augmentation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 14:18:15 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Ma\u0142ki\u0144ski", "Miko\u0142aj", ""], ["Ma\u0144dziuk", "Jacek", ""]]}, {"id": "2012.01950", "submitter": "Tiancai Wang", "authors": "Tiancai Wang, Tong Yang, Jiale Cao, Xiangyu Zhang", "title": "Co-mining: Self-Supervised Learning for Sparsely Annotated Object\n  Detection", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detectors usually achieve promising results with the supervision of\ncomplete instance annotations. However, their performance is far from\nsatisfactory with sparse instance annotations. Most existing methods for\nsparsely annotated object detection either re-weight the loss of hard negative\nsamples or convert the unlabeled instances into ignored regions to reduce the\ninterference of false negatives. We argue that these strategies are\ninsufficient since they can at most alleviate the negative effect caused by\nmissing annotations. In this paper, we propose a simple but effective\nmechanism, called Co-mining, for sparsely annotated object detection. In our\nCo-mining, two branches of a Siamese network predict the pseudo-label sets for\neach other. To enhance multi-view learning and better mine unlabeled instances,\nthe original image and corresponding augmented image are used as the inputs of\ntwo branches of the Siamese network, respectively. Co-mining can serve as a\ngeneral training mechanism applied to most of modern object detectors.\nExperiments are performed on MS COCO dataset with three different sparsely\nannotated settings using two typical frameworks: anchor-based detector\nRetinaNet and anchor-free detector FCOS. Experimental results show that our\nCo-mining with RetinaNet achieves 1.4%~2.1% improvements compared with\ndifferent baselines and surpasses existing methods under the same sparsely\nannotated setting.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 14:23:43 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Wang", "Tiancai", ""], ["Yang", "Tong", ""], ["Cao", "Jiale", ""], ["Zhang", "Xiangyu", ""]]}, {"id": "2012.01955", "submitter": "Gustavo Marfia", "authors": "Lorenzo Stacchio, Alessia Angeli, Giuseppe Lisanti, Daniela Calanca,\n  Gustavo Marfia", "title": "IMAGO: A family photo album dataset for a socio-historical analysis of\n  the twentieth century", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although one of the most popular practices in photography since the end of\nthe 19th century, an increase in scholarly interest in family photo albums\ndates back to the early 1980s. Such collections of photos may reveal\nsociological and historical insights regarding specific cultures and times.\nThey are, however, in most cases scattered among private homes and only\navailable on paper or photographic film, thus making their analysis by\nacademics such as historians, social-cultural anthropologists and cultural\ntheorists very cumbersome. In this paper, we analyze the IMAGO dataset\nincluding photos belonging to family albums assembled at the University of\nBologna's Rimini campus since 2004. Following a deep learning-based approach,\nthe IMAGO dataset has offered the opportunity of experimenting with photos\ntaken between year 1845 and year 2009, with the goals of assessing the dates\nand the socio-historical contexts of the images, without use of any other\nsources of information. Exceeding our initial expectations, such analysis has\nrevealed its merit not only in terms of the performance of the approach adopted\nin this work, but also in terms of the foreseeable implications and use for the\nbenefit of socio-historical research. To the best of our knowledge, this is the\nfirst work that moves along this path in literature.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 14:28:58 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Stacchio", "Lorenzo", ""], ["Angeli", "Alessia", ""], ["Lisanti", "Giuseppe", ""], ["Calanca", "Daniela", ""], ["Marfia", "Gustavo", ""]]}, {"id": "2012.01975", "submitter": "Silas {\\O}rting", "authors": "Silas Nyboe {\\O}rting", "title": "A small note on variation in segmentation annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We report on the results of a small crowdsourcing experiment conducted at a\nworkshop on machine learning for segmentation held at the Danish Bio Imaging\nnetwork meeting 2020. During the workshop we asked participants to manually\nsegment mitochondria in three 2D patches. The aim of the experiment was to\nillustrate that manual annotations should not be seen as the ground truth, but\nas a reference standard that is subject to substantial variation. In this note\nwe show how the large variation we observed in the segmentations can be reduced\nby removing the annotators with worst pair-wise agreement. Having removed the\nannotators with worst performance, we illustrate that the remaining variance is\nsemantically meaningful and can be exploited to obtain segmentations of cell\nboundary and cell interior.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 14:55:23 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["\u00d8rting", "Silas Nyboe", ""]]}, {"id": "2012.01980", "submitter": "Wei Zhou", "authors": "Wei Zhou and Zhibo Chen", "title": "Deep Multi-Scale Features Learning for Distorted Image Quality\n  Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image quality assessment (IQA) aims to estimate human perception based image\nvisual quality. Although existing deep neural networks (DNNs) have shown\nsignificant effectiveness for tackling the IQA problem, it still needs to\nimprove the DNN-based quality assessment models by exploiting efficient\nmulti-scale features. In this paper, motivated by the human visual system (HVS)\ncombining multi-scale features for perception, we propose to use pyramid\nfeatures learning to build a DNN with hierarchical multi-scale features for\ndistorted image quality prediction. Our model is based on both residual maps\nand distorted images in luminance domain, where the proposed network contains\nspatial pyramid pooling and feature pyramid from the network structure. Our\nproposed network is optimized in a deep end-to-end supervision manner. To\nvalidate the effectiveness of the proposed method, extensive experiments are\nconducted on four widely-used image quality assessment databases, demonstrating\nthe superiority of our algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 23:39:01 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Zhou", "Wei", ""], ["Chen", "Zhibo", ""]]}, {"id": "2012.01986", "submitter": "Zhipeng Li", "authors": "Zhipeng Li, Yong Long, Il Yong Chun", "title": "An Improved Iterative Neural Network for High-Quality Image-Domain\n  Material Decomposition in Dual-Energy CT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dual-energy computed tomography (DECT) has been widely used in many\napplications that need material decomposition. Image-domain methods directly\ndecompose material images from high- and low-energy attenuation images, and\nthus, are susceptible to noise and artifacts on attenuation images. To obtain\nhigh-quality material images, various data-driven methods have been proposed.\nIterative neural network (INN) methods combine regression NNs and model-based\nimage reconstruction algorithm. INNs reduced the generalization error of\n(noniterative) deep regression NNs, and achieved high-quality reconstruction in\ndiverse medical imaging applications. BCD-Net is a recent INN architecture that\nincorporates imaging refining NNs into the block coordinate descent (BCD)\nmodel-based image reconstruction algorithm. We propose a new INN architecture,\ndistinct cross-material BCD-Net, for DECT material decomposition. The proposed\nINN architecture uses distinct cross-material convolutional neural network\n(CNN) in image refining modules, and uses image decomposition physics in image\nreconstruction modules. The distinct cross-material CNN refiners incorporate\ndistinct encoding-decoding filters and cross-material model that captures\ncorrelations between different materials. We interpret the distinct\ncross-material CNN refiner with patch perspective. Numerical experiments with\nextended cardiactorso (XCAT) phantom and clinical data show that proposed\ndistinct cross-material BCD-Net significantly improves the image quality over\nseveral image-domain material decomposition methods, including a conventional\nmodel-based image decomposition (MBID) method using an edge-preserving\nregularizer, a state-of-the-art MBID method using pre-learned material-wise\nsparsifying transforms, and a noniterative deep CNN denoiser.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 16:27:38 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Li", "Zhipeng", ""], ["Long", "Yong", ""], ["Chun", "Il Yong", ""]]}, {"id": "2012.01988", "submitter": "Xiaofang Wang", "authors": "Xiaofang Wang, Dan Kondratyuk, Eric Christiansen, Kris M. Kitani, Yair\n  Movshovitz-Attias, Elad Eban", "title": "On the Surprising Efficiency of Committee-based Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Committee-based models, i.e., model ensembles or cascades, are underexplored\nin recent work on developing efficient models. While committee-based models\nthemselves are not new, there lacks a systematic understanding of their\nefficiency in comparison with single models. To fill this gap, we conduct a\ncomprehensive analysis of the efficiency of committee-based models and find\nthat committee-based models provide a complementary paradigm to achieve\nsuperior efficiency without tuning the architecture: a simple ensemble or\ncascade of existing networks can be considerably more efficient than\nstate-of-the-art single models, even outperforming sophisticated neural\narchitecture search methods. The superior efficiency of committee-based models\nholds true for several tasks, including image classification, video\nclassification, and semantic segmentation, and various architecture families,\nsuch as EfficientNet, ResNet, MobileNetV2, and X3D.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 14:59:16 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 18:26:58 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Wang", "Xiaofang", ""], ["Kondratyuk", "Dan", ""], ["Christiansen", "Eric", ""], ["Kitani", "Kris M.", ""], ["Movshovitz-Attias", "Yair", ""], ["Eban", "Elad", ""]]}, {"id": "2012.01994", "submitter": "Rose Nakasi", "authors": "Rose Nakasi, Aminah Zawedde, Ernest Mwebaze, Jeremy Francis Tusubira,\n  Gilbert Maiga", "title": "Localization of Malaria Parasites and White Blood Cells in Thick Blood\n  Smears", "comments": "6 pages, 3 figures,2020 NeurIPS ML4D workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Effectively determining malaria parasitemia is a critical aspect in assisting\nclinicians to accurately determine the severity of the disease and provide\nquality treatment. Microscopy applied to thick smear blood smears is the de\nfacto method for malaria parasitemia determination. However, manual\nquantification of parasitemia is time consuming, laborious and requires\nconsiderable trained expertise which is particularly inadequate in highly\nendemic and low resourced areas. This study presents an end-to-end approach for\nlocalisation and count of malaria parasites and white blood cells (WBCs) which\naid in the effective determination of parasitemia; the quantitative content of\nparasites in the blood. On a dataset of slices of images of thick blood smears,\nwe build models to analyse the obtained digital images. To improve model\nperformance due to the limited size of the dataset, data augmentation was\napplied. Our preliminary results show that our deep learning approach reliably\ndetects and returns a count of malaria parasites and WBCs with a high precision\nand recall. We also evaluate our system against human experts and results\nindicate a strong correlation between our deep learning model counts and the\nmanual expert counts (p=0.998 for parasites, p=0.987 for WBCs). This approach\ncould potentially be applied to support malaria parasitemia determination\nespecially in settings that lack sufficient Microscopists.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 15:14:38 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Nakasi", "Rose", ""], ["Zawedde", "Aminah", ""], ["Mwebaze", "Ernest", ""], ["Tusubira", "Jeremy Francis", ""], ["Maiga", "Gilbert", ""]]}, {"id": "2012.02024", "submitter": "Michael Heffels", "authors": "Michael R. Heffels and Joaquin Vanschoren", "title": "Aerial Imagery Pixel-level Segmentation", "comments": "30 pages, 15 figures, 4 tables. Code available through GitHub repo at\n  https://github.com/mrheffels/aerial-imagery-segmentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aerial imagery can be used for important work on a global scale.\nNevertheless, the analysis of this data using neural network architectures lags\nbehind the current state-of-the-art on popular datasets such as PASCAL VOC,\nCityScapes and Camvid. In this paper we bridge the performance-gap between\nthese popular datasets and aerial imagery data. Little work is done on aerial\nimagery with state-of-the-art neural network architectures in a multi-class\nsetting. Our experiments concerning data augmentation, normalisation, image\nsize and loss functions give insight into a high performance setup for aerial\nimagery segmentation datasets. Our work, using the state-of-the-art DeepLabv3+\nXception65 architecture, achieves a mean IOU of 70% on the DroneDeploy\nvalidation set. With this result, we clearly outperform the current publicly\navailable state-of-the-art validation set mIOU (65%) performance with 5%.\nFurthermore, to our knowledge, there is no mIOU benchmark for the test set.\nHence, we also propose a new benchmark on the DroneDeploy test set using the\nbest performing DeepLabv3+ Xception65 architecture, with a mIOU score of 52.5%.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 16:09:09 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Heffels", "Michael R.", ""], ["Vanschoren", "Joaquin", ""]]}, {"id": "2012.02033", "submitter": "Baohua Sun", "authors": "Baohua Sun, Michael Lin, Hao Sha, Lin Yang", "title": "SuperOCR: A Conversion from Optical Character Recognition to Image\n  Captioning", "comments": "8 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optical Character Recognition (OCR) has many real world applications. The\nexisting methods normally detect where the characters are, and then recognize\nthe character for each detected location. Thus the accuracy of characters\nrecognition is impacted by the performance of characters detection. In this\npaper, we propose a method for recognizing characters without detecting the\nlocation of each character. This is done by converting the OCR task into an\nimage captioning task. One advantage of the proposed method is that the labeled\nbounding boxes for the characters are not needed during training. The\nexperimental results show the proposed method outperforms the existing methods\non both the license plate recognition and the watermeter character recognition\ntasks. The proposed method is also deployed into a low-power (300mW) CNN\naccelerator chip connected to a Raspberry Pi 3 for on-device applications.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 06:40:04 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Sun", "Baohua", ""], ["Lin", "Michael", ""], ["Sha", "Hao", ""], ["Yang", "Lin", ""]]}, {"id": "2012.02043", "submitter": "Suhas Lohit", "authors": "Suhas Lohit, Rushil Anirudh, Pavan Turaga", "title": "Recovering Trajectories of Unmarked Joints in 3D Human Actions Using\n  Latent Space Optimization", "comments": "Accepted at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion capture (mocap) and time-of-flight based sensing of human actions are\nbecoming increasingly popular modalities to perform robust activity analysis.\nApplications range from action recognition to quantifying movement quality for\nhealth applications. While marker-less motion capture has made great progress,\nin critical applications such as healthcare, marker-based systems, especially\nactive markers, are still considered gold-standard. However, there are several\npractical challenges in both modalities such as visibility, tracking errors,\nand simply the need to keep marker setup convenient wherein movements are\nrecorded with a reduced marker-set. This implies that certain joint locations\nwill not even be marked-up, making downstream analysis of full body movement\nchallenging. To address this gap, we first pose the problem of reconstructing\nthe unmarked joint data as an ill-posed linear inverse problem. We recover\nmissing joints for a given action by projecting it onto the manifold of human\nactions, this is achieved by optimizing the latent space representation of a\ndeep autoencoder. Experiments on both mocap and Kinect datasets clearly\ndemonstrate that the proposed method performs very well in recovering semantics\nof the actions and dynamics of missing joints. We will release all the code and\nmodels publicly.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 16:25:07 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Lohit", "Suhas", ""], ["Anirudh", "Rushil", ""], ["Turaga", "Pavan", ""]]}, {"id": "2012.02046", "submitter": "Meike Nauta", "authors": "Meike Nauta, Ron van Bree, Christin Seifert", "title": "Neural Prototype Trees for Interpretable Fine-grained Image Recognition", "comments": "Accepted to IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2021. 11 pages, and 9 pages supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prototype-based methods use interpretable representations to address the\nblack-box nature of deep learning models, in contrast to post-hoc explanation\nmethods that only approximate such models. We propose the Neural Prototype Tree\n(ProtoTree), an intrinsically interpretable deep learning method for\nfine-grained image recognition. ProtoTree combines prototype learning with\ndecision trees, and thus results in a globally interpretable model by design.\nAdditionally, ProtoTree can locally explain a single prediction by outlining a\ndecision path through the tree. Each node in our binary tree contains a\ntrainable prototypical part. The presence or absence of this learned prototype\nin an image determines the routing through a node. Decision making is therefore\nsimilar to human reasoning: Does the bird have a red throat? And an elongated\nbeak? Then it's a hummingbird! We tune the accuracy-interpretability trade-off\nusing ensemble methods, pruning and binarizing. We apply pruning without\nsacrificing accuracy, resulting in a small tree with only 8 learned prototypes\nalong a path to classify a bird from 200 species. An ensemble of 5 ProtoTrees\nachieves competitive accuracy on the CUB-200- 2011 and Stanford Cars data sets.\nCode is available at https://github.com/M-Nauta/ProtoTree\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 16:28:04 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 08:49:31 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Nauta", "Meike", ""], ["van Bree", "Ron", ""], ["Seifert", "Christin", ""]]}, {"id": "2012.02047", "submitter": "Xingran Zhou", "authors": "Xingran Zhou, Bo Zhang, Ting Zhang, Pan Zhang, Jianmin Bao, Dong Chen,\n  Zhongfei Zhang, Fang Wen", "title": "CoCosNet v2: Full-Resolution Correspondence Learning for Image\n  Translation", "comments": "CVPR 2021 oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the full-resolution correspondence learning for cross-domain\nimages, which aids image translation. We adopt a hierarchical strategy that\nuses the correspondence from coarse level to guide the fine levels. At each\nhierarchy, the correspondence can be efficiently computed via PatchMatch that\niteratively leverages the matchings from the neighborhood. Within each\nPatchMatch iteration, the ConvGRU module is employed to refine the current\ncorrespondence considering not only the matchings of larger context but also\nthe historic estimates. The proposed CoCosNet v2, a GRU-assisted PatchMatch\napproach, is fully differentiable and highly efficient. When jointly trained\nwith image translation, full-resolution semantic correspondence can be\nestablished in an unsupervised manner, which in turn facilitates the\nexemplar-based image translation. Experiments on diverse translation tasks show\nthat CoCosNet v2 performs considerably better than state-of-the-art literature\non producing high-resolution images.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 16:28:05 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 07:50:44 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhou", "Xingran", ""], ["Zhang", "Bo", ""], ["Zhang", "Ting", ""], ["Zhang", "Pan", ""], ["Bao", "Jianmin", ""], ["Chen", "Dong", ""], ["Zhang", "Zhongfei", ""], ["Wen", "Fang", ""]]}, {"id": "2012.02073", "submitter": "Kha Gia Quach", "authors": "Ngan Le, Kashu Yamazaki, Dat Truong, Kha Gia Quach, Marios Savvides", "title": "A Multi-task Contextual Atrous Residual Network for Brain Tumor\n  Detection & Segmentation", "comments": "Accepted in ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep neural networks have achieved state-of-the-art\nperformance in a variety of recognition and segmentation tasks in medical\nimaging including brain tumor segmentation. We investigate that segmenting a\nbrain tumor is facing to the imbalanced data problem where the number of pixels\nbelonging to the background class (non tumor pixel) is much larger than the\nnumber of pixels belonging to the foreground class (tumor pixel). To address\nthis problem, we propose a multi-task network which is formed as a cascaded\nstructure. Our model consists of two targets, i.e., (i) effectively\ndifferentiate the brain tumor regions and (ii) estimate the brain tumor mask.\nThe first objective is performed by our proposed contextual brain tumor\ndetection network, which plays a role of an attention gate and focuses on the\nregion around brain tumor only while ignoring the far neighbor background which\nis less correlated to the tumor. The second objective is built upon a 3D atrous\nresidual network and under an encode-decode network in order to effectively\nsegment both large and small objects (brain tumor). Our 3D atrous residual\nnetwork is designed with a skip connection to enables the gradient from the\ndeep layers to be directly propagated to shallow layers, thus, features of\ndifferent depths are preserved and used for refining each other. In order to\nincorporate larger contextual information from volume MRI data, our network\nutilizes the 3D atrous convolution with various kernel sizes, which enlarges\nthe receptive field of filters. Our proposed network has been evaluated on\nvarious datasets including BRATS2015, BRATS2017 and BRATS2018 datasets with\nboth validation set and testing set. Our performance has been benchmarked by\nboth region-based metrics and surface-based metrics. We also have conducted\ncomparisons against state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 17:04:16 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Le", "Ngan", ""], ["Yamazaki", "Kashu", ""], ["Truong", "Dat", ""], ["Quach", "Kha Gia", ""], ["Savvides", "Marios", ""]]}, {"id": "2012.02076", "submitter": "Jinyan Wang", "authors": "Jinhuan Duan, Xianxian Li, Shiqi Gao, Jinyan Wang and Zili Zhong", "title": "SSGD: A safe and efficient method of gradient descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the vigorous development of artificial intelligence technology, various\nengineering technology applications have been implemented one after another.\nThe gradient descent method plays an important role in solving various\noptimization problems, due to its simple structure, good stability and easy\nimplementation. In multi-node machine learning system, the gradients usually\nneed to be shared. Shared gradients are generally unsafe. Attackers can obtain\ntraining data simply by knowing the gradient information. In this paper, to\nprevent gradient leakage while keeping the accuracy of model, we propose the\nsuper stochastic gradient descent approach to update parameters by concealing\nthe modulus length of gradient vectors and converting it or them into a unit\nvector. Furthermore, we analyze the security of super stochastic gradient\ndescent approach. Our algorithm can defend against attacks on the gradient.\nExperiment results show that our approach is obviously superior to prevalent\ngradient descent approaches in terms of accuracy, robustness, and adaptability\nto large-scale batches.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 17:09:20 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 04:33:08 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Duan", "Jinhuan", ""], ["Li", "Xianxian", ""], ["Gao", "Shiqi", ""], ["Wang", "Jinyan", ""], ["Zhong", "Zili", ""]]}, {"id": "2012.02094", "submitter": "Alexey Bokhovkin", "authors": "Alexey Bokhovkin, Vladislav Ishimtsev, Emil Bogomolov, Denis Zorin,\n  Alexey Artemov, Evgeny Burnaev, Angela Dai", "title": "Towards Part-Based Understanding of RGB-D Scans", "comments": "https://youtu.be/iuixmPNs4v4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in 3D semantic scene understanding have shown impressive\nprogress in 3D instance segmentation, enabling object-level reasoning about 3D\nscenes; however, a finer-grained understanding is required to enable\ninteractions with objects and their functional understanding. Thus, we propose\nthe task of part-based scene understanding of real-world 3D environments: from\nan RGB-D scan of a scene, we detect objects, and for each object predict its\ndecomposition into geometric part masks, which composed together form the\ncomplete geometry of the observed object. We leverage an intermediary part\ngraph representation to enable robust completion as well as building of part\npriors, which we use to construct the final part mask predictions. Our\nexperiments demonstrate that guiding part understanding through part graph to\npart prior-based predictions significantly outperforms alternative approaches\nto the task of semantic part completion.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 17:30:02 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Bokhovkin", "Alexey", ""], ["Ishimtsev", "Vladislav", ""], ["Bogomolov", "Emil", ""], ["Zorin", "Denis", ""], ["Artemov", "Alexey", ""], ["Burnaev", "Evgeny", ""], ["Dai", "Angela", ""]]}, {"id": "2012.02107", "submitter": "Xiaoding Yuan", "authors": "Xiaoding Yuan, Adam Kortylewski, Yihong Sun and Alan Yuille", "title": "Robust Instance Segmentation through Reasoning about Multi-Object\n  Occlusion", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing complex scenes with Deep Neural Networks is a challenging task,\nparticularly when images contain multiple objects that partially occlude each\nother. Existing approaches to image analysis mostly process objects\nindependently and do not take into account the relative occlusion of nearby\nobjects. In this paper, we propose a deep network for multi-object instance\nsegmentation that is robust to occlusion and can be trained from bounding box\nsupervision only. Our work builds on Compositional Networks, which learn a\ngenerative model of neural feature activations to locate occluders and to\nclassify objects based on their non-occluded parts. We extend their generative\nmodel to include multiple objects and introduce a framework for efficient\ninference in challenging occlusion scenarios. In particular, we obtain\nfeed-forward predictions of the object classes and their instance and occluder\nsegmentations. We introduce an Occlusion Reasoning Module (ORM) that locates\nerroneous segmentations and estimates the occlusion order to correct them. The\nimproved segmentation masks are, in turn, integrated into the network in a\ntop-down manner to improve the image classification. Our experiments on the\nKITTI INStance dataset (KINS) and a synthetic occlusion dataset demonstrate the\neffectiveness and robustness of our model at multi-object instance segmentation\nunder occlusion. Code is publically available at\nhttps://github.com/XD7479/Multi-Object-Occlusion.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 17:41:55 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 13:22:12 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 13:32:03 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Yuan", "Xiaoding", ""], ["Kortylewski", "Adam", ""], ["Sun", "Yihong", ""], ["Yuille", "Alan", ""]]}, {"id": "2012.02109", "submitter": "Tae Soo Kim", "authors": "Tae Soo Kim, Gregory D. Hager", "title": "SAFCAR: Structured Attention Fusion for Compositional Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework for compositional action recognition -- i.e.\naction recognition where the labels are composed out of simpler components such\nas subjects, atomic-actions and objects. The main challenge in compositional\naction recognition is that there is a combinatorially large set of possible\nactions that can be composed using basic components. However, compositionality\nalso provides a structure that can be exploited. To do so, we develop and test\na novel Structured Attention Fusion (SAF) self-attention mechanism to combine\ninformation from object detections, which capture the time-series structure of\nan action, with visual cues that capture contextual information. We show that\nour approach recognizes novel verb-noun compositions more effectively than\ncurrent state of the art systems, and it generalizes to unseen action\ncategories quite efficiently from only a few labeled examples. We validate our\napproach on the challenging Something-Else tasks from the\nSomething-Something-V2 dataset. We further show that our framework is flexible\nand can generalize to a new domain by showing competitive results on the\nCharades-Fewshot dataset.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 17:45:01 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 21:15:37 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Kim", "Tae Soo", ""], ["Hager", "Gregory D.", ""]]}, {"id": "2012.02124", "submitter": "Senthil Yogamani", "authors": "Hazem Rashed, Eslam Mohamed, Ganesh Sistu, Varun Ravi Kumar, Ciaran\n  Eising, Ahmad El-Sallab and Senthil Yogamani", "title": "Generalized Object Detection on Fisheye Cameras for Autonomous Driving:\n  Dataset, Representations and Baseline", "comments": "Camera ready version. Accepted for presentation at Winter Conference\n  on Applications of Computer Vision 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a comprehensively studied problem in autonomous driving.\nHowever, it has been relatively less explored in the case of fisheye cameras.\nThe standard bounding box fails in fisheye cameras due to the strong radial\ndistortion, particularly in the image's periphery. We explore better\nrepresentations like oriented bounding box, ellipse, and generic polygon for\nobject detection in fisheye images in this work. We use the IoU metric to\ncompare these representations using accurate instance segmentation ground\ntruth. We design a novel curved bounding box model that has optimal properties\nfor fisheye distortion models. We also design a curvature adaptive perimeter\nsampling method for obtaining polygon vertices, improving relative mAP score by\n4.9% compared to uniform sampling. Overall, the proposed polygon model improves\nmIoU relative accuracy by 40.3%. It is the first detailed study on object\ndetection on fisheye cameras for autonomous driving scenarios to the best of\nour knowledge. The dataset comprising of 10,000 images along with all the\nobject representations ground truth will be made public to encourage further\nresearch. We summarize our work in a short video with qualitative results at\nhttps://youtu.be/iLkOzvJpL-A.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:00:16 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Rashed", "Hazem", ""], ["Mohamed", "Eslam", ""], ["Sistu", "Ganesh", ""], ["Kumar", "Varun Ravi", ""], ["Eising", "Ciaran", ""], ["El-Sallab", "Ahmad", ""], ["Yogamani", "Senthil", ""]]}, {"id": "2012.02128", "submitter": "Jing Su", "authors": "Jing Su, Qingyun Dai, Frank Guerin, Mian Zhou", "title": "BERT-hLSTMs: BERT and Hierarchical LSTMs for Visual Storytelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual storytelling is a creative and challenging task, aiming to\nautomatically generate a story-like description for a sequence of images. The\ndescriptions generated by previous visual storytelling approaches lack\ncoherence because they use word-level sequence generation methods and do not\nadequately consider sentence-level dependencies. To tackle this problem, we\npropose a novel hierarchical visual storytelling framework which separately\nmodels sentence-level and word-level semantics. We use the transformer-based\nBERT to obtain embeddings for sentences and words. We then employ a\nhierarchical LSTM network: the bottom LSTM receives as input the sentence\nvector representation from BERT, to learn the dependencies between the\nsentences corresponding to images, and the top LSTM is responsible for\ngenerating the corresponding word vector representations, taking input from the\nbottom LSTM. Experimental results demonstrate that our model outperforms most\nclosely related baselines under automatic evaluation metrics BLEU and CIDEr,\nand also show the effectiveness of our method with human evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:07:28 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Su", "Jing", ""], ["Dai", "Qingyun", ""], ["Guerin", "Frank", ""], ["Zhou", "Mian", ""]]}, {"id": "2012.02148", "submitter": "Tiffany Yau", "authors": "Tiffany Yau, Saber Malekmohammadi, Amir Rasouli, Peter Lakner, Mohsen\n  Rohani, Jun Luo", "title": "Graph-SIM: A Graph-based Spatiotemporal Interaction Modelling for\n  Pedestrian Action Prediction", "comments": "7 pages, 3 figures, 4 tables, accepted at ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most crucial yet challenging tasks for autonomous vehicles in\nurban environments is predicting the future behaviour of nearby pedestrians,\nespecially at points of crossing. Predicting behaviour depends on many social\nand environmental factors, particularly interactions between road users.\nCapturing such interactions requires a global view of the scene and dynamics of\nthe road users in three-dimensional space. This information, however, is\nmissing from the current pedestrian behaviour benchmark datasets. Motivated by\nthese challenges, we propose 1) a novel graph-based model for predicting\npedestrian crossing action. Our method models pedestrians' interactions with\nnearby road users through clustering and relative importance weighting of\ninteractions using features obtained from the bird's-eye-view. 2) We introduce\na new dataset that provides 3D bounding box and pedestrian behavioural\nannotations for the existing nuScenes dataset. On the new data, our approach\nachieves state-of-the-art performance by improving on various metrics by more\nthan 15% in comparison to existing methods. The dataset is available at\nhttps://github.com/huawei-noah/datasets/PePScenes.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:28:27 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 15:58:01 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 14:12:10 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Yau", "Tiffany", ""], ["Malekmohammadi", "Saber", ""], ["Rasouli", "Amir", ""], ["Lakner", "Peter", ""], ["Rohani", "Mohsen", ""], ["Luo", "Jun", ""]]}, {"id": "2012.02157", "submitter": "Dokhyam Hoshen", "authors": "Dokhyam Hoshen", "title": "MakeupBag: Disentangling Makeup Extraction and Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces MakeupBag, a novel method for automatic makeup style\ntransfer. Our proposed technique can transfer a new makeup style from a\nreference face image to another previously unseen facial photograph. We solve\nmakeup disentanglement and facial makeup application as separable objectives,\nin contrast to other current deep methods that entangle the two tasks.\nMakeupBag presents a significant advantage for our approach as it allows\ncustomization and pixel specific modification of the extracted makeup style,\nwhich is not possible using current methods. Extensive experiments, both\nqualitative and numerical, are conducted demonstrating the high quality and\naccuracy of the images produced by our method. Furthermore, in contrast to most\nother current methods, MakeupBag tackles both classical and extreme and costume\nmakeup transfer. In a comparative analysis, MakeupBag is shown to outperform\ncurrent state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:44:24 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Hoshen", "Dokhyam", ""]]}, {"id": "2012.02162", "submitter": "Mehdi Noroozi", "authors": "Mehdi Noroozi", "title": "Self-labeled Conditional GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel and fully unsupervised framework for\nconditional GAN training in which labels are automatically obtained from data.\nWe incorporate a clustering network into the standard conditional GAN framework\nthat plays against the discriminator. With the generator, it aims to find a\nshared structured mapping for associating pseudo-labels with the real and fake\nimages. Our generator outperforms unconditional GANs in terms of FID with\nsignificant margins on large scale datasets like ImageNet and LSUN. It also\noutperforms class conditional GANs trained on human labels on CIFAR10 and\nCIFAR100 where fine-grained annotations or a large number of samples per class\nare not available. Additionally, our clustering network exceeds the\nstate-of-the-art on CIFAR100 clustering.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:46:46 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Noroozi", "Mehdi", ""]]}, {"id": "2012.02166", "submitter": "Shir Gur", "authors": "Shir Gur, Ameen Ali, Lior Wolf", "title": "Visualization of Supervised and Self-Supervised Neural Networks via\n  Attribution Guided Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural network visualization techniques mark image locations by their\nrelevancy to the network's classification. Existing methods are effective in\nhighlighting the regions that affect the resulting classification the most.\nHowever, as we show, these methods are limited in their ability to identify the\nsupport for alternative classifications, an effect we name {\\em the saliency\nbias} hypothesis. In this work, we integrate two lines of research:\ngradient-based methods and attribution-based methods, and develop an algorithm\nthat provides per-class explainability. The algorithm back-projects the per\npixel local influence, in a manner that is guided by the local attributions,\nwhile correcting for salient features that would otherwise bias the\nexplanation. In an extensive battery of experiments, we demonstrate the ability\nof our methods to class-specific visualization, and not just the predicted\nlabel. Remarkably, the method obtains state of the art results in benchmarks\nthat are commonly applied to gradient-based methods as well as in those that\nare employed mostly for evaluating attribution methods. Using a new\nunsupervised procedure, our method is also successful in demonstrating that\nself-supervised methods learn semantic information.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:48:39 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Gur", "Shir", ""], ["Ali", "Ameen", ""], ["Wolf", "Lior", ""]]}, {"id": "2012.02175", "submitter": "Md Sirajus Salekin", "authors": "Md Sirajus Salekin, Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi,\n  Thao Ho, Yu Sun", "title": "Multimodal Spatio-Temporal Deep Learning Approach for Neonatal\n  Postoperative Pain Assessment", "comments": "Accepted in Computers in Biology and Medicine, 2020", "journal-ref": null, "doi": "10.1016/j.compbiomed.2020.104150", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The current practice for assessing neonatal postoperative pain relies on\nbedside caregivers. This practice is subjective, inconsistent, slow, and\ndiscontinuous. To develop a reliable medical interpretation, several automated\napproaches have been proposed to enhance the current practice. These approaches\nare unimodal and focus mainly on assessing neonatal procedural (acute) pain. As\npain is a multimodal emotion that is often expressed through multiple\nmodalities, the multimodal assessment of pain is necessary especially in case\nof postoperative (acute prolonged) pain. Additionally, spatio-temporal analysis\nis more stable over time and has been proven to be highly effective at\nminimizing misclassification errors. In this paper, we present a novel\nmultimodal spatio-temporal approach that integrates visual and vocal signals\nand uses them for assessing neonatal postoperative pain. We conduct\ncomprehensive experiments to investigate the effectiveness of the proposed\napproach. We compare the performance of the multimodal and unimodal\npostoperative pain assessment, and measure the impact of temporal information\nintegration. The experimental results, on a real-world dataset, show that the\nproposed multimodal spatio-temporal approach achieves the highest AUC (0.87)\nand accuracy (79%), which are on average 6.67% and 6.33% higher than unimodal\napproaches. The results also show that the integration of temporal information\nmarkedly improves the performance as compared to the non-temporal approach as\nit captures changes in the pain dynamic. These results demonstrate that the\nproposed approach can be used as a viable alternative to manual assessment,\nwhich would tread a path toward fully automated pain monitoring in clinical\nsettings, point-of-care testing, and homes.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:52:35 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Salekin", "Md Sirajus", ""], ["Zamzmi", "Ghada", ""], ["Goldgof", "Dmitry", ""], ["Kasturi", "Rangachar", ""], ["Ho", "Thao", ""], ["Sun", "Yu", ""]]}, {"id": "2012.02177", "submitter": "Arda D\\\"uz\\c{c}eker", "authors": "Arda D\\\"uz\\c{c}eker, Silvano Galliani, Christoph Vogel, Pablo\n  Speciale, Mihai Dusmanu, Marc Pollefeys", "title": "DeepVideoMVS: Multi-View Stereo on Video with Recurrent Spatio-Temporal\n  Fusion", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an online multi-view depth prediction approach on posed video\nstreams, where the scene geometry information computed in the previous time\nsteps is propagated to the current time step in an efficient and geometrically\nplausible way. The backbone of our approach is a real-time capable, lightweight\nencoder-decoder that relies on cost volumes computed from pairs of images. We\nextend it by placing a ConvLSTM cell at the bottleneck layer, which compresses\nan arbitrary amount of past information in its states. The novelty lies in\npropagating the hidden state of the cell by accounting for the viewpoint\nchanges between time steps. At a given time step, we warp the previous hidden\nstate into the current camera plane using the previous depth prediction. Our\nextension brings only a small overhead of computation time and memory\nconsumption, while improving the depth predictions significantly. As a result,\nwe outperform the existing state-of-the-art multi-view stereo methods on most\nof the evaluated metrics in hundreds of indoor scenes while maintaining a\nreal-time performance. Code available:\nhttps://github.com/ardaduz/deep-video-mvs\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:54:03 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 17:59:04 GMT"}, {"version": "v3", "created": "Wed, 21 Jul 2021 18:12:44 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["D\u00fcz\u00e7eker", "Arda", ""], ["Galliani", "Silvano", ""], ["Vogel", "Christoph", ""], ["Speciale", "Pablo", ""], ["Dusmanu", "Mihai", ""], ["Pollefeys", "Marc", ""]]}, {"id": "2012.02181", "submitter": "Kelvin C.K. Chan", "authors": "Kelvin C.K. Chan, Xintao Wang, Ke Yu, Chao Dong, Chen Change Loy", "title": "BasicVSR: The Search for Essential Components in Video Super-Resolution\n  and Beyond", "comments": "CVPR 2021 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video super-resolution (VSR) approaches tend to have more components than the\nimage counterparts as they need to exploit the additional temporal dimension.\nComplex designs are not uncommon. In this study, we wish to untangle the knots\nand reconsider some most essential components for VSR guided by four basic\nfunctionalities, i.e., Propagation, Alignment, Aggregation, and Upsampling. By\nreusing some existing components added with minimal redesigns, we show a\nsuccinct pipeline, BasicVSR, that achieves appealing improvements in terms of\nspeed and restoration quality in comparison to many state-of-the-art\nalgorithms. We conduct systematic analysis to explain how such gain can be\nobtained and discuss the pitfalls. We further show the extensibility of\nBasicVSR by presenting an information-refill mechanism and a coupled\npropagation scheme to facilitate information aggregation. The BasicVSR and its\nextension, IconVSR, can serve as strong baselines for future VSR approaches.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:56:14 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 11:23:38 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Chan", "Kelvin C. K.", ""], ["Wang", "Xintao", ""], ["Yu", "Ke", ""], ["Dong", "Chao", ""], ["Loy", "Chen Change", ""]]}, {"id": "2012.02189", "submitter": "Matthew Tancik", "authors": "Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P.\n  Srinivasan, Jonathan T. Barron, Ren Ng", "title": "Learned Initializations for Optimizing Coordinate-Based Neural\n  Representations", "comments": "Project page: https://www.matthewtancik.com/learnit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordinate-based neural representations have shown significant promise as an\nalternative to discrete, array-based representations for complex low\ndimensional signals. However, optimizing a coordinate-based network from\nrandomly initialized weights for each new signal is inefficient. We propose\napplying standard meta-learning algorithms to learn the initial weight\nparameters for these fully-connected networks based on the underlying class of\nsignals being represented (e.g., images of faces or 3D models of chairs).\nDespite requiring only a minor change in implementation, using these learned\ninitial weights enables faster convergence during optimization and can serve as\na strong prior over the signal class being modeled, resulting in better\ngeneralization when only partial observations of a given signal are available.\nWe explore these benefits across a variety of tasks, including representing 2D\nimages, reconstructing CT scans, and recovering 3D shapes and scenes from 2D\nimage observations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:59:52 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 17:11:16 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Tancik", "Matthew", ""], ["Mildenhall", "Ben", ""], ["Wang", "Terrance", ""], ["Schmidt", "Divi", ""], ["Srinivasan", "Pratul P.", ""], ["Barron", "Jonathan T.", ""], ["Ng", "Ren", ""]]}, {"id": "2012.02190", "submitter": "Alex Yu", "authors": "Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa", "title": "pixelNeRF: Neural Radiance Fields from One or Few Images", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose pixelNeRF, a learning framework that predicts a continuous neural\nscene representation conditioned on one or few input images. The existing\napproach for constructing neural radiance fields involves optimizing the\nrepresentation to every scene independently, requiring many calibrated views\nand significant compute time. We take a step towards resolving these\nshortcomings by introducing an architecture that conditions a NeRF on image\ninputs in a fully convolutional manner. This allows the network to be trained\nacross multiple scenes to learn a scene prior, enabling it to perform novel\nview synthesis in a feed-forward manner from a sparse set of views (as few as\none). Leveraging the volume rendering approach of NeRF, our model can be\ntrained directly from images with no explicit 3D supervision. We conduct\nextensive experiments on ShapeNet benchmarks for single image novel view\nsynthesis tasks with held-out objects as well as entire unseen categories. We\nfurther demonstrate the flexibility of pixelNeRF by demonstrating it on\nmulti-object ShapeNet scenes and real scenes from the DTU dataset. In all\ncases, pixelNeRF outperforms current state-of-the-art baselines for novel view\nsynthesis and single image 3D reconstruction. For the video and code, please\nvisit the project website: https://alexyu.net/pixelnerf\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:59:54 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 05:41:26 GMT"}, {"version": "v3", "created": "Sun, 30 May 2021 17:52:24 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Yu", "Alex", ""], ["Ye", "Vickie", ""], ["Tancik", "Matthew", ""], ["Kanazawa", "Angjoo", ""]]}, {"id": "2012.02206", "submitter": "Dave Zhenyu Chen", "authors": "Dave Zhenyu Chen, Ali Gholami, Matthias Nie{\\ss}ner, Angel X. Chang", "title": "Scan2Cap: Context-aware Dense Captioning in RGB-D Scans", "comments": "Video: https://youtu.be/AgmIpDbwTCY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the task of dense captioning in 3D scans from commodity RGB-D\nsensors. As input, we assume a point cloud of a 3D scene; the expected output\nis the bounding boxes along with the descriptions for the underlying objects.\nTo address the 3D object detection and description problems, we propose\nScan2Cap, an end-to-end trained method, to detect objects in the input scene\nand describe them in natural language. We use an attention mechanism that\ngenerates descriptive tokens while referring to the related components in the\nlocal context. To reflect object relations (i.e. relative spatial relations) in\nthe generated captions, we use a message passing graph module to facilitate\nlearning object relation features. Our method can effectively localize and\ndescribe 3D objects in scenes from the ScanRefer dataset, outperforming 2D\nbaseline methods by a significant margin (27.61% CiDEr@0.5IoUimprovement).\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 19:00:05 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Chen", "Dave Zhenyu", ""], ["Gholami", "Ali", ""], ["Nie\u00dfner", "Matthias", ""], ["Chang", "Angel X.", ""]]}, {"id": "2012.02218", "submitter": "Md Saif Hassan Onim", "authors": "Md. Saif Hassan Onim, Muhaiminul Islam Akash, Mahmudul Haque, Raiyan\n  Ibne Hafiz", "title": "Traffic Surveillance using Vehicle License Plate Detection and\n  Recognition in Bangladesh", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer vision coupled with Deep Learning (DL) techniques bring out a\nsubstantial prospect in the field of traffic control, monitoring and law\nenforcing activities. This paper presents a YOLOv4 object detection model in\nwhich the Convolutional Neural Network (CNN) is trained and tuned for detecting\nthe license plate of the vehicles of Bangladesh and recognizing characters\nusing tesseract from the detected license plates. Here we also present a\nGraphical User Interface (GUI) based on Tkinter, a python package. The license\nplate detection model is trained with mean average precision (mAP) of 90.50%\nand performed in a single TESLA T4 GPU with an average of 14 frames per second\n(fps) on real time video footage.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 19:16:49 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Onim", "Md. Saif Hassan", ""], ["Akash", "Muhaiminul Islam", ""], ["Haque", "Mahmudul", ""], ["Hafiz", "Raiyan Ibne", ""]]}, {"id": "2012.02224", "submitter": "Funda Durupinar Babur", "authors": "Funda Durupinar", "title": "Personality-Driven Gaze Animation with Conditional Generative\n  Adversarial Networks", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a generative adversarial learning approach to synthesize gaze\nbehavior of a given personality. We train the model using an existing data set\nthat comprises eye-tracking data and personality traits of 42 participants\nperforming an everyday task. Given the values of Big-Five personality traits\n(openness, conscientiousness, extroversion, agreeableness, and neuroticism),\nour model generates time series data consisting of gaze target, blinking times,\nand pupil dimensions. We use the generated data to synthesize the gaze motion\nof virtual agents on a game engine.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 00:31:45 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Durupinar", "Funda", ""]]}, {"id": "2012.02228", "submitter": "Sachin Mehta", "authors": "Sachin Mehta and Amit Kumar and Fitsum Reda and Varun Nasery and\n  Vikram Mulukutla and Rakesh Ranjan and Vikas Chandra", "title": "EVRNet: Efficient Video Restoration on Edge Devices", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video transmission applications (e.g., conferencing) are gaining momentum,\nespecially in times of global health pandemic. Video signals are transmitted\nover lossy channels, resulting in low-quality received signals. To restore\nvideos on recipient edge devices in real-time, we introduce an efficient video\nrestoration network, EVRNet. EVRNet efficiently allocates parameters inside the\nnetwork using alignment, differential, and fusion modules. With extensive\nexperiments on video restoration tasks (deblocking, denoising, and\nsuper-resolution), we demonstrate that EVRNet delivers competitive performance\nto existing methods with significantly fewer parameters and MACs. For example,\nEVRNet has 260 times fewer parameters and 958 times fewer MACs than enhanced\ndeformable convolution-based video restoration network (EDVR) for 4 times video\nsuper-resolution while its SSIM score is 0.018 less than EDVR. We also\nevaluated the performance of EVRNet under multiple distortions on unseen\ndataset to demonstrate its ability in modeling variable-length sequences under\nboth camera and object motion.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 19:42:38 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Mehta", "Sachin", ""], ["Kumar", "Amit", ""], ["Reda", "Fitsum", ""], ["Nasery", "Varun", ""], ["Mulukutla", "Vikram", ""], ["Ranjan", "Rakesh", ""], ["Chandra", "Vikas", ""]]}, {"id": "2012.02234", "submitter": "Almabrok Essa", "authors": "Khalfalla Awedat and Almabrok Essa", "title": "COVID-CLNet: COVID-19 Detection with Compressive Deep Learning\n  Approaches", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the most serious global health threat is COVID-19 pandemic. The\nemphasis on improving diagnosis and increasing the diagnostic capability helps\nstopping its spread significantly. Therefore, to assist the radiologist or\nother medical professional to detect and identify the COVID-19 cases in the\nshortest possible time, we propose a computer-aided detection (CADe) system\nthat uses the computed tomography (CT) scan images. This proposed boosted deep\nlearning network (CLNet) is based on the implementation of Deep Learning (DL)\nnetworks as a complementary to the Compressive Learning (CL). We utilize our\ninception feature extraction technique in the measurement domain using CL to\nrepresent the data features into a new space with less dimensionality before\naccessing the Convolutional Neural Network. All original features have been\ncontributed equally in the new space using a sensing matrix. Experiments\nperformed on different compressed methods show promising results for COVID-19\ndetection. In addition, our novel weighted method based on different sensing\nmatrices that used to capture boosted features demonstrates an improvement in\nthe performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 19:56:48 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Awedat", "Khalfalla", ""], ["Essa", "Almabrok", ""]]}, {"id": "2012.02238", "submitter": "Muhammad E. H. Chowdhury", "authors": "Tawsifur Rahman, Amith Khandakar, Yazan Qiblawey, Anas Tahir, Serkan\n  Kiranyaz, Saad Bin Abul Kashem, Mohammad Tariqul Islam, Somaya Al Maadeed,\n  Susu M Zughaier, Muhammad Salman Khan, Muhammad E. H. Chowdhury", "title": "Exploring the Effect of Image Enhancement Techniques on COVID-19\n  Detection using Chest X-rays Images", "comments": "34 pages, 6 Tables, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of computer-aided diagnosis in the reliable and fast detection of\ncoronavirus disease (COVID-19) has become a necessity to prevent the spread of\nthe virus during the pandemic to ease the burden on the medical infrastructure.\nChest X-ray (CXR) imaging has several advantages over other imaging techniques\nas it is cheap, easily accessible, fast and portable. This paper explores the\neffect of various popular image enhancement techniques and states the effect of\neach of them on the detection performance. We have compiled the largest X-ray\ndataset called COVQU-20, consisting of 18,479 normal, non-COVID lung opacity\nand COVID-19 CXR images. To the best of our knowledge, this is the largest\npublic COVID positive database. Ground glass opacity is the common symptom\nreported in COVID-19 pneumonia patients and so a mixture of 3616 COVID-19, 6012\nnon-COVID lung opacity, and 8851 normal chest X-ray images were used to create\nthis dataset. Five different image enhancement techniques: histogram\nequalization, contrast limited adaptive histogram equalization, image\ncomplement, gamma correction, and Balance Contrast Enhancement Technique were\nused to improve COVID-19 detection accuracy. Six different Convolutional Neural\nNetworks (CNNs) were investigated in this study. Gamma correction technique\noutperforms other enhancement techniques in detecting COVID-19 from standard\nand segmented lung CXR images. The accuracy, precision, sensitivity, f1-score,\nand specificity in the detection of COVID-19 with gamma correction on CXR\nimages were 96.29%, 96.28%, 96.29%, 96.28% and 96.27% respectively. The\naccuracy, precision, sensitivity, F1-score, and specificity were 95.11 %, 94.55\n%, 94.56 %, 94.53 % and 95.59 % respectively for segmented lung images. The\nproposed approach with very high and comparable performance will boost the fast\nand robust COVID-19 detection using chest X-ray images.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 20:58:27 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Rahman", "Tawsifur", ""], ["Khandakar", "Amith", ""], ["Qiblawey", "Yazan", ""], ["Tahir", "Anas", ""], ["Kiranyaz", "Serkan", ""], ["Kashem", "Saad Bin Abul", ""], ["Islam", "Mohammad Tariqul", ""], ["Maadeed", "Somaya Al", ""], ["Zughaier", "Susu M", ""], ["Khan", "Muhammad Salman", ""], ["Chowdhury", "Muhammad E. H.", ""]]}, {"id": "2012.02248", "submitter": "Wanda Benesova", "authors": "Martin Stano, Wanda Benesova, Lukas Samuel Martak", "title": "Explaining Predictions of Deep Neural Classifier via Activation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many practical applications, deep neural networks have been typically\ndeployed to operate as a black box predictor. Despite the high amount of work\non interpretability and high demand on the reliability of these systems, they\ntypically still have to include a human actor in the loop, to validate the\ndecisions and handle unpredictable failures and unexpected corner cases. This\nis true in particular for failure-critical application domains, such as medical\ndiagnosis. We present a novel approach to explain and support an interpretation\nof the decision-making process to a human expert operating a deep learning\nsystem based on Convolutional Neural Network (CNN). By modeling activation\nstatistics on selected layers of a trained CNN via Gaussian Mixture Models\n(GMM), we develop a novel perceptual code in binary vector space that describes\nhow the input sample is processed by the CNN. By measuring distances between\npairs of samples in this perceptual encoding space, for any new input sample,\nwe can now retrieve a set of most perceptually similar and dissimilar samples\nfrom an existing atlas of labeled samples, to support and clarify the decision\nmade by the CNN model. Possible uses of this approach include for example\nComputer-Aided Diagnosis (CAD) systems working with medical imaging data, such\nas Magnetic Resonance Imaging (MRI) or Computed Tomography (CT) scans. We\ndemonstrate the viability of our method in the domain of medical imaging for\npatient condition diagnosis, as the proposed decision explanation method via\nsimilar ground truth domain examples (e.g. from existing diagnosis archives)\nwill be interpretable by the operating medical personnel. Our results indicate\nthat our method is capable of detecting distinct prediction strategies that\nenable us to identify the most similar predictions from an existing atlas.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 20:36:19 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Stano", "Martin", ""], ["Benesova", "Wanda", ""], ["Martak", "Lukas Samuel", ""]]}, {"id": "2012.02264", "submitter": "Ying Chen", "authors": "Ying Chen, Xu Ouyang, Kaiyue Zhu, Gady Agam", "title": "Domain Adaptation on Semantic Segmentation for Aerial Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic segmentation has achieved significant advances in recent years.\nWhile deep neural networks perform semantic segmentation well, their success\nrely on pixel level supervision which is expensive and time-consuming. Further,\ntraining using data from one domain may not generalize well to data from a new\ndomain due to a domain gap between data distributions in the different domains.\nThis domain gap is particularly evident in aerial images where visual\nappearance depends on the type of environment imaged, season, weather, and time\nof day when the environment is imaged. Subsequently, this distribution gap\nleads to severe accuracy loss when using a pretrained segmentation model to\nanalyze new data with different characteristics. In this paper, we propose a\nnovel unsupervised domain adaptation framework to address domain shift in the\ncontext of aerial semantic image segmentation. To this end, we solve the\nproblem of domain shift by learn the soft label distribution difference between\nthe source and target domains. Further, we also apply entropy minimization on\nthe target domain to produce high-confident prediction rather than using\nhigh-confident prediction by pseudo-labeling. We demonstrate the effectiveness\nof our domain adaptation framework using the challenge image segmentation\ndataset of ISPRS, and show improvement over state-of-the-art methods in terms\nof various metrics.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 20:58:27 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 16:09:12 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Chen", "Ying", ""], ["Ouyang", "Xu", ""], ["Zhu", "Kaiyue", ""], ["Agam", "Gady", ""]]}, {"id": "2012.02275", "submitter": "Susmit Jha", "authors": "Karan Sikka, Indranil Sur, Susmit Jha, Anirban Roy and Ajay Divakaran", "title": "Detecting Trojaned DNNs Using Counterfactual Attributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We target the problem of detecting Trojans or backdoors in DNNs. Such models\nbehave normally with typical inputs but produce specific incorrect predictions\nfor inputs poisoned with a Trojan trigger. Our approach is based on a novel\nobservation that the trigger behavior depends on a few ghost neurons that\nactivate on trigger pattern and exhibit abnormally higher relative attribution\nfor wrong decisions when activated. Further, these trigger neurons are also\nactive on normal inputs of the target class. Thus, we use counterfactual\nattributions to localize these ghost neurons from clean inputs and then\nincrementally excite them to observe changes in the model's accuracy. We use\nthis information for Trojan detection by using a deep set encoder that enables\ninvariance to the number of model classes, architecture, etc. Our approach is\nimplemented in the TrinityAI tool that exploits the synergies between\ntrustworthiness, resilience, and interpretability challenges in deep learning.\nWe evaluate our approach on benchmarks with high diversity in model\narchitectures, triggers, etc. We show consistent gains (+10%) over\nstate-of-the-art methods that rely on the susceptibility of the DNN to specific\nadversarial attacks, which in turn requires strong assumptions on the nature of\nthe Trojan attack.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 21:21:33 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Sikka", "Karan", ""], ["Sur", "Indranil", ""], ["Jha", "Susmit", ""], ["Roy", "Anirban", ""], ["Divakaran", "Ajay", ""]]}, {"id": "2012.02278", "submitter": "Jingxiong Li", "authors": "Jingxiong Li, Yaqi Wang, Shuai Wang, Jun Wang, Jun Liu, Qun Jin,\n  Lingling Sun", "title": "Multiscale Attention Guided Network for COVID-19 Diagnosis Using Chest\n  X-ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus disease 2019 (COVID-19) is one of the most destructive pandemic\nafter millennium, forcing the world to tackle a health crisis. Automated lung\ninfections classification using chest X-ray (CXR) images could strengthen\ndiagnostic capability when handling COVID-19. However, classifying COVID-19\nfrom pneumonia cases using CXR image is a difficult task because of shared\nspatial characteristics, high feature variation and contrast diversity between\ncases. Moreover, massive data collection is impractical for a newly emerged\ndisease, which limited the performance of data thirsty deep learning models. To\naddress these challenges, Multiscale Attention Guided deep network with Soft\nDistance regularization (MAG-SD) is proposed to automatically classify COVID-19\nfrom pneumonia CXR images. In MAG-SD, MA-Net is used to produce prediction\nvector and attention from multiscale feature maps. To improve the robustness of\ntrained model and relieve the shortage of training data, attention guided\naugmentations along with a soft distance regularization are posed, which aims\nat generating meaningful augmentations and reduce noise. Our multiscale\nattention model achieves better classification performance on our pneumonia CXR\nimage dataset. Plentiful experiments are proposed for MAG-SD which demonstrates\nits unique advantage in pneumonia classification over cutting-edge models. The\ncode is available at https://github.com/JasonLeeGHub/MAG-SD.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 11:20:10 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 07:10:15 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Li", "Jingxiong", ""], ["Wang", "Yaqi", ""], ["Wang", "Shuai", ""], ["Wang", "Jun", ""], ["Liu", "Jun", ""], ["Jin", "Qun", ""], ["Sun", "Lingling", ""]]}, {"id": "2012.02310", "submitter": "Chunhua Shen", "authors": "Zhi Tian, Chunhua Shen, Xinlong Wang, Hao Chen", "title": "BoxInst: High-Performance Instance Segmentation with Box Annotations", "comments": "Code is available at: https://git.io/AdelaiDet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a high-performance method that can achieve mask-level instance\nsegmentation with only bounding-box annotations for training. While this\nsetting has been studied in the literature, here we show significantly stronger\nperformance with a simple design (e.g., dramatically improving previous best\nreported mask AP of 21.1% in Hsu et al. (2019) to 31.6% on the COCO dataset).\nOur core idea is to redesign the loss of learning masks in instance\nsegmentation, with no modification to the segmentation network itself. The new\nloss functions can supervise the mask training without relying on mask\nannotations. This is made possible with two loss terms, namely, 1) a surrogate\nterm that minimizes the discrepancy between the projections of the ground-truth\nbox and the predicted mask; 2) a pairwise loss that can exploit the prior that\nproximal pixels with similar colors are very likely to have the same category\nlabel. Experiments demonstrate that the redesigned mask loss can yield\nsurprisingly high-quality instance masks with only box annotations. For\nexample, without using any mask annotations, with a ResNet-101 backbone and 3x\ntraining schedule, we achieve 33.2% mask AP on COCO test-dev split (vs. 39.1%\nof the fully supervised counterpart). Our excellent experiment results on COCO\nand Pascal VOC indicate that our method dramatically narrows the performance\ngap between weakly and fully supervised instance segmentation.\n  Code is available at: https://git.io/AdelaiDet\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 22:27:55 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Tian", "Zhi", ""], ["Shen", "Chunhua", ""], ["Wang", "Xinlong", ""], ["Chen", "Hao", ""]]}, {"id": "2012.02319", "submitter": "Ningcheng Li", "authors": "Ningcheng Li, Jonathan Wakim, Yilun Koethe, Timothy Huber, Terence\n  Gade, Stephen Hunt, Brian Park", "title": "Multicenter Assessment of Augmented Reality Registration Methods for\n  Image-guided Interventions", "comments": "16 pages, 5 figures (plus 1 supplemental figure), and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To evaluate manual and automatic registration times as well as\naccuracy with augmented reality during alignment of a holographic 3-dimensional\n(3D) model onto the real-world environment.\n  Method: 18 participants in various stages of clinical training across two\nacademic centers registered a 3D CT phantom model onto a CT grid using the\nHoloLens 2 augmented reality headset 3 consecutive times. Registration times\nand accuracy were compared among different registration methods (hand gesture,\nXbox controller, and automatic registration), levels of clinical experience,\nand consecutive attempts. Registration times were also compared with prior\nHoloLens 1 data.\n  Results: Mean aggregate manual registration times were 27.7, 24.3, and 72.8\nseconds for one-handed gesture, two-handed gesture, and Xbox controller,\nrespectively; mean automatic registration time was 5.3s (ANOVA p<0.0001). No\nsignificant difference in registration times was found among attendings,\nresidents and fellows, and medical students (p>0.05). Significant improvements\nin registration times were detected across consecutive attempts using hand\ngestures (p<0.01). Compared with previously reported HoloLens 1 experience,\nhand gesture registration times were 81.7% faster (p<0.05). Registration\naccuracies were not significantly different across manual registration methods,\nmeasuring at 5.9, 9.5, and 8.6 mm with one-handed gesture, two-handed gesture,\nand Xbox controller, respectively (p>0.05).\n  Conclusions: Manual registration times decreased significantly with updated\nhand gesture maneuvers on HoloLens 2 versus HoloLens 1, approaching the\nregistration times of automatic registration and outperforming Xbox controller\nmediated registration. These results will encourage wider clinical integration\nof HoloLens 2 in procedural medical care.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 23:05:48 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Li", "Ningcheng", ""], ["Wakim", "Jonathan", ""], ["Koethe", "Yilun", ""], ["Huber", "Timothy", ""], ["Gade", "Terence", ""], ["Hunt", "Stephen", ""], ["Park", "Brian", ""]]}, {"id": "2012.02337", "submitter": "Fatemeh Sadat Saleh", "authors": "Fatemeh Saleh, Sadegh Aliakbarian, Hamid Rezatofighi, Mathieu\n  Salzmann, Stephen Gould", "title": "Probabilistic Tracklet Scoring and Inpainting for Multiple Object\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent advances in multiple object tracking (MOT), achieved by\njoint detection and tracking, dealing with long occlusions remains a challenge.\nThis is due to the fact that such techniques tend to ignore the long-term\nmotion information. In this paper, we introduce a probabilistic autoregressive\nmotion model to score tracklet proposals by directly measuring their\nlikelihood. This is achieved by training our model to learn the underlying\ndistribution of natural tracklets. As such, our model allows us not only to\nassign new detections to existing tracklets, but also to inpaint a tracklet\nwhen an object has been lost for a long time, e.g., due to occlusion, by\nsampling tracklets so as to fill the gap caused by misdetections. Our\nexperiments demonstrate the superiority of our approach at tracking objects in\nchallenging sequences; it outperforms the state of the art in most standard MOT\nmetrics on multiple MOT benchmark datasets, including MOT16, MOT17, and MOT20.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 23:59:27 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 04:11:29 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Saleh", "Fatemeh", ""], ["Aliakbarian", "Sadegh", ""], ["Rezatofighi", "Hamid", ""], ["Salzmann", "Mathieu", ""], ["Gould", "Stephen", ""]]}, {"id": "2012.02339", "submitter": "Edwin Ng", "authors": "Edwin G. Ng, Bo Pang, Piyush Sharma, Radu Soricut", "title": "Understanding Guided Image Captioning Performance across Domains", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image captioning models generally lack the capability to take into account\nuser interest, and usually default to global descriptions that try to balance\nreadability, informativeness, and information overload. On the other hand, VQA\nmodels generally lack the ability to provide long descriptive answers, while\nexpecting the textual question to be quite precise. We present a method to\ncontrol the concepts that an image caption should focus on, using an additional\ninput called the guiding text that refers to either groundable or ungroundable\nconcepts in the image. Our model consists of a Transformer-based multimodal\nencoder that uses the guiding text together with global and object-level image\nfeatures to derive early-fusion representations used to generate the guided\ncaption. While models trained on Visual Genome data have an in-domain advantage\nof fitting well when guided with automatic object labels, we find that guided\ncaptioning models trained on Conceptual Captions generalize better on\nout-of-domain images and guiding texts. Our human-evaluation results indicate\nthat attempting in-the-wild guided image captioning requires access to large,\nunrestricted-domain training datasets, and that increased style diversity (even\nwithout increasing vocabulary size) is a key factor for improved performance.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 00:05:02 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Ng", "Edwin G.", ""], ["Pang", "Bo", ""], ["Sharma", "Piyush", ""], ["Soricut", "Radu", ""]]}, {"id": "2012.02346", "submitter": "Takashi Matsubara", "authors": "Takumi Kimura, Takashi Matsubara, Kuniaki Uehara", "title": "ChartPointFlow for Topology-Aware 3D Point Cloud Generation", "comments": "9 pages + appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A point cloud serves as a representation of the surface of a\nthree-dimensional shape. Deep generative models have been adapted to model\ntheir variations typically by a map from a ball-like set of latent variables.\nHowever, previous approaches have not paid much attention to the topological\nstructure of a point cloud; a continuous map cannot express the varying number\nof holes and intersections. Moreover, a point cloud is often composed of\nmultiple subparts, and it is also hardly expressed. In this paper, we propose\nChartPointFlow, which is a flow-based generative model with multiple latent\nlabels. By maximizing the mutual information, a map conditioned by a label is\nassigned to a continuous subset of a given point cloud, like a chart of a\nmanifold. This enables our proposed model to preserve the topological structure\nwith clear boundaries, while previous approaches tend to suffer from blurs and\nto fail in generating holes. Experimental results demonstrate that\nChartPointFlow achieves the state-of-the-art performance in generation and\nreconstruction among sampling-based point cloud generators.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 00:49:25 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Kimura", "Takumi", ""], ["Matsubara", "Takashi", ""], ["Uehara", "Kuniaki", ""]]}, {"id": "2012.02356", "submitter": "Pratyay Banerjee", "authors": "Pratyay Banerjee, Tejas Gokhale, Yezhou Yang, Chitta Baral", "title": "WeaQA: Weak Supervision via Captions for Visual Question Answering", "comments": "Accepted in Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methodologies for training visual question answering (VQA) models assume the\navailability of datasets with human-annotated \\textit{Image-Question-Answer}\n(I-Q-A) triplets. This has led to heavy reliance on datasets and a lack of\ngeneralization to new types of questions and scenes. Linguistic priors along\nwith biases and errors due to annotator subjectivity have been shown to\npercolate into VQA models trained on such samples. We study whether models can\nbe trained without any human-annotated Q-A pairs, but only with images and\ntheir associated textual descriptions or captions. We present a method to train\nmodels with synthetic Q-A pairs generated procedurally from captions.\nAdditionally, we demonstrate the efficacy of spatial-pyramid image patches as a\nsimple but effective alternative to dense and costly object bounding box\nannotations used in existing VQA models. Our experiments on three VQA\nbenchmarks demonstrate the efficacy of this weakly-supervised approach,\nespecially on the VQA-CP challenge, which tests performance under changing\nlinguistic priors.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 01:22:05 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 07:09:48 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Banerjee", "Pratyay", ""], ["Gokhale", "Tejas", ""], ["Yang", "Yezhou", ""], ["Baral", "Chitta", ""]]}, {"id": "2012.02364", "submitter": "Tharindu Fernando", "authors": "Tharindu Fernando, Harshala Gammulle, Simon Denman, Sridha Sridharan,\n  Clinton Fookes", "title": "Deep Learning for Medical Anomaly Detection -- A Survey", "comments": "Preprint submitted to ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning-based medical anomaly detection is an important problem that\nhas been extensively studied. Numerous approaches have been proposed across\nvarious medical application domains and we observe several similarities across\nthese distinct applications. Despite this comparability, we observe a lack of\nstructured organisation of these diverse research applications such that their\nadvantages and limitations can be studied. The principal aim of this survey is\nto provide a thorough theoretical analysis of popular deep learning techniques\nin medical anomaly detection. In particular, we contribute a coherent and\nsystematic review of state-of-the-art techniques, comparing and contrasting\ntheir architectural differences as well as training algorithms. Furthermore, we\nprovide a comprehensive overview of deep model interpretation strategies that\ncan be used to interpret model decisions. In addition, we outline the key\nlimitations of existing deep medical anomaly detection techniques and propose\nkey research directions for further investigation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 02:09:37 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 04:43:59 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Fernando", "Tharindu", ""], ["Gammulle", "Harshala", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "2012.02366", "submitter": "Yiming Cui", "authors": "Dongfang Liu, Yiming Cui, Liqi Yan, Christos Mousas, Baijian Yang,\n  Yingjie Chen", "title": "DenserNet: Weakly Supervised Visual Localization Using Multi-scale\n  Feature Aggregation", "comments": "Proceeding with The Thirty-Fifth AAAI Conference on Artificial\n  Intelligence (AAAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a Denser Feature Network (DenserNet) for visual\nlocalization. Our work provides three principal contributions. First, we\ndevelop a convolutional neural network (CNN) architecture which aggregates\nfeature maps at different semantic levels for image representations. Using\ndenser feature maps, our method can produce more keypoint features and increase\nimage retrieval accuracy. Second, our model is trained end-to-end without\npixel-level annotation other than positive and negative GPS-tagged image pairs.\nWe use a weakly supervised triplet ranking loss to learn discriminative\nfeatures and encourage keypoint feature repeatability for image representation.\nFinally, our method is computationally efficient as our architecture has shared\nfeatures and parameters during computation. Our method can perform accurate\nlarge-scale localization under challenging conditions while remaining the\ncomputational constraint. Extensive experiment results indicate that our method\nsets a new state-of-the-art on four challenging large-scale localization\nbenchmarks and three image retrieval benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 02:16:47 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 15:46:43 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2020 13:59:32 GMT"}, {"version": "v4", "created": "Thu, 11 Mar 2021 21:06:01 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Liu", "Dongfang", ""], ["Cui", "Yiming", ""], ["Yan", "Liqi", ""], ["Mousas", "Christos", ""], ["Yang", "Baijian", ""], ["Chen", "Yingjie", ""]]}, {"id": "2012.02368", "submitter": "Gongbo Liang", "authors": "Gongbo Liang, Yuanyuan Su, Sheng-Chieh Lin, Yu Zhang, Yuanyuan Zhang,\n  Nathan Jacobs", "title": "Optical Wavelength Guided Self-Supervised Feature Learning For Galaxy\n  Cluster Richness Estimate", "comments": "Accepted to NeurIPS 2020 Workshop on Machine Learning and the\n  Physical Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.CO cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most galaxies in the nearby Universe are gravitationally bound to a cluster\nor group of galaxies. Their optical contents, such as optical richness, are\ncrucial for understanding the co-evolution of galaxies and large-scale\nstructures in modern astronomy and cosmology. The determination of optical\nrichness can be challenging. We propose a self-supervised approach for\nestimating optical richness from multi-band optical images. The method uses the\ndata properties of the multi-band optical images for pre-training, which\nenables learning feature representations from a large but unlabeled dataset. We\napply the proposed method to the Sloan Digital Sky Survey. The result shows our\nestimate of optical richness lowers the mean absolute error and intrinsic\nscatter by 11.84% and 20.78%, respectively, while reducing the need for labeled\ntraining data by up to 60%. We believe the proposed method will benefit\nastronomy and cosmology, where a large number of unlabeled multi-band images\nare available, but acquiring image labels is costly.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 02:21:00 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Liang", "Gongbo", ""], ["Su", "Yuanyuan", ""], ["Lin", "Sheng-Chieh", ""], ["Zhang", "Yu", ""], ["Zhang", "Yuanyuan", ""], ["Jacobs", "Nathan", ""]]}, {"id": "2012.02371", "submitter": "Xl Li", "authors": "Songhai Zhang and Xiangli Li and Yingtian Liu and Hongbo Fu", "title": "Scale-aware Insertion of Virtual Objects in Monocular Videos", "comments": "Accepted at ISMAR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a scale-aware method for inserting virtual objects\nwith proper sizes into monocular videos. To tackle the scale ambiguity problem\nof geometry recovery from monocular videos, we estimate the global scale\nobjects in a video with a Bayesian approach incorporating the size priors of\nobjects, where the scene objects sizes should strictly conform to the same\nglobal scale and the possibilities of global scales are maximized according to\nthe size distribution of object categories. To do so, we propose a dataset of\nsizes of object categories: Metric-Tree, a hierarchical representation of sizes\nof more than 900 object categories with the corresponding images. To handle the\nincompleteness of objects recovered from videos, we propose a novel scale\nestimation method that extracts plausible dimensions of objects for scale\noptimization. Experiments have shown that our method for scale estimation\nperforms better than the state-of-the-art methods, and has considerable\nvalidity and robustness for different video scenes. Metric-Tree has been made\navailable at: https://metric-tree.github.io\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 02:25:24 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Zhang", "Songhai", ""], ["Li", "Xiangli", ""], ["Liu", "Yingtian", ""], ["Fu", "Hongbo", ""]]}, {"id": "2012.02374", "submitter": "Shivangi Yadav", "authors": "Shivangi Yadav and Arun Ross", "title": "CIT-GAN: Cyclic Image Translation Generative Adversarial Network With\n  Application in Iris Presentation Attack Detection", "comments": "10 pages (8 pages + 2 reference pages) and 10 figures", "journal-ref": "WACV 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we propose a novel Cyclic Image Translation Generative\nAdversarial Network (CIT-GAN) for multi-domain style transfer. To facilitate\nthis, we introduce a Styling Network that has the capability to learn style\ncharacteristics of each domain represented in the training dataset. The Styling\nNetwork helps the generator to drive the translation of images from a source\ndomain to a reference domain and generate synthetic images with style\ncharacteristics of the reference domain. The learned style characteristics for\neach domain depend on both the style loss and domain classification loss. This\ninduces variability in style characteristics within each domain. The proposed\nCIT-GAN is used in the context of iris presentation attack detection (PAD) to\ngenerate synthetic presentation attack (PA) samples for classes that are\nunder-represented in the training set. Evaluation using current\nstate-of-the-art iris PAD methods demonstrates the efficacy of using such\nsynthetically generated PA samples for training PAD methods. Further, the\nquality of the synthetically generated samples is evaluated using Frechet\nInception Distance (FID) score. Results show that the quality of synthetic\nimages generated by the proposed method is superior to that of other competing\nmethods, including StarGan.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 02:44:54 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Yadav", "Shivangi", ""], ["Ross", "Arun", ""]]}, {"id": "2012.02381", "submitter": "Leilei Cao", "authors": "Leilei Cao, Tong Yang, Yixu Wang, Bo Yan, Yandong Guo", "title": "Generator Pyramid for High-Resolution Image Inpainting", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inpainting high-resolution images with large holes challenges existing deep\nlearning based image inpainting methods. We present a novel framework --\nPyramidFill for high-resolution image inpainting task, which explicitly\ndisentangles content completion and texture synthesis. PyramidFill attempts to\ncomplete the content of unknown regions in a lower-resolution image, and\nsynthesis the textures of unknown regions in a higher-resolution image,\nprogressively. Thus, our model consists of a pyramid of fully convolutional\nGANs, wherein the content GAN is responsible for completing contents in the\nlowest-resolution masked image, and each texture GAN is responsible for\nsynthesizing textures in a higher-resolution image. Since completing contents\nand synthesising textures demand different abilities from generators, we\ncustomize different architectures for the content GAN and texture GAN.\nExperiments on multiple datasets including CelebA-HQ, Places2 and a new natural\nscenery dataset (NSHQ) with different resolutions demonstrate that PyramidFill\ngenerates higher-quality inpainting results than the state-of-the-art methods.\nTo better assess high-resolution image inpainting methods, we will release\nNSHQ, high-quality natural scenery images with high-resolution\n1920$\\times$1080.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 03:27:48 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Cao", "Leilei", ""], ["Yang", "Tong", ""], ["Wang", "Yixu", ""], ["Yan", "Bo", ""], ["Guo", "Yandong", ""]]}, {"id": "2012.02383", "submitter": "Ke Yan", "authors": "Ke Yan, Jinzheng Cai, Dakai Jin, Shun Miao, Adam P. Harrison, Dazhou\n  Guo, Youbao Tang, Jing Xiao, Jingjing Lu, Le Lu", "title": "Self-supervised Learning of Pixel-wise Anatomical Embeddings in\n  Radiological Images", "comments": "16 pages including supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Radiological images such as computed tomography (CT) and X-rays render\nanatomy with intrinsic structures. Being able to reliably locate the same\nanatomical or semantic structure across varying images is a fundamental task in\nmedical image analysis. In principle it is possible to use landmark detection\nor semantic segmentation for this task, but to work well these require large\nnumbers of labeled data for each anatomical structure and sub-structure of\ninterest. A more universal approach would discover the intrinsic structure from\nunlabeled images. We introduce such an approach, called Self-supervised\nAnatomical eMbedding (SAM). SAM generates semantic embeddings for each image\npixel that describes its anatomical location or body part. To produce such\nembeddings, we propose a pixel-level contrastive learning framework. A\ncoarse-to-fine strategy ensures both global and local anatomical information\nare encoded. Negative sample selection strategies are designed to enhance the\ndiscriminability among different body parts. Using SAM, one can label any point\nof interest on a template image, and then locate the same body part in other\nimages by simple nearest neighbor searching. We demonstrate the effectiveness\nof SAM in multiple tasks with 2D and 3D image modalities. On a chest CT dataset\nwith 19 landmarks, SAM outperforms widely-used registration algorithms while\nbeing 200 times faster. On two X-ray datasets, SAM, with only one labeled\ntemplate image, outperforms supervised methods trained on 50 labeled images. We\nalso apply SAM on whole-body follow-up lesion matching in CT and obtain an\naccuracy of 91%.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 03:31:20 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Yan", "Ke", ""], ["Cai", "Jinzheng", ""], ["Jin", "Dakai", ""], ["Miao", "Shun", ""], ["Harrison", "Adam P.", ""], ["Guo", "Dazhou", ""], ["Tang", "Youbao", ""], ["Xiao", "Jing", ""], ["Lu", "Jingjing", ""], ["Lu", "Le", ""]]}, {"id": "2012.02407", "submitter": "Cheng Peng", "authors": "Cheng Peng, Haofu Liao, Gina Wong, Jiebo Luo, Shaohua Kevin Zhou, Rama\n  Chellappa", "title": "XraySyn: Realistic View Synthesis From a Single Radiograph Through CT\n  Priors", "comments": "Accepted to AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A radiograph visualizes the internal anatomy of a patient through the use of\nX-ray, which projects 3D information onto a 2D plane. Hence, radiograph\nanalysis naturally requires physicians to relate the prior about 3D human\nanatomy to 2D radiographs. Synthesizing novel radiographic views in a small\nrange can assist physicians in interpreting anatomy more reliably; however,\nradiograph view synthesis is heavily ill-posed, lacking in paired data, and\nlacking in differentiable operations to leverage learning-based approaches. To\naddress these problems, we use Computed Tomography (CT) for radiograph\nsimulation and design a differentiable projection algorithm, which enables us\nto achieve geometrically consistent transformations between the radiography and\nCT domains. Our method, XraySyn, can synthesize novel views on real radiographs\nthrough a combination of realistic simulation and finetuning on real\nradiographs. To the best of our knowledge, this is the first work on radiograph\nview synthesis. We show that by gaining an understanding of radiography in 3D\nspace, our method can be applied to radiograph bone extraction and suppression\nwithout groundtruth bone labels.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 05:08:53 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Peng", "Cheng", ""], ["Liao", "Haofu", ""], ["Wong", "Gina", ""], ["Luo", "Jiebo", ""], ["Zhou", "Shaohua Kevin", ""], ["Chellappa", "Rama", ""]]}, {"id": "2012.02408", "submitter": "Parshwa Shah", "authors": "Parshwa Shah, Arpit Garg and Vandit Gajjar", "title": "PeR-ViS: Person Retrieval in Video Surveillance using Semantic\n  Description", "comments": "10 pages, 6 figures, 3 tables; Human Activity Detection in\n  multi-camera, Continuous, long-duration Video (HADCV'21)under the IEEE Winter\n  Conf. on Applications of Computer Vision (WACV), Virtual Conference, January\n  5, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A person is usually characterized by descriptors like age, gender, height,\ncloth type, pattern, color, etc. Such descriptors are known as attributes\nand/or soft-biometrics. They link the semantic gap between a person's\ndescription and retrieval in video surveillance. Retrieving a specific person\nwith the query of semantic description has an important application in video\nsurveillance. Using computer vision to fully automate the person retrieval task\nhas been gathering interest within the research community. However, the\nCurrent, trend mainly focuses on retrieving persons with image-based queries,\nwhich have major limitations for practical usage. Instead of using an image\nquery, in this paper, we study the problem of person retrieval in video\nsurveillance with a semantic description. To solve this problem, we develop a\ndeep learning-based cascade filtering approach (PeR-ViS), which uses Mask R-CNN\n[14] (person detection and instance segmentation) and DenseNet-161 [16]\n(soft-biometric classification). On the standard person retrieval dataset of\nSoftBioSearch [6], we achieve 0.566 Average IoU and 0.792 %w $IoU > 0.4$,\nsurpassing the current state-of-the-art by a large margin. We hope our simple,\nreproducible, and effective approach will help ease future research in the\ndomain of person retrieval in video surveillance. The source code and\npretrained weights available at https://parshwa1999.github.io/PeR-ViS/.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 05:11:21 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Shah", "Parshwa", ""], ["Garg", "Arpit", ""], ["Gajjar", "Vandit", ""]]}, {"id": "2012.02426", "submitter": "Junwei Liang", "authors": "Junwei Liang, Liangliang Cao, Xuehan Xiong, Ting Yu, Alexander\n  Hauptmann", "title": "Spatial-Temporal Alignment Network for Action Recognition and Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies how to introduce viewpoint-invariant feature\nrepresentations that can help action recognition and detection. Although we\nhave witnessed great progress of action recognition in the past decade, it\nremains challenging yet interesting how to efficiently model the geometric\nvariations in large scale datasets. This paper proposes a novel\nSpatial-Temporal Alignment Network (STAN) that aims to learn geometric\ninvariant representations for action recognition and action detection. The STAN\nmodel is very light-weighted and generic, which could be plugged into existing\naction recognition models like ResNet3D and the SlowFast with a very low extra\ncomputational cost. We test our STAN model extensively on AVA, Kinetics-400,\nAVA-Kinetics, Charades, and Charades-Ego datasets. The experimental results\nshow that the STAN model can consistently improve the state of the arts in both\naction detection and action recognition tasks. We will release our data, models\nand code.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 06:23:40 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Liang", "Junwei", ""], ["Cao", "Liangliang", ""], ["Xiong", "Xuehan", ""], ["Yu", "Ting", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "2012.02452", "submitter": "Haoyu Chu", "authors": "Haoyu Chu, Shikui Wei, Yao Zhao", "title": "Towards Natural Robustness Against Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent studies have shown that deep neural networks are vulnerable to\nadversarial examples, but most of the methods proposed to defense adversarial\nexamples cannot solve this problem fundamentally. In this paper, we\ntheoretically prove that there is an upper bound for neural networks with\nidentity mappings to constrain the error caused by adversarial noises. However,\nin actual computations, this kind of neural network no longer holds any upper\nbound and is therefore susceptible to adversarial examples. Following similar\nprocedures, we explain why adversarial examples can fool other deep neural\nnetworks with skip connections. Furthermore, we demonstrate that a new family\nof deep neural networks called Neural ODEs (Chen et al., 2018) holds a weaker\nupper bound. This weaker upper bound prevents the amount of change in the\nresult from being too large. Thus, Neural ODEs have natural robustness against\nadversarial examples. We evaluate the performance of Neural ODEs compared with\nResNet under three white-box adversarial attacks (FGSM, PGD, DI2-FGSM) and one\nblack-box adversarial attack (Boundary Attack). Finally, we show that the\nnatural robustness of Neural ODEs is even better than the robustness of neural\nnetworks that are trained with adversarial training methods, such as TRADES and\nYOPO.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 08:12:38 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Chu", "Haoyu", ""], ["Wei", "Shikui", ""], ["Zhao", "Yao", ""]]}, {"id": "2012.02459", "submitter": "Jie Yang", "authors": "Jie Yang, Lin Gao, Qingyang Tan, Yihua Huang, Shihong Xia and Yu-Kun\n  Lai", "title": "Multiscale Mesh Deformation Component Analysis with Attention-based\n  Autoencoders", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformation component analysis is a fundamental problem in geometry\nprocessing and shape understanding. Existing approaches mainly extract\ndeformation components in local regions at a similar scale while deformations\nof real-world objects are usually distributed in a multi-scale manner. In this\npaper, we propose a novel method to exact multiscale deformation components\nautomatically with a stacked attention-based autoencoder. The attention\nmechanism is designed to learn to softly weight multi-scale deformation\ncomponents in active deformation regions, and the stacked attention-based\nautoencoder is learned to represent the deformation components at different\nscales. Quantitative and qualitative evaluations show that our method\noutperforms state-of-the-art methods. Furthermore, with the multiscale\ndeformation components extracted by our method, the user can edit shapes in a\ncoarse-to-fine fashion which facilitates effective modeling of new shapes.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 08:30:57 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Yang", "Jie", ""], ["Gao", "Lin", ""], ["Tan", "Qingyang", ""], ["Huang", "Yihua", ""], ["Xia", "Shihong", ""], ["Lai", "Yu-Kun", ""]]}, {"id": "2012.02463", "submitter": "Duc Toan Bui", "authors": "Ngan Le, Trung Le, Kashu Yamazaki, Toan Duc Bui, Khoa Luu, Marios\n  Savides", "title": "Offset Curves Loss for Imbalanced Problem in Medical Segmentation", "comments": "ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image segmentation has played an important role in medical analysis\nand widely developed for many clinical applications. Deep learning-based\napproaches have achieved high performance in semantic segmentation but they are\nlimited to pixel-wise setting and imbalanced classes data problem. In this\npaper, we tackle those limitations by developing a new deep learning-based\nmodel which takes into account both higher feature level i.e. region inside\ncontour, intermediate feature level i.e. offset curves around the contour and\nlower feature level i.e. contour. Our proposed Offset Curves (OsC) loss\nconsists of three main fitting terms. The first fitting term focuses on\npixel-wise level segmentation whereas the second fitting term acts as attention\nmodel which pays attention to the area around the boundaries (offset curves).\nThe third terms plays a role as regularization term which takes the length of\nboundaries into account. We evaluate our proposed OsC loss on both 2D network\nand 3D network. Two common medical datasets, i.e. retina DRIVE and brain tumor\nBRATS 2018 datasets are used to benchmark our proposed loss performance. The\nexperiments have shown that our proposed OsC loss function outperforms other\nmainstream loss functions such as Cross-Entropy, Dice, Focal on the most common\nsegmentation networks Unet, FCN.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 08:35:21 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Le", "Ngan", ""], ["Le", "Trung", ""], ["Yamazaki", "Kashu", ""], ["Bui", "Toan Duc", ""], ["Luu", "Khoa", ""], ["Savides", "Marios", ""]]}, {"id": "2012.02472", "submitter": "Hengrong Lan", "authors": "Hengrong Lan, Changchun Yang, Fei Gao", "title": "A Jointed Feature Fusion Framework for Photoacoustic Reconstruction", "comments": "under the peer-review procedure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photoacoustic (PA) computed tomography (PACT) reconstructs the initial\npressure distribution from raw PA signals. The standard reconstruction of\nmedical image could cause the artifacts due to interferences or ill-posed\nsetup. Recently, deep learning has been used to reconstruct the PA image with\nill-posed conditions. Most works remove the artifacts from image domain, and\ncompensate the limited-view from dataset. In this paper, we propose a jointed\nfeature fusion framework (JEFF-Net) based on deep learning to reconstruct the\nPA image using limited-view data. The cross-domain features from limited-view\nposition-wise data and the reconstructed image are fused by a backtracked\nsupervision. Specifically, our results could generate superior performance,\nwhose artifacts are drastically reduced in the output compared to ground-truth\n(full-view reconstructed result). In this paper, a quarter position-wise data\n(32 channels) is fed into model, which outputs another 3-quarters-view data (96\nchannels). Moreover, two novel losses are designed to restrain the artifacts by\nsufficiently manipulating superposed data. The numerical and in-vivo results\nhave demonstrated the superior performance of our method to reconstruct the\nfull-view image without artifacts. Finally, quantitative evaluations show that\nour proposed method outperformed the ground-truth in some metrics.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 08:53:46 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 01:54:11 GMT"}, {"version": "v3", "created": "Tue, 25 May 2021 15:44:00 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Lan", "Hengrong", ""], ["Yang", "Changchun", ""], ["Gao", "Fei", ""]]}, {"id": "2012.02478", "submitter": "Rita Pucci", "authors": "Rita Pucci, Christian Micheloni, Gian Luca Foresti, Niki Martinel", "title": "Is It a Plausible Colour? UCapsNet for Image Colourisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human beings can imagine the colours of a grayscale image with no particular\neffort thanks to their ability of semantic feature extraction. Can an\nautonomous system achieve that? Can it hallucinate plausible and vibrant\ncolours? This is the colourisation problem. Different from existing works\nrelying on convolutional neural network models pre-trained with supervision, we\ncast such colourisation problem as a self-supervised learning task. We tackle\nthe problem with the introduction of a novel architecture based on Capsules\ntrained following the adversarial learning paradigm. Capsule networks are able\nto extract a semantic representation of the entities in the image but loose\ndetails about their spatial information, which is important for colourising a\ngrayscale image. Thus our UCapsNet structure comes with an encoding phase that\nextracts entities through capsules and spatial details through convolutional\nneural networks. A decoding phase merges the entity features with the spatial\nfeatures to hallucinate a plausible colour version of the input datum. Results\non the ImageNet benchmark show that our approach is able to generate more\nvibrant and plausible colours than exiting solutions and achieves superior\nperformance than models pre-trained with supervision.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 09:07:13 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Pucci", "Rita", ""], ["Micheloni", "Christian", ""], ["Foresti", "Gian Luca", ""], ["Martinel", "Niki", ""]]}, {"id": "2012.02493", "submitter": "Songfang Han", "authors": "Songfang Han, Jiayuan Gu, Kaichun Mo, Li Yi, Siyu Hu, Xuejin Chen, Hao\n  Su", "title": "Compositionally Generalizable 3D Structure Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single-image 3D shape reconstruction is an important and long-standing\nproblem in computer vision. A plethora of existing works is constantly pushing\nthe state-of-the-art performance in the deep learning era. However, there\nremains a much more difficult and under-explored issue on how to generalize the\nlearned skills over unseen object categories that have very different shape\ngeometry distributions. In this paper, we bring in the concept of compositional\ngeneralizability and propose a novel framework that could better generalize to\nthese unseen categories. We factorize the 3D shape reconstruction problem into\nproper sub-problems, each of which is tackled by a carefully designed neural\nsub-module with generalizability concerns. The intuition behind our formulation\nis that object parts (slates and cylindrical parts), their relationships\n(adjacency and translation symmetry), and shape substructures (T-junctions and\na symmetric group of parts) are mostly shared across object categories, even\nthough object geometries may look very different (e.g. chairs and cabinets).\nExperiments on PartNet show that we achieve superior performance than\nstate-of-the-art. This validates our problem factorization and network designs.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 09:53:14 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 20:34:48 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 02:15:38 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Han", "Songfang", ""], ["Gu", "Jiayuan", ""], ["Mo", "Kaichun", ""], ["Yi", "Li", ""], ["Hu", "Siyu", ""], ["Chen", "Xuejin", ""], ["Su", "Hao", ""]]}, {"id": "2012.02495", "submitter": "Frauke Wilm", "authors": "Frauke Wilm, Christof A. Bertram, Christian Marzahl, Alexander Bartel,\n  Taryn A. Donovan, Charles-Antoine Assenmacher, Kathrin Becker, Mark Bennett,\n  Sarah Corner, Brieuc Cossic, Daniela Denk, Martina Dettwiler, Beatriz Garcia\n  Gonzalez, Corinne Gurtner, Annika Lehmbecker, Sophie Merz, Stephanie Plog,\n  Anja Schmidt, Rebecca C. Smedley, Marco Tecilla, Tuddow Thaiwong, Katharina\n  Breininger, Matti Kiupel, Andreas Maier, Robert Klopfleisch, Marc Aubreville", "title": "How Many Annotators Do We Need? -- A Study on the Influence of\n  Inter-Observer Variability on the Reliability of Automatic Mitotic Figure\n  Assessment", "comments": "Due to data inconsistencies experiments had to be repeated with a\n  reduced number of annotators (17 in version 1). All findings of the previous\n  version were reproducible. 7 pages, 2 figures, accepted at BVM workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density of mitotic figures in histologic sections is a prognostically\nrelevant characteristic for many tumours. Due to high inter-pathologist\nvariability, deep learning-based algorithms are a promising solution to improve\ntumour prognostication. Pathologists are the gold standard for database\ndevelopment, however, labelling errors may hamper development of accurate\nalgorithms. In the present work we evaluated the benefit of multi-expert\nconsensus (n = 3, 5, 7, 9, 11) on algorithmic performance. While training with\nindividual databases resulted in highly variable F$_1$ scores, performance was\nnotably increased and more consistent when using the consensus of three\nannotators. Adding more annotators only resulted in minor improvements. We\nconclude that databases by few pathologists and high label accuracy may be the\nbest compromise between high algorithmic performance and time investment.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 09:54:00 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 12:03:01 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Wilm", "Frauke", ""], ["Bertram", "Christof A.", ""], ["Marzahl", "Christian", ""], ["Bartel", "Alexander", ""], ["Donovan", "Taryn A.", ""], ["Assenmacher", "Charles-Antoine", ""], ["Becker", "Kathrin", ""], ["Bennett", "Mark", ""], ["Corner", "Sarah", ""], ["Cossic", "Brieuc", ""], ["Denk", "Daniela", ""], ["Dettwiler", "Martina", ""], ["Gonzalez", "Beatriz Garcia", ""], ["Gurtner", "Corinne", ""], ["Lehmbecker", "Annika", ""], ["Merz", "Sophie", ""], ["Plog", "Stephanie", ""], ["Schmidt", "Anja", ""], ["Smedley", "Rebecca C.", ""], ["Tecilla", "Marco", ""], ["Thaiwong", "Tuddow", ""], ["Breininger", "Katharina", ""], ["Kiupel", "Matti", ""], ["Maier", "Andreas", ""], ["Klopfleisch", "Robert", ""], ["Aubreville", "Marc", ""]]}, {"id": "2012.02502", "submitter": "Marco Boschi", "authors": "Marco Boschi, Luigi Di Stefano, Martino Alessandrini", "title": "SAFFIRE: System for Autonomous Feature Filtering and Intelligent ROI\n  Estimation", "comments": "14 pages, 23 figures, 2 tables", "journal-ref": "ICPR International Workshops and Challenges. ICPR 2021. Lecture\n  Notes in Computer Science, vol 12664, pp 552-565", "doi": "10.1007/978-3-030-68799-1_40", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work introduces a new framework, named SAFFIRE, to automatically extract\na dominant recurrent image pattern from a set of image samples. Such a pattern\nshall be used to eliminate pose variations between samples, which is a common\nrequirement in many computer vision and machine learning tasks. The framework\nis specialized here in the context of a machine vision system for automated\nproduct inspection. Here, it is customary to ask the user for the\nidentification of an anchor pattern, to be used by the automated system to\nnormalize data before further processing. Yet, this is a very sensitive\noperation which is intrinsically subjective and requires high expertise.\nHereto, SAFFIRE provides a unique and disruptive framework for unsupervised\nidentification of an optimal anchor pattern in a way which is fully transparent\nto the user. SAFFIRE is thoroughly validated on several realistic case studies\nfor a machine vision inspection pipeline.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 10:07:24 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 20:29:14 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Boschi", "Marco", ""], ["Di Stefano", "Luigi", ""], ["Alessandrini", "Martino", ""]]}, {"id": "2012.02512", "submitter": "Davide Cozzolino", "authors": "Davide Cozzolino and Andreas R\\\"ossler and Justus Thies and Matthias\n  Nie{\\ss}ner and Luisa Verdoliva", "title": "ID-Reveal: Identity-aware DeepFake Video Detection", "comments": "Video: https://www.youtube.com/watch?v=RsFxsOLvRdY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in DeepFake forgery detection is that state-of-the-art\nalgorithms are mostly trained to detect a specific fake method. As a result,\nthese approaches show poor generalization across different types of facial\nmanipulations, e.g., from face swapping to facial reenactment. To this end, we\nintroduce ID-Reveal, a new approach that learns temporal facial features,\nspecific of how a person moves while talking, by means of metric learning\ncoupled with an adversarial training strategy. The advantage is that we do not\nneed any training data of fakes, but only train on real videos. Moreover, we\nutilize high-level semantic features, which enables robustess to widespread and\ndisruptive forms of post-processing. We perform a thorough experimental\nanalysis on several publicly available benchmarks. Compared to state of the\nart, our method improves generalization and is more robust to low-quality\nvideos, that are usually spread over social networks. In particular, we obtain\nan average improvement of more than 15% in terms of accuracy for facial\nreenactment on high compressed videos.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 10:43:16 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 07:43:18 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Cozzolino", "Davide", ""], ["R\u00f6ssler", "Andreas", ""], ["Thies", "Justus", ""], ["Nie\u00dfner", "Matthias", ""], ["Verdoliva", "Luisa", ""]]}, {"id": "2012.02515", "submitter": "Pravan Omprakash", "authors": "Mohit Raghavendra, Pravan Omprakash, B R Mukesh, Sowmya Kamath", "title": "AuthNet: A Deep Learning based Authentication Mechanism using Temporal\n  Facial Feature Movements", "comments": "2-page version accepted in AAAI-21 Student Abstract and Poster\n  Program", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biometric systems based on Machine learning and Deep learning are being\nextensively used as authentication mechanisms in resource-constrained\nenvironments like smartphones and other small computing devices. These\nAI-powered facial recognition mechanisms have gained enormous popularity in\nrecent years due to their transparent, contact-less and non-invasive nature.\nWhile they are effective to a large extent, there are ways to gain unauthorized\naccess using photographs, masks, glasses, etc. In this paper, we propose an\nalternative authentication mechanism that uses both facial recognition and the\nunique movements of that particular face while uttering a password, that is,\nthe temporal facial feature movements. The proposed model is not inhibited by\nlanguage barriers because a user can set a password in any language. When\nevaluated on the standard MIRACL-VC1 dataset, the proposed model achieved an\naccuracy of 98.1%, underscoring its effectiveness as an effective and robust\nsystem. The proposed method is also data-efficient since the model gave good\nresults even when trained with only 10 positive video samples. The competence\nof the training of the network is also demonstrated by benchmarking the\nproposed system against various compounded Facial recognition and Lip reading\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 10:46:12 GMT"}, {"version": "v2", "created": "Sat, 19 Dec 2020 08:52:14 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Raghavendra", "Mohit", ""], ["Omprakash", "Pravan", ""], ["Mukesh", "B R", ""], ["Kamath", "Sowmya", ""]]}, {"id": "2012.02516", "submitter": "Patrick Esser", "authors": "Patrick Esser and Robin Rombach and Bj\\\"orn Ommer", "title": "A Note on Data Biases in Generative Models", "comments": "Extended Abstract for the NeurIPS 2020 Workshop on Machine Learning\n  for Creativity and Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is tempting to think that machines are less prone to unfairness and\nprejudice. However, machine learning approaches compute their outputs based on\ndata. While biases can enter at any stage of the development pipeline, models\nare particularly receptive to mirror biases of the datasets they are trained on\nand therefore do not necessarily reflect truths about the world but, primarily,\ntruths about the data. To raise awareness about the relationship between modern\nalgorithms and the data that shape them, we use a conditional invertible neural\nnetwork to disentangle the dataset-specific information from the information\nwhich is shared across different datasets. In this way, we can project the same\nimage onto different datasets, thereby revealing their inherent biases. We use\nthis methodology to (i) investigate the impact of dataset quality on the\nperformance of generative models, (ii) show how societal biases of datasets are\nreplicated by generative models, and (iii) present creative applications\nthrough unpaired transfer between diverse datasets such as photographs, oil\nportraits, and animes. Our code and an interactive demonstration are available\nat https://github.com/CompVis/net2net .\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 10:46:37 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Esser", "Patrick", ""], ["Rombach", "Robin", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "2012.02525", "submitter": "Qizhang Li", "authors": "Qizhang Li, Yiwen Guo, Hao Chen", "title": "Practical No-box Adversarial Attacks against DNNs", "comments": "Accepted by NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of adversarial vulnerabilities of deep neural networks (DNNs) has\nprogressed rapidly. Existing attacks require either internal access (to the\narchitecture, parameters, or training set of the victim model) or external\naccess (to query the model). However, both the access may be infeasible or\nexpensive in many scenarios. We investigate no-box adversarial examples, where\nthe attacker can neither access the model information or the training set nor\nquery the model. Instead, the attacker can only gather a small number of\nexamples from the same problem domain as that of the victim model. Such a\nstronger threat model greatly expands the applicability of adversarial attacks.\nWe propose three mechanisms for training with a very small dataset (on the\norder of tens of examples) and find that prototypical reconstruction is the\nmost effective. Our experiments show that adversarial examples crafted on\nprototypical auto-encoding models transfer well to a variety of image\nclassification and face verification models. On a commercial celebrity\nrecognition system held by clarifai.com, our approach significantly diminishes\nthe average prediction accuracy of the system to only 15.40%, which is on par\nwith the attack that transfers adversarial examples from a pre-trained Arcface\nmodel.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 11:10:03 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Li", "Qizhang", ""], ["Guo", "Yiwen", ""], ["Chen", "Hao", ""]]}, {"id": "2012.02526", "submitter": "Alex Hernandez Garcia", "authors": "Alex Hernandez-Garcia", "title": "Rethinking supervised learning: insights from biological learning and\n  from calling it by its name", "comments": "Perspective paper. 8 pages + references. Earlier, shorter version\n  accepted at the workshop SVRHM, NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The renaissance of artificial neural networks was catalysed by the success of\nclassification models, tagged by the community with the broader term supervised\nlearning. The extraordinary results gave rise to a hype loaded with ambitious\npromises and overstatements. Soon the community realised that the success owed\nmuch to the availability of thousands of labelled examples and supervised\nlearning went, for many, from glory to shame: Some criticised deep learning as\na whole and others proclaimed that the way forward had to be alternatives to\nsupervised learning: predictive, unsupervised, semi-supervised and, more\nrecently, self-supervised learning. However, all these seem brand names, rather\nthan actual categories of a theoretically grounded taxonomy. Moreover, the call\nto banish supervised learning was motivated by the questionable claim that\nhumans learn with little or no supervision and are capable of robust\nout-of-distribution generalisation. Here, we review insights about learning and\nsupervision in nature, revisit the notion that learning and generalisation are\nnot possible without supervision or inductive biases and argue that we will\nmake better progress if we just call it by its name.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 11:13:31 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 18:18:44 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Hernandez-Garcia", "Alex", ""]]}, {"id": "2012.02534", "submitter": "Daizong Liu", "authors": "Daizong Liu, Dongdong Yu, Changhu Wang, Pan Zhou", "title": "F2Net: Learning to Focus on the Foreground for Unsupervised Video Object\n  Segmentation", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning based methods have achieved great progress in\nunsupervised video object segmentation, difficult scenarios (e.g., visual\nsimilarity, occlusions, and appearance changing) are still not well-handled. To\nalleviate these issues, we propose a novel Focus on Foreground Network (F2Net),\nwhich delves into the intra-inter frame details for the foreground objects and\nthus effectively improve the segmentation performance. Specifically, our\nproposed network consists of three main parts: Siamese Encoder Module, Center\nGuiding Appearance Diffusion Module, and Dynamic Information Fusion Module.\nFirstly, we take a siamese encoder to extract the feature representations of\npaired frames (reference frame and current frame). Then, a Center Guiding\nAppearance Diffusion Module is designed to capture the inter-frame feature\n(dense correspondences between reference frame and current frame), intra-frame\nfeature (dense correspondences in current frame), and original semantic feature\nof current frame. Specifically, we establish a Center Prediction Branch to\npredict the center location of the foreground object in current frame and\nleverage the center point information as spatial guidance prior to enhance the\ninter-frame and intra-frame feature extraction, and thus the feature\nrepresentation considerably focus on the foreground objects. Finally, we\npropose a Dynamic Information Fusion Module to automatically select relatively\nimportant features through three aforementioned different level features.\nExtensive experiments on DAVIS2016, Youtube-object, and FBMS datasets show that\nour proposed F2Net achieves the state-of-the-art performance with significant\nimprovement.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 11:30:50 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Liu", "Daizong", ""], ["Yu", "Dongdong", ""], ["Wang", "Changhu", ""], ["Zhou", "Pan", ""]]}, {"id": "2012.02542", "submitter": "Mehmet Ozgur Turkoglu", "authors": "Nando Metzger, Mehmet Ozgur Turkoglu, Stefano D'Aronco, Jan Dirk\n  Wegner, Konrad Schindler", "title": "Crop Classification under Varying Cloud Cover with Neural Ordinary\n  Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optical satellite sensors cannot see the Earth's surface through clouds.\nDespite the periodic revisit cycle, image sequences acquired by Earth\nobservation satellites are therefore irregularly sampled in time.\nState-of-the-art methods for crop classification (and other time series\nanalysis tasks) rely on techniques that implicitly assume regular temporal\nspacing between observations, such as recurrent neural networks (RNNs). We\npropose to use neural ordinary differential equations (NODEs) in combination\nwith RNNs to classify crop types in irregularly spaced image sequences. The\nresulting ODE-RNN models consist of two steps: an update step, where a\nrecurrent unit assimilates new input data into the model's hidden state; and a\nprediction step, in which NODE propagates the hidden state until the next\nobservation arrives. The prediction step is based on a continuous\nrepresentation of the latent dynamics, which has several advantages. At the\nconceptual level, it is a more natural way to describe the mechanisms that\ngovern the phenological cycle. From a practical point of view, it makes it\npossible to sample the system state at arbitrary points in time, such that one\ncan integrate observations whenever they are available, and extrapolate beyond\nthe last observation. Our experiments show that ODE-RNN indeed improves\nclassification accuracy over common baselines such as LSTM, GRU, and temporal\nconvolution. The gains are most prominent in the challenging scenario where\nonly few observations are available (i.e., frequent cloud cover). Moreover, we\nshow that the ability to extrapolate translates to better classification\nperformance early in the season, which is important for forecasting.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 11:56:50 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Metzger", "Nando", ""], ["Turkoglu", "Mehmet Ozgur", ""], ["D'Aronco", "Stefano", ""], ["Wegner", "Jan Dirk", ""], ["Schindler", "Konrad", ""]]}, {"id": "2012.02544", "submitter": "Jos\\'e Carlos Aradillas Jaramillo", "authors": "Jos\\'e Carlos Aradillas, Juan Jos\\'e Murillo-Fuentes, Pablo M. Olmos", "title": "Boosting offline handwritten text recognition in historical documents\n  with few labeled lines", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2021.3082689", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we face the problem of offline handwritten text recognition\n(HTR) in historical documents when few labeled samples are available and some\nof them contain errors in the train set. Three main contributions are\ndeveloped. First we analyze how to perform transfer learning (TL) from a\nmassive database to a smaller historical database, analyzing which layers of\nthe model need a fine-tuning process. Second, we analyze methods to efficiently\ncombine TL and data augmentation (DA). Finally, an algorithm to mitigate the\neffects of incorrect labelings in the training set is proposed. The methods are\nanalyzed over the ICFHR 2018 competition database, Washington and Parzival.\nCombining all these techniques, we demonstrate a remarkable reduction of CER\n(up to 6% in some cases) in the test set with little complexity overhead.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 11:59:35 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Aradillas", "Jos\u00e9 Carlos", ""], ["Murillo-Fuentes", "Juan Jos\u00e9", ""], ["Olmos", "Pablo M.", ""]]}, {"id": "2012.02577", "submitter": "Abu Mohammed Raisuddin", "authors": "Abu Mohammed Raisuddin, Elias Vaattovaara, Mika Nevalainen, Marko\n  Nikki, Elina J\\\"arvenp\\\"a\\\"a, Kaisa Makkonen, Pekka Pinola, Tuula Palsio,\n  Arttu Niemensivu, Osmo Tervonen, Aleksei Tiulpin", "title": "Critical Evaluation of Deep Neural Networks for Wrist Fracture Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wrist Fracture is the most common type of fracture with a high incidence\nrate. Conventional radiography (i.e. X-ray imaging) is used for wrist fracture\ndetection routinely, but occasionally fracture delineation poses issues and an\nadditional confirmation by computed tomography (CT) is needed for diagnosis.\nRecent advances in the field of Deep Learning (DL), a subfield of Artificial\nIntelligence (AI), have shown that wrist fracture detection can be automated\nusing Convolutional Neural Networks. However, previous studies did not pay\nclose attention to the difficult cases which can only be confirmed via CT\nimaging. In this study, we have developed and analyzed a state-of-the-art\nDL-based pipeline for wrist (distal radius) fracture detection -- DeepWrist,\nand evaluated it against one general population test set, and one challenging\ntest set comprising only cases requiring confirmation by CT. Our results reveal\nthat a typical state-of-the-art approach, such as DeepWrist, while having a\nnear-perfect performance on the general independent test set, has a\nsubstantially lower performance on the challenging test set -- average\nprecision of 0.99 (0.99-0.99) vs 0.64 (0.46-0.83), respectively. Similarly, the\narea under the ROC curve was of 0.99 (0.98-0.99) vs 0.84 (0.72-0.93),\nrespectively. Our findings highlight the importance of a meticulous analysis of\nDL-based models before clinical use, and unearth the need for more challenging\nsettings for testing medical AI systems.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 13:35:36 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 08:32:54 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Raisuddin", "Abu Mohammed", ""], ["Vaattovaara", "Elias", ""], ["Nevalainen", "Mika", ""], ["Nikki", "Marko", ""], ["J\u00e4rvenp\u00e4\u00e4", "Elina", ""], ["Makkonen", "Kaisa", ""], ["Pinola", "Pekka", ""], ["Palsio", "Tuula", ""], ["Niemensivu", "Arttu", ""], ["Tervonen", "Osmo", ""], ["Tiulpin", "Aleksei", ""]]}, {"id": "2012.02579", "submitter": "Chiman Kwan", "authors": "Chiman Kwan and Bence Budavari", "title": "A high performance approach to detecting small targets in long range low\n  quality infrared videos", "comments": "Submitted to Journal of Signal, Image and Video Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since targets are small in long range infrared (IR) videos, it is challenging\nto accurately detect targets in those videos. In this paper, we propose a high\nperformance approach to detecting small targets in long range and low quality\ninfrared videos. Our approach consists of a video resolution enhancement\nmodule, a proven small target detector based on local intensity and gradient\n(LIG), a connected component (CC) analysis module, and a track association\nmodule to connect detections from multiple frames. Extensive experiments using\nactual mid-wave infrared (MWIR) videos in ranges between 3500 m and 5000 m from\na benchmark dataset clearly demonstrated the efficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 13:36:49 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Kwan", "Chiman", ""], ["Budavari", "Bence", ""]]}, {"id": "2012.02598", "submitter": "Jingwei Xu", "authors": "Jingwei Xu, Jianjin Zhang, Zhiyu Yao, Yunbo Wang", "title": "Towards Good Practices of U-Net for Traffic Forecasting", "comments": "Code is available at\n  \\<https://github.com/ZJianjin/Traffic4cast2020_LDS>", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report presents a solution for the 2020 Traffic4Cast\nChallenge. We consider the traffic forecasting problem as a future frame\nprediction task with relatively weak temporal dependencies (might be due to\nstochastic urban traffic dynamics) and strong prior knowledge, \\textit{i.e.},\nthe roadmaps of the cities. For these reasons, we use the U-Net as the backbone\nmodel, and we propose a roadmap generation method to make the predicted traffic\nflows more rational. Meanwhile, we use a fine-tuning strategy based on the\nvalidation set to prevent overfitting, which effectively improves the\nprediction results. At the end of this report, we further discuss several\napproaches that we have considered or could be explored in future work: (1)\nharnessing inherent data patterns, such as seasonality; (2) distilling and\ntransferring common knowledge between different cities. We also analyze the\nvalidity of the evaluation metric.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 13:54:49 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Xu", "Jingwei", ""], ["Zhang", "Jianjin", ""], ["Yao", "Zhiyu", ""], ["Wang", "Yunbo", ""]]}, {"id": "2012.02604", "submitter": "Yusuke Uchida", "authors": "Panumate Chetprayoon, Fumihiko Takahashi, Yusuke Uchida", "title": "Prediction of Lane Number Using Results From Lane Detection", "comments": "GCCE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The lane number that the vehicle is traveling in is a key factor in\nintelligent vehicle fields. Many lane detection algorithms were proposed and if\nwe can perfectly detect the lanes, we can directly calculate the lane number\nfrom the lane detection results. However, in fact, lane detection algorithms\nsometimes underperform. Therefore, we propose a new approach for predicting the\nlane number, where we combine the drive recorder image with the lane detection\nresults to predict the lane number. Experiments on our own dataset confirmed\nthat our approach delivered outstanding results without significantly\nincreasing computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 14:01:00 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Chetprayoon", "Panumate", ""], ["Takahashi", "Fumihiko", ""], ["Uchida", "Yusuke", ""]]}, {"id": "2012.02621", "submitter": "Kekai Sheng", "authors": "Zhiyong Huang, Kekai Sheng, Weiming Dong, Xing Mei, Chongyang Ma,\n  Feiyue Huang, Dengwen Zhou, Changsheng Xu", "title": "Effective Label Propagation for Discriminative Semi-Supervised Domain\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised domain adaptation (SSDA) methods have demonstrated great\npotential in large-scale image classification tasks when massive labeled data\nare available in the source domain but very few labeled samples are provided in\nthe target domain. Existing solutions usually focus on feature alignment\nbetween the two domains while paying little attention to the discrimination\ncapability of learned representations in the target domain. In this paper, we\npresent a novel and effective method, namely Effective Label Propagation (ELP),\nto tackle this problem by using effective inter-domain and intra-domain\nsemantic information propagation. For inter-domain propagation, we propose a\nnew cycle discrepancy loss to encourage consistency of semantic information\nbetween the two domains. For intra-domain propagation, we propose an effective\nself-training strategy to mitigate the noises in pseudo-labeled target domain\ndata and improve the feature discriminability in the target domain. As a\ngeneral method, our ELP can be easily applied to various domain adaptation\napproaches and can facilitate their feature discrimination in the target\ndomain. Experiments on Office-Home and DomainNet benchmarks show ELP\nconsistently improves the classification accuracy of mainstream SSDA methods by\n2%~3%. Additionally, ELP also improves the performance of UDA methods as well\n(81.5% vs 86.1%), based on UDA experiments on the VisDA-2017 benchmark. Our\nsource code and pre-trained models will be released soon.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 14:28:19 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Huang", "Zhiyong", ""], ["Sheng", "Kekai", ""], ["Dong", "Weiming", ""], ["Mei", "Xing", ""], ["Ma", "Chongyang", ""], ["Huang", "Feiyue", ""], ["Zhou", "Dengwen", ""], ["Xu", "Changsheng", ""]]}, {"id": "2012.02637", "submitter": "Wenchao Zhang", "authors": "Wenchao Zhang, Chong Fu, Haoyu Xie, Mai Zhu, Ming Tie, Junxin Chen", "title": "Global Context Aware RCNN for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RoIPool/RoIAlign is an indispensable process for the typical two-stage object\ndetection algorithm, it is used to rescale the object proposal cropped from the\nfeature pyramid to generate a fixed size feature map. However, these cropped\nfeature maps of local receptive fields will heavily lose global context\ninformation. To tackle this problem, we propose a novel end-to-end trainable\nframework, called Global Context Aware (GCA) RCNN, aiming at assisting the\nneural network in strengthening the spatial correlation between the background\nand the foreground by fusing global context information. The core component of\nour GCA framework is a context aware mechanism, in which both global feature\npyramid and attention strategies are used for feature extraction and feature\nrefinement, respectively. Specifically, we leverage the dense connection to\nimprove the information flow of the global context at different stages in the\ntop-down process of FPN, and further use the attention mechanism to refine the\nglobal context at each level in the feature pyramid. In the end, we also\npresent a lightweight version of our method, which only slightly increases\nmodel complexity and computational burden. Experimental results on COCO\nbenchmark dataset demonstrate the significant advantages of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 14:56:46 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Zhang", "Wenchao", ""], ["Fu", "Chong", ""], ["Xie", "Haoyu", ""], ["Zhu", "Mai", ""], ["Tie", "Ming", ""], ["Chen", "Junxin", ""]]}, {"id": "2012.02639", "submitter": "Edward Fish", "authors": "Edward Fish, Jon Weinbren, Andrew Gilbert", "title": "Rethinking movie genre classification with fine-grained semantic\n  clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Movie genre classification is an active research area in machine learning.\nHowever, due to the limited labels available, there can be large semantic\nvariations between movies within a single genre definition. We expand these\n'coarse' genre labels by identifying 'fine-grained' semantic information within\nthe multi-modal content of movies. By leveraging pre-trained 'expert' networks,\nwe learn the influence of different combinations of modes for multi-label genre\nclassification. Using a contrastive loss, we continue to fine-tune this\n'coarse' genre classification network to identify high-level intertextual\nsimilarities between the movies across all genre labels. This leads to a more\n'fine-grained' and detailed clustering, based on semantic similarities while\nstill retaining some genre information. Our approach is demonstrated on a newly\nintroduced multi-modal 37,866,450 frame, 8,800 movie trailer dataset,\nMMX-Trailer-20, which includes pre-computed audio, location, motion, and image\nembeddings.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 14:58:31 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 10:30:01 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2021 16:46:09 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Fish", "Edward", ""], ["Weinbren", "Jon", ""], ["Gilbert", "Andrew", ""]]}, {"id": "2012.02643", "submitter": "Akbar Siami Namin", "authors": "Faranak Abri, Luis Felipe Guti\\'errez, Akbar Siami Namin, David R. W.\n  Sears, Keith S. Jones", "title": "Predicting Emotions Perceived from Sounds", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sonification is the science of communication of data and events to users\nthrough sounds. Auditory icons, earcons, and speech are the common auditory\ndisplay schemes utilized in sonification, or more specifically in the use of\naudio to convey information. Once the captured data are perceived, their\nmeanings, and more importantly, intentions can be interpreted more easily and\nthus can be employed as a complement to visualization techniques. Through\nauditory perception it is possible to convey information related to temporal,\nspatial, or some other context-oriented information. An important research\nquestion is whether the emotions perceived from these auditory icons or earcons\nare predictable in order to build an automated sonification platform. This\npaper conducts an experiment through which several mainstream and conventional\nmachine learning algorithms are developed to study the prediction of emotions\nperceived from sounds. To do so, the key features of sounds are captured and\nthen are modeled using machine learning algorithms using feature reduction\ntechniques. We observe that it is possible to predict perceived emotions with\nhigh accuracy. In particular, the regression based on Random Forest\ndemonstrated its superiority compared to other machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 15:01:59 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Abri", "Faranak", ""], ["Guti\u00e9rrez", "Luis Felipe", ""], ["Namin", "Akbar Siami", ""], ["Sears", "David R. W.", ""], ["Jones", "Keith S.", ""]]}, {"id": "2012.02646", "submitter": "Songyang Zhang", "authors": "Songyang Zhang, Houwen Peng, Jianlong Fu, Yijuan Lu, Jiebo Luo", "title": "Multi-Scale 2D Temporal Adjacent Networks for Moment Localization with\n  Natural Language", "comments": "arXiv admin note: text overlap with arXiv:1912.03590", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of retrieving a specific moment from an untrimmed\nvideo by natural language. It is a challenging problem because a target moment\nmay take place in the context of other temporal moments in the untrimmed video.\nExisting methods cannot tackle this challenge well since they do not fully\nconsider the temporal contexts between temporal moments. In this paper, we\nmodel the temporal context between video moments by a set of predefined\ntwo-dimensional maps under different temporal scales. For each map, one\ndimension indicates the starting time of a moment and the other indicates the\nduration. These 2D temporal maps can cover diverse video moments with different\nlengths, while representing their adjacent contexts at different temporal\nscales. Based on the 2D temporal maps, we propose a Multi-Scale Temporal\nAdjacent Network (MS-2D-TAN), a single-shot framework for moment localization.\nIt is capable of encoding the adjacent temporal contexts at each scale, while\nlearning discriminative features for matching video moments with referring\nexpressions. We evaluate the proposed MS-2D-TAN on three challenging\nbenchmarks, i.e., Charades-STA, ActivityNet Captions, and TACoS, where our\nMS-2D-TAN outperforms the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 15:09:35 GMT"}], "update_date": "2020-12-28", "authors_parsed": [["Zhang", "Songyang", ""], ["Peng", "Houwen", ""], ["Fu", "Jianlong", ""], ["Lu", "Yijuan", ""], ["Luo", "Jiebo", ""]]}, {"id": "2012.02647", "submitter": "Taylor Mordan", "authors": "Taylor Mordan, Matthieu Cord, Patrick P\\'erez and Alexandre Alahi", "title": "Detecting 32 Pedestrian Attributes for Autonomous Vehicles", "comments": "Submission to IEEE Transactions on Intelligent Transportation Systems\n  (T-ITS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Pedestrians are arguably one of the most safety-critical road users to\nconsider for autonomous vehicles in urban areas. In this paper, we address the\nproblem of jointly detecting pedestrians and recognizing 32 pedestrian\nattributes. These encompass visual appearance and behavior, and also include\nthe forecasting of road crossing, which is a main safety concern. For this, we\nintroduce a Multi-Task Learning (MTL) model relying on a composite field\nframework, which achieves both goals in an efficient way. Each field spatially\nlocates pedestrian instances and aggregates attribute predictions over them.\nThis formulation naturally leverages spatial context, making it well suited to\nlow resolution scenarios such as autonomous driving. By increasing the number\nof attributes jointly learned, we highlight an issue related to the scales of\ngradients, which arises in MTL with numerous tasks. We solve it by normalizing\nthe gradients coming from different objective functions when they join at the\nfork in the network architecture during the backward pass, referred to as\nfork-normalization. Experimental validation is performed on JAAD, a dataset\nproviding numerous attributes for pedestrian analysis from autonomous vehicles,\nand shows competitive detection and attribute recognition results, as well as a\nmore stable MTL training.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 15:10:12 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Mordan", "Taylor", ""], ["Cord", "Matthieu", ""], ["P\u00e9rez", "Patrick", ""], ["Alahi", "Alexandre", ""]]}, {"id": "2012.02672", "submitter": "Ji Eun Kim", "authors": "Ji Eun Kim, Cory Henson, Kevin Huang, Tuan A. Tran, Wan-Yi Lin", "title": "Accelerating Road Sign Ground Truth Construction with Knowledge Graph\n  and Machine Learning", "comments": "12 pages, 5 figures", "journal-ref": "Computing Conference 2021", "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Having a comprehensive, high-quality dataset of road sign annotation is\ncritical to the success of AI-based Road Sign Recognition (RSR) systems. In\npractice, annotators often face difficulties in learning road sign systems of\ndifferent countries; hence, the tasks are often time-consuming and produce poor\nresults. We propose a novel approach using knowledge graphs and a machine\nlearning algorithm - variational prototyping-encoder (VPE) - to assist human\nannotators in classifying road signs effectively. Annotators can query the Road\nSign Knowledge Graph using visual attributes and receive closest matching\ncandidates suggested by the VPE model. The VPE model uses the candidates from\nthe knowledge graph and a real sign image patch as inputs. We show that our\nknowledge graph approach can reduce sign search space by 98.9%. Furthermore,\nwith VPE, our system can propose the correct single candidate for 75% of signs\nin the tested datasets, eliminating the human search effort entirely in those\ncases.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 15:42:08 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Kim", "Ji Eun", ""], ["Henson", "Cory", ""], ["Huang", "Kevin", ""], ["Tran", "Tuan A.", ""], ["Lin", "Wan-Yi", ""]]}, {"id": "2012.02689", "submitter": "Maolin Gao", "authors": "Maolin Gao, Zorah L\\\"ahner, Johan Thunberg, Daniel Cremers, Florian\n  Bernard", "title": "Isometric Multi-Shape Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding correspondences between shapes is a fundamental problem in computer\nvision and graphics, which is relevant for many applications, including 3D\nreconstruction, object tracking, and style transfer. The vast majority of\ncorrespondence methods aim to find a solution between pairs of shapes, even if\nmultiple instances of the same class are available. While isometries are often\nstudied in shape correspondence problems, they have not been considered\nexplicitly in the multi-matching setting. This paper closes this gap by\nproposing a novel optimisation formulation for isometric multi-shape matching.\nWe present a suitable optimisation algorithm for solving our formulation and\nprovide a convergence and complexity analysis. Our algorithm obtains\nmulti-matchings that are by construction provably cycle-consistent. We\ndemonstrate the superior performance of our method on various datasets and set\nthe new state-of-the-art in isometric multi-shape matching.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 15:58:34 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Gao", "Maolin", ""], ["L\u00e4hner", "Zorah", ""], ["Thunberg", "Johan", ""], ["Cremers", "Daniel", ""], ["Bernard", "Florian", ""]]}, {"id": "2012.02706", "submitter": "Nicolas Wagner", "authors": "Nicolas Wagner, Anirban Mukhopadhyay", "title": "Super-Selfish: Self-Supervised Learning on Images with PyTorch", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Super-Selfish is an easy to use PyTorch framework for image-based\nself-supervised learning. Features can be learned with 13 algorithms that span\nfrom simple classification to more complex state of theart contrastive pretext\ntasks. The framework is easy to use and allows for pretraining any PyTorch\nneural network with only two lines of code. Simultaneously, full flexibility is\nmaintained through modular design choices. The code can be found at\nhttps://github.com/MECLabTUDA/Super_Selfish and installed using pip install\nsuper-selfish.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 16:30:02 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 07:22:08 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Wagner", "Nicolas", ""], ["Mukhopadhyay", "Anirban", ""]]}, {"id": "2012.02733", "submitter": "Haohang Xu", "authors": "Haohang Xu, Xiaopeng Zhang, Hao Li, Lingxi Xie, Hongkai Xiong, Qi Tian", "title": "Seed the Views: Hierarchical Semantic Alignment for Contrastive\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning based on instance discrimination has shown\nremarkable progress. In particular, contrastive learning, which regards each\nimage as well as its augmentations as an individual class and tries to\ndistinguish them from all other images, has been verified effective for\nrepresentation learning. However, pushing away two images that are de facto\nsimilar is suboptimal for general representation. In this paper, we propose a\nhierarchical semantic alignment strategy via expanding the views generated by a\nsingle image to \\textbf{Cross-samples and Multi-level} representation, and\nmodels the invariance to semantically similar images in a hierarchical way.\nThis is achieved by extending the contrastive loss to allow for multiple\npositives per anchor, and explicitly pulling semantically similar\nimages/patches together at different layers of the network. Our method, termed\nas CsMl, has the ability to integrate multi-level visual representations across\nsamples in a robust way. CsMl is applicable to current contrastive learning\nbased methods and consistently improves the performance. Notably, using the\nmoco as an instantiation, CsMl achieves a \\textbf{76.6\\% }top-1 accuracy with\nlinear evaluation using ResNet-50 as backbone, and \\textbf{66.7\\%} and\n\\textbf{75.1\\%} top-1 accuracy with only 1\\% and 10\\% labels, respectively.\n\\textbf{All these numbers set the new state-of-the-art.}\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 17:26:24 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 08:44:47 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Xu", "Haohang", ""], ["Zhang", "Xiaopeng", ""], ["Li", "Hao", ""], ["Xie", "Lingxi", ""], ["Xiong", "Hongkai", ""], ["Tian", "Qi", ""]]}, {"id": "2012.02738", "submitter": "Ali K. Z. Tehrani", "authors": "Ali K. Z. Tehrani, Mina Amiri, Ivan M. Rosado-Mendez, Timothy J. Hall,\n  and Hassan Rivaz", "title": "Ultrasound Scatterer Density Classification Using Convolutional Neural\n  Networks by Exploiting Patch Statistics", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantitative ultrasound (QUS) can reveal crucial information on tissue\nproperties such as scatterer density. If the scatterer density per resolution\ncell is above or below 10, the tissue is considered as fully developed speckle\n(FDS) or low-density scatterers (LDS), respectively. Conventionally, the\nscatterer density has been classified using estimated statistical parameters of\nthe amplitude of backscattered echoes. However, if the patch size is small, the\nestimation is not accurate. These parameters are also highly dependent on\nimaging settings. In this paper, we propose a convolutional neural network\n(CNN) architecture for QUS, and train it using simulation data. We further\nimprove the network performance by utilizing patch statistics as additional\ninput channels. We evaluate the network using simulation data, experimental\nphantoms and in vivo data. We also compare our proposed network with different\nclassic and deep learning models, and demonstrate its superior performance in\nclassification of tissues with different scatterer density values. The results\nalso show that the proposed network is able to work with different imaging\nparameters with no need for a reference phantom. This work demonstrates the\npotential of CNNs in classifying scatterer density in ultrasound images.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 17:36:57 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Tehrani", "Ali K. Z.", ""], ["Amiri", "Mina", ""], ["Rosado-Mendez", "Ivan M.", ""], ["Hall", "Timothy J.", ""], ["Rivaz", "Hassan", ""]]}, {"id": "2012.02743", "submitter": "Vincent Leroy", "authors": "Vincent Leroy, Philippe Weinzaepfel, Romain Br\\'egier, Hadrien\n  Combaluzier, Gr\\'egory Rogez", "title": "SMPLy Benchmarking 3D Human Pose Estimation in the Wild", "comments": "3DV 2020 Oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting 3D human pose from images has seen great recent improvements.\nNovel approaches that can even predict both pose and shape from a single input\nimage have been introduced, often relying on a parametric model of the human\nbody such as SMPL. While qualitative results for such methods are often shown\nfor images captured in-the-wild, a proper benchmark in such conditions is still\nmissing, as it is cumbersome to obtain ground-truth 3D poses elsewhere than in\na motion capture room. This paper presents a pipeline to easily produce and\nvalidate such a dataset with accurate ground-truth, with which we benchmark\nrecent 3D human pose estimation methods in-the-wild. We make use of the\nrecently introduced Mannequin Challenge dataset which contains in-the-wild\nvideos of people frozen in action like statues and leverage the fact that\npeople are static and the camera moving to accurately fit the SMPL model on the\nsequences. A total of 24,428 frames with registered body models are then\nselected from 567 scenes at almost no cost, using only online RGB videos. We\nbenchmark state-of-the-art SMPL-based human pose estimation methods on this\ndataset. Our results highlight that challenges remain, in particular for\ndifficult poses or for scenes where the persons are partially truncated or\noccluded.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 17:48:32 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Leroy", "Vincent", ""], ["Weinzaepfel", "Philippe", ""], ["Br\u00e9gier", "Romain", ""], ["Combaluzier", "Hadrien", ""], ["Rogez", "Gr\u00e9gory", ""]]}, {"id": "2012.02749", "submitter": "Calden Wloka", "authors": "Calden Wloka and John K. Tsotsos", "title": "An Empirical Method to Quantify the Peripheral Performance Degradation\n  in Deep Networks", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When applying a convolutional kernel to an image, if the output is to remain\nthe same size as the input then some form of padding is required around the\nimage boundary, meaning that for each layer of convolution in a convolutional\nneural network (CNN), a strip of pixels equal to the half-width of the kernel\nsize is produced with a non-veridical representation. Although most CNN kernels\nare small to reduce the parameter load of a network, this non-veridical area\ncompounds with each convolutional layer. The tendency toward deeper and deeper\nnetworks combined with stride-based down-sampling means that the propagation of\nthis region can end up covering a non-negligable portion of the image. Although\nthis issue with convolutions has been well acknowledged over the years, the\nimpact of this degraded peripheral representation on modern network behavior\nhas not been fully quantified. What are the limits of translation invariance?\nDoes image padding successfully mitigate the issue, or is performance affected\nas an object moves between the image border and center? Using Mask R-CNN as an\nexperimental model, we design a dataset and methodology to quantify the spatial\ndependency of network performance. Our dataset is constructed by inserting\nobjects into high resolution backgrounds, thereby allowing us to crop\nsub-images which place target objects at specific locations relative to the\nimage border. By probing the behaviour of Mask R-CNN across a selection of\ntarget locations, we see clear patterns of performance degredation near the\nimage boundary, and in particular in the image corners. Quantifying both the\nextent and magnitude of this spatial anisotropy in network performance is\nimportant for the deployment of deep networks into unconstrained and realistic\nenvironments in which the location of objects or regions of interest are not\nguaranteed to be well localized within a given image.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 18:00:47 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Wloka", "Calden", ""], ["Tsotsos", "John K.", ""]]}, {"id": "2012.02755", "submitter": "Kevin Raina", "authors": "Kevin Raina", "title": "Statistical inference of the inter-sample Dice distribution for\n  discriminative CNN brain lesion segmentation models", "comments": "Proceedings of the 14th International Joint Conference on Biomedical\n  Engineering Systems and Technologies, Volume 2: BIOIMAGING 2021", "journal-ref": null, "doi": "10.5220/0010286201680173", "report-no": null, "categories": "eess.IV cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Discriminative convolutional neural networks (CNNs), for which a voxel-wise\nconditional Multinoulli distribution is assumed, have performed well in many\nbrain lesion segmentation tasks. For a trained discriminative CNN to be used in\nclinical practice, the patient's radiological features are inputted into the\nmodel, in which case a conditional distribution of segmentations is produced.\nCapturing the uncertainty of the predictions can be useful in deciding whether\nto abandon a model, or choose amongst competing models. In practice, however,\nwe never know the ground truth segmentation, and therefore can never know the\ntrue model variance. In this work, segmentation sampling on discriminative CNNs\nis used to assess a trained model's robustness by analyzing the inter-sample\nDice distribution on a new patient solely based on their magnetic resonance\n(MR) images. Furthermore, by demonstrating the inter-sample Dice observations\nare independent and identically distributed with a finite mean and variance\nunder certain conditions, a rigorous confidence based decision rule is proposed\nto decide whether to reject or accept a CNN model for a particular patient.\nApplied to the ISLES 2015 (SISS) dataset, the model identified 7 predictions as\nnon-robust, and the average Dice coefficient calculated on the remaining brains\nimproved by 12 percent.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 18:18:24 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 14:45:24 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Raina", "Kevin", ""]]}, {"id": "2012.02771", "submitter": "Carlos Esteves", "authors": "Carlos Esteves", "title": "Learning Equivariant Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep learning systems often require large amounts of data\nand computation. For this reason, leveraging known or unknown structure of the\ndata is paramount. Convolutional neural networks (CNNs) are successful examples\nof this principle, their defining characteristic being the shift-equivariance.\nBy sliding a filter over the input, when the input shifts, the response shifts\nby the same amount, exploiting the structure of natural images where semantic\ncontent is independent of absolute pixel positions. This property is essential\nto the success of CNNs in audio, image and video recognition tasks. In this\nthesis, we extend equivariance to other kinds of transformations, such as\nrotation and scaling. We propose equivariant models for different\ntransformations defined by groups of symmetries. The main contributions are (i)\npolar transformer networks, achieving equivariance to the group of similarities\non the plane, (ii) equivariant multi-view networks, achieving equivariance to\nthe group of symmetries of the icosahedron, (iii) spherical CNNs, achieving\nequivariance to the continuous 3D rotation group, (iv) cross-domain image\nembeddings, achieving equivariance to 3D rotations for 2D inputs, and (v)\nspin-weighted spherical CNNs, generalizing the spherical CNNs and achieving\nequivariance to 3D rotations for spherical vector fields. Applications include\nimage classification, 3D shape classification and retrieval, panoramic image\nclassification and segmentation, shape alignment and pose estimation. What\nthese models have in common is that they leverage symmetries in the data to\nreduce sample and model complexity and improve generalization performance. The\nadvantages are more significant on (but not limited to) challenging tasks where\ndata is limited or input perturbations such as arbitrary rotations are present.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 18:46:17 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Esteves", "Carlos", ""]]}, {"id": "2012.02775", "submitter": "Parth Natekar", "authors": "Parth Natekar, Manik Sharma", "title": "Representation Based Complexity Measures for Predicting Generalization\n  in Deep Learning", "comments": "Winning Solution of the NeurIPS 2020 Competition on Predicting\n  Generalization in Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Neural Networks can generalize despite being significantly\noverparametrized. Recent research has tried to examine this phenomenon from\nvarious view points and to provide bounds on the generalization error or\nmeasures predictive of the generalization gap based on these viewpoints, such\nas norm-based, PAC-Bayes based, and margin-based analysis. In this work, we\nprovide an interpretation of generalization from the perspective of quality of\ninternal representations of deep neural networks, based on neuroscientific\ntheories of how the human visual system creates invariant and untangled object\nrepresentations. Instead of providing theoretical bounds, we demonstrate\npractical complexity measures which can be computed ad-hoc to uncover\ngeneralization behaviour in deep models. We also provide a detailed description\nof our solution that won the NeurIPS competition on Predicting Generalization\nin Deep Learning held at NeurIPS 2020. An implementation of our solution is\navailable at https://github.com/parthnatekar/pgdl.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 18:53:44 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Natekar", "Parth", ""], ["Sharma", "Manik", ""]]}, {"id": "2012.02776", "submitter": "Jianbing Shen", "authors": "Wencheng Han, Xingping Dong, Fahad Shahbaz Khan, Ling Shao, Jianbing\n  Shen", "title": "Learning to Fuse Asymmetric Feature Maps in Siamese Trackers", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, Siamese-based trackers have achieved promising performance in\nvisual tracking. Most recent Siamese-based trackers typically employ a\ndepth-wise cross-correlation (DW-XCorr) to obtain multi-channel correlation\ninformation from the two feature maps (target and search region). However,\nDW-XCorr has several limitations within Siamese-based tracking: it can easily\nbe fooled by distractors, has fewer activated channels, and provides weak\ndiscrimination of object boundaries. Further, DW-XCorr is a handcrafted\nparameter-free module and cannot fully benefit from offline learning on\nlarge-scale data. We propose a learnable module, called the asymmetric\nconvolution (ACM), which learns to better capture the semantic correlation\ninformation in offline training on large-scale data. Different from DW-XCorr\nand its predecessor(XCorr), which regard a single feature map as the\nconvolution kernel, our ACM decomposes the convolution operation on a\nconcatenated feature map into two mathematically equivalent operations, thereby\navoiding the need for the feature maps to be of the same size (width and\nheight)during concatenation. Our ACM can incorporate useful prior information,\nsuch as bounding-box size, with standard visual features. Furthermore, ACM can\neasily be integrated into existing Siamese trackers based on DW-XCorror XCorr.\nTo demonstrate its generalization ability, we integrate ACM into three\nrepresentative trackers: SiamFC, SiamRPN++, and SiamBAN. Our experiments reveal\nthe benefits of the proposed ACM, which outperforms existing methods on six\ntracking benchmarks. On the LaSOT test set, our ACM-based tracker obtains a\nsignificant improvement of 5.8% in terms of success (AUC), over the baseline.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 18:55:53 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 03:57:46 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Han", "Wencheng", ""], ["Dong", "Xingping", ""], ["Khan", "Fahad Shahbaz", ""], ["Shao", "Ling", ""], ["Shen", "Jianbing", ""]]}, {"id": "2012.02780", "submitter": "Yijun Li", "authors": "Yijun Li, Richard Zhang, Jingwan Lu, Eli Shechtman", "title": "Few-shot Image Generation with Elastic Weight Consolidation", "comments": "Accepted by NeurIPS 2020, see\n  https://yijunmaverick.github.io/publications/ewc/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot image generation seeks to generate more data of a given domain, with\nonly few available training examples. As it is unreasonable to expect to fully\ninfer the distribution from just a few observations (e.g., emojis), we seek to\nleverage a large, related source domain as pretraining (e.g., human faces).\nThus, we wish to preserve the diversity of the source domain, while adapting to\nthe appearance of the target. We adapt a pretrained model, without introducing\nany additional parameters, to the few examples of the target domain. Crucially,\nwe regularize the changes of the weights during this adaptation, in order to\nbest preserve the information of the source dataset, while fitting the target.\nWe demonstrate the effectiveness of our algorithm by generating high-quality\nresults of different target domains, including those with extremely few\nexamples (e.g., <10). We also analyze the performance of our method with\nrespect to some important factors, such as the number of examples and the\ndissimilarity between the source and target domain.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 18:57:13 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Li", "Yijun", ""], ["Zhang", "Richard", ""], ["Lu", "Jingwan", ""], ["Shechtman", "Eli", ""]]}, {"id": "2012.02782", "submitter": "Xiao-Yun Zhou", "authors": "Xiao-Yun Zhou, Jiacheng Sun, Nanyang Ye, Xu Lan, Qijun Luo, Bo-Lin\n  Lai, Pedro Esperanca, Guang-Zhong Yang, Zhenguo Li", "title": "Batch Group Normalization", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Convolutional Neural Networks (DCNNs) are hard and time-consuming to\ntrain. Normalization is one of the effective solutions. Among previous\nnormalization methods, Batch Normalization (BN) performs well at medium and\nlarge batch sizes and is with good generalizability to multiple vision tasks,\nwhile its performance degrades significantly at small batch sizes. In this\npaper, we find that BN saturates at extreme large batch sizes, i.e., 128 images\nper worker, i.e., GPU, as well and propose that the degradation/saturation of\nBN at small/extreme large batch sizes is caused by noisy/confused statistic\ncalculation. Hence without adding new trainable parameters, using\nmultiple-layer or multi-iteration information, or introducing extra\ncomputation, Batch Group Normalization (BGN) is proposed to solve the\nnoisy/confused statistic calculation of BN at small/extreme large batch sizes\nwith introducing the channel, height and width dimension to compensate. The\ngroup technique in Group Normalization (GN) is used and a hyper-parameter G is\nused to control the number of feature instances used for statistic calculation,\nhence to offer neither noisy nor confused statistic for different batch sizes.\nWe empirically demonstrate that BGN consistently outperforms BN, Instance\nNormalization (IN), Layer Normalization (LN), GN, and Positional Normalization\n(PN), across a wide spectrum of vision tasks, including image classification,\nNeural Architecture Search (NAS), adversarial learning, Few Shot Learning (FSL)\nand Unsupervised Domain Adaptation (UDA), indicating its good performance,\nrobust stability to batch size and wide generalizability. For example, for\ntraining ResNet-50 on ImageNet with a batch size of 2, BN achieves Top1\naccuracy of 66.512% while BGN achieves 76.096% with notable improvement.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 18:57:52 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 01:26:51 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Zhou", "Xiao-Yun", ""], ["Sun", "Jiacheng", ""], ["Ye", "Nanyang", ""], ["Lan", "Xu", ""], ["Luo", "Qijun", ""], ["Lai", "Bo-Lin", ""], ["Esperanca", "Pedro", ""], ["Yang", "Guang-Zhong", ""], ["Li", "Zhenguo", ""]]}, {"id": "2012.02788", "submitter": "Deepak Pathak", "authors": "Shikhar Bahl, Mustafa Mukadam, Abhinav Gupta, Deepak Pathak", "title": "Neural Dynamic Policies for End-to-End Sensorimotor Learning", "comments": "NeurIPS 2020 (Spotlight). Code and videos at\n  https://shikharbahl.github.io/neural-dynamic-policies/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current dominant paradigm in sensorimotor control, whether imitation or\nreinforcement learning, is to train policies directly in raw action spaces such\nas torque, joint angle, or end-effector position. This forces the agent to make\ndecisions individually at each timestep in training, and hence, limits the\nscalability to continuous, high-dimensional, and long-horizon tasks. In\ncontrast, research in classical robotics has, for a long time, exploited\ndynamical systems as a policy representation to learn robot behaviors via\ndemonstrations. These techniques, however, lack the flexibility and\ngeneralizability provided by deep learning or reinforcement learning and have\nremained under-explored in such settings. In this work, we begin to close this\ngap and embed the structure of a dynamical system into deep neural\nnetwork-based policies by reparameterizing action spaces via second-order\ndifferential equations. We propose Neural Dynamic Policies (NDPs) that make\npredictions in trajectory distribution space as opposed to prior policy\nlearning methods where actions represent the raw control space. The embedded\nstructure allows end-to-end policy learning for both reinforcement and\nimitation learning setups. We show that NDPs outperform the prior\nstate-of-the-art in terms of either efficiency or performance across several\nrobotic control tasks for both imitation and reinforcement learning setups.\nProject video and code are available at\nhttps://shikharbahl.github.io/neural-dynamic-policies/\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 18:59:32 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Bahl", "Shikhar", ""], ["Mukadam", "Mustafa", ""], ["Gupta", "Abhinav", ""], ["Pathak", "Deepak", ""]]}, {"id": "2012.02813", "submitter": "Paul Pu Liang", "authors": "Paul Pu Liang, Peter Wu, Liu Ziyin, Louis-Philippe Morency, Ruslan\n  Salakhutdinov", "title": "Cross-Modal Generalization: Learning in Low Resource Modalities via\n  Meta-Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The natural world is abundant with concepts expressed via visual, acoustic,\ntactile, and linguistic modalities. Much of the existing progress in multimodal\nlearning, however, focuses primarily on problems where the same set of\nmodalities are present at train and test time, which makes learning in\nlow-resource modalities particularly difficult. In this work, we propose\nalgorithms for cross-modal generalization: a learning paradigm to train a model\nthat can (1) quickly perform new tasks in a target modality (i.e.\nmeta-learning) and (2) doing so while being trained on a different source\nmodality. We study a key research question: how can we ensure generalization\nacross modalities despite using separate encoders for different source and\ntarget modalities? Our solution is based on meta-alignment, a novel method to\nalign representation spaces using strongly and weakly paired cross-modal data\nwhile ensuring quick generalization to new tasks across different modalities.\nWe study this problem on 3 classification tasks: text to image, image to audio,\nand text to speech. Our results demonstrate strong performance even when the\nnew target modality has only a few (1-10) labeled samples and in the presence\nof noisy labels, a scenario particularly prevalent in low-resource modalities.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 19:27:26 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Liang", "Paul Pu", ""], ["Wu", "Peter", ""], ["Ziyin", "Liu", ""], ["Morency", "Louis-Philippe", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "2012.02818", "submitter": "Gianni Franchi", "authors": "Gianni Franchi, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson,\n  Isabelle Bloch", "title": "Encoding the latent posterior of Bayesian Neural Networks for\n  uncertainty quantification", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian neural networks (BNNs) have been long considered an ideal, yet\nunscalable solution for improving the robustness and the predictive uncertainty\nof deep neural networks. While they could capture more accurately the posterior\ndistribution of the network parameters, most BNN approaches are either limited\nto small networks or rely on constraining assumptions such as parameter\nindependence. These drawbacks have enabled prominence of simple, but\ncomputationally heavy approaches such as Deep Ensembles, whose training and\ntesting costs increase linearly with the number of networks. In this work we\naim for efficient deep BNNs amenable to complex computer vision architectures,\ne.g. ResNet50 DeepLabV3+, and tasks, e.g. semantic segmentation, with fewer\nassumptions on the parameters. We achieve this by leveraging variational\nautoencoders (VAEs) to learn the interaction and the latent distribution of the\nparameters at each network layer. Our approach, Latent-Posterior BNN (LP-BNN),\nis compatible with the recent BatchEnsemble method, leading to highly efficient\n({in terms of computation and} memory during both training and testing)\nensembles. LP-BNN s attain competitive results across multiple metrics in\nseveral challenging benchmarks for image classification, semantic segmentation\nand out-of-distribution detection.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 19:50:09 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 12:12:19 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Franchi", "Gianni", ""], ["Bursuc", "Andrei", ""], ["Aldea", "Emanuel", ""], ["Dubuisson", "Severine", ""], ["Bloch", "Isabelle", ""]]}, {"id": "2012.02821", "submitter": "Fangda Han", "authors": "Fangda Han, Guoyao Hao, Ricardo Guerrero, Vladimir Pavlovic", "title": "MPG: A Multi-ingredient Pizza Image Generator with Conditional StyleGANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multilabel conditional image generation is a challenging problem in computer\nvision. In this work we propose Multi-ingredient Pizza Generator (MPG), a\nconditional Generative Neural Network (GAN) framework for synthesizing\nmultilabel images. We design MPG based on a state-of-the-art GAN structure\ncalled StyleGAN2, in which we develop a new conditioning technique by enforcing\nintermediate feature maps to learn scalewise label information. Because of the\ncomplex nature of the multilabel image generation problem, we also regularize\nsynthetic image by predicting the corresponding ingredients as well as\nencourage the discriminator to distinguish between matched image and mismatched\nimage. To verify the efficacy of MPG, we test it on Pizza10, which is a\ncarefully annotated multi-ingredient pizza image dataset. MPG can successfully\ngenerate photo-realist pizza images with desired ingredients. The framework can\nbe easily extend to other multilabel image generation scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 19:51:31 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Han", "Fangda", ""], ["Hao", "Guoyao", ""], ["Guerrero", "Ricardo", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "2012.02897", "submitter": "Utkarsh Mall", "authors": "Utkarsh Mall, Kavita Bala, Tamara Berg, Kristen Grauman", "title": "Discovering Underground Maps from Fashion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The fashion sense -- meaning the clothing styles people wear -- in a\ngeographical region can reveal information about that region. For example, it\ncan reflect the kind of activities people do there, or the type of crowds that\nfrequently visit the region (e.g., tourist hot spot, student neighborhood,\nbusiness center). We propose a method to automatically create underground\nneighborhood maps of cities by analyzing how people dress. Using publicly\navailable images from across a city, our method finds neighborhoods with a\nsimilar fashion sense and segments the map without supervision. For 37 cities\nworldwide, we show promising results in creating good underground maps, as\nevaluated using experiments with human judges and underground map benchmarks\nderived from non-image data. Our approach further allows detecting distinct\nneighborhoods (what is the most unique region of LA?) and answering analogy\nquestions between cities (what is the \"Downtown LA\" of Bogota?).\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 23:40:59 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Mall", "Utkarsh", ""], ["Bala", "Kavita", ""], ["Berg", "Tamara", ""], ["Grauman", "Kristen", ""]]}, {"id": "2012.02899", "submitter": "Reza Maalek", "authors": "Reza Maalek and Derek Lichti", "title": "Automated Calibration of Mobile Cameras for 3D Reconstruction of\n  Mechanical Pipes", "comments": null, "journal-ref": null, "doi": "10.1111/phor.12364", "report-no": null, "categories": "cs.CV cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This manuscript provides a new framework for calibration of optical\ninstruments, in particular mobile cameras, using large-scale circular black and\nwhite target fields. New methods were introduced for (i) matching targets\nbetween images; (ii) adjusting the systematic eccentricity error of target\ncenters; and (iii) iteratively improving the calibration solution through a\nfree-network self-calibrating bundle adjustment. It was observed that the\nproposed target matching effectively matched circular targets in 270 mobile\nphone images from a complete calibration laboratory with robustness to Type II\nerrors. The proposed eccentricity adjustment, which requires only camera\nprojective matrices from two views, behaved synonymous to available closed-form\nsolutions, which require several additional object space target information a\npriori. Finally, specifically for the case of the mobile devices, the\ncalibration parameters obtained using our framework was found superior compared\nto in-situ calibration for estimating the 3D reconstructed radius of a\nmechanical pipe (approximately 45% improvement).\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 23:41:25 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Maalek", "Reza", ""], ["Lichti", "Derek", ""]]}, {"id": "2012.02906", "submitter": "Sandipan Banerjee", "authors": "Sandipan Banerjee, Ajjen Joshi, Jay Turcot, Bryan Reimer and Taniya\n  Mishra", "title": "Driver Glance Classification In-the-wild: Towards Generalization Across\n  Domains and Subjects", "comments": "Preprint version. Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distracted drivers are dangerous drivers. Equipping advanced driver\nassistance systems (ADAS) with the ability to detect driver distraction can\nhelp prevent accidents and improve driver safety. In order to detect driver\ndistraction, an ADAS must be able to monitor their visual attention. We propose\na model that takes as input a patch of the driver's face along with a crop of\nthe eye-region and classifies their glance into 6 coarse regions-of-interest\n(ROIs) in the vehicle. We demonstrate that an hourglass network, trained with\nan additional reconstruction loss, allows the model to learn stronger\ncontextual feature representations than a traditional encoder-only\nclassification module. To make the system robust to subject-specific variations\nin appearance and behavior, we design a personalized hourglass model tuned with\nan auxiliary input representing the driver's baseline glance behavior. Finally,\nwe present a weakly supervised multi-domain training regimen that enables the\nhourglass to jointly learn representations from different domains (varying in\ncamera type, angle), utilizing unlabeled samples and thereby reducing\nannotation cost.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 00:23:01 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 03:33:19 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Banerjee", "Sandipan", ""], ["Joshi", "Ajjen", ""], ["Turcot", "Jay", ""], ["Reimer", "Bryan", ""], ["Mishra", "Taniya", ""]]}, {"id": "2012.02909", "submitter": "Huan Wang", "authors": "Huan Wang, Suhas Lohit, Michael Jones, Yun Fu", "title": "Knowledge Distillation Thrives on Data Augmentation", "comments": "Code will be updated soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation (KD) is a general deep neural network training\nframework that uses a teacher model to guide a student model. Many works have\nexplored the rationale for its success, however, its interplay with data\naugmentation (DA) has not been well recognized so far. In this paper, we are\nmotivated by an interesting observation in classification: KD loss can benefit\nfrom extended training iterations while the cross-entropy loss does not. We\nshow this disparity arises because of data augmentation: KD loss can tap into\nthe extra information from different input views brought by DA. By this\nexplanation, we propose to enhance KD via a stronger data augmentation scheme\n(e.g., mixup, CutMix). Furthermore, an even stronger new DA approach is\ndeveloped specifically for KD based on the idea of active learning. The\nfindings and merits of the proposed method are validated by extensive\nexperiments on CIFAR-100, Tiny ImageNet, and ImageNet datasets. We can achieve\nimproved performance simply by using the original KD loss combined with\nstronger augmentation schemes, compared to existing state-of-the-art methods,\nwhich employ more advanced distillation losses. In addition, when our\napproaches are combined with more advanced distillation losses, we can advance\nthe state-of-the-art performance even more. On top of the encouraging\nperformance, this paper also sheds some light on explaining the success of\nknowledge distillation. The discovered interplay between KD and DA may inspire\nmore advanced KD algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 00:32:04 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Wang", "Huan", ""], ["Lohit", "Suhas", ""], ["Jones", "Michael", ""], ["Fu", "Yun", ""]]}, {"id": "2012.02910", "submitter": "Diego Pati\\~no", "authors": "Diego Pati\\~no and John Branch", "title": "Cosine-Pruned Medial Axis: A new method for isometric equivariant and\n  noise-free medial axis extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the CPMA, a new method for medial axis pruning with noise\nrobustness and equivariance to isometric transformations. Our method leverages\nthe discrete cosine transform to create smooth versions of a shape $\\Omega$. We\nuse the smooth shapes to compute a score function $\\scorefunction$ that filters\nout spurious branches from the medial axis. We extensively compare the CPMA\nwith state-of-the-art pruning methods and highlight our method's noise\nrobustness and isometric equivariance. We found that our pruning approach\nachieves competitive results and yields stable medial axes even in scenarios\nwith significant contour perturbations.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 00:44:05 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Pati\u00f1o", "Diego", ""], ["Branch", "John", ""]]}, {"id": "2012.02911", "submitter": "Huan Wang", "authors": "Huan Wang, Suhas Lohit, Michael Jones, Yun Fu", "title": "Multi-head Knowledge Distillation for Model Compression", "comments": "Copyright: 2020 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods of knowledge distillation have been developed for neural\nnetwork compression. While they all use the KL divergence loss to align the\nsoft outputs of the student model more closely with that of the teacher, the\nvarious methods differ in how the intermediate features of the student are\nencouraged to match those of the teacher. In this paper, we propose a\nsimple-to-implement method using auxiliary classifiers at intermediate layers\nfor matching features, which we refer to as multi-head knowledge distillation\n(MHKD). We add loss terms for training the student that measure the\ndissimilarity between student and teacher outputs of the auxiliary classifiers.\nAt the same time, the proposed method also provides a natural way to measure\ndifferences at the intermediate layers even though the dimensions of the\ninternal teacher and student features may be different. Through several\nexperiments in image classification on multiple datasets we show that the\nproposed method outperforms prior relevant approaches presented in the\nliterature.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 00:49:14 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Wang", "Huan", ""], ["Lohit", "Suhas", ""], ["Jones", "Michael", ""], ["Fu", "Yun", ""]]}, {"id": "2012.02924", "submitter": "Bokui Shen", "authors": "Bokui Shen, Fei Xia, Chengshu Li, Roberto Mart\\'in-Mart\\'in, Linxi\n  Fan, Guanzhi Wang, Claudia P\\'erez-D'Arpino, Shyamal Buch, Sanjana\n  Srivastava, Lyne P. Tchapmi, Micael E. Tchapmi, Kent Vainio, Li Fei-Fei,\n  Silvio Savarese", "title": "iGibson 1.0: a Simulation Environment for Interactive Tasks in Large\n  Realistic Scenes", "comments": null, "journal-ref": "2021 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2021)", "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present iGibson 1.0, a novel simulation environment to develop robotic\nsolutions for interactive tasks in large-scale realistic scenes. Our\nenvironment contains 15 fully interactive home-sized scenes with 108 rooms\npopulated with rigid and articulated objects. The scenes are replicas of\nreal-world homes, with distribution and the layout of objects aligned to those\nof the real world. iGibson 1.0 integrates several key features to facilitate\nthe study of interactive tasks: i) generation of high-quality virtual sensor\nsignals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain\nrandomization to change the materials of the objects (both visual and physical)\nand/or their shapes, iii) integrated sampling-based motion planners to generate\ncollision-free trajectories for robot bases and arms, and iv) intuitive\nhuman-iGibson interface that enables efficient collection of human\ndemonstrations. Through experiments, we show that the full interactivity of the\nscenes enables agents to learn useful visual representations that accelerate\nthe training of downstream manipulation tasks. We also show that iGibson 1.0\nfeatures enable the generalization of navigation agents, and that the\nhuman-iGibson interface and integrated motion planners facilitate efficient\nimitation learning of human demonstrated (mobile) manipulation behaviors.\niGibson 1.0 is open-source, equipped with comprehensive examples and\ndocumentation. For more information, visit our project website:\nhttp://svl.stanford.edu/igibson/\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 02:14:17 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 02:44:59 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 20:45:12 GMT"}, {"version": "v4", "created": "Mon, 19 Jul 2021 21:24:52 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Shen", "Bokui", ""], ["Xia", "Fei", ""], ["Li", "Chengshu", ""], ["Mart\u00edn-Mart\u00edn", "Roberto", ""], ["Fan", "Linxi", ""], ["Wang", "Guanzhi", ""], ["P\u00e9rez-D'Arpino", "Claudia", ""], ["Buch", "Shyamal", ""], ["Srivastava", "Sanjana", ""], ["Tchapmi", "Lyne P.", ""], ["Tchapmi", "Micael E.", ""], ["Vainio", "Kent", ""], ["Fei-Fei", "Li", ""], ["Savarese", "Silvio", ""]]}, {"id": "2012.02938", "submitter": "Ze Wang", "authors": "Ze Wang, Sihao Ding, Ying Li, Jonas Fenn, Sohini Roychowdhury, Andreas\n  Wallin, Lane Martin, Scott Ryvola, Guillermo Sapiro, and Qiang Qiu", "title": "Cirrus: A Long-range Bi-pattern LiDAR Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Cirrus, a new long-range bi-pattern LiDAR public\ndataset for autonomous driving tasks such as 3D object detection, critical to\nhighway driving and timely decision making. Our platform is equipped with a\nhigh-resolution video camera and a pair of LiDAR sensors with a 250-meter\neffective range, which is significantly longer than existing public datasets.\nWe record paired point clouds simultaneously using both Gaussian and uniform\nscanning patterns. Point density varies significantly across such a long range,\nand different scanning patterns further diversify object representation in\nLiDAR. In Cirrus, eight categories of objects are exhaustively annotated in the\nLiDAR point clouds for the entire effective range. To illustrate the kind of\nstudies supported by this new dataset, we introduce LiDAR model adaptation\nacross different ranges, scanning patterns, and sensor devices. Promising\nresults show the great potential of this new dataset to the robotics and\ncomputer vision communities.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 03:18:31 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Wang", "Ze", ""], ["Ding", "Sihao", ""], ["Li", "Ying", ""], ["Fenn", "Jonas", ""], ["Roychowdhury", "Sohini", ""], ["Wallin", "Andreas", ""], ["Martin", "Lane", ""], ["Ryvola", "Scott", ""], ["Sapiro", "Guillermo", ""], ["Qiu", "Qiang", ""]]}, {"id": "2012.02951", "submitter": "Tashnim Chowdhury", "authors": "Maryam Rahnemoonfar, Tashnim Chowdhury, Argho Sarkar, Debvrat\n  Varshney, Masoud Yari, Robin Murphy", "title": "FloodNet: A High Resolution Aerial Imagery Dataset for Post Flood Scene\n  Understanding", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual scene understanding is the core task in making any crucial decision in\nany computer vision system. Although popular computer vision datasets like\nCityscapes, MS-COCO, PASCAL provide good benchmarks for several tasks (e.g.\nimage classification, segmentation, object detection), these datasets are\nhardly suitable for post disaster damage assessments. On the other hand,\nexisting natural disaster datasets include mainly satellite imagery which have\nlow spatial resolution and a high revisit period. Therefore, they do not have a\nscope to provide quick and efficient damage assessment tasks. Unmanned Aerial\nVehicle(UAV) can effortlessly access difficult places during any disaster and\ncollect high resolution imagery that is required for aforementioned tasks of\ncomputer vision. To address these issues we present a high resolution UAV\nimagery, FloodNet, captured after the hurricane Harvey. This dataset\ndemonstrates the post flooded damages of the affected areas. The images are\nlabeled pixel-wise for semantic segmentation task and questions are produced\nfor the task of visual question answering. FloodNet poses several challenges\nincluding detection of flooded roads and buildings and distinguishing between\nnatural water and flooded water. With the advancement of deep learning\nalgorithms, we can analyze the impact of any disaster which can make a precise\nunderstanding of the affected areas. In this paper, we compare and contrast the\nperformances of baseline methods for image classification, semantic\nsegmentation, and visual question answering on our dataset.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 05:15:36 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Rahnemoonfar", "Maryam", ""], ["Chowdhury", "Tashnim", ""], ["Sarkar", "Argho", ""], ["Varshney", "Debvrat", ""], ["Yari", "Masoud", ""], ["Murphy", "Robin", ""]]}, {"id": "2012.02961", "submitter": "Yerbolat Khassanov", "authors": "Madina Abdrakhmanova, Askat Kuzdeuov, Sheikh Jarju, Yerbolat\n  Khassanov, Michael Lewis and Huseyin Atakan Varol", "title": "SpeakingFaces: A Large-Scale Multimodal Dataset of Voice Commands with\n  Visual and Thermal Video Streams", "comments": "20 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present SpeakingFaces as a publicly-available large-scale multimodal\ndataset developed to support machine learning research in contexts that utilize\na combination of thermal, visual, and audio data streams; examples include\nhuman-computer interaction, biometric authentication, recognition systems,\ndomain transfer, and speech recognition. SpeakingFaces is comprised of aligned\nhigh-resolution thermal and visual spectra image streams of fully-framed faces\nsynchronized with audio recordings of each subject speaking approximately 100\nimperative phrases. Data were collected from 142 subjects, yielding over 13,000\ninstances of synchronized data (~3.8 TB). For technical validation, we\ndemonstrate two baseline examples. The first baseline shows classification by\ngender, utilizing different combinations of the three data streams in both\nclean and noisy environments. The second example consists of thermal-to-visual\nfacial image translation, as an instance of domain transfer.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 06:49:42 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 05:00:44 GMT"}, {"version": "v3", "created": "Sat, 1 May 2021 05:27:42 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Abdrakhmanova", "Madina", ""], ["Kuzdeuov", "Askat", ""], ["Jarju", "Sheikh", ""], ["Khassanov", "Yerbolat", ""], ["Lewis", "Michael", ""], ["Varol", "Huseyin Atakan", ""]]}, {"id": "2012.02970", "submitter": "Tingwei Li", "authors": "Tingwei Li, Ruiwen Zhang, Qing Li", "title": "Multi Scale Temporal Graph Networks For Skeleton-based Action\n  Recognition", "comments": null, "journal-ref": "2020,Computer Science & Information Technology (CS & IT)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph convolutional networks (GCNs) can effectively capture the features of\nrelated nodes and improve the performance of the model. More attention is paid\nto employing GCN in Skeleton-Based action recognition. But existing methods\nbased on GCNs have two problems. First, the consistency of temporal and spatial\nfeatures is ignored for extracting features node by node and frame by frame. To\nobtain spatiotemporal features simultaneously, we design a generic\nrepresentation of skeleton sequences for action recognition and propose a novel\nmodel called Temporal Graph Networks (TGN). Secondly, the adjacency matrix of\nthe graph describing the relation of joints is mostly dependent on the physical\nconnection between joints. To appropriately describe the relations between\njoints in the skeleton graph, we propose a multi-scale graph strategy, adopting\na full-scale graph, part-scale graph, and core-scale graph to capture the local\nfeatures of each joint and the contour features of important joints.\nExperiments were carried out on two large datasets and results show that TGN\nwith our graph strategy outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 08:08:25 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Li", "Tingwei", ""], ["Zhang", "Ruiwen", ""], ["Li", "Qing", ""]]}, {"id": "2012.02992", "submitter": "Tamar Rott Shaham", "authors": "Tamar Rott Shaham, Michael Gharbi, Richard Zhang, Eli Shechtman, Tomer\n  Michaeli", "title": "Spatially-Adaptive Pixelwise Networks for Fast Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new generator architecture, aimed at fast and efficient\nhigh-resolution image-to-image translation. We design the generator to be an\nextremely lightweight function of the full-resolution image. In fact, we use\npixel-wise networks; that is, each pixel is processed independently of others,\nthrough a composition of simple affine transformations and nonlinearities. We\ntake three important steps to equip such a seemingly simple function with\nadequate expressivity. First, the parameters of the pixel-wise networks are\nspatially varying so they can represent a broader function class than simple\n1x1 convolutions. Second, these parameters are predicted by a fast\nconvolutional network that processes an aggressively low-resolution\nrepresentation of the input; Third, we augment the input image with a\nsinusoidal encoding of spatial coordinates, which provides an effective\ninductive bias for generating realistic novel high-frequency image content. As\na result, our model is up to 18x faster than state-of-the-art baselines. We\nachieve this speedup while generating comparable visual quality across\ndifferent image resolutions and translation domains.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 10:02:03 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Shaham", "Tamar Rott", ""], ["Gharbi", "Michael", ""], ["Zhang", "Richard", ""], ["Shechtman", "Eli", ""], ["Michaeli", "Tomer", ""]]}, {"id": "2012.02994", "submitter": "Jin Ye", "authors": "Jin Ye, Junjun He, Xiaojiang Peng, Wenhao Wu, and Yu Qiao", "title": "Attention-Driven Dynamic Graph Convolutional Network for Multi-Label\n  Image Recognition", "comments": "This paper has been accepted by ECCV 2020 (Source codes have been\n  released on https://github.com/Yejin0111/ADD-GCN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies often exploit Graph Convolutional Network (GCN) to model label\ndependencies to improve recognition accuracy for multi-label image recognition.\nHowever, constructing a graph by counting the label co-occurrence possibilities\nof the training data may degrade model generalizability, especially when there\nexist occasional co-occurrence objects in test images. Our goal is to eliminate\nsuch bias and enhance the robustness of the learnt features. To this end, we\npropose an Attention-Driven Dynamic Graph Convolutional Network (ADD-GCN) to\ndynamically generate a specific graph for each image. ADD-GCN adopts a Dynamic\nGraph Convolutional Network (D-GCN) to model the relation of content-aware\ncategory representations that are generated by a Semantic Attention Module\n(SAM). Extensive experiments on public multi-label benchmarks demonstrate the\neffectiveness of our method, which achieves mAPs of 85.2%, 96.0%, and 95.5% on\nMS-COCO, VOC2007, and VOC2012, respectively, and outperforms current\nstate-of-the-art methods with a clear margin. All codes can be found at\nhttps://github.com/Yejin0111/ADD-GCN.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 10:10:12 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Ye", "Jin", ""], ["He", "Junjun", ""], ["Peng", "Xiaojiang", ""], ["Wu", "Wenhao", ""], ["Qiao", "Yu", ""]]}, {"id": "2012.03003", "submitter": "Xiuxiu Bai", "authors": "Xiuxiu Bai, Lele Ye, Zhe Liu", "title": "ProMask: Probability Mask for Skeleton Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting object skeletons in natural images presents challenging, due to\nvaried object scales, the complexity of backgrounds and various noises. The\nskeleton is a highly compressing shape representation, which can bring some\nessential advantages but cause the difficulties of detection. This skeleton\nline occupies a rare proportion of an image and is overly sensitive to spatial\nposition. Inspired by these issues, we propose the ProMask, which is a novel\nskeleton detection model. The ProMask includes the probability mask and vector\nrouter. The skeleton probability mask representation explicitly encodes\nskeletons with segmentation signals, which can provide more supervised\ninformation to learn and pay more attention to ground-truth skeleton pixels.\nMoreover, the vector router module possesses two sets of orthogonal basis\nvectors in a two-dimensional space, which can dynamically adjust the predicted\nskeleton position. We evaluate our method on the well-known skeleton datasets,\nrealizing the better performance than state-of-the-art approaches. Especially,\nProMask significantly outperforms the competitive DeepFlux by 6.2% on the\nchallenging SYM-PASCAL dataset. We consider that our proposed skeleton\nprobability mask could serve as a solid baseline for future skeleton detection,\nsince it is very effective and it requires about 10 lines of code.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 10:54:36 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Bai", "Xiuxiu", ""], ["Ye", "Lele", ""], ["Liu", "Zhe", ""]]}, {"id": "2012.03014", "submitter": "Matthieu Martin", "authors": "Matthieu Martin, Bruno Sciolla, Micha\\\"el Sdika, Philippe Qu\\'etin,\n  Philippe Delachartre", "title": "Automatic Segmentation and Location Learning of Neonatal Cerebral\n  Ventricles in 3D Ultrasound Data Combining CNN and CPPN", "comments": "18 pages, 14 figures", "journal-ref": null, "doi": "10.1016/j.compbiomed.2021.104268", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Preterm neonates are highly likely to suffer from ventriculomegaly, a\ndilation of the Cerebral Ventricular System (CVS). This condition can develop\ninto life-threatening hydrocephalus and is correlated with future\nneuro-developmental impairments. Consequently, it must be detected and\nmonitored by physicians. In clinical routing, manual 2D measurements are\nperformed on 2D ultrasound (US) images to estimate the CVS volume but this\npractice is imprecise due to the unavailability of 3D information. A way to\ntackle this problem would be to develop automatic CVS segmentation algorithms\nfor 3D US data. In this paper, we investigate the potential of 2D and 3D\nConvolutional Neural Networks (CNN) to solve this complex task and propose to\nuse Compositional Pattern Producing Network (CPPN) to enable the CNNs to learn\nCVS location. Our database was composed of 25 3D US volumes collected on 21\npreterm nenonates at the age of $35.8 \\pm 1.6$ gestational weeks. We found that\nthe CPPN enables to encode CVS location, which increases the accuracy of the\nCNNs when they have few layers. Accuracy of the 2D and 3D CNNs reached\nintraobserver variability (IOV) in the case of dilated ventricles with Dice of\n$0.893 \\pm 0.008$ and $0.886 \\pm 0.004$ respectively (IOV = $0.898 \\pm 0.008$)\nand with volume errors of $0.45 \\pm 0.42$ cm$^3$ and $0.36 \\pm 0.24$ cm$^3$\nrespectively (IOV = $0.41 \\pm 0.05$ cm$^3$). 3D CNNs were more accurate than 2D\nCNNs in the case of normal ventricles with Dice of $0.797 \\pm 0.041$ against\n$0.776 \\pm 0.038$ (IOV = $0.816 \\pm 0.009$) and volume errors of $0.35 \\pm\n0.29$ cm$^3$ against $0.35 \\pm 0.24$ cm$^3$ (IOV = $0.2 \\pm 0.11$ cm$^3$). The\nbest segmentation time of volumes of size $320 \\times 320 \\times 320$ was\nobtained by a 2D CNN in $3.5 \\pm 0.2$ s.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 11:57:26 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 14:48:22 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Martin", "Matthieu", ""], ["Sciolla", "Bruno", ""], ["Sdika", "Micha\u00ebl", ""], ["Qu\u00e9tin", "Philippe", ""], ["Delachartre", "Philippe", ""]]}, {"id": "2012.03015", "submitter": "Wu Zheng", "authors": "Wu Zheng, Weiliang Tang, Sijin Chen, Li Jiang, Chi-Wing Fu", "title": "CIA-SSD: Confident IoU-Aware Single-Stage Object Detector From Point\n  Cloud", "comments": "Accepted by the 35th AAAI Conference on Artificial Intelligence\n  (AAAI), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing single-stage detectors for locating objects in point clouds often\ntreat object localization and category classification as separate tasks, so the\nlocalization accuracy and classification confidence may not well align. To\naddress this issue, we present a new single-stage detector named the Confident\nIoU-Aware Single-Stage object Detector (CIA-SSD). First, we design the\nlightweight Spatial-Semantic Feature Aggregation module to adaptively fuse\nhigh-level abstract semantic features and low-level spatial features for\naccurate predictions of bounding boxes and classification confidence. Also, the\npredicted confidence is further rectified with our designed IoU-aware\nconfidence rectification module to make the confidence more consistent with the\nlocalization accuracy. Based on the rectified confidence, we further formulate\nthe Distance-variant IoU-weighted NMS to obtain smoother regressions and avoid\nredundant predictions. We experiment CIA-SSD on 3D car detection in the KITTI\ntest set and show that it attains top performance in terms of the official\nranking metric (moderate AP 80.28%) and above 32 FPS inference speed,\noutperforming all prior single-stage detectors. The code is available at\nhttps://github.com/Vegeta2020/CIA-SSD.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 12:00:08 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zheng", "Wu", ""], ["Tang", "Weiliang", ""], ["Chen", "Sijin", ""], ["Jiang", "Li", ""], ["Fu", "Chi-Wing", ""]]}, {"id": "2012.03021", "submitter": "Takahiro Kinoshita", "authors": "Takahiro Kinoshita and Satoshi Ono", "title": "Depth estimation from 4D light field videos", "comments": "6 pages, 6 figures, International Workshop on Advanced Image\n  Technology (IWAIT) 2021", "journal-ref": null, "doi": "10.1117/12.2591012", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth (disparity) estimation from 4D Light Field (LF) images has been a\nresearch topic for the last couple of years. Most studies have focused on depth\nestimation from static 4D LF images while not considering temporal information,\ni.e., LF videos. This paper proposes an end-to-end neural network architecture\nfor depth estimation from 4D LF videos. This study also constructs a\nmedium-scale synthetic 4D LF video dataset that can be used for training deep\nlearning-based methods. Experimental results using synthetic and real-world 4D\nLF videos show that temporal information contributes to the improvement of\ndepth estimation accuracy in noisy regions. Dataset and code is available at:\nhttps://mediaeng-lfv.github.io/LFV_Disparity_Estimation\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 12:33:15 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 08:25:32 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Kinoshita", "Takahiro", ""], ["Ono", "Satoshi", ""]]}, {"id": "2012.03028", "submitter": "Qijian Zhang", "authors": "Qijian Zhang, Junhui Hou, Yue Qian, Juyong Zhang, Ying He", "title": "ParaNet: Deep Regular Representation for 3D Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although convolutional neural networks have achieved remarkable success in\nanalyzing 2D images/videos, it is still non-trivial to apply the well-developed\n2D techniques in regular domains to the irregular 3D point cloud data. To\nbridge this gap, we propose ParaNet, a novel end-to-end deep learning\nframework, for representing 3D point clouds in a completely regular and nearly\nlossless manner. To be specific, ParaNet converts an irregular 3D point cloud\ninto a regular 2D color image, named point geometry image (PGI), where each\npixel encodes the spatial coordinates of a point. In contrast to conventional\nregular representation modalities based on multi-view projection and\nvoxelization, the proposed representation is differentiable and reversible.\nTechnically, ParaNet is composed of a surface embedding module, which\nparameterizes 3D surface points onto a unit square, and a grid resampling\nmodule, which resamples the embedded 2D manifold over regular dense grids. Note\nthat ParaNet is unsupervised, i.e., the training simply relies on\nreference-free geometry constraints. The PGIs can be seamlessly coupled with a\ntask network established upon standard and mature techniques for 2D\nimages/videos to realize a specific task for 3D point clouds. We evaluate\nParaNet over shape classification and point cloud upsampling, in which our\nsolutions perform favorably against the existing state-of-the-art methods. We\nbelieve such a paradigm will open up many possibilities to advance the progress\nof deep learning-based point cloud processing and understanding.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 13:19:55 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zhang", "Qijian", ""], ["Hou", "Junhui", ""], ["Qian", "Yue", ""], ["Zhang", "Juyong", ""], ["He", "Ying", ""]]}, {"id": "2012.03040", "submitter": "Yigit Baran Can", "authors": "Yigit Baran Can, Alexander Liniger, Ozan Unal, Danda Paudel, Luc Van\n  Gool", "title": "Understanding Bird's-Eye View Semantic HD-Maps Using an Onboard\n  Monocular Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous navigation requires scene understanding of the action-space to\nmove or anticipate events. For planner agents moving on the ground plane, such\nas autonomous vehicles, this translates to scene understanding in the\nbird's-eye view. However, the onboard cameras of autonomous cars are\ncustomarily mounted horizontally for a better view of the surrounding. In this\nwork, we study scene understanding in the form of online estimation of semantic\nbird's-eye-view HD-maps using the video input from a single onboard camera. We\nstudy three key aspects of this task, image-level understanding, BEV level\nunderstanding, and the aggregation of temporal information. Based on these\nthree pillars we propose a novel architecture that combines these three\naspects. In our extensive experiments, we demonstrate that the considered\naspects are complementary to each other for HD-map understanding. Furthermore,\nthe proposed architecture significantly surpasses the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 14:39:14 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Can", "Yigit Baran", ""], ["Liniger", "Alexander", ""], ["Unal", "Ozan", ""], ["Paudel", "Danda", ""], ["Van Gool", "Luc", ""]]}, {"id": "2012.03044", "submitter": "Xiao Zhang", "authors": "Xiao Zhang, Michael Maire", "title": "Self-Supervised Visual Representation Learning from Hierarchical\n  Grouping", "comments": "Accepted by NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We create a framework for bootstrapping visual representation learning from a\nprimitive visual grouping capability. We operationalize grouping via a contour\ndetector that partitions an image into regions, followed by merging of those\nregions into a tree hierarchy. A small supervised dataset suffices for training\nthis grouping primitive. Across a large unlabeled dataset, we apply this\nlearned primitive to automatically predict hierarchical region structure. These\npredictions serve as guidance for self-supervised contrastive feature learning:\nwe task a deep network with producing per-pixel embeddings whose pairwise\ndistances respect the region hierarchy. Experiments demonstrate that our\napproach can serve as state-of-the-art generic pre-training, benefiting\ndownstream tasks. We additionally explore applications to semantic region\nsearch and video-based object instance tracking.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 14:54:08 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zhang", "Xiao", ""], ["Maire", "Michael", ""]]}, {"id": "2012.03061", "submitter": "Filipe Cordeiro", "authors": "Filipe R. Cordeiro and Gustavo Carneiro", "title": "A Survey on Deep Learning with Noisy Labels: How to train your model\n  when you cannot trust on the annotations?", "comments": "Paper published at SIBRAPI, 2020 (camera ready version)", "journal-ref": "2020 33rd SIBGRAPI Conference on Graphics, Patterns and Images\n  (SIBGRAPI)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy Labels are commonly present in data sets automatically collected from\nthe internet, mislabeled by non-specialist annotators, or even specialists in a\nchallenging task, such as in the medical field. Although deep learning models\nhave shown significant improvements in different domains, an open issue is\ntheir ability to memorize noisy labels during training, reducing their\ngeneralization potential. As deep learning models depend on correctly labeled\ndata sets and label correctness is difficult to guarantee, it is crucial to\nconsider the presence of noisy labels for deep learning training. Several\napproaches have been proposed in the literature to improve the training of deep\nlearning models in the presence of noisy labels. This paper presents a survey\non the main techniques in literature, in which we classify the algorithm in the\nfollowing groups: robust losses, sample weighting, sample selection,\nmeta-learning, and combined approaches. We also present the commonly used\nexperimental setup, data sets, and results of the state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 15:45:20 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Cordeiro", "Filipe R.", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "2012.03065", "submitter": "Guy Gafni", "authors": "Guy Gafni, Justus Thies, Michael Zollh\\\"ofer, Matthias Nie{\\ss}ner", "title": "Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar\n  Reconstruction", "comments": "Video: https://youtu.be/m7oROLdQnjk | Project page:\n  https://gafniguy.github.io/4D-Facial-Avatars/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present dynamic neural radiance fields for modeling the appearance and\ndynamics of a human face. Digitally modeling and reconstructing a talking human\nis a key building-block for a variety of applications. Especially, for\ntelepresence applications in AR or VR, a faithful reproduction of the\nappearance including novel viewpoints or head-poses is required. In contrast to\nstate-of-the-art approaches that model the geometry and material properties\nexplicitly, or are purely image-based, we introduce an implicit representation\nof the head based on scene representation networks. To handle the dynamics of\nthe face, we combine our scene representation network with a low-dimensional\nmorphable model which provides explicit control over pose and expressions. We\nuse volumetric rendering to generate images from this hybrid representation and\ndemonstrate that such a dynamic neural scene representation can be learned from\nmonocular input data only, without the need of a specialized capture setup. In\nour experiments, we show that this learned volumetric representation allows for\nphoto-realistic image generation that surpasses the quality of state-of-the-art\nvideo-based reenactment methods.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 16:01:16 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Gafni", "Guy", ""], ["Thies", "Justus", ""], ["Zollh\u00f6fer", "Michael", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2012.03087", "submitter": "Filipe Cordeiro", "authors": "Charles N. C. Freitas, Filipe R. Cordeiro and Valmir Macario", "title": "MyFood: A Food Segmentation and Classification System to Aid Nutritional\n  Monitoring", "comments": "Paper published at SIBRAPI 2020 (Camera ready version)", "journal-ref": "2020 33rd SIBGRAPI Conference on Graphics, Patterns and Images\n  (SIBGRAPI)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The absence of food monitoring has contributed significantly to the increase\nin the population's weight. Due to the lack of time and busy routines, most\npeople do not control and record what is consumed in their diet. Some solutions\nhave been proposed in computer vision to recognize food images, but few are\nspecialized in nutritional monitoring. This work presents the development of an\nintelligent system that classifies and segments food presented in images to\nhelp the automatic monitoring of user diet and nutritional intake. This work\nshows a comparative study of state-of-the-art methods for image classification\nand segmentation, applied to food recognition. In our methodology, we compare\nthe FCN, ENet, SegNet, DeepLabV3+, and Mask RCNN algorithms. We build a dataset\ncomposed of the most consumed Brazilian food types, containing nine classes and\na total of 1250 images. The models were evaluated using the following metrics:\nIntersection over Union, Sensitivity, Specificity, Balanced Precision, and\nPositive Predefined Value. We also propose an system integrated into a mobile\napplication that automatically recognizes and estimates the nutrients in a\nmeal, assisting people with better nutritional monitoring. The proposed\nsolution showed better results than the existing ones in the market. The\ndataset is publicly available at the following link\nhttp://doi.org/10.5281/zenodo.4041488\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 17:40:05 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Freitas", "Charles N. C.", ""], ["Cordeiro", "Filipe R.", ""], ["Macario", "Valmir", ""]]}, {"id": "2012.03093", "submitter": "Hamed Alemohammad", "authors": "Aditya Kulkarni, Tharun Mohandoss, Daniel Northrup, Ernest Mwebaze,\n  Hamed Alemohammad", "title": "Semantic Segmentation of Medium-Resolution Satellite Imagery using\n  Conditional Generative Adversarial Networks", "comments": "Presented at the AI for Earth Sciences Workshop at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic segmentation of satellite imagery is a common approach to identify\npatterns and detect changes around the planet. Most of the state-of-the-art\nsemantic segmentation models are trained in a fully supervised way using\nConvolutional Neural Network (CNN). The generalization property of CNN is poor\nfor satellite imagery because the data can be very diverse in terms of\nlandscape types, image resolutions, and scarcity of labels for different\ngeographies and seasons. Hence, the performance of CNN doesn't translate well\nto images from unseen regions or seasons. Inspired by Conditional Generative\nAdversarial Networks (CGAN) based approach of image-to-image translation for\nhigh-resolution satellite imagery, we propose a CGAN framework for land cover\nclassification using medium-resolution Sentinel-2 imagery. We find that the\nCGAN model outperforms the CNN model of similar complexity by a significant\nmargin on an unseen imbalanced test dataset.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 18:18:45 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Kulkarni", "Aditya", ""], ["Mohandoss", "Tharun", ""], ["Northrup", "Daniel", ""], ["Mwebaze", "Ernest", ""], ["Alemohammad", "Hamed", ""]]}, {"id": "2012.03107", "submitter": "Xiaoixa Wu", "authors": "Xiaoxia Wu and Ethan Dyer and Behnam Neyshabur", "title": "When Do Curricula Work?", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inspired by human learning, researchers have proposed ordering examples\nduring training based on their difficulty. Both curriculum learning, exposing a\nnetwork to easier examples early in training, and anti-curriculum learning,\nshowing the most difficult examples first, have been suggested as improvements\nto the standard i.i.d. training. In this work, we set out to investigate the\nrelative benefits of ordered learning. We first investigate the \\emph{implicit\ncurricula} resulting from architectural and optimization bias and find that\nsamples are learned in a highly consistent order. Next, to quantify the benefit\nof \\emph{explicit curricula}, we conduct extensive experiments over thousands\nof orderings spanning three kinds of learning: curriculum, anti-curriculum, and\nrandom-curriculum -- in which the size of the training dataset is dynamically\nincreased over time, but the examples are randomly ordered. We find that for\nstandard benchmark datasets, curricula have only marginal benefits, and that\nrandomly ordered samples perform as well or better than curricula and\nanti-curricula, suggesting that any benefit is entirely due to the dynamic\ntraining set size. Inspired by common use cases of curriculum learning in\npractice, we investigate the role of limited training time budget and noisy\ndata in the success of curriculum learning. Our experiments demonstrate that\ncurriculum, but not anti-curriculum can indeed improve the performance either\nwith limited training time budget or in existence of noisy data.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 19:41:30 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 05:08:30 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2021 17:38:58 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Wu", "Xiaoxia", ""], ["Dyer", "Ethan", ""], ["Neyshabur", "Behnam", ""]]}, {"id": "2012.03108", "submitter": "Hamed Alemohammad", "authors": "Tharun Mohandoss, Aditya Kulkarni, Daniel Northrup, Ernest Mwebaze,\n  Hamed Alemohammad", "title": "Generating Synthetic Multispectral Satellite Imagery from Sentinel-2", "comments": "Presented at the AI for Earth Sciences Workshop at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-spectral satellite imagery provides valuable data at global scale for\nmany environmental and socio-economic applications. Building supervised machine\nlearning models based on these imagery, however, may require ground reference\nlabels which are not available at global scale. Here, we propose a generative\nmodel to produce multi-resolution multi-spectral imagery based on Sentinel-2\ndata. The resulting synthetic images are indistinguishable from real ones by\nhumans. This technique paves the road for future work to generate labeled\nsynthetic imagery that can be used for data augmentation in data scarce regions\nand applications.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 19:41:33 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Mohandoss", "Tharun", ""], ["Kulkarni", "Aditya", ""], ["Northrup", "Daniel", ""], ["Mwebaze", "Ernest", ""], ["Alemohammad", "Hamed", ""]]}, {"id": "2012.03110", "submitter": "Steffen Jung", "authors": "Steffen Jung and Margret Keuper", "title": "Spectral Distribution Aware Image Generation", "comments": "Accepted at AAAI 2021 (conference version). Code:\n  https://github.com/steffen-jung/SpectralGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep generative models for photo-realistic images have led\nto high quality visual results. Such models learn to generate data from a given\ntraining distribution such that generated images can not be easily\ndistinguished from real images by the human eye. Yet, recent work on the\ndetection of such fake images pointed out that they are actually easily\ndistinguishable by artifacts in their frequency spectra. In this paper, we\npropose to generate images according to the frequency distribution of the real\ndata by employing a spectral discriminator. The proposed discriminator is\nlightweight, modular and works stably with different commonly used GAN losses.\nWe show that the resulting models can better generate images with realistic\nfrequency spectra, which are thus harder to detect by this cue.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 19:46:48 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 15:33:15 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Jung", "Steffen", ""], ["Keuper", "Margret", ""]]}, {"id": "2012.03111", "submitter": "Hamed Alemohammad", "authors": "Hamed Alemohammad, Kevin Booth", "title": "LandCoverNet: A global benchmark land cover classification training\n  dataset", "comments": "Presented at the AI for Earth Sciences Workshop at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Regularly updated and accurate land cover maps are essential for monitoring\n14 of the 17 Sustainable Development Goals. Multispectral satellite imagery\nprovide high-quality and valuable information at global scale that can be used\nto develop land cover classification models. However, such a global application\nrequires a geographically diverse training dataset. Here, we present\nLandCoverNet, a global training dataset for land cover classification based on\nSentinel-2 observations at 10m spatial resolution. Land cover class labels are\ndefined based on annual time-series of Sentinel-2, and verified by consensus\namong three human annotators.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 19:48:57 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Alemohammad", "Hamed", ""], ["Booth", "Kevin", ""]]}, {"id": "2012.03121", "submitter": "Meytal Rapoport-Lavie", "authors": "Meytal Rapoport-Lavie and Dan Raviv", "title": "It's All Around You: Range-Guided Cylindrical Network for 3D Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern perception systems in the field of autonomous driving rely on 3D data\nanalysis. LiDAR sensors are frequently used to acquire such data due to their\nincreased resilience to different lighting conditions. Although rotating LiDAR\nscanners produce ring-shaped patterns in space, most networks analyze their\ndata using an orthogonal voxel sampling strategy. This work presents a novel\napproach for analyzing 3D data produced by 360-degree depth scanners, utilizing\na more suitable coordinate system, which is aligned with the scanning pattern.\nFurthermore, we introduce a novel notion of range-guided convolutions, adapting\nthe receptive field by distance from the ego vehicle and the object's scale.\nOur network demonstrates powerful results on the nuScenes challenge, comparable\nto current state-of-the-art architectures. The backbone architecture introduced\nin this work can be easily integrated onto other pipelines as well.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 21:02:18 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Rapoport-Lavie", "Meytal", ""], ["Raviv", "Dan", ""]]}, {"id": "2012.03124", "submitter": "Kaiwen Xu", "authors": "Kaiwen Xu, Riqiang Gao, Mirza S. Khan, Shunxing Bao, Yucheng Tang,\n  Steve A. Deppen, Yuankai Huo, Kim L. Sandler, Pierre P. Massion, Mattias P.\n  Heinrich, Bennett A. Landman", "title": "Development and Characterization of a Chest CT Atlas", "comments": "Accepted by SPIE2021 Medical Imaging (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major goal of lung cancer screening is to identify individuals with\nparticular phenotypes that are associated with high risk of cancer. Identifying\nrelevant phenotypes is complicated by the variation in body position and body\ncomposition. In the brain, standardized coordinate systems (e.g., atlases) have\nenabled separate consideration of local features from gross/global structure.\nTo date, no analogous standard atlas has been presented to enable spatial\nmapping and harmonization in chest computational tomography (CT). In this\npaper, we propose a thoracic atlas built upon a large low dose CT (LDCT)\ndatabase of lung cancer screening program. The study cohort includes 466 male\nand 387 female subjects with no screening detected malignancy (age 46-79 years,\nmean 64.9 years). To provide spatial mapping, we optimize a multi-stage\ninter-subject non-rigid registration pipeline for the entire thoracic space. We\nevaluate the optimized pipeline relative to two baselines with alternative\nnon-rigid registration module: the same software with default parameters and an\nalternative software. We achieve a significant improvement in terms of\nregistration success rate based on manual QA. For the entire study cohort, the\noptimized pipeline achieves a registration success rate of 91.7%. The\napplication validity of the developed atlas is evaluated in terms of\ndiscriminative capability for different anatomic phenotypes, including body\nmass index (BMI), chronic obstructive pulmonary disease (COPD), and coronary\nartery calcification (CAC).\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 21:20:57 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Xu", "Kaiwen", ""], ["Gao", "Riqiang", ""], ["Khan", "Mirza S.", ""], ["Bao", "Shunxing", ""], ["Tang", "Yucheng", ""], ["Deppen", "Steve A.", ""], ["Huo", "Yuankai", ""], ["Sandler", "Kim L.", ""], ["Massion", "Pierre P.", ""], ["Heinrich", "Mattias P.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "2012.03129", "submitter": "Saeed Khaki", "authors": "Saeed Khaki, Hieu Pham and Lizhi Wang", "title": "Simultaneous Corn and Soybean Yield Prediction from Remote Sensing Data\n  Using Deep Transfer Learning", "comments": "14 pages, 8 figures, 7 tables", "journal-ref": "Scientific Reports, 11(1), 1-14 (2021)", "doi": "10.1038/s41598-021-89779-z", "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Large-scale crop yield estimation is, in part, made possible due to the\navailability of remote sensing data allowing for the continuous monitoring of\ncrops throughout their growth cycle. Having this information allows\nstakeholders the ability to make real-time decisions to maximize yield\npotential. Although various models exist that predict yield from remote sensing\ndata, there currently does not exist an approach that can estimate yield for\nmultiple crops simultaneously, and thus leads to more accurate predictions. A\nmodel that predicts the yield of multiple crops and concurrently considers the\ninteraction between multiple crop yields. We propose a new convolutional neural\nnetwork model called YieldNet which utilizes a novel deep learning framework\nthat uses transfer learning between corn and soybean yield predictions by\nsharing the weights of the backbone feature extractor. Additionally, to\nconsider the multi-target response variable, we propose a new loss function. We\nconduct our experiment using data from 1,132 counties for corn and 1,076\ncounties for soybean across the United States. Numerical results demonstrate\nthat our proposed method accurately predicts corn and soybean yield from one to\nfour months before the harvest with a MAE being 8.74% and 8.70% of the average\nyield, respectively, and is competitive to other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 22:09:07 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 18:10:16 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 15:20:12 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Khaki", "Saeed", ""], ["Pham", "Hieu", ""], ["Wang", "Lizhi", ""]]}, {"id": "2012.03145", "submitter": "Hemanth Manjunatha", "authors": "Chaitanya Thammineni, Hemanth Manjunatha, Ehsan T. Esfahani", "title": "Selective Eye-gaze Augmentation To Enhance Imitation Learning In Atari\n  Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the selective use of eye-gaze information in learning\nhuman actions in Atari games. Vast evidence suggests that our eye movement\nconvey a wealth of information about the direction of our attention and mental\nstates and encode the information necessary to complete a task. Based on this\nevidence, we hypothesize that selective use of eye-gaze, as a clue for\nattention direction, will enhance the learning from demonstration. For this\npurpose, we propose a selective eye-gaze augmentation (SEA) network that learns\nwhen to use the eye-gaze information. The proposed network architecture\nconsists of three sub-networks: gaze prediction, gating, and action prediction\nnetwork. Using the prior 4 game frames, a gaze map is predicted by the gaze\nprediction network which is used for augmenting the input frame. The gating\nnetwork will determine whether the predicted gaze map should be used in\nlearning and is fed to the final network to predict the action at the current\nframe. To validate this approach, we use publicly available Atari Human\nEye-Tracking And Demonstration (Atari-HEAD) dataset consists of 20 Atari games\nwith 28 million human demonstrations and 328 million eye-gazes (over game\nframes) collected from four subjects. We demonstrate the efficacy of selective\neye-gaze augmentation in comparison with state of the art Attention Guided\nImitation Learning (AGIL), Behavior Cloning (BC). The results indicate that the\nselective augmentation approach (the SEA network) performs significantly better\nthan the AGIL and BC. Moreover, to demonstrate the significance of selective\nuse of gaze through the gating network, we compare our approach with the random\nselection of the gaze. Even in this case, the SEA network performs\nsignificantly better validating the advantage of selectively using the gaze in\ndemonstration learning.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 23:35:55 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Thammineni", "Chaitanya", ""], ["Manjunatha", "Hemanth", ""], ["Esfahani", "Ehsan T.", ""]]}, {"id": "2012.03152", "submitter": "Pei Wang", "authors": "Zichu Liu, Qing Zhang, Pei Wang, Yaxin Li, Jingqian Sun", "title": "Automatic sampling and training method for wood-leaf classification\n  based on tree terrestrial point cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Terrestrial laser scanning technology provides an efficient and accuracy\nsolution for acquiring three-dimensional information of plants. The leaf-wood\nclassification of plant point cloud data is a fundamental step for some\nforestry and biological research. An automatic sampling and training method for\nclassification was proposed based on tree point cloud data. The plane fitting\nmethod was used for selecting leaf sample points and wood sample points\nautomatically, then two local features were calculated for training and\nclassification by using support vector machine (SVM) algorithm. The point cloud\ndata of ten trees were tested by using the proposed method and a manual\nselection method. The average correct classification rate and kappa coefficient\nare 0.9305 and 0.7904, respectively. The results show that the proposed method\nhad better efficiency and accuracy comparing to the manual selection method.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 00:18:41 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Liu", "Zichu", ""], ["Zhang", "Qing", ""], ["Wang", "Pei", ""], ["Li", "Yaxin", ""], ["Sun", "Jingqian", ""]]}, {"id": "2012.03153", "submitter": "Thanh Vu", "authors": "Thanh Vu, Marc Eder, True Price, Jan-Michael Frahm", "title": "Any-Width Networks", "comments": "8 pages. Published at CVPR 2020 Workshop on Efficient Deep Learning\n  in Computer Vision. Code at https://github.com/thanhmvu/awn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite remarkable improvements in speed and accuracy, convolutional neural\nnetworks (CNNs) still typically operate as monolithic entities at inference\ntime. This poses a challenge for resource-constrained practical applications,\nwhere both computational budgets and performance needs can vary with the\nsituation. To address these constraints, we propose the Any-Width Network\n(AWN), an adjustable-width CNN architecture and associated training routine\nthat allow for fine-grained control over speed and accuracy during inference.\nOur key innovation is the use of lower-triangular weight matrices which\nexplicitly address width-varying batch statistics while being naturally suited\nfor multi-width operations. We also show that this design facilitates an\nefficient training routine based on random width sampling. We empirically\ndemonstrate that our proposed AWNs compare favorably to existing methods while\nproviding maximally granular control during inference.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 00:22:01 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Vu", "Thanh", ""], ["Eder", "Marc", ""], ["Price", "True", ""], ["Frahm", "Jan-Michael", ""]]}, {"id": "2012.03170", "submitter": "Joshua Ball", "authors": "Joshua Ball", "title": "Food Classification with Convolutional Neural Networks and Multi-Class\n  Linear Discernment Analysis", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) have been successful in representing the\nfully-connected inferencing ability perceived to be seen in the human brain:\nthey take full advantage of the hierarchy-style patterns commonly seen in\ncomplex data and develop more patterns using simple features. Countless\nimplementations of CNNs have shown how strong their ability is to learn these\ncomplex patterns, particularly in the realm of image classification. However,\nthe cost of getting a high performance CNN to a so-called \"state of the art\"\nlevel is computationally costly. Even when using transfer learning, which\nutilize the very deep layers from models such as MobileNetV2, CNNs still take a\ngreat amount of time and resources. Linear discriminant analysis (LDA), a\ngeneralization of Fisher's linear discriminant, can be implemented in a\nmulti-class classification method to increase separability of class features\nwhile not needing a high performance system to do so for image classification.\nSimilarly, we also believe LDA has great promise in performing well. In this\npaper, we discuss our process of developing a robust CNN for food\nclassification as well as our effective implementation of multi-class LDA and\nprove that (1) CNN is superior to LDA for image classification and (2) why LDA\nshould not be left out of the races for image classification, particularly for\nbinary cases.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 03:28:58 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 04:27:23 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Ball", "Joshua", ""]]}, {"id": "2012.03173", "submitter": "Zhuoning Yuan", "authors": "Zhuoning Yuan, Yan Yan, Milan Sonka, Tianbao Yang", "title": "Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies\n  on Medical Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep AUC Maximization (DAM) is a paradigm for learning a deep neural network\nby maximizing the AUC score of the model on a dataset. Most previous works of\nAUC maximization focus on the perspective of optimization by designing\nefficient stochastic algorithms, and studies on generalization performance of\nDAM on difficult tasks are missing. In this work, we aim to make DAM more\npractical for interesting real-world applications (e.g., medical image\nclassification). First, we propose a new margin-based surrogate loss function\nfor the AUC score (named as the AUC margin loss). It is more robust than the\ncommonly used AUC square loss, while enjoying the same advantage in terms of\nlarge-scale stochastic optimization. Second, we conduct empirical studies of\nour DAM method on difficult medical image classification tasks, namely\nclassification of chest x-ray images for identifying many threatening diseases\nand classification of images of skin lesions for identifying melanoma. Our DAM\nmethod has achieved great success on these difficult tasks, i.e., the 1st place\non Stanford CheXpert competition (by the paper submission date) and Top 1% rank\n(rank 33 out of 3314 teams) on Kaggle 2020 Melanoma classification competition.\nWe also conduct extensive ablation studies to demonstrate the advantages of the\nnew AUC margin loss over the AUC square loss on benchmark datasets. To the best\nof our knowledge, this is the first work that makes DAM succeed on large-scale\nmedical image datasets.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 03:41:51 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Yuan", "Zhuoning", ""], ["Yan", "Yan", ""], ["Sonka", "Milan", ""], ["Yang", "Tianbao", ""]]}, {"id": "2012.03176", "submitter": "Zhihao Peng", "authors": "Zhihao Peng, Yuheng Jia, Hui Liu, Junhui Hou, Qingfu Zhang", "title": "Maximum Entropy Subspace Clustering Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep subspace clustering networks have attracted much attention in subspace\nclustering, in which an auto-encoder non-linearly maps the input data into a\nlatent space, and a fully connected layer named self-expressiveness module is\nintroduced to learn the affinity matrix via a typical regularization term\n(e.g., sparse or low-rank). However, the adopted regularization terms ignore\nthe connectivity within each subspace, limiting their clustering performance.\nIn addition, the adopted framework suffers from the coupling issue between the\nauto-encoder module and the self-expressiveness module, making the network\ntraining non-trivial. To tackle these two issues, we propose a novel deep\nsubspace clustering method named Maximum Entropy Subspace Clustering Network\n(MESC-Net). Specifically, MESC-Net maximizes the entropy of the affinity matrix\nto promote the connectivity within each subspace, in which its elements\ncorresponding to the same subspace are uniformly and densely distributed.\nFurthermore, we design a novel framework to explicitly decouple the\nauto-encoder module and the self-expressiveness module. We also theoretically\nprove that the learned affinity matrix satisfies the block-diagonal property\nunder the independent subspaces. Extensive quantitative and qualitative results\non commonly used benchmark datasets validate MESC-Net significantly outperforms\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 03:50:49 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 02:58:59 GMT"}, {"version": "v3", "created": "Sat, 5 Jun 2021 06:47:08 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Peng", "Zhihao", ""], ["Jia", "Yuheng", ""], ["Liu", "Hui", ""], ["Hou", "Junhui", ""], ["Zhang", "Qingfu", ""]]}, {"id": "2012.03194", "submitter": "Rui Fan", "authors": "Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas", "title": "Computer Stereo Vision for Autonomous Driving", "comments": "Book chapter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important component of autonomous systems, autonomous car perception\nhas had a big leap with recent advances in parallel computing architectures.\nWith the use of tiny but full-feature embedded supercomputers, computer stereo\nvision has been prevalently applied in autonomous cars for depth perception.\nThe two key aspects of computer stereo vision are speed and accuracy. They are\nboth desirable but conflicting properties, as the algorithms with better\ndisparity accuracy usually have higher computational complexity. Therefore, the\nmain aim of developing a computer stereo vision algorithm for resource-limited\nhardware is to improve the trade-off between speed and accuracy. In this\nchapter, we introduce both the hardware and software aspects of computer stereo\nvision for autonomous car systems. Then, we discuss four autonomous car\nperception tasks, including 1) visual feature detection, description and\nmatching, 2) 3D information acquisition, 3) object detection/recognition and 4)\nsemantic image segmentation. The principles of computer stereo vision and\nparallel computing on multi-threading CPU and GPU architectures are then\ndetailed.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 06:54:03 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 03:42:39 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Fan", "Rui", ""], ["Wang", "Li", ""], ["Bocus", "Mohammud Junaid", ""], ["Pitas", "Ioannis", ""]]}, {"id": "2012.03195", "submitter": "Yiran Zhong", "authors": "Yiran Zhong, Yuchao Dai, Hongdong Li", "title": "Depth Completion using Piecewise Planar Model", "comments": "This work was accomplished in 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A depth map can be represented by a set of learned bases and can be\nefficiently solved in a closed form solution. However, one issue with this\nmethod is that it may create artifacts when colour boundaries are inconsistent\nwith depth boundaries. In fact, this is very common in a natural image. To\naddress this issue, we enforce a more strict model in depth recovery: a\npiece-wise planar model. More specifically, we represent the desired depth map\nas a collection of 3D planar and the reconstruction problem is formulated as\nthe optimization of planar parameters. Such a problem can be formulated as a\ncontinuous CRF optimization problem and can be solved through particle based\nmethod (MP-PBP) \\cite{Yamaguchi14}. Extensive experimental evaluations on the\nKITTI visual odometry dataset show that our proposed methods own high\nresistance to false object boundaries and can generate useful and visually\npleasant 3D point clouds.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 07:11:46 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zhong", "Yiran", ""], ["Dai", "Yuchao", ""], ["Li", "Hongdong", ""]]}, {"id": "2012.03196", "submitter": "Sifei Liu", "authors": "Xueting Li, Sifei Liu, Shalini De Mello, Kihwan Kim, Xiaolong Wang,\n  Ming-Hsuan Yang, Jan Kautz", "title": "Online Adaptation for Consistent Mesh Reconstruction in the Wild", "comments": "NeurIPS 2020, https://sites.google.com/nvidia.com/vmr-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an algorithm to reconstruct temporally consistent 3D\nmeshes of deformable object instances from videos in the wild. Without\nrequiring annotations of 3D mesh, 2D keypoints, or camera pose for each video\nframe, we pose video-based reconstruction as a self-supervised online\nadaptation problem applied to any incoming test video. We first learn a\ncategory-specific 3D reconstruction model from a collection of single-view\nimages of the same category that jointly predicts the shape, texture, and\ncamera pose of an image. Then, at inference time, we adapt the model to a test\nvideo over time using self-supervised regularization terms that exploit\ntemporal consistency of an object instance to enforce that all reconstructed\nmeshes share a common texture map, a base shape, as well as parts. We\ndemonstrate that our algorithm recovers temporally consistent and reliable 3D\nstructures from videos of non-rigid objects including those of animals captured\nin the wild -- an extremely challenging task rarely addressed before.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 07:22:27 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Li", "Xueting", ""], ["Liu", "Sifei", ""], ["De Mello", "Shalini", ""], ["Kim", "Kihwan", ""], ["Wang", "Xiaolong", ""], ["Yang", "Ming-Hsuan", ""], ["Kautz", "Jan", ""]]}, {"id": "2012.03197", "submitter": "Liangjian Chen", "authors": "Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Yen-Yu Lin, Wei Fan, and\n  Xiaohui Xie", "title": "DGGAN: Depth-image Guided Generative Adversarial Networks for\n  Disentangling RGB and Depth Images in 3D Hand Pose Estimation", "comments": null, "journal-ref": "2020 IEEE Winter Conference on Applications of Computer Vision\n  (WACV)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Estimating3D hand poses from RGB images is essentialto a wide range of\npotential applications, but is challengingowing to substantial ambiguity in the\ninference of depth in-formation from RGB images. State-of-the-art estimators\nad-dress this problem by regularizing3D hand pose estimationmodels during\ntraining to enforce the consistency betweenthe predicted3D poses and the\nground-truth depth maps.However, these estimators rely on both RGB images and\nthepaired depth maps during training. In this study, we proposea conditional\ngenerative adversarial network (GAN) model,called Depth-image Guided GAN\n(DGGAN), to generate re-alistic depth maps conditioned on the input RGB image,\nanduse the synthesized depth maps to regularize the3D handpose estimation\nmodel, therefore eliminating the need forground-truth depth maps. Experimental\nresults on multiplebenchmark datasets show that the synthesized depth\nmapsproduced by DGGAN are quite effective in regularizing thepose estimation\nmodel, yielding new state-of-the-art resultsin estimation accuracy, notably\nreducing the mean3D end-point errors (EPE) by4.7%,16.5%, and6.8%on the RHD,STB\nand MHP datasets, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 07:23:21 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Chen", "Liangjian", ""], ["Lin", "Shih-Yao", ""], ["Xie", "Yusheng", ""], ["Lin", "Yen-Yu", ""], ["Fan", "Wei", ""], ["Xie", "Xiaohui", ""]]}, {"id": "2012.03205", "submitter": "Liangjian Chen", "authors": "Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Yen-Yu Lin, and Xiaohui Xie", "title": "Temporal-Aware Self-Supervised Learning for 3D Hand Pose and Mesh\n  Estimation in Videos", "comments": null, "journal-ref": "2021 IEEE Winter Conference on Applications of Computer Vision\n  (WACV)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating 3D hand pose directly from RGB imagesis challenging but has gained\nsteady progress recently bytraining deep models with annotated 3D poses.\nHoweverannotating 3D poses is difficult and as such only a few 3Dhand pose\ndatasets are available, all with limited samplesizes. In this study, we propose\na new framework of training3D pose estimation models from RGB images without\nusingexplicit 3D annotations, i.e., trained with only 2D informa-tion. Our\nframework is motivated by two observations: 1)Videos provide richer information\nfor estimating 3D posesas opposed to static images; 2) Estimated 3D poses\noughtto be consistent whether the videos are viewed in the for-ward order or\nreverse order. We leverage these two obser-vations to develop a self-supervised\nlearning model calledtemporal-aware self-supervised network (TASSN). By\nen-forcing temporal consistency constraints, TASSN learns 3Dhand poses and\nmeshes from videos with only 2D keypointposition annotations. Experiments show\nthat our modelachieves surprisingly good results, with 3D estimation ac-curacy\non par with the state-of-the-art models trained with3D annotations,\nhighlighting the benefit of the temporalconsistency in constraining 3D\nprediction models.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 07:54:18 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Chen", "Liangjian", ""], ["Lin", "Shih-Yao", ""], ["Xie", "Yusheng", ""], ["Lin", "Yen-Yu", ""], ["Xie", "Xiaohui", ""]]}, {"id": "2012.03206", "submitter": "Liangjian Chen", "authors": "Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Yen-Yu Lin, and Xiaohui Xie", "title": "MVHM: A Large-Scale Multi-View Hand Mesh Benchmark for Accurate 3D Hand\n  Pose Estimation", "comments": null, "journal-ref": "2021 IEEE Winter Conference on Applications of Computer Vision\n  (WACV)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Estimating 3D hand poses from a single RGB image is challenging because depth\nambiguity leads the problem ill-posed. Training hand pose estimators with 3D\nhand mesh annotations and multi-view images often results in significant\nperformance gains. However, existing multi-view datasets are relatively small\nwith hand joints annotated by off-the-shelf trackers or automated through model\npredictions, both of which may be inaccurate and can introduce biases.\nCollecting a large-scale multi-view 3D hand pose images with accurate mesh and\njoint annotations is valuable but strenuous. In this paper, we design a spin\nmatch algorithm that enables a rigid mesh model matching with any target mesh\nground truth. Based on the match algorithm, we propose an efficient pipeline to\ngenerate a large-scale multi-view hand mesh (MVHM) dataset with accurate 3D\nhand mesh and joint labels. We further present a multi-view hand pose\nestimation approach to verify that training a hand pose estimator with our\ngenerated dataset greatly enhances the performance. Experimental results show\nthat our approach achieves the performance of 0.990 in\n$\\text{AUC}_{\\text{20-50}}$ on the MHP dataset compared to the previous\nstate-of-the-art of 0.939 on this dataset. Our datasset is public available.\n\\footnote{\\url{https://github.com/Kuzphi/MVHM}} Our datasset is available\nat~\\href{https://github.com/Kuzphi/MVHM}{\\color{blue}{https://github.com/Kuzphi/MVHM}}.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 07:55:08 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Chen", "Liangjian", ""], ["Lin", "Shih-Yao", ""], ["Xie", "Yusheng", ""], ["Lin", "Yen-Yu", ""], ["Xie", "Xiaohui", ""]]}, {"id": "2012.03208", "submitter": "Byeonghwi Kim", "authors": "Kunal Pratap Singh, Suvaansh Bhambri, Byeonghwi Kim, Roozbeh Mottaghi,\n  Jonghyun Choi", "title": "MOCA: A Modular Object-Centric Approach for Interactive Instruction\n  Following", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Performing simple household tasks based on language directives is very\nnatural to humans, yet it remains an open challenge for an AI agent. Recently,\nan 'interactive instruction following' task has been proposed to foster\nresearch in reasoning over long instruction sequences that requires object\ninteractions in a simulated environment. It involves solving open problems in\nvision, language and navigation literature at each step. To address this\nmultifaceted problem, we propose a modular architecture that decouples the task\ninto visual perception and action policy, and name it as MOCA, a Modular\nObject-Centric Approach. We evaluate our method on the ALFRED benchmark and\nempirically validate that it outperforms prior arts by significant margins in\nall metrics with good generalization performance (high success rate in unseen\nenvironments). Our code is available at https://github.com/gistvision/moca.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 07:59:22 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 15:49:23 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Singh", "Kunal Pratap", ""], ["Bhambri", "Suvaansh", ""], ["Kim", "Byeonghwi", ""], ["Mottaghi", "Roozbeh", ""], ["Choi", "Jonghyun", ""]]}, {"id": "2012.03212", "submitter": "Lior Gelberg", "authors": "Lior Gelberg, David Mendlovic, and Dan Raviv", "title": "Skeleon-Based Typing Style Learning For Person Identification", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel architecture for person identification based on\ntyping-style, constructed of adaptive non-local spatio-temporal graph\nconvolutional network. Since type style dynamics convey meaningful information\nthat can be useful for person identification, we extract the joints positions\nand then learn their movements' dynamics. Our non-local approach increases our\nmodel's robustness to noisy input data while analyzing joints locations instead\nof RGB data provides remarkable robustness to alternating environmental\nconditions, e.g., lighting, noise, etc. We further present two new datasets for\ntyping style based person identification task and extensive evaluation that\ndisplays our model's superior discriminative and generalization abilities, when\ncompared with state-of-the-art skeleton-based models.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 08:14:06 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Gelberg", "Lior", ""], ["Mendlovic", "David", ""], ["Raviv", "Dan", ""]]}, {"id": "2012.03223", "submitter": "Roi Ronen", "authors": "Roi Ronen (1) and Yoav Y. Schechner (1) and Eshkol Eytan (2) ((1)\n  Viterbi Faculty of Electrical Engineering, Technion - Israel Institute of\n  Technology, Haifa, Israel, (2) Department of Earth and Planetary Sciences,\n  The Weizmann Institute of Science, Rehovot, Israel)", "title": "Spatiotemporal tomography based on scattered multiangular signals and\n  its application for resolving evolving clouds using moving platforms", "comments": "14 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive computed tomography (CT) of a time-varying volumetric translucent\nobject, using a small number of moving cameras. We particularly focus on\npassive scattering tomography, which is a non-linear problem. We demonstrate\nthe approach on dynamic clouds, as clouds have a major effect on Earth's\nclimate. State of the art scattering CT assumes a static object. Existing 4D CT\nmethods rely on a linear image formation model and often on significant priors.\nIn this paper, the angular and temporal sampling rates needed for a proper\nrecovery are discussed. If these rates are used, the paper leads to a\nrepresentation of the time-varying object, which simplifies 4D CT tomography.\nThe task is achieved using gradient-based optimization. We demonstrate this in\nphysics-based simulations and in an experiment that had yielded real-world\ndata.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 09:22:08 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Ronen", "Roi", ""], ["Schechner", "Yoav Y.", ""], ["Eytan", "Eshkol", ""]]}, {"id": "2012.03236", "submitter": "Defang Chen", "authors": "Defang Chen, Jian-Ping Mei, Yuan Zhang, Can Wang, Zhe Wang, Yan Feng,\n  Chun Chen", "title": "Cross-Layer Distillation with Semantic Calibration", "comments": "AAAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently proposed knowledge distillation approaches based on feature-map\ntransfer validate that intermediate layers of a teacher model can serve as\neffective targets for training a student model to obtain better generalization\nability. Existing studies mainly focus on particular representation forms for\nknowledge transfer between manually specified pairs of teacher-student\nintermediate layers. However, semantics of intermediate layers may vary in\ndifferent networks and manual association of layers might lead to negative\nregularization caused by semantic mismatch between certain teacher-student\nlayer pairs. To address this problem, we propose Semantic Calibration for\nCross-layer Knowledge Distillation (SemCKD), which automatically assigns proper\ntarget layers of the teacher model for each student layer with an attention\nmechanism. With a learned attention distribution, each student layer distills\nknowledge contained in multiple layers rather than a single fixed intermediate\nlayer from the teacher model for appropriate cross-layer supervision in\ntraining. Consistent improvements over state-of-the-art approaches are observed\nin extensive experiments with various network architectures for teacher and\nstudent models, demonstrating the effectiveness and flexibility of the proposed\nattention based soft layer association mechanism for cross-layer distillation.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 11:16:07 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Chen", "Defang", ""], ["Mei", "Jian-Ping", ""], ["Zhang", "Yuan", ""], ["Wang", "Can", ""], ["Wang", "Zhe", ""], ["Feng", "Yan", ""], ["Chen", "Chun", ""]]}, {"id": "2012.03242", "submitter": "Sahar Yousefi", "authors": "Sahar Yousefi, Hessam Sokooti, Mohamed S. Elmahdy, Irene M. Lips,\n  Mohammad T. Manzuri Shalmani, Roel T. Zinkstok, Frank J.W.M. Dankers, Marius\n  Staring", "title": "Esophageal Tumor Segmentation in CT Images using Dilated Dense Attention\n  Unet (DDAUnet)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Manual or automatic delineation of the esophageal tumor in CT images is known\nto be very challenging. This is due to the low contrast between the tumor and\nadjacent tissues, the anatomical variation of the esophagus, as well as the\noccasional presence of foreign bodies (e.g. feeding tubes). Physicians\ntherefore usually exploit additional knowledge such as endoscopic findings,\nclinical history, additional imaging modalities like PET scans. Achieving his\nadditional information is time-consuming, while the results are error-prone and\nmight lead to non-deterministic results. In this paper we aim to investigate if\nand to what extent a simplified clinical workflow based on CT alone, allows one\nto automatically segment the esophageal tumor with sufficient quality. For this\npurpose, we present a fully automatic end-to-end esophageal tumor segmentation\nmethod based on convolutional neural networks (CNNs). The proposed network,\ncalled Dilated Dense Attention Unet (DDAUnet), leverages spatial and channel\nattention gates in each dense block to selectively concentrate on determinant\nfeature maps and regions. Dilated convolutional layers are used to manage GPU\nmemory and increase the network receptive field. We collected a dataset of 792\nscans from 288 distinct patients including varying anatomies with \\mbox{air\npockets}, feeding tubes and proximal tumors. Repeatability and reproducibility\nstudies were conducted for three distinct splits of training and validation\nsets. The proposed network achieved a $\\mathrm{DSC}$ value of $0.79 \\pm 0.20$,\na mean surface distance of $5.4 \\pm 20.2mm$ and $95\\%$ Hausdorff distance of\n$14.7 \\pm 25.0mm$ for 287 test scans, demonstrating promising results with a\nsimplified clinical workflow based on CT alone. Our code is publicly available\nvia \\url{https://github.com/yousefis/DenseUnet_Esophagus_Segmentation}.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 11:42:52 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 19:48:32 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 12:59:55 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Yousefi", "Sahar", ""], ["Sokooti", "Hessam", ""], ["Elmahdy", "Mohamed S.", ""], ["Lips", "Irene M.", ""], ["Shalmani", "Mohammad T. Manzuri", ""], ["Zinkstok", "Roel T.", ""], ["Dankers", "Frank J. W. M.", ""], ["Staring", "Marius", ""]]}, {"id": "2012.03255", "submitter": "Abdullah Abuolaim", "authors": "Abdullah Abuolaim, Mauricio Delbracio, Damien Kelly, Michael S. Brown,\n  Peyman Milanfar", "title": "Learning to Reduce Defocus Blur by Realistically Modeling Dual-Pixel\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown impressive results on data-driven defocus deblurring\nusing the two-image views available on modern dual-pixel (DP) sensors. One\nsignificant challenge in this line of research is access to DP data. Despite\nmany cameras having DP sensors, only a limited number provide access to the\nlow-level DP sensor images. In addition, capturing training data for defocus\ndeblurring involves a time-consuming and tedious setup requiring the camera's\naperture to be adjusted. Some cameras with DP sensors (e.g., smartphones) do\nnot have adjustable apertures, further limiting the ability to produce the\nnecessary training data. We address the data capture bottleneck by proposing a\nprocedure to generate realistic DP data synthetically. Our synthesis approach\nmimics the optical image formation found on DP sensors and can be applied to\nvirtual scenes rendered with standard computer software. Leveraging these\nrealistic synthetic DP images, we introduce a new recurrent convolutional\nnetwork (RCN) architecture that can improve deblurring results and is suitable\nfor use with single-frame and multi-frame data captured by DP sensors. Finally,\nwe show that our synthetic DP data is useful for training DNN models targeting\nvideo deblurring applications where access to DP data remains challenging.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 13:12:43 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Abuolaim", "Abdullah", ""], ["Delbracio", "Mauricio", ""], ["Kelly", "Damien", ""], ["Brown", "Michael S.", ""], ["Milanfar", "Peyman", ""]]}, {"id": "2012.03257", "submitter": "Liekang Zeng", "authors": "Liekang Zeng, Xu Chen, Zhi Zhou, Lei Yang, Junshan Zhang", "title": "CoEdge: Cooperative DNN Inference with Adaptive Workload Partitioning\n  over Heterogeneous Edge Devices", "comments": "Accepted by IEEE/ACM Transactions on Networking, Nov. 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in artificial intelligence have driven increasing intelligent\napplications at the network edge, such as smart home, smart factory, and smart\ncity. To deploy computationally intensive Deep Neural Networks (DNNs) on\nresource-constrained edge devices, traditional approaches have relied on either\noffloading workload to the remote cloud or optimizing computation at the end\ndevice locally. However, the cloud-assisted approaches suffer from the\nunreliable and delay-significant wide-area network, and the local computing\napproaches are limited by the constrained computing capability. Towards\nhigh-performance edge intelligence, the cooperative execution mechanism offers\na new paradigm, which has attracted growing research interest recently. In this\npaper, we propose CoEdge, a distributed DNN computing system that orchestrates\ncooperative DNN inference over heterogeneous edge devices. CoEdge utilizes\navailable computation and communication resources at the edge and dynamically\npartitions the DNN inference workload adaptive to devices' computing\ncapabilities and network conditions. Experimental evaluations based on a\nrealistic prototype show that CoEdge outperforms status-quo approaches in\nsaving energy with close inference latency, achieving up to 25.5%~66.9% energy\nreduction for four widely-adopted CNN models.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 13:15:52 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zeng", "Liekang", ""], ["Chen", "Xu", ""], ["Zhou", "Zhi", ""], ["Yang", "Lei", ""], ["Zhang", "Junshan", ""]]}, {"id": "2012.03265", "submitter": "Zehui Gong", "authors": "Zehui Gong, Dong Li", "title": "Towards Better Object Detection in Scale Variation with Adaptive Feature\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is a common practice to exploit pyramidal feature representation to tackle\nthe problem of scale variation in object instances. However, most of them still\npredict the objects in a certain range of scales based solely or mainly on a\nsingle-level representation, yielding inferior detection performance. To this\nend, we propose a novel adaptive feature selection module (AFSM), to\nautomatically learn the way to fuse multi-level representations in the channel\ndimension, in a data-driven manner. It significantly improves the performance\nof the detectors that have a feature pyramid structure, while introducing\nnearly free inference overhead. Moreover, a class-aware sampling mechanism\n(CASM) is proposed to tackle the class imbalance problem, by re-weighting the\nsampling ratio to each of the training images, based on the statistical\ncharacteristics of each class. This is crucial to improve the performance of\nthe minor classes. Experimental results demonstrate the effectiveness of the\nproposed method, with 83.04% mAP at 15.96 FPS on the VOC dataset, and 39.48% AP\non the VisDrone-DET validation subset, respectively, outperforming other\nstate-of-the-art detectors considerably. The code is available at\nhttps://github.com/ZeHuiGong/AFSM.git.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 13:41:20 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 13:43:09 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Gong", "Zehui", ""], ["Li", "Dong", ""]]}, {"id": "2012.03298", "submitter": "Amir Rasouli", "authors": "Amir Rasouli and Mohsen Rohani and Jun Luo", "title": "Pedestrian Behavior Prediction via Multitask Learning and Categorical\n  Interaction Modeling", "comments": "11 pages; 4 Figures; 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian behavior prediction is one of the major challenges for intelligent\ndriving systems. Pedestrians often exhibit complex behaviors influenced by\nvarious contextual elements. To address this problem, we propose a multitask\nlearning framework that simultaneously predicts trajectories and actions of\npedestrians by relying on multimodal data. Our method benefits from 1) a hybrid\nmechanism to encode different input modalities independently allowing them to\ndevelop their own representations, and jointly to produce a representation for\nall modalities using shared parameters; 2) a novel interaction modeling\ntechnique that relies on categorical semantic parsing of the scenes to capture\ninteractions between target pedestrians and their surroundings; and 3) a dual\nprediction mechanism that uses both independent and shared decoding of\nmultimodal representations. Using public pedestrian behavior benchmark datasets\nfor driving, PIE and JAAD, we highlight the benefits of multitask learning for\nbehavior prediction and show that our model achieves state-of-the-art\nperformance and improves trajectory and action prediction by up to 22% and 6%\nrespectively. We further investigate the contributions of the proposed\nprocessing and interaction modeling techniques via extensive ablation studies.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 15:57:11 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Rasouli", "Amir", ""], ["Rohani", "Mohsen", ""], ["Luo", "Jun", ""]]}, {"id": "2012.03308", "submitter": "Weihao Xia", "authors": "Weihao Xia and Yujiu Yang and Jing-Hao Xue and Baoyuan Wu", "title": "TediGAN: Text-Guided Diverse Face Image Generation and Manipulation", "comments": "CVPR 2021. Code: https://github.com/weihaox/TediGAN Data:\n  https://github.com/weihaox/Multi-Modal-CelebA-HQ Video:\n  https://youtu.be/L8Na2f5viAM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose TediGAN, a novel framework for multi-modal image\ngeneration and manipulation with textual descriptions. The proposed method\nconsists of three components: StyleGAN inversion module, visual-linguistic\nsimilarity learning, and instance-level optimization. The inversion module maps\nreal images to the latent space of a well-trained StyleGAN. The\nvisual-linguistic similarity learns the text-image matching by mapping the\nimage and text into a common embedding space. The instance-level optimization\nis for identity preservation in manipulation. Our model can produce diverse and\nhigh-quality images with an unprecedented resolution at 1024. Using a control\nmechanism based on style-mixing, our TediGAN inherently supports image\nsynthesis with multi-modal inputs, such as sketches or semantic labels, with or\nwithout instance guidance. To facilitate text-guided multi-modal synthesis, we\npropose the Multi-Modal CelebA-HQ, a large-scale dataset consisting of real\nface images and corresponding semantic segmentation map, sketch, and textual\ndescriptions. Extensive experiments on the introduced dataset demonstrate the\nsuperior performance of our proposed method. Code and data are available at\nhttps://github.com/weihaox/TediGAN.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 16:20:19 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 11:52:51 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 06:40:59 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Xia", "Weihao", ""], ["Yang", "Yujiu", ""], ["Xue", "Jing-Hao", ""], ["Wu", "Baoyuan", ""]]}, {"id": "2012.03316", "submitter": "Jie Ou", "authors": "Jie Ou and Hong Wu", "title": "Efficient Human Pose Estimation with Depthwise Separable Convolution and\n  Person Centroid Guided Joint Grouping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose efficient and effective methods for 2D human pose\nestimation. A new ResBlock is proposed based on depthwise separable convolution\nand is utilized instead of the original one in Hourglass network. It can be\nfurther enhanced by replacing the vanilla depthwise convolution with a mixed\ndepthwise convolution. Based on it, we propose a bottom-up multi-person pose\nestimation method. A rooted tree is used to represent human pose by introducing\nperson centroid as the root which connects to all body joints directly or\nhierarchically. Two branches of sub-networks are used to predict the centroids,\nbody joints and their offsets to their parent nodes. Joints are grouped by\ntracing along their offsets to the closest centroids. Experimental results on\nthe MPII human dataset and the LSP dataset show that both our single-person and\nmulti-person pose estimation methods can achieve competitive accuracies with\nlow computational costs.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 16:39:54 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Ou", "Jie", ""], ["Wu", "Hong", ""]]}, {"id": "2012.03321", "submitter": "Jiunn-Kai Huang", "authors": "Jiunn-Kai Huang, Chenxi Feng, Madhav Achar, Maani Ghaffari, and Jessy\n  W. Grizzle", "title": "Global Unifying Intrinsic Calibration for Spinning and Solid-State\n  LiDARs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensor calibration, which can be intrinsic or extrinsic, is an essential step\nto achieve the measurement accuracy required for modern perception and\nnavigation systems deployed on autonomous robots. To date, intrinsic\ncalibration models for spinning LiDARs have been based on hypothesized based on\ntheir physical mechanisms, resulting in anywhere from three to ten parameters\nto be estimated from data, while no phenomenological models have yet been\nproposed for solid-state LiDARs. Instead of going down that road, we propose to\nabstract away from the physics of a LiDAR type (spinning vs solid-state, for\nexample), and focus on the spatial geometry of the point cloud generated by the\nsensor. By modeling the calibration parameters as an element of a special\nmatrix Lie Group, we achieve a unifying view of calibration for different types\nof LiDARs. We further prove mathematically that the proposed model is\nwell-constrained (has a unique answer) given four appropriately orientated\ntargets. The proof provides a guideline for target positioning in the form of a\ntetrahedron. Moreover, an existing Semidefinite programming global solver for\nSE(3) can be modified to compute efficiently the optimal calibration\nparameters. For solid state LiDARs, we illustrate how the method works in\nsimulation. For spinning LiDARs, we show with experimental data that the\nproposed matrix Lie Group model performs equally well as physics-based models\nin terms of reducing the P2P distance, while being more robust to noise.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 16:55:58 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 01:39:08 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Huang", "Jiunn-Kai", ""], ["Feng", "Chenxi", ""], ["Achar", "Madhav", ""], ["Ghaffari", "Maani", ""], ["Grizzle", "Jessy W.", ""]]}, {"id": "2012.03322", "submitter": "Aymene Mohammed Bouayed", "authors": "Aymene Mohammed Bouayed and Karim Atif and Rachid Deriche and\n  Abdelhakim Saim", "title": "A Pseudo-labelling Auto-Encoder for unsupervised image classification", "comments": "13 pages, 17 figures, 9 tables, title simplified, references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we introduce a unique variant of the denoising Auto-Encoder\nand combine it with the perceptual loss to classify images in an unsupervised\nmanner. The proposed method, called Pseudo Labelling, consists of first\napplying a randomly sampled set of data augmentation transformations to each\ntraining image. As a result, each initial image can be considered as a\npseudo-label to its corresponding augmented ones. Then, an Auto-Encoder is used\nto learn the mapping between each set of the augmented images and its\ncorresponding pseudo-label. Furthermore, the perceptual loss is employed to\ntake into consideration the existing dependencies between the pixels in the\nsame neighbourhood of an image. This combination encourages the encoder to\noutput richer encodings that are highly informative of the input's class.\nConsequently, the Auto-Encoder's performance on unsupervised image\nclassification is improved in terms of stability, accuracy and consistency\nacross all tested datasets. Previous state-of-the-art accuracy on the MNIST,\nCIFAR-10 and SVHN datasets is improved by 0.3\\%, 3.11\\% and 9.21\\%\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 17:03:34 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 17:16:14 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bouayed", "Aymene Mohammed", ""], ["Atif", "Karim", ""], ["Deriche", "Rachid", ""], ["Saim", "Abdelhakim", ""]]}, {"id": "2012.03352", "submitter": "Roger David Soberanis-Mukul", "authors": "Roger D. Soberanis-Mukul, Nassir Navab, Shadi Albarqouni", "title": "An Uncertainty-Driven GCN Refinement Strategy for Organ Segmentation", "comments": "Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Organ segmentation in CT volumes is an important pre-processing step in many\ncomputer assisted intervention and diagnosis methods. In recent years,\nconvolutional neural networks have dominated the state of the art in this task.\nHowever, since this problem presents a challenging environment due to high\nvariability in the organ's shape and similarity between tissues, the generation\nof false negative and false positive regions in the output segmentation is a\ncommon issue. Recent works have shown that the uncertainty analysis of the\nmodel can provide us with useful information about potential errors in the\nsegmentation. In this context, we proposed a segmentation refinement method\nbased on uncertainty analysis and graph convolutional networks. We employ the\nuncertainty levels of the convolutional network in a particular input volume to\nformulate a semi-supervised graph learning problem that is solved by training a\ngraph convolutional network. To test our method we refine the initial output of\na 2D U-Net. We validate our framework with the NIH pancreas dataset and the\nspleen dataset of the medical segmentation decathlon. We show that our method\noutperforms the state-of-the-art CRF refinement method by improving the dice\nscore by 1% for the pancreas and 2% for spleen, with respect to the original\nU-Net's prediction. Finally, we perform a sensitivity analysis on the\nparameters of our proposal and discuss the applicability to other CNN\narchitectures, the results, and current limitations of the model for future\nwork in this research direction. For reproducibility purposes, we make our code\npublicly available at https://github.com/rodsom22/gcn_refinement.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 18:55:07 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Soberanis-Mukul", "Roger D.", ""], ["Navab", "Nassir", ""], ["Albarqouni", "Shadi", ""]]}, {"id": "2012.03357", "submitter": "Kfir Goldberg", "authors": "Kfir Goldberg, Stav Shapiro, Elad Richardson, Shai Avidan", "title": "Rethinking FUN: Frequency-Domain Utilization Networks", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The search for efficient neural network architectures has gained much focus\nin recent years, where modern architectures focus not only on accuracy but also\non inference time and model size. Here, we present FUN, a family of novel\nFrequency-domain Utilization Networks. These networks utilize the inherent\nefficiency of the frequency-domain by working directly in that domain,\nrepresented with the Discrete Cosine Transform. Using modern techniques and\nbuilding blocks such as compound-scaling and inverted-residual layers we\ngenerate a set of such networks allowing one to balance between size, latency\nand accuracy while outperforming competing RGB-based models. Extensive\nevaluations verifies that our networks present strong alternatives to previous\napproaches. Moreover, we show that working in frequency domain allows for\ndynamic compression of the input at inference time without any explicit change\nto the architecture.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 19:16:37 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Goldberg", "Kfir", ""], ["Shapiro", "Stav", ""], ["Richardson", "Elad", ""], ["Avidan", "Shai", ""]]}, {"id": "2012.03358", "submitter": "Aadarsh Sahoo", "authors": "Aadarsh Sahoo, Rameswar Panda, Rogerio Feris, Kate Saenko, Abir Das", "title": "Select, Label, and Mix: Learning Discriminative Invariant Feature\n  Representations for Partial Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial domain adaptation which assumes that the unknown target label space\nis a subset of the source label space has attracted much attention in computer\nvision. Despite recent progress, existing methods often suffer from three key\nproblems: negative transfer, lack of discriminability and domain invariance in\nthe latent space. To alleviate the above issues, we develop a novel 'Select,\nLabel, and Mix' (SLM) framework that aims to learn discriminative invariant\nfeature representations for partial domain adaptation. First, we present a\nsimple yet efficient \"select\" module that automatically filters out the outlier\nsource samples to avoid negative transfer while aligning distributions across\nboth domains. Second, the \"label\" module iteratively trains the classifier\nusing both the labeled source domain data and the generated pseudo-labels for\nthe target domain to enhance the discriminability of the latent space. Finally,\nthe \"mix\" module utilizes domain mixup regularization jointly with the other\ntwo modules to explore more intrinsic structures across domains leading to a\ndomain-invariant latent space for partial domain adaptation. Extensive\nexperiments on several benchmark datasets demonstrate the superiority of our\nproposed framework over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 19:29:32 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Sahoo", "Aadarsh", ""], ["Panda", "Rameswar", ""], ["Feris", "Rogerio", ""], ["Saenko", "Kate", ""], ["Das", "Abir", ""]]}, {"id": "2012.03362", "submitter": "Lu Yu", "authors": "Lu Yu, Xialei Liu, Joost van de Weijer", "title": "Self-Training for Class-Incremental Semantic Segmentation", "comments": "Submitted to TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In class-incremental semantic segmentation we have no access to the labeled\ndata of previous tasks. Therefore, when incrementally learning new classes,\ndeep neural networks suffer from catastrophic forgetting of previously learned\nknowledge. To address this problem, we propose to apply a self-training\napproach that leverages unlabeled data, which is used for rehearsal of previous\nknowledge. Additionally, conflict reduction is proposed to resolve the\nconflicts of pseudo labels generated from both the old and new models. We show\nthat maximizing self-entropy can further improve results by smoothing the\noverconfident predictions. Interestingly, in the experiments we show that the\nauxiliary data can be different from the training data and that even\ngeneral-purpose but diverse auxiliary data can lead to large performance gains.\nThe experiments demonstrate state-of-the-art results: obtaining a relative gain\nof up to 114% on Pascal-VOC 2012 and 8.5% on the more challenging ADE20K\ncompared to previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 19:48:35 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 09:49:26 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Yu", "Lu", ""], ["Liu", "Xialei", ""], ["van de Weijer", "Joost", ""]]}, {"id": "2012.03363", "submitter": "Chao Pan", "authors": "Chao Pan, Siheng Chen, Antonio Ortega", "title": "Spatio-Temporal Graph Scattering Transform", "comments": "18 pages, ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Although spatio-temporal graph neural networks have achieved great empirical\nsuccess in handling multiple correlated time series, they may be impractical in\nsome real-world scenarios due to a lack of sufficient high-quality training\ndata. Furthermore, spatio-temporal graph neural networks lack theoretical\ninterpretation. To address these issues, we put forth a novel mathematically\ndesigned framework to analyze spatio-temporal data. Our proposed\nspatio-temporal graph scattering transform (ST-GST) extends traditional\nscattering transforms to the spatio-temporal domain. It performs iterative\napplications of spatio-temporal graph wavelets and nonlinear activation\nfunctions, which can be viewed as a forward pass of spatio-temporal graph\nconvolutional networks without training. Since all the filter coefficients in\nST-GST are mathematically designed, it is promising for the real-world\nscenarios with limited training data, and also allows for a theoretical\nanalysis, which shows that the proposed ST-GST is stable to small perturbations\nof input signals and structures. Finally, our experiments show that i) ST-GST\noutperforms spatio-temporal graph convolutional networks by an increase of 35%\nin accuracy for MSR Action3D dataset; ii) it is better and computationally more\nefficient to design the transform based on separable spatio-temporal graphs\nthan the joint ones; and iii) the nonlinearity in ST-GST is critical to\nempirical performance.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 19:49:55 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 20:07:42 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2021 05:08:41 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Pan", "Chao", ""], ["Chen", "Siheng", ""], ["Ortega", "Antonio", ""]]}, {"id": "2012.03368", "submitter": "Runyu Mao", "authors": "Runyu Mao, Jiangpeng He, Zeman Shao, Sri Kalyan Yarlagadda, Fengqing\n  Zhu", "title": "Visual Aware Hierarchy Based Food Recognition", "comments": "MADiMA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food recognition is one of the most important components in image-based\ndietary assessment. However, due to the different complexity level of food\nimages and inter-class similarity of food categories, it is challenging for an\nimage-based food recognition system to achieve high accuracy for a variety of\npublicly available datasets. In this work, we propose a new two-step food\nrecognition system that includes food localization and hierarchical food\nclassification using Convolutional Neural Networks (CNNs) as the backbone\narchitecture. The food localization step is based on an implementation of the\nFaster R-CNN method to identify food regions. In the food classification step,\nvisually similar food categories can be clustered together automatically to\ngenerate a hierarchical structure that represents the semantic visual relations\namong food categories, then a multi-task CNN model is proposed to perform the\nclassification task based on the visual aware hierarchical structure. Since the\nsize and quality of dataset is a key component of data driven methods, we\nintroduce a new food image dataset, VIPER-FoodNet (VFN) dataset, consists of 82\nfood categories with 15k images based on the most commonly consumed foods in\nthe United States. A semi-automatic crowdsourcing tool is used to provide the\nground-truth information for this dataset including food object bounding boxes\nand food object labels. Experimental results demonstrate that our system can\nsignificantly improve both classification and recognition performance on 4\npublicly available datasets and the new VFN dataset.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 20:25:31 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Mao", "Runyu", ""], ["He", "Jiangpeng", ""], ["Shao", "Zeman", ""], ["Yarlagadda", "Sri Kalyan", ""], ["Zhu", "Fengqing", ""]]}, {"id": "2012.03369", "submitter": "Dong Wang", "authors": "Dong Wang, Yuewei Yang, Chenyang Tao, Zhe Gan, Liqun Chen, Fanjie\n  Kong, Ricardo Henao, Lawrence Carin", "title": "Proactive Pseudo-Intervention: Causally Informed Contrastive Learning\n  For Interpretable Vision Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks excel at comprehending complex visual signals,\ndelivering on par or even superior performance to that of human experts.\nHowever, ad-hoc visual explanations of model decisions often reveal an alarming\nlevel of reliance on exploiting non-causal visual cues that strongly correlate\nwith the target label in training data. As such, deep neural nets suffer\ncompromised generalization to novel inputs collected from different sources,\nand the reverse engineering of their decision rules offers limited\ninterpretability. To overcome these limitations, we present a novel contrastive\nlearning strategy called {\\it Proactive Pseudo-Intervention} (PPI) that\nleverages proactive interventions to guard against image features with no\ncausal relevance. We also devise a novel causally informed salience mapping\nmodule to identify key image pixels to intervene, and show it greatly\nfacilitates model interpretability. To demonstrate the utility of our\nproposals, we benchmark on both standard natural images and challenging medical\nimage datasets. PPI-enhanced models consistently deliver superior performance\nrelative to competing solutions, especially on out-of-domain predictions and\ndata integration from heterogeneous sources. Further, our causally trained\nsaliency maps are more succinct and meaningful relative to their non-causal\ncounterparts.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 20:30:26 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 21:28:56 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Wang", "Dong", ""], ["Yang", "Yuewei", ""], ["Tao", "Chenyang", ""], ["Gan", "Zhe", ""], ["Chen", "Liqun", ""], ["Kong", "Fanjie", ""], ["Henao", "Ricardo", ""], ["Carin", "Lawrence", ""]]}, {"id": "2012.03377", "submitter": "Akshay Joshi", "authors": "Akshay Joshi, Ankit Agrawal, Sushmita Nair", "title": "Art Style Classification with Self-Trained Ensemble of AutoEncoding\n  Transformations", "comments": "6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The artistic style of a painting is a rich descriptor that reveals both\nvisual and deep intrinsic knowledge about how an artist uniquely portrays and\nexpresses their creative vision. Accurate categorization of paintings across\ndifferent artistic movements and styles is critical for large-scale indexing of\nart databases. However, the automatic extraction and recognition of these\nhighly dense artistic features has received little to no attention in the field\nof computer vision research. In this paper, we investigate the use of deep\nself-supervised learning methods to solve the problem of recognizing complex\nartistic styles with high intra-class and low inter-class variation. Further,\nwe outperform existing approaches by almost 20% on a highly class imbalanced\nWikiArt dataset with 27 art categories. To achieve this, we train the EnAET\nsemi-supervised learning model (Wang et al., 2019) with limited annotated data\nsamples and supplement it with self-supervised representations learned from an\nensemble of spatial and non-spatial transformations.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 21:05:23 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Joshi", "Akshay", ""], ["Agrawal", "Ankit", ""], ["Nair", "Sushmita", ""]]}, {"id": "2012.03391", "submitter": "Edmondo Trentin", "authors": "Edmondo Trentin (DIISM, University of Siena, Italy)", "title": "Multivariate Density Estimation with Deep Neural Mixture Models", "comments": "Extended journal version of E. Trentin, \"Maximum-Likelihood\n  Estimation of Neural Mixture Densities: Model, Algorithm, and Preliminary\n  Experimental Evaluation\". In Proc. of ANNPR 2018: 178-189, Springer, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Albeit worryingly underrated in the recent literature on machine learning in\ngeneral (and, on deep learning in particular), multivariate density estimation\nis a fundamental task in many applications, at least implicitly, and still an\nopen issue. With a few exceptions, deep neural networks (DNNs) have seldom been\napplied to density estimation, mostly due to the unsupervised nature of the\nestimation task, and (especially) due to the need for constrained training\nalgorithms that ended up realizing proper probabilistic models that satisfy\nKolmogorov's axioms. Moreover, in spite of the well-known improvement in terms\nof modeling capabilities yielded by mixture models over plain single-density\nstatistical estimators, no proper mixtures of multivariate DNN-based component\ndensities have been investigated so far. The paper fills this gap by extending\nour previous work on Neural Mixture Densities (NMMs) to multivariate DNN\nmixtures. A maximum-likelihood (ML) algorithm for estimating Deep NMMs (DNMMs)\nis handed out, which satisfies numerically a combination of hard and soft\nconstraints aimed at ensuring satisfaction of Kolmogorov's axioms. The class of\nprobability density functions that can be modeled to any degree of precision\nvia DNMMs is formally defined. A procedure for the automatic selection of the\nDNMM architecture, as well as of the hyperparameters for its ML training\nalgorithm, is presented (exploiting the probabilistic nature of the DNMM).\nExperimental results on univariate and multivariate data are reported on,\ncorroborating the effectiveness of the approach and its superiority to the most\npopular statistical estimation techniques.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 23:03:48 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Trentin", "Edmondo", "", "DIISM, University of Siena, Italy"]]}, {"id": "2012.03400", "submitter": "Yang Fu", "authors": "Yang Fu, Linjie Yang, Ding Liu, Thomas S. Huang, Humphrey Shi", "title": "CompFeat: Comprehensive Feature Aggregation for Video Instance\n  Segmentation", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video instance segmentation is a complex task in which we need to detect,\nsegment, and track each object for any given video. Previous approaches only\nutilize single-frame features for the detection, segmentation, and tracking of\nobjects and they suffer in the video scenario due to several distinct\nchallenges such as motion blur and drastic appearance change. To eliminate\nambiguities introduced by only using single-frame features, we propose a novel\ncomprehensive feature aggregation approach (CompFeat) to refine features at\nboth frame-level and object-level with temporal and spatial context\ninformation. The aggregation process is carefully designed with a new attention\nmechanism which significantly increases the discriminative power of the learned\nfeatures. We further improve the tracking capability of our model through a\nsiamese design by incorporating both feature similarities and spatial\nsimilarities. Experiments conducted on the YouTube-VIS dataset validate the\neffectiveness of proposed CompFeat. Our code will be available at\nhttps://github.com/SHI-Labs/CompFeat-for-Video-Instance-Segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 00:31:42 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Fu", "Yang", ""], ["Yang", "Linjie", ""], ["Liu", "Ding", ""], ["Huang", "Thomas S.", ""], ["Shi", "Humphrey", ""]]}, {"id": "2012.03408", "submitter": "Xin Wen", "authors": "Xin Wen, Peng Xiang, Zhizhong Han, Yan-Pei Cao, Pengfei Wan, Wen\n  Zheng, Yu-Shen Liu", "title": "PMP-Net: Point Cloud Completion by Learning Multi-step Point Moving\n  Paths", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of point cloud completion aims to predict the missing part for an\nincomplete 3D shape. A widely used strategy is to generate a complete point\ncloud from the incomplete one. However, the unordered nature of point clouds\nwill degrade the generation of high-quality 3D shapes, as the detailed topology\nand structure of discrete points are hard to be captured by the generative\nprocess only using a latent code. In this paper, we address the above problem\nby reconsidering the completion task from a new perspective, where we formulate\nthe prediction as a point cloud deformation process. Specifically, we design a\nnovel neural network, named PMP-Net, to mimic the behavior of an earth mover.\nIt moves each point of the incomplete input to complete the point cloud, where\nthe total distance of point moving paths (PMP) should be shortest. Therefore,\nPMP-Net predicts a unique point moving path for each point according to the\nconstraint of total point moving distances. As a result, the network learns a\nstrict and unique correspondence on point-level, which can capture the detailed\ntopology and structure relationships between the incomplete shape and the\ncomplete target, and thus improves the quality of the predicted complete shape.\nWe conduct comprehensive experiments on Completion3D and PCN datasets, which\ndemonstrate our advantages over the state-of-the-art point cloud completion\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 01:34:38 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 02:32:52 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 13:52:34 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wen", "Xin", ""], ["Xiang", "Peng", ""], ["Han", "Zhizhong", ""], ["Cao", "Yan-Pei", ""], ["Wan", "Pengfei", ""], ["Zheng", "Wen", ""], ["Liu", "Yu-Shen", ""]]}, {"id": "2012.03417", "submitter": "Fan Wang", "authors": "Fan Wang, Jiangxin Yang, Yanlong Cao, Yanpeng Cao, and Michael Ying\n  Yang", "title": "Boosting Image Super-Resolution Via Fusion of Complementary Information\n  Captured by Multi-Modal Sensors", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Super-Resolution (SR) provides a promising technique to enhance the\nimage quality of low-resolution optical sensors, facilitating better-performing\ntarget detection and autonomous navigation in a wide range of robotics\napplications. It is noted that the state-of-the-art SR methods are typically\ntrained and tested using single-channel inputs, neglecting the fact that the\ncost of capturing high-resolution images in different spectral domains varies\nsignificantly. In this paper, we attempt to leverage complementary information\nfrom a low-cost channel (visible/depth) to boost image quality of an expensive\nchannel (thermal) using fewer parameters. To this end, we first present an\neffective method to virtually generate pixel-wise aligned visible and thermal\nimages based on real-time 3D reconstruction of multi-modal data captured at\nvarious viewpoints. Then, we design a feature-level multispectral fusion\nresidual network model to perform high-accuracy SR of thermal images by\nadaptively integrating co-occurrence features presented in multispectral\nimages. Experimental results demonstrate that this new approach can effectively\nalleviate the ill-posed inverse problem of image SR by taking into account\ncomplementary information from an additional low-cost channel, significantly\noutperforming state-of-the-art SR approaches in terms of both accuracy and\nefficiency.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 02:15:28 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Wang", "Fan", ""], ["Yang", "Jiangxin", ""], ["Cao", "Yanlong", ""], ["Cao", "Yanpeng", ""], ["Yang", "Michael Ying", ""]]}, {"id": "2012.03434", "submitter": "Woojeoung Nam", "authors": "Woo-Jeoung Nam, Jaesik Choi, Seong-Whan Lee", "title": "Interpreting Deep Neural Networks with Relative Sectional Propagation by\n  Analyzing Comparative Gradients and Hostile Activations", "comments": "9 pages, 8 figures, Accepted paper in AAAI Conference on Artificial\n  Intelligence (AAAI), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clear transparency of Deep Neural Networks (DNNs) is hampered by complex\ninternal structures and nonlinear transformations along deep hierarchies. In\nthis paper, we propose a new attribution method, Relative Sectional Propagation\n(RSP), for fully decomposing the output predictions with the characteristics of\nclass-discriminative attributions and clear objectness. We carefully revisit\nsome shortcomings of backpropagation-based attribution methods, which are\ntrade-off relations in decomposing DNNs. We define hostile factor as an element\nthat interferes with finding the attributions of the target and propagate it in\na distinguishable way to overcome the non-suppressed nature of activated\nneurons. As a result, it is possible to assign the bi-polar relevance scores of\nthe target (positive) and hostile (negative) attributions while maintaining\neach attribution aligned with the importance. We also present the purging\ntechniques to prevent the decrement of the gap between the relevance scores of\nthe target and hostile attributions during backward propagation by eliminating\nthe conflicting units to channel attribution map. Therefore, our method makes\nit possible to decompose the predictions of DNNs with clearer\nclass-discriminativeness and detailed elucidations of activation neurons\ncompared to the conventional attribution methods. In a verified experimental\nenvironment, we report the results of the assessments: (i) Pointing Game, (ii)\nmIoU, and (iii) Model Sensitivity with PASCAL VOC 2007, MS COCO 2014, and\nImageNet datasets. The results demonstrate that our method outperforms existing\nbackward decomposition methods, including distinctive and intuitive\nvisualizations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 03:11:07 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 10:49:00 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Nam", "Woo-Jeoung", ""], ["Choi", "Jaesik", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2012.03438", "submitter": "Bingyu Liu", "authors": "Bingyu Liu, Yuhong Guo, Jieping Ye, Weihong Deng", "title": "Selective Pseudo-Labeling with Reinforcement Learning for\n  Semi-Supervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent domain adaptation methods have demonstrated impressive improvement on\nunsupervised domain adaptation problems. However, in the semi-supervised domain\nadaptation (SSDA) setting where the target domain has a few labeled instances\navailable, these methods can fail to improve performance. Inspired by the\neffectiveness of pseudo-labels in domain adaptation, we propose a reinforcement\nlearning based selective pseudo-labeling method for semi-supervised domain\nadaptation. It is difficult for conventional pseudo-labeling methods to balance\nthe correctness and representativeness of pseudo-labeled data. To address this\nlimitation, we develop a deep Q-learning model to select both accurate and\nrepresentative pseudo-labeled instances. Moreover, motivated by large margin\nloss's capacity on learning discriminative features with little data, we\nfurther propose a novel target margin loss for our base model training to\nimprove its discriminability. Our proposed method is evaluated on several\nbenchmark datasets for SSDA, and demonstrates superior performance to all the\ncomparison methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 03:37:38 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Liu", "Bingyu", ""], ["Guo", "Yuhong", ""], ["Ye", "Jieping", ""], ["Deng", "Weihong", ""]]}, {"id": "2012.03439", "submitter": "Haokui Zhang", "authors": "Haokui Zhang, Ying Li, Yenan Jiang, Peng Wang, Qiang Shen, and Chunhua\n  Shen", "title": "Hyperspectral Classification Based on Lightweight 3-D-CNN With Transfer\n  Learning", "comments": "16 pages. Accepted to IEEE Trans. Geosci. Remote Sens. Code is\n  available at: https://github.com/hkzhang91/LWNet", "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, 2019, 57(8):\n  5813-5828", "doi": "10.1109/TGRS.2019.2902568", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, hyperspectral image (HSI) classification approaches based on deep\nlearning (DL) models have been proposed and shown promising performance.\nHowever, because of very limited available training samples and massive model\nparameters, DL methods may suffer from overfitting. In this paper, we propose\nan end-to-end 3-D lightweight convolutional neural network (CNN) (abbreviated\nas 3-D-LWNet) for limited samples-based HSI classification. Compared with\nconventional 3-D-CNN models, the proposed 3-D-LWNet has a deeper network\nstructure, less parameters, and lower computation cost, resulting in better\nclassification performance. To further alleviate the small sample problem, we\nalso propose two transfer learning strategies: 1) cross-sensor strategy, in\nwhich we pretrain a 3-D model in the source HSI data sets containing a greater\nnumber of labeled samples and then transfer it to the target HSI data sets and\n2) cross-modal strategy, in which we pretrain a 3-D model in the 2-D RGB image\ndata sets containing a large number of samples and then transfer it to the\ntarget HSI data sets. In contrast to previous approaches, we do not impose\nrestrictions over the source data sets, in which they do not have to be\ncollected by the same sensors as the target data sets. Experiments on three\npublic HSI data sets captured by different sensors demonstrate that our model\nachieves competitive performance for HSI classification compared to several\nstate-of-the-art methods\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 03:44:35 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zhang", "Haokui", ""], ["Li", "Ying", ""], ["Jiang", "Yenan", ""], ["Wang", "Peng", ""], ["Shen", "Qiang", ""], ["Shen", "Chunhua", ""]]}, {"id": "2012.03457", "submitter": "Sangdoo Yun", "authors": "Sangdoo Yun, Seong Joon Oh, Byeongho Heo, Dongyoon Han, Jinhyung Kim", "title": "VideoMix: Rethinking Data Augmentation for Video Classification", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art video action classifiers often suffer from overfitting. They\ntend to be biased towards specific objects and scene cues, rather than the\nforeground action content, leading to sub-optimal generalization performances.\nRecent data augmentation strategies have been reported to address the\noverfitting problems in static image classifiers. Despite the effectiveness on\nthe static image classifiers, data augmentation has rarely been studied for\nvideos. For the first time in the field, we systematically analyze the efficacy\nof various data augmentation strategies on the video classification task. We\nthen propose a powerful augmentation strategy VideoMix. VideoMix creates a new\ntraining video by inserting a video cuboid into another video. The ground truth\nlabels are mixed proportionally to the number of voxels from each video. We\nshow that VideoMix lets a model learn beyond the object and scene biases and\nextract more robust cues for action recognition. VideoMix consistently\noutperforms other augmentation baselines on Kinetics and the challenging\nSomething-Something-V2 benchmarks. It also improves the weakly-supervised\naction localization performance on THUMOS'14. VideoMix pretrained models\nexhibit improved accuracies on the video detection task (AVA).\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 05:40:33 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Yun", "Sangdoo", ""], ["Oh", "Seong Joon", ""], ["Heo", "Byeongho", ""], ["Han", "Dongyoon", ""], ["Kim", "Jinhyung", ""]]}, {"id": "2012.03459", "submitter": "Zhizhong Huang", "authors": "Zhizhong Huang, Shouzhen Chen, Junping Zhang, Hongming Shan", "title": "PFA-GAN: Progressive Face Aging with Generative Adversarial Network", "comments": null, "journal-ref": "IEEE Transactions on Information Forensics and Security, 2021", "doi": "10.1109/TIFS.2020.3047753", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face aging is to render a given face to predict its future appearance, which\nplays an important role in the information forensics and security field as the\nappearance of the face typically varies with age. Although impressive results\nhave been achieved with conditional generative adversarial networks (cGANs),\nthe existing cGANs-based methods typically use a single network to learn\nvarious aging effects between any two different age groups. However, they\ncannot simultaneously meet three essential requirements of face aging --\nincluding image quality, aging accuracy, and identity preservation -- and\nusually generate aged faces with strong ghost artifacts when the age gap\nbecomes large. Inspired by the fact that faces gradually age over time, this\npaper proposes a novel progressive face aging framework based on generative\nadversarial network (PFA-GAN) to mitigate these issues. Unlike the existing\ncGANs-based methods, the proposed framework contains several sub-networks to\nmimic the face aging process from young to old, each of which only learns some\nspecific aging effects between two adjacent age groups. The proposed framework\ncan be trained in an end-to-end manner to eliminate accumulative artifacts and\nblurriness. Moreover, this paper introduces an age estimation loss to take into\naccount the age distribution for an improved aging accuracy, and proposes to\nuse the Pearson correlation coefficient as an evaluation metric measuring the\naging smoothness for face aging methods. Extensively experimental results\ndemonstrate superior performance over existing (c)GANs-based methods, including\nthe state-of-the-art one, on two benchmarked datasets. The source code is\navailable at~\\url{https://github.com/Hzzone/PFA-GAN}.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 05:45:13 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Huang", "Zhizhong", ""], ["Chen", "Shouzhen", ""], ["Zhang", "Junping", ""], ["Shan", "Hongming", ""]]}, {"id": "2012.03466", "submitter": "Jiansheng Fang", "authors": "Jiansheng Fang, Yanwu Xu, Xiaoqing Zhang, Yan Hu, Jiang Liu", "title": "Attention-based Saliency Hashing for Ophthalmic Image Retrieval", "comments": "8 pages, 4 figures, BIBM2020 conference", "journal-ref": null, "doi": "10.1109/BIBM49941.2020.9313536", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep hashing methods have been proved to be effective for the large-scale\nmedical image search assisting reference-based diagnosis for clinicians.\nHowever, when the salient region plays a maximal discriminative role in\nophthalmic image, existing deep hashing methods do not fully exploit the\nlearning ability of the deep network to capture the features of salient regions\npointedly. The different grades or classes of ophthalmic images may be share\nsimilar overall performance but have subtle differences that can be\ndifferentiated by mining salient regions. To address this issue, we propose a\nnovel end-to-end network, named Attention-based Saliency Hashing (ASH), for\nlearning compact hash-code to represent ophthalmic images. ASH embeds a\nspatial-attention module to focus more on the representation of salient regions\nand highlights their essential role in differentiating ophthalmic images.\nBenefiting from the spatial-attention module, the information of salient\nregions can be mapped into the hash-code for similarity calculation. In the\ntraining stage, we input the image pairs to share the weights of the network,\nand a pairwise loss is designed to maximize the discriminability of the\nhash-code. In the retrieval stage, ASH obtains the hash-code by inputting an\nimage with an end-to-end manner, then the hash-code is used to similarity\ncalculation to return the most similar images. Extensive experiments on two\ndifferent modalities of ophthalmic image datasets demonstrate that the proposed\nASH can further improve the retrieval performance compared to the\nstate-of-the-art deep hashing methods due to the huge contributions of the\nspatial-attention module.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 06:04:12 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Fang", "Jiansheng", ""], ["Xu", "Yanwu", ""], ["Zhang", "Xiaoqing", ""], ["Hu", "Yan", ""], ["Liu", "Jiang", ""]]}, {"id": "2012.03478", "submitter": "Eli Shlizerman", "authors": "Kun Su, Xiulong Liu, Eli Shlizerman", "title": "Multi-Instrumentalist Net: Unsupervised Generation of Music from Body\n  Movements", "comments": "Please see associated video at\n  https://www.youtube.com/watch?v=yo5OZKBbBh4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel system that takes as an input body movements of a musician\nplaying a musical instrument and generates music in an unsupervised setting.\nLearning to generate multi-instrumental music from videos without labeling the\ninstruments is a challenging problem. To achieve the transformation, we built a\npipeline named 'Multi-instrumentalistNet' (MI Net). At its base, the pipeline\nlearns a discrete latent representation of various instruments music from\nlog-spectrogram using a Vector Quantized Variational Autoencoder (VQ-VAE) with\nmulti-band residual blocks. The pipeline is then trained along with an\nautoregressive prior conditioned on the musician's body keypoints movements\nencoded by a recurrent neural network. Joint training of the prior with the\nbody movements encoder succeeds in the disentanglement of the music into latent\nfeatures indicating the musical components and the instrumental features. The\nlatent space results in distributions that are clustered into distinct\ninstruments from which new music can be generated. Furthermore, the VQ-VAE\narchitecture supports detailed music generation with additional conditioning.\nWe show that a Midi can further condition the latent space such that the\npipeline will generate the exact content of the music being played by the\ninstrument in the video. We evaluate MI Net on two datasets containing videos\nof 13 instruments and obtain generated music of reasonable audio quality,\neasily associated with the corresponding instrument, and consistent with the\nmusic audio content.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 06:54:10 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Su", "Kun", ""], ["Liu", "Xiulong", ""], ["Shlizerman", "Eli", ""]]}, {"id": "2012.03480", "submitter": "Yiming Lei", "authors": "Yiming Lei, Haiping Zhu, Junping Zhang, Hongming Shan", "title": "Meta Ordinal Regression Forest For Learning with Unsure Lung Nodules", "comments": null, "journal-ref": "IEEE International Conference on Bioinformatics and Biomedicine\n  (BIBM 2020)", "doi": "10.1109/BIBM49941.2020.9313554", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning-based methods have achieved promising performance in early\ndetection and classification of lung nodules, most of which discard unsure\nnodules and simply deal with a binary classification -- malignant vs benign.\nRecently, an unsure data model (UDM) was proposed to incorporate those unsure\nnodules by formulating this problem as an ordinal regression, showing better\nperformance over traditional binary classification. To further explore the\nordinal relationship for lung nodule classification, this paper proposes a meta\nordinal regression forest (MORF), which improves upon the state-of-the-art\nordinal regression method, deep ordinal regression forest (DORF), in three\nmajor ways. First, MORF can alleviate the biases of the predictions by making\nfull use of deep features while DORF needs to fix the composition of decision\ntrees before training. Second, MORF has a novel grouped feature selection (GFS)\nmodule to re-sample the split nodes of decision trees. Last, combined with GFS,\nMORF is equipped with a meta learning-based weighting scheme to map the\nfeatures selected by GFS to tree-wise weights while DORF assigns equal weights\nfor all trees. Experimental results on the LIDC-IDRI dataset demonstrate\nsuperior performance over existing methods, including the state-of-the-art\nDORF.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 06:59:43 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Lei", "Yiming", ""], ["Zhu", "Haiping", ""], ["Zhang", "Junping", ""], ["Shan", "Hongming", ""]]}, {"id": "2012.03482", "submitter": "Lin Song", "authors": "Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin\n  Sun, Jian Sun, Nanning Zheng", "title": "Rethinking Learnable Tree Filter for Generic Feature Transform", "comments": "Accepted by NeurIPS-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Learnable Tree Filter presents a remarkable approach to model\nstructure-preserving relations for semantic segmentation. Nevertheless, the\nintrinsic geometric constraint forces it to focus on the regions with close\nspatial distance, hindering the effective long-range interactions. To relax the\ngeometric constraint, we give the analysis by reformulating it as a Markov\nRandom Field and introduce a learnable unary term. Besides, we propose a\nlearnable spanning tree algorithm to replace the original non-differentiable\none, which further improves the flexibility and robustness. With the above\nimprovements, our method can better capture long-range dependencies and\npreserve structural details with linear complexity, which is extended to\nseveral vision tasks for more generic feature transform. Extensive experiments\non object detection/instance segmentation demonstrate the consistent\nimprovements over the original version. For semantic segmentation, we achieve\nleading performance (82.1% mIoU) on the Cityscapes benchmark without\nbells-and-whistles. Code is available at\nhttps://github.com/StevenGrove/LearnableTreeFilterV2.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 07:16:47 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Song", "Lin", ""], ["Li", "Yanwei", ""], ["Jiang", "Zhengkai", ""], ["Li", "Zeming", ""], ["Zhang", "Xiangyu", ""], ["Sun", "Hongbin", ""], ["Sun", "Jian", ""], ["Zheng", "Nanning", ""]]}, {"id": "2012.03487", "submitter": "Bonaventure F. P. Dossou", "authors": "Bonaventure F. P. Dossou, Alena Iureva, Sayali R. Rajhans, Vamsi S.\n  Pidikiti", "title": "An Approach to Intelligent Pneumonia Detection and Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Each year, over 2.5 million people, most of them in developed countries, die\nfrom pneumonia [1]. Since many studies have proved pneumonia is successfully\ntreatable when timely and correctly diagnosed, many of diagnosis aids have been\ndeveloped, with AI-based methods achieving high accuracies [2]. However,\ncurrently, the usage of AI in pneumonia detection is limited, in particular,\ndue to challenges in generalizing a locally achieved result. In this report, we\npropose a roadmap for creating and integrating a system that attempts to solve\nthis challenge. We also address various technical, legal, ethical, and\nlogistical issues, with a blueprint of possible solutions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 07:27:45 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Dossou", "Bonaventure F. P.", ""], ["Iureva", "Alena", ""], ["Rajhans", "Sayali R.", ""], ["Pidikiti", "Vamsi S.", ""]]}, {"id": "2012.03515", "submitter": "Leonid Karlinsky", "authors": "Guy Bukchin, Eli Schwartz, Kate Saenko, Ori Shahar, Rogerio Feris,\n  Raja Giryes, Leonid Karlinsky", "title": "Fine-grained Angular Contrastive Learning with Coarse Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning methods offer pre-training techniques optimized for easier\nlater adaptation of the model to new classes (unseen during training) using one\nor a few examples. This adaptivity to unseen classes is especially important\nfor many practical applications where the pre-trained label space cannot remain\nfixed for effective use and the model needs to be \"specialized\" to support new\ncategories on the fly. One particularly interesting scenario, essentially\noverlooked by the few-shot literature, is Coarse-to-Fine Few-Shot (C2FS), where\nthe training classes (e.g. animals) are of much `coarser granularity' than the\ntarget (test) classes (e.g. breeds). A very practical example of C2FS is when\nthe target classes are sub-classes of the training classes. Intuitively, it is\nespecially challenging as (both regular and few-shot) supervised pre-training\ntends to learn to ignore intra-class variability which is essential for\nseparating sub-classes. In this paper, we introduce a novel 'Angular\nnormalization' module that allows to effectively combine supervised and\nself-supervised contrastive pre-training to approach the proposed C2FS task,\ndemonstrating significant gains in a broad study over multiple baselines and\ndatasets. We hope that this work will help to pave the way for future research\non this new, challenging, and very practical topic of C2FS classification.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 08:09:02 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 14:17:23 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 09:39:03 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Bukchin", "Guy", ""], ["Schwartz", "Eli", ""], ["Saenko", "Kate", ""], ["Shahar", "Ori", ""], ["Feris", "Rogerio", ""], ["Giryes", "Raja", ""], ["Karlinsky", "Leonid", ""]]}, {"id": "2012.03516", "submitter": "Malhar Jere", "authors": "Malhar Jere, Maghav Kumar, Farinaz Koushanfar", "title": "A Singular Value Perspective on Model Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) have made significant progress on\nseveral computer vision benchmarks, but are fraught with numerous non-human\nbiases such as vulnerability to adversarial samples. Their lack of\nexplainability makes identification and rectification of these biases\ndifficult, and understanding their generalization behavior remains an open\nproblem. In this work we explore the relationship between the generalization\nbehavior of CNNs and the Singular Value Decomposition (SVD) of images. We show\nthat naturally trained and adversarially robust CNNs exploit highly different\nfeatures for the same dataset. We demonstrate that these features can be\ndisentangled by SVD for ImageNet and CIFAR-10 trained networks. Finally, we\npropose Rank Integrated Gradients (RIG), the first rank-based feature\nattribution method to understand the dependence of CNNs on image rank.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 08:09:07 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Jere", "Malhar", ""], ["Kumar", "Maghav", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "2012.03519", "submitter": "Lin Song", "authors": "Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Hongbin Sun, Jian Sun,\n  Nanning Zheng", "title": "Fine-Grained Dynamic Head for Object Detection", "comments": "Accepted by NeurIPS-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Feature Pyramid Network (FPN) presents a remarkable approach to alleviate\nthe scale variance in object representation by performing instance-level\nassignments. Nevertheless, this strategy ignores the distinct characteristics\nof different sub-regions in an instance. To this end, we propose a fine-grained\ndynamic head to conditionally select a pixel-level combination of FPN features\nfrom different scales for each instance, which further releases the ability of\nmulti-scale feature representation. Moreover, we design a spatial gate with the\nnew activation function to reduce computational complexity dramatically through\nspatially sparse convolutions. Extensive experiments demonstrate the\neffectiveness and efficiency of the proposed method on several state-of-the-art\ndetection benchmarks. Code is available at\nhttps://github.com/StevenGrove/DynamicHead.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 08:16:32 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Song", "Lin", ""], ["Li", "Yanwei", ""], ["Jiang", "Zhengkai", ""], ["Li", "Zeming", ""], ["Sun", "Hongbin", ""], ["Sun", "Jian", ""], ["Zheng", "Nanning", ""]]}, {"id": "2012.03528", "submitter": "Qizhang Li", "authors": "Yiwen Guo, Qizhang Li, Hao Chen", "title": "Backpropagating Linearly Improves Transferability of Adversarial\n  Examples", "comments": "Accepted by NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vulnerability of deep neural networks (DNNs) to adversarial examples has\ndrawn great attention from the community. In this paper, we study the\ntransferability of such examples, which lays the foundation of many black-box\nattacks on DNNs. We revisit a not so new but definitely noteworthy hypothesis\nof Goodfellow et al.'s and disclose that the transferability can be enhanced by\nimproving the linearity of DNNs in an appropriate manner. We introduce linear\nbackpropagation (LinBP), a method that performs backpropagation in a more\nlinear fashion using off-the-shelf attacks that exploit gradients. More\nspecifically, it calculates forward as normal but backpropagates loss as if\nsome nonlinear activations are not encountered in the forward pass.\nExperimental results demonstrate that this simple yet effective method\nobviously outperforms current state-of-the-arts in crafting transferable\nadversarial examples on CIFAR-10 and ImageNet, leading to more effective\nattacks on a variety of DNNs.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 08:40:56 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Guo", "Yiwen", ""], ["Li", "Qizhang", ""], ["Chen", "Hao", ""]]}, {"id": "2012.03544", "submitter": "Jianfeng Wang", "authors": "Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian Sun, Nanning\n  Zheng", "title": "End-to-End Object Detection with Fully Convolutional Network", "comments": "Accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mainstream object detectors based on the fully convolutional network has\nachieved impressive performance. While most of them still need a hand-designed\nnon-maximum suppression (NMS) post-processing, which impedes fully end-to-end\ntraining. In this paper, we give the analysis of discarding NMS, where the\nresults reveal that a proper label assignment plays a crucial role. To this\nend, for fully convolutional detectors, we introduce a Prediction-aware\nOne-To-One (POTO) label assignment for classification to enable end-to-end\ndetection, which obtains comparable performance with NMS. Besides, a simple 3D\nMax Filtering (3DMF) is proposed to utilize the multi-scale features and\nimprove the discriminability of convolutions in the local region. With these\ntechniques, our end-to-end framework achieves competitive performance against\nmany state-of-the-art detectors with NMS on COCO and CrowdHuman datasets. The\ncode is available at https://github.com/Megvii-BaseDetection/DeFCN .\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 09:14:55 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 04:18:14 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 03:38:55 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Wang", "Jianfeng", ""], ["Song", "Lin", ""], ["Li", "Zeming", ""], ["Sun", "Hongbin", ""], ["Sun", "Jian", ""], ["Zheng", "Nanning", ""]]}, {"id": "2012.03547", "submitter": "Samim Ahmadi", "authors": "Samim Ahmadi, Jan Christian Hauffen, Linh K\\\"astner, Peter Jung,\n  Giuseppe Caire, Mathias Ziegler", "title": "Learned Block Iterative Shrinkage Thresholding Algorithm for\n  Photothermal Super Resolution Imaging", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible. 11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI physics.app-ph physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Block-sparse regularization is already well-known in active thermal imaging\nand is used for multiple measurement based inverse problems. The main\nbottleneck of this method is the choice of regularization parameters which\ndiffers for each experiment. To avoid time-consuming manually selected\nregularization parameter, we propose a learned block-sparse optimization\napproach using an iterative algorithm unfolded into a deep neural network. More\nprecisely, we show the benefits of using a learned block iterative shrinkage\nthresholding algorithm that is able to learn the choice of regularization\nparameters. In addition, this algorithm enables the determination of a suitable\nweight matrix to solve the underlying inverse problem. Therefore, in this paper\nwe present the algorithm and compare it with state of the art block iterative\nshrinkage thresholding using synthetically generated test data and experimental\ntest data from active thermography for defect reconstruction. Our results show\nthat the use of the learned block-sparse optimization approach provides smaller\nnormalized mean square errors for a small fixed number of iterations than\nwithout learning. Thus, this new approach allows to improve the convergence\nspeed and only needs a few iterations to generate accurate defect\nreconstruction in photothermal super resolution imaging.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 09:27:16 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 14:15:57 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Ahmadi", "Samim", ""], ["Hauffen", "Jan Christian", ""], ["K\u00e4stner", "Linh", ""], ["Jung", "Peter", ""], ["Caire", "Giuseppe", ""], ["Ziegler", "Mathias", ""]]}, {"id": "2012.03579", "submitter": "Chang Liu", "authors": "Chang Liu, Yixing Huang, Joscha Maier, Laura Klein, Marc\n  Kachelrie{\\ss}, Andreas Maier", "title": "Robustness Investigation on Deep Learning CT Reconstruction for\n  Real-Time Dose Optimization", "comments": "Proceedings for \"2020 IEEE Nuclear Science Symposium and Medical\n  Imaging Conference\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In computed tomography (CT), automatic exposure control (AEC) is frequently\nused to reduce radiation dose exposure to patients. For organ-specific AEC, a\npreliminary CT reconstruction is necessary to estimate organ shapes for dose\noptimization, where only a few projections are allowed for real-time\nreconstruction. In this work, we investigate the performance of automated\ntransform by manifold approximation (AUTOMAP) in such applications. For proof\nof concept, we investigate its performance on the MNIST dataset first, where\nthe dataset containing all the 10 digits are randomly split into a training set\nand a test set. We train the AUTOMAP model for image reconstruction from 2\nprojections or 4 projections directly. The test results demonstrate that\nAUTOMAP is able to reconstruct most digits well with a false rate of 1.6% and\n6.8% respectively. In our subsequent experiment, the MNIST dataset is split in\na way that the training set contains 9 digits only while the test set contains\nthe excluded digit only, for instance \"2\". In the test results, the digit \"2\"s\nare falsely predicted as \"3\" or \"5\" when using 2 projections for\nreconstruction, reaching a false rate of 94.4%. For the application in medical\nimages, AUTOMAP is also trained on patients' CT images. The test images reach\nan average root-mean-square error of 290 HU. Although the coarse body outlines\nare well reconstructed, some organs are misshaped.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 10:55:54 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Liu", "Chang", ""], ["Huang", "Yixing", ""], ["Maier", "Joscha", ""], ["Klein", "Laura", ""], ["Kachelrie\u00df", "Marc", ""], ["Maier", "Andreas", ""]]}, {"id": "2012.03581", "submitter": "Francesco Picetti", "authors": "Francesco Picetti, Sara Mandelli, Paolo Bestagini, Vincenzo Lipari and\n  Stefano Tubaro", "title": "DIPPAS: A Deep Image Prior PRNU Anonymization Scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source device identification is an important topic in image forensics since\nit allows to trace back the origin of an image. Its forensics counter-part is\nsource device anonymization, that is, to mask any trace on the image that can\nbe useful for identifying the source device. A typical trace exploited for\nsource device identification is the Photo Response Non-Uniformity (PRNU), a\nnoise pattern left by the device on the acquired images. In this paper, we\ndevise a methodology for suppressing such a trace from natural images without\nsignificant impact on image quality. Specifically, we turn PRNU anonymization\ninto an optimization problem in a Deep Image Prior (DIP) framework. In a\nnutshell, a Convolutional Neural Network (CNN) acts as generator and returns an\nimage that is anonymized with respect to the source PRNU, still maintaining\nhigh visual quality. With respect to widely-adopted deep learning paradigms,\nour proposed CNN is not trained on a set of input-target pairs of images.\nInstead, it is optimized to reconstruct the PRNU-free image from the original\nimage under analysis itself. This makes the approach particularly suitable in\nscenarios where large heterogeneous databases are analyzed and prevents any\nproblem due to lack of generalization. Through numerical examples on publicly\navailable datasets, we prove our methodology to be effective compared to\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 10:56:50 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Picetti", "Francesco", ""], ["Mandelli", "Sara", ""], ["Bestagini", "Paolo", ""], ["Lipari", "Vincenzo", ""], ["Tubaro", "Stefano", ""]]}, {"id": "2012.03583", "submitter": "Olivier Dehaene", "authors": "Olivier Dehaene, Axel Camara, Olivier Moindrot, Axel de Lavergne,\n  Pierre Courtiol", "title": "Self-Supervision Closes the Gap Between Weak and Strong Supervision in\n  Histology", "comments": "Accepted as a poster for the ML4H 2020 NeurIPS workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the biggest challenges for applying machine learning to histopathology\nis weak supervision: whole-slide images have billions of pixels yet often only\none global label. The state of the art therefore relies on strongly-supervised\nmodel training using additional local annotations from domain experts. However,\nin the absence of detailed annotations, most weakly-supervised approaches\ndepend on a frozen feature extractor pre-trained on ImageNet. We identify this\nas a key weakness and propose to train an in-domain feature extractor on\nhistology images using MoCo v2, a recent self-supervised learning algorithm.\nExperimental results on Camelyon16 and TCGA show that the proposed extractor\ngreatly outperforms its ImageNet counterpart. In particular, our results\nimprove the weakly-supervised state of the art on Camelyon16 from 91.4% to\n98.7% AUC, thereby closing the gap with strongly-supervised models that reach\n99.3% AUC. Through these experiments, we demonstrate that feature extractors\ntrained via self-supervised learning can act as drop-in replacements to\nsignificantly improve existing machine learning techniques in histology.\nLastly, we show that the learned embedding space exhibits biologically\nmeaningful separation of tissue structures.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 10:59:38 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Dehaene", "Olivier", ""], ["Camara", "Axel", ""], ["Moindrot", "Olivier", ""], ["de Lavergne", "Axel", ""], ["Courtiol", "Pierre", ""]]}, {"id": "2012.03597", "submitter": "Guangshuai Gao", "authors": "Guangshuai Gao, Qingjie Liu, Qi Wen, Yunhong Wang", "title": "PSCNet: Pyramidal Scale and Global Context Guided Network for Crowd\n  Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crowd counting, which towards to accurately count the number of the objects\nin images, has been attracted more and more attention by researchers recently.\nHowever, challenges from severely occlusion, large scale variation, complex\nbackground interference and non-uniform density distribution, limit the crowd\nnumber estimation accuracy. To mitigate above issues, this paper proposes a\nnovel crowd counting approach based on pyramidal scale module (PSM) and global\ncontext module (GCM), dubbed PSCNet. Moreover, a reliable supervision manner\ncombined Bayesian and counting loss (BCL) is utilized to learn the density\nprobability and then computes the count exception at each annotation point.\nSpecifically, PSM is used to adaptively capture multi-scale information, which\ncan identify a fine boundary of crowds with different image scales. GCM is\ndevised with low-complexity and lightweight manner, to make the interactive\ninformation across the channels of the feature maps more efficient, meanwhile\nguide the model to select more suitable scales generated from PSM. Furthermore,\nBL is leveraged to construct a credible density contribution probability\nsupervision manner, which relieves non-uniform density distribution in crowds\nto a certain extent. Extensive experiments on four crowd counting datasets show\nthe effectiveness and superiority of the proposed model. Additionally, some\nexperiments extended on a remote sensing object counting (RSOC) dataset further\nvalidate the generalization ability of the model. Our resource code will be\nreleased upon the acceptance of this work.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 11:35:56 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Gao", "Guangshuai", ""], ["Liu", "Qingjie", ""], ["Wen", "Qi", ""], ["Wang", "Yunhong", ""]]}, {"id": "2012.03601", "submitter": "Nagendra Pratap Singh", "authors": "Sushil Kumar Saroj, Vikas Ratna, Rakesh Kumar, Nagendra Pratap Singh", "title": "Efficient Kernel based Matched Filter Approach for Segmentation of\n  Retinal Blood Vessels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Retinal blood vessels structure contains information about diseases like\nobesity, diabetes, hypertension and glaucoma. This information is very useful\nin identification and treatment of these fatal diseases. To obtain this\ninformation, there is need to segment these retinal vessels. Many kernel based\nmethods have been given for segmentation of retinal vessels but their kernels\nare not appropriate to vessel profile cause poor performance. To overcome this,\na new and efficient kernel based matched filter approach has been proposed. The\nnew matched filter is used to generate the matched filter response (MFR) image.\nWe have applied Otsu thresholding method on obtained MFR image to extract the\nvessels. We have conducted extensive experiments to choose best value of\nparameters for the proposed matched filter kernel. The proposed approach has\nexamined and validated on two online available DRIVE and STARE datasets. The\nproposed approach has specificity 98.50%, 98.23% and accuracy 95.77 %, 95.13%\nfor DRIVE and STARE dataset respectively. Obtained results confirm that the\nproposed method has better performance than others. The reason behind increased\nperformance is due to appropriate proposed kernel which matches retinal blood\nvessel profile more accurately.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 11:41:00 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Saroj", "Sushil Kumar", ""], ["Ratna", "Vikas", ""], ["Kumar", "Rakesh", ""], ["Singh", "Nagendra Pratap", ""]]}, {"id": "2012.03603", "submitter": "Gengwei Zhang", "authors": "Gengwei Zhang, Yiming Gao, Hang Xu, Hao Zhang, Zhenguo Li, Xiaodan\n  Liang", "title": "Ada-Segment: Automated Multi-loss Adaptation for Panoptic Segmentation", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoptic segmentation that unifies instance segmentation and semantic\nsegmentation has recently attracted increasing attention. While most existing\nmethods focus on designing novel architectures, we steer toward a different\nperspective: performing automated multi-loss adaptation (named Ada-Segment) on\nthe fly to flexibly adjust multiple training losses over the course of training\nusing a controller trained to capture the learning dynamics. This offers a few\nadvantages: it bypasses manual tuning of the sensitive loss combination, a\ndecisive factor for panoptic segmentation; it allows to explicitly model the\nlearning dynamics, and reconcile the learning of multiple objectives (up to ten\nin our experiments); with an end-to-end architecture, it generalizes to\ndifferent datasets without the need of re-tuning hyperparameters or\nre-adjusting the training process laboriously. Our Ada-Segment brings 2.7%\npanoptic quality (PQ) improvement on COCO val split from the vanilla baseline,\nachieving the state-of-the-art 48.5% PQ on COCO test-dev split and 32.9% PQ on\nADE20K dataset. The extensive ablation studies reveal the ever-changing\ndynamics throughout the training process, necessitating the incorporation of an\nautomated and adaptive learning strategy as presented in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 11:43:10 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zhang", "Gengwei", ""], ["Gao", "Yiming", ""], ["Xu", "Hang", ""], ["Zhang", "Hao", ""], ["Li", "Zhenguo", ""], ["Liang", "Xiaodan", ""]]}, {"id": "2012.03623", "submitter": "Kanggeun Lee", "authors": "Kanggeun Lee and Won-Ki Jeong", "title": "Noise2Kernel: Adaptive Self-Supervised Blind Denoising using a Dilated\n  Convolutional Kernel Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the advent of recent advances in unsupervised learning, efficient\ntraining of a deep network for image denoising without pairs of noisy and clean\nimages has become feasible. However, most current unsupervised denoising\nmethods are built on the assumption of zero-mean noise under the\nsignal-independent condition. This assumption causes blind denoising techniques\nto suffer brightness shifting problems on images that are greatly corrupted by\nextreme noise such as salt-and-pepper noise. Moreover, most blind denoising\nmethods require a random masking scheme for training to ensure the invariance\nof the denoising process. In this paper, we propose a dilated convolutional\nnetwork that satisfies an invariant property, allowing efficient kernel-based\ntraining without random masking. We also propose an adaptive self-supervision\nloss to circumvent the requirement of zero-mean constraint, which is\nspecifically effective in removing salt-and-pepper or hybrid noise where a\nprior knowledge of noise statistics is not readily available. We demonstrate\nthe efficacy of the proposed method by comparing it with state-of-the-art\ndenoising methods using various examples.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 12:13:17 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Lee", "Kanggeun", ""], ["Jeong", "Won-Ki", ""]]}, {"id": "2012.03662", "submitter": "Zhaokai Wang", "authors": "Zhaokai Wang, Renda Bao, Qi Wu, Si Liu", "title": "Confidence-aware Non-repetitive Multimodal Transformers for TextCaps", "comments": "9 pages; Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When describing an image, reading text in the visual scene is crucial to\nunderstand the key information. Recent work explores the TextCaps task, i.e.\nimage captioning with reading Optical Character Recognition (OCR) tokens, which\nrequires models to read text and cover them in generated captions. Existing\napproaches fail to generate accurate descriptions because of their (1) poor\nreading ability; (2) inability to choose the crucial words among all extracted\nOCR tokens; (3) repetition of words in predicted captions. To this end, we\npropose a Confidence-aware Non-repetitive Multimodal Transformers (CNMT) to\ntackle the above challenges. Our CNMT consists of a reading, a reasoning and a\ngeneration modules, in which Reading Module employs better OCR systems to\nenhance text reading ability and a confidence embedding to select the most\nnoteworthy tokens. To address the issue of word redundancy in captions, our\nGeneration Module includes a repetition mask to avoid predicting repeated word\nin captions. Our model outperforms state-of-the-art models on TextCaps dataset,\nimproving from 81.0 to 93.0 in CIDEr. Our source code is publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 13:20:12 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 04:32:18 GMT"}, {"version": "v3", "created": "Sun, 21 Mar 2021 14:28:04 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Wang", "Zhaokai", ""], ["Bao", "Renda", ""], ["Wu", "Qi", ""], ["Liu", "Si", ""]]}, {"id": "2012.03663", "submitter": "Xiang Li", "authors": "Aoxiao Zhong, Xiang Li, Dufan Wu, Hui Ren, Kyungsang Kim, Younggon\n  Kim, Varun Buch, Nir Neumark, Bernardo Bizzo, Won Young Tak, Soo Young Park,\n  Yu Rim Lee, Min Kyu Kang, Jung Gil Park, Byung Seok Kim, Woo Jin Chung, Ning\n  Guo, Ittai Dayan, Mannudeep K. Kalra, Quanzheng Li", "title": "Deep Metric Learning-based Image Retrieval System for Chest Radiograph\n  and its Clinical Applications in COVID-19", "comments": "Aoxiao Zhong and Xiang Li contribute equally to this work", "journal-ref": "Medical Image Analysis. 70 (2021) 101993", "doi": "10.1016/j.media.2021.101993", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, deep learning-based image analysis methods have been widely\napplied in computer-aided detection, diagnosis and prognosis, and has shown its\nvalue during the public health crisis of the novel coronavirus disease 2019\n(COVID-19) pandemic. Chest radiograph (CXR) has been playing a crucial role in\nCOVID-19 patient triaging, diagnosing and monitoring, particularly in the\nUnited States. Considering the mixed and unspecific signals in CXR, an image\nretrieval model of CXR that provides both similar images and associated\nclinical information can be more clinically meaningful than a direct image\ndiagnostic model. In this work we develop a novel CXR image retrieval model\nbased on deep metric learning. Unlike traditional diagnostic models which aims\nat learning the direct mapping from images to labels, the proposed model aims\nat learning the optimized embedding space of images, where images with the same\nlabels and similar contents are pulled together. It utilizes multi-similarity\nloss with hard-mining sampling strategy and attention mechanism to learn the\noptimized embedding space, and provides similar images to the query image. The\nmodel is trained and validated on an international multi-site COVID-19 dataset\ncollected from 3 different sources. Experimental results of COVID-19 image\nretrieval and diagnosis tasks show that the proposed model can serve as a\nrobust solution for CXR analysis and patient management for COVID-19. The model\nis also tested on its transferability on a different clinical decision support\ntask, where the pre-trained model is applied to extract image features from a\nnew dataset without any further training. These results demonstrate our deep\nmetric learning based image retrieval model is highly efficient in the CXR\nretrieval, diagnosis and prognosis, and thus has great clinical value for the\ntreatment and management of COVID-19 patients.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 03:16:48 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Zhong", "Aoxiao", ""], ["Li", "Xiang", ""], ["Wu", "Dufan", ""], ["Ren", "Hui", ""], ["Kim", "Kyungsang", ""], ["Kim", "Younggon", ""], ["Buch", "Varun", ""], ["Neumark", "Nir", ""], ["Bizzo", "Bernardo", ""], ["Tak", "Won Young", ""], ["Park", "Soo Young", ""], ["Lee", "Yu Rim", ""], ["Kang", "Min Kyu", ""], ["Park", "Jung Gil", ""], ["Kim", "Byung Seok", ""], ["Chung", "Woo Jin", ""], ["Guo", "Ning", ""], ["Dayan", "Ittai", ""], ["Kalra", "Mannudeep K.", ""], ["Li", "Quanzheng", ""]]}, {"id": "2012.03666", "submitter": "Shane Gilroy", "authors": "Shane Gilroy", "title": "Impact of Power Supply Noise on Image Sensor Performance in Automotive\n  Applications", "comments": null, "journal-ref": "Waterford Institute of Technology 2016", "doi": null, "report-no": null, "categories": "eess.IV cs.AR cs.CV cs.RO eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vision Systems are quickly becoming a large component of Active Automotive\nSafety Systems. In order to be effective in critical safety applications these\nsystems must produce high quality images in both daytime and night-time\nscenarios in order to provide the large informational content required for\nsoftware analysis in applications such as lane departure, pedestrian detection\nand collision detection. The challenge in capturing high quality images in low\nlight scenarios is that the signal to noise ratio is greatly reduced, which can\nresult in noise becoming the dominant factor in a captured image, thereby\nmaking these safety systems less effective at night. Research has been\nundertaken to develop a systematic method of characterising image sensor\nperformance in response to electrical noise in order to improve the design and\nperformance of automotive cameras in low light scenarios. The root cause of\nimage row noise has been established and a mathematical algorithm for\ndetermining the magnitude of row noise in an image has been devised. An\nautomated characterisation method has been developed to allow performance\ncharacterisation in response to a large frequency spectrum of electrical noise\non the image sensor power supply. Various strategies of improving image sensor\nperformance for low light applications have also been proposed from the\nresearch outcomes.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 22:25:30 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Gilroy", "Shane", ""]]}, {"id": "2012.03673", "submitter": "Zhenghua Xu", "authors": "Di Yuan, Junyang Chen, Zhenghua Xu, Thomas Lukasiewicz, Zhigang Fu,\n  Guizhi Xu", "title": "Efficient Medical Image Segmentation with Intermediate Supervision\n  Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Because the expansion path of U-Net may ignore the characteristics of small\ntargets, intermediate supervision mechanism is proposed. The original mask is\nalso entered into the network as a label for intermediate output. However,\nU-Net is mainly engaged in segmentation, and the extracted features are also\ntargeted at segmentation location information, and the input and output are\ndifferent. The label we need is that the input and output are both original\nmasks, which is more similar to the refactoring process, so we propose another\nintermediate supervision mechanism. However, the features extracted by the\ncontraction path of this intermediate monitoring mechanism are not necessarily\nconsistent. For example, U-Net's contraction path extracts transverse features,\nwhile auto-encoder extracts longitudinal features, which may cause the output\nof the expansion path to be inconsistent with the label. Therefore, we put\nforward the intermediate supervision mechanism of shared-weight decoder module.\nAlthough the intermediate supervision mechanism improves the segmentation\naccuracy, the training time is too long due to the extra input and multiple\nloss functions. For one of these problems, we have introduced tied-weight\ndecoder. To reduce the redundancy of the model, we combine shared-weight\ndecoder module with tied-weight decoder module.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 13:46:00 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Yuan", "Di", ""], ["Chen", "Junyang", ""], ["Xu", "Zhenghua", ""], ["Lukasiewicz", "Thomas", ""], ["Fu", "Zhigang", ""], ["Xu", "Guizhi", ""]]}, {"id": "2012.03674", "submitter": "Bo Wang", "authors": "Bo Wang, Lei Wang, Junyang Chen, Zhenghua Xu, Thomas Lukasiewicz and\n  Zhigang Fu", "title": "w-Net: Dual Supervised Medical Image Segmentation Model with\n  Multi-Dimensional Attention and Cascade Multi-Scale Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning-based medical image segmentation technology aims at automatic\nrecognizing and annotating objects on the medical image. Non-local attention\nand feature learning by multi-scale methods are widely used to model network,\nwhich drives progress in medical image segmentation. However, those attention\nmechanism methods have weakly non-local receptive fields' strengthened\nconnection for small objects in medical images. Then, the features of important\nsmall objects in abstract or coarse feature maps may be deserted, which leads\nto unsatisfactory performance. Moreover, the existing multi-scale methods only\nsimply focus on different sizes of view, whose sparse multi-scale features\ncollected are not abundant enough for small objects segmentation. In this work,\na multi-dimensional attention segmentation model with cascade multi-scale\nconvolution is proposed to predict accurate segmentation for small objects in\nmedical images. As the weight function, multi-dimensional attention modules\nprovide coefficient modification for significant/informative small objects\nfeatures. Furthermore, The cascade multi-scale convolution modules in each\nskip-connection path are exploited to capture multi-scale features in different\nsemantic depth. The proposed method is evaluated on three datasets: KiTS19,\nPancreas CT of Decathlon-10, and MICCAI 2018 LiTS Challenge, demonstrating\nbetter segmentation performances than the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 13:54:22 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Wang", "Bo", ""], ["Wang", "Lei", ""], ["Chen", "Junyang", ""], ["Xu", "Zhenghua", ""], ["Lukasiewicz", "Thomas", ""], ["Fu", "Zhigang", ""]]}, {"id": "2012.03675", "submitter": "Gefersom Lima", "authors": "Gefersom Lima, Gabriel Ramos, Sandro Rigo, Felipe Zeiser, Ariane da\n  Silveira", "title": "Binary Segmentation of Seismic Facies Using Encoder-Decoder Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The interpretation of seismic data is vital for characterizing sediments'\nshape in areas of geological study. In seismic interpretation, deep learning\nbecomes useful for reducing the dependence on handcrafted facies segmentation\ngeometry and the time required to study geological areas. This work presents a\nDeep Neural Network for Facies Segmentation (DNFS) to obtain state-of-the-art\nresults for seismic facies segmentation. DNFS is trained using a combination of\ncross-entropy and Jaccard loss functions. Our results show that DNFS obtains\nhighly detailed predictions for seismic facies segmentation using fewer\nparameters than StNet and U-Net.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 01:36:52 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Lima", "Gefersom", ""], ["Ramos", "Gabriel", ""], ["Rigo", "Sandro", ""], ["Zeiser", "Felipe", ""], ["da Silveira", "Ariane", ""]]}, {"id": "2012.03677", "submitter": "Li Xiao", "authors": "Yufan Luo, Li Xiao", "title": "G-RCN: Optimizing the Gap between Classification and Localization Tasks\n  for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-task learning is widely used in computer vision. Currently, object\ndetection models utilize shared feature map to complete classification and\nlocalization tasks simultaneously. By comparing the performance between the\noriginal Faster R-CNN and that with partially separated feature maps, we show\nthat: (1) Sharing high-level features for the classification and localization\ntasks is sub-optimal; (2) Large stride is beneficial for classification but\nharmful for localization; (3) Global context information could improve the\nperformance of classification. Based on these findings, we proposed a paradigm\ncalled Gap-optimized region based convolutional network (G-RCN), which aims to\nseparating these two tasks and optimizing the gap between them. The paradigm\nwas firstly applied to correct the current ResNet protocol by simply reducing\nthe stride and moving the Conv5 block from the head to the feature extraction\nnetwork, which brings 3.6 improvement of AP70 on the PASCAL VOC dataset and 1.5\nimprovement of AP on the COCO dataset for ResNet50. Next, the new method is\napplied on the Faster R-CNN with backbone of VGG16,ResNet50 and ResNet101,\nwhich brings above 2.0 improvement of AP70 on the PASCAL VOC dataset and above\n1.9 improvement of AP on the COCO dataset. Noticeably, the implementation of\nG-RCN only involves a few structural modifications, with no extra module added.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 04:14:01 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Luo", "Yufan", ""], ["Xiao", "Li", ""]]}, {"id": "2012.03678", "submitter": "Alkesh Patel", "authors": "Alkesh Patel, Akanksha Bindal, Hadas Kotek, Christopher Klein, Jason\n  Williams", "title": "Generating Natural Questions from Images for Multimodal Assistants", "comments": "4 pages, 1 reference page, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating natural, diverse, and meaningful questions from images is an\nessential task for multimodal assistants as it confirms whether they have\nunderstood the object and scene in the images properly. The research in visual\nquestion answering (VQA) and visual question generation (VQG) is a great step.\nHowever, this research does not capture questions that a visually-abled person\nwould ask multimodal assistants. Recently published datasets such as KB-VQA,\nFVQA, and OK-VQA try to collect questions that look for external knowledge\nwhich makes them appropriate for multimodal assistants. However, they still\ncontain many obvious and common-sense questions that humans would not usually\nask a digital assistant. In this paper, we provide a new benchmark dataset that\ncontains questions generated by human annotators keeping in mind what they\nwould ask multimodal digital assistants. Large scale annotations for several\nhundred thousand images are expensive and time-consuming, so we also present an\neffective way of automatically generating questions from unseen images. In this\npaper, we present an approach for generating diverse and meaningful questions\nthat consider image content and metadata of image (e.g., location, associated\nkeyword). We evaluate our approach using standard evaluation metrics such as\nBLEU, METEOR, ROUGE, and CIDEr to show the relevance of generated questions\nwith human-provided questions. We also measure the diversity of generated\nquestions using generative strength and inventiveness metrics. We report new\nstate-of-the-art results on the public and our datasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 19:12:23 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Patel", "Alkesh", ""], ["Bindal", "Akanksha", ""], ["Kotek", "Hadas", ""], ["Klein", "Christopher", ""], ["Williams", "Jason", ""]]}, {"id": "2012.03679", "submitter": "Elisa Chotzoglou", "authors": "Elisa Chotzoglou, Thomas Day, Jeremy Tan, Jacqueline Matthew, David\n  Lloyd, Reza Razavi, John Simpson, Bernhard Kainz", "title": "Learning normal appearance for fetal anomaly screening: Application to\n  the unsupervised detection of Hypoplastic Left Heart Syndrome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Congenital heart disease is considered as one the most common groups of\ncongenital malformations which affects $6-11$ per $1000$ newborns. In this\nwork, an automated framework for detection of cardiac anomalies during\nultrasound screening is proposed and evaluated on the example of Hypoplastic\nLeft Heart Syndrome (HLHS), a sub-category of congenital heart disease. We\npropose an unsupervised approach that learns healthy anatomy exclusively from\nclinically confirmed normal control patients. We evaluate a number of known\nanomaly detection frameworks together with a new model architecture based on\nthe $\\alpha$-GAN network and find evidence that the proposed model performs\nsignificantly better than the state-of-the-art in image-based anomaly\ndetection, yielding average $0.81$ AUC \\emph{and} a better robustness towards\ninitialisation compared to previous works.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 17:18:37 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Chotzoglou", "Elisa", ""], ["Day", "Thomas", ""], ["Tan", "Jeremy", ""], ["Matthew", "Jacqueline", ""], ["Lloyd", "David", ""], ["Razavi", "Reza", ""], ["Simpson", "John", ""], ["Kainz", "Bernhard", ""]]}, {"id": "2012.03680", "submitter": "Mathias Parger", "authors": "Mathias Parger, Chengcheng Tang, Yuanlu Xu, Christopher Twigg,\n  Lingling Tao, Yijing Li, Robert Wang, and Markus Steinberger", "title": "UNOC: Understanding Occlusion for Embodied Presence in Virtual Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tracking body and hand motions in the 3D space is essential for social and\nself-presence in augmented and virtual environments. Unlike the popular 3D pose\nestimation setting, the problem is often formulated as inside-out tracking\nbased on embodied perception (e.g., egocentric cameras, handheld sensors). In\nthis paper, we propose a new data-driven framework for inside-out body\ntracking, targeting challenges of omnipresent occlusions in optimization-based\nmethods (e.g., inverse kinematics solvers). We first collect a large-scale\nmotion capture dataset with both body and finger motions using optical markers\nand inertial sensors. This dataset focuses on social scenarios and captures\nground truth poses under self-occlusions and body-hand interactions. We then\nsimulate the occlusion patterns in head-mounted camera views on the captured\nground truth using a ray casting algorithm and learn a deep neural network to\ninfer the occluded body parts. In the experiments, we show that our method is\nable to generate high-fidelity embodied poses by applying the proposed method\non the task of real-time inside-out body tracking, finger motion synthesis, and\n3-point inverse kinematics.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 09:31:09 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Parger", "Mathias", ""], ["Tang", "Chengcheng", ""], ["Xu", "Yuanlu", ""], ["Twigg", "Christopher", ""], ["Tao", "Lingling", ""], ["Li", "Yijing", ""], ["Wang", "Robert", ""], ["Steinberger", "Markus", ""]]}, {"id": "2012.03681", "submitter": "Ergin Isleyen", "authors": "Ergin Isleyen, Sebnem Duzgun, McKell R. Carter", "title": "Roof fall hazard detection with convolutional neural networks using\n  transfer learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Roof falls due to geological conditions are major safety hazards in mining\nand tunneling industries, causing lost work times, injuries, and fatalities.\nSeveral large-opening limestone mines in the Eastern and Midwestern United\nStates have roof fall problems caused by high horizontal stresses. The typical\nhazard management approach for this type of roof fall hazard relies heavily on\nvisual inspections and expert knowledge. In this study, we propose an\nartificial intelligence (AI) based system for the detection roof fall hazards\ncaused by high horizontal stresses. We use images depicting hazardous and\nnon-hazardous roof conditions to develop a convolutional neural network for\nautonomous detection of hazardous roof conditions. To compensate for limited\ninput data, we utilize a transfer learning approach. In transfer learning, an\nalready-trained network is used as a starting point for classification in a\nsimilar domain. Results confirm that this approach works well for classifying\nroof conditions as hazardous or safe, achieving a statistical accuracy of 86%.\nHowever, accuracy alone is not enough to ensure a reliable hazard management\nsystem. System constraints and reliability are improved when the features being\nused by the network are understood. Therefore, we used a deep learning\ninterpretation technique called integrated gradients to identify the important\ngeologic features in each image for prediction. The analysis of integrated\ngradients shows that the system mimics expert judgment on roof fall hazard\ndetection. The system developed in this paper demonstrates the potential of\ndeep learning in geological hazard management to complement human experts, and\nlikely to become an essential part of autonomous tunneling operations in those\ncases where hazard identification heavily depends on expert knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 17:16:36 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Isleyen", "Ergin", ""], ["Duzgun", "Sebnem", ""], ["Carter", "McKell R.", ""]]}, {"id": "2012.03682", "submitter": "Elnaz Soleimani", "authors": "Elnaz Soleimani, Ghazaleh Khodabandelou, Abdelghani Chibani, Yacine\n  Amirat", "title": "Generic Semi-Supervised Adversarial Subject Translation for Sensor-Based\n  Human Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of Human Activity Recognition (HAR) models, particularly deep\nneural networks, is highly contingent upon the availability of the massive\namount of annotated training data which should be sufficiently labeled. Though,\ndata acquisition and manual annotation in the HAR domain are prohibitively\nexpensive due to skilled human resource requirements in both steps. Hence,\ndomain adaptation techniques have been proposed to adapt the knowledge from the\nexisting source of data. More recently, adversarial transfer learning methods\nhave shown very promising results in image classification, yet limited for\nsensor-based HAR problems, which are still prone to the unfavorable effects of\nthe imbalanced distribution of samples. This paper presents a novel generic and\nrobust approach for semi-supervised domain adaptation in HAR, which capitalizes\non the advantages of the adversarial framework to tackle the shortcomings, by\nleveraging knowledge from annotated samples exclusively from the source subject\nand unlabeled ones of the target subject. Extensive subject translation\nexperiments are conducted on three large, middle, and small-size datasets with\ndifferent levels of imbalance to assess the robustness and effectiveness of the\nproposed model to the scale as well as imbalance in the data. The results\ndemonstrate the effectiveness of our proposed algorithms over state-of-the-art\nmethods, which led in up to 13%, 4%, and 13% improvement of our high-level\nactivities recognition metrics for Opportunity, LISSI, and PAMAP2 datasets,\nrespectively. The LISSI dataset is the most challenging one owing to its less\npopulated and imbalanced distribution. Compared to the SA-GAN adversarial\ndomain adaptation method, the proposed approach enhances the final\nclassification performance with an average of 7.5% for the three datasets,\nwhich emphasizes the effectiveness of micro-mini-batch training.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 12:16:23 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Soleimani", "Elnaz", ""], ["Khodabandelou", "Ghazaleh", ""], ["Chibani", "Abdelghani", ""], ["Amirat", "Yacine", ""]]}, {"id": "2012.03683", "submitter": "Maani Ghaffari Jadidi", "authors": "Ray Zhang, Tzu-Yuan Lin, Chien Erh Lin, Steven A. Parkison, William\n  Clark, Jessy W. Grizzle, Ryan M. Eustice and Maani Ghaffari", "title": "A New Framework for Registration of Semantic Point Clouds from Stereo\n  and RGB-D Cameras", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper reports on a novel nonparametric rigid point cloud registration\nframework that jointly integrates geometric and semantic measurements such as\ncolor or semantic labels into the alignment process and does not require\nexplicit data association. The point clouds are represented as nonparametric\nfunctions in a reproducible kernel Hilbert space. The alignment problem is\nformulated as maximizing the inner product between two functions, essentially a\nsum of weighted kernels, each of which exploits the local geometric and\nsemantic features. As a result of the continuous models, analytical gradients\ncan be computed, and a local solution can be obtained by optimization over the\nrigid body transformation group. Besides, we present a new point cloud\nalignment metric that is intrinsic to the proposed framework and takes into\naccount geometric and semantic information. The evaluations using publicly\navailable stereo and RGB-D datasets show that the proposed method outperforms\nstate-of-the-art outdoor and indoor frame-to-frame registration methods. An\nopen-source GPU implementation is also provided.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 23:26:14 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zhang", "Ray", ""], ["Lin", "Tzu-Yuan", ""], ["Lin", "Chien Erh", ""], ["Parkison", "Steven A.", ""], ["Clark", "William", ""], ["Grizzle", "Jessy W.", ""], ["Eustice", "Ryan M.", ""], ["Ghaffari", "Maani", ""]]}, {"id": "2012.03684", "submitter": "Minh Vu", "authors": "Minh H. Vu and Tufve Nyholm and Tommy L\\\"ofstedt", "title": "Multi-Decoder Networks with Multi-Denoising Inputs for Tumor\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic segmentation of brain glioma from multimodal MRI scans plays a key\nrole in clinical trials and practice. Unfortunately, manual segmentation is\nvery challenging, time-consuming, costly, and often inaccurate despite human\nexpertise due to the high variance and high uncertainty in the human\nannotations. In the present work, we develop an end-to-end deep-learning-based\nsegmentation method using a multi-decoder architecture by jointly learning\nthree separate sub-problems using a partly shared encoder. We also propose to\napply smoothing methods to the input images to generate denoised versions as\nadditional inputs to the network. The validation performance indicate an\nimprovement when using the proposed method. The proposed method was ranked 2nd\nin the task of Quantification of Uncertainty in Segmentation in the Brain\nTumors in Multimodal Magnetic Resonance Imaging Challenge 2020.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 12:58:03 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Vu", "Minh H.", ""], ["Nyholm", "Tufve", ""], ["L\u00f6fstedt", "Tommy", ""]]}, {"id": "2012.03690", "submitter": "Kevin Mayer", "authors": "Benjamin Rausch, Kevin Mayer, Marie-Louise Arlt, Gunther Gust, Philipp\n  Staudt, Christof Weinhardt, Dirk Neumann, Ram Rajagopal", "title": "An Enriched Automated PV Registry: Combining Image Recognition and 3D\n  Building Data", "comments": "Tackling Climate Change with Machine Learning at NeurIPS 2020\n  (Spotlight talk)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While photovoltaic (PV) systems are installed at an unprecedented rate,\nreliable information on an installation level remains scarce. As a result,\nautomatically created PV registries are a timely contribution to optimize grid\nplanning and operations. This paper demonstrates how aerial imagery and\nthree-dimensional building data can be combined to create an address-level PV\nregistry, specifying area, tilt, and orientation angles. We demonstrate the\nbenefits of this approach for PV capacity estimation. In addition, this work\npresents, for the first time, a comparison between automated and\nofficially-created PV registries. Our results indicate that our enriched\nautomated registry proves to be useful to validate, update, and complement\nofficial registries.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 13:45:08 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Rausch", "Benjamin", ""], ["Mayer", "Kevin", ""], ["Arlt", "Marie-Louise", ""], ["Gust", "Gunther", ""], ["Staudt", "Philipp", ""], ["Weinhardt", "Christof", ""], ["Neumann", "Dirk", ""], ["Rajagopal", "Ram", ""]]}, {"id": "2012.03711", "submitter": "Eiman Kanjo Dr", "authors": "Kieran Woodward, Eiman Kanjo, Athanasios Tsanas", "title": "Combining Deep Transfer Learning with Signal-image Encoding for\n  Multi-Modal Mental Wellbeing Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The quantification of emotional states is an important step to understanding\nwellbeing. Time series data from multiple modalities such as physiological and\nmotion sensor data have proven to be integral for measuring and quantifying\nemotions. Monitoring emotional trajectories over long periods of time inherits\nsome critical limitations in relation to the size of the training data. This\nshortcoming may hinder the development of reliable and accurate machine\nlearning models. To address this problem, this paper proposes a framework to\ntackle the limitation in performing emotional state recognition on multiple\nmultimodal datasets: 1) encoding multivariate time series data into coloured\nimages; 2) leveraging pre-trained object recognition models to apply a Transfer\nLearning (TL) approach using the images from step 1; 3) utilising a 1D\nConvolutional Neural Network (CNN) to perform emotion classification from\nphysiological data; 4) concatenating the pre-trained TL model with the 1D CNN.\nFurthermore, the possibility of performing TL to infer stress from\nphysiological data is explored by initially training a 1D CNN using a large\nphysical activity dataset and then applying the learned knowledge to the target\ndataset. We demonstrate that model performance when inferring real-world\nwellbeing rated on a 5-point Likert scale can be enhanced using our framework,\nresulting in up to 98.5% accuracy, outperforming a conventional CNN by 4.5%.\nSubject-independent models using the same approach resulted in an average of\n72.3% accuracy (SD 0.038). The proposed CNN-TL-based methodology may overcome\nproblems with small training datasets, thus improving on the performance of\nconventional deep learning methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 13:37:23 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Woodward", "Kieran", ""], ["Kanjo", "Eiman", ""], ["Tsanas", "Athanasios", ""]]}, {"id": "2012.03753", "submitter": "Dengpan Fu", "authors": "Dengpan Fu, Dongdong Chen, Jianmin Bao, Hao Yang, Lu Yuan, Lei Zhang,\n  Houqiang Li, Dong Chen", "title": "Unsupervised Pre-training for Person Re-identification", "comments": "To appear at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a large scale unlabeled person re-identification\n(Re-ID) dataset \"LUPerson\" and make the first attempt of performing\nunsupervised pre-training for improving the generalization ability of the\nlearned person Re-ID feature representation. This is to address the problem\nthat all existing person Re-ID datasets are all of limited scale due to the\ncostly effort required for data annotation. Previous research tries to leverage\nmodels pre-trained on ImageNet to mitigate the shortage of person Re-ID data\nbut suffers from the large domain gap between ImageNet and person Re-ID data.\nLUPerson is an unlabeled dataset of 4M images of over 200K identities, which is\n30X larger than the largest existing Re-ID dataset. It also covers a much\ndiverse range of capturing environments (eg, camera settings, scenes, etc.).\nBased on this dataset, we systematically study the key factors for learning\nRe-ID features from two perspectives: data augmentation and contrastive loss.\nUnsupervised pre-training performed on this large-scale dataset effectively\nleads to a generic Re-ID feature that can benefit all existing person Re-ID\nmethods. Using our pre-trained model in some basic frameworks, our methods\nachieve state-of-the-art results without bells and whistles on four widely used\nRe-ID datasets: CUHK03, Market1501, DukeMTMC, and MSMT17. Our results also show\nthat the performance improvement is more significant on small-scale target\ndatasets or under few-shot setting.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 14:48:26 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 04:51:41 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Fu", "Dengpan", ""], ["Chen", "Dongdong", ""], ["Bao", "Jianmin", ""], ["Yang", "Hao", ""], ["Yuan", "Lu", ""], ["Zhang", "Lei", ""], ["Li", "Houqiang", ""], ["Chen", "Dong", ""]]}, {"id": "2012.03762", "submitter": "Xu Yan", "authors": "Xu Yan, Jiantao Gao, Jie Li, Ruimao Zhang, Zhen Li, Rui Huang,\n  Shuguang Cui", "title": "Sparse Single Sweep LiDAR Point Cloud Segmentation via Learning\n  Contextual Shape Priors from Scene Completion", "comments": "To appear in AAAI 2021. Codes are available at\n  https://github.com/yanx27/JS3C-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  LiDAR point cloud analysis is a core task for 3D computer vision, especially\nfor autonomous driving. However, due to the severe sparsity and noise\ninterference in the single sweep LiDAR point cloud, the accurate semantic\nsegmentation is non-trivial to achieve. In this paper, we propose a novel\nsparse LiDAR point cloud semantic segmentation framework assisted by learned\ncontextual shape priors. In practice, an initial semantic segmentation (SS) of\na single sweep point cloud can be achieved by any appealing network and then\nflows into the semantic scene completion (SSC) module as the input. By merging\nmultiple frames in the LiDAR sequence as supervision, the optimized SSC module\nhas learned the contextual shape priors from sequential LiDAR data, completing\nthe sparse single sweep point cloud to the dense one. Thus, it inherently\nimproves SS optimization through fully end-to-end training. Besides, a\nPoint-Voxel Interaction (PVI) module is proposed to further enhance the\nknowledge fusion between SS and SSC tasks, i.e., promoting the interaction of\nincomplete local geometry of point cloud and complete voxel-wise global\nstructure. Furthermore, the auxiliary SSC and PVI modules can be discarded\nduring inference without extra burden for SS. Extensive experiments confirm\nthat our JS3C-Net achieves superior performance on both SemanticKITTI and\nSemanticPOSS benchmarks, i.e., 4% and 3% improvement correspondingly.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 14:58:25 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Yan", "Xu", ""], ["Gao", "Jiantao", ""], ["Li", "Jie", ""], ["Zhang", "Ruimao", ""], ["Li", "Zhen", ""], ["Huang", "Rui", ""], ["Cui", "Shuguang", ""]]}, {"id": "2012.03768", "submitter": "Paloma Sodhi", "authors": "Paloma Sodhi, Michael Kaess, Mustafa Mukadam, Stuart Anderson", "title": "Learning Tactile Models for Factor Graph-based Estimation", "comments": "Accepted to ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We're interested in the problem of estimating object states from touch during\nmanipulation under occlusions. In this work, we address the problem of\nestimating object poses from touch during planar pushing. Vision-based tactile\nsensors provide rich, local image measurements at the point of contact. A\nsingle such measurement, however, contains limited information and multiple\nmeasurements are needed to infer latent object state. We solve this inference\nproblem using a factor graph. In order to incorporate tactile measurements in\nthe graph, we need local observation models that can map high-dimensional\ntactile images onto a low-dimensional state space. Prior work has used\nlow-dimensional force measurements or engineered functions to interpret tactile\nmeasurements. These methods, however, can be brittle and difficult to scale\nacross objects and sensors. Our key insight is to directly learn tactile\nobservation models that predict the relative pose of the sensor given a pair of\ntactile images. These relative poses can then be incorporated as factors within\na factor graph. We propose a two-stage approach: first we learn local tactile\nobservation models supervised with ground truth data, and then integrate these\nmodels along with physics and geometric factors within a factor graph\noptimizer. We demonstrate reliable object tracking using only tactile feedback\nfor 150 real-world planar pushing sequences with varying trajectories across\nthree object shapes. Supplementary video: https://youtu.be/y1kBfSmi8w0\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 15:09:31 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 19:34:52 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Sodhi", "Paloma", ""], ["Kaess", "Michael", ""], ["Mukadam", "Mustafa", ""], ["Anderson", "Stuart", ""]]}, {"id": "2012.03769", "submitter": "August DuMont Sch\\\"utte", "authors": "August DuMont Sch\\\"utte, J\\\"urgen Hetzel, Sergios Gatidis, Tobias\n  Hepp, Benedikt Dietz, Stefan Bauer and Patrick Schwab", "title": "Overcoming Barriers to Data Sharing with Medical Image Generation: A\n  Comprehensive Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy concerns around sharing personally identifiable information are a\nmajor practical barrier to data sharing in medical research. However, in many\ncases, researchers have no interest in a particular individual's information\nbut rather aim to derive insights at the level of cohorts. Here, we utilize\nGenerative Adversarial Networks (GANs) to create derived medical imaging\ndatasets consisting entirely of synthetic patient data. The synthetic images\nideally have, in aggregate, similar statistical properties to those of a source\ndataset but do not contain sensitive personal information. We assess the\nquality of synthetic data generated by two GAN models for chest radiographs\nwith 14 different radiology findings and brain computed tomography (CT) scans\nwith six types of intracranial hemorrhages. We measure the synthetic image\nquality by the performance difference of predictive models trained on either\nthe synthetic or the real dataset. We find that synthetic data performance\ndisproportionately benefits from a reduced number of unique label combinations.\nOur open-source benchmark also indicates that at low number of samples per\nclass, label overfitting effects start to dominate GAN training. We\nadditionally conducted a reader study in which trained radiologists do not\nperform better than random on discriminating between synthetic and real medical\nimages for intermediate levels of resolutions. In accordance with our benchmark\nresults, the classification accuracy of radiologists increases at higher\nspatial resolution levels. Our study offers valuable guidelines and outlines\npractical conditions under which insights derived from synthetic medical images\nare similar to those that would have been derived from real imaging data. Our\nresults indicate that synthetic data sharing may be an attractive and\nprivacy-preserving alternative to sharing real patient-level data in the right\nsettings.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 15:41:46 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 07:59:44 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Sch\u00fctte", "August DuMont", ""], ["Hetzel", "J\u00fcrgen", ""], ["Gatidis", "Sergios", ""], ["Hepp", "Tobias", ""], ["Dietz", "Benedikt", ""], ["Bauer", "Stefan", ""], ["Schwab", "Patrick", ""]]}, {"id": "2012.03775", "submitter": "Ruan van der Merwe Mr", "authors": "Ruan van der Merwe", "title": "Triplet Entropy Loss: Improving The Generalisation of Short Speech\n  Language Identification Systems", "comments": "22 pages, 26 figures, Code available at\n  https://github.com/ruanvdmerwe/triplet-entropy-loss", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present several methods to improve the generalisation of language\nidentification (LID) systems to new speakers and to new domains. These methods\ninvolve Spectral augmentation, where spectrograms are masked in the frequency\nor time bands during training and CNN architectures that are pre-trained on the\nImagenet dataset. The paper also introduces the novel Triplet Entropy Loss\ntraining method, which involves training a network simultaneously using Cross\nEntropy and Triplet loss. It was found that all three methods improved the\ngeneralisation of the models, though not significantly. Even though the models\ntrained using Triplet Entropy Loss showed a better understanding of the\nlanguages and higher accuracies, it appears as though the models still memorise\nword patterns present in the spectrograms rather than learning the finer\nnuances of a language. The research shows that Triplet Entropy Loss has great\npotential and should be investigated further, not only in language\nidentification tasks but any classification task.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 08:20:03 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["van der Merwe", "Ruan", ""]]}, {"id": "2012.03790", "submitter": "Fariborz Taherkhani", "authors": "Fariborz Taherkhani, Hadi Kazemi, Ali Dabouei, Jeremy Dawson, Nasser\n  M. Nasrabadi", "title": "Matching Distributions via Optimal Transport for Semi-Supervised\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semi-Supervised Learning (SSL) approaches have been an influential framework\nfor the usage of unlabeled data when there is not a sufficient amount of\nlabeled data available over the course of training. SSL methods based on\nConvolutional Neural Networks (CNNs) have recently provided successful results\non standard benchmark tasks such as image classification. In this work, we\nconsider the general setting of SSL problem where the labeled and unlabeled\ndata come from the same underlying probability distribution. We propose a new\napproach that adopts an Optimal Transport (OT) technique serving as a metric of\nsimilarity between discrete empirical probability measures to provide\npseudo-labels for the unlabeled data, which can then be used in conjunction\nwith the initial labeled data to train the CNN model in an SSL manner. We have\nevaluated and compared our proposed method with state-of-the-art SSL algorithms\non standard datasets to demonstrate the superiority and effectiveness of our\nSSL algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 11:15:14 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Taherkhani", "Fariborz", ""], ["Kazemi", "Hadi", ""], ["Dabouei", "Ali", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "2012.03796", "submitter": "Jae Shin Yoon", "authors": "Jae Shin Yoon, Lingjie Liu, Vladislav Golyanik, Kripasindhu Sarkar,\n  Hyun Soo Park, Christian Theobalt", "title": "Pose-Guided Human Animation from a Single Image in the Wild", "comments": "14 pages including Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new pose transfer method for synthesizing a human animation from\na single image of a person controlled by a sequence of body poses. Existing\npose transfer methods exhibit significant visual artifacts when applying to a\nnovel scene, resulting in temporal inconsistency and failures in preserving the\nidentity and textures of the person. To address these limitations, we design a\ncompositional neural network that predicts the silhouette, garment labels, and\ntextures. Each modular network is explicitly dedicated to a subtask that can be\nlearned from the synthetic data. At the inference time, we utilize the trained\nnetwork to produce a unified representation of appearance and its labels in UV\ncoordinates, which remains constant across poses. The unified representation\nprovides an incomplete yet strong guidance to generating the appearance in\nresponse to the pose change. We use the trained network to complete the\nappearance and render it with the background. With these strategies, we are\nable to synthesize human animations that can preserve the identity and\nappearance of the person in a temporally coherent way without any fine-tuning\nof the network on the testing scene. Experiments show that our method\noutperforms the state-of-the-arts in terms of synthesis quality, temporal\ncoherence, and generalization ability.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 15:38:29 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Yoon", "Jae Shin", ""], ["Liu", "Lingjie", ""], ["Golyanik", "Vladislav", ""], ["Sarkar", "Kripasindhu", ""], ["Park", "Hyun Soo", ""], ["Theobalt", "Christian", ""]]}, {"id": "2012.03806", "submitter": "Sebastian H\\\"ofer", "authors": "Sebastian H\\\"ofer, Kostas Bekris, Ankur Handa, Juan Camilo Gamboa,\n  Florian Golemo, Melissa Mozifian, Chris Atkeson, Dieter Fox, Ken Goldberg,\n  John Leonard, C. Karen Liu, Jan Peters, Shuran Song, Peter Welinder, Martha\n  White", "title": "Perspectives on Sim2Real Transfer for Robotics: A Summary of the R:SS\n  2020 Workshop", "comments": "Summary of the \"2nd Workshop on Closing the Reality Gap in Sim2Real\n  Transfer for Robotics\" held in conjunction with \"Robotics: Science and System\n  2020\". Website: https://sim2real.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report presents the debates, posters, and discussions of the Sim2Real\nworkshop held in conjunction with the 2020 edition of the \"Robotics: Science\nand System\" conference. Twelve leaders of the field took competing debate\npositions on the definition, viability, and importance of transferring skills\nfrom simulation to the real world in the context of robotics problems. The\ndebaters also joined a large panel discussion, answering audience questions and\noutlining the future of Sim2Real in robotics. Furthermore, we invited extended\nabstracts to this workshop which are summarized in this report. Based on the\nworkshop, this report concludes with directions for practitioners exploiting\nthis technology and for researchers further exploring open problems in this\narea.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 15:48:26 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["H\u00f6fer", "Sebastian", ""], ["Bekris", "Kostas", ""], ["Handa", "Ankur", ""], ["Gamboa", "Juan Camilo", ""], ["Golemo", "Florian", ""], ["Mozifian", "Melissa", ""], ["Atkeson", "Chris", ""], ["Fox", "Dieter", ""], ["Goldberg", "Ken", ""], ["Leonard", "John", ""], ["Liu", "C. Karen", ""], ["Peters", "Jan", ""], ["Song", "Shuran", ""], ["Welinder", "Peter", ""], ["White", "Martha", ""]]}, {"id": "2012.03820", "submitter": "Zhengyang Yu", "authors": "Zhengyang Yu, Song Wu, Zhihao Dou and Erwin M.Bakker", "title": "Self-supervised asymmetric deep hashing with margin-scalable constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its effectivity and efficiency, deep hashing approaches are widely\nused for large-scale visual search. However, it is still challenging to produce\ncompact and discriminative hash codes for images associated with multiple\nsemantics for two main reasons, 1) similarity constraints designed in most of\nthe existing methods are based upon an oversimplified similarity\nassignment(i.e., 0 for instance pairs sharing no label, 1 for instance pairs\nsharing at least 1 label), 2) the exploration in multi-semantic relevance are\ninsufficient or even neglected in many of the existing methods. These problems\nsignificantly limit the discrimination of generated hash codes. In this paper,\nwe propose a novel self-supervised asymmetric deep hashing method with a\nmargin-scalable constraint(SADH) approach to cope with these problems. SADH\nimplements a self-supervised network to sufficiently preserve semantic\ninformation in a semantic feature dictionary and a semantic code dictionary for\nthe semantics of the given dataset, which efficiently and precisely guides a\nfeature learning network to preserve multilabel semantic information using an\nasymmetric learning strategy. By further exploiting semantic dictionaries, a\nnew margin-scalable constraint is employed for both precise similarity\nsearching and robust hash code generation. Extensive empirical research on four\npopular benchmarks validates the proposed method and shows it outperforms\nseveral state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 16:09:37 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 09:30:39 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 15:46:37 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Yu", "Zhengyang", ""], ["Wu", "Song", ""], ["Dou", "Zhihao", ""], ["Bakker", "Erwin M.", ""]]}, {"id": "2012.03827", "submitter": "Yael Ben Guigui", "authors": "Yael Ben-Guigui, Jacob Goldberger, Tammy Riklin-Raviv", "title": "The Role of Regularization in Shaping Weight and Node Pruning Dependency\n  and Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pressing need to reduce the capacity of deep neural networks has\nstimulated the development of network dilution methods and their analysis.\nWhile the ability of $L_1$ and $L_0$ regularization to encourage sparsity is\noften mentioned, $L_2$ regularization is seldom discussed in this context. We\npresent a novel framework for weight pruning by sampling from a probability\nfunction that favors the zeroing of smaller weights. In addition, we examine\nthe contribution of $L_1$ and $L_2$ regularization to the dynamics of node\npruning while optimizing for weight pruning. We then demonstrate the\neffectiveness of the proposed stochastic framework when used together with a\nweight decay regularizer on popular classification models in removing 50% of\nthe nodes in an MLP for MNIST classification, 60% of the filters in VGG-16 for\nCIFAR10 classification, and on medical image models in removing 60% of the\nchannels in a U-Net for instance segmentation and 50% of the channels in CNN\nmodel for COVID-19 detection. For these node-pruned networks, we also present\ncompetitive weight pruning results that are only slightly less accurate than\nthe original, dense networks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 16:22:20 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Ben-Guigui", "Yael", ""], ["Goldberger", "Jacob", ""], ["Riklin-Raviv", "Tammy", ""]]}, {"id": "2012.03842", "submitter": "Jong Chul Ye", "authors": "Gyutaek Oh, Hyokyoung Bae, Hyun-Seo Ahn, Sung-Hong Park, and Jong Chul\n  Ye", "title": "CycleQSM: Unsupervised QSM Deep Learning using Physics-Informed CycleGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantitative susceptibility mapping (QSM) is a useful magnetic resonance\nimaging (MRI) technique which provides spatial distribution of magnetic\nsusceptibility values of tissues. QSMs can be obtained by deconvolving the\ndipole kernel from phase images, but the spectral nulls in the dipole kernel\nmake the inversion ill-posed. In recent times, deep learning approaches have\nshown a comparable QSM reconstruction performance as the classic approaches,\ndespite the fast reconstruction time. Most of the existing deep learning\nmethods are, however, based on supervised learning, so matched pairs of input\nphase images and the ground-truth maps are needed. Moreover, it was reported\nthat the supervised learning often leads to underestimated QSM values. To\naddress this, here we propose a novel unsupervised QSM deep learning method\nusing physics-informed cycleGAN, which is derived from optimal transport\nperspective. In contrast to the conventional cycleGAN, our novel cycleGAN has\nonly one generator and one discriminator thanks to the known dipole kernel.\nExperimental results confirm that the proposed method provides more accurate\nQSM maps compared to the existing deep learning approaches, and provide\ncompetitive performance to the best classical approaches despite the ultra-fast\nreconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 16:46:15 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Oh", "Gyutaek", ""], ["Bae", "Hyokyoung", ""], ["Ahn", "Hyun-Seo", ""], ["Park", "Sung-Hong", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2012.03843", "submitter": "Soichiro Kumano", "authors": "Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki", "title": "Sparse Fooling Images: Fooling Machine Perception through Unrecognizable\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years, deep neural networks (DNNs) have achieved equivalent or even\nhigher accuracy in various recognition tasks than humans. However, some images\nexist that lead DNNs to a completely wrong decision, whereas humans never fail\nwith these images. Among others, fooling images are those that are not\nrecognizable as natural objects such as dogs and cats, but DNNs classify these\nimages into classes with high confidence scores. In this paper, we propose a\nnew class of fooling images, sparse fooling images (SFIs), which are single\ncolor images with a small number of altered pixels. Unlike existing fooling\nimages, which retain some characteristic features of natural objects, SFIs do\nnot have any local or global features that can be recognizable to humans;\nhowever, in machine perception (i.e., by DNN classifiers), SFIs are\nrecognizable as natural objects and classified to certain classes with high\nconfidence scores. We propose two methods to generate SFIs for different\nsettings~(semiblack-box and white-box). We also experimentally demonstrate the\nvulnerability of DNNs through out-of-distribution detection and compare three\narchitectures in terms of the robustness against SFIs. This study gives rise to\nquestions on the structure and robustness of CNNs and discusses the differences\nbetween human and machine perception.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 16:47:33 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Kumano", "Soichiro", ""], ["Kera", "Hiroshi", ""], ["Yamasaki", "Toshihiko", ""]]}, {"id": "2012.03849", "submitter": "Simone Palazzo", "authors": "Simone Palazzo, Concetto Spampinato, Joseph Schmidt, Isaak Kavasidis,\n  Daniela Giordano, Mubarak Shah", "title": "Correct block-design experiments mitigate temporal correlation bias in\n  EEG classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is argued in [1] that [2] was able to classify EEG responses to visual\nstimuli solely because of the temporal correlation that exists in all EEG data\nand the use of a block design. We here show that the main claim in [1] is\ndrastically overstated and their other analyses are seriously flawed by wrong\nmethodological choices. To validate our counter-claims, we evaluate the\nperformance of state-of-the-art methods on the dataset in [2] reaching about\n50% classification accuracy over 40 classes, lower than in [2], but still\nsignificant. We then investigate the influence of EEG temporal correlation on\nclassification accuracy by testing the same models in two additional\nexperimental settings: one that replicates [1]'s rapid-design experiment, and\nanother one that examines the data between blocks while subjects are shown a\nblank screen. In both cases, classification accuracy is at or near chance, in\ncontrast to what [1] reports, indicating a negligible contribution of temporal\ncorrelation to classification accuracy. We, instead, are able to replicate the\nresults in [1] only when intentionally contaminating our data by inducing a\ntemporal correlation. This suggests that what Li et al. [1] demonstrate is that\ntheir data are strongly contaminated by temporal correlation and low\nsignal-to-noise ratio. We argue that the reason why Li et al. [1] observe such\nhigh correlation in EEG data is their unconventional experimental design and\nsettings that violate the basic cognitive neuroscience design recommendations,\nfirst and foremost the one of limiting the experiments' duration, as instead\ndone in [2]. Our analyses in this paper refute the claims of the \"perils and\npitfalls of block-design\" in [1]. Finally, we conclude the paper by examining a\nnumber of other oversimplistic statements, inconsistencies, misinterpretation\nof machine learning concepts, speculations and misleading claims in [1].\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 22:25:21 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Palazzo", "Simone", ""], ["Spampinato", "Concetto", ""], ["Schmidt", "Joseph", ""], ["Kavasidis", "Isaak", ""], ["Giordano", "Daniela", ""], ["Shah", "Mubarak", ""]]}, {"id": "2012.03868", "submitter": "Denis Coquenet", "authors": "Denis Coquenet, Cl\\'ement Chatelain, Thierry Paquet", "title": "End-to-end Handwritten Paragraph Text Recognition Using a Vertical\n  Attention Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unconstrained handwritten text recognition remains challenging for computer\nvision systems. Paragraph text recognition is traditionally achieved by two\nmodels: the first one for line segmentation and the second one for text line\nrecognition. We propose a unified end-to-end model using hybrid attention to\ntackle this task. We achieve state-of-the-art character error rate at line and\nparagraph levels on three popular datasets: 1.90% for RIMES, 4.32% for IAM and\n3.63% for READ 2016. The proposed model can be trained from scratch, without\nusing any segmentation label contrary to the standard approach. Our code and\ntrained model weights are available at\nhttps://github.com/FactoDeepLearning/VerticalAttentionOCR.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 17:31:20 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Coquenet", "Denis", ""], ["Chatelain", "Cl\u00e9ment", ""], ["Paquet", "Thierry", ""]]}, {"id": "2012.03874", "submitter": "Alabi Bojesomo", "authors": "Alabi Bojesomo, Panos Liatsis, Hasan Al Marzouqi", "title": "Traffic flow prediction using Deep Sedenion Networks", "comments": "5 pages, 4 figures, 1 table, 2 equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present our solution to the Traffic4cast2020 traffic\nprediction challenge. In this competition, participants are to predict future\ntraffic parameters (speed and volume) in three different cities: Berlin,\nIstanbul and Moscow. The information provided includes nine channels where the\nfirst eight represent the speed and volume for four different direction of\ntraffic (NE, NW, SE and SW), while the last channel is used to indicate\npresence of traffic incidents. The expected output should have the first 8\nchannels of the input at six future timing intervals (5, 10, 15, 30, 45, and\n60min), while a one hour duration of past traffic data, in 5mins intervals, are\nprovided as input. We solve the problem using a novel sedenion U-Net neural\nnetwork. Sedenion networks provide the means for efficient encoding of\ncorrelated multimodal datasets. We use 12 of the 15 sedenion imaginary parts\nfor the dynamic inputs and the real sedenion component is used for the static\ninput. The sedenion output of the network is used to represent the multimodal\ntraffic predictions. Proposed system achieved a validation MSE of 1.33e-3 and a\ntest MSE of 1.31e-3.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 17:37:56 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 10:31:58 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Bojesomo", "Alabi", ""], ["Liatsis", "Panos", ""], ["Marzouqi", "Hasan Al", ""]]}, {"id": "2012.03881", "submitter": "Chirag Vashist", "authors": "Avantika Singh, Chirag Vashist, Pratyush Gaurav, Aditya Nigam,\n  Rameshwar Pratap", "title": "IHashNet: Iris Hashing Network based on efficient multi-index hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive biometric deployments are pervasive in today's world. But despite the\nhigh accuracy of biometric systems, their computational efficiency degrades\ndrastically with an increase in the database size. Thus, it is essential to\nindex them. An ideal indexing scheme needs to generate codes that preserve the\nintra-subject similarity as well as inter-subject dissimilarity. Here, in this\npaper, we propose an iris indexing scheme using real-valued deep iris features\nbinarized to iris bar codes (IBC) compatible with the indexing structure.\nFirstly, for extracting robust iris features, we have designed a network\nutilizing the domain knowledge of ordinal filtering and learning their\nnonlinear combinations. Later these real-valued features are binarized.\nFinally, for indexing the iris dataset, we have proposed a loss that can\ntransform the binary feature into an improved feature compatible with the\nMulti-Index Hashing scheme. This loss function ensures the hamming distance\nequally distributed among all the contiguous disjoint sub-strings. To the best\nof our knowledge, this is the first work in the iris indexing domain that\npresents an end-to-end iris indexing structure. Experimental results on four\ndatasets are presented to depict the efficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 17:50:57 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Singh", "Avantika", ""], ["Vashist", "Chirag", ""], ["Gaurav", "Pratyush", ""], ["Nigam", "Aditya", ""], ["Pratap", "Rameshwar", ""]]}, {"id": "2012.03907", "submitter": "Suhas Lohit", "authors": "Suhas Lohit, Michael Jones", "title": "Model Compression Using Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model compression methods are important to allow for easier deployment of\ndeep learning models in compute, memory and energy-constrained environments\nsuch as mobile phones. Knowledge distillation is a class of model compression\nalgorithm where knowledge from a large teacher network is transferred to a\nsmaller student network thereby improving the student's performance. In this\npaper, we show how optimal transport-based loss functions can be used for\ntraining a student network which encourages learning student network parameters\nthat help bring the distribution of student features closer to that of the\nteacher features. We present image classification results on CIFAR-100, SVHN\nand ImageNet and show that the proposed optimal transport loss functions\nperform comparably to or better than other loss functions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:35:33 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Lohit", "Suhas", ""], ["Jones", "Michael", ""]]}, {"id": "2012.03911", "submitter": "Joakim Johnander", "authors": "Joakim Johnander, Emil Brissman, Martin Danelljan, Michael Felsberg", "title": "Learning Video Instance Segmentation with Recurrent Graph Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most existing approaches to video instance segmentation comprise multiple\nmodules that are heuristically combined to produce the final output.\nFormulating a purely learning-based method instead, which models both the\ntemporal aspect as well as a generic track management required to solve the\nvideo instance segmentation task, is a highly challenging problem. In this\nwork, we propose a novel learning formulation, where the entire video instance\nsegmentation problem is modelled jointly. We fit a flexible model to our\nformulation that, with the help of a graph neural network, processes all\navailable new information in each frame. Past information is considered and\nprocessed via a recurrent connection. We demonstrate the effectiveness of the\nproposed approach in comprehensive experiments. Our approach, operating at over\n25 FPS, outperforms previous video real-time methods. We further conduct\ndetailed ablative experiments that validate the different aspects of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:41:35 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Johnander", "Joakim", ""], ["Brissman", "Emil", ""], ["Danelljan", "Martin", ""], ["Felsberg", "Michael", ""]]}, {"id": "2012.03912", "submitter": "Saim Wani", "authors": "Saim Wani, Shivansh Patel, Unnat Jain, Angel X. Chang, Manolis Savva", "title": "MultiON: Benchmarking Semantic Map Memory using Multi-Object Navigation", "comments": "Project page: https://shivanshpatel35.github.io/multi-ON/ ; the first\n  three authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Navigation tasks in photorealistic 3D environments are challenging because\nthey require perception and effective planning under partial observability.\nRecent work shows that map-like memory is useful for long-horizon navigation\ntasks. However, a focused investigation of the impact of maps on navigation\ntasks of varying complexity has not yet been performed. We propose the multiON\ntask, which requires navigation to an episode-specific sequence of objects in a\nrealistic environment. MultiON generalizes the ObjectGoal navigation task and\nexplicitly tests the ability of navigation agents to locate previously observed\ngoal objects. We perform a set of multiON experiments to examine how a variety\nof agent models perform across a spectrum of navigation task complexities. Our\nexperiments show that: i) navigation performance degrades dramatically with\nescalating task complexity; ii) a simple semantic map agent performs\nsurprisingly well relative to more complex neural image feature map agents; and\niii) even oracle map agents achieve relatively low performance, indicating the\npotential for future work in training embodied navigation agents using maps.\nVideo summary: https://youtu.be/yqTlHNIcgnY\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:42:38 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Wani", "Saim", ""], ["Patel", "Shivansh", ""], ["Jain", "Unnat", ""], ["Chang", "Angel X.", ""], ["Savva", "Manolis", ""]]}, {"id": "2012.03918", "submitter": "Mark Boss", "authors": "Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu,\n  Hendrik P.A. Lensch", "title": "NeRD: Neural Reflectance Decomposition from Image Collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decomposing a scene into its shape, reflectance, and illumination is a\nchallenging but essential problem in computer vision and graphics. This problem\nis inherently more challenging when the illumination is not a single light\nsource under laboratory conditions but is instead an unconstrained\nenvironmental illumination. Though recent work has shown that implicit\nrepresentations can be used to model the radiance field of an object, these\ntechniques only enable view synthesis and not relighting. Additionally,\nevaluating these radiance fields is resource and time-intensive. By decomposing\na scene into explicit representations, any rendering framework can be leveraged\nto generate novel views under any illumination in real-time. NeRD is a method\nthat achieves this decomposition by introducing physically-based rendering to\nneural radiance fields. Even challenging non-Lambertian reflectances, complex\ngeometry, and unknown illumination can be decomposed into high-quality models.\nThe datasets and code is available on the project page:\nhttps://markboss.me/publication/2021-nerd/\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:45:57 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 15:48:18 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 07:39:00 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Boss", "Mark", ""], ["Braun", "Raphael", ""], ["Jampani", "Varun", ""], ["Barron", "Jonathan T.", ""], ["Liu", "Ce", ""], ["Lensch", "Hendrik P. A.", ""]]}, {"id": "2012.03927", "submitter": "Pratul Srinivasan", "authors": "Pratul P. Srinivasan and Boyang Deng and Xiuming Zhang and Matthew\n  Tancik and Ben Mildenhall and Jonathan T. Barron", "title": "NeRV: Neural Reflectance and Visibility Fields for Relighting and View\n  Synthesis", "comments": "Project page: https://people.eecs.berkeley.edu/~pratul/nerv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that takes as input a set of images of a scene\nilluminated by unconstrained known lighting, and produces as output a 3D\nrepresentation that can be rendered from novel viewpoints under arbitrary\nlighting conditions. Our method represents the scene as a continuous volumetric\nfunction parameterized as MLPs whose inputs are a 3D location and whose outputs\nare the following scene properties at that input location: volume density,\nsurface normal, material parameters, distance to the first surface intersection\nin any direction, and visibility of the external environment in any direction.\nTogether, these allow us to render novel views of the object under arbitrary\nlighting, including indirect illumination effects. The predicted visibility and\nsurface intersection fields are critical to our model's ability to simulate\ndirect and indirect illumination during training, because the brute-force\ntechniques used by prior work are intractable for lighting conditions outside\nof controlled setups with a single light. Our method outperforms alternative\napproaches for recovering relightable 3D scene representations, and performs\nwell in complex lighting settings that have posed a significant challenge to\nprior work.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:56:08 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Srinivasan", "Pratul P.", ""], ["Deng", "Boyang", ""], ["Zhang", "Xiuming", ""], ["Tancik", "Matthew", ""], ["Mildenhall", "Ben", ""], ["Barron", "Jonathan T.", ""]]}, {"id": "2012.03930", "submitter": "Dongdong Chen", "authors": "Xiaoyi Dong and Jianmin Bao and Dongdong Chen and Weiming Zhang and\n  Nenghai Yu and Dong Chen and Fang Wen and Baining Guo", "title": "Identity-Driven DeepFake Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DeepFake detection has so far been dominated by ``artifact-driven'' methods\nand the detection performance significantly degrades when either the type of\nimage artifacts is unknown or the artifacts are simply too hard to find. In\nthis work, we present an alternative approach: Identity-Driven DeepFake\nDetection. Our approach takes as input the suspect image/video as well as the\ntarget identity information (a reference image or video). We output a decision\non whether the identity in the suspect image/video is the same as the target\nidentity. Our motivation is to prevent the most common and harmful DeepFakes\nthat spread false information of a targeted person. The identity-based approach\nis fundamentally different in that it does not attempt to detect image\nartifacts. Instead, it focuses on whether the identity in the suspect\nimage/video is true. To facilitate research on identity-based detection, we\npresent a new large scale dataset ``Vox-DeepFake\", in which each suspect\ncontent is associated with multiple reference images collected from videos of a\ntarget identity. We also present a simple identity-based detection algorithm\ncalled the OuterFace, which may serve as a baseline for further research. Even\ntrained without fake videos, the OuterFace algorithm achieves superior\ndetection accuracy and generalizes well to different DeepFake methods, and is\nrobust with respect to video degradation techniques -- a performance not\nachievable with existing detection algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:59:08 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Dong", "Xiaoyi", ""], ["Bao", "Jianmin", ""], ["Chen", "Dongdong", ""], ["Zhang", "Weiming", ""], ["Yu", "Nenghai", ""], ["Chen", "Dong", ""], ["Wen", "Fang", ""], ["Guo", "Baining", ""]]}, {"id": "2012.03939", "submitter": "Purvi Goel", "authors": "Purvi Goel, Loudon Cohen, James Guesman, Vikas Thamizharasan, James\n  Tompkin, Daniel Ritchie", "title": "Shape From Tracing: Towards Reconstructing 3D Object Geometry and SVBRDF\n  Material from Images via Differentiable Path Tracing", "comments": "Will be published at 3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reconstructing object geometry and material from multiple views typically\nrequires optimization. Differentiable path tracing is an appealing framework as\nit can reproduce complex appearance effects. However, it is difficult to use\ndue to high computational cost. In this paper, we explore how to use\ndifferentiable ray tracing to refine an initial coarse mesh and per-mesh-facet\nmaterial representation. In simulation, we find that it is possible to\nreconstruct fine geometric and material detail from low resolution input views,\nallowing high-quality reconstructions in a few hours despite the expense of\npath tracing. The reconstructions successfully disambiguate shading, shadow,\nand global illumination effects such as diffuse interreflection from material\nproperties. We demonstrate the impact of different geometry initializations,\nincluding space carving, multi-view stereo, and 3D neural networks. Finally,\nwith input captured using smartphone video and a consumer 360? camera for\nlighting estimation, we also show how to refine initial reconstructions of\nreal-world objects in unconstrained environments.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 18:55:35 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Goel", "Purvi", ""], ["Cohen", "Loudon", ""], ["Guesman", "James", ""], ["Thamizharasan", "Vikas", ""], ["Tompkin", "James", ""], ["Ritchie", "Daniel", ""]]}, {"id": "2012.03998", "submitter": "Mohammad Keshavarzi", "authors": "Mohammad Keshavarzi, Oladapo Afolabi, Luisa Caldas, Allen Y. Yang,\n  Avideh Zakhor", "title": "GenScan: A Generative Method for Populating Parametric 3D Scan Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of rich 3D datasets corresponding to the geometrical\ncomplexity of the built environments is considered an ongoing challenge for 3D\ndeep learning methodologies. To address this challenge, we introduce GenScan, a\ngenerative system that populates synthetic 3D scan datasets in a parametric\nfashion. The system takes an existing captured 3D scan as an input and outputs\nalternative variations of the building layout including walls, doors, and\nfurniture with corresponding textures. GenScan is a fully automated system that\ncan also be manually controlled by a user through an assigned user interface.\nOur proposed system utilizes a combination of a hybrid deep neural network and\na parametrizer module to extract and transform elements of a given 3D scan.\nGenScan takes advantage of style transfer techniques to generate new textures\nfor the generated scenes. We believe our system would facilitate data\naugmentation to expand the currently limited 3D geometry datasets commonly used\nin 3D computer vision, generative design, and general 3D deep learning tasks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 19:09:38 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Keshavarzi", "Mohammad", ""], ["Afolabi", "Oladapo", ""], ["Caldas", "Luisa", ""], ["Yang", "Allen Y.", ""], ["Zakhor", "Avideh", ""]]}, {"id": "2012.04012", "submitter": "Timo Bolkart", "authors": "Yao Feng and Haiwen Feng and Michael J. Black and Timo Bolkart", "title": "Learning an Animatable Detailed 3D Face Model from In-The-Wild Images", "comments": "SIGGRAPH 2021", "journal-ref": "ACM Transactions on Graphics (ToG), Vol. 40, No. 4, Article 88.\n  Publication date: August 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While current monocular 3D face reconstruction methods can recover fine\ngeometric details, they suffer several limitations. Some methods produce faces\nthat cannot be realistically animated because they do not model how wrinkles\nvary with expression. Other methods are trained on high-quality face scans and\ndo not generalize well to in-the-wild images. We present the first approach\nthat regresses 3D face shape and animatable details that are specific to an\nindividual but change with expression. Our model, DECA (Detailed Expression\nCapture and Animation), is trained to robustly produce a UV displacement map\nfrom a low-dimensional latent representation that consists of person-specific\ndetail parameters and generic expression parameters, while a regressor is\ntrained to predict detail, shape, albedo, expression, pose and illumination\nparameters from a single image. To enable this, we introduce a novel\ndetail-consistency loss that disentangles person-specific details from\nexpression-dependent wrinkles. This disentanglement allows us to synthesize\nrealistic person-specific wrinkles by controlling expression parameters while\nkeeping person-specific details unchanged. DECA is learned from in-the-wild\nimages with no paired 3D supervision and achieves state-of-the-art shape\nreconstruction accuracy on two benchmarks. Qualitative results on in-the-wild\ndata demonstrate DECA's robustness and its ability to disentangle identity- and\nexpression-dependent details enabling animation of reconstructed faces. The\nmodel and code are publicly available at https://deca.is.tue.mpg.de.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 19:30:45 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 17:52:42 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Feng", "Yao", ""], ["Feng", "Haiwen", ""], ["Black", "Michael J.", ""], ["Bolkart", "Timo", ""]]}, {"id": "2012.04027", "submitter": "Arantxa Casanova", "authors": "Arantxa Casanova, Michal Drozdzal, Adriana Romero-Soriano", "title": "Generating unseen complex scenes: are we there yet?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although recent complex scene conditional generation models generate\nincreasingly appealing scenes, it is very hard to assess which models perform\nbetter and why. This is often due to models being trained to fit different data\nsplits, and defining their own experimental setups. In this paper, we propose a\nmethodology to compare complex scene conditional generation models, and provide\nan in-depth analysis that assesses the ability of each model to (1) fit the\ntraining distribution and hence perform well on seen conditionings, (2) to\ngeneralize to unseen conditionings composed of seen object combinations, and\n(3) generalize to unseen conditionings composed of unseen object combinations.\nAs a result, we observe that recent methods are able to generate recognizable\nscenes given seen conditionings, and exploit compositionality to generalize to\nunseen conditionings with seen object combinations. However, all methods suffer\nfrom noticeable image quality degradation when asked to generate images from\nconditionings composed of unseen object combinations. Moreover, through our\nanalysis, we identify the advantages of different pipeline components, and find\nthat (1) encouraging compositionality through instance-wise spatial\nconditioning normalizations increases robustness to both types of unseen\nconditionings, (2) using semantically aware losses such as the scene-graph\nperceptual similarity helps improve some dimensions of the generation process,\nand (3) enhancing the quality of generated masks and the quality of the\nindividual objects are crucial steps to improve robustness to both types of\nunseen conditionings.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 20:04:39 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Casanova", "Arantxa", ""], ["Drozdzal", "Michal", ""], ["Romero-Soriano", "Adriana", ""]]}, {"id": "2012.04048", "submitter": "Hugues Thomas", "authors": "Hugues Thomas", "title": "Rotation-Invariant Point Convolution With Multiple Equivariant\n  Alignments", "comments": "3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent attempts at introducing rotation invariance or equivariance in 3D deep\nlearning approaches have shown promising results, but these methods still\nstruggle to reach the performances of standard 3D neural networks. In this work\nwe study the relation between equivariance and invariance in 3D point\nconvolutions. We show that using rotation-equivariant alignments, it is\npossible to make any convolutional layer rotation-invariant. Furthermore, we\nimprove this simple alignment procedure by using the alignment themselves as\nfeatures in the convolution, and by combining multiple alignments together.\nWith this core layer, we design rotation-invariant architectures which improve\nstate-of-the-art results in both object classification and semantic\nsegmentation and reduces the gap between rotation-invariant and standard 3D\ndeep learning approaches.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 20:47:46 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Thomas", "Hugues", ""]]}, {"id": "2012.04060", "submitter": "Andrey Kurenkov", "authors": "Andrey Kurenkov, Roberto Mart\\'in-Mart\\'in, Jeff Ichnowski, Ken\n  Goldberg, Silvio Savarese", "title": "Semantic and Geometric Modeling with Neural Message Passing in 3D Scene\n  Graphs for Hierarchical Mechanical Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Searching for objects in indoor organized environments such as homes or\noffices is part of our everyday activities. When looking for a target object,\nwe jointly reason about the rooms and containers the object is likely to be in;\nthe same type of container will have a different probability of having the\ntarget depending on the room it is in. We also combine geometric and semantic\ninformation to infer what container is best to search, or what other objects\nare best to move, if the target object is hidden from view. We propose to use a\n3D scene graph representation to capture the hierarchical, semantic, and\ngeometric aspects of this problem. To exploit this representation in a search\nprocess, we introduce Hierarchical Mechanical Search (HMS), a method that\nguides an agent's actions towards finding a target object specified with a\nnatural language description. HMS is based on a novel neural network\narchitecture that uses neural message passing of vectors with visual,\ngeometric, and linguistic information to allow HMS to reason across layers of\nthe graph while combining semantic and geometric cues. HMS is evaluated on a\nnovel dataset of 500 3D scene graphs with dense placements of semantically\nrelated objects in storage locations, and is shown to be significantly better\nthan several baselines at finding objects and close to the oracle policy in\nterms of the median number of actions required. Additional qualitative results\ncan be found at https://ai.stanford.edu/mech-search/hms.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 21:04:34 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 20:08:59 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Kurenkov", "Andrey", ""], ["Mart\u00edn-Mart\u00edn", "Roberto", ""], ["Ichnowski", "Jeff", ""], ["Goldberg", "Ken", ""], ["Savarese", "Silvio", ""]]}, {"id": "2012.04066", "submitter": "Yirui Wang", "authors": "Xinyu Zhang, Yirui Wang, Chi-Tung Cheng, Le Lu, Adam P. Harrison, Jing\n  Xiao, Chien-Hung Liao, Shun Miao", "title": "A New Window Loss Function for Bone Fracture Detection and Localization\n  in X-ray Images with Point-based Annotation", "comments": "Accepted to AAAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object detection methods are widely adopted for computer-aided diagnosis\nusing medical images. Anomalous findings are usually treated as objects that\nare described by bounding boxes. Yet, many pathological findings, e.g., bone\nfractures, cannot be clearly defined by bounding boxes, owing to considerable\ninstance, shape and boundary ambiguities. This makes bounding box annotations,\nand their associated losses, highly ill-suited. In this work, we propose a new\nbone fracture detection method for X-ray images, based on a labor effective and\nflexible annotation scheme suitable for abnormal findings with no clear\nobject-level spatial extents or boundaries. Our method employs a simple,\nintuitive, and informative point-based annotation protocol to mark localized\npathology information. To address the uncertainty in the fracture scales\nannotated via point(s), we convert the annotations into pixel-wise supervision\nthat uses lower and upper bounds with positive, negative, and uncertain\nregions. A novel Window Loss is subsequently proposed to only penalize the\npredictions outside of the uncertain regions. Our method has been extensively\nevaluated on 4410 pelvic X-ray images of unique patients. Experiments\ndemonstrate that our method outperforms previous state-of-the-art image\nclassification and object detection baselines by healthy margins, with an AUROC\nof 0.983 and FROC score of 89.6%.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 21:19:04 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 15:55:50 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhang", "Xinyu", ""], ["Wang", "Yirui", ""], ["Cheng", "Chi-Tung", ""], ["Lu", "Le", ""], ["Harrison", "Adam P.", ""], ["Xiao", "Jing", ""], ["Liao", "Chien-Hung", ""], ["Miao", "Shun", ""]]}, {"id": "2012.04109", "submitter": "Xuan Gong", "authors": "Xuan Gong, Xin Xia, Wentao Zhu, Baochang Zhang, David Doermann, Lian\n  Zhuo", "title": "Deformable Gabor Feature Networks for Biomedical Image Classification", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning has dominated progress in the field of medical\nimage analysis. We find however, that the ability of current deep learning\napproaches to represent the complex geometric structures of many medical images\nis insufficient. One limitation is that deep learning models require a\ntremendous amount of data, and it is very difficult to obtain a sufficient\namount with the necessary detail. A second limitation is that there are\nunderlying features of these medical images that are well established, but the\nblack-box nature of existing convolutional neural networks (CNNs) do not allow\nus to exploit them. In this paper, we revisit Gabor filters and introduce a\ndeformable Gabor convolution (DGConv) to expand deep networks interpretability\nand enable complex spatial variations. The features are learned at deformable\nsampling locations with adaptive Gabor convolutions to improve\nrepresentativeness and robustness to complex objects. The DGConv replaces\nstandard convolutional layers and is easily trained end-to-end, resulting in\ndeformable Gabor feature network (DGFN) with few additional parameters and\nminimal additional training cost. We introduce DGFN for addressing deep\nmulti-instance multi-label classification on the INbreast dataset for\nmammograms and on the ChestX-ray14 dataset for pulmonary x-ray images.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 23:25:32 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Gong", "Xuan", ""], ["Xia", "Xin", ""], ["Zhu", "Wentao", ""], ["Zhang", "Baochang", ""], ["Doermann", "David", ""], ["Zhuo", "Lian", ""]]}, {"id": "2012.04111", "submitter": "Yu Yin", "authors": "Yu Yin, Joseph P. Robinson, Songyao Jiang, Yue Bai, Can Qin, Yun Fu", "title": "SuperFront: From Low-resolution to High-resolution Frontal Face\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in face rotation, along with other face-based generative tasks, are\nmore frequent as we advance further in topics of deep learning. Even as\nimpressive milestones are achieved in synthesizing faces, the importance of\npreserving identity is needed in practice and should not be overlooked. Also,\nthe difficulty should not be more for data with obscured faces, heavier poses,\nand lower quality. Existing methods tend to focus on samples with variation in\npose, but with the assumption data is high in quality. We propose a generative\nadversarial network (GAN) -based model to generate high-quality, identity\npreserving frontal faces from one or multiple low-resolution (LR) faces with\nextreme poses. Specifically, we propose SuperFront-GAN (SF-GAN) to synthesize a\nhigh-resolution (HR), frontal face from one-to-many LR faces with various poses\nand with the identity-preserved. We integrate a super-resolution (SR) side-view\nmodule into SF-GAN to preserve identity information and fine details of the\nside-views in HR space, which helps model reconstruct high-frequency\ninformation of faces (i.e., periocular, nose, and mouth regions). Moreover,\nSF-GAN accepts multiple LR faces as input, and improves each added sample. We\nsqueeze additional gain in performance with an orthogonal constraint in the\ngenerator to penalize redundant latent representations and, hence, diversify\nthe learned features space. Quantitative and qualitative results demonstrate\nthe superiority of SF-GAN over others.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 23:30:28 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Yin", "Yu", ""], ["Robinson", "Joseph P.", ""], ["Jiang", "Songyao", ""], ["Bai", "Yue", ""], ["Qin", "Can", ""], ["Fu", "Yun", ""]]}, {"id": "2012.04112", "submitter": "Gil Ben-Artzi", "authors": "Michael Klyuchka, Evgeny Hershkovitch Neiterman, Gil Ben-Artzi", "title": "CEL-Net: Continuous Exposure for Extreme Low-Light Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning methods for enhancing dark images learn a mapping from input\nimages to output images with pre-determined discrete exposure levels. Often, at\ninference time the input and optimal output exposure levels of the given image\nare different from the seen ones during training. As a result the enhanced\nimage might suffer from visual distortions, such as low contrast or dark areas.\nWe address this issue by introducing a deep learning model that can\ncontinuously generalize at inference time to unseen exposure levels without the\nneed to retrain the model. To this end, we introduce a dataset of 1500 raw\nimages captured in both outdoor and indoor scenes, with five different exposure\nlevels and various camera parameters. Using the dataset, we develop a model for\nextreme low-light imaging that can continuously tune the input or output\nexposure level of the image to an unseen one. We investigate the properties of\nour model and validate its performance, showing promising results.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 23:31:59 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Klyuchka", "Michael", ""], ["Neiterman", "Evgeny Hershkovitch", ""], ["Ben-Artzi", "Gil", ""]]}, {"id": "2012.04124", "submitter": "Sangho Lee", "authors": "Sangho Lee, Youngjae Yu, Gunhee Kim, Thomas Breuel, Jan Kautz, Yale\n  Song", "title": "Parameter Efficient Multimodal Transformers for Video Representation\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent success of Transformers in the language domain has motivated\nadapting it to a multimodal setting, where a new visual model is trained in\ntandem with an already pretrained language model. However, due to the excessive\nmemory requirements from Transformers, existing work typically fixes the\nlanguage model and train only the vision module, which limits its ability to\nlearn cross-modal information in an end-to-end manner. In this work, we focus\non reducing the parameters of multimodal Transformers in the context of\naudio-visual video representation learning. We alleviate the high memory\nrequirement by sharing the weights of Transformers across layers and\nmodalities; we decompose the Transformer into modality-specific and\nmodality-shared parts so that the model learns the dynamics of each modality\nboth individually and together, and propose a novel parameter sharing scheme\nbased on low-rank approximation. We show that our approach reduces parameters\nup to 80$\\%$, allowing us to train our model end-to-end from scratch. We also\npropose a negative sampling approach based on an instance similarity measured\non the CNN embedding space that our model learns with the Transformers. To\ndemonstrate our approach, we pretrain our model on 30-second clips from\nKinetics-700 and transfer it to audio-visual classification tasks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 00:16:13 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Lee", "Sangho", ""], ["Yu", "Youngjae", ""], ["Kim", "Gunhee", ""], ["Breuel", "Thomas", ""], ["Kautz", "Jan", ""], ["Song", "Yale", ""]]}, {"id": "2012.04132", "submitter": "Neehar Kondapaneni", "authors": "Neehar Kondapaneni, Pietro Perona", "title": "A Number Sense as an Emergent Property of the Manipulating Brain", "comments": "15 pages, 6 figures, 8 supplemental figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to understand and manipulate numbers and quantities emerges\nduring childhood, but the mechanism through which this ability is developed is\nstill poorly understood. In particular, it is not known whether acquiring such\na {\\em number sense} is possible without supervision from a teacher.\n  To explore this question, we propose a model in which spontaneous and\nundirected manipulation of small objects trains perception to predict the\nresulting scene changes. We find that, from this task, an image representation\nemerges that exhibits regularities that foreshadow numbers and quantity. These\ninclude distinct categories for zero and the first few natural numbers, a\nnotion of order, and a signal that correlates with numerical quantity. As a\nresult, our model acquires the ability to estimate the number of objects in the\nscene, as well as {\\em subitization}, i.e. the ability to recognize at a glance\nthe exact number of objects in small scenes. We conclude that important aspects\nof a facility with numbers and quantities may be learned without explicit\nteacher supervision.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 00:37:35 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 09:51:43 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Kondapaneni", "Neehar", ""], ["Perona", "Pietro", ""]]}, {"id": "2012.04135", "submitter": "Shuvo Kumar Paul", "authors": "Shuvo Kumar Paul, Pourya Hoseini, Mircea Nicolescu and Monica\n  Nicolescu", "title": "Performance Analysis of Keypoint Detectors and Binary Descriptors Under\n  Varying Degrees of Photometric and Geometric Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Detecting image correspondences by feature matching forms the basis of\nnumerous computer vision applications. Several detectors and descriptors have\nbeen presented in the past, addressing the efficient generation of features\nfrom interest points (keypoints) in an image. In this paper, we investigate\neight binary descriptors (AKAZE, BoostDesc, BRIEF, BRISK, FREAK, LATCH, LUCID,\nand ORB) and eight interest point detector (AGAST, AKAZE, BRISK, FAST,\nHarrisLapalce, KAZE, ORB, and StarDetector). We have decoupled the detection\nand description phase to analyze the interest point detectors and then evaluate\nthe performance of the pairwise combination of different detectors and\ndescriptors. We conducted experiments on a standard dataset and analyzed the\ncomparative performance of each method under different image transformations.\nWe observed that: (1) the FAST, AGAST, ORB detectors were faster and detected\nmore keypoints, (2) the AKAZE and KAZE detectors performed better under\nphotometric changes while ORB was more robust against geometric changes, (3) in\ngeneral, descriptors performed better when paired with the KAZE and AKAZE\ndetectors, (4) the BRIEF, LUCID, ORB descriptors were relatively faster, and\n(5) none of the descriptors did particularly well under geometric\ntransformations, only BRISK, FREAK, and AKAZE showed reasonable resiliency.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 00:44:36 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Paul", "Shuvo Kumar", ""], ["Hoseini", "Pourya", ""], ["Nicolescu", "Mircea", ""], ["Nicolescu", "Monica", ""]]}, {"id": "2012.04150", "submitter": "Qi Ming", "authors": "Qi Ming, Zhiqiang Zhou, Lingjuan Miao, Hongwei Zhang, Linhao Li", "title": "Dynamic Anchor Learning for Arbitrary-Oriented Object Detection", "comments": "Accepted to AAAI 2021. The code and models are available at\n  https://github.com/ming71/DAL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arbitrary-oriented objects widely appear in natural scenes, aerial\nphotographs, remote sensing images, etc., thus arbitrary-oriented object\ndetection has received considerable attention. Many current rotation detectors\nuse plenty of anchors with different orientations to achieve spatial alignment\nwith ground truth boxes, then Intersection-over-Union (IoU) is applied to\nsample the positive and negative candidates for training. However, we observe\nthat the selected positive anchors cannot always ensure accurate detections\nafter regression, while some negative samples can achieve accurate\nlocalization. It indicates that the quality assessment of anchors through IoU\nis not appropriate, and this further lead to inconsistency between\nclassification confidence and localization accuracy. In this paper, we propose\na dynamic anchor learning (DAL) method, which utilizes the newly defined\nmatching degree to comprehensively evaluate the localization potential of the\nanchors and carry out a more efficient label assignment process. In this way,\nthe detector can dynamically select high-quality anchors to achieve accurate\nobject detection, and the divergence between classification and regression will\nbe alleviated. With the newly introduced DAL, we achieve superior detection\nperformance for arbitrary-oriented objects with only a few horizontal preset\nanchors. Experimental results on three remote sensing datasets HRSC2016, DOTA,\nUCAS-AOD as well as a scene text dataset ICDAR 2015 show that our method\nachieves substantial improvement compared with the baseline model. Besides, our\napproach is also universal for object detection using horizontal bound box. The\ncode and models are available at https://github.com/ming71/DAL.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 01:30:06 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 13:18:28 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Ming", "Qi", ""], ["Zhou", "Zhiqiang", ""], ["Miao", "Lingjuan", ""], ["Zhang", "Hongwei", ""], ["Li", "Linhao", ""]]}, {"id": "2012.04153", "submitter": "Bernadette Bucher", "authors": "Sadat Shaik, Bernadette Bucher, Nephele Agrafiotis, Stephen Phillips,\n  Kostas Daniilidis, William Schmenner", "title": "Learning Portrait Style Representations", "comments": "Sadat Shaik and Bernadette Bucher contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Style analysis of artwork in computer vision predominantly focuses on\nachieving results in target image generation through optimizing understanding\nof low level style characteristics such as brush strokes. However,\nfundamentally different techniques are required to computationally understand\nand control qualities of art which incorporate higher level style\ncharacteristics. We study style representations learned by neural network\narchitectures incorporating these higher level characteristics. We find\nvariation in learned style features from incorporating triplets annotated by\nart historians as supervision for style similarity. Networks leveraging\nstatistical priors or pretrained on photo collections such as ImageNet can also\nderive useful visual representations of artwork. We align the impact of these\nexpert human knowledge, statistical, and photo realism priors on style\nrepresentations with art historical research and use these representations to\nperform zero-shot classification of artists. To facilitate this work, we also\npresent the first large-scale dataset of portraits prepared for computational\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 01:36:45 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Shaik", "Sadat", ""], ["Bucher", "Bernadette", ""], ["Agrafiotis", "Nephele", ""], ["Phillips", "Stephen", ""], ["Daniilidis", "Kostas", ""], ["Schmenner", "William", ""]]}, {"id": "2012.04164", "submitter": "Junyu Gao", "authors": "Junyu Gao, Tao Han, Yuan Yuan, Qi Wang", "title": "Learning Independent Instance Maps for Crowd Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurately locating each head's position in the crowd scenes is a crucial\ntask in the field of crowd analysis. However, traditional density-based methods\nonly predict coarse prediction, and segmentation/detection-based methods cannot\nhandle extremely dense scenes and large-range scale-variations crowds. To this\nend, we propose an end-to-end and straightforward framework for crowd\nlocalization, named Independent Instance Map segmentation (IIM). Different from\ndensity maps and boxes regression, each instance in IIM is non-overlapped. By\nsegmenting crowds into independent connected components, the positions and the\ncrowd counts (the centers and the number of components, respectively) are\nobtained. Furthermore, to improve the segmentation quality for different\ndensity regions, we present a differentiable Binarization Module (BM) to output\nstructured instance maps. BM brings two advantages into localization models: 1)\nadaptively learn a threshold map for different images to detect each instance\nmore accurately; 2) directly train the model using loss on binary predictions\nand labels. Extensive experiments verify the proposed method is effective and\noutperforms the-state-of-the-art methods on the five popular crowd datasets.\nSignificantly, IIM improves F1-measure by 10.4\\% on the NWPU-Crowd Localization\ntask. The source code and pre-trained models will be released at\n\\url{https://github.com/taohan10200/IIM}.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 02:17:19 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 03:20:09 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Gao", "Junyu", ""], ["Han", "Tao", ""], ["Yuan", "Yuan", ""], ["Wang", "Qi", ""]]}, {"id": "2012.04170", "submitter": "Jiahua Dong", "authors": "Jiahua Dong, Yang Cong, Gan Sun, Yunsheng Yang, Xiaowei Xu and\n  Zhengming Ding", "title": "Weakly-Supervised Cross-Domain Adaptation for Endoscopic Lesions\n  Segmentation", "comments": "Accepted to IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised learning has attracted growing research attention on\nmedical lesions segmentation due to significant saving in pixel-level\nannotation cost. However, 1) most existing methods require effective prior and\nconstraints to explore the intrinsic lesions characterization, which only\ngenerates incorrect and rough prediction; 2) they neglect the underlying\nsemantic dependencies among weakly-labeled target enteroscopy diseases and\nfully-annotated source gastroscope lesions, while forcefully utilizing\nuntransferable dependencies leads to the negative performance. To tackle above\nissues, we propose a new weakly-supervised lesions transfer framework, which\ncan not only explore transferable domain-invariant knowledge across different\ndatasets, but also prevent the negative transfer of untransferable\nrepresentations. Specifically, a Wasserstein quantified transferability\nframework is developed to highlight widerange transferable contextual\ndependencies, while neglecting the irrelevant semantic characterizations.\nMoreover, a novel selfsupervised pseudo label generator is designed to equally\nprovide confident pseudo pixel labels for both hard-to-transfer and\neasyto-transfer target samples. It inhibits the enormous deviation of false\npseudo pixel labels under the self-supervision manner. Afterwards,\ndynamically-searched feature centroids are aligned to narrow category-wise\ndistribution shift. Comprehensive theoretical analysis and experiments show the\nsuperiority of our model on the endoscopic dataset and several public datasets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 02:26:03 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Dong", "Jiahua", ""], ["Cong", "Yang", ""], ["Sun", "Gan", ""], ["Yang", "Yunsheng", ""], ["Xu", "Xiaowei", ""], ["Ding", "Zhengming", ""]]}, {"id": "2012.04176", "submitter": "Pengyu Zhang", "authors": "Pengyu Zhang and Dong Wang and Huchuan Lu", "title": "Multi-modal Visual Tracking: Review and Experimental Comparison", "comments": "39 pages, 9 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual object tracking, as a fundamental task in computer vision, has drawn\nmuch attention in recent years. To extend trackers to a wider range of\napplications, researchers have introduced information from multiple modalities\nto handle specific scenes, which is a promising research prospect with emerging\nmethods and benchmarks. To provide a thorough review of multi-modal track-ing,\nwe summarize the multi-modal tracking algorithms, especially visible-depth\n(RGB-D) tracking and visible-thermal (RGB-T) tracking in a unified taxonomy\nfrom different aspects. Second, we provide a detailed description of the\nrelated benchmarks and challenges. Furthermore, we conduct extensive\nexperiments to analyze the effectiveness of trackers on five datasets: PTB,\nVOT19-RGBD, GTOT, RGBT234, and VOT19-RGBT. Finally, we discuss various future\ndirections from different perspectives, including model design and dataset\nconstruction for further research.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 02:39:38 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Zhang", "Pengyu", ""], ["Wang", "Dong", ""], ["Lu", "Huchuan", ""]]}, {"id": "2012.04196", "submitter": "Swetava Ganguli", "authors": "Xuerong Xiao, Swetava Ganguli, Vipul Pandey", "title": "VAE-Info-cGAN: Generating Synthetic Images by Combining Pixel-level and\n  Feature-level Geospatial Conditional Inputs", "comments": "10 pages, 4 figures, Peer-reviewed and accepted version of the paper\n  published at the 13th ACM SIGSPATIAL International Workshop on Computational\n  Transportation Science (IWCTS 2020)", "journal-ref": "In Proceedings of the 13th ACM SIGSPATIAL International Workshop\n  on Computational Transportation Science, Article No. 1, Pages 1-10, November\n  03, 2020, Seattle, WA, USA", "doi": "10.1145/3423457.3429361", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Training robust supervised deep learning models for many geospatial\napplications of computer vision is difficult due to dearth of class-balanced\nand diverse training data. Conversely, obtaining enough training data for many\napplications is financially prohibitive or may be infeasible, especially when\nthe application involves modeling rare or extreme events. Synthetically\ngenerating data (and labels) using a generative model that can sample from a\ntarget distribution and exploit the multi-scale nature of images can be an\ninexpensive solution to address scarcity of labeled data. Towards this goal, we\npresent a deep conditional generative model, called VAE-Info-cGAN, that\ncombines a Variational Autoencoder (VAE) with a conditional Information\nMaximizing Generative Adversarial Network (InfoGAN), for synthesizing\nsemantically rich images simultaneously conditioned on a pixel-level condition\n(PLC) and a macroscopic feature-level condition (FLC). Dimensionally, the PLC\ncan only vary in the channel dimension from the synthesized image and is meant\nto be a task-specific input. The FLC is modeled as an attribute vector in the\nlatent space of the generated image which controls the contributions of various\ncharacteristic attributes germane to the target distribution. An interpretation\nof the attribute vector to systematically generate synthetic images by varying\na chosen binary macroscopic feature is explored. Experiments on a GPS\ntrajectories dataset show that the proposed model can accurately generate\nvarious forms of spatio-temporal aggregates across different geographic\nlocations while conditioned only on a raster representation of the road\nnetwork. The primary intended application of the VAE-Info-cGAN is synthetic\ndata (and label) generation for targeted data augmentation for computer\nvision-based modeling of problems relevant to geospatial analysis and remote\nsensing.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 03:46:19 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Xiao", "Xuerong", ""], ["Ganguli", "Swetava", ""], ["Pandey", "Vipul", ""]]}, {"id": "2012.04199", "submitter": "Ivan Kukanov", "authors": "Ivan Kukanov, Janne Karttunen, Hannu Sillanp\\\"a\\\"a, Ville Hautam\\\"aki", "title": "Cost Sensitive Optimization of Deepfake Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the invention of cinema, the manipulated videos have existed. But\ngenerating manipulated videos that can fool the viewer has been a\ntime-consuming endeavor. With the dramatic improvements in the deep generative\nmodeling, generating believable looking fake videos has become a reality. In\nthe present work, we concentrate on the so-called deepfake videos, where the\nsource face is swapped with the targets. We argue that deepfake detection task\nshould be viewed as a screening task, where the user, such as the video\nstreaming platform, will screen a large number of videos daily. It is clear\nthen that only a small fraction of the uploaded videos are deepfakes, so the\ndetection performance needs to be measured in a cost-sensitive way. Preferably,\nthe model parameters also need to be estimated in the same way. This is\nprecisely what we propose here.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 04:06:02 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Kukanov", "Ivan", ""], ["Karttunen", "Janne", ""], ["Sillanp\u00e4\u00e4", "Hannu", ""], ["Hautam\u00e4ki", "Ville", ""]]}, {"id": "2012.04207", "submitter": "Sosuke Kobayashi", "authors": "Sosuke Kobayashi, Sho Yokoi, Jun Suzuki, Kentaro Inui", "title": "Efficient Estimation of Influence of a Training Instance", "comments": "This is an extended version of the paper presented at SustaiNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the influence of a training instance on a neural network model\nleads to improving interpretability. However, it is difficult and inefficient\nto evaluate the influence, which shows how a model's prediction would be\nchanged if a training instance were not used. In this paper, we propose an\nefficient method for estimating the influence. Our method is inspired by\ndropout, which zero-masks a sub-network and prevents the sub-network from\nlearning each training instance. By switching between dropout masks, we can use\nsub-networks that learned or did not learn each training instance and estimate\nits influence. Through experiments with BERT and VGGNet on classification\ndatasets, we demonstrate that the proposed method can capture training\ninfluences, enhance the interpretability of error predictions, and cleanse the\ntraining dataset for improving generalization.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 04:31:38 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Kobayashi", "Sosuke", ""], ["Yokoi", "Sho", ""], ["Suzuki", "Jun", ""], ["Inui", "Kentaro", ""]]}, {"id": "2012.04222", "submitter": "Xueqing Deng", "authors": "Xueqing Deng, Yi Zhu, Yuxin Tian and Shawn Newsam", "title": "Scale Aware Adaptation for Land-Cover Classification in Remote Sensing\n  Imagery", "comments": "The open-sourced codes are available on Github:\n  https://github.com/xdeng7/scale-aware_da", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Land-cover classification using remote sensing imagery is an important Earth\nobservation task. Recently, land cover classification has benefited from the\ndevelopment of fully connected neural networks for semantic segmentation. The\nbenchmark datasets available for training deep segmentation models in remote\nsensing imagery tend to be small, however, often consisting of only a handful\nof images from a single location with a single scale. This limits the models'\nability to generalize to other datasets. Domain adaptation has been proposed to\nimprove the models' generalization but we find these approaches are not\neffective for dealing with the scale variation commonly found between remote\nsensing image collections. We therefore propose a scale aware adversarial\nlearning framework to perform joint cross-location and cross-scale land-cover\nclassification. The framework has a dual discriminator architecture with a\nstandard feature discriminator as well as a novel scale discriminator. We also\nintroduce a scale attention module which produces scale-enhanced features.\nExperimental results show that the proposed framework outperforms\nstate-of-the-art domain adaptation methods by a large margin.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 05:15:43 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Deng", "Xueqing", ""], ["Zhu", "Yi", ""], ["Tian", "Yuxin", ""], ["Newsam", "Shawn", ""]]}, {"id": "2012.04224", "submitter": "Shuyu Kong", "authors": "Shuyu Kong and You Li and Jia Wang and Amin Rezaei and Hai Zhou", "title": "KNN-enhanced Deep Learning Against Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Supervised learning on Deep Neural Networks (DNNs) is data hungry. Optimizing\nperformance of DNN in the presence of noisy labels has become of paramount\nimportance since collecting a large dataset will usually bring in noisy labels.\nInspired by the robustness of K-Nearest Neighbors (KNN) against data noise, in\nthis work, we propose to apply deep KNN for label cleanup. Our approach\nleverages DNNs for feature extraction and KNN for ground-truth label inference.\nWe iteratively train the neural network and update labels to simultaneously\nproceed towards higher label recovery rate and better classification\nperformance. Experiment results show that under the same setting, our approach\noutperforms existing label correction methods and achieves better accuracy on\nmultiple datasets, e.g.,76.78% on Clothing1M dataset.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 05:21:29 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Kong", "Shuyu", ""], ["Li", "You", ""], ["Wang", "Jia", ""], ["Rezaei", "Amin", ""], ["Zhou", "Hai", ""]]}, {"id": "2012.04226", "submitter": "Terrance Boult", "authors": "T. E. Boult, P. A. Grabowicz, D. S. Prijatelj, R. Stern, L. Holder, J.\n  Alspector, M. Jafarzadeh, T. Ahmad, A. R. Dhamija, C.Li, S. Cruz, A.\n  Shrivastava, C. Vondrick, W. J. Scheirer", "title": "A Unifying Framework for Formal Theories of Novelty:Framework, Examples\n  and Discussion", "comments": "Extended version/preprint of a AAAI 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managing inputs that are novel, unknown, or out-of-distribution is critical\nas an agent moves from the lab to the open world. Novelty-related problems\ninclude being tolerant to novel perturbations of the normal input, detecting\nwhen the input includes novel items, and adapting to novel inputs. While\nsignificant research has been undertaken in these areas, a noticeable gap\nexists in the lack of a formalized definition of novelty that transcends\nproblem domains. As a team of researchers spanning multiple research groups and\ndifferent domains, we have seen, first hand, the difficulties that arise from\nill-specified novelty problems, as well as inconsistent definitions and\nterminology. Therefore, we present the first unified framework for formal\ntheories of novelty and use the framework to formally define a family of\nnovelty types. Our framework can be applied across a wide range of domains,\nfrom symbolic AI to reinforcement learning, and beyond to open world image\nrecognition. Thus, it can be used to help kick-start new research efforts and\naccelerate ongoing work on these important novelty-related problems. This\nextended version of our AAAI 2021 paper included more details and examples in\nmultiple domains.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 05:24:51 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Boult", "T. E.", ""], ["Grabowicz", "P. A.", ""], ["Prijatelj", "D. S.", ""], ["Stern", "R.", ""], ["Holder", "L.", ""], ["Alspector", "J.", ""], ["Jafarzadeh", "M.", ""], ["Ahmad", "T.", ""], ["Dhamija", "A. R.", ""], ["Li", "C.", ""], ["Cruz", "S.", ""], ["Shrivastava", "A.", ""], ["Vondrick", "C.", ""], ["Scheirer", "W. J.", ""]]}, {"id": "2012.04242", "submitter": "Yejin Kim", "authors": "Yejin Kim and Manri Cheon and Junwoo Lee", "title": "Texture Transform Attention for Realistic Image Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, the performance of inpainting to fill missing\nregions has shown significant improvements by using deep neural networks. Most\nof inpainting work create a visually plausible structure and texture, however,\ndue to them often generating a blurry result, final outcomes appear unrealistic\nand make feel heterogeneity. In order to solve this problem, the existing\nmethods have used a patch based solution with deep neural network, however,\nthese methods also cannot transfer the texture properly. Motivated by these\nobservation, we propose a patch based method. Texture Transform Attention\nnetwork(TTA-Net) that better produces the missing region inpainting with fine\ndetails. The task is a single refinement network and takes the form of U-Net\narchitecture that transfers fine texture features of encoder to coarse semantic\nfeatures of decoder through skip-connection. Texture Transform Attention is\nused to create a new reassembled texture map using fine textures and coarse\nsemantics that can efficiently transfer texture information as a result. To\nstabilize training process, we use a VGG feature layer of ground truth and\npatch discriminator. We evaluate our model end-to-end with the publicly\navailable datasets CelebA-HQ and Places2 and demonstrate that images of higher\nquality can be obtained to the existing state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 06:28:51 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Kim", "Yejin", ""], ["Cheon", "Manri", ""], ["Lee", "Junwoo", ""]]}, {"id": "2012.04251", "submitter": "HyeongJoo Hwang", "authors": "HyeongJoo Hwang, Geon-Hyeong Kim, Seunghoon Hong, Kee-Eung Kim", "title": "Variational Interaction Information Maximization for Cross-domain\n  Disentanglement", "comments": "Published at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cross-domain disentanglement is the problem of learning representations\npartitioned into domain-invariant and domain-specific representations, which is\na key to successful domain transfer or measuring semantic distance between two\ndomains. Grounded in information theory, we cast the simultaneous learning of\ndomain-invariant and domain-specific representations as a joint objective of\nmultiple information constraints, which does not require adversarial training\nor gradient reversal layers. We derive a tractable bound of the objective and\npropose a generative model named Interaction Information Auto-Encoder (IIAE).\nOur approach reveals insights on the desirable representation for cross-domain\ndisentanglement and its connection to Variational Auto-Encoder (VAE). We\ndemonstrate the validity of our model in the image-to-image translation and the\ncross-domain retrieval tasks. We further show that our model achieves the\nstate-of-the-art performance in the zero-shot sketch based image retrieval\ntask, even without external knowledge. Our implementation is publicly available\nat: https://github.com/gr8joo/IIAE\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 07:11:35 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Hwang", "HyeongJoo", ""], ["Kim", "Geon-Hyeong", ""], ["Hong", "Seunghoon", ""], ["Kim", "Kee-Eung", ""]]}, {"id": "2012.04256", "submitter": "Nupur Kumari", "authors": "Puneet Mangla, Nupur Kumari, Mayank Singh, Vineeth N Balasubramanian,\n  Balaji Krishnamurthy", "title": "Data Instance Prior for Transfer Learning in GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in generative adversarial networks (GANs) have shown\nremarkable progress in generating high-quality images. However, this gain in\nperformance depends on the availability of a large amount of training data. In\nlimited data regimes, training typically diverges, and therefore the generated\nsamples are of low quality and lack diversity. Previous works have addressed\ntraining in low data setting by leveraging transfer learning and data\naugmentation techniques. We propose a novel transfer learning method for GANs\nin the limited data domain by leveraging informative data prior derived from\nself-supervised/supervised pre-trained networks trained on a diverse source\ndomain. We perform experiments on several standard vision datasets using\nvarious GAN architectures (BigGAN, SNGAN, StyleGAN2) to demonstrate that the\nproposed method effectively transfers knowledge to domains with few target\nimages, outperforming existing state-of-the-art techniques in terms of image\nquality and diversity. We also show the utility of data instance prior in\nlarge-scale unconditional image generation and image editing tasks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 07:40:30 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Mangla", "Puneet", ""], ["Kumari", "Nupur", ""], ["Singh", "Mayank", ""], ["Balasubramanian", "Vineeth N", ""], ["Krishnamurthy", "Balaji", ""]]}, {"id": "2012.04262", "submitter": "Shao-Yuan Lo", "authors": "Shao-Yuan Lo, Jeya Maria Jose Valanarasu, Vishal M. Patel", "title": "Overcomplete Representations Against Adversarial Videos", "comments": "Accepted at IEEE International Conference on Image Processing (ICIP)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial robustness of deep neural networks is an extensively studied\nproblem in the literature and various methods have been proposed to defend\nagainst adversarial images. However, only a handful of defense methods have\nbeen developed for defending against attacked videos. In this paper, we propose\na novel Over-and-Under complete restoration network for Defending against\nadversarial videos (OUDefend). Most restoration networks adopt an\nencoder-decoder architecture that first shrinks spatial dimension then expands\nit back. This approach learns undercomplete representations, which have large\nreceptive fields to collect global information but overlooks local details. On\nthe other hand, overcomplete representations have opposite properties. Hence,\nOUDefend is designed to balance local and global features by learning those two\nrepresentations. We attach OUDefend to target video recognition models as a\nfeature restoration block and train the entire network end-to-end. Experimental\nresults show that the defenses focusing on images may be ineffective to videos,\nwhile OUDefend enhances robustness against different types of adversarial\nvideos, ranging from additive attacks, multiplicative attacks to physically\nrealizable attacks. Code: https://github.com/shaoyuanlo/OUDefend\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 08:00:17 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 23:17:53 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Lo", "Shao-Yuan", ""], ["Valanarasu", "Jeya Maria Jose", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2012.04263", "submitter": "Qingnan Fan", "authors": "Yingda Yin, Qingnan Fan, Fei Xia, Qihang Fang, Siyan Dong, Leonidas\n  Guibas, Baoquan Chen", "title": "Active Visual Localization in Partially Calibrated Environments", "comments": "https://www.youtube.com/watch?v=DIH-GbytCPM&feature=youtu.be", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can robustly localize themselves without a map after they get lost\nfollowing prominent visual cues or landmarks. In this work, we aim at endowing\nautonomous agents the same ability. Such ability is important in robotics\napplications yet very challenging when an agent is exposed to partially\ncalibrated environments, where camera images with accurate 6 Degree-of-Freedom\npose labels only cover part of the scene. To address the above challenge, we\nexplore using Reinforcement Learning to search for a policy to generate\nintelligent motions so as to actively localize the agent given visual\ninformation in partially calibrated environments. Our core contribution is to\nformulate the active visual localization problem as a Partially Observable\nMarkov Decision Process and propose an algorithmic framework based on Deep\nReinforcement Learning to solve it. We further propose an indoor scene dataset\nACR-6, which consists of both synthetic and real data and simulates challenging\nscenarios for active visual localization. We benchmark our algorithm against\nhandcrafted baselines for localization and demonstrate that our approach\nsignificantly outperforms them on localization success rate.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 08:00:55 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Yin", "Yingda", ""], ["Fan", "Qingnan", ""], ["Xia", "Fei", ""], ["Fang", "Qihang", ""], ["Dong", "Siyan", ""], ["Guibas", "Leonidas", ""], ["Chen", "Baoquan", ""]]}, {"id": "2012.04264", "submitter": "Chih-Hung Liang", "authors": "Chih-Hung Liang, Yu-An Chen, Yueh-Cheng Liu, Winston H. Hsu", "title": "Raw Image Deblurring", "comments": "IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based blind image deblurring plays an essential role in solving\nimage blur since all existing kernels are limited in modeling the real world\nblur. Thus far, researchers focus on powerful models to handle the deblurring\nproblem and achieve decent results. For this work, in a new aspect, we discover\nthe great opportunity for image enhancement (e.g., deblurring) directly from\nRAW images and investigate novel neural network structures benefiting RAW-based\nlearning. However, to the best of our knowledge, there is no available RAW\nimage deblurring dataset. Therefore, we built a new dataset containing both RAW\nimages and processed sRGB images and design a new model to utilize the unique\ncharacteristics of RAW images. The proposed deblurring model, trained solely\nfrom RAW images, achieves the state-of-art performance and outweighs those\ntrained on processed sRGB images. Furthermore, with fine-tuning, the proposed\nmodel, trained on our new dataset, can generalize to other sensors.\nAdditionally, by a series of experiments, we demonstrate that existing\ndeblurring models can also be improved by training on the RAW images in our new\ndataset. Ultimately, we show a new venue for further opportunities based on the\ndevised novel raw-based deblurring method and the brand-new Deblur-RAW dataset.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 08:03:09 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Liang", "Chih-Hung", ""], ["Chen", "Yu-An", ""], ["Liu", "Yueh-Cheng", ""], ["Hsu", "Winston H.", ""]]}, {"id": "2012.04265", "submitter": "Jy Feng", "authors": "Junyi Feng, Jiashen Hua, Baisheng Lai, Jianqiang Huang, Xi Li,\n  Xian-sheng Hua", "title": "Learning to Generate Content-Aware Dynamic Detectors", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model efficiency is crucial for object detection. Mostprevious works rely on\neither hand-crafted design or auto-search methods to obtain a static\narchitecture, regardless ofthe difference of inputs. In this paper, we\nintroduce a newperspective of designing efficient detectors, which is\nautomatically generating sample-adaptive model architectureon the fly. The\nproposed method is named content-aware dynamic detectors (CADDet). It first\napplies a multi-scale densely connected network with dynamic routing as the\nsupernet. Furthermore, we introduce a course-to-fine strat-egy tailored for\nobject detection to guide the learning of dynamic routing, which contains two\nmetrics: 1) dynamic global budget constraint assigns data-dependent\nexpectedbudgets for individual samples; 2) local path similarity regularization\naims to generate more diverse routing paths. With these, our method achieves\nhigher computational efficiency while maintaining good performance. To the best\nof our knowledge, our CADDet is the first work to introduce dynamic routing\nmechanism in object detection. Experiments on MS-COCO dataset demonstrate that\nCADDet achieves 1.8 higher mAP with 10% fewer FLOPs compared with vanilla\nrouting strategy. Compared with the models based upon similar building blocks,\nCADDet achieves a 42% FLOPs reduction with a competitive mAP.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 08:05:20 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Feng", "Junyi", ""], ["Hua", "Jiashen", ""], ["Lai", "Baisheng", ""], ["Huang", "Jianqiang", ""], ["Li", "Xi", ""], ["Hua", "Xian-sheng", ""]]}, {"id": "2012.04268", "submitter": "Tianyu Zhang", "authors": "Tianyu Zhang and Lingxi Xie and Longhui Wei and Zijie Zhuang and\n  Yongfei Zhang and Bo Li and Qi Tian", "title": "UnrealPerson: An Adaptive Pipeline towards Costless Person\n  Re-identification", "comments": "10 pages, 5 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main difficulty of person re-identification (ReID) lies in collecting\nannotated data and transferring the model across different domains. This paper\npresents UnrealPerson, a novel pipeline that makes full use of unreal image\ndata to decrease the costs in both the training and deployment stages. Its\nfundamental part is a system that can generate synthesized images of\nhigh-quality and from controllable distributions. Instance-level annotation\ngoes with the synthesized data and is almost free. We point out some details in\nimage synthesis that largely impact the data quality. With 3,000 IDs and\n120,000 instances, our method achieves a 38.5% rank-1 accuracy when being\ndirectly transferred to MSMT17. It almost doubles the former record using\nsynthesized data and even surpasses previous direct transfer records using real\ndata. This offers a good basis for unsupervised domain adaption, where our\npre-trained model is easily plugged into the state-of-the-art algorithms\ntowards higher accuracy. In addition, the data distribution can be flexibly\nadjusted to fit some corner ReID scenarios, which widens the application of our\npipeline. We will publish our data synthesis toolkit and synthesized data in\nhttps://github.com/FlyHighest/UnrealPerson.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 08:15:30 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 10:23:56 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Zhang", "Tianyu", ""], ["Xie", "Lingxi", ""], ["Wei", "Longhui", ""], ["Zhuang", "Zijie", ""], ["Zhang", "Yongfei", ""], ["Li", "Bo", ""], ["Tian", "Qi", ""]]}, {"id": "2012.04280", "submitter": "Hui Tang", "authors": "Hui Tang, Xiatian Zhu, Ke Chen, Kui Jia, C. L. Philip Chen", "title": "Towards Uncovering the Intrinsic Data Structures for Unsupervised Domain\n  Adaptation using Structurally Regularized Deep Clustering", "comments": "Journal extension of our preliminary CVPR conference paper, under\n  review, 16 pages, 8 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) is to learn classification models that\nmake predictions for unlabeled data on a target domain, given labeled data on a\nsource domain whose distribution diverges from the target one. Mainstream UDA\nmethods strive to learn domain-aligned features such that classifiers trained\non the source features can be readily applied to the target ones. Although\nimpressive results have been achieved, these methods have a potential risk of\ndamaging the intrinsic data structures of target discrimination, raising an\nissue of generalization particularly for UDA tasks in an inductive setting. To\naddress this issue, we are motivated by a UDA assumption of structural\nsimilarity across domains, and propose to directly uncover the intrinsic target\ndiscrimination via constrained clustering, where we constrain the clustering\nsolutions using structural source regularization that hinges on the very same\nassumption. Technically, we propose a hybrid model of Structurally Regularized\nDeep Clustering, which integrates the regularized discriminative clustering of\ntarget data with a generative one, and we thus term our method as H-SRDC. Our\nhybrid model is based on a deep clustering framework that minimizes the\nKullback-Leibler divergence between the distribution of network prediction and\nan auxiliary one, where we impose structural regularization by learning\ndomain-shared classifier and cluster centroids. By enriching the structural\nsimilarity assumption, we are able to extend H-SRDC for a pixel-level UDA task\nof semantic segmentation. We conduct extensive experiments on seven UDA\nbenchmarks of image classification and semantic segmentation. With no explicit\nfeature alignment, our proposed H-SRDC outperforms all the existing methods\nunder both the inductive and transductive settings. We make our implementation\ncodes publicly available at https://github.com/huitangtang/H-SRDC.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 08:52:00 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 03:38:39 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Tang", "Hui", ""], ["Zhu", "Xiatian", ""], ["Chen", "Ke", ""], ["Jia", "Kui", ""], ["Chen", "C. L. Philip", ""]]}, {"id": "2012.04293", "submitter": "Aykut Erdem", "authors": "Tayfun Ates, Muhammed Samil Atesoglu, Cagatay Yigit, Ilker Kesen, Mert\n  Kobas, Erkut Erdem, Aykut Erdem, Tilbe Goksun, Deniz Yuret", "title": "CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions", "comments": "Submitted to the 35th Conference on Neural Information Processing\n  Systems (NeurIPS 2021) Track on Datasets and Benchmarks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans are able to perceive, understand and reason about physical events.\nDeveloping models with similar physical understanding capabilities is a\nlong-standing goal of artificial intelligence. As a step towards this goal, in\nthis work, we introduce CRAFT, a new visual question answering dataset that\nrequires causal reasoning about physical forces and object interactions. It\ncontains 58K video and question pairs that are generated from 10K videos from\n20 different virtual environments, containing various objects in motion that\ninteract with each other and the scene. Two question categories from CRAFT\ninclude previously studied descriptive and counterfactual questions. Besides,\ninspired by the theories of force dynamics in cognitive linguistics, we\nintroduce new question categories that involve understanding the interactions\nof objects through the notions of cause, enable, and prevent. Our results\ndemonstrate that even though these tasks seem to be simple and intuitive for\nhumans, the evaluated baseline models, including existing state-of-the-art\nmethods, do not yet deal with the challenges posed in our benchmark dataset.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 09:11:32 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 10:55:23 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Ates", "Tayfun", ""], ["Atesoglu", "Muhammed Samil", ""], ["Yigit", "Cagatay", ""], ["Kesen", "Ilker", ""], ["Kobas", "Mert", ""], ["Erdem", "Erkut", ""], ["Erdem", "Aykut", ""], ["Goksun", "Tilbe", ""], ["Yuret", "Deniz", ""]]}, {"id": "2012.04298", "submitter": "Deyi Ji", "authors": "Deyi Ji, Haoran Wang, Hanzhe Hu, Weihao Gan, Wei Wu, Junjie Yan", "title": "Context-Aware Graph Convolution Network for Target Re-identification", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing re-identification methods focus on learning robust and\ndiscriminative features with deep convolution networks. However, many of them\nconsider content similarity separately and fail to utilize the context\ninformation of the query and gallery sets, e.g. probe-gallery and\ngallery-gallery relations, thus hard samples may not be well solved due to the\nlimited or even misleading information. In this paper, we present a novel\nContext-Aware Graph Convolution Network (CAGCN), where the probe-gallery\nrelations are encoded into the graph nodes and the graph edge connections are\nwell controlled by the gallery-gallery relations. In this way, hard samples can\nbe addressed with the context information flows among other easy samples during\nthe graph reasoning. Specifically, we adopt an effective hard gallery sampler\nto obtain high recall for positive samples while keeping a reasonable graph\nsize, which can also weaken the imbalanced problem in training process with low\ncomputation complexity.Experiments show that the proposed method achieves\nstate-of-the-art performance on both person and vehicle re-identification\ndatasets in a plug and play fashion with limited overhead.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 09:18:39 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 06:18:30 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 05:10:13 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Ji", "Deyi", ""], ["Wang", "Haoran", ""], ["Hu", "Hanzhe", ""], ["Gan", "Weihao", ""], ["Wu", "Wei", ""], ["Yan", "Junjie", ""]]}, {"id": "2012.04312", "submitter": "Chuan Qin", "authors": "Xinran Li, Chuan Qin, Zhenxing Qian, Heng Yao and Xinpeng Zhang", "title": "Perceptual Robust Hashing for Color Images with Canonical Correlation\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, a novel perceptual image hashing scheme for color images is\nproposed based on ring-ribbon quadtree and color vector angle. First, original\nimage is subjected to normalization and Gaussian low-pass filtering to produce\na secondary image, which is divided into a series of ring-ribbons with\ndifferent radii and the same number of pixels. Then, both textural and color\nfeatures are extracted locally and globally. Quadtree decomposition (QD) is\napplied on luminance values of the ring-ribbons to extract local textural\nfeatures, and the gray level co-occurrence matrix (GLCM) is used to extract\nglobal textural features. Local color features of significant corner points on\nouter boundaries of ring-ribbons are extracted through color vector angles\n(CVA), and color low-order moments (CLMs) is utilized to extract global color\nfeatures. Finally, two types of feature vectors are fused via canonical\ncorrelation analysis (CCA) to prodcue the final hash after scrambling. Compared\nwith direct concatenation, the CCA feature fusion method improves\nclassification performance, which better reflects overall correlation between\ntwo sets of feature vectors. Receiver operating characteristic (ROC) curve\nshows that our scheme has satisfactory performances with respect to robustness,\ndiscrimination and security, which can be effectively used in copy detection\nand content authentication.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 09:35:21 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Li", "Xinran", ""], ["Qin", "Chuan", ""], ["Qian", "Zhenxing", ""], ["Yao", "Heng", ""], ["Zhang", "Xinpeng", ""]]}, {"id": "2012.04324", "submitter": "Riccardo Volpi", "authors": "Riccardo Volpi, Diane Larlus, Gr\\'egory Rogez", "title": "Continual Adaptation of Visual Representations via Domain Randomization\n  and Meta-learning", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most standard learning approaches lead to fragile models which are prone to\ndrift when sequentially trained on samples of a different nature - the\nwell-known \"catastrophic forgetting\" issue. In particular, when a model\nconsecutively learns from different visual domains, it tends to forget the past\ndomains in favor of the most recent ones. In this context, we show that one way\nto learn models that are inherently more robust against forgetting is domain\nrandomization - for vision tasks, randomizing the current domain's distribution\nwith heavy image manipulations. Building on this result, we devise a\nmeta-learning strategy where a regularizer explicitly penalizes any loss\nassociated with transferring the model from the current domain to different\n\"auxiliary\" meta-domains, while also easing adaptation to them. Such\nmeta-domains are also generated through randomized image manipulations. We\nempirically demonstrate in a variety of experiments - spanning from\nclassification to semantic segmentation - that our approach results in models\nthat are less prone to catastrophic forgetting when transferred to new domains.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 09:54:51 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 15:58:04 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Volpi", "Riccardo", ""], ["Larlus", "Diane", ""], ["Rogez", "Gr\u00e9gory", ""]]}, {"id": "2012.04329", "submitter": "Rafael Sampaio De Rezende", "authors": "Andr\\'es Mafla and Rafael Sampaio de Rezende and Llu\\'is G\\'omez and\n  Diane Larlus and Dimosthenis Karatzas", "title": "StacMR: Scene-Text Aware Cross-Modal Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent models for cross-modal retrieval have benefited from an increasingly\nrich understanding of visual scenes, afforded by scene graphs and object\ninteractions to mention a few. This has resulted in an improved matching\nbetween the visual representation of an image and the textual representation of\nits caption. Yet, current visual representations overlook a key aspect: the\ntext appearing in images, which may contain crucial information for retrieval.\nIn this paper, we first propose a new dataset that allows exploration of\ncross-modal retrieval where images contain scene-text instances. Then, armed\nwith this dataset, we describe several approaches which leverage scene text,\nincluding a better scene-text aware cross-modal retrieval method which uses\nspecialized representations for text from the captions and text from the visual\nscene, and reconcile them in a common embedding space. Extensive experiments\nconfirm that cross-modal retrieval approaches benefit from scene text and\nhighlight interesting research questions worth exploring further. Dataset and\ncode are available at http://europe.naverlabs.com/stacmr\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 10:04:25 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Mafla", "Andr\u00e9s", ""], ["de Rezende", "Rafael Sampaio", ""], ["G\u00f3mez", "Llu\u00eds", ""], ["Larlus", "Diane", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "2012.04336", "submitter": "Bas van der Velden", "authors": "Bas H.M. van der Velden, Max A.A. Ragusi, Markus H.A. Janse, Claudette\n  E. Loo, Kenneth G.A. Gilhuijs", "title": "Interpretable deep learning regression for breast density estimation on\n  MRI", "comments": "This paper has been published as: Van der Velden, B.H.M., Ragusi,\n  M.A.A., Janse, M.H.A., Loo, C.E., Gilhuijs, K.G.A. \"Interpretable deep\n  learning regression for breast density estimation on MRI.\" Medical Imaging\n  2020: Computer-Aided Diagnosis. Vol. 11314. International Society for Optics\n  and Photonics, 2020", "journal-ref": null, "doi": "10.1117/12.2549003", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Breast density, which is the ratio between fibroglandular tissue (FGT) and\ntotal breast volume, can be assessed qualitatively by radiologists and\nquantitatively by computer algorithms. These algorithms often rely on\nsegmentation of breast and FGT volume. In this study, we propose a method to\ndirectly assess breast density on MRI, and provide interpretations of these\nassessments.\n  We assessed breast density in 506 patients with breast cancer using a\nregression convolutional neural network (CNN). The input for the CNN were\nslices of breast MRI of 128 x 128 voxels, and the output was a continuous\ndensity value between 0 (fatty breast) and 1 (dense breast). We used 350\npatients to train the CNN, 75 for validation, and 81 for independent testing.\nWe investigated why the CNN came to its predicted density using Deep SHapley\nAdditive exPlanations (SHAP).\n  The density predicted by the CNN on the testing set was significantly\ncorrelated with the ground truth densities (N = 81 patients, Spearman's rho =\n0.86, P < 0.001). When inspecting what the CNN based its predictions on, we\nfound that voxels in FGT commonly had positive SHAP-values, voxels in fatty\ntissue commonly had negative SHAP-values, and voxels in non-breast tissue\ncommonly had SHAP-values near zero. This means that the prediction of density\nis based on the structures we expect it to be based on, namely FGT and fatty\ntissue.\n  To conclude, we presented an interpretable deep learning regression method\nfor breast density estimation on MRI with promising results.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 10:23:49 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["van der Velden", "Bas H. M.", ""], ["Ragusi", "Max A. A.", ""], ["Janse", "Markus H. A.", ""], ["Loo", "Claudette E.", ""], ["Gilhuijs", "Kenneth G. A.", ""]]}, {"id": "2012.04337", "submitter": "Hwanjun Song", "authors": "Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, Jae-Gil Lee", "title": "Robust Learning by Self-Transition for Handling Noisy Labels", "comments": "Accepted at KDD 2021", "journal-ref": null, "doi": "10.1145/3447548.3467222", "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world data inevitably contains noisy labels, which induce the poor\ngeneralization of deep neural networks. It is known that the network typically\nbegins to rapidly memorize false-labeled samples after a certain point of\ntraining. Thus, to counter the label noise challenge, we propose a novel\nself-transitional learning method called MORPH, which automatically switches\nits learning phase at the transition point from seeding to evolution. In the\nseeding phase, the network is updated using all the samples to collect a seed\nof clean samples. Then, in the evolution phase, the network is updated using\nonly the set of arguably clean samples, which precisely keeps expanding by the\nupdated network. Thus, MORPH effectively avoids the overfitting to\nfalse-labeled samples throughout the entire training period. Extensive\nexperiments using five real-world or synthetic benchmark datasets demonstrate\nsubstantial improvements over state-of-the-art methods in terms of robustness\nand efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 10:25:29 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 10:29:02 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Song", "Hwanjun", ""], ["Kim", "Minseok", ""], ["Park", "Dongmin", ""], ["Shin", "Yooju", ""], ["Lee", "Jae-Gil", ""]]}, {"id": "2012.04350", "submitter": "Zhanzhan Cheng", "authors": "Liang Qiao, Ying Chen, Zhanzhan Cheng, Yunlu Xu, Yi Niu, Shiliang Pu\n  and Fei Wu", "title": "MANGO: A Mask Attention Guided One-Stage Scene Text Spotter", "comments": "Accepted to AAAI2021. The code will be published soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently end-to-end scene text spotting has become a popular research topic\ndue to its advantages of global optimization and high maintainability in real\napplications. Most methods attempt to develop various region of interest (RoI)\noperations to concatenate the detection part and the sequence recognition part\ninto a two-stage text spotting framework. However, in such framework, the\nrecognition part is highly sensitive to the detected results (\\emph{e.g.}, the\ncompactness of text contours). To address this problem, in this paper, we\npropose a novel Mask AttentioN Guided One-stage text spotting framework named\nMANGO, in which character sequences can be directly recognized without RoI\noperation. Concretely, a position-aware mask attention module is developed to\ngenerate attention weights on each text instance and its characters. It allows\ndifferent text instances in an image to be allocated on different feature map\nchannels which are further grouped as a batch of instance features. Finally, a\nlightweight sequence decoder is applied to generate the character sequences. It\nis worth noting that MANGO inherently adapts to arbitrary-shaped text spotting\nand can be trained end-to-end with only coarse position information\n(\\emph{e.g.}, rectangular bounding box) and text annotations. Experimental\nresults show that the proposed method achieves competitive and even new\nstate-of-the-art performance on both regular and irregular text spotting\nbenchmarks, i.e., ICDAR 2013, ICDAR 2015, Total-Text, and SCUT-CTW1500.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 10:47:49 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Qiao", "Liang", ""], ["Chen", "Ying", ""], ["Cheng", "Zhanzhan", ""], ["Xu", "Yunlu", ""], ["Niu", "Yi", ""], ["Pu", "Shiliang", ""], ["Wu", "Fei", ""]]}, {"id": "2012.04353", "submitter": "Shashi Kant Gupta", "authors": "Shashi Kant Gupta", "title": "Reinforcement Based Learning on Classification Task Could Yield Better\n  Generalization and Adversarial Accuracy", "comments": "10 pages (5 main, 1 ref, 4 supplementary); Accepted at 2nd Workshop\n  on Shared Visual Representations in Human and Machine Intelligence (SVRHM),\n  NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has become interestingly popular in computer vision, mostly\nattaining near or above human-level performance in various vision tasks. But\nrecent work has also demonstrated that these deep neural networks are very\nvulnerable to adversarial examples (adversarial examples - inputs to a model\nwhich are naturally similar to original data but fools the model in classifying\nit into a wrong class). Humans are very robust against such perturbations; one\npossible reason could be that humans do not learn to classify based on an error\nbetween \"target label\" and \"predicted label\" but possibly due to reinforcements\nthat they receive on their predictions. In this work, we proposed a novel\nmethod to train deep learning models on an image classification task. We used a\nreward-based optimization function, similar to the vanilla policy gradient\nmethod used in reinforcement learning, to train our model instead of\nconventional cross-entropy loss. An empirical evaluation on the cifar10 dataset\nshowed that our method learns a more robust classifier than the same model\narchitecture trained using cross-entropy loss function (on adversarial\ntraining). At the same time, our method shows a better generalization with the\ndifference in test accuracy and train accuracy $< 2\\%$ for most of the time\ncompared to the cross-entropy one, whose difference most of the time remains $>\n2\\%$.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 11:03:17 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Gupta", "Shashi Kant", ""]]}, {"id": "2012.04355", "submitter": "Yezhen Cong", "authors": "He Wang, Yezhen Cong, Or Litany, Yue Gao, Leonidas J. Guibas", "title": "3DIoUMatch: Leveraging IoU Prediction for Semi-Supervised 3D Object\n  Detection", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D object detection is an important yet demanding task that heavily relies on\ndifficult to obtain 3D annotations. To reduce the required amount of\nsupervision, we propose 3DIoUMatch, a novel semi-supervised method for 3D\nobject detection applicable to both indoor and outdoor scenes. We leverage a\nteacher-student mutual learning framework to propagate information from the\nlabeled to the unlabeled train set in the form of pseudo-labels. However, due\nto the high task complexity, we observe that the pseudo-labels suffer from\nsignificant noise and are thus not directly usable. To that end, we introduce a\nconfidence-based filtering mechanism, inspired by FixMatch. We set confidence\nthresholds based upon the predicted objectness and class probability to filter\nlow-quality pseudo-labels. While effective, we observe that these two measures\ndo not sufficiently capture localization quality. We therefore propose to use\nthe estimated 3D IoU as a localization metric and set category-aware\nself-adjusted thresholds to filter poorly localized proposals. We adopt VoteNet\nas our backbone detector on indoor datasets while we use PV-RCNN on the\nautonomous driving dataset, KITTI. Our method consistently improves\nstate-of-the-art methods on both ScanNet and SUN-RGBD benchmarks by significant\nmargins under all label ratios (including fully labeled setting). For example,\nwhen training using only 10\\% labeled data on ScanNet, 3DIoUMatch achieves 7.7%\nabsolute improvement on mAP@0.25 and 8.5% absolute improvement on mAP@0.5 upon\nthe prior art. On KITTI, we are the first to demonstrate semi-supervised 3D\nobject detection and our method surpasses a fully supervised baseline from 1.8%\nto 7.6% under different label ratios and categories.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 11:06:26 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 13:31:55 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 17:51:55 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Wang", "He", ""], ["Cong", "Yezhen", ""], ["Litany", "Or", ""], ["Gao", "Yue", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "2012.04382", "submitter": "Weipeng Xu", "authors": "Weipeng Xu, Hongcheng Huang, Shaoyou Pan", "title": "Using Feature Alignment Can Improve Clean Average Precision and\n  Adversarial Robustness in Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2D object detection in clean images has been a well studied topic, but\nits vulnerability against adversarial attack is still worrying. Existing work\nhas improved robustness of object detectors by adversarial training, at the\nsame time, the average precision (AP) on clean images drops significantly. In\nthis paper, we propose that using feature alignment of intermediate layer can\nimprove clean AP and robustness in object detection. Further, on the basis of\nadversarial training, we present two feature alignment modules:\nKnowledge-Distilled Feature Alignment (KDFA) module and Self-Supervised Feature\nAlignment (SSFA) module, which can guide the network to generate more effective\nfeatures. We conduct extensive experiments on PASCAL VOC and MS-COCO datasets\nto verify the effectiveness of our proposed approach. The code of our\nexperiments is available at https://github.com/grispeut/Feature-Alignment.git.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 11:54:39 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 05:04:38 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Xu", "Weipeng", ""], ["Huang", "Hongcheng", ""], ["Pan", "Shaoyou", ""]]}, {"id": "2012.04404", "submitter": "Yu Siyue", "authors": "Siyue Yu, Bingfeng Zhang, Jimin Xiao, Eng Gee Lim", "title": "Structure-Consistent Weakly Supervised Salient Object Detection with\n  Local Saliency Coherence", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sparse labels have been attracting much attention in recent years. However,\nthe performance gap between weakly supervised and fully supervised salient\nobject detection methods is huge, and most previous weakly supervised works\nadopt complex training methods with many bells and whistles. In this work, we\npropose a one-round end-to-end training approach for weakly supervised salient\nobject detection via scribble annotations without pre/post-processing\noperations or extra supervision data. Since scribble labels fail to offer\ndetailed salient regions, we propose a local coherence loss to propagate the\nlabels to unlabeled regions based on image features and pixel distance, so as\nto predict integral salient regions with complete object structures. We design\na saliency structure consistency loss as self-consistent mechanism to ensure\nconsistent saliency maps are predicted with different scales of the same image\nas input, which could be viewed as a regularization technique to enhance the\nmodel generalization ability. Additionally, we design an aggregation module\n(AGGM) to better integrate high-level features, low-level features and global\ncontext information for the decoder to aggregate various information. Extensive\nexperiments show that our method achieves a new state-of-the-art performance on\nsix benchmarks (e.g. for the ECSSD dataset: F_\\beta = 0.8995, E_\\xi = 0.9079\nand MAE = 0.0489$), with an average gain of 4.60\\% for F-measure, 2.05\\% for\nE-measure and 1.88\\% for MAE over the previous best method on this task. Source\ncode is available at http://github.com/siyueyu/SCWSSOD.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 12:49:40 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 03:22:46 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Yu", "Siyue", ""], ["Zhang", "Bingfeng", ""], ["Xiao", "Jimin", ""], ["Lim", "Eng Gee", ""]]}, {"id": "2012.04439", "submitter": "Xinhai Liu", "authors": "Xinhai Liu, Xinchen Liu, Zhizhong Han, Yu-Shen Liu", "title": "SPU-Net: Self-Supervised Point Cloud Upsampling by Coarse-to-Fine\n  Reconstruction with Self-Projection Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of point cloud upsampling aims to acquire dense and uniform point\nsets from sparse and irregular point sets. Although significant progress has\nbeen made with deep learning models, they require ground-truth dense point sets\nas the supervision information, which can only trained on synthetic paired\ntraining data and are not suitable for training under real-scanned sparse data.\nHowever, it is expensive and tedious to obtain large scale paired sparse-dense\npoint sets for training from real scanned sparse data. To address this problem,\nwe propose a self-supervised point cloud upsampling network, named SPU-Net, to\ncapture the inherent upsampling patterns of points lying on the underlying\nobject surface. Specifically, we propose a coarse-to-fine reconstruction\nframework, which contains two main components: point feature extraction and\npoint feature expansion, respectively. In the point feature extraction, we\nintegrate self-attention module with graph convolution network (GCN) to\nsimultaneously capture context information inside and among local regions. In\nthe point feature expansion, we introduce a hierarchically learnable folding\nstrategy to generate the upsampled point sets with learnable 2D grids.\nMoreover, to further optimize the noisy points in the generated point sets, we\npropose a novel self-projection optimization associated with uniform and\nreconstruction terms, as a joint loss, to facilitate the self-supervised point\ncloud upsampling. We conduct various experiments on both synthetic and\nreal-scanned datasets, and the results demonstrate that we achieve comparable\nperformance to the state-of-the-art supervised methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 14:14:09 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Liu", "Xinhai", ""], ["Liu", "Xinchen", ""], ["Han", "Zhizhong", ""], ["Liu", "Yu-Shen", ""]]}, {"id": "2012.04460", "submitter": "Andres Asensio Ramos", "authors": "A. Asensio Ramos and E. Pall\\'e", "title": "Planet cartography with neural learned regularization", "comments": "12 pages, 9 figures, accepted for publication in A&A, code on\n  https://github.com/aasensio/neural_exocartography", "journal-ref": "A&A 646, A4 (2021)", "doi": "10.1051/0004-6361/202040066", "report-no": null, "categories": "astro-ph.EP cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding potential life harboring exo-Earths is one of the aims of\nexoplanetary science. Detecting signatures of life in exoplanets will likely\nfirst be accomplished by determining the bulk composition of the planetary\natmosphere via reflected/transmitted spectroscopy. However, a complete\nunderstanding of the habitability conditions will surely require mapping the\npresence of liquid water, continents and/or clouds. Spin-orbit tomography is a\ntechnique that allows us to obtain maps of the surface of exoplanets around\nother stars using the light scattered by the planetary surface. We leverage the\npotential of deep learning and propose a mapping technique for exo-Earths in\nwhich the regularization is learned from mock surfaces. The solution of the\ninverse mapping problem is posed as a deep neural network that can be trained\nend-to-end with suitable training data. We propose in this work to use methods\nbased on the procedural generation of planets, inspired by what we found on\nEarth. We also consider mapping the recovery of surfaces and the presence of\npersistent cloud in cloudy planets. We show that the a reliable mapping can be\ncarried out with our approach, producing very compact continents, even when\nusing single passband observations. More importantly, if exoplanets are\npartially cloudy like the Earth is, we show that one can potentially map the\ndistribution of persistent clouds that always occur on the same position on the\nsurface (associated to orography and sea surface temperatures) together with\nnon-persistent clouds that move across the surface. This will become the first\ntest one can perform on an exoplanet for the detection of an active climate\nsystem. For small rocky planets in the habitable zone of their stars, this\nweather system will be driven by water, and the detection can be considered as\na strong proxy for truly habitable conditions.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 14:57:33 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 09:05:27 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Ramos", "A. Asensio", ""], ["Pall\u00e9", "E.", ""]]}, {"id": "2012.04462", "submitter": "Diego Ortego", "authors": "Diego Ortego, Eric Arazo, Paul Albert, Noel E. O'Connor and Kevin\n  McGuinness", "title": "Multi-Objective Interpolation Training for Robustness to Label Noise", "comments": "Accepted to CVPR 2021. 10 pages, 1 figure, and 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks trained with standard cross-entropy loss memorize noisy\nlabels, which degrades their performance. Most research to mitigate this\nmemorization proposes new robust classification loss functions. Conversely, we\npropose a Multi-Objective Interpolation Training (MOIT) approach that jointly\nexploits contrastive learning and classification to mutually help each other\nand boost performance against label noise. We show that standard supervised\ncontrastive learning degrades in the presence of label noise and propose an\ninterpolation training strategy to mitigate this behavior. We further propose a\nnovel label noise detection method that exploits the robust feature\nrepresentations learned via contrastive learning to estimate per-sample\nsoft-labels whose disagreements with the original labels accurately identify\nnoisy samples. This detection allows treating noisy samples as unlabeled and\ntraining a classifier in a semi-supervised manner to prevent noise memorization\nand improve representation learning. We further propose MOIT+, a refinement of\nMOIT by fine-tuning on detected clean samples. Hyperparameter and ablation\nstudies verify the key components of our method. Experiments on synthetic and\nreal-world noise benchmarks demonstrate that MOIT/MOIT+ achieves\nstate-of-the-art results. Code is available at https://git.io/JI40X.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 15:01:54 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 07:44:28 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Ortego", "Diego", ""], ["Arazo", "Eric", ""], ["Albert", "Paul", ""], ["O'Connor", "Noel E.", ""], ["McGuinness", "Kevin", ""]]}, {"id": "2012.04468", "submitter": "Gustau Camps-Valls", "authors": "ochem Verrelst, Sara Dethier, Juan Pablo Rivera, Jordi Mu\\~noz-Mar\\'i,\n  Gustau Camps-Valls, Jos\\'e Moreno", "title": "Active Learning Methods for Efficient Hybrid Biophysical Variable\n  Retrieval", "comments": null, "journal-ref": "IEEE Geoscience and Remote Sensing Letters, vol. 13, no. 7, pp.\n  1012-1016, July 2016", "doi": "10.1109/LGRS.2016.2560799", "report-no": null, "categories": "eess.SP cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Kernel-based machine learning regression algorithms (MLRAs) are potentially\npowerful methods for being implemented into operational biophysical variable\nretrieval schemes. However, they face difficulties in coping with large\ntraining datasets. With the increasing amount of optical remote sensing data\nmade available for analysis and the possibility of using a large amount of\nsimulated data from radiative transfer models (RTMs) to train kernel MLRAs,\nefficient data reduction techniques will need to be implemented. Active\nlearning (AL) methods enable to select the most informative samples in a\ndataset. This letter introduces six AL methods for achieving optimized\nbiophysical variable estimation with a manageable training dataset, and their\nimplementation into a Matlab-based MLRA toolbox for semi-automatic use. The AL\nmethods were analyzed on their efficiency of improving the estimation accuracy\nof leaf area index and chlorophyll content based on PROSAIL simulations. Each\nof the implemented methods outperformed random sampling, improving retrieval\naccuracy with lower sampling rates. Practically, AL methods open opportunities\nto feed advanced MLRAs with RTM-generated training data for development of\noperational retrieval models.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 08:56:40 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Verrelst", "ochem", ""], ["Dethier", "Sara", ""], ["Rivera", "Juan Pablo", ""], ["Mu\u00f1oz-Mar\u00ed", "Jordi", ""], ["Camps-Valls", "Gustau", ""], ["Moreno", "Jos\u00e9", ""]]}, {"id": "2012.04469", "submitter": "Gustau Camps-Valls", "authors": "Devis Tuia, Diego Marcos, Gustau Camps-Valls", "title": "Multi-temporal and multi-source remote sensing image classification by\n  nonlinear relative normalization", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing 120, DOI:\n  10.1016/j.isprsjprs.2016.07.004", "doi": "10.1016/j.isprsjprs.2016.07.004", "report-no": null, "categories": "eess.SP cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Remote sensing image classification exploiting multiple sensors is a very\nchallenging problem: data from different modalities are affected by spectral\ndistortions and mis-alignments of all kinds, and this hampers re-using models\nbuilt for one image to be used successfully in other scenes. In order to adapt\nand transfer models across image acquisitions, one must be able to cope with\ndatasets that are not co-registered, acquired under different illumination and\natmospheric conditions, by different sensors, and with scarce ground\nreferences. Traditionally, methods based on histogram matching have been used.\nHowever, they fail when densities have very different shapes or when there is\nno corresponding band to be matched between the images. An alternative builds\nupon \\emph{manifold alignment}. Manifold alignment performs a multidimensional\nrelative normalization of the data prior to product generation that can cope\nwith data of different dimensionality (e.g. different number of bands) and\npossibly unpaired examples. Aligning data distributions is an appealing\nstrategy, since it allows to provide data spaces that are more similar to each\nother, regardless of the subsequent use of the transformed data. In this paper,\nwe study a methodology that aligns data from different domains in a nonlinear\nway through {\\em kernelization}. We introduce the Kernel Manifold Alignment\n(KEMA) method, which provides a flexible and discriminative projection map,\nexploits only a few labeled samples (or semantic ties) in each domain, and\nreduces to solving a generalized eigenvalue problem. We successfully test KEMA\nin multi-temporal and multi-source very high resolution classification tasks,\nas well as on the task of making a model invariant to shadowing for\nhyperspectral imaging.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 08:46:11 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Tuia", "Devis", ""], ["Marcos", "Diego", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.04474", "submitter": "Suhas Lohit", "authors": "Suhas Lohit, Shubhendu Trivedi", "title": "Rotation-Invariant Autoencoders for Signals on Spheres", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omnidirectional images and spherical representations of $3D$ shapes cannot be\nprocessed with conventional 2D convolutional neural networks (CNNs) as the\nunwrapping leads to large distortion. Using fast implementations of spherical\nand $SO(3)$ convolutions, researchers have recently developed deep learning\nmethods better suited for classifying spherical images. These newly proposed\nconvolutional layers naturally extend the notion of convolution to functions on\nthe unit sphere $S^2$ and the group of rotations $SO(3)$ and these layers are\nequivariant to 3D rotations. In this paper, we consider the problem of\nunsupervised learning of rotation-invariant representations for spherical\nimages. In particular, we carefully design an autoencoder architecture\nconsisting of $S^2$ and $SO(3)$ convolutional layers. As 3D rotations are often\na nuisance factor, the latent space is constrained to be exactly invariant to\nthese input transformations. As the rotation information is discarded in the\nlatent space, we craft a novel rotation-invariant loss function for training\nthe network. Extensive experiments on multiple datasets demonstrate the\nusefulness of the learned representations on clustering, retrieval and\nclassification applications.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 15:15:03 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Lohit", "Suhas", ""], ["Trivedi", "Shubhendu", ""]]}, {"id": "2012.04512", "submitter": "Yiqing Liang", "authors": "Yiqing Liang, Boyuan Chen, Shuran Song", "title": "SSCNav: Confidence-Aware Semantic Scene Completion for Visual Semantic\n  Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on visual semantic navigation, the task of producing\nactions for an active agent to navigate to a specified target object category\nin an unknown environment. To complete this task, the algorithm should\nsimultaneously locate and navigate to an instance of the category. In\ncomparison to the traditional point goal navigation, this task requires the\nagent to have a stronger contextual prior to indoor environments. We introduce\nSSCNav, an algorithm that explicitly models scene priors using a\nconfidence-aware semantic scene completion module to complete the scene and\nguide the agent's navigation planning. Given a partial observation of the\nenvironment, SSCNav first infers a complete scene representation with semantic\nlabels for the unobserved scene together with a confidence map associated with\nits own prediction. Then, a policy network infers the action from the scene\ncompletion result and confidence map. Our experiments demonstrate that the\nproposed scene completion module improves the efficiency of the downstream\nnavigation policies. Video, code, and data: https://sscnav.cs.columbia.edu/\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 15:59:47 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 01:15:16 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Liang", "Yiqing", ""], ["Chen", "Boyuan", ""], ["Song", "Shuran", ""]]}, {"id": "2012.04514", "submitter": "Radu Horaud P", "authors": "Radu Horaud, Matti Niskanen, Guillaume Dewaele, and Edmond Boyer", "title": "Human Motion Tracking by Registering an Articulated Surface to 3-D\n  Points and Normals", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  31(1) 2009", "doi": "10.1109/TPAMI.2008.108", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of human motion tracking by registering a surface to\n3-D data. We propose a method that iteratively computes two things: Maximum\nlikelihood estimates for both the kinematic and free-motion parameters of a\nkinematic human-body representation, as well as probabilities that the data are\nassigned either to a body part, or to an outlier cluster. We introduce a new\nmetric between observed points and normals on one side, and a parameterized\nsurface on the other side, the latter being defined as a blending over a set of\nellipsoids. We claim that this metric is well suited when one deals with either\nvisual-hull or visual-shape observations. We illustrate the method by tracking\nhuman motions using sparse visual-shape data (3-D surface points and normals)\ngathered from imperfect silhouettes.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 16:02:16 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Horaud", "Radu", ""], ["Niskanen", "Matti", ""], ["Dewaele", "Guillaume", ""], ["Boyer", "Edmond", ""]]}, {"id": "2012.04515", "submitter": "Omer Dahary", "authors": "Omer Dahary, Matan Jacoby, Alex M. Bronstein", "title": "Digital Gimbal: End-to-end Deep Image Stabilization with Learnable\n  Exposure Times", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mechanical image stabilization using actuated gimbals enables capturing\nlong-exposure shots without suffering from blur due to camera motion. These\ndevices, however, are often physically cumbersome and expensive, limiting their\nwidespread use. In this work, we propose to digitally emulate a mechanically\nstabilized system from the input of a fast unstabilized camera. To exploit the\ntrade-off between motion blur at long exposures and low SNR at short exposures,\nwe train a CNN that estimates a sharp high-SNR image by aggregating a burst of\nnoisy short-exposure frames, related by unknown motion. We further suggest\nlearning the burst's exposure times in an end-to-end manner, thus balancing the\nnoise and blur across the frames. We demonstrate this method's advantage over\nthe traditional approach of deblurring a single image or denoising a\nfixed-exposure burst on both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 16:04:20 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 15:51:22 GMT"}, {"version": "v3", "created": "Mon, 10 May 2021 12:31:23 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Dahary", "Omer", ""], ["Jacoby", "Matan", ""], ["Bronstein", "Alex M.", ""]]}, {"id": "2012.04525", "submitter": "Yuri Feigin", "authors": "Yuri Feigin and Hedva Spitzer and Raja Giryes", "title": "GMM-Based Generative Adversarial Encoder Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While GAN is a powerful model for generating images, its inability to infer a\nlatent space directly limits its use in applications requiring an encoder. Our\npaper presents a simple architectural setup that combines the generative\ncapabilities of GAN with an encoder. We accomplish this by combining the\nencoder with the discriminator using shared weights, then training them\nsimultaneously using a new loss term. We model the output of the encoder latent\nspace via a GMM, which leads to both good clustering using this latent space\nand improved image generation by the GAN. Our framework is generic and can be\neasily plugged into any GAN strategy. In particular, we demonstrate it both\nwith Vanilla GAN and Wasserstein GAN, where in both it leads to an improvement\nin the generated images in terms of both the IS and FID scores. Moreover, we\nshow that our encoder learns a meaningful representation as its clustering\nresults are competitive with the current GAN-based state-of-the-art in\nclustering.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 16:12:16 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Feigin", "Yuri", ""], ["Spitzer", "Hedva", ""], ["Giryes", "Raja", ""]]}, {"id": "2012.04529", "submitter": "Lingbo Liu", "authors": "Lingbo Liu, Jiaqi Chen, Hefeng Wu, Guanbin Li, Chenglong Li, Liang Lin", "title": "Cross-Modal Collaborative Representation Learning and a Large-Scale RGBT\n  Benchmark for Crowd Counting", "comments": "Accepted by CVPR2021. Our code and benchmark for RGBT crowd counting\n  are released at {\\url{http://lingboliu.com/RGBT_Crowd_Counting.html}}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting is a fundamental yet challenging task, which desires rich\ninformation to generate pixel-wise crowd density maps. However, most previous\nmethods only used the limited information of RGB images and cannot well\ndiscover potential pedestrians in unconstrained scenarios. In this work, we\nfind that incorporating optical and thermal information can greatly help to\nrecognize pedestrians. To promote future researches in this field, we introduce\na large-scale RGBT Crowd Counting (RGBT-CC) benchmark, which contains 2,030\npairs of RGB-thermal images with 138,389 annotated people. Furthermore, to\nfacilitate the multimodal crowd counting, we propose a cross-modal\ncollaborative representation learning framework, which consists of multiple\nmodality-specific branches, a modality-shared branch, and an Information\nAggregation-Distribution Module (IADM) to capture the complementary information\nof different modalities fully. Specifically, our IADM incorporates two\ncollaborative information transfers to dynamically enhance the modality-shared\nand modality-specific representations with a dual information propagation\nmechanism. Extensive experiments conducted on the RGBT-CC benchmark demonstrate\nthe effectiveness of our framework for RGBT crowd counting. Moreover, the\nproposed approach is universal for multimodal crowd counting and is also\ncapable to achieve superior performance on the ShanghaiTechRGBD dataset.\nFinally, our source code and benchmark are released at\n{\\url{http://lingboliu.com/RGBT_Crowd_Counting.html}}.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 16:18:29 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 03:02:31 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Liu", "Lingbo", ""], ["Chen", "Jiaqi", ""], ["Wu", "Hefeng", ""], ["Li", "Guanbin", ""], ["Li", "Chenglong", ""], ["Lin", "Liang", ""]]}, {"id": "2012.04551", "submitter": "Ajinkya Kadu", "authors": "Ajinkya Kadu, Tristan van Leeuwen, and K. Joost Batenburg", "title": "CoShaRP: A Convex Program for Single-shot Tomographic Shape Sensing", "comments": "Paper is currently under consideration for Pattern Recognition\n  Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE cs.IR eess.IV math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce single-shot X-ray tomography that aims to estimate the target\nimage from a single cone-beam projection measurement. This linear inverse\nproblem is extremely under-determined since the measurements are far fewer than\nthe number of unknowns. Moreover, it is more challenging than conventional\ntomography where a sufficiently large number of projection angles forms the\nmeasurements, allowing for a simple inversion process. However, single-shot\ntomography becomes less severe if the target image is only composed of known\nshapes. Hence, the shape prior transforms a linear ill-posed image estimation\nproblem to a non-linear problem of estimating the roto-translations of the\nshapes. In this paper, we circumvent the non-linearity by using a dictionary of\npossible roto-translations of the shapes. We propose a convex program CoShaRP\nto recover the dictionary-coefficients successfully. CoShaRP relies on\nsimplex-type constraint and can be solved quickly using a primal-dual\nalgorithm. The numerical experiments show that CoShaRP recovers shapes stably\nfrom moderately noisy measurements.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 16:44:34 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 21:02:42 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Kadu", "Ajinkya", ""], ["van Leeuwen", "Tristan", ""], ["Batenburg", "K. Joost", ""]]}, {"id": "2012.04567", "submitter": "Razvan Marinescu", "authors": "Razvan V Marinescu, Daniel Moyer, Polina Golland", "title": "Bayesian Image Reconstruction using Deep Generative Models", "comments": "25 pages, 18 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning models are commonly trained end-to-end and in a supervised\nsetting, using paired (input, output) data. Examples include recent\nsuper-resolution methods that train on pairs of (low-resolution,\nhigh-resolution) images. However, these end-to-end approaches require\nre-training every time there is a distribution shift in the inputs (e.g., night\nimages vs daylight) or relevant latent variables (e.g., camera blur or hand\nmotion). In this work, we leverage state-of-the-art (SOTA) generative models\n(here StyleGAN2) for building powerful image priors, which enable application\nof Bayes' theorem for many downstream reconstruction tasks. Our method,\nBayesian Reconstruction through Generative Models (BRGM), uses a single\npre-trained generator model to solve different image restoration tasks, i.e.,\nsuper-resolution and in-painting, by combining it with different forward\ncorruption models. We keep the weights of the generator model fixed, and\nreconstruct the image by estimating the Bayesian maximum a-posteriori (MAP)\nestimate over the input latent vector that generated the reconstructed image.\nWe further use variational inference to approximate the posterior distribution\nover the latent vectors, from which we sample multiple solutions. We\ndemonstrate BRGM on three large and diverse datasets: (i) 60,000 images from\nthe Flick Faces High Quality dataset (ii) 240,000 chest X-rays from MIMIC III\nand (iii) a combined collection of 5 brain MRI datasets with 7,329 scans.\nAcross all three datasets and without any dataset-specific hyperparameter\ntuning, our simple approach yields performance competitive with current\ntask-specific state-of-the-art methods on super-resolution and in-painting,\nwhile being more generalisable and without requiring any training. Our source\ncode and pre-trained models are available online:\nhttps://razvanmarinescu.github.io/brgm/.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 17:11:26 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 21:48:44 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2021 21:44:29 GMT"}, {"version": "v4", "created": "Tue, 8 Jun 2021 13:44:01 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Marinescu", "Razvan V", ""], ["Moyer", "Daniel", ""], ["Golland", "Polina", ""]]}, {"id": "2012.04578", "submitter": "Parichehr Behjati", "authors": "Parichehr Behjati, Pau Rodriguez, Armin Mehri, Isabelle Hupont, Carles\n  Fern\\'andez Tena, Jordi Gonzalez", "title": "Hierarchical Residual Attention Network for Single Image\n  Super-Resolution", "comments": "8 pages, Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks are the most successful models in single image\nsuper-resolution. Deeper networks, residual connections, and attention\nmechanisms have further improved their performance. However, these strategies\noften improve the reconstruction performance at the expense of considerably\nincreasing the computational cost. This paper introduces a new lightweight\nsuper-resolution model based on an efficient method for residual feature and\nattention aggregation. In order to make an efficient use of the residual\nfeatures, these are hierarchically aggregated into feature banks for posterior\nusage at the network output. In parallel, a lightweight hierarchical attention\nmechanism extracts the most relevant features from the network into attention\nbanks for improving the final output and preventing the information loss\nthrough the successive operations inside the network. Therefore, the processing\nis split into two independent paths of computation that can be simultaneously\ncarried out, resulting in a highly efficient and effective model for\nreconstructing fine details on high-resolution images from their low-resolution\ncounterparts. Our proposed architecture surpasses state-of-the-art performance\nin several datasets, while maintaining relatively low computation and memory\nfootprint.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 17:24:28 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Behjati", "Parichehr", ""], ["Rodriguez", "Pau", ""], ["Mehri", "Armin", ""], ["Hupont", "Isabelle", ""], ["Tena", "Carles Fern\u00e1ndez", ""], ["Gonzalez", "Jordi", ""]]}, {"id": "2012.04581", "submitter": "Shiv Ram Dubey", "authors": "Viswanatha Reddy Gajjala, Sai Prasanna Teja Reddy, Snehasis Mukherjee,\n  Shiv Ram Dubey", "title": "MERANet: Facial Micro-Expression Recognition using 3D Residual Attention\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a facial micro-expression recognition model using 3D residual\nattention network called MERANet. The proposed model takes advantage of\nspatial-temporal attention and channel attention together, to learn deeper\nfine-grained subtle features for classification of emotions. The proposed model\nalso encompasses both spatial and temporal information simultaneously using the\n3D kernels and residual connections. Moreover, the channel features and\nspatio-temporal features are re-calibrated using the channel and\nspatio-temporal attentions, respectively in each residual module. The\nexperiments are conducted on benchmark facial micro-expression datasets. A\nsuperior performance is observed as compared to the state-of-the-art for facial\nmicro-expression recognition.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 16:41:42 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Gajjala", "Viswanatha Reddy", ""], ["Reddy", "Sai Prasanna Teja", ""], ["Mukherjee", "Snehasis", ""], ["Dubey", "Shiv Ram", ""]]}, {"id": "2012.04597", "submitter": "Nikolas Tapia", "authors": "Joscha Diehl, Kurusch Ebrahimi-Fard, Nikolas Tapia", "title": "Generalized iterated-sums signatures", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.RA cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the algebraic properties of a generalized version of the\niterated-sums signature, inspired by previous work of F.~Kir\\'aly and\nH.~Oberhauser. In particular, we show how to recover the character property of\nthe associated linear map over the tensor algebra by considering a deformed\nquasi-shuffle product of words on the latter. We introduce three non-linear\ntransformations on iterated-sums signatures, close in spirit to Machine\nLearning applications, and show some of their properties.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 17:59:29 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Diehl", "Joscha", ""], ["Ebrahimi-Fard", "Kurusch", ""], ["Tapia", "Nikolas", ""]]}, {"id": "2012.04630", "submitter": "Ramprasaath R. Selvaraju", "authors": "Ramprasaath R. Selvaraju, Karan Desai, Justin Johnson, Nikhil Naik", "title": "CASTing Your Model: Learning to Localize Improves Self-Supervised\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in self-supervised learning (SSL) have largely closed the gap\nwith supervised ImageNet pretraining. Despite their success these methods have\nbeen primarily applied to unlabeled ImageNet images, and show marginal gains\nwhen trained on larger sets of uncurated images. We hypothesize that current\nSSL methods perform best on iconic images, and struggle on complex scene images\nwith many objects. Analyzing contrastive SSL methods shows that they have poor\nvisual grounding and receive poor supervisory signal when trained on scene\nimages. We propose Contrastive Attention-Supervised Tuning(CAST) to overcome\nthese limitations. CAST uses unsupervised saliency maps to intelligently sample\ncrops, and to provide grounding supervision via a Grad-CAM attention loss.\nExperiments on COCO show that CAST significantly improves the features learned\nby SSL methods on scene images, and further experiments show that CAST-trained\nmodels are more robust to changes in backgrounds.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 18:50:18 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Selvaraju", "Ramprasaath R.", ""], ["Desai", "Karan", ""], ["Johnson", "Justin", ""], ["Naik", "Nikhil", ""]]}, {"id": "2012.04631", "submitter": "Didac Suris Coll-Vinent", "authors": "D\\'idac Sur\\'is, Dave Epstein, Carl Vondrick", "title": "Globetrotter: Unsupervised Multilingual Translation from Visual\n  Alignment", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-language machine translation without parallel corpora is challenging\nbecause there is no explicit supervision between languages. Existing\nunsupervised methods typically rely on topological properties of the language\nrepresentations. We introduce a framework that instead uses the visual modality\nto align multiple languages, using images as the bridge between them. We\nestimate the cross-modal alignment between language and images, and use this\nestimate to guide the learning of cross-lingual representations. Our language\nrepresentations are trained jointly in one model with a single stage.\nExperiments with fifty-two languages show that our method outperforms baselines\non unsupervised word-level and sentence-level translation using retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 18:50:40 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Sur\u00eds", "D\u00eddac", ""], ["Epstein", "Dave", ""], ["Vondrick", "Carl", ""]]}, {"id": "2012.04634", "submitter": "Fredrik K. Gustafsson", "authors": "Fredrik K. Gustafsson, Martin Danelljan, Thomas B. Sch\\\"on", "title": "Accurate 3D Object Detection using Energy-Based Models", "comments": "Code is available at https://github.com/fregu856/ebms_3dod", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate 3D object detection (3DOD) is crucial for safe navigation of complex\nenvironments by autonomous robots. Regressing accurate 3D bounding boxes in\ncluttered environments based on sparse LiDAR data is however a highly\nchallenging problem. We address this task by exploring recent advances in\nconditional energy-based models (EBMs) for probabilistic regression. While\nmethods employing EBMs for regression have demonstrated impressive performance\non 2D object detection in images, these techniques are not directly applicable\nto 3D bounding boxes. In this work, we therefore design a differentiable\npooling operator for 3D bounding boxes, serving as the core module of our EBM\nnetwork. We further integrate this general approach into the state-of-the-art\n3D object detector SA-SSD. On the KITTI dataset, our proposed approach\nconsistently outperforms the SA-SSD baseline across all 3DOD metrics,\ndemonstrating the potential of EBM-based regression for highly accurate 3DOD.\nCode is available at https://github.com/fregu856/ebms_3dod.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 18:53:42 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Gustafsson", "Fredrik K.", ""], ["Danelljan", "Martin", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "2012.04638", "submitter": "Zhengyuan Yang", "authors": "Zhengyuan Yang, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei Florencio,\n  Lijuan Wang, Cha Zhang, Lei Zhang, Jiebo Luo", "title": "TAP: Text-Aware Pre-training for Text-VQA and Text-Caption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Text-Aware Pre-training (TAP) for Text-VQA and\nText-Caption tasks. These two tasks aim at reading and understanding scene text\nin images for question answering and image caption generation, respectively. In\ncontrast to the conventional vision-language pre-training that fails to capture\nscene text and its relationship with the visual and text modalities, TAP\nexplicitly incorporates scene text (generated from OCR engines) in\npre-training. With three pre-training tasks, including masked language modeling\n(MLM), image-text (contrastive) matching (ITM), and relative (spatial) position\nprediction (RPP), TAP effectively helps the model learn a better aligned\nrepresentation among the three modalities: text word, visual object, and scene\ntext. Due to this aligned representation learning, even pre-trained on the same\ndownstream task dataset, TAP already boosts the absolute accuracy on the\nTextVQA dataset by +5.4%, compared with a non-TAP baseline. To further improve\nthe performance, we build a large-scale dataset based on the Conceptual Caption\ndataset, named OCR-CC, which contains 1.4 million scene text-related image-text\npairs. Pre-trained on this OCR-CC dataset, our approach outperforms the state\nof the art by large margins on multiple tasks, i.e., +8.3% accuracy on TextVQA,\n+8.6% accuracy on ST-VQA, and +10.2 CIDEr score on TextCaps.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 18:55:21 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Yang", "Zhengyuan", ""], ["Lu", "Yijuan", ""], ["Wang", "Jianfeng", ""], ["Yin", "Xi", ""], ["Florencio", "Dinei", ""], ["Wang", "Lijuan", ""], ["Zhang", "Cha", ""], ["Zhang", "Lei", ""], ["Luo", "Jiebo", ""]]}, {"id": "2012.04641", "submitter": "Kevis-Kokitsi Maninis", "authors": "Kevis-Kokitsi Maninis, Stefan Popov, Matthias Nie{\\ss}ner, Vittorio\n  Ferrari", "title": "Vid2CAD: CAD Model Alignment using Multi-View Constraints from Videos", "comments": "Video: https://www.youtube.com/watch?v=R1cXg0vpwe4 | Project page:\n  https://www.kmaninis.com/vid2cad/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of aligning CAD models to a video sequence of a complex\nscene containing multiple objects. Our method is able to process arbitrary\nvideos and fully automatically recover the 9 DoF pose for each object appearing\nin it, thus aligning them in a common 3D coordinate frame. The core idea of our\nmethod is to integrate neural network predictions from individual frames with a\ntemporally global, multi-view constraint optimization formulation. This\nintegration process resolves the scale and depth ambiguities in the per-frame\npredictions, and generally improves the estimate of all pose parameters. By\nleveraging multi-view constraints, our method also resolves occlusions and\nhandles objects that are out of view in individual frames, thus reconstructing\nall objects into a single globally consistent CAD representation of the scene.\nIn comparison to the state-of-the-art single-frame method Mask2CAD that we\nbuild on, we achieve substantial improvements on Scan2CAD (from 11.6% to 30.2%\nclass average accuracy).\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 18:57:45 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Maninis", "Kevis-Kokitsi", ""], ["Popov", "Stefan", ""], ["Nie\u00dfner", "Matthias", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "2012.04643", "submitter": "Sharath Girish", "authors": "Sharath Girish, Shishira R. Maiya, Kamal Gupta, Hao Chen, Larry Davis,\n  Abhinav Shrivastava", "title": "The Lottery Ticket Hypothesis for Object Recognition", "comments": "To appear at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition tasks, such as object recognition and keypoint estimation, have\nseen widespread adoption in recent years. Most state-of-the-art methods for\nthese tasks use deep networks that are computationally expensive and have huge\nmemory footprints. This makes it exceedingly difficult to deploy these systems\non low power embedded devices. Hence, the importance of decreasing the storage\nrequirements and the amount of computation in such models is paramount. The\nrecently proposed Lottery Ticket Hypothesis (LTH) states that deep neural\nnetworks trained on large datasets contain smaller subnetworks that achieve on\npar performance as the dense networks. In this work, we perform the first\nempirical study investigating LTH for model pruning in the context of object\ndetection, instance segmentation, and keypoint estimation. Our studies reveal\nthat lottery tickets obtained from ImageNet pretraining do not transfer well to\nthe downstream tasks. We provide guidance on how to find lottery tickets with\nup to 80% overall sparsity on different sub-tasks without incurring any drop in\nthe performance. Finally, we analyse the behavior of trained tickets with\nrespect to various task attributes such as object size, frequency, and\ndifficulty of detection.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 18:59:13 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 17:59:57 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Girish", "Sharath", ""], ["Maiya", "Shishira R.", ""], ["Gupta", "Kamal", ""], ["Chen", "Hao", ""], ["Davis", "Larry", ""], ["Shrivastava", "Abhinav", ""]]}, {"id": "2012.04644", "submitter": "Dongdong Chen", "authors": "Zhentao Tan and Dongdong Chen and Qi Chu and Menglei Chai and Jing\n  Liao and Mingming He and Lu Yuan and Gang Hua and Nenghai Yu", "title": "Efficient Semantic Image Synthesis via Class-Adaptive Normalization", "comments": "To appear at TPAMI 2021, code is available\n  https://github.com/tzt101/CLADE.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatially-adaptive normalization (SPADE) is remarkably successful recently in\nconditional semantic image synthesis \\cite{park2019semantic}, which modulates\nthe normalized activation with spatially-varying transformations learned from\nsemantic layouts, to prevent the semantic information from being washed away.\nDespite its impressive performance, a more thorough understanding of the\nadvantages inside the box is still highly demanded to help reduce the\nsignificant computation and parameter overhead introduced by this novel\nstructure. In this paper, from a return-on-investment point of view, we conduct\nan in-depth analysis of the effectiveness of this spatially-adaptive\nnormalization and observe that its modulation parameters benefit more from\nsemantic-awareness rather than spatial-adaptiveness, especially for\nhigh-resolution input masks. Inspired by this observation, we propose\nclass-adaptive normalization (CLADE), a lightweight but equally-effective\nvariant that is only adaptive to semantic class. In order to further improve\nspatial-adaptiveness, we introduce intra-class positional map encoding\ncalculated from semantic layouts to modulate the normalization parameters of\nCLADE and propose a truly spatially-adaptive variant of CLADE, namely\nCLADE-ICPE.Through extensive experiments on multiple challenging datasets, we\ndemonstrate that the proposed CLADE can be generalized to different SPADE-based\nmethods while achieving comparable generation quality compared to SPADE, but it\nis much more efficient with fewer extra parameters and lower computational\ncost. The code and pretrained models are available at\n\\url{https://github.com/tzt101/CLADE.git}.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 18:59:32 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 23:20:35 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Tan", "Zhentao", ""], ["Chen", "Dongdong", ""], ["Chu", "Qi", ""], ["Chai", "Menglei", ""], ["Liao", "Jing", ""], ["He", "Mingming", ""], ["Yuan", "Lu", ""], ["Hua", "Gang", ""], ["Yu", "Nenghai", ""]]}, {"id": "2012.04689", "submitter": "Otto Brookes", "authors": "Otto Brookes, Tilo Burghardt", "title": "A Dataset and Application for Facial Recognition of Individual Gorillas\n  in Zoo Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We put forward a video dataset with 5k+ facial bounding box annotations\nacross a troop of 7 western lowland gorillas at Bristol Zoo Gardens. Training\non this dataset, we implement and evaluate a standard deep learning pipeline on\nthe task of facially recognising individual gorillas in a zoo environment. We\nshow that a basic YOLOv3-powered application is able to perform identifications\nat 92% mAP when utilising single frames only. Tracking-by-detection-association\nand identity voting across short tracklets yields an improved robust\nperformance of 97% mAP. To facilitate easy utilisation for enriching the\nresearch capabilities of zoo environments, we publish the code, video dataset,\nweights, and ground-truth annotations at data.bris.ac.uk.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 19:23:22 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 17:38:09 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Brookes", "Otto", ""], ["Burghardt", "Tilo", ""]]}, {"id": "2012.04692", "submitter": "Amish Goel", "authors": "Amish Goel, Pierre Moulin", "title": "Locally optimal detection of stochastic targeted universal adversarial\n  perturbations", "comments": "Submitted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning image classifiers are known to be vulnerable to small\nadversarial perturbations of input images. In this paper, we derive the locally\noptimal generalized likelihood ratio test (LO-GLRT) based detector for\ndetecting stochastic targeted universal adversarial perturbations (UAPs) of the\nclassifier inputs. We also describe a supervised training method to learn the\ndetector's parameters, and demonstrate better performance of the detector\ncompared to other detection methods on several popular image classification\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 19:27:39 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Goel", "Amish", ""], ["Moulin", "Pierre", ""]]}, {"id": "2012.04701", "submitter": "Tianyi Zhao", "authors": "Tianyi Zhao, Kai Cao, Jiawen Yao, Isabella Nogues, Le Lu, Lingyun\n  Huang, Jing Xiao, Zhaozheng Yin, Ling Zhang", "title": "3D Graph Anatomy Geometry-Integrated Network for Pancreatic Mass\n  Segmentation, Diagnosis, and Quantitative Patient Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pancreatic disease taxonomy includes ten types of masses (tumors or\ncysts)[20,8]. Previous work focuses on developing segmentation or\nclassification methods only for certain mass types. Differential diagnosis of\nall mass types is clinically highly desirable [20] but has not been\ninvestigated using an automated image understanding approach. We exploit the\nfeasibility to distinguish pancreatic ductal adenocarcinoma (PDAC) from the\nnine other nonPDAC masses using multi-phase CT imaging. Both image appearance\nand the 3D organ-mass geometry relationship are critical. We propose a holistic\nsegmentation-mesh-classification network (SMCN) to provide patient-level\ndiagnosis, by fully utilizing the geometry and location information, which is\naccomplished by combining the anatomical structure and the semantic\ndetection-by-segmentation network. SMCN learns the pancreas and mass\nsegmentation task and builds an anatomical correspondence-aware organ mesh\nmodel by progressively deforming a pancreas prototype on the raw segmentation\nmask (i.e., mask-to-mesh). A new graph-based residual convolutional network\n(Graph-ResNet), whose nodes fuse the information of the mesh model and feature\nvectors extracted from the segmentation network, is developed to produce the\npatient-level differential classification results. Extensive experiments on 661\npatients' CT scans (five phases per patient) show that SMCN can improve the\nmass segmentation and detection accuracy compared to the strong baseline method\nnnUNet (e.g., for nonPDAC, Dice: 0.611 vs. 0.478; detection rate: 89% vs. 70%),\nachieve similar sensitivity and specificity in differentiating PDAC and nonPDAC\nas expert radiologists (i.e., 94% and 90%), and obtain results comparable to a\nmultimodality test [20] that combines clinical, imaging, and molecular testing\nfor clinical management of patients.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 19:38:01 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Zhao", "Tianyi", ""], ["Cao", "Kai", ""], ["Yao", "Jiawen", ""], ["Nogues", "Isabella", ""], ["Lu", "Le", ""], ["Huang", "Lingyun", ""], ["Xiao", "Jing", ""], ["Yin", "Zhaozheng", ""], ["Zhang", "Ling", ""]]}, {"id": "2012.04708", "submitter": "Yusuf H. Sahin", "authors": "Yusuf H. Sahin, Alican Mertan, Gozde Unal", "title": "ODFNet: Using orientation distribution functions to characterize 3D\n  point clouds", "comments": "The paper is under consideration at Computer Vision and Image\n  Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learning new representations of 3D point clouds is an active research area in\n3D vision, as the order-invariant point cloud structure still presents\nchallenges to the design of neural network architectures. Recent works explored\nlearning either global or local features or both for point clouds, however none\nof the earlier methods focused on capturing contextual shape information by\nanalysing local orientation distribution of points. In this paper, we leverage\non point orientation distributions around a point in order to obtain an\nexpressive local neighborhood representation for point clouds. We achieve this\nby dividing the spherical neighborhood of a given point into predefined cone\nvolumes, and statistics inside each volume are used as point features. In this\nway, a local patch can be represented by not only the selected point's nearest\nneighbors, but also considering a point density distribution defined along\nmultiple orientations around the point. We are then able to construct an\norientation distribution function (ODF) neural network that involves an\nODFBlock which relies on mlp (multi-layer perceptron) layers. The new ODFNet\nmodel achieves state-of the-art accuracy for object classification on\nModelNet40 and ScanObjectNN datasets, and segmentation on ShapeNet S3DIS\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 19:54:20 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Sahin", "Yusuf H.", ""], ["Mertan", "Alican", ""], ["Unal", "Gozde", ""]]}, {"id": "2012.04718", "submitter": "Weiwei Sun", "authors": "Weiwei Sun, Andrea Tagliasacchi, Boyang Deng, Sara Sabour, Soroosh\n  Yazdani, Geoffrey Hinton, Kwang Moo Yi", "title": "Canonical Capsules: Unsupervised Capsules in Canonical Pose", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised capsule architecture for 3D point clouds. We\ncompute capsule decompositions of objects through permutation-equivariant\nattention, and self-supervise the process by training with pairs of randomly\nrotated objects. Our key idea is to aggregate the attention masks into semantic\nkeypoints, and use these to supervise a decomposition that satisfies the\ncapsule invariance/equivariance properties. This not only enables the training\nof a semantically consistent decomposition, but also allows us to learn a\ncanonicalization operation that enables object-centric reasoning. In doing so,\nwe require neither classification labels nor manually-aligned training datasets\nto train. Yet, by learning an object-centric representation in an unsupervised\nmanner, our method outperforms the state-of-the-art on 3D point cloud\nreconstruction, registration, and unsupervised classification. We will release\nthe code and dataset to reproduce our results as soon as the paper is\npublished.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 20:13:28 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Sun", "Weiwei", ""], ["Tagliasacchi", "Andrea", ""], ["Deng", "Boyang", ""], ["Sabour", "Sara", ""], ["Yazdani", "Soroosh", ""], ["Hinton", "Geoffrey", ""], ["Yi", "Kwang Moo", ""]]}, {"id": "2012.04726", "submitter": "Jeff Da", "authors": "Jeff Da and Maxwell Forbes and Rowan Zellers and Anthony Zheng and\n  Jena D. Hwang and Antoine Bosselut and Yejin Choi", "title": "Edited Media Understanding: Reasoning About Implications of Manipulated\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal disinformation, from `deepfakes' to simple edits that deceive, is\nan important societal problem. Yet at the same time, the vast majority of media\nedits are harmless -- such as a filtered vacation photo. The difference between\nthis example, and harmful edits that spread disinformation, is one of intent.\nRecognizing and describing this intent is a major challenge for today's AI\nsystems.\n  We present the task of Edited Media Understanding, requiring models to answer\nopen-ended questions that capture the intent and implications of an image edit.\nWe introduce a dataset for our task, EMU, with 48k question-answer pairs\nwritten in rich natural language. We evaluate a wide variety of\nvision-and-language models for our task, and introduce a new model PELICAN,\nwhich builds upon recent progress in pretrained multimodal representations. Our\nmodel obtains promising results on our dataset, with humans rating its answers\nas accurate 40.35% of the time. At the same time, there is still much work to\nbe done -- humans prefer human-annotated captions 93.56% of the time -- and we\nprovide analysis that highlights areas for further progress.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 20:30:43 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Da", "Jeff", ""], ["Forbes", "Maxwell", ""], ["Zellers", "Rowan", ""], ["Zheng", "Anthony", ""], ["Hwang", "Jena D.", ""], ["Bosselut", "Antoine", ""], ["Choi", "Yejin", ""]]}, {"id": "2012.04731", "submitter": "Sena Kiciroglu", "authors": "Sena Kiciroglu, Wei Wang, Mathieu Salzmann, Pascal Fua", "title": "Long Term Motion Prediction Using Keyposes", "comments": "See supplementary video at: https://youtu.be/T1WjdF8ux6o", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long term human motion prediction is essential in safety-critical\napplications such as human-robot interaction and autonomous driving. In this\npaper, we show that, to achieve long term forecasting, predicting human pose at\nevery time instant is unnecessary. Instead, it is more effective to predict a\nfew keyposes and approximate intermediate ones by linearly interpolating the\nkeyposes.\n  We will demonstrate that our approach enables us to predict realistic motions\nfor up to 5 seconds in the future, which is far larger than the typical 1\nsecond encountered in the literature. Over this extended time period, our\npredictions are more realistic and better preserve the motion dynamics than\nthose state-of-the-art methods yield.\n  Furthermore, because we model future keyposes probabilistically, we can\ngenerate multiple plausible future motions by sampling at inference time. This\nis useful to model because people usually can do one of several things given\nwhat they have already done.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 20:45:51 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 14:35:08 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kiciroglu", "Sena", ""], ["Wang", "Wei", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "2012.04733", "submitter": "Jiaqi Wang", "authors": "Jiaqi Wang, Kai Chen, Rui Xu, Ziwei Liu, Chen Change Loy, Dahua Lin", "title": "CARAFE++: Unified Content-Aware ReAssembly of FEatures", "comments": "Technical Report. Extended journal version of the conference paper\n  that appeared as arXiv:1905.02188", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature reassembly, i.e. feature downsampling and upsampling, is a key\noperation in a number of modern convolutional network architectures, e.g.,\nresidual networks and feature pyramids. Its design is critical for dense\nprediction tasks such as object detection and semantic/instance segmentation.\nIn this work, we propose unified Content-Aware ReAssembly of FEatures\n(CARAFE++), a universal, lightweight and highly effective operator to fulfill\nthis goal. CARAFE++ has several appealing properties: (1) Unlike conventional\nmethods such as pooling and interpolation that only exploit sub-pixel\nneighborhood, CARAFE++ aggregates contextual information within a large\nreceptive field. (2) Instead of using a fixed kernel for all samples (e.g.\nconvolution and deconvolution), CARAFE++ generates adaptive kernels on-the-fly\nto enable instance-specific content-aware handling. (3) CARAFE++ introduces\nlittle computational overhead and can be readily integrated into modern network\narchitectures. We conduct comprehensive evaluations on standard benchmarks in\nobject detection, instance/semantic segmentation and image inpainting. CARAFE++\nshows consistent and substantial gains across all the tasks (2.5% APbox, 2.1%\nAPmask, 1.94% mIoU, 1.35 dB respectively) with negligible computational\noverhead. It shows great potential to serve as a strong building block for\nmodern deep networks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 07:34:57 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Wang", "Jiaqi", ""], ["Chen", "Kai", ""], ["Xu", "Rui", ""], ["Liu", "Ziwei", ""], ["Loy", "Chen Change", ""], ["Lin", "Dahua", ""]]}, {"id": "2012.04743", "submitter": "Haoyu Wei", "authors": "Haoyu Wei, Florian Schiffers, Tobias W\\\"urfl, Daming Shen, Daniel Kim,\n  Aggelos K. Katsaggelos, Oliver Cossairt", "title": "2-Step Sparse-View CT Reconstruction with a Domain-Specific Perceptual\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computed tomography is widely used to examine internal structures in a\nnon-destructive manner. To obtain high-quality reconstructions, one typically\nhas to acquire a densely sampled trajectory to avoid angular undersampling.\nHowever, many scenarios require a sparse-view measurement leading to\nstreak-artifacts if unaccounted for. Current methods do not make full use of\nthe domain-specific information, and hence fail to provide reliable\nreconstructions for highly undersampled data. We present a novel framework for\nsparse-view tomography by decoupling the reconstruction into two steps: First,\nwe overcome its ill-posedness using a super-resolution network, SIN, trained on\nthe sparse projections. The intermediate result allows for a closed-form\ntomographic reconstruction with preserved details and highly reduced\nstreak-artifacts. Second, a refinement network, PRN, trained on the\nreconstructions reduces any remaining artifacts. We further propose a\nlight-weight variant of the perceptual-loss that enhances domain-specific\ninformation, boosting restoration accuracy. Our experiments demonstrate an\nimprovement over current solutions by 4 dB.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 21:16:43 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Wei", "Haoyu", ""], ["Schiffers", "Florian", ""], ["W\u00fcrfl", "Tobias", ""], ["Shen", "Daming", ""], ["Kim", "Daniel", ""], ["Katsaggelos", "Aggelos K.", ""], ["Cossairt", "Oliver", ""]]}, {"id": "2012.04746", "submitter": "Qingnan Fan", "authors": "Siyan Dong, Qingnan Fan, He Wang, Ji Shi, Li Yi, Thomas Funkhouser,\n  Baoquan Chen, Leonidas Guibas", "title": "Robust Neural Routing Through Space Partitions for Camera Relocalization\n  in Dynamic Indoor Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localizing the camera in a known indoor environment is a key building block\nfor scene mapping, robot navigation, AR, etc. Recent advances estimate the\ncamera pose via optimization over the 2D/3D-3D correspondences established\nbetween the coordinates in 2D/3D camera space and 3D world space. Such a\nmapping is estimated with either a convolution neural network or a decision\ntree using only the static input image sequence, which makes these approaches\nvulnerable to dynamic indoor environments that are quite common yet challenging\nin the real world. To address the aforementioned issues, in this paper, we\npropose a novel outlier-aware neural tree which bridges the two worlds, deep\nlearning and decision tree approaches. It builds on three important blocks: (a)\na hierarchical space partition over the indoor scene to construct the decision\ntree; (b) a neural routing function, implemented as a deep classification\nnetwork, employed for better 3D scene understanding; and (c) an outlier\nrejection module used to filter out dynamic points during the hierarchical\nrouting process. Our proposed algorithm is evaluated on the RIO-10 benchmark\ndeveloped for camera relocalization in dynamic indoor environments. It achieves\nrobust neural routing through space partitions and outperforms the\nstate-of-the-art approaches by around 30% on camera pose accuracy, while\nrunning comparably fast for evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 21:20:54 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 22:28:33 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Dong", "Siyan", ""], ["Fan", "Qingnan", ""], ["Wang", "He", ""], ["Shi", "Ji", ""], ["Yi", "Li", ""], ["Funkhouser", "Thomas", ""], ["Chen", "Baoquan", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2012.04750", "submitter": "Mohammed Hassanin", "authors": "Mohammed Hassanin, Ibrahim Radwan, Nour Moustafa, Murat Tahtali,\n  Neeraj Kumar", "title": "Mitigating the Impact of Adversarial Attacks in Very Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Neural Network (DNN) models have vulnerabilities related to security\nconcerns, with attackers usually employing complex hacking techniques to expose\ntheir structures. Data poisoning-enabled perturbation attacks are complex\nadversarial ones that inject false data into models. They negatively impact the\nlearning process, with no benefit to deeper networks, as they degrade a model's\naccuracy and convergence rates. In this paper, we propose an\nattack-agnostic-based defense method for mitigating their influence. In it, a\nDefensive Feature Layer (DFL) is integrated with a well-known DNN architecture\nwhich assists in neutralizing the effects of illegitimate perturbation samples\nin the feature space. To boost the robustness and trustworthiness of this\nmethod for correctly classifying attacked input samples, we regularize the\nhidden space of a trained model with a discriminative loss function called\nPolarized Contrastive Loss (PCL). It improves discrimination among samples in\ndifferent classes and maintains the resemblance of those in the same class.\nAlso, we integrate a DFL and PCL in a compact model for defending against data\npoisoning attacks. This method is trained and tested using the CIFAR-10 and\nMNIST datasets with data poisoning-enabled perturbation attacks, with the\nexperimental results revealing its excellent performance compared with those of\nrecent peer techniques.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 21:25:44 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Hassanin", "Mohammed", ""], ["Radwan", "Ibrahim", ""], ["Moustafa", "Nour", ""], ["Tahtali", "Murat", ""], ["Kumar", "Neeraj", ""]]}, {"id": "2012.04764", "submitter": "Mohammad Havaei", "authors": "Mohammad Havaei, Ximeng Mao, Yiping Wang, Qicheng Lao", "title": "Conditional Generation of Medical Images via Disentangled Adversarial\n  Inference", "comments": "Accepted by Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Synthetic medical image generation has a huge potential for improving\nhealthcare through many applications, from data augmentation for training\nmachine learning systems to preserving patient privacy. Conditional Adversarial\nGenerative Networks (cGANs) use a conditioning factor to generate images and\nhave shown great success in recent years. Intuitively, the information in an\nimage can be divided into two parts: 1) content which is presented through the\nconditioning vector and 2) style which is the undiscovered information missing\nfrom the conditioning vector. Current practices in using cGANs for medical\nimage generation, only use a single variable for image generation (i.e.,\ncontent) and therefore, do not provide much flexibility nor control over the\ngenerated image. In this work we propose a methodology to learn from the image\nitself, disentangled representations of style and content, and use this\ninformation to impose control over the generation process. In this framework,\nstyle is learned in a fully unsupervised manner, while content is learned\nthrough both supervised learning (using the conditioning vector) and\nunsupervised learning (with the inference mechanism). We undergo two novel\nregularization steps to ensure content-style disentanglement. First, we\nminimize the shared information between content and style by introducing a\nnovel application of the gradient reverse layer (GRL); second, we introduce a\nself-supervised regularization method to further separate information in the\ncontent and style variables. We show that in general, two latent variable\nmodels achieve better performance and give more control over the generated\nimage. We also show that our proposed model (DRAI) achieves the best\ndisentanglement score and has the best overall performance.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 22:10:04 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 02:35:19 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Havaei", "Mohammad", ""], ["Mao", "Ximeng", ""], ["Wang", "Yiping", ""], ["Lao", "Qicheng", ""]]}, {"id": "2012.04781", "submitter": "Edgar Sch\\\"onfeld", "authors": "Vadim Sushko, Edgar Sch\\\"onfeld, Dan Zhang, Juergen Gall, Bernt\n  Schiele, Anna Khoreva", "title": "You Only Need Adversarial Supervision for Semantic Image Synthesis", "comments": "Published at ICLR 2021 (Main Conference). Code repository:\n  https://github.com/boschresearch/OASIS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their recent successes, GAN models for semantic image synthesis still\nsuffer from poor image quality when trained with only adversarial supervision.\nHistorically, additionally employing the VGG-based perceptual loss has helped\nto overcome this issue, significantly improving the synthesis quality, but at\nthe same time limiting the progress of GAN models for semantic image synthesis.\nIn this work, we propose a novel, simplified GAN model, which needs only\nadversarial supervision to achieve high quality results. We re-design the\ndiscriminator as a semantic segmentation network, directly using the given\nsemantic label maps as the ground truth for training. By providing stronger\nsupervision to the discriminator as well as to the generator through spatially-\nand semantically-aware discriminator feedback, we are able to synthesize images\nof higher fidelity with better alignment to their input label maps, making the\nuse of the perceptual loss superfluous. Moreover, we enable high-quality\nmulti-modal image synthesis through global and local sampling of a 3D noise\ntensor injected into the generator, which allows complete or partial image\nchange. We show that images synthesized by our model are more diverse and\nfollow the color and texture distributions of real images more closely. We\nachieve an average improvement of $6$ FID and $5$ mIoU points over the state of\nthe art across different datasets using only adversarial supervision.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 23:00:48 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 22:31:50 GMT"}, {"version": "v3", "created": "Fri, 19 Mar 2021 23:35:42 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sushko", "Vadim", ""], ["Sch\u00f6nfeld", "Edgar", ""], ["Zhang", "Dan", ""], ["Gall", "Juergen", ""], ["Schiele", "Bernt", ""], ["Khoreva", "Anna", ""]]}, {"id": "2012.04794", "submitter": "Zhibo Zhang", "authors": "Zhibo Zhang, Chen Zeng, Maulikkumar Dhameliya, Souma Chowdhury, Rahul\n  Rai", "title": "Deep Learning based Multi-Modal Sensing for Tracking and State\n  Extraction of Small Quadcopters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a multi-sensor based approach to detect, track, and\nlocalize a quadcopter unmanned aerial vehicle (UAV). Specifically, a pipeline\nis developed to process monocular RGB and thermal video (captured from a fixed\nplatform) to detect and track the UAV in our FoV. Subsequently, a 2D planar\nlidar is used to allow conversion of pixel data to actual distance\nmeasurements, and thereby enable localization of the UAV in global coordinates.\nThe monocular data is processed through a deep learning-based object detection\nmethod that computes an initial bounding box for the UAV. The thermal data is\nprocessed through a thresholding and Kalman filter approach to detect and track\nthe bounding box. Training and testing data are prepared by combining a set of\noriginal experiments conducted in a motion capture environment and publicly\navailable UAV image data. The new pipeline compares favorably to existing\nmethods and demonstrates promising tracking and localization capacity of sample\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 23:59:48 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Zhang", "Zhibo", ""], ["Zeng", "Chen", ""], ["Dhameliya", "Maulikkumar", ""], ["Chowdhury", "Souma", ""], ["Rai", "Rahul", ""]]}, {"id": "2012.04828", "submitter": "Inkyu Shin", "authors": "Inkyu Shin, Sanghyun Woo, Fei Pan and InSo Kweon", "title": "Two-phase Pseudo Label Densification for Self-training based Domain\n  Adaptation", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, deep self-training approaches emerged as a powerful solution to the\nunsupervised domain adaptation. The self-training scheme involves iterative\nprocessing of target data; it generates target pseudo labels and retrains the\nnetwork. However, since only the confident predictions are taken as pseudo\nlabels, existing self-training approaches inevitably produce sparse pseudo\nlabels in practice. We see this is critical because the resulting insufficient\ntraining-signals lead to a suboptimal, error-prone model. In order to tackle\nthis problem, we propose a novel Two-phase Pseudo Label Densification\nframework, referred to as TPLD. In the first phase, we use sliding window\nvoting to propagate the confident predictions, utilizing intrinsic\nspatial-correlations in the images. In the second phase, we perform a\nconfidence-based easy-hard classification. For the easy samples, we now employ\ntheir full pseudo labels. For the hard ones, we instead adopt adversarial\nlearning to enforce hard-to-easy feature alignment. To ease the training\nprocess and avoid noisy predictions, we introduce the bootstrapping mechanism\nto the original self-training loss. We show the proposed TPLD can be easily\nintegrated into existing self-training based approaches and improves the\nperformance significantly. Combined with the recently proposed CRST\nself-training framework, we achieve new state-of-the-art results on two\nstandard UDA benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 02:35:25 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Shin", "Inkyu", ""], ["Woo", "Sanghyun", ""], ["Pan", "Fei", ""], ["Kweon", "InSo", ""]]}, {"id": "2012.04829", "submitter": "Jun Wang", "authors": "Jun Wang, Shaoguo Wen, Kaixing Chen, Jianghua Yu, Xin Zhou, Peng Gao,\n  Changsheng Li, Guotong Xie", "title": "Semi-supervised Active Learning for Instance Segmentation via Scoring\n  Predictions", "comments": "13 pages, 7 figures, accepted for presentation at BMVC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Active learning generally involves querying the most representative samples\nfor human labeling, which has been widely studied in many fields such as image\nclassification and object detection. However, its potential has not been\nexplored in the more complex instance segmentation task that usually has\nrelatively higher annotation cost. In this paper, we propose a novel and\nprincipled semi-supervised active learning framework for instance segmentation.\nSpecifically, we present an uncertainty sampling strategy named Triplet Scoring\nPredictions (TSP) to explicitly incorporate samples ranking clues from classes,\nbounding boxes and masks. Moreover, we devise a progressive pseudo labeling\nregime using the above TSP in semi-supervised manner, it can leverage both the\nlabeled and unlabeled data to minimize labeling effort while maximize\nperformance of instance segmentation. Results on medical images datasets\ndemonstrate that the proposed method results in the embodiment of knowledge\nfrom available data in a meaningful way. The extensive quantitatively and\nqualitatively experiments show that, our method can yield the best-performing\nmodel with notable less annotation costs, compared with state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 02:36:52 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Wang", "Jun", ""], ["Wen", "Shaoguo", ""], ["Chen", "Kaixing", ""], ["Yu", "Jianghua", ""], ["Zhou", "Xin", ""], ["Gao", "Peng", ""], ["Li", "Changsheng", ""], ["Xie", "Guotong", ""]]}, {"id": "2012.04830", "submitter": "Xiaoqing Zhang", "authors": "Xiaoqing Zhang, Yan Hu, Jiansheng Fang, Zunjie Xiao, Risa Higashita\n  and Jiang Liu", "title": "Machine Learning for Cataract Classification and Grading on Ophthalmic\n  Imaging Modalities: A Survey", "comments": "15 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cataract is one of the leading causes of reversible visual impairment and\nblindness globally. Over the years, researchers have achieved significant\nprogress in developing state-of-the-art artificial intelligence techniques for\nautomatic cataract classification and grading, helping clinicians prevent and\ntreat cataract in time. This paper provides a comprehensive survey of recent\nadvances in machine learning for cataract classification and grading based on\nophthalmic images. We summarize existing literature from two research\ndirections: conventional machine learning techniques and deep learning\ntechniques. This paper also provides insights into existing works of both\nmerits and limitations. In addition, we discuss several challenges of automatic\ncataract classification and grading based on machine learning techniques and\npresent possible solutions to these challenges for future research.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 02:37:41 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 11:46:18 GMT"}, {"version": "v3", "created": "Sat, 26 Jun 2021 10:59:38 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhang", "Xiaoqing", ""], ["Hu", "Yan", ""], ["Fang", "Jiansheng", ""], ["Xiao", "Zunjie", ""], ["Higashita", "Risa", ""], ["Liu", "Jiang", ""]]}, {"id": "2012.04835", "submitter": "Pengxiang Wu", "authors": "Pengxiang Wu, Songzhu Zheng, Mayank Goswami, Dimitris Metaxas, Chao\n  Chen", "title": "A Topological Filter for Learning with Label Noise", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy labels can impair the performance of deep neural networks. To tackle\nthis problem, in this paper, we propose a new method for filtering label noise.\nUnlike most existing methods relying on the posterior probability of a noisy\nclassifier, we focus on the much richer spatial behavior of data in the latent\nrepresentational space. By leveraging the high-order topological information of\ndata, we are able to collect most of the clean data and train a high-quality\nmodel. Theoretically we prove that this topological approach is guaranteed to\ncollect the clean data with high probability. Empirical results show that our\nmethod outperforms the state-of-the-arts and is robust to a broad spectrum of\nnoise types and levels.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 02:52:45 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Wu", "Pengxiang", ""], ["Zheng", "Songzhu", ""], ["Goswami", "Mayank", ""], ["Metaxas", "Dimitris", ""], ["Chen", "Chao", ""]]}, {"id": "2012.04837", "submitter": "Chaoqin Huang", "authors": "Fei Ye, Huangjie Zheng, Chaoqin Huang, Ya Zhang", "title": "Deep Unsupervised Image Anomaly Detection: An Information Theoretic\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Surrogate task based methods have recently shown great promise for\nunsupervised image anomaly detection. However, there is no guarantee that the\nsurrogate tasks share the consistent optimization direction with anomaly\ndetection. In this paper, we return to a direct objective function for anomaly\ndetection with information theory, which maximizes the distance between normal\nand anomalous data in terms of the joint distribution of images and their\nrepresentation. Unfortunately, this objective function is not directly\noptimizable under the unsupervised setting where no anomalous data is provided\nduring training. Through mathematical analysis of the above objective function,\nwe manage to decompose it into four components. In order to optimize in an\nunsupervised fashion, we show that, under the assumption that distribution of\nthe normal and anomalous data are separable in the latent space, its lower\nbound can be considered as a function which weights the trade-off between\nmutual information and entropy. This objective function is able to explain why\nthe surrogate task based methods are effective for anomaly detection and\nfurther point out the potential direction of improvement. Based on this object\nfunction we introduce a novel information theoretic framework for unsupervised\nimage anomaly detection. Extensive experiments have demonstrated that the\nproposed framework significantly outperforms several state-of-the-arts on\nmultiple benchmark data sets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 03:07:00 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Ye", "Fei", ""], ["Zheng", "Huangjie", ""], ["Huang", "Chaoqin", ""], ["Zhang", "Ya", ""]]}, {"id": "2012.04841", "submitter": "Rui Fan", "authors": "Rui Fan, Christopher Bowd, Nicole Brye, Mark Christopher, Robert N.\n  Weinreb, David Kriegman, Linda Zangwill", "title": "One-Vote Veto: Semi-Supervised Learning for Low-Shot Glaucoma Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are a promising technique for automated\nglaucoma diagnosis from images of the fundus, and these images are routinely\nacquired as part of an ophthalmic exam. Nevertheless, CNNs typically require a\nlarge amount of well-labeled data for training, which may not be available in\nmany biomedical image classification applications, especially when diseases are\nrare and where labeling by experts is costly.\n  This paper makes two contributions to address this issue: (1) It extends the\nconventional twin neural network and introduces a training method for low-shot\nlearning when labeled data are limited and imbalanced, and (2) it introduces a\nnovel semi-supervised learning strategy that uses additional unlabeled training\ndata to achieve greater accuracy. Our proposed multi-task twin neural network\n(MTTNN) can employ any backbone CNN, and we demonstrate with four backbone CNNs\nthat its accuracy with limited training data approaches the accuracy of\nbackbone CNNs trained with a dataset that is 50 times larger. We also introduce\nOne-Vote Veto (OVV) self-training, a semi-supervised learning strategy that is\ndesigned specifically for MTTNNs. By taking both self-predictions and\ncontrastive-predictions of the unlabeled training data into account, OVV\nself-training provides additional pseudo labels for fine tuning a pretrained\nMTTNN. Using a large (imbalanced) dataset with 66715 fundus photographs\nacquired over 15 years, extensive experimental results demonstrate the\neffectiveness of low-shot learning with MTTNN and semi-supervised learning with\nOVV self-training. Three additional, smaller clinical datasets of fundus images\nacquired under different conditions (cameras, instruments, locations,\npopulations) are used to demonstrate the generalizability of the proposed\nmethods. Source code and pretrained models will be publicly available upon\npublication.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 03:20:06 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 06:19:00 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 20:32:41 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Fan", "Rui", ""], ["Bowd", "Christopher", ""], ["Brye", "Nicole", ""], ["Christopher", "Mark", ""], ["Weinreb", "Robert N.", ""], ["Kriegman", "David", ""], ["Zangwill", "Linda", ""]]}, {"id": "2012.04842", "submitter": "Shuhan Tan", "authors": "Shuhan Tan, Yujun Shen, Bolei Zhou", "title": "Improving the Fairness of Deep Generative Models without Retraining", "comments": "Project page: https://genforce.github.io/fairgen/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) advance face synthesis through\nlearning the underlying distribution of observed data. Despite the high-quality\ngenerated faces, some minority groups can be rarely generated from the trained\nmodels due to a biased image generation process. To study the issue, we first\nconduct an empirical study on a pre-trained face synthesis model. We observe\nthat after training the GAN model not only carries the biases in the training\ndata but also amplifies them to some degree in the image generation process. To\nfurther improve the fairness of image generation, we propose an interpretable\nbaseline method to balance the output facial attributes without retraining. The\nproposed method shifts the interpretable semantic distribution in the latent\nspace for a more balanced image generation while preserving the sample\ndiversity. Besides producing more balanced data regarding a particular\nattribute (e.g., race, gender, etc.), our method is generalizable to handle\nmore than one attribute at a time and synthesize samples of fine-grained\nsubgroups. We further show the positive applicability of the balanced data\nsampled from GANs to quantify the biases in other face recognition systems,\nlike commercial face attribute classifiers and face super-resolution\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 03:20:41 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 08:55:12 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Tan", "Shuhan", ""], ["Shen", "Yujun", ""], ["Zhou", "Bolei", ""]]}, {"id": "2012.04846", "submitter": "Shaoli Huang", "authors": "Shaoli Huang, Xinchao Wang, Dacheng Tao", "title": "SnapMix: Semantically Proportional Mixing for Augmenting Fine-grained\n  Data", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data mixing augmentation has proved effective in training deep models. Recent\nmethods mix labels mainly based on the mixture proportion of image pixels. As\nthe main discriminative information of a fine-grained image usually resides in\nsubtle regions, methods along this line are prone to heavy label noise in\nfine-grained recognition. We propose in this paper a novel scheme, termed as\nSemantically Proportional Mixing (SnapMix), which exploits class activation map\n(CAM) to lessen the label noise in augmenting fine-grained data. SnapMix\ngenerates the target label for a mixed image by estimating its intrinsic\nsemantic composition, and allows for asymmetric mixing operations and ensures\nsemantic correspondence between synthetic images and target labels. Experiments\nshow that our method consistently outperforms existing mixed-based approaches\non various datasets and under different network depths. Furthermore, by\nincorporating the mid-level features, the proposed SnapMix achieves top-level\nperformance, demonstrating its potential to serve as a solid baseline for\nfine-grained recognition. Our code is available at\nhttps://github.com/Shaoli-Huang/SnapMix.git.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 03:37:30 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Huang", "Shaoli", ""], ["Wang", "Xinchao", ""], ["Tao", "Dacheng", ""]]}, {"id": "2012.04863", "submitter": "Pengtao Xie", "authors": "Pengtao Xie, Xuefeng Du, Hao Ban", "title": "Skillearn: Machine Learning Inspired by Humans' Learning Skills", "comments": "arXiv admin note: substantial text overlap with arXiv:2011.15102,\n  arXiv:2012.12502, arXiv:2012.12899", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans, as the most powerful learners on the planet, have accumulated a lot\nof learning skills, such as learning through tests, interleaving learning,\nself-explanation, active recalling, to name a few. These learning skills and\nmethodologies enable humans to learn new topics more effectively and\nefficiently. We are interested in investigating whether humans' learning skills\ncan be borrowed to help machines to learn better. Specifically, we aim to\nformalize these skills and leverage them to train better machine learning (ML)\nmodels. To achieve this goal, we develop a general framework -- Skillearn,\nwhich provides a principled way to represent humans' learning skills\nmathematically and use the formally-represented skills to improve the training\nof ML models. In two case studies, we apply Skillearn to formalize two learning\nskills of humans: learning by passing tests and interleaving learning, and use\nthe formalized skills to improve neural architecture search. Experiments on\nvarious datasets show that trained using the skills formalized by Skillearn, ML\nmodels achieve significantly better performance.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 04:56:22 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 06:38:40 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Xie", "Pengtao", ""], ["Du", "Xuefeng", ""], ["Ban", "Hao", ""]]}, {"id": "2012.04872", "submitter": "Jinzheng Cai", "authors": "Jinzheng Cai, Youbao Tang, Ke Yan, Adam P. Harrison, Jing Xiao, Gigin\n  Lin, Le Lu", "title": "Deep Lesion Tracker: Monitoring Lesions in 4D Longitudinal Imaging\n  Studies", "comments": "Accepted by CVPR2021. Main manuscript: 11 pages, 4 figures, and 5\n  tables. Supplementary materials: 6 pages, 5 figures, and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring treatment response in longitudinal studies plays an important role\nin clinical practice. Accurately identifying lesions across serial imaging\nfollow-up is the core to the monitoring procedure. Typically this incorporates\nboth image and anatomical considerations. However, matching lesions manually is\nlabor-intensive and time-consuming. In this work, we present deep lesion\ntracker (DLT), a deep learning approach that uses both appearance- and\nanatomical-based signals. To incorporate anatomical constraints, we propose an\nanatomical signal encoder, which prevents lesions being matched with visually\nsimilar but spurious regions. In addition, we present a new formulation for\nSiamese networks that avoids the heavy computational loads of 3D\ncross-correlation. To present our network with greater varieties of images, we\nalso propose a self-supervised learning (SSL) strategy to train trackers with\nunpaired images, overcoming barriers to data collection. To train and evaluate\nour tracker, we introduce and release the first lesion tracking benchmark,\nconsisting of 3891 lesion pairs from the public DeepLesion database. The\nproposed method, DLT, locates lesion centers with a mean error distance of 7\nmm. This is 5% better than a leading registration algorithm while running 14\ntimes faster on whole CT volumes. We demonstrate even greater improvements over\ndetector or similarity-learning alternatives. DLT also generalizes well on an\nexternal clinical test set of 100 longitudinal studies, achieving 88% accuracy.\nFinally, we plug DLT into an automatic tumor monitoring workflow where it leads\nto an accuracy of 85% in assessing lesion treatment responses, which is only\n0.46% lower than the accuracy of manual inputs.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 05:23:46 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 16:53:41 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Cai", "Jinzheng", ""], ["Tang", "Youbao", ""], ["Yan", "Ke", ""], ["Harrison", "Adam P.", ""], ["Xiao", "Jing", ""], ["Lin", "Gigin", ""], ["Lu", "Le", ""]]}, {"id": "2012.04878", "submitter": "Qiwei Li", "authors": "Esteban Fern\\'andez Morales and Cong Zhang and Guanghua Xiao and Chul\n  Moon and Qiwei Li", "title": "Discovering Clinically Meaningful Shape Features for the Analysis of\n  Tumor Pathology Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advanced imaging technology, digital pathology imaging of tumor\ntissue slides is becoming a routine clinical procedure for cancer diagnosis.\nThis process produces massive imaging data that capture histological details in\nhigh resolution. Recent developments in deep-learning methods have enabled us\nto automatically detect and characterize the tumor regions in pathology images\nat large scale. From each identified tumor region, we extracted 30 well-defined\ndescriptors that quantify its shape, geometry, and topology. We demonstrated\nhow those descriptor features were associated with patient survival outcome in\nlung adenocarcinoma patients from the National Lung Screening Trial (n=143).\nBesides, a descriptor-based prognostic model was developed and validated in an\nindependent patient cohort from The Cancer Genome Atlas Program program\n(n=318). This study proposes new insights into the relationship between tumor\nshape, geometrical, and topological features and patient prognosis. We provide\nsoftware in the form of R code on GitHub:\nhttps://github.com/estfernandez/Slide_Image_Segmentation_and_Extraction.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 05:56:41 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Morales", "Esteban Fern\u00e1ndez", ""], ["Zhang", "Cong", ""], ["Xiao", "Guanghua", ""], ["Moon", "Chul", ""], ["Li", "Qiwei", ""]]}, {"id": "2012.04880", "submitter": "Somali Chaterji", "authors": "Karthick Shankar, Pengcheng Wang, Ran Xu, Ashraf Mahgoub, Somali\n  Chaterji", "title": "JANUS: Benchmarking Commercial and Open-Source Cloud and Edge Platforms\n  for Object and Anomaly Detection Workloads", "comments": "Appeared at the IEEE Cloud 2020 conference. 10 pages", "journal-ref": "\"JANUS: Benchmarking Commercial and Open-Source Cloud and Edge\n  Platforms for Object and Anomaly Detection Workloads,\" IEEE International\n  Conference on Cloud Computing (IEEE Cloud), pp. 1--10, Oct 18-24, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With diverse IoT workloads, placing compute and analytics close to where data\nis collected is becoming increasingly important. We seek to understand what is\nthe performance and the cost implication of running analytics on IoT data at\nthe various available platforms. These workloads can be compute-light, such as\noutlier detection on sensor data, or compute-intensive, such as object\ndetection from video feeds obtained from drones. In our paper, JANUS, we\nprofile the performance/$ and the compute versus communication cost for a\ncompute-light IoT workload and a compute-intensive IoT workload. In addition,\nwe also look at the pros and cons of some of the proprietary deep-learning\nobject detection packages, such as Amazon Rekognition, Google Vision, and Azure\nCognitive Services, to contrast with open-source and tunable solutions, such as\nFaster R-CNN (FRCNN). We find that AWS IoT Greengrass delivers at least 2X\nlower latency and 1.25X lower cost compared to all other cloud platforms for\nthe compute-light outlier detection workload. For the compute-intensive\nstreaming video analytics task, an opensource solution to object detection\nrunning on cloud VMs saves on dollar costs compared to proprietary solutions\nprovided by Amazon, Microsoft, and Google, but loses out on latency (up to 6X).\nIf it runs on a low-powered edge device, the latency is up to 49X lower.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 06:07:26 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Shankar", "Karthick", ""], ["Wang", "Pengcheng", ""], ["Xu", "Ran", ""], ["Mahgoub", "Ashraf", ""], ["Chaterji", "Somali", ""]]}, {"id": "2012.04885", "submitter": "Cheng Li", "authors": "Cheng Li, Rongpin Wang, Zaiyi Liu, Meiyun Wang, Hongna Tan, Yaping Wu,\n  Xinfeng Liu, Hui Sun, Rui Yang, Xin Liu, Ismail Ben Ayed, Hairong Zheng,\n  Hanchuan Peng, Shanshan Wang", "title": "AIDE: Annotation-efficient deep learning for automatic medical image\n  segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate image segmentation is crucial for medical imaging applications. The\nprevailing deep learning approaches typically rely on very large training\ndatasets with high-quality manual annotations, which are often not available in\nmedical imaging. We introduce Annotation-effIcient Deep lEarning (AIDE) to\nhandle imperfect datasets with an elaborately designed cross-model\nself-correcting mechanism. AIDE improves the segmentation Dice scores of\nconventional deep learning models on open datasets possessing scarce or noisy\nannotations by up to 30%. For three clinical datasets containing 11,852 breast\nimages of 872 patients from three medical centers, AIDE consistently produces\nsegmentation maps comparable to those generated by the fully supervised\ncounterparts as well as the manual annotations of independent radiologists by\nutilizing only 10% training annotations. Such a 10-fold improvement of\nefficiency in utilizing experts' labels has the potential to promote a wide\nrange of biomedical applications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 06:27:09 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 09:47:02 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Li", "Cheng", ""], ["Wang", "Rongpin", ""], ["Liu", "Zaiyi", ""], ["Wang", "Meiyun", ""], ["Tan", "Hongna", ""], ["Wu", "Yaping", ""], ["Liu", "Xinfeng", ""], ["Sun", "Hui", ""], ["Yang", "Rui", ""], ["Liu", "Xin", ""], ["Ayed", "Ismail Ben", ""], ["Zheng", "Hairong", ""], ["Peng", "Hanchuan", ""], ["Wang", "Shanshan", ""]]}, {"id": "2012.04886", "submitter": "Weikang Wang", "authors": "Yuting Su, Weikang Wang, Jing Liu, Peiguang Jing and Xiaokang Yang", "title": "DS-Net: Dynamic Spatiotemporal Network for Video Salient Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As moving objects always draw more attention of human eyes, the temporal\nmotive information is always exploited complementarily with spatial information\nto detect salient objects in videos. Although efficient tools such as optical\nflow have been proposed to extract temporal motive information, it often\nencounters difficulties when used for saliency detection due to the movement of\ncamera or the partial movement of salient objects. In this paper, we\ninvestigate the complimentary roles of spatial and temporal information and\npropose a novel dynamic spatiotemporal network (DS-Net) for more effective\nfusion of spatiotemporal information. We construct a symmetric two-bypass\nnetwork to explicitly extract spatial and temporal features. A dynamic weight\ngenerator (DWG) is designed to automatically learn the reliability of\ncorresponding saliency branch. And a top-down cross attentive aggregation (CAA)\nprocedure is designed so as to facilitate dynamic complementary aggregation of\nspatiotemporal features. Finally, the features are modified by spatial\nattention with the guidance of coarse saliency map and then go through decoder\npart for final saliency map. Experimental results on five benchmarks VOS,\nDAVIS, FBMS, SegTrack-v2, and ViSal demonstrate that the proposed method\nachieves superior performance than state-of-the-art algorithms. The source code\nis available at https://github.com/TJUMMG/DS-Net.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 06:42:30 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Su", "Yuting", ""], ["Wang", "Weikang", ""], ["Liu", "Jing", ""], ["Jing", "Peiguang", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2012.04902", "submitter": "Cihan \\\"Ong\\\"un", "authors": "Hilmi Kumdakc{\\i}, Cihan \\\"Ong\\\"un, Alptekin Temizel", "title": "Generative Data Augmentation for Vehicle Detection in Aerial Images", "comments": "Workshop on Analysis of Aerial Motion Imagery (WAAMI 2020) in\n  conjunction with 25th International Conference on Pattern Recognition (ICPR\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scarcity of training data is one of the prominent problems for deep networks\nwhich require large amounts data. Data augmentation is a widely used method to\nincrease the number of training samples and their variations. In this paper, we\nfocus on improving vehicle detection performance in aerial images and propose a\ngenerative augmentation method which does not need any extra supervision than\nthe bounding box annotations of the vehicle objects in the training dataset.\nThe proposed method increases the performance of vehicle detection by allowing\ndetectors to be trained with higher number of instances, especially when there\nare limited number of training instances. The proposed method is generic in the\nsense that it can be integrated with different generators. The experiments show\nthat the method increases the Average Precision by up to 25.2% and 25.7% when\nintegrated with Pluralistic and DeepFill respectively.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 08:03:40 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Kumdakc\u0131", "Hilmi", ""], ["\u00d6ng\u00fcn", "Cihan", ""], ["Temizel", "Alptekin", ""]]}, {"id": "2012.04905", "submitter": "Chaoqin Huang", "authors": "Chaoqin Huang, Fei Ye, Ya Zhang, Yan-Feng Wang, Qi Tian", "title": "ESAD: End-to-end Deep Semi-supervised Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores semi-supervised anomaly detection, a more practical\nsetting for anomaly detection where a small additional set of labeled samples\nare provided. Based on the analysis of Deep SAD, the state-of-the-art for\nsemi-supervised anomaly detection, we propose a new KL-divergence based\nobjective function and show that two factors: the mutual information between\nthe data and latent representations, and the entropy of latent representations,\nconstitute an integral objective function for anomaly detection. To resolve the\ncontradiction in simultaneously optimizing the two factors, we propose a novel\nencoder-decoder-encoder structure, with the first encoder focusing on\noptimizing the mutual information and the second encoder focusing on optimizing\nthe entropy. The two encoders are enforced to share similar encoding with a\nconsistent constraint on their latent representations. Extensive experiments\nhave revealed that the proposed method significantly outperforms several\nstate-of-the-arts on multiple benchmark datasets, including medical diagnosis\nand several classic anomaly detection benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 08:16:35 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 11:18:36 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Huang", "Chaoqin", ""], ["Ye", "Fei", ""], ["Zhang", "Ya", ""], ["Wang", "Yan-Feng", ""], ["Tian", "Qi", ""]]}, {"id": "2012.04915", "submitter": "Chengchao Shen", "authors": "Chengchao Shen, Xinchao Wang, Youtan Yin, Jie Song, Sihui Luo, Mingli\n  Song", "title": "Progressive Network Grafting for Few-Shot Knowledge Distillation", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge distillation has demonstrated encouraging performances in deep\nmodel compression. Most existing approaches, however, require massive labeled\ndata to accomplish the knowledge transfer, making the model compression a\ncumbersome and costly process. In this paper, we investigate the practical\nfew-shot knowledge distillation scenario, where we assume only a few samples\nwithout human annotations are available for each category. To this end, we\nintroduce a principled dual-stage distillation scheme tailored for few-shot\ndata. In the first step, we graft the student blocks one by one onto the\nteacher, and learn the parameters of the grafted block intertwined with those\nof the other teacher blocks. In the second step, the trained student blocks are\nprogressively connected and then together grafted onto the teacher network,\nallowing the learned student blocks to adapt themselves to each other and\neventually replace the teacher network. Experiments demonstrate that our\napproach, with only a few unlabeled samples, achieves gratifying results on\nCIFAR10, CIFAR100, and ILSVRC-2012. On CIFAR10 and CIFAR100, our performances\nare even on par with those of knowledge distillation schemes that utilize the\nfull datasets. The source code is available at\nhttps://github.com/zju-vipa/NetGraft.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 08:34:36 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 07:38:41 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Shen", "Chengchao", ""], ["Wang", "Xinchao", ""], ["Yin", "Youtan", ""], ["Song", "Jie", ""], ["Luo", "Sihui", ""], ["Song", "Mingli", ""]]}, {"id": "2012.04920", "submitter": "Valero Laparra", "authors": "Jos\\'e A. Padr\\'on-Hidalgo and Valero Laparra and Nathan Longbotham\n  and Gustau Camps-Valls", "title": "Kernel Anomalous Change Detection for Remote Sensing Imagery", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing ( Volume: 57,\n  Issue: 10, Oct. 2019)", "doi": "10.1109/TGRS.2019.2916212", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Anomalous change detection (ACD) is an important problem in remote sensing\nimage processing. Detecting not only pervasive but also anomalous or extreme\nchanges has many applications for which methodologies are available. This paper\nintroduces a nonlinear extension of a full family of anomalous change\ndetectors. In particular, we focus on algorithms that utilize Gaussian and\nelliptically contoured (EC) distribution and extend them to their nonlinear\ncounterparts based on the theory of reproducing kernels' Hilbert space. We\nillustrate the performance of the kernel methods introduced in both pervasive\nand ACD problems with real and simulated changes in multispectral and\nhyperspectral imagery with different resolutions (AVIRIS, Sentinel-2,\nWorldView-2, and Quickbird). A wide range of situations is studied in real\nexamples, including droughts, wildfires, and urbanization. Excellent\nperformance in terms of detection accuracy compared to linear formulations is\nachieved, resulting in improved detection accuracy and reduced false-alarm\nrates. Results also reveal that the EC assumption may be still valid in Hilbert\nspaces. We provide an implementation of the algorithms as well as a database of\nnatural anomalous changes in real scenarios http://isp.uv.es/kacd.html.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 08:57:36 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Padr\u00f3n-Hidalgo", "Jos\u00e9 A.", ""], ["Laparra", "Valero", ""], ["Longbotham", "Nathan", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.04925", "submitter": "Aozhu Chen", "authors": "Aozhu Chen, Xinyi Huang, Hailan Lin, Xirong Li", "title": "Towards Annotation-Free Evaluation of Cross-Lingual Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual image captioning, with its ability to caption an unlabeled\nimage in a target language other than English, is an emerging topic in the\nmultimedia field. In order to save the precious human resource from re-writing\nreference sentences per target language, in this paper we make a brave attempt\ntowards annotation-free evaluation of cross-lingual image captioning. Depending\non whether we assume the availability of English references, two scenarios are\ninvestigated. For the first scenario with the references available, we propose\ntwo metrics, i.e., WMDRel and CLinRel. WMDRel measures the semantic relevance\nbetween a model-generated caption and machine translation of an English\nreference using their Word Mover's Distance. By projecting both captions into a\ndeep visual feature space, CLinRel is a visual-oriented cross-lingual relevance\nmeasure. As for the second scenario, which has zero reference and is thus more\nchallenging, we propose CMedRel to compute a cross-media relevance between the\ngenerated caption and the image content, in the same visual feature space as\nused by CLinRel. The promising results show high potential of the new metrics\nfor evaluation with no need of references in the target language.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 09:09:22 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Chen", "Aozhu", ""], ["Huang", "Xinyi", ""], ["Lin", "Hailan", ""], ["Li", "Xirong", ""]]}, {"id": "2012.04926", "submitter": "Chonghyuk Song", "authors": "Chonghyuk Song, Eunseok Kim, Inwook Shim", "title": "Improving Gradient Flow with Unrolled Highway Expectation Maximization", "comments": "Accepted at AAAI 2021. Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating model-based machine learning methods into deep neural\narchitectures allows one to leverage both the expressive power of deep neural\nnets and the ability of model-based methods to incorporate domain-specific\nknowledge. In particular, many works have employed the expectation maximization\n(EM) algorithm in the form of an unrolled layer-wise structure that is jointly\ntrained with a backbone neural network. However, it is difficult to\ndiscriminatively train the backbone network by backpropagating through the EM\niterations as they are prone to the vanishing gradient problem. To address this\nissue, we propose Highway Expectation Maximization Networks (HEMNet), which is\ncomprised of unrolled iterations of the generalized EM (GEM) algorithm based on\nthe Newton-Rahpson method. HEMNet features scaled skip connections, or\nhighways, along the depths of the unrolled architecture, resulting in improved\ngradient flow during backpropagation while incurring negligible additional\ncomputation and memory costs compared to standard unrolled EM. Furthermore,\nHEMNet preserves the underlying EM procedure, thereby fully retaining the\nconvergence properties of the original EM algorithm. We achieve significant\nimprovement in performance on several semantic segmentation benchmarks and\nempirically show that HEMNet effectively alleviates gradient decay.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 09:11:45 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Song", "Chonghyuk", ""], ["Kim", "Eunseok", ""], ["Shim", "Inwook", ""]]}, {"id": "2012.04927", "submitter": "Jun Wan", "authors": "Jun Wan, Zhihui Lai, Jing Li, Jie Zhou, Can Gao", "title": "Robust Facial Landmark Detection by Multi-order Multi-constraint Deep\n  Networks", "comments": "This paper has been accepted by TNNLS December 2020. Project\n  page:https://github.com/junwan2014/MMDN-master", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, heatmap regression has been widely explored in facial landmark\ndetection and obtained remarkable performance. However, most of the existing\nheatmap regression-based facial landmark detection methods neglect to explore\nthe high-order feature correlations, which is very important to learn more\nrepresentative features and enhance shape constraints. Moreover, no explicit\nglobal shape constraints have been added to the final predicted landmarks,\nwhich leads to a reduction in accuracy. To address these issues, in this paper,\nwe propose a Multi-order Multi-constraint Deep Network (MMDN) for more powerful\nfeature correlations and shape constraints learning. Specifically, an Implicit\nMulti-order Correlating Geometry-aware (IMCG) model is proposed to introduce\nthe multi-order spatial correlations and multi-order channel correlations for\nmore discriminative representations. Furthermore, an Explicit Probability-based\nBoundary-adaptive Regression (EPBR) method is developed to enhance the global\nshape constraints and further search the semantically consistent landmarks in\nthe predicted boundary for robust facial landmark detection. It's interesting\nto show that the proposed MMDN can generate more accurate boundary-adaptive\nlandmark heatmaps and effectively enhance shape constraints to the predicted\nlandmarks for faces with large pose variations and heavy occlusions.\nExperimental results on challenging benchmark datasets demonstrate the\nsuperiority of our MMDN over state-of-the-art facial landmark detection\nmethods. The code has been publicly available at\nhttps://github.com/junwan2014/MMDN-master.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 09:11:47 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 23:39:38 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Wan", "Jun", ""], ["Lai", "Zhihui", ""], ["Li", "Jing", ""], ["Zhou", "Jie", ""], ["Gao", "Can", ""]]}, {"id": "2012.04932", "submitter": "Zhiwei Jia", "authors": "Zhiwei Jia, Bodi Yuan, Kangkang Wang, Hong Wu, David Clifford,\n  Zhiqiang Yuan, Hao Su", "title": "Lipschitz Regularized CycleGAN for Improving Semantic Robustness in\n  Unpaired Image-to-image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For unpaired image-to-image translation tasks, GAN-based approaches are\nsusceptible to semantic flipping, i.e., contents are not preserved\nconsistently. We argue that this is due to (1) the difference in semantic\nstatistics between source and target domains and (2) the learned generators\nbeing non-robust. In this paper, we proposed a novel approach, Lipschitz\nregularized CycleGAN, for improving semantic robustness and thus alleviating\nthe semantic flipping issue. During training, we add a gradient penalty loss to\nthe generators, which encourages semantically consistent transformations. We\nevaluate our approach on multiple common datasets and compare with several\nexisting GAN-based methods. Both quantitative and visual results suggest the\neffectiveness and advantage of our approach in producing robust transformations\nwith fewer semantic flipping.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 09:28:53 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Jia", "Zhiwei", ""], ["Yuan", "Bodi", ""], ["Wang", "Kangkang", ""], ["Wu", "Hong", ""], ["Clifford", "David", ""], ["Yuan", "Zhiqiang", ""], ["Su", "Hao", ""]]}, {"id": "2012.04934", "submitter": "Venice Erin Liong", "authors": "Venice Erin Liong, Thi Ngoc Tho Nguyen, Sergi Widjaja, Dhananjai\n  Sharma, Zhuang Jie Chong", "title": "AMVNet: Assertion-based Multi-View Fusion Network for LiDAR Semantic\n  Segmentation", "comments": "10 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an Assertion-based Multi-View Fusion network\n(AMVNet) for LiDAR semantic segmentation which aggregates the semantic features\nof individual projection-based networks using late fusion. Given class scores\nfrom different projection-based networks, we perform assertion-guided point\nsampling on score disagreements and pass a set of point-level features for each\nsampled point to a simple point head which refines the predictions. This\nmodular-and-hierarchical late fusion approach provides the flexibility of\nhaving two independent networks with a minor overhead from a light-weight\nnetwork. Such approaches are desirable for robotic systems, e.g. autonomous\nvehicles, for which the computational and memory resources are often limited.\nExtensive experiments show that AMVNet achieves state-of-the-art results in\nboth the SemanticKITTI and nuScenes benchmark datasets and that our approach\noutperforms the baseline method of combining the class scores of the\nprojection-based networks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 09:34:25 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Liong", "Venice Erin", ""], ["Nguyen", "Thi Ngoc Tho", ""], ["Widjaja", "Sergi", ""], ["Sharma", "Dhananjai", ""], ["Chong", "Zhuang Jie", ""]]}, {"id": "2012.04937", "submitter": "Deepshikha Kumari", "authors": "Kumari Deepshikha and Anugunj Naman", "title": "Removing Class Imbalance using Polarity-GAN: An Uncertainty Sampling\n  Approach", "comments": "CVPR2021 Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Class imbalance is a challenging issue in practical classification problems\nfor deep learning models as well as for traditional models. Traditionally\nsuccessful countermeasures such as synthetic over-sampling have had limited\nsuccess with complex, structured data handled by deep learning models. In this\nwork, we propose to use a Generative Adversarial Network (GAN) equipped with a\ngenerator network G, a discriminator network D and a classifier network C to\nremove the class-imbalance in visual data sets. The generator network is\ninitialized with auto-encoder to make it stable. The discriminator D ensures\nthat G adheres to class distribution of imbalanced class. In conventional\nmethods, where Generator G competes with discriminator D in a min-max game, we\npropose to further add an additional classifier network to the original\nnetwork. Now, the generator network tries to compete in a min-max game with\nDiscriminator as well as the new classifier that we have introduced. An\nadditional condition is enforced on generator network G to produce points in\nthe convex hull of desired imbalanced class. Further the contention of\nadversarial game with classifier C, pushes conditional distribution learned by\nG towards the periphery of the respective class, compensating the problem of\nclass imbalance. Experimental evidence shows that this initialization results\nin stable training of the network. We achieve state of the art performance on\nextreme visual classification task on the FashionMNIST, MNIST, SVHN, ExDark,\nMVTec Anomaly Detection dataset, Chest X-Ray dataset and others.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 09:40:07 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Deepshikha", "Kumari", ""], ["Naman", "Anugunj", ""]]}, {"id": "2012.04951", "submitter": "Radu Horaud P", "authors": "Vasil Khalidov, Florence Forbes and Radu Horaud", "title": "Conjugate Mixture Models for Clustering Multimodal Data", "comments": null, "journal-ref": "Neural Computation, 23(2), 2011", "doi": "10.1162/NECO_a_00074", "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of multimodal clustering arises whenever the data are gathered\nwith several physically different sensors. Observations from different\nmodalities are not necessarily aligned in the sense there there is no obvious\nway to associate or to compare them in some common space. A solution may\nconsist in considering multiple clustering tasks independently for each\nmodality. The main difficulty with such an approach is to guarantee that the\nunimodal clusterings are mutually consistent. In this paper we show that\nmultimodal clustering can be addressed within a novel framework, namely\nconjugate mixture models. These models exploit the explicit transformations\nthat are often available between an unobserved parameter space (objects) and\neach one of the observation spaces (sensors). We formulate the problem as a\nlikelihood maximization task and we derive the associated conjugate\nexpectation-maximization algorithm. The convergence properties of the proposed\nalgorithm are thoroughly investigated. Several local/global optimization\ntechniques are proposed in order to increase its convergence speed. Two\ninitialization strategies are proposed and compared. A consistent\nmodel-selection criterion is proposed. The algorithm and its variants are\ntested and evaluated within the task of 3D localization of several speakers\nusing both auditory and visual data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 10:13:22 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Khalidov", "Vasil", ""], ["Forbes", "Florence", ""], ["Horaud", "Radu", ""]]}, {"id": "2012.04954", "submitter": "Denis Coquenet", "authors": "Denis Coquenet, Yann Soullard, Cl\\'ement Chatelain, Thierry Paquet", "title": "Have convolutions already made recurrence obsolete for unconstrained\n  handwritten text recognition ?", "comments": null, "journal-ref": "2019 International Conference on Document Analysis and Recognition\n  Workshops (ICDARW)", "doi": "10.1109/ICDARW.2019.40083", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unconstrained handwritten text recognition remains an important challenge for\ndeep neural networks. These last years, recurrent networks and more\nspecifically Long Short-Term Memory networks have achieved state-of-the-art\nperformance in this field. Nevertheless, they are made of a large number of\ntrainable parameters and training recurrent neural networks does not support\nparallelism. This has a direct influence on the training time of such\narchitectures, with also a direct consequence on the time required to explore\nvarious architectures. Recently, recurrence-free architectures such as Fully\nConvolutional Networks with gated mechanisms have been proposed as one possible\nalternative achieving competitive results. In this paper, we explore\nconvolutional architectures and compare them to a CNN+BLSTM baseline. We\npropose an experimental study regarding different architectures on an offline\nhandwriting recognition task using the RIMES dataset, and a modified version of\nit that consists of augmenting the images with notebook backgrounds that are\nprinted grids.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 10:15:24 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Coquenet", "Denis", ""], ["Soullard", "Yann", ""], ["Chatelain", "Cl\u00e9ment", ""], ["Paquet", "Thierry", ""]]}, {"id": "2012.04961", "submitter": "Denis Coquenet", "authors": "Denis Coquenet, Cl\\'ement Chatelain, Thierry Paquet", "title": "Recurrence-free unconstrained handwritten text recognition using gated\n  fully convolutional network", "comments": null, "journal-ref": "2020 17th International Conference on Frontiers in Handwriting\n  Recognition (ICFHR)", "doi": "10.1109/ICFHR2020.2020.00015", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unconstrained handwritten text recognition is a major step in most document\nanalysis tasks. This is generally processed by deep recurrent neural networks\nand more specifically with the use of Long Short-Term Memory cells. The main\ndrawbacks of these components are the large number of parameters involved and\ntheir sequential execution during training and prediction. One alternative\nsolution to using LSTM cells is to compensate the long time memory loss with an\nheavy use of convolutional layers whose operations can be executed in parallel\nand which imply fewer parameters. In this paper we present a Gated Fully\nConvolutional Network architecture that is a recurrence-free alternative to the\nwell-known CNN+LSTM architectures. Our model is trained with the CTC loss and\nshows competitive results on both the RIMES and IAM datasets. We release all\ncode to enable reproduction of our experiments:\nhttps://github.com/FactoDeepLearning/LinePytorchOCR.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 10:30:13 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Coquenet", "Denis", ""], ["Chatelain", "Cl\u00e9ment", ""], ["Paquet", "Thierry", ""]]}, {"id": "2012.04974", "submitter": "Caner Mercan", "authors": "Caner Mercan, Maschenka Balkenhol, Roberto Salgado, Mark Sherman,\n  Philippe Vielh, Willem Vreuls, Antonio Polonia, Hugo M. Horlings, Wilko\n  Weichert, Jodi M. Carter, Peter Bult, Matthias Christgen, Carsten Denkert,\n  Koen van de Vijver, Jeroen van der Laak, Francesco Ciompi", "title": "Automated Scoring of Nuclear Pleomorphism Spectrum with\n  Pathologist-level Performance in Breast Cancer", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclear pleomorphism, defined herein as the extent of abnormalities in the\noverall appearance of tumor nuclei, is one of the components of the\nthree-tiered breast cancer grading. Given that nuclear pleomorphism reflects a\ncontinuous spectrum of variation, we trained a deep neural network on a large\nvariety of tumor regions from the collective knowledge of several pathologists,\nwithout constraining the network to the traditional three-category\nclassification. We also motivate an additional approach in which we discuss the\nadditional benefit of normal epithelium as baseline, following the routine\nclinical practice where pathologists are trained to score nuclear pleomorphism\nin tumor, having the normal breast epithelium for comparison. In multiple\nexperiments, our fully-automated approach could achieve top pathologist-level\nperformance in select regions of interest as well as at whole slide images,\ncompared to ten and four pathologists, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 11:02:42 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 09:48:54 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Mercan", "Caner", ""], ["Balkenhol", "Maschenka", ""], ["Salgado", "Roberto", ""], ["Sherman", "Mark", ""], ["Vielh", "Philippe", ""], ["Vreuls", "Willem", ""], ["Polonia", "Antonio", ""], ["Horlings", "Hugo M.", ""], ["Weichert", "Wilko", ""], ["Carter", "Jodi M.", ""], ["Bult", "Peter", ""], ["Christgen", "Matthias", ""], ["Denkert", "Carsten", ""], ["van de Vijver", "Koen", ""], ["van der Laak", "Jeroen", ""], ["Ciompi", "Francesco", ""]]}, {"id": "2012.04977", "submitter": "Fuqing Zhu", "authors": "Weibo Zhang, Guihua Liu, Zhuohua Li, Fuqing Zhu", "title": "Hateful Memes Detection via Complementary Visual and Linguistic Networks", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hateful memes are widespread in social media and convey negative information.\nThe main challenge of hateful memes detection is that the expressive meaning\ncan not be well recognized by a single modality. In order to further integrate\nmodal information, we investigate a candidate solution based on complementary\nvisual and linguistic network in Hateful Memes Challenge 2020. In this way,\nmore comprehensive information of the multi-modality could be explored in\ndetail. Both contextual-level and sensitive object-level information are\nconsidered in visual and linguistic embedding to formulate the complex\nmulti-modal scenarios. Specifically, a pre-trained classifier and object\ndetector are utilized to obtain the contextual features and region-of-interests\n(RoIs) from the input, followed by the position representation fusion for\nvisual embedding. While linguistic embedding is composed of three components,\ni.e., the sentence words embedding, position embedding and the corresponding\nSpacy embedding (Sembedding), which is a symbol represented by vocabulary\nextracted by Spacy. Both visual and linguistic embedding are fed into the\ndesigned Complementary Visual and Linguistic (CVL) networks to produce the\nprediction for hateful memes. Experimental results on Hateful Memes Challenge\nDataset demonstrate that CVL provides a decent performance, and produces 78:48%\nand 72:95% on the criteria of AUROC and Accuracy. Code is available at\nhttps://github.com/webYFDT/hateful.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 11:11:09 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Zhang", "Weibo", ""], ["Liu", "Guihua", ""], ["Li", "Zhuohua", ""], ["Zhu", "Fuqing", ""]]}, {"id": "2012.04983", "submitter": "Eloi Zablocki", "authors": "H\\'edi Ben-Younes and \\'Eloi Zablocki and Patrick P\\'erez and Matthieu\n  Cord", "title": "Driving Behavior Explanation with Multi-level Fusion", "comments": "Accepted at NeurIPS Workshop ML4AD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this era of active development of autonomous vehicles, it becomes crucial\nto provide driving systems with the capacity to explain their decisions. In\nthis work, we focus on generating high-level driving explanations as the\nvehicle drives. We present BEEF, for BEhavior Explanation with Fusion, a deep\narchitecture which explains the behavior of a trajectory prediction model.\nSupervised by annotations of human driving decisions justifications, BEEF\nlearns to fuse features from multiple levels. Leveraging recent advances in the\nmulti-modal fusion literature, BEEF is carefully designed to model the\ncorrelations between high-level decisions features and mid-level perceptual\nfeatures. The flexibility and efficiency of our approach are validated with\nextensive experiments on the HDD and BDD-X datasets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 11:19:50 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Ben-Younes", "H\u00e9di", ""], ["Zablocki", "\u00c9loi", ""], ["P\u00e9rez", "Patrick", ""], ["Cord", "Matthieu", ""]]}, {"id": "2012.05007", "submitter": "Tianfei Zhou", "authors": "Xueyi Li, Tianfei Zhou, Jianwu Li, Yi Zhou, Zhaoxiang Zhang", "title": "Group-Wise Semantic Mining for Weakly Supervised Semantic Segmentation", "comments": "Accepted to AAAI 2021. Code: https://github.com/Lixy1997/Group-WSSS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Acquiring sufficient ground-truth supervision to train deep visual models has\nbeen a bottleneck over the years due to the data-hungry nature of deep\nlearning. This is exacerbated in some structured prediction tasks, such as\nsemantic segmentation, which requires pixel-level annotations. This work\naddresses weakly supervised semantic segmentation (WSSS), with the goal of\nbridging the gap between image-level annotations and pixel-level segmentation.\nWe formulate WSSS as a novel group-wise learning task that explicitly models\nsemantic dependencies in a group of images to estimate more reliable pseudo\nground-truths, which can be used for training more accurate segmentation\nmodels. In particular, we devise a graph neural network (GNN) for group-wise\nsemantic mining, wherein input images are represented as graph nodes, and the\nunderlying relations between a pair of images are characterized by an efficient\nco-attention mechanism. Moreover, in order to prevent the model from paying\nexcessive attention to common semantics only, we further propose a graph\ndropout layer, encouraging the model to learn more accurate and complete object\nresponses. The whole network is end-to-end trainable by iterative message\npassing, which propagates interaction cues over the images to progressively\nimprove the performance. We conduct experiments on the popular PASCAL VOC 2012\nand COCO benchmarks, and our model yields state-of-the-art performance. Our\ncode is available at: https://github.com/Lixy1997/Group-WSSS.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 12:40:13 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Li", "Xueyi", ""], ["Zhou", "Tianfei", ""], ["Li", "Jianwu", ""], ["Zhou", "Yi", ""], ["Zhang", "Zhaoxiang", ""]]}, {"id": "2012.05010", "submitter": "Haijun Liu", "authors": "Haijun Liu, Yanxia Chai, Xiaoheng Tan, Dong Li and Xichuan Zhou", "title": "Strong but Simple Baseline with Dual-Granularity Triplet Loss for\n  Visible-Thermal Person Re-Identification", "comments": "to be published in IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2021.3065903", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this letter, we propose a conceptually simple and effective\ndual-granularity triplet loss for visible-thermal person re-identification\n(VT-ReID). In general, ReID models are always trained with the sample-based\ntriplet loss and identification loss from the fine granularity level. It is\npossible when a center-based loss is introduced to encourage the intra-class\ncompactness and inter-class discrimination from the coarse granularity level.\nOur proposed dual-granularity triplet loss well organizes the sample-based\ntriplet loss and center-based triplet loss in a hierarchical fine to coarse\ngranularity manner, just with some simple configurations of typical operations,\nsuch as pooling and batch normalization. Experiments on RegDB and SYSU-MM01\ndatasets show that with only the global features our dual-granularity triplet\nloss can improve the VT-ReID performance by a significant margin. It can be a\nstrong VT-ReID baseline to boost future research with high quality.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 12:43:34 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 08:01:33 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Liu", "Haijun", ""], ["Chai", "Yanxia", ""], ["Tan", "Xiaoheng", ""], ["Li", "Dong", ""], ["Zhou", "Xichuan", ""]]}, {"id": "2012.05013", "submitter": "Shimaa Baraka", "authors": "Shimaa Baraka, Benjamin Akera, Bibek Aryal, Tenzing Sherpa, Finu\n  Shresta, Anthony Ortiz, Kris Sankaran, Juan Lavista Ferres, Mir Matin, Yoshua\n  Bengio", "title": "Machine Learning for Glacier Monitoring in the Hindu Kush Himalaya", "comments": "Accepted for a spotlight talk and a poster at the Tackling Climate\n  Change with Machine Learning workshop at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Glacier mapping is key to ecological monitoring in the hkh region. Climate\nchange poses a risk to individuals whose livelihoods depend on the health of\nglacier ecosystems. In this work, we present a machine learning based approach\nto support ecological monitoring, with a focus on glaciers. Our approach is\nbased on semi-automated mapping from satellite images. We utilize readily\navailable remote sensing data to create a model to identify and outline both\nclean ice and debris-covered glaciers from satellite imagery. We also release\ndata and develop a web tool that allows experts to visualize and correct model\npredictions, with the ultimate aim of accelerating the glacier mapping process.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 12:48:06 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Baraka", "Shimaa", ""], ["Akera", "Benjamin", ""], ["Aryal", "Bibek", ""], ["Sherpa", "Tenzing", ""], ["Shresta", "Finu", ""], ["Ortiz", "Anthony", ""], ["Sankaran", "Kris", ""], ["Ferres", "Juan Lavista", ""], ["Matin", "Mir", ""], ["Bengio", "Yoshua", ""]]}, {"id": "2012.05018", "submitter": "Zhijian Qiao", "authors": "Zhijian Qiao, Hanjiang Hu, Siyuan Chen, Zhe Liu, Zhuowen Shen, Hesheng\n  Wang", "title": "vLPD-Net: A Registration-aided Domain Adaptation Network for 3D Point\n  Cloud Based Place Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of large-scale SLAM for autonomous driving and mobile robotics,\n3D point cloud based place recognition has aroused significant research\ninterest due to its robustness to changing environments with drastic daytime\nand weather variance. However, it is time-consuming and effort-costly to obtain\nhigh-quality point cloud data and groundtruth for registration and place\nrecognition model training in the real world. To this end, a novel\nregistration-aided 3D domain adaptation network for point cloud based place\nrecognition is proposed. A structure-aware registration network is introduced\nto help learn feature from geometric properties and a matching rate based\ntriplet loss is involved for metric learning. The model is trained through a\nnew virtual LiDAR dataset through GTA-V with diverse weather and daytime\nconditions and domain adaptation is implemented to the real-world domain by\naligning the local and global features. Extensive experiments have been\nconducted to validate the effectiveness of the structure-aware registration\nnetwork and domain adaptation. Our results outperform state-of-the-art 3D place\nrecognition baselines on the real-world Oxford RobotCar dataset with the\nvisualization of large-scale registration on the virtual dataset.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 12:56:01 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Qiao", "Zhijian", ""], ["Hu", "Hanjiang", ""], ["Chen", "Siyuan", ""], ["Liu", "Zhe", ""], ["Shen", "Zhuowen", ""], ["Wang", "Hesheng", ""]]}, {"id": "2012.05027", "submitter": "Ujjwal Upadhyay", "authors": "Ujjwal Upadhyay and Prerana Mukherjee", "title": "Generating Out of Distribution Adversarial Attack using Latent Space\n  Poisoning", "comments": "Submitted to IEEE SPL", "journal-ref": null, "doi": "10.1109/LSP.2021.3061327", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional adversarial attacks rely upon the perturbations generated by\ngradients from the network which are generally safeguarded by gradient guided\nsearch to provide an adversarial counterpart to the network. In this paper, we\npropose a novel mechanism of generating adversarial examples where the actual\nimage is not corrupted rather its latent space representation is utilized to\ntamper with the inherent structure of the image while maintaining the\nperceptual quality intact and to act as legitimate data samples. As opposed to\ngradient-based attacks, the latent space poisoning exploits the inclination of\nclassifiers to model the independent and identical distribution of the training\ndataset and tricks it by producing out of distribution samples. We train a\ndisentangled variational autoencoder (beta-VAE) to model the data in latent\nspace and then we add noise perturbations using a class-conditioned\ndistribution function to the latent space under the constraint that it is\nmisclassified to the target label. Our empirical results on MNIST, SVHN, and\nCelebA dataset validate that the generated adversarial examples can easily fool\nrobust l_0, l_2, l_inf norm classifiers designed using provably robust defense\nmechanisms.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 13:05:44 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Upadhyay", "Ujjwal", ""], ["Mukherjee", "Prerana", ""]]}, {"id": "2012.05030", "submitter": "Wenqing Zhang", "authors": "Wenqing Zhang, Yang Qiu, Minghui Liao, Rui Zhang, Xiaolin Wei, Xiang\n  Bai", "title": "Scene Text Detection with Scribble Lines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text detection, which is one of the most popular topics in both\nacademia and industry, can achieve remarkable performance with sufficient\ntraining data. However, the annotation costs of scene text detection are huge\nwith traditional labeling methods due to the various shapes of texts. Thus, it\nis practical and insightful to study simpler labeling methods without harming\nthe detection performance. In this paper, we propose to annotate the texts by\nscribble lines instead of polygons for text detection. It is a general labeling\nmethod for texts with various shapes and requires low labeling costs.\nFurthermore, a weakly-supervised scene text detection framework is proposed to\nuse the scribble lines for text detection. The experiments on several\nbenchmarks show that the proposed method bridges the performance gap between\nthe weakly labeling method and the original polygon-based labeling methods,\nwith even better performance. We will release the weak annotations of the\nbenchmarks in our experiments and hope it will benefit the field of scene text\ndetection to achieve better performance with simpler annotations.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 13:14:53 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 13:35:55 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Zhang", "Wenqing", ""], ["Qiu", "Yang", ""], ["Liao", "Minghui", ""], ["Zhang", "Rui", ""], ["Wei", "Xiaolin", ""], ["Bai", "Xiang", ""]]}, {"id": "2012.05057", "submitter": "Ning Wang", "authors": "Ning Wang and Wengang Zhou and Houqiang Li", "title": "Contrastive Transformation for Self-supervised Correspondence Learning", "comments": "To appear in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the self-supervised learning of visual\ncorrespondence using unlabeled videos in the wild. Our method simultaneously\nconsiders intra- and inter-video representation associations for reliable\ncorrespondence estimation. The intra-video learning transforms the image\ncontents across frames within a single video via the frame pair-wise affinity.\nTo obtain the discriminative representation for instance-level separation, we\ngo beyond the intra-video analysis and construct the inter-video affinity to\nfacilitate the contrastive transformation across different videos. By forcing\nthe transformation consistency between intra- and inter-video levels, the\nfine-grained correspondence associations are well preserved and the\ninstance-level feature discrimination is effectively reinforced. Our simple\nframework outperforms the recent self-supervised correspondence methods on a\nrange of visual tasks including video object tracking (VOT), video object\nsegmentation (VOS), pose keypoint tracking, etc. It is worth mentioning that\nour method also surpasses the fully-supervised affinity representation (e.g.,\nResNet) and performs competitively against the recent fully-supervised\nalgorithms designed for the specific tasks (e.g., VOT and VOS).\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 14:05:06 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Wang", "Ning", ""], ["Zhou", "Wengang", ""], ["Li", "Houqiang", ""]]}, {"id": "2012.05073", "submitter": "Asifullah Khan", "authors": "Saddam Hussain Khan, Anabia Sohail, and Asifullah Khan", "title": "COVID-19 Detection in Chest X-Ray Images using a New Channel Boosted CNN", "comments": "Pages: 26 Tables: 3 Figures: 10 Equations: 8", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 is a highly contagious respiratory infection that has affected a\nlarge population across the world and continues with its devastating\nconsequences. It is imperative to detect COVID-19 at the earliest to limit the\nspan of infection. In this work, a new classification technique CB-STM-RENet\nbased on deep Convolutional Neural Network (CNN) and Channel Boosting is\nproposed for the screening of COVID-19 in chest X-Rays. In this connection, to\nlearn the COVID-19 specific radiographic patterns, a new convolution block\nbased on split-transform-merge (STM) is developed. This new block\nsystematically incorporates region and edge-based operations at each branch to\ncapture the diverse set of features at various levels, especially those related\nto region homogeneity, textural variations, and boundaries of the infected\nregion. The learning and discrimination capability of the proposed CNN\narchitecture is enhanced by exploiting the Channel Boosting idea that\nconcatenates the auxiliary channels along with the original channels. The\nauxiliary channels are generated from the pre-trained CNNs using Transfer\nLearning. The effectiveness of the proposed technique CB-STM-RENet is evaluated\non three different datasets of chest X-Rays namely CoV-Healthy-6k,\nCoV-NonCoV-10k, and CoV-NonCoV-15k. The performance comparison of the proposed\nCB-STM-RENet with the existing techniques exhibits high performance both in\ndiscriminating COVID-19 chest infections from Healthy, as well as, other types\nof chest infections. CB-STM-RENet provides the highest performance on all these\nthree datasets; especially on the stringent CoV-NonCoV-15k dataset. The good\ndetection rate (97%), and high precision (93%) of the proposed technique\nsuggest that it can be adapted for the diagnosis of COVID-19 infected patients.\nThe test code is available at\nhttps://github.com/PRLAB21/COVID-19-Detection-System-using-Chest-X-Ray-Images.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 11:23:02 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 07:16:29 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Khan", "Saddam Hussain", ""], ["Sohail", "Anabia", ""], ["Khan", "Asifullah", ""]]}, {"id": "2012.05107", "submitter": "Pranav Aggarwal", "authors": "Pranav Aggarwal, Ajinkya Kale", "title": "Towards Zero-shot Cross-lingual Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There has been a recent spike in interest in multi-modal Language and Vision\nproblems. On the language side, most of these models primarily focus on English\nsince most multi-modal datasets are monolingual. We try to bridge this gap with\na zero-shot approach for learning multi-modal representations using\ncross-lingual pre-training on the text side. We present a simple yet practical\napproach for building a cross-lingual image retrieval model which trains on a\nmonolingual training dataset but can be used in a zero-shot cross-lingual\nfashion during inference. We also introduce a new objective function which\ntightens the text embedding clusters by pushing dissimilar texts from each\nother. Finally, we introduce a new 1K multi-lingual MSCOCO2014 caption test\ndataset (XTD10) in 7 languages that we collected using a crowdsourcing\nplatform. We use this as the test set for evaluating zero-shot model\nperformance across languages. XTD10 dataset is made publicly available here:\nhttps://github.com/adobe-research/Cross-lingual-Test-Dataset-XTD10\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 22:13:21 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Aggarwal", "Pranav", ""], ["Kale", "Ajinkya", ""]]}, {"id": "2012.05116", "submitter": "Ayan Chakrabarti", "authors": "Zhihao Xia, Micha\\\"el Gharbi, Federico Perazzi, Kalyan Sunkavalli,\n  Ayan Chakrabarti", "title": "Deep Denoising of Flash and No-Flash Pairs for Photography in Low-Light\n  Environments", "comments": "CVPR 2021. Project page at\n  https://www.cse.wustl.edu/~zhihao.xia/deepfnf/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a neural network-based method to denoise pairs of images taken\nin quick succession, with and without a flash, in low-light environments. Our\ngoal is to produce a high-quality rendering of the scene that preserves the\ncolor and mood from the ambient illumination of the noisy no-flash image, while\nrecovering surface texture and detail revealed by the flash. Our network\noutputs a gain map and a field of kernels, the latter obtained by linearly\nmixing elements of a per-image low-rank kernel basis. We first apply the kernel\nfield to the no-flash image, and then multiply the result with the gain map to\ncreate the final output. We show our network effectively learns to produce\nhigh-quality images by combining a smoothed out estimate of the scene's ambient\nappearance from the no-flash image, with high-frequency albedo details\nextracted from the flash input. Our experiments show significant improvements\nover alternative captures without a flash, and baseline denoisers that use\nflash no-flash pairs. In particular, our method produces images that are both\nnoise-free and contain accurate ambient colors without the sharp shadows or\nstrong specular highlights visible in the flash image.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 15:41:16 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 19:26:22 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Xia", "Zhihao", ""], ["Gharbi", "Micha\u00ebl", ""], ["Perazzi", "Federico", ""], ["Sunkavalli", "Kalyan", ""], ["Chakrabarti", "Ayan", ""]]}, {"id": "2012.05119", "submitter": "Isinsu Katircioglu", "authors": "Isinsu Katircioglu, Helge Rhodin, J\\\"org Sp\\\"orri, Mathieu Salzmann,\n  Pascal Fua", "title": "Self-supervised Human Detection and Segmentation via Multi-view\n  Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised detection and segmentation of foreground objects in complex\nscenes is gaining attention as their fully-supervised counterparts require\noverly large amounts of annotated data to deliver sufficient accuracy in\ndomain-specific applications. However, existing self-supervised approaches\npredominantly rely on restrictive assumptions on appearance and motion, which\nprecludes their use in scenes depicting highly dynamic activities or involve\ncamera motion.\n  To mitigate this problem, we propose using a multi-camera framework in which\ngeometric constraints are embedded in the form of multi-view consistency during\ntraining via coarse 3D localization in a voxel grid and fine-grained offset\nregression. In this manner, we learn a joint distribution of proposals over\nmultiple views. At inference time, our method operates on single RGB images.\n  We show that our approach outperforms state-of-the-art self-supervised person\ndetection and segmentation techniques on images that visually depart from those\nof standard benchmarks, as well as on those of the classical Human3.6M dataset.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 15:47:21 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Katircioglu", "Isinsu", ""], ["Rhodin", "Helge", ""], ["Sp\u00f6rri", "J\u00f6rg", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "2012.05153", "submitter": "Qi Zhu", "authors": "Qi Zhu, Chenyu Gao, Peng Wang, Qi Wu", "title": "Simple is not Easy: A Simple Strong Baseline for TextVQA and TextCaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texts appearing in daily scenes that can be recognized by OCR (Optical\nCharacter Recognition) tools contain significant information, such as street\nname, product brand and prices. Two tasks -- text-based visual question\nanswering and text-based image captioning, with a text extension from existing\nvision-language applications, are catching on rapidly. To address these\nproblems, many sophisticated multi-modality encoding frameworks (such as\nheterogeneous graph structure) are being used. In this paper, we argue that a\nsimple attention mechanism can do the same or even better job without any bells\nand whistles. Under this mechanism, we simply split OCR token features into\nseparate visual- and linguistic-attention branches, and send them to a popular\nTransformer decoder to generate answers or captions. Surprisingly, we find this\nsimple baseline model is rather strong -- it consistently outperforms\nstate-of-the-art (SOTA) models on two popular benchmarks, TextVQA and all three\ntasks of ST-VQA, although these SOTA models use far more complex encoding\nmechanisms. Transferring it to text-based image captioning, we also surpass the\nTextCaps Challenge 2020 winner. We wish this work to set the new baseline for\nthis two OCR text related applications and to inspire new thinking of\nmulti-modality encoder design. Code is available at\nhttps://github.com/ZephyrZhuQi/ssbaseline\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 16:43:39 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Zhu", "Qi", ""], ["Gao", "Chenyu", ""], ["Wang", "Peng", ""], ["Wu", "Qi", ""]]}, {"id": "2012.05169", "submitter": "Arda Sahiner", "authors": "Arda Sahiner, Morteza Mardani, Batu Ozturkler, Mert Pilanci, John\n  Pauly", "title": "Convex Regularization Behind Neural Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks have shown tremendous potential for reconstructing\nhigh-resolution images in inverse problems. The non-convex and opaque nature of\nneural networks, however, hinders their utility in sensitive applications such\nas medical imaging. To cope with this challenge, this paper advocates a convex\nduality framework that makes a two-layer fully-convolutional ReLU denoising\nnetwork amenable to convex optimization. The convex dual network not only\noffers the optimum training with convex solvers, but also facilitates\ninterpreting training and prediction. In particular, it implies training neural\nnetworks with weight decay regularization induces path sparsity while the\nprediction is piecewise linear filtering. A range of experiments with MNIST and\nfastMRI datasets confirm the efficacy of the dual network optimization problem.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 16:57:16 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Sahiner", "Arda", ""], ["Mardani", "Morteza", ""], ["Ozturkler", "Batu", ""], ["Pilanci", "Mert", ""], ["Pauly", "John", ""]]}, {"id": "2012.05185", "submitter": "Yaojie Liu", "authors": "Yaojie Liu and Xiaoming Liu", "title": "Physics-Guided Spoof Trace Disentanglement for Generic Face\n  Anti-Spoofing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prior studies show that the key to face anti-spoofing lies in the subtle\nimage pattern, termed \"spoof trace\", e.g., color distortion, 3D mask edge,\nMoire pattern, and many others. Designing a generic face anti-spoofing model to\nestimate those spoof traces can improve not only the generalization of the\nspoof detection, but also the interpretability of the model's decision. Yet,\nthis is a challenging task due to the diversity of spoof types and the lack of\nground truth in spoof traces. In this work, we design a novel adversarial\nlearning framework to disentangle spoof faces into the spoof traces and the\nlive counterparts. Guided by physical properties, the spoof generation is\nrepresented as a combination of additive process and inpainting process.\nAdditive process describes spoofing as spoof material introducing extra\npatterns (e.g., moire pattern), where the live counterpart can be recovered by\nremoving those patterns. Inpainting process describes spoofing as spoof\nmaterial fully covering certain regions, where the live counterpart of those\nregions has to be \"guessed\". We use 3 additive components and 1 inpainting\ncomponent to represent traces at different frequency bands. The disentangled\nspoof traces can be utilized to synthesize realistic new spoof faces after\nproper geometric correction, and the synthesized spoof can be used for training\nand improve the generalization of spoof detection. Our approach demonstrates\nsuperior spoof detection performance on 3 testing scenarios: known attacks,\nunknown attacks, and open-set attacks. Meanwhile, it provides a\nvisually-convincing estimation of the spoof traces. Source code and pre-trained\nmodels will be publicly available upon publication.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 17:22:44 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Liu", "Yaojie", ""], ["Liu", "Xiaoming", ""]]}, {"id": "2012.05191", "submitter": "Radu Horaud P", "authors": "Radu Horaud, Florence Forbes, Manuel Yguel, Guillaume Dewaele, and\n  Jian Zhang", "title": "Rigid and Articulated Point Registration with Expectation Conditional\n  Maximization", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  33(3), March 2011", "doi": "10.1109/TPAMI.2010.94", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the issue of matching rigid and articulated shapes\nthrough probabilistic point registration. The problem is recast into a missing\ndata framework where unknown correspondences are handled via mixture models.\nAdopting a maximum likelihood principle, we introduce an innovative EM-like\nalgorithm, namely the Expectation Conditional Maximization for Point\nRegistration (ECMPR) algorithm. The algorithm allows the use of general\ncovariance matrices for the mixture model components and improves over the\nisotropic covariance case. We analyse in detail the associated consequences in\nterms of estimation of the registration parameters, and we propose an optimal\nmethod for estimating the rotational and translational parameters based on\nsemi-definite positive relaxation. We extend rigid registration to articulated\nregistration. Robustness is ensured by detecting and rejecting outliers through\nthe addition of a uniform component to the Gaussian mixture model at hand. We\nprovide an in-depth analysis of our method and we compare it both theoretically\nand experimentally with other robust methods for point registration.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 17:36:11 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Horaud", "Radu", ""], ["Forbes", "Florence", ""], ["Yguel", "Manuel", ""], ["Dewaele", "Guillaume", ""], ["Zhang", "Jian", ""]]}, {"id": "2012.05197", "submitter": "Yun Liu", "authors": "Ellery Wulczyn, Kunal Nagpal, Matthew Symonds, Melissa Moran, Markus\n  Plass, Robert Reihs, Farah Nader, Fraser Tan, Yuannan Cai, Trissia Brown,\n  Isabelle Flament-Auvigne, Mahul B. Amin, Martin C. Stumpe, Heimo Muller,\n  Peter Regitnig, Andreas Holzinger, Greg S. Corrado, Lily H. Peng, Po-Hsuan\n  Cameron Chen, David F. Steiner, Kurt Zatloukal, Yun Liu, Craig H. Mermel", "title": "Predicting Prostate Cancer-Specific Mortality with A.I.-based Gleason\n  Grading", "comments": null, "journal-ref": null, "doi": "10.1038/s43856-021-00005-3", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gleason grading of prostate cancer is an important prognostic factor but\nsuffers from poor reproducibility, particularly among non-subspecialist\npathologists. Although artificial intelligence (A.I.) tools have demonstrated\nGleason grading on-par with expert pathologists, it remains an open question\nwhether A.I. grading translates to better prognostication. In this study, we\ndeveloped a system to predict prostate-cancer specific mortality via A.I.-based\nGleason grading and subsequently evaluated its ability to risk-stratify\npatients on an independent retrospective cohort of 2,807 prostatectomy cases\nfrom a single European center with 5-25 years of follow-up (median: 13,\ninterquartile range 9-17). The A.I.'s risk scores produced a C-index of 0.84\n(95%CI 0.80-0.87) for prostate cancer-specific mortality. Upon discretizing\nthese risk scores into risk groups analogous to pathologist Grade Groups (GG),\nthe A.I. had a C-index of 0.82 (95%CI 0.78-0.85). On the subset of cases with a\nGG in the original pathology report (n=1,517), the A.I.'s C-indices were 0.87\nand 0.85 for continuous and discrete grading, respectively, compared to 0.79\n(95%CI 0.71-0.86) for GG obtained from the reports. These represent\nimprovements of 0.08 (95%CI 0.01-0.15) and 0.07 (95%CI 0.00-0.14) respectively.\nOur results suggest that A.I.-based Gleason grading can lead to effective\nrisk-stratification and warrants further evaluation for improving disease\nmanagement.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 02:05:24 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Wulczyn", "Ellery", ""], ["Nagpal", "Kunal", ""], ["Symonds", "Matthew", ""], ["Moran", "Melissa", ""], ["Plass", "Markus", ""], ["Reihs", "Robert", ""], ["Nader", "Farah", ""], ["Tan", "Fraser", ""], ["Cai", "Yuannan", ""], ["Brown", "Trissia", ""], ["Flament-Auvigne", "Isabelle", ""], ["Amin", "Mahul B.", ""], ["Stumpe", "Martin C.", ""], ["Muller", "Heimo", ""], ["Regitnig", "Peter", ""], ["Holzinger", "Andreas", ""], ["Corrado", "Greg S.", ""], ["Peng", "Lily H.", ""], ["Chen", "Po-Hsuan Cameron", ""], ["Steiner", "David F.", ""], ["Zatloukal", "Kurt", ""], ["Liu", "Yun", ""], ["Mermel", "Craig H.", ""]]}, {"id": "2012.05205", "submitter": "Maria Bauza", "authors": "Maria Bauza, Eric Valls, Bryan Lim, Theo Sechopoulos, Alberto\n  Rodriguez", "title": "Tactile Object Pose Estimation from the First Touch with Geometric\n  Contact Rendering", "comments": "CORL 2020, 5 figures + 2 in appendix Video:\n  https://youtu.be/2ygtSJTmo08", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we present an approach to tactile pose estimation from the\nfirst touch for known objects. First, we create an object-agnostic map from\nreal tactile observations to contact shapes. Next, for a new object with known\ngeometry, we learn a tailored perception model completely in simulation. To do\nso, we simulate the contact shapes that a dense set of object poses would\nproduce on the sensor. Then, given a new contact shape obtained from the sensor\noutput, we match it against the pre-computed set using the object-specific\nembedding learned purely in simulation using contrastive learning.\n  This results in a perception model that can localize objects from a single\ntactile observation. It also allows reasoning over pose distributions and\nincluding additional pose constraints coming from other perception systems or\nmultiple contacts. We provide quantitative results for four objects. Our\napproach provides high accuracy pose estimations from distinctive tactile\nobservations while regressing pose distributions to account for those contact\nshapes that could result from different object poses. We further extend and\ntest our approach in multi-contact scenarios where several tactile sensors are\nsimultaneously in contact with the object. Website:\nhttp://mcube.mit.edu/research/tactile_loc_first_touch.html\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 18:00:35 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Bauza", "Maria", ""], ["Valls", "Eric", ""], ["Lim", "Bryan", ""], ["Sechopoulos", "Theo", ""], ["Rodriguez", "Alberto", ""]]}, {"id": "2012.05214", "submitter": "Alexis Baudron", "authors": "Alexis Baudron, Zihao W. Wang, Oliver Cossairt and Aggelos K.\n  Katsaggelos", "title": "E3D: Event-Based 3D Shape Reconstruction", "comments": "Correct author names and only include primary author email", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D shape reconstruction is a primary component of augmented/virtual reality.\nDespite being highly advanced, existing solutions based on RGB, RGB-D and Lidar\nsensors are power and data intensive, which introduces challenges for\ndeployment in edge devices. We approach 3D reconstruction with an event camera,\na sensor with significantly lower power, latency and data expense while\nenabling high dynamic range. While previous event-based 3D reconstruction\nmethods are primarily based on stereo vision, we cast the problem as multi-view\nshape from silhouette using a monocular event camera. The output from a moving\nevent camera is a sparse point set of space-time gradients, largely sketching\nscene/object edges and contours. We first introduce an event-to-silhouette\n(E2S) neural network module to transform a stack of event frames to the\ncorresponding silhouettes, with additional neural branches for camera pose\nregression. Second, we introduce E3D, which employs a 3D differentiable\nrenderer (PyTorch3D) to enforce cross-view 3D mesh consistency and fine-tune\nthe E2S and pose network. Lastly, we introduce a 3D-to-events simulation\npipeline and apply it to publicly available object datasets and generate\nsynthetic event/silhouette training pairs for supervised learning.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 18:23:21 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 12:26:59 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Baudron", "Alexis", ""], ["Wang", "Zihao W.", ""], ["Cossairt", "Oliver", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "2012.05217", "submitter": "Rui Xu", "authors": "Rui Xu, Xintao Wang, Kai Chen, Bolei Zhou, Chen Change Loy", "title": "Positional Encoding as Spatial Inductive Bias in GANs", "comments": "paper with appendix, project page:\n  https://nbei.github.io/gan-pos-encoding.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  SinGAN shows impressive capability in learning internal patch distribution\ndespite its limited effective receptive field. We are interested in knowing how\nsuch a translation-invariant convolutional generator could capture the global\nstructure with just a spatially i.i.d. input. In this work, taking SinGAN and\nStyleGAN2 as examples, we show that such capability, to a large extent, is\nbrought by the implicit positional encoding when using zero padding in the\ngenerators. Such positional encoding is indispensable for generating images\nwith high fidelity. The same phenomenon is observed in other generative\narchitectures such as DCGAN and PGGAN. We further show that zero padding leads\nto an unbalanced spatial bias with a vague relation between locations. To offer\na better spatial inductive bias, we investigate alternative positional\nencodings and analyze their effects. Based on a more flexible positional\nencoding explicitly, we propose a new multi-scale training strategy and\ndemonstrate its effectiveness in the state-of-the-art unconditional generator\nStyleGAN2. Besides, the explicit spatial inductive bias substantially improve\nSinGAN for more versatile image manipulation.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 18:27:16 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Xu", "Rui", ""], ["Wang", "Xintao", ""], ["Chen", "Kai", ""], ["Zhou", "Bolei", ""], ["Loy", "Chen Change", ""]]}, {"id": "2012.05225", "submitter": "Nataniel Ruiz", "authors": "Nataniel Ruiz, Barry-John Theobald, Anurag Ranjan, Ahmed Hussein\n  Abdelaziz, Nicholas Apostoloff", "title": "MorphGAN: One-Shot Face Synthesis GAN for Detecting Recognition Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To detect bias in face recognition networks, it can be useful to probe a\nnetwork under test using samples in which only specific attributes vary in some\ncontrolled way. However, capturing a sufficiently large dataset with specific\ncontrol over the attributes of interest is difficult. In this work, we describe\na simulator that applies specific head pose and facial expression adjustments\nto images of previously unseen people. The simulator first fits a 3D morphable\nmodel to a provided image, applies the desired head pose and facial expression\ncontrols, then renders the model into an image. Next, a conditional Generative\nAdversarial Network (GAN) conditioned on the original image and the rendered\nmorphable model is used to produce the image of the original person with the\nnew facial expression and head pose. We call this conditional GAN -- MorphGAN.\nImages generated using MorphGAN conserve the identity of the person in the\noriginal image, and the provided control over head pose and facial expression\nallows test sets to be created to identify robustness issues of a facial\nrecognition deep network with respect to pose and expression. Images generated\nby MorphGAN can also serve as data augmentation when training data are scarce.\nWe show that by augmenting small datasets of faces with new poses and\nexpressions improves the recognition performance by up to 9% depending on the\naugmentation and data scarcity.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 18:43:03 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 18:48:22 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Ruiz", "Nataniel", ""], ["Theobald", "Barry-John", ""], ["Ranjan", "Anurag", ""], ["Abdelaziz", "Ahmed Hussein", ""], ["Apostoloff", "Nicholas", ""]]}, {"id": "2012.05228", "submitter": "Xuanchi Ren", "authors": "Xuanchi Ren, Zian Qian, Qifeng Chen", "title": "Video Deblurring by Fitting to Test Data", "comments": "Project Page: https://github.com/xrenaa/Deblur-by-Fitting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion blur in videos captured by autonomous vehicles and robots can degrade\ntheir perception capability. In this work, we present a novel approach to video\ndeblurring by fitting a deep network to the test video. Our key observation is\nthat some frames in a video with motion blur are much sharper than others, and\nthus we can transfer the texture information in those sharp frames to blurry\nframes. Our approach heuristically selects sharp frames from a video and then\ntrains a convolutional neural network on these sharp frames. The trained\nnetwork often absorbs enough details in the scene to perform deblurring on all\nthe video frames. As an internal learning method, our approach has no domain\ngap between training and test data, which is a problematic issue for existing\nvideo deblurring approaches. The conducted experiments on real-world video data\nshow that our model can reconstruct clearer and sharper videos than\nstate-of-the-art video deblurring approaches. Code and data are available at\nhttps://github.com/xrenaa/Deblur-by-Fitting.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 18:49:24 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 07:22:22 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ren", "Xuanchi", ""], ["Qian", "Zian", ""], ["Chen", "Qifeng", ""]]}, {"id": "2012.05258", "submitter": "Siyuan Qiao", "authors": "Siyuan Qiao, Yukun Zhu, Hartwig Adam, Alan Yuille, Liang-Chieh Chen", "title": "ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic\n  Segmentation", "comments": "Video: https://youtu.be/XR4HFiwwao0 GitHub:\n  https://github.com/joe-siyuan-qiao/ViP-DeepLab", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present ViP-DeepLab, a unified model attempting to tackle\nthe long-standing and challenging inverse projection problem in vision, which\nwe model as restoring the point clouds from perspective image sequences while\nproviding each point with instance-level semantic interpretations. Solving this\nproblem requires the vision models to predict the spatial location, semantic\nclass, and temporally consistent instance label for each 3D point. ViP-DeepLab\napproaches it by jointly performing monocular depth estimation and video\npanoptic segmentation. We name this joint task as Depth-aware Video Panoptic\nSegmentation, and propose a new evaluation metric along with two derived\ndatasets for it, which will be made available to the public. On the individual\nsub-tasks, ViP-DeepLab also achieves state-of-the-art results, outperforming\nprevious methods by 5.1% VPQ on Cityscapes-VPS, ranking 1st on the KITTI\nmonocular depth estimation benchmark, and 1st on KITTI MOTS pedestrian. The\ndatasets and the evaluation codes are made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 19:00:35 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Qiao", "Siyuan", ""], ["Zhu", "Yukun", ""], ["Adam", "Hartwig", ""], ["Yuille", "Alan", ""], ["Chen", "Liang-Chieh", ""]]}, {"id": "2012.05273", "submitter": "Hongxin Wei", "authors": "Hongxin Wei, Lei Feng, Rundong Wang, Bo An", "title": "MetaInfoNet: Learning Task-Guided Information for Sample Reweighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been shown to easily overfit to biased training\ndata with label noise or class imbalance. Meta-learning algorithms are commonly\ndesigned to alleviate this issue in the form of sample reweighting, by learning\na meta weighting network that takes training losses as inputs to generate\nsample weights. In this paper, we advocate that choosing proper inputs for the\nmeta weighting network is crucial for desired sample weights in a specific\ntask, while training loss is not always the correct answer. In view of this, we\npropose a novel meta-learning algorithm, MetaInfoNet, which automatically\nlearns effective representations as inputs for the meta weighting network by\nemphasizing task-related information with an information bottleneck strategy.\nExtensive experimental results on benchmark datasets with label noise or class\nimbalance validate that MetaInfoNet is superior to many state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 19:21:20 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Wei", "Hongxin", ""], ["Feng", "Lei", ""], ["Wang", "Rundong", ""], ["An", "Bo", ""]]}, {"id": "2012.05292", "submitter": "Kevin Chen", "authors": "Kevin Chen, Junshen K. Chen, Jo Chuang, Marynel V\\'azquez, Silvio\n  Savarese", "title": "Topological Planning with Transformers for Vision-and-Language\n  Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional approaches to vision-and-language navigation (VLN) are trained\nend-to-end but struggle to perform well in freely traversable environments.\nInspired by the robotics community, we propose a modular approach to VLN using\ntopological maps. Given a natural language instruction and topological map, our\napproach leverages attention mechanisms to predict a navigation plan in the\nmap. The plan is then executed with low-level actions (e.g. forward, rotate)\nusing a robust controller. Experiments show that our method outperforms\nprevious end-to-end approaches, generates interpretable navigation plans, and\nexhibits intelligent behaviors such as backtracking.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 20:02:03 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Chen", "Kevin", ""], ["Chen", "Junshen K.", ""], ["Chuang", "Jo", ""], ["V\u00e1zquez", "Marynel", ""], ["Savarese", "Silvio", ""]]}, {"id": "2012.05304", "submitter": "Naif Alshammari PhD", "authors": "Naif Alshammari, Samet Akcay, and Toby P. Breckon", "title": "Competitive Simplicity for Multi-Task Learning for Real-Time Foggy Scene\n  Understanding via Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automotive scene understanding under adverse weather conditions raises a\nrealistic and challenging problem attributable to poor outdoor scene visibility\n(e.g. foggy weather). However, because most contemporary scene understanding\napproaches are applied under ideal-weather conditions, such approaches may not\nprovide genuinely optimal performance when compared to established a priori\ninsights on extreme-weather understanding. In this paper, we propose a complex\nbut competitive multi-task learning approach capable of performing in real-time\nsemantic scene understanding and monocular depth estimation under foggy weather\nconditions by leveraging both recent advances in adversarial training and\ndomain adaptation. As an end-to-end pipeline, our model provides a novel\nsolution to surpass degraded visibility in foggy weather conditions by\ntransferring scenes from foggy to normal using a GAN-based model. For optimal\nperformance in semantic segmentation, our model generates depth to be used as\ncomplementary source information with RGB in the segmentation network. We\nprovide a robust method for foggy scene understanding by training two models\n(normal and foggy) simultaneously with shared weights (each model is trained on\neach weather condition independently). Our model incorporates RGB colour,\ndepth, and luminance images via distinct encoders with dense connectivity and\nfeatures fusing, and leverages skip connections to produce consistent depth and\nsegmentation predictions. Using this architectural formulation with light\ncomputational complexity at inference time, we are able to achieve comparable\nperformance to contemporary approaches at a fraction of the overall model\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 20:38:34 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Alshammari", "Naif", ""], ["Akcay", "Samet", ""], ["Breckon", "Toby P.", ""]]}, {"id": "2012.05316", "submitter": "Numan Celik", "authors": "Numan Celik, Soumya Gupta, Sharib Ali, Jens Rittscher", "title": "Unsupervised Adversarial Domain Adaptation For Barrett's Segmentation", "comments": "5 pages, 3 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Barrett's oesophagus (BE) is one of the early indicators of esophageal\ncancer. Patients with BE are monitored and undergo ablation therapies to\nminimise the risk, thereby making it eminent to identify the BE area precisely.\nAutomated segmentation can help clinical endoscopists to assess and treat BE\narea more accurately. Endoscopy imaging of BE can include multiple modalities\nin addition to the conventional white light (WL) modality. Supervised models\nrequire large amount of manual annotations incorporating all data variability\nin the training data. However, it becomes cumbersome, tedious and labour\nintensive work to generate manual annotations, and additionally modality\nspecific expertise is required. In this work, we aim to alleviate this problem\nby applying an unsupervised domain adaptation technique (UDA). Here, UDA is\ntrained on white light endoscopy images as source domain and are well-adapted\nto generalise to produce segmentation on different imaging modalities as target\ndomain, namely narrow band imaging and post acetic-acid WL imaging. Our dataset\nconsists of a total of 871 images consisting of both source and target domains.\nOur results show that the UDA-based approach outperforms traditional supervised\nU-Net segmentation by nearly 10% on both Dice similarity coefficient and\nintersection-over-union.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 20:59:25 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Celik", "Numan", ""], ["Gupta", "Soumya", ""], ["Ali", "Sharib", ""], ["Rittscher", "Jens", ""]]}, {"id": "2012.05320", "submitter": "Naif Alshammari PhD", "authors": "Naif Alshammari, Samet Akcay, and Toby P. Breckon", "title": "Multi-Model Learning for Real-Time Automotive Semantic Foggy Scene\n  Understanding via Domain Adaptation", "comments": "arXiv admin note: substantial text overlap with arXiv:1909.07697", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust semantic scene segmentation for automotive applications is a\nchallenging problem in two key aspects: (1) labelling every individual scene\npixel and (2) performing this task under unstable weather and illumination\nchanges (e.g., foggy weather), which results in poor outdoor scene visibility.\nSuch visibility limitations lead to non-optimal performance of generalised deep\nconvolutional neural network-based semantic scene segmentation. In this paper,\nwe propose an efficient end-to-end automotive semantic scene understanding\napproach that is robust to foggy weather conditions. As an end-to-end pipeline,\nour proposed approach provides: (1) the transformation of imagery from foggy to\nclear weather conditions using a domain transfer approach (correcting for poor\nvisibility) and (2) semantically segmenting the scene using a competitive\nencoder-decoder architecture with low computational complexity (enabling\nreal-time performance). Our approach incorporates RGB colour, depth and\nluminance images via distinct encoders with dense connectivity and features\nfusion to effectively exploit information from different inputs, which\ncontributes to an optimal feature representation within the overall model.\nUsing this architectural formulation with dense skip connections, our model\nachieves comparable performance to contemporary approaches at a fraction of the\noverall model complexity.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 21:04:05 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Alshammari", "Naif", ""], ["Akcay", "Samet", ""], ["Breckon", "Toby P.", ""]]}, {"id": "2012.05325", "submitter": "Luis G\\'omez-Chova", "authors": "Gonzalo Mateo-Garc\\'ia, Luis G\\'omez-Chova, Gustau Camps-Valls", "title": "Convolutional Neural Networks for Multispectral Image Cloud Masking", "comments": "Preprint corresponding to the paper published in 2017 IEEE\n  International Geoscience and Remote Sensing Symposium (IGARSS), Fort Worth,\n  TX, USA, pp. 2255-2258", "journal-ref": null, "doi": "10.1109/IGARSS.2017.8127438", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Convolutional neural networks (CNN) have proven to be state of the art\nmethods for many image classification tasks and their use is rapidly increasing\nin remote sensing problems. One of their major strengths is that, when enough\ndata is available, CNN perform an end-to-end learning without the need of\ncustom feature extraction methods. In this work, we study the use of different\nCNN architectures for cloud masking of Proba-V multispectral images. We compare\nsuch methods with the more classical machine learning approach based on feature\nextraction plus supervised classification. Experimental results suggest that\nCNN are a promising alternative for solving cloud masking problems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 21:33:20 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Mateo-Garc\u00eda", "Gonzalo", ""], ["G\u00f3mez-Chova", "Luis", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.05328", "submitter": "Nurit Spingarn Eliezer", "authors": "Nurit Spingarn-Eliezer, Ron Banner and Tomer Michaeli", "title": "GAN \"Steerability\" without optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has shown remarkable success in revealing \"steering\"\ndirections in the latent spaces of pre-trained GANs. These directions\ncorrespond to semantically meaningful image transformations e.g., shift, zoom,\ncolor manipulations), and have similar interpretable effects across all\ncategories that the GAN can generate. Some methods focus on user-specified\ntransformations, while others discover transformations in an unsupervised\nmanner. However, all existing techniques rely on an optimization procedure to\nexpose those directions, and offer no control over the degree of allowed\ninteraction between different transformations. In this paper, we show that\n\"steering\" trajectories can be computed in closed form directly from the\ngenerator's weights without any form of training or optimization. This applies\nto user-prescribed geometric transformations, as well as to unsupervised\ndiscovery of more complex effects. Our approach allows determining both linear\nand nonlinear trajectories, and has many advantages over previous methods. In\nparticular, we can control whether one transformation is allowed to come on the\nexpense of another (e.g. zoom-in with or without allowing translation to keep\nthe object centered). Moreover, we can determine the natural end-point of the\ntrajectory, which corresponds to the largest extent to which a transformation\ncan be applied without incurring degradation. Finally, we show how transferring\nattributes between images can be achieved without optimization, even across\ndifferent categories.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 21:34:34 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 16:50:39 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Spingarn-Eliezer", "Nurit", ""], ["Banner", "Ron", ""], ["Michaeli", "Tomer", ""]]}, {"id": "2012.05339", "submitter": "Chenjie Gu", "authors": "Hongzi Mao, Chenjie Gu, Miaosen Wang, Angie Chen, Nevena Lazic, Nir\n  Levine, Derek Pang, Rene Claus, Marisabel Hechtman, Ching-Han Chiang, Cheng\n  Chen, Jingning Han", "title": "Neural Rate Control for Video Encoding using Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern video encoders, rate control is a critical component and has been\nheavily engineered. It decides how many bits to spend to encode each frame, in\norder to optimize the rate-distortion trade-off over all video frames. This is\na challenging constrained planning problem because of the complex dependency\namong decisions for different video frames and the bitrate constraint defined\nat the end of the episode.\n  We formulate the rate control problem as a Partially Observable Markov\nDecision Process (POMDP), and apply imitation learning to learn a neural rate\ncontrol policy. We demonstrate that by learning from optimal video encoding\ntrajectories obtained through evolution strategies, our learned policy achieves\nbetter encoding efficiency and has minimal constraint violation. In addition to\nimitating the optimal actions, we find that additional auxiliary losses, data\naugmentation/refinement and inference-time policy improvements are critical for\nlearning a good rate control policy. We evaluate the learned policy against the\nrate control policy in libvpx, a widely adopted open source VP9 codec library,\nin the two-pass variable bitrate (VBR) mode. We show that over a diverse set of\nreal-world videos, our learned policy achieves 8.5% median bitrate reduction\nwithout sacrificing video quality.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 21:59:20 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Mao", "Hongzi", ""], ["Gu", "Chenjie", ""], ["Wang", "Miaosen", ""], ["Chen", "Angie", ""], ["Lazic", "Nevena", ""], ["Levine", "Nir", ""], ["Pang", "Derek", ""], ["Claus", "Rene", ""], ["Hechtman", "Marisabel", ""], ["Chiang", "Ching-Han", ""], ["Chen", "Cheng", ""], ["Han", "Jingning", ""]]}, {"id": "2012.05342", "submitter": "Pierre-Etienne Martin", "authors": "Pierre-Etienne Martin (LaBRI, UB), Jenny Benois-Pineau (LaBRI), Renaud\n  P\\'eteri, Julien Morlier", "title": "3D attention mechanism for fine-grained classification of table tennis\n  strokes using a Twin Spatio-Temporal Convolutional Neural Networks", "comments": null, "journal-ref": "25th International Conference on Pattern Recognition (ICPR2020),\n  Jan 2021, Milano, Italy", "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the problem of recognition of actions in video with low\ninter-class variability such as Table Tennis strokes. Two stream, \"twin\"\nconvolutional neural networks are used with 3D convolutions both on RGB data\nand optical flow. Actions are recognized by classification of temporal windows.\nWe introduce 3D attention modules and examine their impact on classification\nefficiency. In the context of the study of sportsmen performances, a corpus of\nthe particular actions of table tennis strokes is considered. The use of\nattention blocks in the network speeds up the training step and improves the\nclassification scores up to 5% with our twin model. We visualize the impact on\nthe obtained features and notice correlation between attention and player\nmovements and position. Score comparison of state-of-the-art action\nclassification method and proposed approach with attentional blocks is\nperformed on the corpus. Proposed model with attention blocks outperforms\nprevious model without them and our baseline.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 09:55:12 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Martin", "Pierre-Etienne", "", "LaBRI, UB"], ["Benois-Pineau", "Jenny", "", "LaBRI"], ["P\u00e9teri", "Renaud", ""], ["Morlier", "Julien", ""]]}, {"id": "2012.05344", "submitter": "Eklavya Sarkar", "authors": "Eklavya Sarkar, Pavel Korshunov, Laurent Colbois, S\\'ebastien Marcel", "title": "Vulnerability Analysis of Face Morphing Attacks from Landmarks and\n  Generative Adversarial Networks", "comments": "Submitted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Morphing attacks is a threat to biometric systems where the biometric\nreference in an identity document can be altered. This form of attack presents\nan important issue in applications relying on identity documents such as border\nsecurity or access control. Research in face morphing attack detection is\ndeveloping rapidly, however very few datasets with several forms of attacks are\npublicly available. This paper bridges this gap by providing a new dataset with\nfour different types of morphing attacks, based on OpenCV, FaceMorpher,\nWebMorph and a generative adversarial network (StyleGAN), generated with\noriginal face images from three public face datasets. We also conduct extensive\nexperiments to assess the vulnerability of the state-of-the-art face\nrecognition systems, notably FaceNet, VGG-Face, and ArcFace. The experiments\ndemonstrate that VGG-Face, while being less accurate face recognition system\ncompared to FaceNet, is also less vulnerable to morphing attacks. Also, we\nobserved that na\\\"ive morphs generated with a StyleGAN do not pose a\nsignificant threat.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 22:10:17 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Sarkar", "Eklavya", ""], ["Korshunov", "Pavel", ""], ["Colbois", "Laurent", ""], ["Marcel", "S\u00e9bastien", ""]]}, {"id": "2012.05350", "submitter": "Tanvir Mahmud", "authors": "Tanvir Mahmud and Shaikh Anowarul Fattah", "title": "Automatic Diagnosis of Malaria from Thin Blood Smear Images using Deep\n  Convolutional Neural Network with Multi-Resolution Feature Fusion", "comments": "9 Pages, 10 Figures, This Manuscript is under review in Expert\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Malaria, a life-threatening disease, infects millions of people every year\nthroughout the world demanding faster diagnosis for proper treatment before any\ndamages occur. In this paper, an end-to-end deep learning-based approach is\nproposed for faster diagnosis of malaria from thin blood smear images by making\nefficient optimizations of features extracted from diversified receptive\nfields. Firstly, an efficient, highly scalable deep neural network, named as\nDilationNet, is proposed that incorporates features from a large spectrum by\nvarying dilation rates of convolutions to extract features from different\nreceptive areas. Next, the raw images are resampled to various resolutions to\nintroduce variations in the receptive fields that are used for independently\noptimizing different forms of DilationNet scaled for different resolutions of\nimages. Afterward, a feature fusion scheme is introduced with the proposed\nDeepFusionNet architecture for jointly optimizing the feature space of these\nindividually trained networks operating on different levels of observations.\nAll the convolutional layers of various forms of DilationNets that are\noptimized to extract spatial features from different resolutions of images are\ndirectly transferred to provide a variegated feature space from any image.\nLater, joint optimization of these spatial features is carried out in the\nDeepFusionNet to extract the most relevant representation of the sample image.\nThis scheme offers the opportunity to explore the feature space extensively by\nvarying the observation level to accurately diagnose the abnormality. Intense\nexperimentations on a publicly available dataset show outstanding performance\nwith accuracy over 99.5% outperforming other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 22:44:05 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Mahmud", "Tanvir", ""], ["Fattah", "Shaikh Anowarul", ""]]}, {"id": "2012.05360", "submitter": "Kejie Li", "authors": "Kejie Li, Hamid Rezatofighi, Ian Reid", "title": "MOLTR: Multiple Object Localisation, Tracking, and Reconstruction from\n  Monocular RGB Videos", "comments": "Accepted at IEEE Robotics and Automation Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic aware reconstruction is more advantageous than geometric-only\nreconstruction for future robotic and AR/VR applications because it represents\nnot only where things are, but also what things are. Object-centric mapping is\na task to build an object-level reconstruction where objects are separate and\nmeaningful entities that convey both geometry and semantic information. In this\npaper, we present MOLTR, a solution to object-centric mapping using only\nmonocular image sequences and camera poses. It is able to localise, track, and\nreconstruct multiple objects in an online fashion when an RGB camera captures a\nvideo of the surrounding. Given a new RGB frame, MOLTR firstly applies a\nmonocular 3D detector to localise objects of interest and extract their shape\ncodes that represent the object shapes in a learned embedding space. Detections\nare then merged to existing objects in the map after data association. Motion\nstate (i.e. kinematics and the motion status) of each object is tracked by a\nmultiple model Bayesian filter and object shape is progressively refined by\nfusing multiple shape code. We evaluate localisation, tracking, and\nreconstruction on benchmarking datasets for indoor and outdoor scenes, and show\nsuperior performance over previous approaches.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 23:15:08 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 03:12:24 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Li", "Kejie", ""], ["Rezatofighi", "Hamid", ""], ["Reid", "Ian", ""]]}, {"id": "2012.05373", "submitter": "Mohammad Rafayet Ali", "authors": "Mohammad Rafayet Ali, Taylor Myers, Ellen Wagner, Harshil Ratnu, E.\n  Ray Dorsey, Ehsan Hoque", "title": "Facial expressions can detect Parkinson's disease: preliminary evidence\n  from videos collected online", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the symptoms of Parkinson's disease (PD) is hypomimia or reduced\nfacial expressions. In this paper, we present a digital biomarker for PD that\nutilizes the study of micro-expressions. We analyzed the facial action units\n(AU) from 1812 videos of 604 individuals (61 with PD and 543 without PD, mean\nage 63.9 yo, sd 7.8 ) collected online using a web-based tool\n(www.parktest.net). In these videos, participants were asked to make three\nfacial expressions (a smiling, disgusted, and surprised face) followed by a\nneutral face. Using techniques from computer vision and machine learning, we\nobjectively measured the variance of the facial muscle movements and used it to\ndistinguish between individuals with and without PD. The prediction accuracy\nusing the facial micro-expressions was comparable to those methodologies that\nutilize motor symptoms. Logistic regression analysis revealed that participants\nwith PD had less variance in AU6 (cheek raiser), AU12 (lip corner puller), and\nAU4 (brow lowerer) than non-PD individuals. An automated classifier using\nSupport Vector Machine was trained on the variances and achieved 95.6%\naccuracy. Using facial expressions as a biomarker for PD could be potentially\ntransformative for patients in need of physical separation (e.g., due to COVID)\nor are immobile.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 23:53:32 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Ali", "Mohammad Rafayet", ""], ["Myers", "Taylor", ""], ["Wagner", "Ellen", ""], ["Ratnu", "Harshil", ""], ["Dorsey", "E. Ray", ""], ["Hoque", "Ehsan", ""]]}, {"id": "2012.05400", "submitter": "Di Xie", "authors": "Xianfeng Li and Weijie Chen and Di Xie and Shicai Yang and Peng Yuan\n  and Shiliang Pu and Yueting Zhuang", "title": "A Free Lunch for Unsupervised Domain Adaptive Object Detection without\n  Source Data", "comments": "accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised domain adaptation (UDA) assumes that source and target domain\ndata are freely available and usually trained together to reduce the domain\ngap. However, considering the data privacy and the inefficiency of data\ntransmission, it is impractical in real scenarios. Hence, it draws our eyes to\noptimize the network in the target domain without accessing labeled source\ndata. To explore this direction in object detection, for the first time, we\npropose a source data-free domain adaptive object detection (SFOD) framework\nvia modeling it into a problem of learning with noisy labels. Generally, a\nstraightforward method is to leverage the pre-trained network from the source\ndomain to generate the pseudo labels for target domain optimization. However,\nit is difficult to evaluate the quality of pseudo labels since no labels are\navailable in target domain. In this paper, self-entropy descent (SED) is a\nmetric proposed to search an appropriate confidence threshold for reliable\npseudo label generation without using any handcrafted labels. Nonetheless,\ncompletely clean labels are still unattainable. After a thorough experimental\nanalysis, false negatives are found to dominate in the generated noisy labels.\nUndoubtedly, false negatives mining is helpful for performance improvement, and\nwe ease it to false negatives simulation through data augmentation like Mosaic.\nExtensive experiments conducted in four representative adaptation tasks have\ndemonstrated that the proposed framework can easily achieve state-of-the-art\nperformance. From another view, it also reminds the UDA community that the\nlabeled source data are not fully exploited in the existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 01:42:35 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Li", "Xianfeng", ""], ["Chen", "Weijie", ""], ["Xie", "Di", ""], ["Yang", "Shicai", ""], ["Yuan", "Peng", ""], ["Pu", "Shiliang", ""], ["Zhuang", "Yueting", ""]]}, {"id": "2012.05434", "submitter": "YueFeng Chen", "authors": "Xiaofeng Mao, Yuefeng Chen, Shuhui Wang, Hang Su, Yuan He, Hui Xue", "title": "Composite Adversarial Attacks", "comments": "To appear in AAAI 2021, code will be released later", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attack is a technique for deceiving Machine Learning (ML) models,\nwhich provides a way to evaluate the adversarial robustness. In practice,\nattack algorithms are artificially selected and tuned by human experts to break\na ML system. However, manual selection of attackers tends to be sub-optimal,\nleading to a mistakenly assessment of model security. In this paper, a new\nprocedure called Composite Adversarial Attack (CAA) is proposed for\nautomatically searching the best combination of attack algorithms and their\nhyper-parameters from a candidate pool of \\textbf{32 base attackers}. We design\na search space where attack policy is represented as an attacking sequence,\ni.e., the output of the previous attacker is used as the initialization input\nfor successors. Multi-objective NSGA-II genetic algorithm is adopted for\nfinding the strongest attack policy with minimum complexity. The experimental\nresult shows CAA beats 10 top attackers on 11 diverse defenses with less\nelapsed time (\\textbf{6 $\\times$ faster than AutoAttack}), and achieves the new\nstate-of-the-art on $l_{\\infty}$, $l_{2}$ and unrestricted adversarial attacks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 03:21:16 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Mao", "Xiaofeng", ""], ["Chen", "Yuefeng", ""], ["Wang", "Shuhui", ""], ["Su", "Hang", ""], ["He", "Yuan", ""], ["Xue", "Hui", ""]]}, {"id": "2012.05435", "submitter": "Risheng Liu", "authors": "Risheng Liu, Zhu Liu, Pan Mu, Zhouchen Lin, Xin Fan, Zhongxuan Luo", "title": "Learning Optimization-inspired Image Propagation with Control Mechanisms\n  and Architecture Augmentations for Low-level Vision", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, building deep learning models from optimization perspectives\nhas becoming a promising direction for solving low-level vision problems. The\nmain idea of most existing approaches is to straightforwardly combine numerical\niterations with manually designed network architectures to generate image\npropagations for specific kinds of optimization models. However, these\nheuristic learning models often lack mechanisms to control the propagation and\nrely on architecture engineering heavily. To mitigate the above issues, this\npaper proposes a unified optimization-inspired deep image propagation framework\nto aggregate Generative, Discriminative and Corrective (GDC for short)\nprinciples for a variety of low-level vision tasks. Specifically, we first\nformulate low-level vision tasks using a generic optimization objective and\nconstruct our fundamental propagative modules from three different viewpoints,\ni.e., the solution could be obtained/learned 1) in generative manner; 2) based\non discriminative metric, and 3) with domain knowledge correction. By designing\ncontrol mechanisms to guide image propagations, we then obtain convergence\nguarantees of GDC for both fully- and partially-defined optimization\nformulations. Furthermore, we introduce two architecture augmentation\nstrategies (i.e., normalization and automatic search) to respectively enhance\nthe propagation stability and task/data-adaption ability. Extensive experiments\non different low-level vision applications demonstrate the effectiveness and\nflexibility of GDC.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 03:24:53 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Liu", "Risheng", ""], ["Liu", "Zhu", ""], ["Mu", "Pan", ""], ["Lin", "Zhouchen", ""], ["Fan", "Xin", ""], ["Luo", "Zhongxuan", ""]]}, {"id": "2012.05438", "submitter": "Yu Sun", "authors": "Maxat Alibayev, David Paulius, and Yu Sun", "title": "Developing Motion Code Embedding for Action Recognition in Videos", "comments": "Accepted by 25th International Conference on Pattern Recognition\n  (ICPR2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a motion embedding strategy known as motion codes,\nwhich is a vectorized representation of motions based on a manipulation's\nsalient mechanical attributes. These motion codes provide a robust motion\nrepresentation, and they are obtained using a hierarchy of features called the\nmotion taxonomy. We developed and trained a deep neural network model that\ncombines visual and semantic features to identify the features found in our\nmotion taxonomy to embed or annotate videos with motion codes. To demonstrate\nthe potential of motion codes as features for machine learning tasks, we\nintegrated the extracted features from the motion embedding model into the\ncurrent state-of-the-art action recognition model. The obtained model achieved\nhigher accuracy than the baseline model for the verb classification task on\negocentric videos from the EPIC-KITCHENS dataset.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 03:49:23 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Alibayev", "Maxat", ""], ["Paulius", "David", ""], ["Sun", "Yu", ""]]}, {"id": "2012.05440", "submitter": "Xinghao Ding", "authors": "Liyan Sun, Chenxin Li, Xinghao Ding, Yue Huang, Guisheng Wang and\n  Yizhou Yu", "title": "Few-shot Medical Image Segmentation using a Global Correlation Network\n  with Discriminative Embedding", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite deep convolutional neural networks achieved impressive progress in\nmedical image computing and analysis, its paradigm of supervised learning\ndemands a large number of annotations for training to avoid overfitting and\nachieving promising results. In clinical practices, massive semantic\nannotations are difficult to acquire in some conditions where specialized\nbiomedical expert knowledge is required, and it is also a common condition\nwhere only few annotated classes are available. In this work, we proposed a\nnovel method for few-shot medical image segmentation, which enables a\nsegmentation model to fast generalize to an unseen class with few training\nimages. We construct our few-shot image segmentor using a deep convolutional\nnetwork trained episodically. Motivated by the spatial consistency and\nregularity in medical images, we developed an efficient global correlation\nmodule to capture the correlation between a support and query image and\nincorporate it into the deep network called global correlation network.\nMoreover, we enhance discriminability of deep embedding to encourage clustering\nof the feature domains of the same class while keep the feature domains of\ndifferent organs far apart. Ablation Study proved the effectiveness of the\nproposed global correlation module and discriminative embedding loss. Extensive\nexperiments on anatomical abdomen images on both CT and MRI modalities are\nperformed to demonstrate the state-of-the-art performance of our proposed\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 04:01:07 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Sun", "Liyan", ""], ["Li", "Chenxin", ""], ["Ding", "Xinghao", ""], ["Huang", "Yue", ""], ["Wang", "Guisheng", ""], ["Yu", "Yizhou", ""]]}, {"id": "2012.05447", "submitter": "Michael Horry Mr", "authors": "Michael J. Horry, Subrata Chakraborty, Biswajeet Pradhan, Manoranjan\n  Paul, Douglas P. S. Gomes, Anwaar Ul-Haq", "title": "Automatic Generation of Interpretable Lung Cancer Scoring Models from\n  Chest X-Ray Images", "comments": "10 pages, 14 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lung cancer is the leading cause of cancer death worldwide with early\ndetection being the key to a positive patient prognosis. Although a multitude\nof studies have demonstrated that machine learning, and particularly deep\nlearning, techniques are effective at automatically diagnosing lung cancer,\nthese techniques have yet to be clinically approved and adopted by the medical\ncommunity. Most research in this field is focused on the narrow task of nodule\ndetection to provide an artificial radiological second reading. We instead\nfocus on extracting, from chest X-ray images, a wider range of pathologies\nassociated with lung cancer using a computer vision model trained on a large\ndataset. We then find the set of best fit decision trees against an\nindependent, smaller dataset for which lung cancer malignancy metadata is\nprovided. For this small inferencing dataset, our best model achieves\nsensitivity and specificity of 85% and 75% respectively with a positive\npredictive value of 85% which is comparable to the performance of human\nradiologists. Furthermore, the decision trees created by this method may be\nconsidered as a starting point for refinement by medical experts into\nclinically usable multi-variate lung cancer scoring and diagnostic models.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 04:11:59 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 08:57:50 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Horry", "Michael J.", ""], ["Chakraborty", "Subrata", ""], ["Pradhan", "Biswajeet", ""], ["Paul", "Manoranjan", ""], ["Gomes", "Douglas P. S.", ""], ["Ul-Haq", "Anwaar", ""]]}, {"id": "2012.05463", "submitter": "Schrasing Tong", "authors": "Schrasing Tong (1), Lalana Kagal (1) ((1) Massachusetts Institute of\n  Technology)", "title": "Investigating Bias in Image Classification using Model Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We evaluated whether model explanations could efficiently detect bias in\nimage classification by highlighting discriminating features, thereby removing\nthe reliance on sensitive attributes for fairness calculations. To this end, we\nformulated important characteristics for bias detection and observed how\nexplanations change as the degree of bias in models change. The paper\nidentifies strengths and best practices for detecting bias using explanations,\nas well as three main weaknesses: explanations poorly estimate the degree of\nbias, could potentially introduce additional bias into the analysis, and are\nsometimes inefficient in terms of human effort involved.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 05:27:49 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Tong", "Schrasing", ""], ["Kagal", "Lalana", ""]]}, {"id": "2012.05473", "submitter": "Yuting Qiang Ms", "authors": "Yuting Qiang, Yongxin Yang, Yanwen Guo and Timothy M. Hospedales", "title": "Tensor Composition Net for Visual Relationship Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Tensor Composition Network (TCN) to predict visual\nrelationships in images. Visual Relationships in subject-predicate-object form\nprovide a more powerful query modality than simple image tags. However Visual\nRelationship Prediction (VRP) also provides a more challenging test of image\nunderstanding than conventional image tagging, and is difficult to learn due to\na large label-space and incomplete annotation. The key idea of our TCN is to\nexploit the low rank property of the visual relationship tensor, so as to\nleverage correlations within and across objects and relationships, and make a\nstructured prediction of all objects and their relations in an image. To show\nthe effectiveness of our method, we first empirically compare our model with\nmulti-label classification alternatives on VRP, and show that our model\noutperforms state-of-the-art MLIC methods. We then show that, thanks to our\ntensor (de)composition layer, our model can predict visual relationships which\nhave not been seen in training dataset. We finally show our TCN's image-level\nvisual relationship prediction provides a simple and efficient mechanism for\nrelation-based image retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 06:27:20 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Qiang", "Yuting", ""], ["Yang", "Yongxin", ""], ["Guo", "Yanwen", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "2012.05475", "submitter": "Xinyang Jiang", "authors": "Enwei Zhang, Xinyang Jiang, Hao Cheng, Ancong Wu, Fufu Yu, Ke Li,\n  Xiaowei Guo, Feng Zheng, Wei-Shi Zheng, Xing Sun", "title": "One for More: Selecting Generalizable Samples for Generalizable ReID\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current training objectives of existing person Re-IDentification (ReID)\nmodels only ensure that the loss of the model decreases on selected training\nbatch, with no regards to the performance on samples outside the batch. It will\ninevitably cause the model to over-fit the data in the dominant position (e.g.,\nhead data in imbalanced class, easy samples or noisy samples). %We call the\nsample that updates the model towards generalizing on more data a generalizable\nsample. The latest resampling methods address the issue by designing specific\ncriterion to select specific samples that trains the model generalize more on\ncertain type of data (e.g., hard samples, tail data), which is not adaptive to\nthe inconsistent real world ReID data distributions. Therefore, instead of\nsimply presuming on what samples are generalizable, this paper proposes a\none-for-more training objective that directly takes the generalization ability\nof selected samples as a loss function and learn a sampler to automatically\nselect generalizable samples. More importantly, our proposed one-for-more based\nsampler can be seamlessly integrated into the ReID training framework which is\nable to simultaneously train ReID models and the sampler in an end-to-end\nfashion. The experimental results show that our method can effectively improve\nthe ReID model training and boost the performance of ReID models.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 06:37:09 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 06:37:21 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Zhang", "Enwei", ""], ["Jiang", "Xinyang", ""], ["Cheng", "Hao", ""], ["Wu", "Ancong", ""], ["Yu", "Fufu", ""], ["Li", "Ke", ""], ["Guo", "Xiaowei", ""], ["Zheng", "Feng", ""], ["Zheng", "Wei-Shi", ""], ["Sun", "Xing", ""]]}, {"id": "2012.05493", "submitter": "Zhaoqun Li", "authors": "Zhaoqun Li, Hongren Wang, Jinxing Li", "title": "Auto-MVCNN: Neural Architecture Search for Multi-view 3D Shape\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 3D shape recognition, multi-view based methods leverage human's\nperspective to analyze 3D shapes and have achieved significant outcomes. Most\nexisting research works in deep learning adopt handcrafted networks as\nbackbones due to their high capacity of feature extraction, and also benefit\nfrom ImageNet pretraining. However, whether these network architectures are\nsuitable for 3D analysis or not remains unclear. In this paper, we propose a\nneural architecture search method named Auto-MVCNN which is particularly\ndesigned for optimizing architecture in multi-view 3D shape recognition.\nAuto-MVCNN extends gradient-based frameworks to process multi-view images, by\nautomatically searching the fusion cell to explore intrinsic correlation among\nview features. Moreover, we develop an end-to-end scheme to enhance retrieval\nperformance through the trade-off parameter search. Extensive experimental\nresults show that the searched architectures significantly outperform manually\ndesigned counterparts in various aspects, and our method achieves\nstate-of-the-art performance at the same time.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 07:40:28 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Li", "Zhaoqun", ""], ["Wang", "Hongren", ""], ["Li", "Jinxing", ""]]}, {"id": "2012.05499", "submitter": "Daizong Liu", "authors": "Daizong Liu, Shuangjie Xu, Xiao-Yang Liu, Zichuan Xu, Wei Wei, Pan\n  Zhou", "title": "Spatiotemporal Graph Neural Network based Mask Reconstruction for Video\n  Object Segmentation", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of segmenting class-agnostic objects in\nsemi-supervised setting. Although previous detection based methods achieve\nrelatively good performance, these approaches extract the best proposal by a\ngreedy strategy, which may lose the local patch details outside the chosen\ncandidate. In this paper, we propose a novel spatiotemporal graph neural\nnetwork (STG-Net) to reconstruct more accurate masks for video object\nsegmentation, which captures the local contexts by utilizing all proposals. In\nthe spatial graph, we treat object proposals of a frame as nodes and represent\ntheir correlations with an edge weight strategy for mask context aggregation.\nTo capture temporal information from previous frames, we use a memory network\nto refine the mask of current frame by retrieving historic masks in a temporal\ngraph. The joint use of both local patch details and temporal relationships\nallow us to better address the challenges such as object occlusion and missing.\nWithout online learning and fine-tuning, our STG-Net achieves state-of-the-art\nperformance on four large benchmarks (DAVIS, YouTube-VOS, SegTrack-v2, and\nYouTube-Objects), demonstrating the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 07:57:44 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Liu", "Daizong", ""], ["Xu", "Shuangjie", ""], ["Liu", "Xiao-Yang", ""], ["Xu", "Zichuan", ""], ["Wei", "Wei", ""], ["Zhou", "Pan", ""]]}, {"id": "2012.05508", "submitter": "Diego Valsesia", "authors": "Giulia Fracastoro, Enrico Magli, Giovanni Poggi, Giuseppe Scarpa,\n  Diego Valsesia, Luisa Verdoliva", "title": "Deep Learning Methods For Synthetic Aperture Radar Image Despeckling: An\n  Overview Of Trends And Perspectives", "comments": null, "journal-ref": null, "doi": "10.1109/MGRS.2021.3070956", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Synthetic aperture radar (SAR) images are affected by a spatially-correlated\nand signal-dependent noise called speckle, which is very severe and may hinder\nimage exploitation. Despeckling is an important task that aims at removing such\nnoise, so as to improve the accuracy of all downstream image processing tasks.\nThe first despeckling methods date back to the 1970's, and several model-based\nalgorithms have been developed in the subsequent years. The field has received\ngrowing attention, sparkled by the availability of powerful deep learning\nmodels that have yielded excellent performance for inverse problems in image\nprocessing. This paper surveys the literature on deep learning methods applied\nto SAR despeckling, covering both the supervised and the more recent\nself-supervised approaches. We provide a critical analysis of existing methods\nwith the objective to recognize the most promising research lines, to identify\nthe factors that have limited the success of deep models, and to propose ways\nforward in an attempt to fully exploit the potential of deep learning for SAR\ndespeckling.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 08:30:43 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 17:13:48 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Fracastoro", "Giulia", ""], ["Magli", "Enrico", ""], ["Poggi", "Giovanni", ""], ["Scarpa", "Giuseppe", ""], ["Valsesia", "Diego", ""], ["Verdoliva", "Luisa", ""]]}, {"id": "2012.05509", "submitter": "Guoqing Bao", "authors": "Guoqing Bao, Huai Chen, Tongliang Liu, Guanzhong Gong, Yong Yin,\n  Lisheng Wang and Xiuying Wang", "title": "COVID-MTL: Multitask Learning with Shift3D and Random-weighted Loss for\n  Automated Diagnosis and Severity Assessment of COVID-19", "comments": "COVID-19 research; computer vision and pattern recognition; 13 pages,\n  10 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is an urgent need for automated methods to assist accurate and\neffective assessment of COVID-19. Radiology and nucleic acid test (NAT) are\ncomplementary COVID-19 diagnosis methods. In this paper, we present an\nend-to-end multitask learning (MTL) framework (COVID-MTL) that is capable of\nautomated and simultaneous detection (against both radiology and NAT) and\nseverity assessment of COVID-19. COVID-MTL learns different COVID-19 tasks in\nparallel through our novel random-weighted loss function, which assigns\nlearning weights under Dirichlet distribution to prevent task dominance; our\nnew 3D real-time augmentation algorithm (Shift3D) introduces space variances\nfor 3D CNN components by shifting low-level feature representations of\nvolumetric inputs in three dimensions; thereby, the MTL framework is able to\naccelerate convergence and improve joint learning performance compared to\nsingle-task models. By only using chest CT scans, COVID-MTL was trained on 930\nCT scans and tested on separate 399 cases. COVID-MTL achieved AUCs of 0.939 and\n0.846, and accuracies of 90.23% and 79.20% for detection of COVID-19 against\nradiology and NAT, respectively, which outperformed the state-of-the-art\nmodels. Meanwhile, COVID-MTL yielded AUC of 0.800 $\\pm$ 0.020 and 0.813 $\\pm$\n0.021 (with transfer learning) for classifying control/suspected, mild/regular,\nand severe/critically-ill cases. To decipher the recognition mechanism, we also\nidentified high-throughput lung features that were significantly related (P <\n0.001) to the positivity and severity of COVID-19.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 08:30:46 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 09:56:57 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2020 14:27:16 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Bao", "Guoqing", ""], ["Chen", "Huai", ""], ["Liu", "Tongliang", ""], ["Gong", "Guanzhong", ""], ["Yin", "Yong", ""], ["Wang", "Lisheng", ""], ["Wang", "Xiuying", ""]]}, {"id": "2012.05522", "submitter": "Jiashun Wang", "authors": "Jiashun Wang and Huazhe Xu and Jingwei Xu and Sifei Liu and Xiaolong\n  Wang", "title": "Synthesizing Long-Term 3D Human Motion and Interaction in 3D Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing 3D human motion plays an important role in many graphics\napplications as well as understanding human activity. While many efforts have\nbeen made on generating realistic and natural human motion, most approaches\nneglect the importance of modeling human-scene interactions and affordance. On\nthe other hand, affordance reasoning (e.g., standing on the floor or sitting on\nthe chair) has mainly been studied with static human pose and gestures, and it\nhas rarely been addressed with human motion. In this paper, we propose to\nbridge human motion synthesis and scene affordance reasoning. We present a\nhierarchical generative framework to synthesize long-term 3D human motion\nconditioning on the 3D scene structure. Building on this framework, we further\nenforce multiple geometry constraints between the human mesh and scene point\nclouds via optimization to improve realistic synthesis. Our experiments show\nsignificant improvements over previous approaches on generating natural and\nphysically plausible human motion in a scene.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 09:09:38 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 15:15:51 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wang", "Jiashun", ""], ["Xu", "Huazhe", ""], ["Xu", "Jingwei", ""], ["Liu", "Sifei", ""], ["Wang", "Xiaolong", ""]]}, {"id": "2012.05525", "submitter": "Ali Narin", "authors": "Ali Narin", "title": "Detection of Covid-19 Patients with Convolutional Neural Network Based\n  Features on Multi-class X-ray Chest Images", "comments": "Presented at 2020 Medical Technologies Congress, TIPTEKNO2020 (IEEE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covid-19 is a very serious deadly disease that has been announced as a\npandemic by the world health organization (WHO). The whole world is working\nwith all its might to end Covid-19 pandemic, which puts countries in serious\nhealth and economic problems, as soon as possible. The most important of these\nis to correctly identify those who get the Covid-19. Methods and approaches to\nsupport the reverse transcription polymerase chain reaction (RT-PCR) test have\nbegun to take place in the literature. In this study, chest X-ray images, which\ncan be accessed easily and quickly, were used because the covid-19 attacked the\nrespiratory systems. Classification performances with support vector machines\nhave been obtained by using the features extracted with residual networks\n(ResNet-50), one of the convolutional neural network models, from these images.\nWhile Covid-19 detection is obtained with support vector machines\n(SVM)-quadratic with the highest sensitivity value of 96.35% with the 5-fold\ncross-validation method, the highest overall performance value has been\ndetected with both SVM-quadratic and SVM-cubic above 99%. According to these\nhigh results, it is thought that this method, which has been studied, will help\nradiology specialists and reduce the rate of false detection.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 09:11:26 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Narin", "Ali", ""]]}, {"id": "2012.05534", "submitter": "Ali Narin", "authors": "Ali Narin and Ziynet Pamuk", "title": "Effect of Different Batch Size Parameters on Predicting of COVID19 Cases", "comments": "Presented for International Conference on Artificial Intelligence\n  towards Industry 4.0 (ICAII4.0 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The new coronavirus 2019, also known as COVID19, is a very serious epidemic\nthat has killed thousands or even millions of people since December 2019. It\nwas defined as a pandemic by the world health organization in March 2020. It is\nstated that this virus is usually transmitted by droplets caused by sneezing or\ncoughing, or by touching infected surfaces. The presence of the virus is\ndetected by real-time reverse transcriptase polymerase chain reaction (rRT-PCR)\ntests with the help of a swab taken from the nose or throat. In addition, X-ray\nand CT imaging methods are also used to support this method. Since it is known\nthat the accuracy sensitivity in rRT-PCR test is low, auxiliary diagnostic\nmethods have a very important place. Computer-aided diagnosis and detection\nsystems are developed especially with the help of X-ray and CT images. Studies\non the detection of COVID19 in the literature are increasing day by day. In\nthis study, the effect of different batch size (BH=3, 10, 20, 30, 40, and 50)\nparameter values on their performance in detecting COVID19 and other classes\nwas investigated using data belonging to 4 different (Viral Pneumonia, COVID19,\nNormal, Bacterial Pneumonia) classes. The study was carried out using a\npre-trained ResNet50 convolutional neural network. According to the obtained\nresults, they performed closely on the training and test data. However, it was\nobserved that the steady state in the test data was delayed as the batch size\nvalue increased. The highest COVID19 detection was 95.17% for BH = 3, while the\noverall accuracy value was 97.97% with BH = 20. According to the findings, it\ncan be said that the batch size value does not affect the overall performance\nsignificantly, but the increase in the batch size value delays obtaining stable\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 09:25:05 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Narin", "Ali", ""], ["Pamuk", "Ziynet", ""]]}, {"id": "2012.05535", "submitter": "Yuanqi Chen", "authors": "Yuanqi Chen, Ge Li, Cece Jin, Shan Liu, Thomas Li", "title": "SSD-GAN: Measuring the Realness in the Spatial and Spectral Domains", "comments": "Accepted to AAAI 2021. Code: https://github.com/cyq373/SSD-GAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper observes that there is an issue of high frequencies missing in the\ndiscriminator of standard GAN, and we reveal it stems from downsampling layers\nemployed in the network architecture. This issue makes the generator lack the\nincentive from the discriminator to learn high-frequency content of data,\nresulting in a significant spectrum discrepancy between generated images and\nreal images. Since the Fourier transform is a bijective mapping, we argue that\nreducing this spectrum discrepancy would boost the performance of GANs. To this\nend, we introduce SSD-GAN, an enhancement of GANs to alleviate the spectral\ninformation loss in the discriminator. Specifically, we propose to embed a\nfrequency-aware classifier into the discriminator to measure the realness of\nthe input in both the spatial and spectral domains. With the enhanced\ndiscriminator, the generator of SSD-GAN is encouraged to learn high-frequency\ncontent of real data and generate exact details. The proposed method is general\nand can be easily integrated into most existing GANs framework without\nexcessive cost. The effectiveness of SSD-GAN is validated on various network\narchitectures, objective functions, and datasets. Code will be available at\nhttps://github.com/cyq373/SSD-GAN.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 09:26:34 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 09:37:58 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 09:45:49 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Chen", "Yuanqi", ""], ["Li", "Ge", ""], ["Jin", "Cece", ""], ["Liu", "Shan", ""], ["Li", "Thomas", ""]]}, {"id": "2012.05536", "submitter": "Radu Horaud P", "authors": "Andrei Zaharescu, Edmond Boyer, and Radu Horaud", "title": "Topology-Adaptive Mesh Deformation for Surface Evolution, Morphing, and\n  Multi-View Reconstruction", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  33(4), April 2011", "doi": "10.1109/TPAMI.2010.116", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Triangulated meshes have become ubiquitous discrete-surface representations.\nIn this paper we address the problem of how to maintain the manifold properties\nof a surface while it undergoes strong deformations that may cause topological\nchanges. We introduce a new self-intersection removal algorithm, TransforMesh,\nand we propose a mesh evolution framework based on this algorithm. Numerous\nshape modelling applications use surface evolution in order to improve shape\nproperties, such as appearance or accuracy. Both explicit and implicit\nrepresentations can be considered for that purpose. However, explicit mesh\nrepresentations, while allowing for accurate surface modelling, suffer from the\ninherent difficulty of reliably dealing with self-intersections and topological\nchanges such as merges and splits. As a consequence, a majority of methods rely\non implicit representations of surfaces, e.g. level-sets, that naturally\novercome these issues. Nevertheless, these methods are based on volumetric\ndiscretizations, which introduce an unwanted precision-complexity trade-off.\nThe method that we propose handles topological changes in a robust manner and\nremoves self intersections, thus overcoming the traditional limitations of\nmesh-based approaches. To illustrate the effectiveness of TransforMesh, we\ndescribe two challenging applications, namely surface morphing and 3-D\nreconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 09:26:40 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Zaharescu", "Andrei", ""], ["Boyer", "Edmond", ""], ["Horaud", "Radu", ""]]}, {"id": "2012.05545", "submitter": "Zeliang Song", "authors": "Zeliang Song, Xiaofei Zhou, Zhendong Mao, Jianlong Tan", "title": "Image Captioning with Context-Aware Auxiliary Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning is a challenging computer vision task, which aims to\ngenerate a natural language description of an image. Most recent researches\nfollow the encoder-decoder framework which depends heavily on the previous\ngenerated words for the current prediction. Such methods can not effectively\ntake advantage of the future predicted information to learn complete semantics.\nIn this paper, we propose Context-Aware Auxiliary Guidance (CAAG) mechanism\nthat can guide the captioning model to perceive global contexts. Upon the\ncaptioning model, CAAG performs semantic attention that selectively\nconcentrates on useful information of the global predictions to reproduce the\ncurrent generation. To validate the adaptability of the method, we apply CAAG\nto three popular captioners and our proposal achieves competitive performance\non the challenging Microsoft COCO image captioning benchmark, e.g. 132.2\nCIDEr-D score on Karpathy split and 130.7 CIDEr-D (c40) score on official\nonline evaluation server.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 09:39:08 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 01:52:43 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Song", "Zeliang", ""], ["Zhou", "Xiaofei", ""], ["Mao", "Zhendong", ""], ["Tan", "Jianlong", ""]]}, {"id": "2012.05551", "submitter": "Jiahui Huang", "authors": "Jiahui Huang, Shi-Sheng Huang, Haoxuan Song, Shi-Min Hu", "title": "DI-Fusion: Online Implicit 3D Reconstruction with Deep Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous online 3D dense reconstruction methods struggle to achieve the\nbalance between memory storage and surface quality, largely due to the usage of\nstagnant underlying geometry representation, such as TSDF (truncated signed\ndistance functions) or surfels, without any knowledge of the scene priors. In\nthis paper, we present DI-Fusion (Deep Implicit Fusion), based on a novel 3D\nrepresentation, i.e. Probabilistic Local Implicit Voxels (PLIVoxs), for online\n3D reconstruction with a commodity RGB-D camera. Our PLIVox encodes scene\npriors considering both the local geometry and uncertainty parameterized by a\ndeep neural network. With such deep priors, we are able to perform online\nimplicit 3D reconstruction achieving state-of-the-art camera trajectory\nestimation accuracy and mapping quality, while achieving better storage\nefficiency compared with previous online 3D reconstruction approaches. Our\nimplementation is available at https://www.github.com/huangjh-pub/di-fusion.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 09:46:35 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 14:51:43 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Huang", "Jiahui", ""], ["Huang", "Shi-Sheng", ""], ["Song", "Haoxuan", ""], ["Hu", "Shi-Min", ""]]}, {"id": "2012.05567", "submitter": "Brian Lim", "authors": "Wencan Zhang, Mariella Dimiccoli, Brian Y. Lim", "title": "Debiased-CAM for bias-agnostic faithful visual explanations of deep\n  convolutional networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class activation maps (CAMs) explain convolutional neural network predictions\nby identifying salient pixels, but they become misaligned and misleading when\nexplaining predictions on images under bias, such as images blurred\naccidentally or deliberately for privacy protection, or images with improper\nwhite balance. Despite model fine-tuning to improve prediction performance on\nthese biased images, we demonstrate that CAM explanations become more deviated\nand unfaithful with increased image bias. We present Debiased-CAM to recover\nexplanation faithfulness across various bias types and levels by training a\nmulti-input, multi-task model with auxiliary tasks for CAM and bias level\npredictions. With CAM as a prediction task, explanations are made tunable by\nretraining the main model layers and made faithful by self-supervised learning\nfrom CAMs of unbiased images. The model provides representative, bias-agnostic\nCAM explanations about the predictions on biased images as if generated from\ntheir unbiased form. In four simulation studies with different biases and\nprediction tasks, Debiased-CAM improved both CAM faithfulness and task\nperformance. We further conducted two controlled user studies to validate its\ntruthfulness and helpfulness, respectively. Quantitative and qualitative\nanalyses of participant responses confirmed Debiased-CAM as more truthful and\nhelpful. Debiased-CAM thus provides a basis to generate more faithful and\nrelevant explanations for a wide range of real-world applications with various\nsources of bias.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 10:28:47 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Zhang", "Wencan", ""], ["Dimiccoli", "Mariella", ""], ["Lim", "Brian Y.", ""]]}, {"id": "2012.05570", "submitter": "Hong Zhang", "authors": "Hong Zhang and Haojie Li and Shenglun Chen and Tiantian Yan and Zhihui\n  Wang and Guo Lu and Wanli Ouyang", "title": "Direct Depth Learning Network for Stereo Matching", "comments": "10 pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being a crucial task of autonomous driving, Stereo matching has made great\nprogress in recent years. Existing stereo matching methods estimate disparity\ninstead of depth. They treat the disparity errors as the evaluation metric of\nthe depth estimation errors, since the depth can be calculated from the\ndisparity according to the triangulation principle. However, we find that the\nerror of the depth depends not only on the error of the disparity but also on\nthe depth range of the points. Therefore, even if the disparity error is low,\nthe depth error is still large, especially for the distant points. In this\npaper, a novel Direct Depth Learning Network (DDL-Net) is designed for stereo\nmatching. DDL-Net consists of two stages: the Coarse Depth Estimation stage and\nthe Adaptive-Grained Depth Refinement stage, which are all supervised by depth\ninstead of disparity. Specifically, Coarse Depth Estimation stage uniformly\nsamples the matching candidates according to depth range to construct cost\nvolume and output coarse depth. Adaptive-Grained Depth Refinement stage\nperforms further matching near the coarse depth to correct the imprecise\nmatching and wrong matching. To make the Adaptive-Grained Depth Refinement\nstage robust to the coarse depth and adaptive to the depth range of the points,\nthe Granularity Uncertainty is introduced to Adaptive-Grained Depth Refinement\nstage. Granularity Uncertainty adjusts the matching range and selects the\ncandidates' features according to coarse prediction confidence and depth range.\nWe verify the performance of DDL-Net on SceneFlow dataset and DrivingStereo\ndataset by different depth metrics. Results show that DDL-Net achieves an\naverage improvement of 25% on the SceneFlow dataset and $12\\%$ on the\nDrivingStereo dataset comparing the classical methods. More importantly, we\nachieve state-of-the-art accuracy at a large distance.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 10:33:57 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Zhang", "Hong", ""], ["Li", "Haojie", ""], ["Chen", "Shenglun", ""], ["Yan", "Tiantian", ""], ["Wang", "Zhihui", ""], ["Lu", "Guo", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2012.05578", "submitter": "Liangchen Luo", "authors": "Liangchen Luo, Mark Sandler, Zi Lin, Andrey Zhmoginov, Andrew Howard", "title": "Large-Scale Generative Data-Free Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge distillation is one of the most popular and effective techniques\nfor knowledge transfer, model compression and semi-supervised learning. Most\nexisting distillation approaches require the access to original or augmented\ntraining samples. But this can be problematic in practice due to privacy,\nproprietary and availability concerns. Recent work has put forward some methods\nto tackle this problem, but they are either highly time-consuming or unable to\nscale to large datasets. To this end, we propose a new method to train a\ngenerative image model by leveraging the intrinsic normalization layers'\nstatistics of the trained teacher network. This enables us to build an ensemble\nof generators without training data that can efficiently produce substitute\ninputs for subsequent distillation. The proposed method pushes forward the\ndata-free distillation performance on CIFAR-10 and CIFAR-100 to 95.02% and\n77.02% respectively. Furthermore, we are able to scale it to ImageNet dataset,\nwhich to the best of our knowledge, has never been done using generative models\nin a data-free setting.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 10:54:38 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Luo", "Liangchen", ""], ["Sandler", "Mark", ""], ["Lin", "Zi", ""], ["Zhmoginov", "Andrey", ""], ["Howard", "Andrew", ""]]}, {"id": "2012.05582", "submitter": "Radu Horaud P", "authors": "Yves Dufournaud, Cordelia Schmid, and Radu Horaud", "title": "Image Matching with Scale Adjustment", "comments": null, "journal-ref": "Computer Vision and Image Understanding, volume 93, 2004", "doi": "10.1016/j.cviu.2003.07.003", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we address the problem of matching two images with two\ndifferent resolutions: a high-resolution image and a low-resolution one. The\ndifference in resolution between the two images is not known and without loss\nof generality one of the images is assumed to be the high-resolution one. On\nthe premise that changes in resolution act as a smoothing equivalent to changes\nin scale, a scale-space representation of the high-resolution image is\nproduced. Hence the one-to-one classical image matching paradigm becomes\none-to-many because the low-resolution image is compared with all the\nscale-space representations of the high-resolution one. Key to the success of\nsuch a process is the proper representation of the features to be matched in\nscale-space. We show how to represent and extract interest points at variable\nscales and we devise a method allowing the comparison of two images at two\ndifferent resolutions. The method comprises the use of photometric- and\nrotation-invariant descriptors, a geometric model mapping the high-resolution\nimage onto a low-resolution image region, and an image matching strategy based\non local constraints and on the robust estimation of this geometric model.\nExtensive experiments show that our matching method can be used for scale\nchanges up to a factor of 6.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 11:03:25 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Dufournaud", "Yves", ""], ["Schmid", "Cordelia", ""], ["Horaud", "Radu", ""]]}, {"id": "2012.05585", "submitter": "Ali Narin", "authors": "Ali Narin", "title": "Performance Comparison of Balanced and Unbalanced Cancer Datasets using\n  Pre-Trained Convolutional Neural Network", "comments": "Presented for International Conference on Artificial Intelligence\n  towards Industry 4.0 (ICAII4.0 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cancer disease is one of the leading causes of death all over the world.\nBreast cancer, which is a common cancer disease especially in women, is quite\ncommon. The most important tool used for early detection of this cancer type,\nwhich requires a long process to establish a definitive diagnosis, is\nhistopathological images taken by biopsy. These obtained images are examined by\npathologists and a definitive diagnosis is made. It is quite common to detect\nthis process with the help of a computer. Detection of benign or malignant\ntumors, especially by using data with different magnification rates, takes\nplace in the literature. In this study, two different balanced and unbalanced\nstudy groups have been formed by using the histopathological data in the\nBreakHis data set. We have examined how the performances of balanced and\nunbalanced data sets change in detecting tumor type. In conclusion, in the\nstudy performed using the InceptionV3 convolution neural network model, 93.55%\naccuracy, 99.19% recall and 87.10% specificity values have been obtained for\nbalanced data, while 89.75% accuracy, 82.89% recall and 91.51% specificity\nvalues have been obtained for unbalanced data. According to the results\nobtained in two different studies, the balance of the data increases the\noverall performance as well as the detection performance of both benign and\nmalignant tumors. It can be said that the model trained with the help of data\nsets created in a balanced way will give pathology specialists higher and\naccurate results.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 11:07:59 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Narin", "Ali", ""]]}, {"id": "2012.05586", "submitter": "Hong Zhang", "authors": "Hong Zhang and Shenglun Chen and Zhihui Wang and Haojie Li and Wanli\n  Ouyang", "title": "Full Matching on Low Resolution for Disparity Estimation", "comments": "9pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Multistage Full Matching disparity estimation scheme (MFM) is proposed in\nthis work. We demonstrate that decouple all similarity scores directly from the\nlow-resolution 4D volume step by step instead of estimating low-resolution 3D\ncost volume through focusing on optimizing the low-resolution 4D volume\niteratively leads to more accurate disparity. To this end, we first propose to\ndecompose the full matching task into multiple stages of the cost aggregation\nmodule. Specifically, we decompose the high-resolution predicted results into\nmultiple groups, and every stage of the newly designed cost aggregation module\nlearns only to estimate the results for a group of points. This alleviates the\nproblem of feature internal competitive when learning similarity scores of all\ncandidates from one low-resolution 4D volume output from one stage. Then, we\npropose the strategy of \\emph{Stages Mutual Aid}, which takes advantage of the\nrelationship of multiple stages to boost similarity scores estimation of each\nstage, to solve the unbalanced prediction of multiple stages caused by serial\nmultistage framework. Experiment results demonstrate that the proposed method\nachieves more accurate disparity estimation results and outperforms\nstate-of-the-art methods on Scene Flow, KITTI 2012 and KITTI 2015 datasets.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 11:11:23 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Zhang", "Hong", ""], ["Chen", "Shenglun", ""], ["Wang", "Zhihui", ""], ["Li", "Haojie", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2012.05590", "submitter": "Ziwei Wang", "authors": "Ziwei Wang, Yonhon Ng, Cedric Scheerlinck, Robert Mahony", "title": "An Asynchronous Kalman Filter for Hybrid Event Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are ideally suited to capture HDR visual information without\nblur but perform poorly on static or slowly changing scenes. Conversely,\nconventional image sensors measure absolute intensity of slowly changing scenes\neffectively but do poorly on high dynamic range or quickly changing scenes. In\nthis paper, we present an event-based video reconstruction pipeline for High\nDynamic Range (HDR) scenarios. The proposed algorithm includes a frame\naugmentation pre-processing step that deblurs and temporally interpolates frame\ndata using events. The augmented frame and event data are then fused using a\nnovel asynchronous Kalman filter under a unifying uncertainty model for both\nsensors. Our experimental results are evaluated on both publicly available\ndatasets with challenging lighting conditions and fast motions and our new\ndataset with HDR reference. The proposed algorithm outperforms state-of-the-art\nmethods in both absolute intensity error (48% reduction) and image similarity\nindexes (average 11% improvement).\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 11:24:07 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 08:32:43 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wang", "Ziwei", ""], ["Ng", "Yonhon", ""], ["Scheerlinck", "Cedric", ""], ["Mahony", "Robert", ""]]}, {"id": "2012.05598", "submitter": "Yuting Xiao", "authors": "Yuting Xiao, Yanyu Xu, Ziming Zhong, Weixin Luo, Jiawei Li, Shenghua\n  Gao", "title": "Amodal Segmentation Based on Visible Region Segmentation and Shape Prior", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Almost all existing amodal segmentation methods make the inferences of\noccluded regions by using features corresponding to the whole image. This is\nagainst the human's amodal perception, where human uses the visible part and\nthe shape prior knowledge of the target to infer the occluded region. To mimic\nthe behavior of human and solve the ambiguity in the learning, we propose a\nframework, it firstly estimates a coarse visible mask and a coarse amodal mask.\nThen based on the coarse prediction, our model infers the amodal mask by\nconcentrating on the visible region and utilizing the shape prior in the\nmemory. In this way, features corresponding to background and occlusion can be\nsuppressed for amodal mask estimation. Consequently, the amodal mask would not\nbe affected by what the occlusion is given the same visible regions. The\nleverage of shape prior makes the amodal mask estimation more robust and\nreasonable. Our proposed model is evaluated on three datasets. Experiments show\nthat our proposed model outperforms existing state-of-the-art methods. The\nvisualization of shape prior indicates that the category-specific feature in\nthe codebook has certain interpretability.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 11:39:09 GMT"}, {"version": "v2", "created": "Sat, 19 Dec 2020 13:24:36 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Xiao", "Yuting", ""], ["Xu", "Yanyu", ""], ["Zhong", "Ziming", ""], ["Luo", "Weixin", ""], ["Li", "Jiawei", ""], ["Gao", "Shenghua", ""]]}, {"id": "2012.05608", "submitter": "Bowen Cai", "authors": "Bowen Cai, Huan Fu, Rongfei Jia, Binqiang Zhao, Hua Li, Yinghui Xu", "title": "Exploiting Diverse Characteristics and Adversarial Ambivalence for\n  Domain Adaptive Segmentation", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adapting semantic segmentation models to new domains is an important but\nchallenging problem. Recently enlightening progress has been made, but the\nperformance of existing methods are unsatisfactory on real datasets where the\nnew target domain comprises of heterogeneous sub-domains (e.g., diverse weather\ncharacteristics). We point out that carefully reasoning about the multiple\nmodalities in the target domain can improve the robustness of adaptation\nmodels. To this end, we propose a condition-guided adaptation framework that is\nempowered by a special attentive progressive adversarial training (APAT)\nmechanism and a novel self-training policy. The APAT strategy progressively\nperforms condition-specific alignment and attentive global feature matching.\nThe new self-training scheme exploits the adversarial ambivalences of easy and\nhard adaptation regions and the correlations among target sub-domains\neffectively. We evaluate our method (DCAA) on various adaptation scenarios\nwhere the target images vary in weather conditions. The comparisons against\nbaselines and the state-of-the-art approaches demonstrate the superiority of\nDCAA over the competitors.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 11:50:59 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 16:53:26 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Cai", "Bowen", ""], ["Fu", "Huan", ""], ["Jia", "Rongfei", ""], ["Zhao", "Binqiang", ""], ["Li", "Hua", ""], ["Xu", "Yinghui", ""]]}, {"id": "2012.05609", "submitter": "Risheng Liu", "authors": "Risheng Liu and Long Ma and Jiaao Zhang and Xin Fan and Zhongxuan Luo", "title": "Retinex-inspired Unrolling with Cooperative Prior Architecture Search\n  for Low-light Image Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-light image enhancement plays very important roles in low-level vision\nfield. Recent works have built a large variety of deep learning models to\naddress this task. However, these approaches mostly rely on significant\narchitecture engineering and suffer from high computational burden. In this\npaper, we propose a new method, named Retinex-inspired Unrolling with\nArchitecture Search (RUAS), to construct lightweight yet effective enhancement\nnetwork for low-light images in real-world scenario. Specifically, building\nupon Retinex rule, RUAS first establishes models to characterize the intrinsic\nunderexposed structure of low-light images and unroll their optimization\nprocesses to construct our holistic propagation structure. Then by designing a\ncooperative reference-free learning strategy to discover low-light prior\narchitectures from a compact search space, RUAS is able to obtain a\ntop-performing image enhancement network, which is with fast speed and requires\nfew computational resources. Extensive experiments verify the superiority of\nour RUAS framework against recently proposed state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 11:51:23 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Liu", "Risheng", ""], ["Ma", "Long", ""], ["Zhang", "Jiaao", ""], ["Fan", "Xin", ""], ["Luo", "Zhongxuan", ""]]}, {"id": "2012.05616", "submitter": "Ronak Kosti", "authors": "Prathmesh Madhu, Angel Villar-Corrales, Ronak Kosti, Torsten\n  Bendschus, Corinna Reinhardt, Peter Bell, Andreas Maier, Vincent Christlein", "title": "Enhancing Human Pose Estimation in Ancient Vase Paintings via\n  Perceptually-grounded Style Transfer Learning", "comments": "Link to the repository containing the code to reproduce the\n  experiments. For further details, please read the README. Link:\n  https://anonymous.4open.science/r/3b1bd8ac-bd3a-4df6-8671-56d4f9bdbd8d/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation (HPE) is a central part of understanding the visual\nnarration and body movements of characters depicted in artwork collections,\nsuch as Greek vase paintings. Unfortunately, existing HPE methods do not\ngeneralise well across domains resulting in poorly recognized poses. Therefore,\nwe propose a two step approach: (1) adapting a dataset of natural images of\nknown person and pose annotations to the style of Greek vase paintings by means\nof image style-transfer. We introduce a perceptually-grounded style transfer\ntraining to enforce perceptual consistency. Then, we fine-tune the base model\nwith this newly created dataset. We show that using style-transfer learning\nsignificantly improves the SOTA performance on unlabelled data by more than 6%\nmean average precision (mAP) as well as mean average recall (mAR). (2) To\nimprove the already strong results further, we created a small dataset\n(ClassArch) consisting of ancient Greek vase paintings from the 6-5th century\nBCE with person and pose annotations. We show that fine-tuning on this data\nwith a style-transferred model improves the performance further. In a thorough\nablation study, we give a targeted analysis of the influence of style\nintensities, revealing that the model learns generic domain styles.\nAdditionally, we provide a pose-based image retrieval to demonstrate the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 12:08:03 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Madhu", "Prathmesh", ""], ["Villar-Corrales", "Angel", ""], ["Kosti", "Ronak", ""], ["Bendschus", "Torsten", ""], ["Reinhardt", "Corinna", ""], ["Bell", "Peter", ""], ["Maier", "Andreas", ""], ["Christlein", "Vincent", ""]]}, {"id": "2012.05633", "submitter": "Gerasimos Spanakis", "authors": "Adam Vandor, Marie van Vollenhoven, Gerhard Weiss, Gerasimos Spanakis", "title": "Can we detect harmony in artistic compositions? A machine learning\n  approach", "comments": "9 pages, ICAART 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Harmony in visual compositions is a concept that cannot be defined or easily\nexpressed mathematically, even by humans. The goal of the research described in\nthis paper was to find a numerical representation of artistic compositions with\ndifferent levels of harmony. We ask humans to rate a collection of grayscale\nimages based on the harmony they convey. To represent the images, a set of\nspecial features were designed and extracted. By doing so, it became possible\nto assign objective measures to subjectively judged compositions. Given the\nratings and the extracted features, we utilized machine learning algorithms to\nevaluate the efficiency of such representations in a harmony classification\nproblem. The best performing model (SVM) achieved 80% accuracy in\ndistinguishing between harmonic and disharmonic images, which reinforces the\nassumption that concept of harmony can be expressed in a mathematical way that\ncan be assessed by humans.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 12:31:12 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Vandor", "Adam", ""], ["van Vollenhoven", "Marie", ""], ["Weiss", "Gerhard", ""], ["Spanakis", "Gerasimos", ""]]}, {"id": "2012.05649", "submitter": "Mert B\\\"ulent Sar{\\i}y{\\i}ld{\\i}z", "authors": "Mert Bulent Sariyildiz, Yannis Kalantidis, Diane Larlus, Karteek\n  Alahari", "title": "Concept Generalization in Visual Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring concept generalization, i.e., the extent to which models trained on\na set of (seen) visual concepts can be used to recognize a new set of (unseen)\nconcepts, is a popular way of evaluating visual representations, especially\nwhen they are learned with self-supervised learning. Nonetheless, the choice of\nwhich unseen concepts to use is usually made arbitrarily, and independently\nfrom the seen concepts used to train representations, thus ignoring any\nsemantic relationships between the two. In this paper, we argue that semantic\nrelationships between seen and unseen concepts affect generalization\nperformance and propose ImageNet-CoG, a novel benchmark on the ImageNet dataset\nthat enables measuring concept generalization in a principled way. Our\nbenchmark leverages expert knowledge that comes from WordNet in order to define\na sequence of unseen ImageNet concept sets that are semantically more and more\ndistant from the ImageNet-1K subset, a ubiquitous training set. This allows us\nto benchmark visual representations learned on ImageNet-1K out-of-the box: we\nanalyse a number of such models from supervised, semi-supervised and\nself-supervised approaches under the prism of concept generalization, and show\nhow our benchmark is able to uncover a number of interesting insights. We will\nprovide resources for the benchmark at\nhttps://europe.naverlabs.com/cog-benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 13:13:22 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Sariyildiz", "Mert Bulent", ""], ["Kalantidis", "Yannis", ""], ["Larlus", "Diane", ""], ["Alahari", "Karteek", ""]]}, {"id": "2012.05657", "submitter": "Itai Lang", "authors": "Itai Lang, Uriel Kotlicki, Shai Avidan", "title": "Geometric Adversarial Attacks and Defenses on 3D Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are prone to adversarial examples that maliciously alter\nthe network's outcome. Due to the increasing popularity of 3D sensors in\nsafety-critical systems and the vast deployment of deep learning models for 3D\npoint sets, there is a growing interest in adversarial attacks and defenses for\nsuch models. So far, the research has focused on the semantic level, namely,\ndeep point cloud classifiers. However, point clouds are also widely used in a\ngeometric-related form that includes encoding and reconstructing the geometry.\nIn this work, we explore adversarial examples at a geometric level. That is, a\nsmall change to a clean source point cloud leads, after passing through an\nautoencoder model, to a shape from a different target class. On the defense\nside, we show that remnants of the attack's target shape are still present at\nthe reconstructed output after applying the defense to the adversarial input.\nOur code is publicly available at https://github.com/itailang/geometric_adv.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 13:30:06 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Lang", "Itai", ""], ["Kotlicki", "Uriel", ""], ["Avidan", "Shai", ""]]}, {"id": "2012.05661", "submitter": "Olivier Rukundo", "authors": "Olivier Rukundo", "title": "Effect of the regularization hyperparameter on deep learning-based\n  segmentation in LGE-MRI", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, the author aims at demonstrating the extent to which the\narbitrary selection of the L2 regularization hyperparameter can affect the\noutcome of deep learning-based segmentation in LGE-MRI. Here, arbitrary L2\nregularization values are used to create different deep learning-based\nsegmentation networks. Also, the author adopts the manual adjustment or\ntunning, of other deep learning hyperparameters, to be done only when 10% of\nall epochs are reached before achieving the 90% validation accuracy. The\nexperimental comparisons demonstrate that small L2 regularization values can\nlead to better segmentation of the myocardial boundaries.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 13:35:40 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 08:56:25 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 13:21:29 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Rukundo", "Olivier", ""]]}, {"id": "2012.05689", "submitter": "Rui Yan", "authors": "Rui Yan, Lingxi Xie, Xiangbo Shu, and Jinhui Tang", "title": "Interactive Fusion of Multi-level Features for Compositional Activity\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand a complex action, multiple sources of information, including\nappearance, positional, and semantic features, need to be integrated. However,\nthese features are difficult to be fused since they often differ significantly\nin modality and dimensionality. In this paper, we present a novel framework\nthat accomplishes this goal by interactive fusion, namely, projecting features\nacross different spaces and guiding it using an auxiliary prediction task.\nSpecifically, we implement the framework in three steps, namely,\npositional-to-appearance feature extraction, semantic feature interaction, and\nsemantic-to-positional prediction. We evaluate our approach on two action\nrecognition datasets, Something-Something and Charades. Interactive fusion\nachieves consistent accuracy gain beyond off-the-shelf action recognition\nalgorithms. In particular, on Something-Else, the compositional setting of\nSomething-Something, interactive fusion reports a remarkable gain of 2.9% in\nterms of top-1 accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 14:17:18 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Yan", "Rui", ""], ["Xie", "Lingxi", ""], ["Shu", "Xiangbo", ""], ["Tang", "Jinhui", ""]]}, {"id": "2012.05694", "submitter": "Sayan Nag", "authors": "Sayan Nag", "title": "Lookahead optimizer improves the performance of Convolutional\n  Autoencoders for reconstruction of natural images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autoencoders are a class of artificial neural networks which have gained a\nlot of attention in the recent past. Using the encoder block of an autoencoder\nthe input image can be compressed into a meaningful representation. Then a\ndecoder is employed to reconstruct the compressed representation back to a\nversion which looks like the input image. It has plenty of applications in the\nfield of data compression and denoising. Another version of Autoencoders (AE)\nexist, called Variational AE (VAE) which acts as a generative model like GAN.\nRecently, an optimizer was introduced which is known as lookahead optimizer\nwhich significantly enhances the performances of Adam as well as SGD. In this\npaper, we implement Convolutional Autoencoders (CAE) and Convolutional\nVariational Autoencoders (CVAE) with lookahead optimizer (with Adam) and\ncompare them with the Adam (only) optimizer counterparts. For this purpose, we\nhave used a movie dataset comprising of natural images for the former case and\nCIFAR100 for the latter case. We show that lookahead optimizer (with Adam)\nimproves the performance of CAEs for reconstruction of natural images.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 03:18:28 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Nag", "Sayan", ""]]}, {"id": "2012.05695", "submitter": "Giovanni Cerchiari", "authors": "M. Norouzisadeh, G. Cerchiari and F. Croccolo", "title": "Increased performance in DDM analysis by calculating structure functions\n  through Fourier transform in time", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV physics.app-ph physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differential Dynamic Microscopy (DDM) is the combination of optical\nmicroscopy to statistical analysis to obtain information about the dynamical\nbehaviour of a variety of samples spanning from soft matter physics to biology.\nIn DDM, the dynamical evolution of the samples is investigated separately at\ndifferent length scales and extracted from a set of images recorded at\ndifferent times. A specific result of interest is the structure function that\ncan be computed via spatial Fourier transforms and differences of signals. In\nthis work, we present an algorithm to efficiently process a set of images\naccording to the DDM analysis scheme. We bench-marked the new approach against\nthe state-of-the-art algorithm reported in previous work. The new\nimplementation computes the DDM analysis faster, thanks to an additional\nFourier transform in time instead of performing differences of signals. This\nallows obtaining very fast analysis also in CPU based machine. In order to test\nthe new code, we performed the DDM analysis over sets of more than 1000 images\nwith and without the help of GPU hardware acceleration. As an example, for\nimages of $512 \\times 512$ pixels, the new algorithm is 10 times faster than\nthe previous GPU code. Without GPU hardware acceleration and for the same set\nof images, we found that the new algorithm is 300 faster than the old one both\nrunning only on the CPU.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 21:12:45 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Norouzisadeh", "M.", ""], ["Cerchiari", "G.", ""], ["Croccolo", "F.", ""]]}, {"id": "2012.05698", "submitter": "Agelos Kratimenos", "authors": "Agelos Kratimenos, Georgios Pavlakos, Petros Maragos", "title": "Independent Sign Language Recognition with 3D Body, Hands, and Face\n  Reconstruction", "comments": "Submitted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Independent Sign Language Recognition is a complex visual recognition problem\nthat combines several challenging tasks of Computer Vision due to the necessity\nto exploit and fuse information from hand gestures, body features and facial\nexpressions. While many state-of-the-art works have managed to deeply elaborate\non these features independently, to the best of our knowledge, no work has\nadequately combined all three information channels to efficiently recognize\nSign Language. In this work, we employ SMPL-X, a contemporary parametric model\nthat enables joint extraction of 3D body shape, face and hands information from\na single image. We use this holistic 3D reconstruction for SLR, demonstrating\nthat it leads to higher accuracy than recognition from raw RGB images and their\noptical flow fed into the state-of-the-art I3D-type network for 3D action\nrecognition and from 2D Openpose skeletons fed into a Recurrent Neural Network.\nFinally, a set of experiments on the body, face and hand features showed that\nneglecting any of these, significantly reduces the classification accuracy,\nproving the importance of jointly modeling body shape, facial expression and\nhand pose for Sign Language Recognition.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 23:50:26 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Kratimenos", "Agelos", ""], ["Pavlakos", "Georgios", ""], ["Maragos", "Petros", ""]]}, {"id": "2012.05701", "submitter": "Michael Fulton", "authors": "Karin de Langis, Michael Fulton, Junaed Sattar", "title": "An Analysis of Deep Object Detectors For Diver Detection", "comments": "14 pages, submitted for ICRA 21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the end goal of selecting and using diver detection models to support\nhuman-robot collaboration capabilities such as diver following, we thoroughly\nanalyze a large set of deep neural networks for diver detection. We begin by\nproducing a dataset of approximately 105,000 annotated images of divers sourced\nfrom videos -- one of the largest and most varied diver detection datasets ever\ncreated. Using this dataset, we train a variety of state-of-the-art deep neural\nnetworks for object detection, including SSD with Mobilenet, Faster R-CNN, and\nYOLO. Along with these single-frame detectors, we also train networks designed\nfor detection of objects in a video stream, using temporal information as well\nas single-frame image information. We evaluate these networks on typical\naccuracy and efficiency metrics, as well as on the temporal stability of their\ndetections. Finally, we analyze the failures of these detectors, pointing out\nthe most common scenarios of failure. Based on our results, we recommend SSDs\nor Tiny-YOLOv4 for real-time applications on robots and recommend further\ninvestigation of video object detection methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 01:50:32 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["de Langis", "Karin", ""], ["Fulton", "Michael", ""], ["Sattar", "Junaed", ""]]}, {"id": "2012.05703", "submitter": "Kaixuan Wei", "authors": "Kaixuan Wei, Angelica Aviles-Rivero, Jingwei Liang, Ying Fu, Hua\n  Huang, Carola-Bibiane Sch\\\"onlieb", "title": "TFPnP: Tuning-free Plug-and-Play Proximal Algorithm with Applications to\n  Inverse Imaging Problems", "comments": "The Journal extension of arXiv:2002.09611", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plug-and-Play (PnP) is a non-convex framework that combines proximal\nalgorithms, for example alternating direction method of multipliers (ADMM),\nwith advanced denoiser priors. Over the past few years, great empirical success\nhas been obtained by PnP algorithms, especially for the ones integrated with\ndeep learning-based denoisers. However, a crucial issue of PnP approaches is\nthe need of manual parameter tweaking. As it is essential to obtain\nhigh-quality results across the high discrepancy in terms of imaging conditions\nand varying scene content. In this work, we present a tuning-free PnP proximal\nalgorithm, which can automatically determine the internal parameters including\nthe penalty parameter, the denoising strength and the termination time. A core\npart of our approach is to develop a policy network for automatic search of\nparameters, which can be effectively learned via mixed model-free and\nmodel-based deep reinforcement learning. We demonstrate, through a set of\nnumerical and visual experiments, that the learned policy can customize\ndifferent parameters for different states, and often more efficient and\neffective than existing handcrafted criteria. Moreover, we discuss the\npractical considerations of the plugged denoisers, which together with our\nlearned policy yield to state-of-the-art results. This is prevalent on both\nlinear and nonlinear exemplary inverse imaging problems, and in particular, we\nshow promising results on compressed sensing MRI, sparse-view CT and phase\nretrieval.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 14:19:30 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 04:03:20 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Wei", "Kaixuan", ""], ["Aviles-Rivero", "Angelica", ""], ["Liang", "Jingwei", ""], ["Fu", "Ying", ""], ["Huang", "Hua", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "2012.05710", "submitter": "Paul Hongsuck Seo", "authors": "Paul Hongsuck Seo, Arsha Nagrani, Cordelia Schmid", "title": "Look Before you Speak: Visually Contextualized Utterances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most conversational AI systems focus on textual dialogue only,\nconditioning utterances on visual context (when it's available) can lead to\nmore realistic conversations. Unfortunately, a major challenge for\nincorporating visual context into conversational dialogue is the lack of\nlarge-scale labeled datasets. We provide a solution in the form of a new\nvisually conditioned Future Utterance Prediction task. Our task involves\npredicting the next utterance in a video, using both visual frames and\ntranscribed speech as context. By exploiting the large number of instructional\nvideos online, we train a model to solve this task at scale, without the need\nfor manual annotations. Leveraging recent advances in multimodal learning, our\nmodel consists of a novel co-attentional multimodal video transformer, and when\ntrained on both textual and visual context, outperforms baselines that use\ntextual inputs alone. Further, we demonstrate that our model trained for this\ntask on unlabelled videos achieves state-of-the-art performance on a number of\ndownstream VideoQA benchmarks such as MSRVTT-QA, MSVD-QA, ActivityNet-QA and\nHow2QA.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 14:47:02 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 01:54:02 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Seo", "Paul Hongsuck", ""], ["Nagrani", "Arsha", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2012.05739", "submitter": "Chiawei Tang", "authors": "Chia-Wei Tang, Chao-Lin Liu and Po-Sen Chiu", "title": "HRCenterNet: An Anchorless Approach to Chinese Character Segmentation in\n  Historical Documents", "comments": null, "journal-ref": null, "doi": "10.1109/BigData50022.2020.9378051", "report-no": null, "categories": "cs.CV cs.DL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The information provided by historical documents has always been\nindispensable in the transmission of human civilization, but it has also made\nthese books susceptible to damage due to various factors. Thanks to recent\ntechnology, the automatic digitization of these documents are one of the\nquickest and most effective means of preservation. The main steps of automatic\ntext digitization can be divided into two stages, mainly: character\nsegmentation and character recognition, where the recognition results depend\nlargely on the accuracy of segmentation. Therefore, in this study, we will only\nfocus on the character segmentation of historical Chinese documents. In this\nresearch, we propose a model named HRCenterNet, which is combined with an\nanchorless object detection method and parallelized architecture. The MTHv2\ndataset consists of over 3000 Chinese historical document images and over 1\nmillion individual Chinese characters; with these enormous data, the\nsegmentation capability of our model achieves IoU 0.81 on average with the best\nspeed-accuracy trade-off compared to the others. Our source code is available\nat https://github.com/Tverous/HRCenterNet.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 15:21:02 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Tang", "Chia-Wei", ""], ["Liu", "Chao-Lin", ""], ["Chiu", "Po-Sen", ""]]}, {"id": "2012.05740", "submitter": "Ruddy Th\\'eodose", "authors": "Ruddy Th\\'eodose, Dieumet Denis, Thierry Chateau, Vincent Fr\\'emont,\n  Paul Checchin", "title": "R-AGNO-RPN: A LIDAR-Camera Region Deep Network for Resolution-Agnostic\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current neural networks-based object detection approaches processing LiDAR\npoint clouds are generally trained from one kind of LiDAR sensors. However,\ntheir performances decrease when they are tested with data coming from a\ndifferent LiDAR sensor than the one used for training, i.e., with a different\npoint cloud resolution. In this paper, R-AGNO-RPN, a region proposal network\nbuilt on fusion of 3D point clouds and RGB images is proposed for 3D object\ndetection regardless of point cloud resolution. As our approach is designed to\nbe also applied on low point cloud resolutions, the proposed method focuses on\nobject localization instead of estimating refined boxes on reduced data. The\nresilience to low-resolution point cloud is obtained through image features\naccurately mapped to Bird's Eye View and a specific data augmentation procedure\nthat improves the contribution of the RGB images. To show the proposed\nnetwork's ability to deal with different point clouds resolutions, experiments\nare conducted on both data coming from the KITTI 3D Object Detection and the\nnuScenes datasets. In addition, to assess its performances, our method is\ncompared to PointPillars, a well-known 3D detection network. Experimental\nresults show that even on point cloud data reduced by $80\\%$ of its original\npoints, our method is still able to deliver relevant proposals localization.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 15:22:58 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Th\u00e9odose", "Ruddy", ""], ["Denis", "Dieumet", ""], ["Chateau", "Thierry", ""], ["Fr\u00e9mont", "Vincent", ""], ["Checchin", "Paul", ""]]}, {"id": "2012.05745", "submitter": "Andre Mastmeyer", "authors": "Daria Kern, Andre Mastmeyer", "title": "3D Bounding Box Detection in Volumetric Medical Image Data: A Systematic\n  Literature Review", "comments": "10 pages, 5 figures, 1 table", "journal-ref": "IEEE ICIEA / JOIG 2021", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses current methods and trends for 3D bounding box detection\nin volumetric medical image data. For this purpose, an overview of relevant\npapers from recent years is given. 2D and 3D implementations are discussed and\ncompared. Multiple identified approaches for localizing anatomical structures\nare presented. The results show that most research recently focuses on Deep\nLearning methods, such as Convolutional Neural Networks vs. methods with manual\nfeature engineering, e.g. Random-Regression-Forests. An overview of bounding\nbox detection options is presented and helps researchers to select the most\npromising approach for their target objects.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 15:28:34 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Kern", "Daria", ""], ["Mastmeyer", "Andre", ""]]}, {"id": "2012.05767", "submitter": "Yulei Qin", "authors": "Yulei Qin, Hao Zheng, Yun Gu, Xiaolin Huang, Jie Yang, Lihui Wang,\n  Feng Yao, Yue-Min Zhu, Guang-Zhong Yang", "title": "Learning Tubule-Sensitive CNNs for Pulmonary Airway and Artery-Vein\n  Segmentation in CT", "comments": "15 pages, IEEE TMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training convolutional neural networks (CNNs) for segmentation of pulmonary\nairway, artery, and vein is challenging due to sparse supervisory signals\ncaused by the severe class imbalance between tubular targets and background. We\npresent a CNNs-based method for accurate airway and artery-vein segmentation in\nnon-contrast computed tomography. It enjoys superior sensitivity to tenuous\nperipheral bronchioles, arterioles, and venules. The method first uses a\nfeature recalibration module to make the best use of features learned from the\nneural networks. Spatial information of features is properly integrated to\nretain relative priority of activated regions, which benefits the subsequent\nchannel-wise recalibration. Then, attention distillation module is introduced\nto reinforce representation learning of tubular objects. Fine-grained details\nin high-resolution attention maps are passing down from one layer to its\nprevious layer recursively to enrich context. Anatomy prior of lung context map\nand distance transform map is designed and incorporated for better artery-vein\ndifferentiation capacity. Extensive experiments demonstrated considerable\nperformance gains brought by these components. Compared with state-of-the-art\nmethods, our method extracted much more branches while maintaining competitive\noverall segmentation performance. Codes and models are available at\nhttp://www.pami.sjtu.edu.cn/News/56\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 15:56:08 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 00:37:48 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2021 08:34:53 GMT"}, {"version": "v4", "created": "Wed, 24 Feb 2021 10:32:47 GMT"}, {"version": "v5", "created": "Thu, 25 Feb 2021 08:22:26 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Qin", "Yulei", ""], ["Zheng", "Hao", ""], ["Gu", "Yun", ""], ["Huang", "Xiaolin", ""], ["Yang", "Jie", ""], ["Wang", "Lihui", ""], ["Yao", "Feng", ""], ["Zhu", "Yue-Min", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "2012.05780", "submitter": "Peize Sun", "authors": "Peize Sun, Yi Jiang, Enze Xie, Wenqi Shao, Zehuan Yuan, Changhu Wang,\n  Ping Luo", "title": "What Makes for End-to-End Object Detection?", "comments": "ICML version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection has recently achieved a breakthrough for removing the last\none non-differentiable component in the pipeline, Non-Maximum Suppression\n(NMS), and building up an end-to-end system. However, what makes for its\none-to-one prediction has not been well understood. In this paper, we first\npoint out that one-to-one positive sample assignment is the key factor, while,\none-to-many assignment in previous detectors causes redundant predictions in\ninference. Second, we surprisingly find that even training with one-to-one\nassignment, previous detectors still produce redundant predictions. We identify\nthat classification cost in matching cost is the main ingredient: (1) previous\ndetectors only consider location cost, (2) by additionally introducing\nclassification cost, previous detectors immediately produce one-to-one\nprediction during inference. We introduce the concept of score gap to explore\nthe effect of matching cost. Classification cost enlarges the score gap by\nchoosing positive samples as those of highest score in the training iteration\nand reducing noisy positive samples brought by only location cost. Finally, we\ndemonstrate the advantages of end-to-end object detection on crowded scenes.\nThe code is available at: \\url{https://github.com/PeizeSun/OneNet}.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 16:15:19 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 15:53:21 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Sun", "Peize", ""], ["Jiang", "Yi", ""], ["Xie", "Enze", ""], ["Shao", "Wenqi", ""], ["Yuan", "Zehuan", ""], ["Wang", "Changhu", ""], ["Luo", "Ping", ""]]}, {"id": "2012.05795", "submitter": "Gustau Camps-Valls", "authors": "S. Salcedo-Sanz, P. Ghamisi, M. Piles, M. Werner, L. Cuadra, A.\n  Moreno-Mart\\'inez, E. Izquierdo-Verdiguier, J. Mu\\~noz-Mar\\'i, Amirhosein\n  Mosavi, G. Camps-Valls", "title": "Machine Learning Information Fusion in Earth Observation: A\n  Comprehensive Review of Methods, Applications and Data Sources", "comments": null, "journal-ref": "Information Fusion, Volume 63, November 2020, Pages 256-272", "doi": "10.1016/j.inffus.2020.07.004", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper reviews the most important information fusion data-driven\nalgorithms based on Machine Learning (ML) techniques for problems in Earth\nobservation. Nowadays we observe and model the Earth with a wealth of\nobservations, from a plethora of different sensors, measuring states, fluxes,\nprocesses and variables, at unprecedented spatial and temporal resolutions.\nEarth observation is well equipped with remote sensing systems, mounted on\nsatellites and airborne platforms, but it also involves in-situ observations,\nnumerical models and social media data streams, among other data sources.\nData-driven approaches, and ML techniques in particular, are the natural choice\nto extract significant information from this data deluge. This paper produces a\nthorough review of the latest work on information fusion for Earth observation,\nwith a practical intention, not only focusing on describing the most relevant\nprevious works in the field, but also the most important Earth observation\napplications where ML information fusion has obtained significant results. We\nalso review some of the most currently used data sets, models and sources for\nEarth observation problems, describing their importance and how to obtain the\ndata when needed. Finally, we illustrate the application of ML data fusion with\na representative set of case studies, as well as we discuss and outlook the\nnear future of the field.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 13:35:08 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Salcedo-Sanz", "S.", ""], ["Ghamisi", "P.", ""], ["Piles", "M.", ""], ["Werner", "M.", ""], ["Cuadra", "L.", ""], ["Moreno-Mart\u00ednez", "A.", ""], ["Izquierdo-Verdiguier", "E.", ""], ["Mu\u00f1oz-Mar\u00ed", "J.", ""], ["Mosavi", "Amirhosein", ""], ["Camps-Valls", "G.", ""]]}, {"id": "2012.05796", "submitter": "Andrea Simonelli", "authors": "Andrea Simonelli, Samuel Rota Bul\\`o, Lorenzo Porzi, Peter\n  Kontschieder, Elisa Ricci", "title": "Are we Missing Confidence in Pseudo-LiDAR Methods for Monocular 3D\n  Object Detection?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudo-LiDAR-based methods for monocular 3D object detection have received\nconsiderable attention in the community due to the performance gains exhibited\non the KITTI3D benchmark, in particular on the commonly reported validation\nsplit. This generated a distorted impression about the superiority of\nPseudo-LiDAR-based (PL-based) approaches over methods working with RGB images\nonly. Our first contribution consists in rectifying this view by pointing out\nand showing experimentally that the validation results published by PL-based\nmethods are substantially biased. The source of the bias resides in an overlap\nbetween the KITTI3D object detection validation set and the training/validation\nsets used to train depth predictors feeding PL-based methods. Surprisingly, the\nbias remains also after geographically removing the overlap. This leaves the\ntest set as the only reliable set for comparison, where published PL-based\nmethods do not excel. Our second contribution brings PL-based methods back up\nin the ranking with the design of a novel deep architecture which introduces a\n3D confidence prediction module. We show that 3D confidence estimation\ntechniques derived from RGB-only 3D detection approaches can be successfully\nintegrated into our framework and, more importantly, that improved performance\ncan be obtained with a newly designed 3D confidence measure, leading to\nstate-of-the-art performance on the KITTI3D benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 16:31:41 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 11:46:41 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Simonelli", "Andrea", ""], ["Bul\u00f2", "Samuel Rota", ""], ["Porzi", "Lorenzo", ""], ["Kontschieder", "Peter", ""], ["Ricci", "Elisa", ""]]}, {"id": "2012.05799", "submitter": "Adrian Perez-Suay", "authors": "Jos\\'e A. Padr\\'on Hidalgo, Adri\\'an P\\'erez-Suay, Fatih Nar, and\n  Gustau Camps-Valls", "title": "Efficient Nonlinear RX Anomaly Detectors", "comments": null, "journal-ref": null, "doi": "10.1109/LGRS.2020.2970582", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Current anomaly detection algorithms are typically challenged by either\naccuracy or efficiency. More accurate nonlinear detectors are typically slow\nand not scalable. In this letter, we propose two families of techniques to\nimprove the efficiency of the standard kernel Reed-Xiaoli (RX) method for\nanomaly detection by approximating the kernel function with either {\\em\ndata-independent} random Fourier features or {\\em data-dependent} basis with\nthe Nystr\\\"om approach. We compare all methods for both real multi- and\nhyperspectral images. We show that the proposed efficient methods have a lower\ncomputational cost and they perform similar (or outperform) the standard kernel\nRX algorithm thanks to their implicit regularization effect. Last but not\nleast, the Nystr\\\"om approach has an improved power of detection.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 21:57:54 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Hidalgo", "Jos\u00e9 A. Padr\u00f3n", ""], ["P\u00e9rez-Suay", "Adri\u00e1n", ""], ["Nar", "Fatih", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.05800", "submitter": "Maheshi Dissanayake", "authors": "R.M.L.N. Kumari, and G.A.C.T. Bandara, and Maheshi B. Dissanayake", "title": "Sylvester Matrix Based Similarity Estimation Method for Automation of\n  Defect Detection in Textile Fabrics", "comments": "Journal of Sensors, Hindawi", "journal-ref": "Journal of Sensors,Volume 2021, Article ID 6625421", "doi": "10.1155/2021/6625421", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Fabric defect detection is a crucial quality control step in the textile\nmanufacturing industry. In this article, machine vision system based on the\nSylvester Matrix Based Similarity Method (SMBSM) is proposed to automate the\ndefect detection process. The algorithm involves six phases, namely resolution\nmatching, image enhancement using Histogram Specification and Median-Mean Based\nSub-Image-Clipped Histogram Equalization, image registration through alignment\nand hysteresis process, image subtraction, edge detection, and fault detection\nby means of the rank of the Sylvester matrix. The experimental results\ndemonstrate that the proposed method is robust and yields an accuracy of 93.4%,\nprecision of 95.8%, with 2275 ms computational speed.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 04:20:01 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Kumari", "R. M. L. N.", ""], ["Bandara", "G. A. C. T.", ""], ["Dissanayake", "Maheshi B.", ""]]}, {"id": "2012.05846", "submitter": "Moein Sorkhei", "authors": "Moein Sorkhei, Gustav Eje Henter, Hedvig Kjellstr\\\"om", "title": "Full-Glow: Fully conditional Glow for more realistic image generation", "comments": "17 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous agents, such as driverless cars, require large amounts of labeled\nvisual data for their training. A viable approach for acquiring such data is\ntraining a generative model with collected real data, and then augmenting the\ncollected real dataset with synthetic images from the model, generated with\ncontrol of the scene layout and ground truth labeling. In this paper we propose\nFull-Glow, a fully conditional Glow-based architecture for generating plausible\nand realistic images of novel street scenes given a semantic segmentation map\nindicating the scene layout. Benchmark comparisons show our model to outperform\nrecent works in terms of the semantic segmentation performance of a pretrained\nPSPNet. This indicates that images from our model are, to a higher degree than\nfrom other models, similar to real images of the same kinds of scenes and\nobjects, making them suitable as training data for a visual semantic\nsegmentation or object recognition system.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 17:37:43 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Sorkhei", "Moein", ""], ["Henter", "Gustav Eje", ""], ["Kjellstr\u00f6m", "Hedvig", ""]]}, {"id": "2012.05858", "submitter": "Bingyao Huang", "authors": "Bingyao Huang, Haibin Ling", "title": "SPAA: Stealthy Projector-based Adversarial Attacks on Deep Image\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light-based adversarial attacks aim to fool deep learning-based image\nclassifiers by altering the physical light condition using a controllable light\nsource, e.g., a projector. Compared with physical attacks that place carefully\ndesigned stickers or printed adversarial objects, projector-based ones obviate\nmodifying the physical entities. Moreover, projector-based attacks can be\nperformed transiently and dynamically by altering the projection pattern.\nHowever, existing approaches focus on projecting adversarial patterns that\nresult in clearly perceptible camera-captured perturbations, while the more\ninteresting yet challenging goal, stealthy projector-based attack, remains an\nopen problem. In this paper, for the first time, we formulate this problem as\nan end-to-end differentiable process and propose Stealthy Projector-based\nAdversarial Attack (SPAA). In SPAA, we approximate the real project-and-capture\noperation using a deep neural network named PCNet, then we include PCNet in the\noptimization of projector-based attacks such that the generated adversarial\nprojection is physically plausible. Finally, to generate robust and stealthy\nadversarial projections, we propose an optimization algorithm that uses minimum\nperturbation and adversarial confidence thresholds to alternate between the\nadversarial loss and stealthiness loss optimization. Our experimental\nevaluations show that the proposed SPAA clearly outperforms other methods by\nachieving higher attack success rates and meanwhile being stealthier.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 18:14:03 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Huang", "Bingyao", ""], ["Ling", "Haibin", ""]]}, {"id": "2012.05877", "submitter": "Yen-Chen Lin", "authors": "Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto Rodriguez,\n  Phillip Isola, Tsung-Yi Lin", "title": "iNeRF: Inverting Neural Radiance Fields for Pose Estimation", "comments": "Website: http://yenchenlin.me/inerf/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present iNeRF, a framework that performs mesh-free pose estimation by\n\"inverting\" a Neural RadianceField (NeRF). NeRFs have been shown to be\nremarkably effective for the task of view synthesis - synthesizing\nphotorealistic novel views of real-world scenes or objects. In this work, we\ninvestigate whether we can apply analysis-by-synthesis via NeRF for mesh-free,\nRGB-only 6DoF pose estimation - given an image, find the translation and\nrotation of a camera relative to a 3D object or scene. Our method assumes that\nno object mesh models are available during either training or test time.\nStarting from an initial pose estimate, we use gradient descent to minimize the\nresidual between pixels rendered from a NeRF and pixels in an observed image.\nIn our experiments, we first study 1) how to sample rays during pose refinement\nfor iNeRF to collect informative gradients and 2) how different batch sizes of\nrays affect iNeRF on a synthetic dataset. We then show that for complex\nreal-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating\nthe camera poses of novel images and using these images as additional training\ndata for NeRF. Finally, we show iNeRF can perform category-level object pose\nestimation, including object instances not seen during training, with RGB\nimages by inverting a NeRF model inferred from a single view.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 18:36:40 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 18:51:40 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Yen-Chen", "Lin", ""], ["Florence", "Pete", ""], ["Barron", "Jonathan T.", ""], ["Rodriguez", "Alberto", ""], ["Isola", "Phillip", ""], ["Lin", "Tsung-Yi", ""]]}, {"id": "2012.05894", "submitter": "Xinshuo Weng", "authors": "Xinshuo Weng, Kris Kitani", "title": "AutoSelect: Automatic and Dynamic Detection Selection for 3D\n  Multi-Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D multi-object tracking is an important component in robotic perception\nsystems such as self-driving vehicles. Recent work follows a\ntracking-by-detection pipeline, which aims to match past tracklets with\ndetections in the current frame. To avoid matching with false positive\ndetections, prior work filters out detections with low confidence scores via a\nthreshold. However, finding a proper threshold is non-trivial, which requires\nextensive manual search via ablation study. Also, this threshold is sensitive\nto many factors such as target object category so we need to re-search the\nthreshold if these factors change. To ease this process, we propose to\nautomatically select high-quality detections and remove the efforts needed for\nmanual threshold search. Also, prior work often uses a single threshold per\ndata sequence, which is sub-optimal in particular frames or for certain\nobjects. Instead, we dynamically search threshold per frame or per object to\nfurther boost performance. Through experiments on KITTI and nuScenes, our\nmethod can filter out $45.7\\%$ false positives while maintaining the recall,\nachieving new S.O.T.A. performance and removing the need for manually threshold\ntuning.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 18:55:51 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Weng", "Xinshuo", ""], ["Kitani", "Kris", ""]]}, {"id": "2012.05895", "submitter": "Mengye Ren", "authors": "Mengye Ren, Eleni Triantafillou, Kuan-Chieh Wang, James Lucas, Jake\n  Snell, Xaq Pitkow, Andreas S. Tolias, Richard Zemel", "title": "Flexible Few-Shot Learning with Contextual Similarity", "comments": "Technical report, 29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to few-shot learning deal with tasks that have\npersistent, rigid notions of classes. Typically, the learner observes data only\nfrom a fixed number of classes at training time and is asked to generalize to a\nnew set of classes at test time. Two examples from the same class would always\nbe assigned the same labels in any episode. In this work, we consider a\nrealistic setting where the similarities between examples can change from\nepisode to episode depending on the task context, which is not given to the\nlearner. We define new benchmark datasets for this flexible few-shot scenario,\nwhere the tasks are based on images of faces (Celeb-A), shoes (Zappos50K), and\ngeneral objects (ImageNet-with-Attributes). While classification baselines and\nepisodic approaches learn representations that work well for standard few-shot\nlearning, they suffer in our flexible tasks as novel similarity definitions\narise during testing. We propose to build upon recent contrastive unsupervised\nlearning techniques and use a combination of instance and class invariance\nlearning, aiming to obtain general and flexible features. We find that our\napproach performs strongly on our new flexible few-shot learning benchmarks,\ndemonstrating that unsupervised learning obtains more generalizable\nrepresentations.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 18:58:02 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Ren", "Mengye", ""], ["Triantafillou", "Eleni", ""], ["Wang", "Kuan-Chieh", ""], ["Lucas", "James", ""], ["Snell", "Jake", ""], ["Pitkow", "Xaq", ""], ["Tolias", "Andreas S.", ""], ["Zemel", "Richard", ""]]}, {"id": "2012.05899", "submitter": "Dongdong Chen", "authors": "Suichan Li and Dongdong Chen and Yinpeng Chen and Lu Yuan and Lei\n  Zhang and Qi Chu and Nenghai Yu", "title": "Are Fewer Labels Possible for Few-shot Learning?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning is challenging due to its very limited data and labels.\nRecent studies in big transfer (BiT) show that few-shot learning can greatly\nbenefit from pretraining on large scale labeled dataset in a different domain.\nThis paper asks a more challenging question: \"can we use as few as possible\nlabels for few-shot learning in both pretraining (with no labels) and\nfine-tuning (with fewer labels)?\".\n  Our key insight is that the clustering of target samples in the feature space\nis all we need for few-shot finetuning. It explains why the vanilla\nunsupervised pretraining (poor clustering) is worse than the supervised one. In\nthis paper, we propose transductive unsupervised pretraining that achieves a\nbetter clustering by involving target data even though its amount is very\nlimited. The improved clustering result is of great value for identifying the\nmost representative samples (\"eigen-samples\") for users to label, and in\nreturn, continued finetuning with the labeled eigen-samples further improves\nthe clustering. Thus, we propose eigen-finetuning to enable fewer shot learning\nby leveraging the co-evolution of clustering and eigen-samples in the\nfinetuning. We conduct experiments on 10 different few-shot target datasets,\nand our average few-shot performance outperforms both vanilla inductive\nunsupervised transfer and supervised transfer by a large margin. For instance,\nwhen each target category only has 10 labeled samples, the mean accuracy gain\nover the above two baselines is 9.2% and 3.42 respectively.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 18:59:29 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Li", "Suichan", ""], ["Chen", "Dongdong", ""], ["Chen", "Yinpeng", ""], ["Yuan", "Lu", ""], ["Zhang", "Lei", ""], ["Chu", "Qi", ""], ["Yu", "Nenghai", ""]]}, {"id": "2012.05901", "submitter": "Jia-Bin Huang", "authors": "Johannes Kopf, Xuejian Rong, Jia-Bin Huang", "title": "Robust Consistent Video Depth Estimation", "comments": "Project website: https://robust-cvd.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an algorithm for estimating consistent dense depth maps and camera\nposes from a monocular video. We integrate a learning-based depth prior, in the\nform of a convolutional neural network trained for single-image depth\nestimation, with geometric optimization, to estimate a smooth camera trajectory\nas well as detailed and stable depth reconstruction. Our algorithm combines two\ncomplementary techniques: (1) flexible deformation-splines for low-frequency\nlarge-scale alignment and (2) geometry-aware depth filtering for high-frequency\nalignment of fine depth details. In contrast to prior approaches, our method\ndoes not require camera poses as input and achieves robust reconstruction for\nchallenging hand-held cell phone captures containing a significant amount of\nnoise, shake, motion blur, and rolling shutter deformations. Our method\nquantitatively outperforms state-of-the-arts on the Sintel benchmark for both\ndepth and pose estimations and attains favorable qualitative results across\ndiverse wild datasets.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 18:59:48 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 03:33:03 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Kopf", "Johannes", ""], ["Rong", "Xuejian", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "2012.05903", "submitter": "Chen Gao", "authors": "Chen Gao and Yichang Shih and Wei-Sheng Lai and Chia-Kai Liang and\n  Jia-Bin Huang", "title": "Portrait Neural Radiance Fields from a Single Image", "comments": "Project webpage: https://portrait-nerf.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a method for estimating Neural Radiance Fields (NeRF) from a\nsingle headshot portrait. While NeRF has demonstrated high-quality view\nsynthesis, it requires multiple images of static scenes and thus impractical\nfor casual captures and moving subjects. In this work, we propose to pretrain\nthe weights of a multilayer perceptron (MLP), which implicitly models the\nvolumetric density and colors, with a meta-learning framework using a light\nstage portrait dataset. To improve the generalization to unseen faces, we train\nthe MLP in the canonical coordinate space approximated by 3D face morphable\nmodels. We quantitatively evaluate the method using controlled captures and\ndemonstrate the generalization to real portrait images, showing favorable\nresults against state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 18:59:59 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 20:07:13 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Gao", "Chen", ""], ["Shih", "Yichang", ""], ["Lai", "Wei-Sheng", ""], ["Liang", "Chia-Kai", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "2012.05959", "submitter": "Syeda Nyma Ferdous", "authors": "Syeda Nyma Ferdous, Ali Dabouei, Jeremy Dawson, Nasser M Nasrabadi", "title": "Super-resolution Guided Pore Detection for Fingerprint Recognition", "comments": null, "journal-ref": "ICPR: International Conference on Pattern Recognition 2021", "doi": null, "report-no": "SR:NEW01", "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance of fingerprint recognition algorithms substantially rely on fine\nfeatures extracted from fingerprints. Apart from minutiae and ridge patterns,\npore features have proven to be usable for fingerprint recognition. Although\nfeatures from minutiae and ridge patterns are quite attainable from\nlow-resolution images, using pore features is practical only if the fingerprint\nimage is of high resolution which necessitates a model that enhances the image\nquality of the conventional 500 ppi legacy fingerprints preserving the fine\ndetails. To find a solution for recovering pore information from low-resolution\nfingerprints, we adopt a joint learning-based approach that combines both\nsuper-resolution and pore detection networks. Our modified single image\nSuper-Resolution Generative Adversarial Network (SRGAN) framework helps to\nreliably reconstruct high-resolution fingerprint samples from low-resolution\nones assisting the pore detection network to identify pores with a high\naccuracy. The network jointly learns a distinctive feature representation from\na real low-resolution fingerprint sample and successfully synthesizes a\nhigh-resolution sample from it. To add discriminative information and\nuniqueness for all the subjects, we have integrated features extracted from a\ndeep fingerprint verifier with the SRGAN quality discriminator. We also add\nridge reconstruction loss, utilizing ridge patterns to make the best use of\nextracted features. Our proposed method solves the recognition problem by\nimproving the quality of fingerprint images. High recognition accuracy of the\nsynthesized samples that is close to the accuracy achieved using the original\nhigh-resolution images validate the effectiveness of our proposed model.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 20:30:56 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Ferdous", "Syeda Nyma", ""], ["Dabouei", "Ali", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M", ""]]}, {"id": "2012.05960", "submitter": "Yael Sde-Chen", "authors": "Yael Sde-Chen, Yoav Y. Schechner, Vadim Holodovsky, Eshkol Eytan", "title": "3D Scattering Tomography by Deep Learning with Architecture Tailored to\n  Cloud Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present 3DeepCT, a deep neural network for computed tomography, which\nperforms 3D reconstruction of scattering volumes from multi-view images. Our\narchitecture is dictated by the stationary nature of atmospheric cloud fields.\nThe task of volumetric scattering tomography aims at recovering a volume from\nits 2D projections. This problem has been studied extensively, leading, to\ndiverse inverse methods based on signal processing and physics models. However,\nsuch techniques are typically iterative, exhibiting high computational load and\nlong convergence time. We show that 3DeepCT outperforms physics-based inverse\nscattering methods in term of accuracy as well as offering a significant orders\nof magnitude improvement in computational time. To further improve the recovery\naccuracy, we introduce a hybrid model that combines 3DeepCT and physics-based\nmethod. The resultant hybrid technique enjoys fast inference time and improved\nrecovery performance.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 20:31:44 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Sde-Chen", "Yael", ""], ["Schechner", "Yoav Y.", ""], ["Holodovsky", "Vadim", ""], ["Eytan", "Eshkol", ""]]}, {"id": "2012.05975", "submitter": "Chenyang Lu", "authors": "Chenyang Lu and Gijs Dubbelman", "title": "Image-Graph-Image Translation via Auto-Encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents the first convolutional neural network that learns an\nimage-to-graph translation task without needing external supervision. Obtaining\ngraph representations of image content, where objects are represented as nodes\nand their relationships as edges, is an important task in scene understanding.\nCurrent approaches follow a fully-supervised approach thereby requiring\nmeticulous annotations. To overcome this, we are the first to present a\nself-supervised approach based on a fully-differentiable auto-encoder in which\nthe bottleneck encodes the graph's nodes and edges. This self-supervised\napproach can currently encode simple line drawings into graphs and obtains\ncomparable results to a fully-supervised baseline in terms of F1 score on\ntriplet matching. Besides these promising results, we provide several\ndirections for future research on how our approach can be extended to cover\nmore complex imagery.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 21:01:32 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Lu", "Chenyang", ""], ["Dubbelman", "Gijs", ""]]}, {"id": "2012.05990", "submitter": "Chelsey Edge", "authors": "Chelsey Edge, Md Jahidul Islam, Christopher Morse, Junaed Sattar", "title": "A Generative Approach for Detection-driven Underwater Image Enhancement", "comments": "Under review for ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a generative model for image enhancement\nspecifically for improving diver detection in the underwater domain. In\nparticular, we present a model that integrates generative adversarial network\n(GAN)-based image enhancement with the diver detection task. Our proposed\napproach restructures the GAN objective function to include information from a\npre-trained diver detector with the goal to generate images which would enhance\nthe accuracy of the detector in adverse visual conditions. By incorporating the\ndetector output into both the generator and discriminator networks, our model\nis able to focus on enhancing images beyond aesthetic qualities and\nspecifically to improve robotic detection of scuba divers. We train our network\non a large dataset of scuba divers, using a state-of-the-art diver detector,\nand demonstrate its utility on images collected from oceanic explorations of\nhuman-robot teams. Experimental evaluations demonstrate that our approach\nsignificantly improves diver detection performance over raw, unenhanced images,\nand even outperforms detection performance on the output of state-of-the-art\nunderwater image enhancement algorithms. Finally, we demonstrate the inference\nperformance of our network on embedded devices to highlight the feasibility of\noperating on board mobile robotic platforms.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 21:33:12 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Edge", "Chelsey", ""], ["Islam", "Md Jahidul", ""], ["Morse", "Christopher", ""], ["Sattar", "Junaed", ""]]}, {"id": "2012.06009", "submitter": "Liang Han", "authors": "Liang Han, Zhaozheng Yin, Zhurong Xia, Li Guo, Mingqian Tang, Rong Jin", "title": "Vision-based Price Suggestion for Online Second-hand Items", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from shopping in physical stores, where people have the opportunity\nto closely check a product (e.g., touching the surface of a T-shirt or smelling\nthe scent of perfume) before making a purchase decision, online shoppers rely\ngreatly on the uploaded product images to make any purchase decision. The\ndecision-making is challenging when selling or purchasing second-hand items\nonline since estimating the items' prices is not trivial. In this work, we\npresent a vision-based price suggestion system for the online second-hand item\nshopping platform. The goal of vision-based price suggestion is to help sellers\nset effective prices for their second-hand listings with the images uploaded to\nthe online platforms.\n  First, we propose to better extract representative visual features from the\nimages with the aid of some other image-based item information (e.g., category,\nbrand). Then, we design a vision-based price suggestion module which takes the\nextracted visual features along with some statistical item features from the\nshopping platform as the inputs to determine whether an uploaded item image is\nqualified for price suggestion by a binary classification model, and provide\nprice suggestions for items with qualified images by a regression model.\nAccording to two demands from the platform, two different objective functions\nare proposed to jointly optimize the classification model and the regression\nmodel. For better model training, we also propose a warm-up training strategy\nfor the joint optimization. Extensive experiments on a large real-world dataset\ndemonstrate the effectiveness of our vision-based price prediction system.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 22:56:29 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Han", "Liang", ""], ["Yin", "Zhaozheng", ""], ["Xia", "Zhurong", ""], ["Guo", "Li", ""], ["Tang", "Mingqian", ""], ["Jin", "Rong", ""]]}, {"id": "2012.06018", "submitter": "Vincenzo Liguori", "authors": "Vincenzo Liguori", "title": "A MAC-less Neural Inference Processor Supporting Compressed, Variable\n  Precision Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces two architectures for the inference of convolutional\nneural networks (CNNs). Both architectures exploit weight sparsity and\ncompression to reduce computational complexity and bandwidth. The first\narchitecture uses multiply-accumulators (MACs) but avoids unnecessary\nmultiplications by skipping zero weights. The second architecture exploits\nweight sparsity at the level of their bit representation by substituting\nresource-intensive MACs with much smaller Bit Layer Multiply Accumulators\n(BLMACs). The use of BLMACs also allows variable precision weights as variable\nsize integers and even floating points. Some details of an implementation of\nthe second architecture are given. Weight compression with arithmetic coding is\nalso discussed as well as bandwidth implications. Finally, some implementation\nresults for a pathfinder design and various technologies are presented.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 23:13:17 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Liguori", "Vincenzo", ""]]}, {"id": "2012.06020", "submitter": "Jing Zhang", "authors": "Jing Zhang, Yuchao Dai, Xin Yu, Mehrtash Harandi, Nick Barnes, Richard\n  Hartley", "title": "Uncertainty-Aware Deep Calibrated Salient Object Detection", "comments": "Completed in 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep neural network based salient object detection (SOD) methods\nmainly focus on pursuing high network accuracy. However, those methods overlook\nthe gap between network accuracy and prediction confidence, known as the\nconfidence uncalibration problem. Thus, state-of-the-art SOD networks are prone\nto be overconfident. In other words, the predicted confidence of the networks\ndoes not reflect the real probability of correctness of salient object\ndetection, which significantly hinder their real-world applicability. In this\npaper, we introduce an uncertaintyaware deep SOD network, and propose two\nstrategies from different perspectives to prevent deep SOD networks from being\noverconfident. The first strategy, namely Boundary Distribution Smoothing\n(BDS), generates continuous labels by smoothing the original binary\nground-truth with respect to pixel-wise uncertainty. The second strategy,\nnamely Uncertainty-Aware Temperature Scaling (UATS), exploits a relaxed Sigmoid\nfunction during both training and testing with spatially-variant temperature\nscaling to produce softened output. Both strategies can be incorporated into\nexisting deep SOD networks with minimal efforts. Moreover, we propose a new\nsaliency evaluation metric, namely dense calibration measure C, to measure how\nthe model is calibrated on a given dataset. Extensive experimental results on\nseven benchmark datasets demonstrate that our solutions can not only better\ncalibrate SOD models, but also improve the network accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 23:28:36 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Zhang", "Jing", ""], ["Dai", "Yuchao", ""], ["Yu", "Xin", ""], ["Harandi", "Mehrtash", ""], ["Barnes", "Nick", ""], ["Hartley", "Richard", ""]]}, {"id": "2012.06043", "submitter": "Ang Li", "authors": "Jingwei Sun, Ang Li, Binghui Wang, Huanrui Yang, Hai Li, Yiran Chen", "title": "Provable Defense against Privacy Leakage in Federated Learning from\n  Representation Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a popular distributed learning framework that can\nreduce privacy risks by not explicitly sharing private data. However, recent\nworks demonstrated that sharing model updates makes FL vulnerable to inference\nattacks. In this work, we show our key observation that the data representation\nleakage from gradients is the essential cause of privacy leakage in FL. We also\nprovide an analysis of this observation to explain how the data presentation is\nleaked. Based on this observation, we propose a defense against model inversion\nattack in FL. The key idea of our defense is learning to perturb data\nrepresentation such that the quality of the reconstructed data is severely\ndegraded, while FL performance is maintained. In addition, we derive certified\nrobustness guarantee to FL and convergence guarantee to FedAvg, after applying\nour defense. To evaluate our defense, we conduct experiments on MNIST and\nCIFAR10 for defending against the DLG attack and GS attack. Without sacrificing\naccuracy, the results demonstrate that our proposed defense can increase the\nmean squared error between the reconstructed data and the raw data by as much\nas more than 160X for both DLG attack and GS attack, compared with baseline\ndefense methods. The privacy of the FL system is significantly improved.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 20:42:12 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Sun", "Jingwei", ""], ["Li", "Ang", ""], ["Wang", "Binghui", ""], ["Yang", "Huanrui", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""]]}, {"id": "2012.06044", "submitter": "Kevin Zhou", "authors": "Kevin C. Zhou, Colin Cooke, Jaehee Park, Ruobing Qian, Roarke\n  Horstmeyer, Joseph A. Izatt, Sina Farsiu", "title": "Mesoscopic photogrammetry with an unstabilized phone camera", "comments": null, "journal-ref": "CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a feature-free photogrammetric technique that enables quantitative\n3D mesoscopic (mm-scale height variation) imaging with tens-of-micron accuracy\nfrom sequences of images acquired by a smartphone at close range (several cm)\nunder freehand motion without additional hardware. Our end-to-end,\npixel-intensity-based approach jointly registers and stitches all the images by\nestimating a coaligned height map, which acts as a pixel-wise radial\ndeformation field that orthorectifies each camera image to allow homographic\nregistration. The height maps themselves are reparameterized as the output of\nan untrained encoder-decoder convolutional neural network (CNN) with the raw\ncamera images as the input, which effectively removes many reconstruction\nartifacts. Our method also jointly estimates both the camera's dynamic 6D pose\nand its distortion using a nonparametric model, the latter of which is\nespecially important in mesoscopic applications when using cameras not designed\nfor imaging at short working distances, such as smartphone cameras. We also\npropose strategies for reducing computation time and memory, applicable to\nother multi-frame registration problems. Finally, we demonstrate our method\nusing sequences of multi-megapixel images captured by an unstabilized\nsmartphone on a variety of samples (e.g., painting brushstrokes, circuit board,\nseeds).\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 00:09:18 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Zhou", "Kevin C.", ""], ["Cooke", "Colin", ""], ["Park", "Jaehee", ""], ["Qian", "Ruobing", ""], ["Horstmeyer", "Roarke", ""], ["Izatt", "Joseph A.", ""], ["Farsiu", "Sina", ""]]}, {"id": "2012.06060", "submitter": "Frederic Zhang", "authors": "Frederic Z. Zhang, Dylan Campbell, Stephen Gould", "title": "Spatially Conditioned Graphs for Detecting Human-Object Interactions", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of detecting human-object interactions in images using\ngraphical neural networks. Unlike conventional methods, where nodes send scaled\nbut otherwise identical messages to each of their neighbours, we propose to\ncondition messages between pairs of nodes on their spatial relationships,\nresulting in different messages going to neighbours of the same node. To this\nend, we explore various ways of applying spatial conditioning under a\nmulti-branch structure. Through extensive experimentation we demonstrate the\nadvantages of spatial conditioning for the computation of the adjacency\nstructure, messages and the refined graph features. In particular, we\nempirically show that as the quality of the bounding boxes increases, their\ncoarse appearance features contribute relatively less to the disambiguation of\ninteractions compared to the spatial information. Our method achieves an mAP of\n31.33% on HICO-DET and 54.2% on V-COCO, significantly outperforming\nstate-of-the-art on fine-tuned detections.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 00:55:47 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 05:34:25 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zhang", "Frederic Z.", ""], ["Campbell", "Dylan", ""], ["Gould", "Stephen", ""]]}, {"id": "2012.06087", "submitter": "Yuxiao Zhou", "authors": "Yuxiao Zhou, Marc Habermann, Ikhsanul Habibie, Ayush Tewari, Christian\n  Theobalt, Feng Xu", "title": "Monocular Real-time Full Body Capture with Inter-part Correlations", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the first method for real-time full body capture that estimates\nshape and motion of body and hands together with a dynamic 3D face model from a\nsingle color image. Our approach uses a new neural network architecture that\nexploits correlations between body and hands at high computational efficiency.\nUnlike previous works, our approach is jointly trained on multiple datasets\nfocusing on hand, body or face separately, without requiring data where all the\nparts are annotated at the same time, which is much more difficult to create at\nsufficient variety. The possibility of such multi-dataset training enables\nsuperior generalization ability. In contrast to earlier monocular full body\nmethods, our approach captures more expressive 3D face geometry and color by\nestimating the shape, expression, albedo and illumination parameters of a\nstatistical face model. Our method achieves competitive accuracy on public\nbenchmarks, while being significantly faster and providing more complete face\nreconstructions.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 02:37:56 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 06:18:53 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Zhou", "Yuxiao", ""], ["Habermann", "Marc", ""], ["Habibie", "Ikhsanul", ""], ["Tewari", "Ayush", ""], ["Theobalt", "Christian", ""], ["Xu", "Feng", ""]]}, {"id": "2012.06109", "submitter": "Zhongguo Li", "authors": "Zhongguo Li and Anders Heyden and Magnus Oskarsson", "title": "A novel joint points and silhouette-based method to estimate 3D human\n  pose and shape", "comments": "Accepted to ICPR 2020 3DHU workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a novel method for 3D human pose and shape estimation\nfrom images with sparse views, using joint points and silhouettes, based on a\nparametric model. Firstly, the parametric model is fitted to the joint points\nestimated by deep learning-based human pose estimation. Then, we extract the\ncorrespondence between the parametric model of pose fitting and silhouettes on\n2D and 3D space. A novel energy function based on the correspondence is built\nand minimized to fit parametric model to the silhouettes. Our approach uses\nsufficient shape information because the energy function of silhouettes is\nbuilt from both 2D and 3D space. This also means that our method only needs\nimages from sparse views, which balances data used and the required prior\ninformation. Results on synthetic data and real data demonstrate the\ncompetitive performance of our approach on pose and shape estimation of the\nhuman body.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 03:58:16 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Li", "Zhongguo", ""], ["Heyden", "Anders", ""], ["Oskarsson", "Magnus", ""]]}, {"id": "2012.06117", "submitter": "Erik Wijmans", "authors": "Erik Wijmans and Irfan Essa and Dhruv Batra", "title": "How to Train PointGoal Navigation Agents on a (Sample and Compute)\n  Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PointGoal navigation has seen significant recent interest and progress,\nspurred on by the Habitat platform and associated challenge. In this paper, we\nstudy PointGoal navigation under both a sample budget (75 million frames) and a\ncompute budget (1 GPU for 1 day). We conduct an extensive set of experiments,\ncumulatively totaling over 50,000 GPU-hours, that let us identify and discuss a\nnumber of ostensibly minor but significant design choices -- the advantage\nestimation procedure (a key component in training), visual encoder\narchitecture, and a seemingly minor hyper-parameter change. Overall, these\ndesign choices to lead considerable and consistent improvements over the\nbaselines present in Savva et al. Under a sample budget, performance for RGB-D\nagents improves 8 SPL on Gibson (14% relative improvement) and 20 SPL on\nMatterport3D (38% relative improvement). Under a compute budget, performance\nfor RGB-D agents improves by 19 SPL on Gibson (32% relative improvement) and 35\nSPL on Matterport3D (220% relative improvement). We hope our findings and\nrecommendations will make serve to make the community's experiments more\nefficient.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 04:28:48 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Wijmans", "Erik", ""], ["Essa", "Irfan", ""], ["Batra", "Dhruv", ""]]}, {"id": "2012.06122", "submitter": "Ramtin Hosseini", "authors": "Ramtin Hosseini, Xingyi Yang and Pengtao Xie", "title": "DSRNA: Differentiable Search of Robust Neural Architectures", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In deep learning applications, the architectures of deep neural networks are\ncrucial in achieving high accuracy. Many methods have been proposed to search\nfor high-performance neural architectures automatically. However, these\nsearched architectures are prone to adversarial attacks. A small perturbation\nof the input data can render the architecture to change prediction outcomes\nsignificantly. To address this problem, we propose methods to perform\ndifferentiable search of robust neural architectures. In our methods, two\ndifferentiable metrics are defined to measure architectures' robustness, based\non certified lower bound and Jacobian norm bound. Then we search for robust\narchitectures by maximizing the robustness metrics. Different from previous\napproaches which aim to improve architectures' robustness in an implicit way:\nperforming adversarial training and injecting random noise, our methods\nexplicitly and directly maximize robustness metrics to harvest robust\narchitectures. On CIFAR-10, ImageNet, and MNIST, we perform game-based\nevaluation and verification-based evaluation on the robustness of our methods.\nThe experimental results show that our methods 1) are more robust to various\nnorm-bound attacks than several robust NAS baselines; 2) are more accurate than\nbaselines when there are no attacks; 3) have significantly higher certified\nlower bounds than baselines.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 04:52:54 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Hosseini", "Ramtin", ""], ["Yang", "Xingyi", ""], ["Xie", "Pengtao", ""]]}, {"id": "2012.06123", "submitter": "Basura Fernando", "authors": "Haziq Razali and Basura Fernando", "title": "A Log-likelihood Regularized KL Divergence for Video Prediction with A\n  3D Convolutional Variational Recurrent Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of latent variable models has shown to be a powerful tool for\nmodeling probability distributions over sequences. In this paper, we introduce\na new variational model that extends the recurrent network in two ways for the\ntask of video frame prediction. First, we introduce 3D convolutions inside all\nmodules including the recurrent model for future frame prediction, inputting\nand outputting a sequence of video frames at each timestep. This enables us to\nbetter exploit spatiotemporal information inside the variational recurrent\nmodel, allowing us to generate high-quality predictions. Second, we enhance the\nlatent loss of the variational model by introducing a maximum likelihood\nestimate in addition to the KL divergence that is commonly used in variational\nmodels. This simple extension acts as a stronger regularizer in the variational\nautoencoder loss function and lets us obtain better results and\ngeneralizability. Experiments show that our model outperforms existing video\nprediction methods on several benchmarks while requiring fewer parameters.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 05:05:31 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Razali", "Haziq", ""], ["Fernando", "Basura", ""]]}, {"id": "2012.06125", "submitter": "Zhihao Xia", "authors": "Zhihao Xia, Jason Lawrence, Supreeth Achar", "title": "A Dark Flash Normal Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Casual photography is often performed in uncontrolled lighting that can\nresult in low quality images and degrade the performance of downstream\nprocessing. We consider the problem of estimating surface normal and\nreflectance maps of scenes depicting people despite these conditions by\nsupplementing the available visible illumination with a single near infrared\n(NIR) light source and camera, a so-called \"dark flash image\". Our method takes\nas input a single color image captured under arbitrary visible lighting and a\nsingle dark flash image captured under controlled front-lit NIR lighting at the\nsame viewpoint, and computes a normal map, a diffuse albedo map, and a specular\nintensity map of the scene. Since ground truth normal and reflectance maps of\nfaces are difficult to capture, we propose a novel training technique that\ncombines information from two readily available and complementary sources: a\nstereo depth signal and photometric shading cues. We evaluate our method over a\nrange of subjects and lighting conditions and describe two applications:\noptimizing stereo geometry and filling the shadows in an image.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 05:08:22 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Xia", "Zhihao", ""], ["Lawrence", "Jason", ""], ["Achar", "Supreeth", ""]]}, {"id": "2012.06131", "submitter": "Xin Li", "authors": "Xin Li, Xin Jin, Tao Yu, Yingxue Pang, Simeng Sun, Zhizheng Zhang,\n  Zhibo Chen", "title": "Learning Omni-frequency Region-adaptive Representations for Real Image\n  Super-Resolution", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional single image super-resolution (SISR) methods that focus on\nsolving single and uniform degradation (i.e., bicubic down-sampling), typically\nsuffer from poor performance when applied into real-world low-resolution (LR)\nimages due to the complicated realistic degradations. The key to solving this\nmore challenging real image super-resolution (RealSR) problem lies in learning\nfeature representations that are both informative and content-aware. In this\npaper, we propose an Omni-frequency Region-adaptive Network (ORNet) to address\nboth challenges, here we call features of all low, middle and high frequencies\nomni-frequency features. Specifically, we start from the frequency perspective\nand design a Frequency Decomposition (FD) module to separate different\nfrequency components to comprehensively compensate the information lost for\nreal LR image. Then, considering the different regions of real LR image have\ndifferent frequency information lost, we further design a Region-adaptive\nFrequency Aggregation (RFA) module by leveraging dynamic convolution and\nspatial attention to adaptively restore frequency components for different\nregions. The extensive experiments endorse the effective, and scenario-agnostic\nnature of our OR-Net for RealSR.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 05:17:38 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 06:12:15 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Li", "Xin", ""], ["Jin", "Xin", ""], ["Yu", "Tao", ""], ["Pang", "Yingxue", ""], ["Sun", "Simeng", ""], ["Zhang", "Zhizheng", ""], ["Chen", "Zhibo", ""]]}, {"id": "2012.06132", "submitter": "Bin Xiao", "authors": "Bin Xiao, Tao Geng, Xiuli Bi, Weisheng Li", "title": "Color-related Local Binary Pattern: A Learned Local Descriptor for Color\n  Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local binary pattern (LBP) as a kind of local feature has shown its\nsimplicity, easy implementation and strong discriminating power in image\nrecognition. Although some LBP variants are specifically investigated for color\nimage recognition, the color information of images is not adequately considered\nand the curse of dimensionality in classification is easily caused in these\nmethods. In this paper, a color-related local binary pattern (cLBP) which\nlearns the dominant patterns from the decoded LBP is proposed for color images\nrecognition. This paper first proposes a relative similarity space (RSS) that\nrepresents the color similarity between image channels for describing a color\nimage. Then, the decoded LBP which can mine the correlation information between\nthe LBP feature maps correspond to each color channel of RSS traditional RGB\nspaces, is employed for feature extraction. Finally, a feature learning\nstrategy is employed to learn the dominant color-related patterns for reducing\nthe dimension of feature vector and further improving the discriminatively of\nfeatures. The theoretic analysis show that the proposed RSS can provide more\ndiscriminative information, and has higher noise robustness as well as higher\nillumination variation robustness than traditional RGB space. Experimental\nresults on four groups, totally twelve public color image datasets show that\nthe proposed method outperforms most of the LBP variants for color image\nrecognition in terms of dimension of features, recognition accuracy under\nnoise-free, noisy and illumination variation conditions.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 05:18:25 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Xiao", "Bin", ""], ["Geng", "Tao", ""], ["Bi", "Xiuli", ""], ["Li", "Weisheng", ""]]}, {"id": "2012.06134", "submitter": "Lingbo Yang", "authors": "Lingbo Yang, Zhanning Gao, Peiran Ren, Siwei Ma, Wen Gao", "title": "Intrinsic Temporal Regularization for High-resolution Human Video\n  Synthesis", "comments": "10 pages, work done during internship at Alibaba DAMO Academy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Temporal consistency is crucial for extending image processing pipelines to\nthe video domain, which is often enforced with flow-based warping error over\nadjacent frames. Yet for human video synthesis, such scheme is less reliable\ndue to the misalignment between source and target video as well as the\ndifficulty in accurate flow estimation. In this paper, we propose an effective\nintrinsic temporal regularization scheme to mitigate these issues, where an\nintrinsic confidence map is estimated via the frame generator to regulate\nmotion estimation via temporal loss modulation. This creates a shortcut for\nback-propagating temporal loss gradients directly to the front-end motion\nestimator, thus improving training stability and temporal coherence in output\nvideos. We apply our intrinsic temporal regulation to single-image generator,\nleading to a powerful \"INTERnet\" capable of generating $512\\times512$\nresolution human action videos with temporal-coherent, realistic visual\ndetails. Extensive experiments demonstrate the superiority of proposed INTERnet\nover several competitive baselines.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 05:29:45 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Yang", "Lingbo", ""], ["Gao", "Zhanning", ""], ["Ren", "Peiran", ""], ["Ma", "Siwei", ""], ["Gao", "Wen", ""]]}, {"id": "2012.06136", "submitter": "Beibin Li", "authors": "Beibin Li, Ezgi Mercan, Sachin Mehta, Stevan Knezevich, Corey W.\n  Arnold, Donald L. Weaver, Joann G. Elmore, Linda G. Shapiro", "title": "Classifying Breast Histopathology Images with a Ductal Instance-Oriented\n  Pipeline", "comments": "ICPR 2020. Submitted July 15th, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose the Ductal Instance-Oriented Pipeline (DIOP) that\ncontains a duct-level instance segmentation model, a tissue-level semantic\nsegmentation model, and three-levels of features for diagnostic classification.\nBased on recent advancements in instance segmentation and the Mask R-CNN model,\nour duct-level segmenter tries to identify each ductal individual inside a\nmicroscopic image; then, it extracts tissue-level information from the\nidentified ductal instances. Leveraging three levels of information obtained\nfrom these ductal instances and also the histopathology image, the proposed\nDIOP outperforms previous approaches (both feature-based and CNN-based) in all\ndiagnostic tasks; for the four-way classification task, the DIOP achieves\ncomparable performance to general pathologists in this unique dataset. The\nproposed DIOP only takes a few seconds to run in the inference time, which\ncould be used interactively on most modern computers. More clinical\nexplorations are needed to study the robustness and generalizability of this\nsystem in the future.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 05:43:12 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Li", "Beibin", ""], ["Mercan", "Ezgi", ""], ["Mehta", "Sachin", ""], ["Knezevich", "Stevan", ""], ["Arnold", "Corey W.", ""], ["Weaver", "Donald L.", ""], ["Elmore", "Joann G.", ""], ["Shapiro", "Linda G.", ""]]}, {"id": "2012.06149", "submitter": "Runmin Cong", "authors": "Hua Li, Yuheng Jia, Runmin Cong, Wenhui Wu, Sam Kwong, and Chuanbo\n  Chen", "title": "Superpixel Segmentation Based on Spatially Constrained Subspace\n  Clustering", "comments": "Accepted by IEEE Transactions on Industrial Informatics, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Superpixel segmentation aims at dividing the input image into some\nrepresentative regions containing pixels with similar and consistent intrinsic\nproperties, without any prior knowledge about the shape and size of each\nsuperpixel. In this paper, to alleviate the limitation of superpixel\nsegmentation applied in practical industrial tasks that detailed boundaries are\ndifficult to be kept, we regard each representative region with independent\nsemantic information as a subspace, and correspondingly formulate superpixel\nsegmentation as a subspace clustering problem to preserve more detailed content\nboundaries. We show that a simple integration of superpixel segmentation with\nthe conventional subspace clustering does not effectively work due to the\nspatial correlation of the pixels within a superpixel, which may lead to\nboundary confusion and segmentation error when the correlation is ignored.\nConsequently, we devise a spatial regularization and propose a novel convex\nlocality-constrained subspace clustering model that is able to constrain the\nspatial adjacent pixels with similar attributes to be clustered into a\nsuperpixel and generate the content-aware superpixels with more detailed\nboundaries. Finally, the proposed model is solved by an efficient alternating\ndirection method of multipliers (ADMM) solver. Experiments on different\nstandard datasets demonstrate that the proposed method achieves superior\nperformance both quantitatively and qualitatively compared with some\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 06:18:36 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Li", "Hua", ""], ["Jia", "Yuheng", ""], ["Cong", "Runmin", ""], ["Wu", "Wenhui", ""], ["Kwong", "Sam", ""], ["Chen", "Chuanbo", ""]]}, {"id": "2012.06166", "submitter": "Malik Boudiaf", "authors": "Malik Boudiaf, Hoel Kervadec, Ziko Imtiaz Masud, Pablo Piantanida,\n  Ismail Ben Ayed, Jose Dolz", "title": "Few-Shot Segmentation Without Meta-Learning: A Good Transductive\n  Inference Is All You Need?", "comments": "CVPR 2021. Code available at\n  https://github.com/mboudiaf/RePRI-for-Few-Shot-Segmentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We show that the way inference is performed in few-shot segmentation tasks\nhas a substantial effect on performances -- an aspect often overlooked in the\nliterature in favor of the meta-learning paradigm. We introduce a transductive\ninference for a given query image, leveraging the statistics of its unlabeled\npixels, by optimizing a new loss containing three complementary terms: i) the\ncross-entropy on the labeled support pixels; ii) the Shannon entropy of the\nposteriors on the unlabeled query-image pixels; and iii) a global KL-divergence\nregularizer based on the proportion of the predicted foreground. As our\ninference uses a simple linear classifier of the extracted features, its\ncomputational load is comparable to inductive inference and can be used on top\nof any base training. Foregoing episodic training and using only standard\ncross-entropy training on the base classes, our inference yields competitive\nperformances on standard benchmarks in the 1-shot scenarios. As the number of\navailable shots increases, the gap in performances widens: on PASCAL-5i, our\nmethod brings about 5% and 6% improvements over the state-of-the-art, in the 5-\nand 10-shot scenarios, respectively. Furthermore, we introduce a new setting\nthat includes domain shifts, where the base and novel classes are drawn from\ndifferent datasets. Our method achieves the best performances in this more\nrealistic setting. Our code is freely available online:\nhttps://github.com/mboudiaf/RePRI-for-Few-Shot-Segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 07:11:19 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 21:03:58 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Boudiaf", "Malik", ""], ["Kervadec", "Hoel", ""], ["Masud", "Ziko Imtiaz", ""], ["Piantanida", "Pablo", ""], ["Ayed", "Ismail Ben", ""], ["Dolz", "Jose", ""]]}, {"id": "2012.06170", "submitter": "Samyak Jain", "authors": "Samyak Jain, Pradeep Yarlagadda, Shreyank Jyoti, Shyamgopal Karthik,\n  Ramanathan Subramanian and Vineet Gandhi", "title": "ViNet: Pushing the limits of Visual Modality for Audio-Visual Saliency\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the ViNet architecture for audio-visual saliency prediction. ViNet\nis a fully convolutional encoder-decoder architecture. The encoder uses visual\nfeatures from a network trained for action recognition, and the decoder infers\na saliency map via trilinear interpolation and 3D convolutions, combining\nfeatures from multiple hierarchies. The overall architecture of ViNet is\nconceptually simple; it is causal and runs in real-time (60 fps). ViNet does\nnot use audio as input and still outperforms the state-of-the-art audio-visual\nsaliency prediction models on nine different datasets (three visual-only and\nsix audio-visual datasets). ViNet also surpasses human performance on the CC,\nSIM and AUC metrics for the AVE dataset, and to our knowledge, it is the first\nnetwork to do so. We also explore a variation of ViNet architecture by\naugmenting audio features into the decoder. To our surprise, upon sufficient\ntraining, the network becomes agnostic to the input audio and provides the same\noutput irrespective of the input. Interestingly, we also observe similar\nbehaviour in the previous state-of-the-art models \\cite{tsiami2020stavis} for\naudio-visual saliency prediction. Our findings contrast with previous works on\ndeep learning-based audio-visual saliency prediction, suggesting a clear avenue\nfor future explorations incorporating audio in a more effective manner. The\ncode and pre-trained models are available at\nhttps://github.com/samyak0210/ViNet.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 07:28:02 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 06:00:20 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Jain", "Samyak", ""], ["Yarlagadda", "Pradeep", ""], ["Jyoti", "Shreyank", ""], ["Karthik", "Shyamgopal", ""], ["Subramanian", "Ramanathan", ""], ["Gandhi", "Vineet", ""]]}, {"id": "2012.06178", "submitter": "Zhongguo Li", "authors": "Zhongguo Li, Magnus Oskarsson, Anders Heyden", "title": "Detailed 3D Human Body Reconstruction from Multi-view Images Combining\n  Voxel Super-Resolution and Learned Implicit Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of reconstructing detailed 3D human body models from images is\ninteresting but challenging in computer vision due to the high freedom of human\nbodies. In order to tackle the problem, we propose a coarse-to-fine method to\nreconstruct a detailed 3D human body from multi-view images combining voxel\nsuper-resolution based on learning the implicit representation. Firstly, the\ncoarse 3D models are estimated by learning an implicit representation based on\nmulti-scale features which are extracted by multi-stage hourglass networks from\nthe multi-view images. Then, taking the low resolution voxel grids which are\ngenerated by the coarse 3D models as input, the voxel super-resolution based on\nan implicit representation is learned through a multi-stage 3D convolutional\nneural network. Finally, the refined detailed 3D human body models can be\nproduced by the voxel super-resolution which can preserve the details and\nreduce the false reconstruction of the coarse 3D models. Benefiting from the\nimplicit representation, the training process in our method is memory efficient\nand the detailed 3D human body produced by our method from multi-view images is\nthe continuous decision boundary with high-resolution geometry. In addition,\nthe coarse-to-fine method based on voxel super-resolution can remove false\nreconstructions and preserve the appearance details in the final\nreconstruction, simultaneously. In the experiments, our method quantitatively\nand qualitatively achieves the competitive 3D human body reconstructions from\nimages with various poses and shapes on both the real and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 08:07:39 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Li", "Zhongguo", ""], ["Oskarsson", "Magnus", ""], ["Heyden", "Anders", ""]]}, {"id": "2012.06181", "submitter": "Junze Liu", "authors": "Junze Liu, Jordan Ott, Julian Collado, Benjamin Jargowsky, Wenjie Wu,\n  Jianming Bian, Pierre Baldi", "title": "Deep-Learning-Based Kinematic Reconstruction for DUNE", "comments": "NuerIPS workshop proceeding", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ins-det cs.CV hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of three-active-neutrino mixing, the charge parity phase,\nthe neutrino mass ordering, and the octant of $\\theta_{23}$ remain unknown. The\nDeep Underground Neutrino Experiment (DUNE) is a next-generation long-baseline\nneutrino oscillation experiment, which aims to address these questions by\nmeasuring the oscillation patterns of $\\nu_\\mu/\\nu_e$ and\n$\\bar\\nu_\\mu/\\bar\\nu_e$ over a range of energies spanning the first and second\noscillation maxima. DUNE far detector modules are based on liquid argon TPC\n(LArTPC) technology. A LArTPC offers excellent spatial resolution, high\nneutrino detection efficiency, and superb background rejection, while\nreconstruction in LArTPC is challenging. Deep learning methods, in particular,\nConvolutional Neural Networks (CNNs), have demonstrated success in\nclassification problems such as particle identification in DUNE and other\nneutrino experiments. However, reconstruction of neutrino energy and final\nstate particle momenta with deep learning methods is yet to be developed for a\nfull AI-based reconstruction chain. To precisely reconstruct these kinematic\ncharacteristics of detected interactions at DUNE, we have developed and will\npresent two CNN-based methods, 2-D and 3-D, for the reconstruction of final\nstate particle direction and energy, as well as neutrino energy. Combining\nparticle masses with the kinetic energy and the direction reconstructed by our\nwork, the four-momentum of final state particles can be obtained. Our models\nshow considerable improvements compared to the traditional methods for both\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 08:15:23 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 03:44:20 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Liu", "Junze", ""], ["Ott", "Jordan", ""], ["Collado", "Julian", ""], ["Jargowsky", "Benjamin", ""], ["Wu", "Wenjie", ""], ["Bian", "Jianming", ""], ["Baldi", "Pierre", ""]]}, {"id": "2012.06186", "submitter": "Shervin Rasoulzadeh", "authors": "Shervin Rasoulzadeh, Bagher Babaali", "title": "Writer Identification and Writer Retrieval Based on NetVLAD with\n  Re-ranking", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": "10.1049/bme2.12039", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses writer identification and writer retrieval which is\nconsidered as a challenging problem in the document analysis and recognition\nfield. In this work, a novel pipeline is proposed for the problem at hand by\nemploying a unified neural network architecture consisting of the ResNet-20 as\na feature extractor and an integrated NetVLAD layer, inspired by the vector of\nlocally aggregated descriptors (VLAD), in the head of the latter part. Having\ndefined this architecture, the triplet semi-hard loss function is used to\ndirectly learn an embedding for individual input image patches. Subsequently,\ngeneralized max-pooling technique is employed for the aggregation of embedded\ndescriptors of each handwritten image. Also, a novel re-ranking strategy is\nintroduced for the task of identification and retrieval based on $k$-reciprocal\nnearest neighbors, and it is shown that the pipeline can benefit tremendously\nfrom this step. Experimental evaluation has been done on the three publicly\navailable datasets: the ICDAR 2013, CVL, and KHATT datasets. Results indicate\nthat while we perform comparably to the state-of-the-art on the KHATT, our\nwriter identification and writer retrieval pipeline achieves superior\nperformance on the ICDAR 2013 and CVL datasets in terms of mAP.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 08:22:28 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 17:45:14 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 18:27:50 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Rasoulzadeh", "Shervin", ""], ["Babaali", "Bagher", ""]]}, {"id": "2012.06194", "submitter": "Lang Nie", "authors": "Lang Nie, Chunyu Lin, Kang Liao, Yao Zhao", "title": "Learning Edge-Preserved Image Stitching from Large-Baseline Deep\n  Homography", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image stitching is a classical and crucial technique in computer vision,\nwhich aims to generate the image with a wide field of view. The traditional\nmethods heavily depend on the feature detection and require that scene features\nbe dense and evenly distributed in the image, leading to varying ghosting\neffects and poor robustness. Learning methods usually suffer from fixed view\nand input size limitations, showing a lack of generalization ability on other\nreal datasets. In this paper, we propose an image stitching learning framework,\nwhich consists of a large-baseline deep homography module and an edge-preserved\ndeformation module. First, we propose a large-baseline deep homography module\nto estimate the accurate projective transformation between the reference image\nand the target image in different scales of features. After that, an\nedge-preserved deformation module is designed to learn the deformation rules of\nimage stitching from edge to content, eliminating the ghosting effects as much\nas possible. In particular, the proposed learning framework can stitch images\nof arbitrary views and input sizes, thus contribute to a supervised deep image\nstitching method with excellent generalization capability in other real images.\nExperimental results demonstrate that our homography module significantly\noutperforms the existing deep homography methods in the large baseline scenes.\nIn image stitching, our method is superior to the existing learning method and\nshows competitive performance with state-of-the-art traditional methods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 08:43:30 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Nie", "Lang", ""], ["Lin", "Chunyu", ""], ["Liao", "Kang", ""], ["Zhao", "Yao", ""]]}, {"id": "2012.06200", "submitter": "Federico Becattini", "authors": "Lavinia De Divitiis, Federico Becattini, Claudio Baecchi, Alberto Del\n  Bimbo", "title": "Garment Recommendation with Memory Augmented Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fashion plays a pivotal role in society. Combining garments appropriately is\nessential for people to communicate their personality and style. Also different\nevents require outfits to be thoroughly chosen to comply with underlying social\nclothing rules. Therefore, combining garments appropriately might not be\ntrivial. The fashion industry has turned this into a massive source of income,\nrelying on complex recommendation systems to retrieve and suggest appropriate\nclothing items for customers. To perform better recommendations, personalized\nsuggestions can be performed, taking into account user preferences or purchase\nhistories. In this paper, we propose a garment recommendation system to pair\ndifferent clothing items, namely tops and bottoms, exploiting a Memory\nAugmented Neural Network (MANN). By training a memory writing controller, we\nare able to store a non-redundant subset of samples, which is then used to\nretrieve a ranked list of suitable bottoms to complement a given top. In\nparticular, we aim at retrieving a variety of modalities in which a certain\ngarment can be combined. To refine our recommendations, we then include user\npreferences via Matrix Factorization. We experiment on IQON3000, a dataset\ncollected from an online fashion community, reporting state of the art results.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 09:13:14 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["De Divitiis", "Lavinia", ""], ["Becattini", "Federico", ""], ["Baecchi", "Claudio", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "2012.06257", "submitter": "Liqiang Lin", "authors": "Liqiang Lin, Pengdi Huang, Chi-Wing Fu, Kai Xu, Hao Zhang, Hui Huang", "title": "One Point is All You Need: Directional Attention Point for Feature\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a novel attention-based mechanism for learning enhanced point\nfeatures for tasks such as point cloud classification and segmentation. Our key\nmessage is that if the right attention point is selected, then \"one point is\nall you need\" -- not a sequence as in a recurrent model and not a pre-selected\nset as in all prior works. Also, where the attention point is should be\nlearned, from data and specific to the task at hand. Our mechanism is\ncharacterized by a new and simple convolution, which combines the feature at an\ninput point with the feature at its associated attention point. We call such a\npoint a directional attention point (DAP), since it is found by adding to the\noriginal point an offset vector that is learned by maximizing the task\nperformance in training. We show that our attention mechanism can be easily\nincorporated into state-of-the-art point cloud classification and segmentation\nnetworks. Extensive experiments on common benchmarks such as ModelNet40,\nShapeNetPart, and S3DIS demonstrate that our DAP-enabled networks consistently\noutperform the respective original networks, as well as all other competitive\nalternatives, including those employing pre-selected sets of attention points.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 11:45:39 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 06:47:12 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Lin", "Liqiang", ""], ["Huang", "Pengdi", ""], ["Fu", "Chi-Wing", ""], ["Xu", "Kai", ""], ["Zhang", "Hao", ""], ["Huang", "Hui", ""]]}, {"id": "2012.06277", "submitter": "Guru Swaroop Bennabhaktula", "authors": "Derrick Timmerman, Swaroop Bennabhaktula, Enrique Alegre and George\n  Azzopardi", "title": "Video Camera Identification from Sensor Pattern Noise with a Constrained\n  ConvNet", "comments": "Paper Accepted in - 10th International Conference on Pattern\n  Recognition Applications and Methods (ICPRAM 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The identification of source cameras from videos, though it is a highly\nrelevant forensic analysis topic, has been studied much less than its\ncounterpart that uses images. In this work we propose a method to identify the\nsource camera of a video based on camera specific noise patterns that we\nextract from video frames. For the extraction of noise pattern features, we\npropose an extended version of a constrained convolutional layer capable of\nprocessing color inputs. Our system is designed to classify individual video\nframes which are in turn combined by a majority vote to identify the source\ncamera. We evaluated this approach on the benchmark VISION data set consisting\nof 1539 videos from 28 different cameras. To the best of our knowledge, this is\nthe first work that addresses the challenge of video camera identification on a\ndevice level. The experiments show that our approach is very promising,\nachieving up to 93.1% accuracy while being robust to the WhatsApp and YouTube\ncompression techniques. This work is part of the EU-funded project 4NSEEK\nfocused on forensics against child sexual abuse.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 12:17:30 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Timmerman", "Derrick", ""], ["Bennabhaktula", "Swaroop", ""], ["Alegre", "Enrique", ""], ["Azzopardi", "George", ""]]}, {"id": "2012.06308", "submitter": "Weidi Wang", "authors": "Weidi Wang, Zeyuan Wang, Yinghui Zhang, Bo Sun, and Ke Xia", "title": "Learning Order Parameters from Videos of Dynamical Phases for Skyrmions\n  with Neural Networks", "comments": "15 pages, 15 figures", "journal-ref": "Phys. Rev. Applied 16, 014005 (2021)", "doi": "10.1103/PhysRevApplied.16.014005", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to recognize dynamical phenomena (e.g., dynamical phases) and\ndynamical processes in physical events from videos, then to abstract physical\nconcepts and reveal physical laws, lies at the core of human intelligence. The\nmain purposes of this paper are to use neural networks for classifying the\ndynamical phases of some videos and to demonstrate that neural networks can\nlearn physical concepts from them. To this end, we employ multiple neural\nnetworks to recognize the static phases (image format) and dynamical phases\n(video format) of a particle-based skyrmion model. Our results show that neural\nnetworks, without any prior knowledge, can not only correctly classify these\nphases, but also predict the phase boundaries which agree with those obtained\nby simulation. We further propose a parameter visualization scheme to interpret\nwhat neural networks have learned. We show that neural networks can learn two\norder parameters from videos of dynamical phases and predict the critical\nvalues of two order parameters. Finally, we demonstrate that only two order\nparameters are needed to identify videos of skyrmion dynamical phases. It shows\nthat this parameter visualization scheme can be used to determine how many\norder parameters are needed to fully recognize the input phases. Our work sheds\nlight on the future use of neural networks in discovering new physical concepts\nand revealing unknown yet physical laws from videos.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 08:17:32 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Wang", "Weidi", ""], ["Wang", "Zeyuan", ""], ["Zhang", "Yinghui", ""], ["Sun", "Bo", ""], ["Xia", "Ke", ""]]}, {"id": "2012.06310", "submitter": "Shabir Parah Dr", "authors": "Parsa Sarosh, Shabir A. Parah, Romany F Mansur, G. M. Bhat", "title": "Artificial Intelligence for COVID-19 Detection -- A state-of-the-art\n  review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The emergence of COVID-19 has necessitated many efforts by the scientific\ncommunity for its proper management. An urgent clinical reaction is required in\nthe face of the unending devastation being caused by the pandemic. These\nefforts include technological innovations for improvement in screening,\ntreatment, vaccine development, contact tracing and, survival prediction. The\nuse of Deep Learning (DL) and Artificial Intelligence (AI) can be sought in all\nof the above-mentioned spheres. This paper aims to review the role of Deep\nLearning and Artificial intelligence in various aspects of the overall COVID-19\nmanagement and particularly for COVID-19 detection and classification. The DL\nmodels are developed to analyze clinical modalities like CT scans and X-Ray\nimages of patients and predict their pathological condition. A DL model aims to\ndetect the COVID-19 pneumonia, classify and distinguish between COVID-19,\nCommunity-Acquired Pneumonia (CAP), Viral and Bacterial pneumonia, and normal\nconditions. Furthermore, sophisticated models can be built to segment the\naffected area in the lungs and quantify the infection volume for a better\nunderstanding of the extent of damage. Many models have been developed either\nindependently or with the help of pre-trained models like VGG19, ResNet50, and\nAlexNet leveraging the concept of transfer learning. Apart from model\ndevelopment, data preprocessing and augmentation are also performed to cope\nwith the challenge of insufficient data samples often encountered in medical\napplications. It can be evaluated that DL and AI can be effectively implemented\nto withstand the challenges posed by the global emergency\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 07:02:14 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Sarosh", "Parsa", ""], ["Parah", "Shabir A.", ""], ["Mansur", "Romany F", ""], ["Bhat", "G. M.", ""]]}, {"id": "2012.06318", "submitter": "Matthew Muckley", "authors": "Matthew J. Muckley, Bruno Riemenschneider, Alireza Radmanesh, Sunwoo\n  Kim, Geunu Jeong, Jingyu Ko, Yohan Jun, Hyungseob Shin, Dosik Hwang, Mahmoud\n  Mostapha, Simon Arberet, Dominik Nickel, Zaccharie Ramzi, Philippe Ciuciu,\n  Jean-Luc Starck, Jonas Teuwen, Dimitrios Karkalousos, Chaoping Zhang, Anuroop\n  Sriram, Zhengnan Huang, Nafissa Yakubova, Yvonne Lui, Florian Knoll", "title": "Results of the 2020 fastMRI Challenge for Machine Learning MR Image\n  Reconstruction", "comments": "M. J. Muckley and B. Riemenschneider contributed equally to this\n  work. This updates to version accepted in IEEE Transactions on Medical\n  Imaging. It includes a rewrite of Section II.E as well as minor changes and\n  corrections", "journal-ref": null, "doi": "10.1109/TMI.2021.3075856", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerating MRI scans is one of the principal outstanding problems in the\nMRI research community. Towards this goal, we hosted the second fastMRI\ncompetition targeted towards reconstructing MR images with subsampled k-space\ndata. We provided participants with data from 7,299 clinical brain scans\n(de-identified via a HIPAA-compliant procedure by NYU Langone Health), holding\nback the fully-sampled data from 894 of these scans for challenge evaluation\npurposes. In contrast to the 2019 challenge, we focused our radiologist\nevaluations on pathological assessment in brain images. We also debuted a new\nTransfer track that required participants to submit models evaluated on MRI\nscanners from outside the training set. We received 19 submissions from eight\ndifferent groups. Results showed one team scoring best in both SSIM scores and\nqualitative radiologist evaluations. We also performed analysis on alternative\nmetrics to mitigate the effects of background noise and collected feedback from\nthe participants to inform future challenges. Lastly, we identify common\nfailure modes across the submissions, highlighting areas of need for future\nresearch in the MRI reconstruction community.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 19:20:16 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 05:18:32 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 12:29:30 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Muckley", "Matthew J.", ""], ["Riemenschneider", "Bruno", ""], ["Radmanesh", "Alireza", ""], ["Kim", "Sunwoo", ""], ["Jeong", "Geunu", ""], ["Ko", "Jingyu", ""], ["Jun", "Yohan", ""], ["Shin", "Hyungseob", ""], ["Hwang", "Dosik", ""], ["Mostapha", "Mahmoud", ""], ["Arberet", "Simon", ""], ["Nickel", "Dominik", ""], ["Ramzi", "Zaccharie", ""], ["Ciuciu", "Philippe", ""], ["Starck", "Jean-Luc", ""], ["Teuwen", "Jonas", ""], ["Karkalousos", "Dimitrios", ""], ["Zhang", "Chaoping", ""], ["Sriram", "Anuroop", ""], ["Huang", "Zhengnan", ""], ["Yakubova", "Nafissa", ""], ["Lui", "Yvonne", ""], ["Knoll", "Florian", ""]]}, {"id": "2012.06320", "submitter": "Sirin Haddad", "authors": "Sirin Haddad, Siew-Kei Lam", "title": "Self-Growing Spatial Graph Network for Context-Aware Pedestrian\n  Trajectory Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Pedestrian trajectory prediction is an active research area with recent works\nundertaken to embed accurate models of pedestrians social interactions and\ntheir contextual compliance into dynamic spatial graphs. However, existing\nworks rely on spatial assumptions about the scene and dynamics, which entails a\nsignificant challenge to adapt the graph structure in unknown environments for\nan online system. In addition, there is a lack of assessment approach for the\nrelational modeling impact on prediction performance. To fill this gap, we\npropose Social Trajectory Recommender-Gated Graph Recurrent Neighborhood\nNetwork, (STR-GGRNN), which uses data-driven adaptive online neighborhood\nrecommendation based on the contextual scene features and pedestrian visual\ncues. The neighborhood recommendation is achieved by online Nonnegative Matrix\nFactorization (NMF) to construct the graph adjacency matrices for predicting\nthe pedestrians' trajectories. Experiments based on widely-used datasets show\nthat our method outperforms the state-of-the-art. Our best performing model\nachieves 12 cm ADE and $\\sim$15 cm FDE on ETH-UCY dataset. The proposed method\ntakes only 0.49 seconds when sampling a total of 20K future trajectories per\nframe.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 13:25:58 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 11:00:52 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Haddad", "Sirin", ""], ["Lam", "Siew-Kei", ""]]}, {"id": "2012.06354", "submitter": "Alexander Ziller", "authors": "Alexander Ziller, Jonathan Passerat-Palmbach, Th\\'eo Ryffel, Dmitrii\n  Usynin, Andrew Trask, Ion\\'esio Da Lima Costa Junior, Jason Mancuso, Marcus\n  Makowski, Daniel Rueckert, Rickmer Braren, Georgios Kaissis", "title": "Privacy-preserving medical image analysis", "comments": "Accepted at the workshop for Medical Imaging meets NeurIPS, 34th\n  Conference on Neural Information Processing Systems (NeurIPS) December 11,\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The utilisation of artificial intelligence in medicine and healthcare has led\nto successful clinical applications in several domains. The conflict between\ndata usage and privacy protection requirements in such systems must be resolved\nfor optimal results as well as ethical and legal compliance. This calls for\ninnovative solutions such as privacy-preserving machine learning (PPML). We\npresent PriMIA (Privacy-preserving Medical Image Analysis), a software\nframework designed for PPML in medical imaging. In a real-life case study we\ndemonstrate significantly better classification performance of a securely\naggregated federated learning model compared to human experts on unseen\ndatasets. Furthermore, we show an inference-as-a-service scenario for\nend-to-end encrypted diagnosis, where neither the data nor the model are\nrevealed. Lastly, we empirically evaluate the framework's security against a\ngradient-based model inversion attack and demonstrate that no usable\ninformation can be recovered from the model.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 13:56:00 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Ziller", "Alexander", ""], ["Passerat-Palmbach", "Jonathan", ""], ["Ryffel", "Th\u00e9o", ""], ["Usynin", "Dmitrii", ""], ["Trask", "Andrew", ""], ["Junior", "Ion\u00e9sio Da Lima Costa", ""], ["Mancuso", "Jason", ""], ["Makowski", "Marcus", ""], ["Rueckert", "Daniel", ""], ["Braren", "Rickmer", ""], ["Kaissis", "Georgios", ""]]}, {"id": "2012.06363", "submitter": "Radu P Horaud", "authors": "Miles Hansard and Radu Horaud", "title": "Cyclopean Geometry of Binocular Vision", "comments": null, "journal-ref": "Journal of The Optical Society of America A, 25(9), 2008", "doi": "10.1364/JOSAA.25.002357", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The geometry of binocular projection is analyzed, with reference to the\nprimate visual system. In particular, the effects of coordinated eye movements\non the retinal images are investigated. An appropriate oculomotor\nparameterization is defined, and is shown to complement the classical version\nand vergence angles. The midline horopter is identified, and subsequently used\nto construct the epipolar geometry of the system. It is shown that the\nEssential matrix can be obtained by combining the epipoles with the projection\nof the midline horopter. A local model of the scene is adopted, in which depth\nis measured relative to a plane containing the fixation point. The binocular\ndisparity field is given a symmetric parameterization, in which the unknown\nscene-depths determine the location of corresponding image-features. The\nresulting Cyclopean depth-map can be combined with the estimated oculomotor\nparameters, to produce a local representation of the scene. The recovery of\nvisual direction and depth from retinal images is discussed, with reference to\nthe relevant psychophysical and neurophysiological literature.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 14:05:37 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Hansard", "Miles", ""], ["Horaud", "Radu", ""]]}, {"id": "2012.06365", "submitter": "Peter Drotar", "authors": "Peter Bugata and Peter Drotar", "title": "Feature Selection Based on Sparse Neural Network Layer with Normalizing\n  Constraints", "comments": "in IEEE Transactions on Cybernetics", "journal-ref": null, "doi": "10.1109/TCYB.2021.3087776", "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Feature selection is important step in machine learning since it has shown to\nimprove prediction accuracy while depressing the curse of dimensionality of\nhigh dimensional data. The neural networks have experienced tremendous success\nin solving many nonlinear learning problems. Here, we propose new\nneural-network based feature selection approach that introduces two constrains,\nthe satisfying of which leads to sparse FS layer. We have performed extensive\nexperiments on synthetic and real world data to evaluate performance of the\nproposed FS. In experiments we focus on the high dimension, low sample size\ndata since those represent the main challenge for feature selection. The\nresults confirm that proposed Feature Selection Based on Sparse Neural Network\nLayer with Normalizing Constraints (SNEL-FS) is able to select the important\nfeatures and yields superior performance compared to other conventional FS\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 14:14:33 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 09:00:38 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Bugata", "Peter", ""], ["Drotar", "Peter", ""]]}, {"id": "2012.06380", "submitter": "Dana Kianfar", "authors": "Dana Kianfar, Auke Wiggers, Amir Said, Reza Pourreza, Taco Cohen", "title": "Parallelized Rate-Distortion Optimized Quantization Using Deep Learning", "comments": "6 pages; To be published at IEEE MMSP 2020 Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rate-Distortion Optimized Quantization (RDOQ) has played an important role in\nthe coding performance of recent video compression standards such as H.264/AVC,\nH.265/HEVC, VP9 and AV1. This scheme yields significant reductions in bit-rate\nat the expense of relatively small increases in distortion. Typically, RDOQ\nalgorithms are prohibitively expensive to implement on real-time hardware\nencoders due to their sequential nature and their need to frequently obtain\nentropy coding costs. This work addresses this limitation using a neural\nnetwork-based approach, which learns to trade-off rate and distortion during\noffline supervised training. As these networks are based solely on standard\narithmetic operations that can be executed on existing neural network hardware,\nno additional area-on-chip needs to be reserved for dedicated RDOQ circuitry.\nWe train two classes of neural networks, a fully-convolutional network and an\nauto-regressive network, and evaluate each as a post-quantization step designed\nto refine cheap quantization schemes such as scalar quantization (SQ). Both\nnetwork architectures are designed to have a low computational overhead. After\ntraining they are integrated into the HM 16.20 implementation of HEVC, and\ntheir video coding performance is evaluated on a subset of the H.266/VVC SDR\ncommon test sequences. Comparisons are made to RDOQ and SQ implementations in\nHM 16.20. Our method achieves 1.64% BD-rate savings on luminosity compared to\nthe HM SQ anchor, and on average reaches 45% of the performance of the\niterative HM RDOQ algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 14:28:30 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Kianfar", "Dana", ""], ["Wiggers", "Auke", ""], ["Said", "Amir", ""], ["Pourreza", "Reza", ""], ["Cohen", "Taco", ""]]}, {"id": "2012.06387", "submitter": "William Paul", "authors": "William Paul, Armin Hadzic, Neil Joshi, Fady Alajaji, Phil Burlina", "title": "TARA: Training and Representation Alteration for AI Fairness and Domain\n  Generalization", "comments": "Submitted to MIT Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for enforcing AI fairness with respect to protected\nor sensitive factors. This method uses a dual strategy performing training and\nrepresentation alteration (TARA) for the mitigation of prominent causes of AI\nbias by including: a) the use of representation learning alteration via\nadversarial independence to suppress the bias-inducing dependence of the data\nrepresentation from protected factors; and b) training set alteration via\nintelligent augmentation to address bias-causing data imbalance, by using\ngenerative models that allow the fine control of sensitive factors related to\nunderrepresented populations via domain adaptation and latent space\nmanipulation. When testing our methods on image analytics, experiments\ndemonstrate that TARA significantly or fully debiases baseline models while\noutperforming competing debiasing methods that have the same amount of\ninformation, e.g., with (% overall accuracy, % accuracy gap) = (78.8, 0.5) vs.\nthe baseline method's score of (71.8, 10.5) for EyePACS, and (73.7, 11.8) vs.\n(69.1, 21.7) for CelebA. Furthermore, recognizing certain limitations in\ncurrent metrics used for assessing debiasing performance, we propose novel\nconjunctive debiasing metrics. Our experiments also demonstrate the ability of\nthese novel metrics in assessing the Pareto efficiency of the proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 14:39:10 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 14:40:51 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 15:59:38 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Paul", "William", ""], ["Hadzic", "Armin", ""], ["Joshi", "Neil", ""], ["Alajaji", "Fady", ""], ["Burlina", "Phil", ""]]}, {"id": "2012.06399", "submitter": "Chiara Plizzari", "authors": "Chiara Plizzari, Marco Cannici, Matteo Matteucci", "title": "Spatial Temporal Transformer Network for Skeleton-based Action\n  Recognition", "comments": "Accepted as ICPRW2020 (FBE2020, Workshop on Facial and Body\n  Expressions, micro-expressions and behavior recognition) 8 pages, 2 figures.\n  arXiv admin note: substantial text overlap with arXiv:2008.07404", "journal-ref": "Pattern Recognition. ICPR International Workshops and Challenges.\n  ICPR 2021. Lecture Notes in Computer Science, Springer, vol 12663, 694-701,\n  ISBN: 978-3-030-68796-0", "doi": "10.1007/978-3-030-68796-0_50", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skeleton-based human action recognition has achieved a great interest in\nrecent years, as skeleton data has been demonstrated to be robust to\nillumination changes, body scales, dynamic camera views, and complex\nbackground. Nevertheless, an effective encoding of the latent information\nunderlying the 3D skeleton is still an open problem. In this work, we propose a\nnovel Spatial-Temporal Transformer network (ST-TR) which models dependencies\nbetween joints using the Transformer self-attention operator. In our ST-TR\nmodel, a Spatial Self-Attention module (SSA) is used to understand intra-frame\ninteractions between different body parts, and a Temporal Self-Attention module\n(TSA) to model inter-frame correlations. The two are combined in a two-stream\nnetwork which outperforms state-of-the-art models using the same input data on\nboth NTU-RGB+D 60 and NTU-RGB+D 120.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 14:58:21 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Plizzari", "Chiara", ""], ["Cannici", "Marco", ""], ["Matteucci", "Matteo", ""]]}, {"id": "2012.06405", "submitter": "Nathan Drenkow", "authors": "Nathan Drenkow, Neil Fendley, Philippe Burlina", "title": "Random Projections for Adversarial Attack Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst adversarial attack detection has received considerable attention, it\nremains a fundamentally challenging problem from two perspectives. First, while\nthreat models can be well-defined, attacker strategies may still vary widely\nwithin those constraints. Therefore, detection should be considered as an\nopen-set problem, standing in contrast to most current detection strategies.\nThese methods take a closed-set view and train binary detectors, thus biasing\ndetection toward attacks seen during detector training. Second, information is\nlimited at test time and confounded by nuisance factors including the label and\nunderlying content of the image. Many of the current high-performing techniques\nuse training sets for dealing with some of these issues, but can be limited by\nthe overall size and diversity of those sets during the detection step. We\naddress these challenges via a novel strategy based on random subspace\nanalysis. We present a technique that makes use of special properties of random\nprojections, whereby we can characterize the behavior of clean and adversarial\nexamples across a diverse set of subspaces. We then leverage the\nself-consistency (or inconsistency) of model activations to discern clean from\nadversarial examples. Performance evaluation demonstrates that our technique\noutperforms ($>0.92$ AUC) competing state of the art (SOTA) attack strategies,\nwhile remaining truly agnostic to the attack method itself. It also requires\nsignificantly less training data, composed only of clean examples, when\ncompared to competing SOTA methods, which achieve only chance performance, when\nevaluated in a more rigorous testing scenario.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 15:02:28 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Drenkow", "Nathan", ""], ["Fendley", "Neil", ""], ["Burlina", "Philippe", ""]]}, {"id": "2012.06414", "submitter": "Andrea Loddo", "authors": "A.M.P.G. Vale, M. Ucchesu, C. Di Ruberto, A. Loddo, J.M. Soares,\n  G.Bacchetta", "title": "A new automatic approach to seed image analysis: From acquisition to\n  segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Analysis offers a new tool for classifying vascular plant species based\non the morphological and colorimetric features of the seeds, and has made\nsignificant contributions in systematic studies. However, in order to extract\nthe morphological and colorimetric features, it is necessary to segment the\nimage containing the samples to be analysed. This stage represents one of the\nmost challenging steps in image processing, as it is difficult to separate\nuniform and homogeneous objects from the background. In this paper, we present\na new, open source plugin for the automatic segmentation of an image of a seed\nsample. This plugin was written in Java to allow it to work with ImageJ open\nsource software. The new plugin was tested on a total of 3,386 seed samples\nfrom 120 species belonging to the Fabaceae family. Digital images were acquired\nusing a flatbed scanner. In order to test the efficacy of this approach in\nterms of identifying the edges of objects and separating them from the\nbackground, each sample was scanned using four different hues of blue for the\nbackground, and a total of 480 digital images were elaborated. The performance\nof the new plugin was compared with a method based on double image acquisition\n(with a black and white background) using the same seed samples, in which\nimages were manually segmented using the Core ImageJ plugin. The results showed\nthat the new plugin was able to segment all of the digital images without\ngenerating any object detection errors. In addition, the new plugin was able to\nsegment images within an average of 0.02 s, while the average time for\nexecution with the manual method was 63 s. This new open source plugin is\nproven to be able to work on a single image, and to be highly efficient in\nterms of time and segmentation when working with large numbers of images and a\nwide diversity of shapes.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 15:11:22 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Vale", "A. M. P. G.", ""], ["Ucchesu", "M.", ""], ["Di Ruberto", "C.", ""], ["Loddo", "A.", ""], ["Soares", "J. M.", ""], ["Bacchetta", "G.", ""]]}, {"id": "2012.06418", "submitter": "Ye Li", "authors": "Ye Li, Kangning Yin, Jie Liang, Chunyu Wang, Guangqiang Yin", "title": "A Multi-task Joint Framework for Real-time Person Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person search generally involves three important parts: person detection,\nfeature extraction and identity comparison. However, person search integrating\ndetection, extraction and comparison has the following drawbacks. Firstly, the\naccuracy of detection will affect the accuracy of comparison. Secondly, it is\ndifficult to achieve real-time in real-world applications. To solve these\nproblems, we propose a Multi-task Joint Framework for real-time person search\n(MJF), which optimizes the person detection, feature extraction and identity\ncomparison respectively. For the person detection module, we proposed the\nYOLOv5-GS model, which is trained with person dataset. It combines the\nadvantages of the Ghostnet and the Squeeze-and-Excitation (SE) block, and\nimproves the speed and accuracy. For the feature extraction module, we design\nthe Model Adaptation Architecture (MAA), which could select different network\naccording to the number of people. It could balance the relationship between\naccuracy and speed. For identity comparison, we propose a Three Dimension (3D)\nPooled Table and a matching strategy to improve identification accuracy. On the\ncondition of 1920*1080 resolution video and 500 IDs table, the identification\nrate (IR) and frames per second (FPS) achieved by our method could reach 93.6%\nand 25.7,\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 15:21:15 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Li", "Ye", ""], ["Yin", "Kangning", ""], ["Liang", "Jie", ""], ["Wang", "Chunyu", ""], ["Yin", "Guangqiang", ""]]}, {"id": "2012.06428", "submitter": "Christos Kyrkou", "authors": "Christos Kyrkou", "title": "Imitation-Based Active Camera Control with Deep Convolutional Neural\n  Network", "comments": "Paper accepted in Fourth IEEE International Conference on Image\n  Processing, Applications, and Systems (IEEE IPAS 2020)", "journal-ref": null, "doi": "10.1109/IPAS50080.2020.9334958", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing need for automated visual monitoring and control for\napplications such as smart camera surveillance, traffic monitoring, and\nintelligent environments, necessitates the improvement of methods for visual\nactive monitoring. Traditionally, the active monitoring task has been handled\nthrough a pipeline of modules such as detection, filtering, and control. In\nthis paper we frame active visual monitoring as an imitation learning problem\nto be solved in a supervised manner using deep learning, to go directly from\nvisual information to camera movement in order to provide a satisfactory\nsolution by combining computer vision and control. A deep convolutional neural\nnetwork is trained end-to-end as the camera controller that learns the entire\nprocessing pipeline needed to control a camera to follow multiple targets and\nalso estimate their density from a single image. Experimental results indicate\nthat the proposed solution is robust to varying conditions and is able to\nachieve better monitoring performance both in terms of number of targets\nmonitored as well as in monitoring time than traditional approaches, while\nreaching up to 25 FPS. Thus making it a practical and affordable solution for\nmulti-target active monitoring in surveillance and smart-environment\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 15:37:33 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Kyrkou", "Christos", ""]]}, {"id": "2012.06434", "submitter": "Wang Yifan", "authors": "Wang Yifan, Shihao Wu, Cengiz Oztireli, Olga Sorkine-Hornung", "title": "Iso-Points: Optimizing Neural Implicit Surfaces with Hybrid\n  Representations", "comments": "CVPR 2021 code: https://github.com/yifita/iso-points", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural implicit functions have emerged as a powerful representation for\nsurfaces in 3D. Such a function can encode a high quality surface with\nintricate details into the parameters of a deep neural network. However,\noptimizing for the parameters for accurate and robust reconstructions remains a\nchallenge, especially when the input data is noisy or incomplete. In this work,\nwe develop a hybrid neural surface representation that allows us to impose\ngeometry-aware sampling and regularization, which significantly improves the\nfidelity of reconstructions. We propose to use \\emph{iso-points} as an explicit\nrepresentation for a neural implicit function. These points are computed and\nupdated on-the-fly during training to capture important geometric features and\nimpose geometric constraints on the optimization. We demonstrate that our\nmethod can be adopted to improve state-of-the-art techniques for reconstructing\nneural implicit surfaces from multi-view images or point clouds. Quantitative\nand qualitative evaluations show that, compared with existing sampling and\noptimization methods, our approach allows faster convergence, better\ngeneralization, and accurate recovery of details and topology.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 15:51:04 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 20:11:58 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Yifan", "Wang", ""], ["Wu", "Shihao", ""], ["Oztireli", "Cengiz", ""], ["Sorkine-Hornung", "Olga", ""]]}, {"id": "2012.06436", "submitter": "Richard McKinley", "authors": "Richard McKinley, Micheal Rebsamen, Katrin Daetwyler, Raphael Meier,\n  Piotr Radojewski, Roland Wiest", "title": "Uncertainty-driven refinement of tumor-core segmentation using 3D-to-2D\n  networks with label uncertainty", "comments": "Presented (virtually) in the MICCAI Brainles workshop 2020. Accepted\n  for publication in Brainles proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The BraTS dataset contains a mixture of high-grade and low-grade gliomas,\nwhich have a rather different appearance: previous studies have shown that\nperformance can be improved by separated training on low-grade gliomas (LGGs)\nand high-grade gliomas (HGGs), but in practice this information is not\navailable at test time to decide which model to use. By contrast with HGGs,\nLGGs often present no sharp boundary between the tumor core and the surrounding\nedema, but rather a gradual reduction of tumor-cell density.\n  Utilizing our 3D-to-2D fully convolutional architecture, DeepSCAN, which\nranked highly in the 2019 BraTS challenge and was trained using an\nuncertainty-aware loss, we separate cases into those with a confidently\nsegmented core, and those with a vaguely segmented or missing core. Since by\nassumption every tumor has a core, we reduce the threshold for classification\nof core tissue in those cases where the core, as segmented by the classifier,\nis vaguely defined or missing.\n  We then predict survival of high-grade glioma patients using a fusion of\nlinear regression and random forest classification, based on age, number of\ndistinct tumor components, and number of distinct tumor cores.\n  We present results on the validation dataset of the Multimodal Brain Tumor\nSegmentation Challenge 2020 (segmentation and uncertainty challenge), and on\nthe testing set, where the method achieved 4th place in Segmentation, 1st place\nin uncertainty estimation, and 1st place in Survival prediction.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 15:57:34 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["McKinley", "Richard", ""], ["Rebsamen", "Micheal", ""], ["Daetwyler", "Katrin", ""], ["Meier", "Raphael", ""], ["Radojewski", "Piotr", ""], ["Wiest", "Roland", ""]]}, {"id": "2012.06440", "submitter": "Sanath Narayan", "authors": "Sanath Narayan, Hisham Cholakkal, Munawar Hayat, Fahad Shahbaz Khan,\n  Ming-Hsuan Yang, Ling Shao", "title": "D2-Net: Weakly-Supervised Action Localization via Discriminative\n  Embeddings and Denoised Activations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a weakly-supervised temporal action localization\nframework, called D2-Net, which strives to temporally localize actions using\nvideo-level supervision. Our main contribution is the introduction of a novel\nloss formulation, which jointly enhances the discriminability of latent\nembeddings and robustness of the output temporal class activations with respect\nto foreground-background noise caused by weak supervision. The proposed\nformulation comprises a discriminative and a denoising loss term for enhancing\ntemporal action localization. The discriminative term incorporates a\nclassification loss and utilizes a top-down attention mechanism to enhance the\nseparability of latent foreground-background embeddings. The denoising loss\nterm explicitly addresses the foreground-background noise in class activations\nby simultaneously maximizing intra-video and inter-video mutual information\nusing a bottom-up attention mechanism. As a result, activations in the\nforeground regions are emphasized whereas those in the background regions are\nsuppressed, thereby leading to more robust predictions. Comprehensive\nexperiments are performed on two benchmarks: THUMOS14 and ActivityNet1.2. Our\nD2-Net performs favorably in comparison to the existing methods on both\ndatasets, achieving gains as high as 3.6% in terms of mean average precision on\nTHUMOS14.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 16:01:56 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Narayan", "Sanath", ""], ["Cholakkal", "Hisham", ""], ["Hayat", "Munawar", ""], ["Khan", "Fahad Shahbaz", ""], ["Yang", "Ming-Hsuan", ""], ["Shao", "Ling", ""]]}, {"id": "2012.06444", "submitter": "Yang Liu", "authors": "Yang Liu, Alexandros Neophytou, Sunando Sengupta, Eric Sommerlade", "title": "Relighting Images in the Wild with a Self-Supervised Siamese\n  Auto-Encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a self-supervised method for image relighting of single view\nimages in the wild. The method is based on an auto-encoder which deconstructs\nan image into two separate encodings, relating to the scene illumination and\ncontent, respectively. In order to disentangle this embedding information\nwithout supervision, we exploit the assumption that some augmentation\noperations do not affect the image content and only affect the direction of the\nlight. A novel loss function, called spherical harmonic loss, is introduced\nthat forces the illumination embedding to convert to a spherical harmonic\nvector. We train our model on large-scale datasets such as Youtube 8M and\nCelebA. Our experiments show that our method can correctly estimate scene\nillumination and realistically re-light input images, without any supervision\nor a prior shape model. Compared to supervised methods, our approach has\nsimilar performance and avoids common lighting artifacts.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 16:08:50 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Liu", "Yang", ""], ["Neophytou", "Alexandros", ""], ["Sengupta", "Sunando", ""], ["Sommerlade", "Eric", ""]]}, {"id": "2012.06457", "submitter": "Ke Yu", "authors": "Li Sun, Ke Yu, Kayhan Batmanghelich", "title": "Context Matters: Graph-based Self-supervised Representation Learning for\n  Medical Images", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Supervised learning method requires a large volume of annotated datasets.\nCollecting such datasets is time-consuming and expensive. Until now, very few\nannotated COVID-19 imaging datasets are available. Although self-supervised\nlearning enables us to bootstrap the training by exploiting unlabeled data, the\ngeneric self-supervised methods for natural images do not sufficiently\nincorporate the context. For medical images, a desirable method should be\nsensitive enough to detect deviation from normal-appearing tissue of each\nanatomical region; here, anatomy is the context. We introduce a novel approach\nwith two levels of self-supervised representation learning objectives: one on\nthe regional anatomical level and another on the patient-level. We use graph\nneural networks to incorporate the relationship between different anatomical\nregions. The structure of the graph is informed by anatomical correspondences\nbetween each patient and an anatomical atlas. In addition, the graph\nrepresentation has the advantage of handling any arbitrarily sized image in\nfull resolution. Experiments on large-scale Computer Tomography (CT) datasets\nof lung images show that our approach compares favorably to baseline methods\nthat do not account for the context. We use the learnt embedding to quantify\nthe clinical progression of COVID-19 and show that our method generalizes well\nto COVID-19 patients from different hospitals. Qualitative results suggest that\nour model can identify clinically relevant regions in the images.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 16:26:07 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Sun", "Li", ""], ["Yu", "Ke", ""], ["Batmanghelich", "Kayhan", ""]]}, {"id": "2012.06462", "submitter": "Sepehr Jalali", "authors": "Federica Freddi, Jezabel R Garcia, Michael Bromberg, Sepehr Jalali,\n  Da-Shan Shiu, Alvin Chua, Alberto Bernacchia", "title": "Cyclic orthogonal convolutions for long-range integration of features", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Convolutional Neural Networks (CNNs) information flows across a small\nneighbourhood of each pixel of an image, preventing long-range integration of\nfeatures before reaching deep layers in the network. We propose a novel\narchitecture that allows flexible information flow between features $z$ and\nlocations $(x,y)$ across the entire image with a small number of layers. This\narchitecture uses a cycle of three orthogonal convolutions, not only in $(x,y)$\ncoordinates, but also in $(x,z)$ and $(y,z)$ coordinates. We stack a sequence\nof such cycles to obtain our deep network, named CycleNet. As this only\nrequires a permutation of the axes of a standard convolution, its performance\ncan be directly compared to a CNN. Our model obtains competitive results at\nimage classification on CIFAR-10 and ImageNet datasets, when compared to CNNs\nof similar size. We hypothesise that long-range integration favours recognition\nof objects by shape rather than texture, and we show that CycleNet transfers\nbetter than CNNs to stylised images. On the Pathfinder challenge, where\nintegration of distant features is crucial, CycleNet outperforms CNNs by a\nlarge margin. We also show that even when employing a small convolutional\nkernel, the size of receptive fields of CycleNet reaches its maximum after one\ncycle, while conventional CNNs require a large number of layers.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 16:33:48 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Freddi", "Federica", ""], ["Garcia", "Jezabel R", ""], ["Bromberg", "Michael", ""], ["Jalali", "Sepehr", ""], ["Shiu", "Da-Shan", ""], ["Chua", "Alvin", ""], ["Bernacchia", "Alberto", ""]]}, {"id": "2012.06469", "submitter": "Indra Deep Mastan", "authors": "Indra Deep Mastan and Shanmuganathan Raman", "title": "DILIE: Deep Internal Learning for Image Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the generic deep image enhancement problem where an input image\nis transformed into a perceptually better-looking image. Recent methods for\nimage enhancement consider the problem by performing style transfer and image\nrestoration. The methods mostly fall into two categories: training data-based\nand training data-independent (deep internal learning methods). We perform\nimage enhancement in the deep internal learning framework. Our Deep Internal\nLearning for Image Enhancement framework enhances content features and style\nfeatures and uses contextual content loss for preserving image context in the\nenhanced image. We show results on both hazy and noisy image enhancement. To\nvalidate the results, we use structure similarity and perceptual error, which\nis efficient in measuring the unrealistic deformation present in the images. We\nshow that the proposed framework outperforms the relevant state-of-the-art\nworks for image enhancement.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 16:39:44 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Mastan", "Indra Deep", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "2012.06475", "submitter": "Viktor Rudnev", "authors": "Viktor Rudnev and Vladislav Golyanik and Jiayi Wang and Hans-Peter\n  Seidel and Franziska Mueller and Mohamed Elgharib and Christian Theobalt", "title": "EventHands: Real-Time Neural 3D Hand Reconstruction from an Event Stream", "comments": "Project page: https://gvv.mpi-inf.mpg.de/projects/EventHands/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D hand pose estimation from monocular videos is a long-standing and\nchallenging problem, which is now seeing a strong upturn. In this work, we\naddress it for the first time using a single event camera, i.e., an\nasynchronous vision sensor reacting on brightness changes. Our EventHands\napproach has characteristics previously not demonstrated with a single RGB or\ndepth camera such as high temporal resolution at low data throughputs and\nreal-time performance at 1000 Hz. Due to the different data modality of event\ncameras compared to classical cameras, existing methods cannot be directly\napplied to and re-trained for event streams. We thus design a new neural\napproach which accepts a new event stream representation suitable for learning,\nwhich is trained on newly-generated synthetic event streams and can generalise\nto real data. Experiments show that EventHands outperforms recent monocular\nmethods using a colour (or depth) camera in terms of accuracy and its ability\nto capture hand motions of unprecedented speed. Our method, the event stream\nsimulator and the dataset will be made publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 16:45:34 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 16:12:52 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Rudnev", "Viktor", ""], ["Golyanik", "Vladislav", ""], ["Wang", "Jiayi", ""], ["Seidel", "Hans-Peter", ""], ["Mueller", "Franziska", ""], ["Elgharib", "Mohamed", ""], ["Theobalt", "Christian", ""]]}, {"id": "2012.06494", "submitter": "Hongming Li", "authors": "Hongming Li, Yong Fan", "title": "Unsupervised deep learning for individualized brain functional network\n  identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel unsupervised deep learning method is developed to identify\nindividual-specific large scale brain functional networks (FNs) from\nresting-state fMRI (rsfMRI) in an end-to-end learning fashion. Our method\nleverages deep Encoder-Decoder networks and conventional brain decomposition\nmodels to identify individual-specific FNs in an unsupervised learning\nframework and facilitate fast inference for new individuals with one forward\npass of the deep network. Particularly, convolutional neural networks (CNNs)\nwith an Encoder-Decoder architecture are adopted to identify\nindividual-specific FNs from rsfMRI data by optimizing their data fitting and\nsparsity regularization terms that are commonly used in brain decomposition\nmodels. Moreover, a time-invariant representation learning module is designed\nto learn features invariant to temporal orders of time points of rsfMRI data.\nThe proposed method has been validated based on a large rsfMRI dataset and\nexperimental results have demonstrated that our method could obtain\nindividual-specific FNs which are consistent with well-established FNs and are\ninformative for predicting brain age, indicating that the individual-specific\nFNs identified truly captured the underlying variability of individualized\nfunctional neuroanatomy.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 16:58:55 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Li", "Hongming", ""], ["Fan", "Yong", ""]]}, {"id": "2012.06498", "submitter": "Indra Deep Mastan", "authors": "Indra Deep Mastan and Shanmuganathan Raman", "title": "DeepObjStyle: Deep Object-based Photo Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the major challenges of style transfer is the appropriate image\nfeatures supervision between the output image and the input (style and content)\nimages. An efficient strategy would be to define an object map between the\nobjects of the style and the content images. However, such a mapping is not\nwell established when there are semantic objects of different types and numbers\nin the style and the content images. It also leads to content mismatch in the\nstyle transfer output, which could reduce the visual quality of the results. We\npropose an object-based style transfer approach, called DeepObjStyle, for the\nstyle supervision in the training data-independent framework. DeepObjStyle\npreserves the semantics of the objects and achieves better style transfer in\nthe challenging scenario when the style and the content images have a mismatch\nof image features. We also perform style transfer of images containing a word\ncloud to demonstrate that DeepObjStyle enables an appropriate image features\nsupervision. We validate the results using quantitative comparisons and user\nstudies.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 17:02:01 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Mastan", "Indra Deep", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "2012.06508", "submitter": "Charles Corbi\\`ere", "authors": "Charles Corbi\\`ere, Nicolas Thome, Antoine Saporta, Tuan-Hung Vu,\n  Matthieu Cord, Patrick P\\'erez", "title": "Confidence Estimation via Auxiliary Models", "comments": "Accepted to TPAMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reliably quantifying the confidence of deep neural classifiers is a\nchallenging yet fundamental requirement for deploying such models in\nsafety-critical applications. In this paper, we introduce a novel target\ncriterion for model confidence, namely the true class probability (TCP). We\nshow that TCP offers better properties for confidence estimation than standard\nmaximum class probability (MCP). Since the true class is by essence unknown at\ntest time, we propose to learn TCP criterion from data with an auxiliary model,\nintroducing a specific learning scheme adapted to this context. We evaluate our\napproach on the task of failure prediction and of self-training with\npseudo-labels for domain adaptation, which both necessitate effective\nconfidence estimates. Extensive experiments are conducted for validating the\nrelevance of the proposed approach in each task. We study various network\narchitectures and experiment with small and large datasets for image\nclassification and semantic segmentation. In every tested benchmark, our\napproach outperforms strong baselines.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 17:21:12 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 17:24:34 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Corbi\u00e8re", "Charles", ""], ["Thome", "Nicolas", ""], ["Saporta", "Antoine", ""], ["Vu", "Tuan-Hung", ""], ["Cord", "Matthieu", ""], ["P\u00e9rez", "Patrick", ""]]}, {"id": "2012.06509", "submitter": "Nathan Drenkow", "authors": "Nathan Drenkow, Philippe Burlina, Neil Fendley, Onyekachi Odoemene,\n  Jared Markowitz", "title": "Addressing Visual Search in Open and Closed Set Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for small objects in large images is a task that is both\nchallenging for current deep learning systems and important in numerous\nreal-world applications, such as remote sensing and medical imaging. Thorough\nscanning of very large images is computationally expensive, particularly at\nresolutions sufficient to capture small objects. The smaller an object of\ninterest, the more likely it is to be obscured by clutter or otherwise deemed\ninsignificant. We examine these issues in the context of two complementary\nproblems: closed-set object detection and open-set target search. First, we\npresent a method for predicting pixel-level objectness from a low resolution\ngist image, which we then use to select regions for performing object detection\nlocally at high resolution. This approach has the benefit of not being fixed to\na predetermined grid, thereby requiring fewer costly high-resolution glimpses\nthan existing methods. Second, we propose a novel strategy for open-set visual\nsearch that seeks to find all instances of a target class which may be\npreviously unseen and is defined by a single image. We interpret both detection\nproblems through a probabilistic, Bayesian lens, whereby the objectness maps\nproduced by our method serve as priors in a maximum-a-posteriori approach to\nthe detection step. We evaluate the end-to-end performance of both the\ncombination of our patch selection strategy with this target search approach\nand the combination of our patch selection strategy with standard object\ndetection methods. Both elements of our approach are seen to significantly\noutperform baseline strategies.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 17:21:28 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 21:43:19 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Drenkow", "Nathan", ""], ["Burlina", "Philippe", ""], ["Fendley", "Neil", ""], ["Odoemene", "Onyekachi", ""], ["Markowitz", "Jared", ""]]}, {"id": "2012.06511", "submitter": "Donghwan Shin", "authors": "Fitash Ul Haq, Donghwan Shin, Lionel C. Briand, Thomas Stifter, Jun\n  Wang", "title": "Automatic Test Suite Generation for Key-Points Detection DNNs using\n  Many-Objective Search (Experience Paper)", "comments": "to appear in ISSTA 2021", "journal-ref": null, "doi": "10.1145/3460319.3464802", "report-no": null, "categories": "cs.CV cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically detecting the positions of key-points (e.g., facial key-points\nor finger key-points) in an image is an essential problem in many applications,\nsuch as driver's gaze detection and drowsiness detection in automated driving\nsystems. With the recent advances of Deep Neural Networks (DNNs), Key-Points\ndetection DNNs (KP-DNNs) have been increasingly employed for that purpose.\nNevertheless, KP-DNN testing and validation have remained a challenging problem\nbecause KP-DNNs predict many independent key-points at the same time -- where\neach individual key-point may be critical in the targeted application -- and\nimages can vary a great deal according to many factors.\n  In this paper, we present an approach to automatically generate test data for\nKP-DNNs using many-objective search. In our experiments, focused on facial\nkey-points detection DNNs developed for an industrial automotive application,\nwe show that our approach can generate test suites to severely mispredict, on\naverage, more than 93% of all key-points. In comparison, random search-based\ntest data generation can only severely mispredict 41% of them. Many of these\nmispredictions, however, are not avoidable and should not therefore be\nconsidered failures. We also empirically compare state-of-the-art,\nmany-objective search algorithms and their variants, tailored for test suite\ngeneration. Furthermore, we investigate and demonstrate how to learn specific\nconditions, based on image characteristics (e.g., head posture and skin color),\nthat lead to severe mispredictions. Such conditions serve as a basis for risk\nanalysis or DNN retraining.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 17:28:03 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 17:11:38 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Haq", "Fitash Ul", ""], ["Shin", "Donghwan", ""], ["Briand", "Lionel C.", ""], ["Stifter", "Thomas", ""], ["Wang", "Jun", ""]]}, {"id": "2012.06516", "submitter": "Hamid Sarmadi", "authors": "Hamid Sarmadi, Rafael Mu\\~noz-Salinas, Miguel A. Olivares-Mendez,\n  Rafael Medina-Carnicer", "title": "Detection of Binary Square Fiducial Markers Using an Event Camera", "comments": "An error in the abstract of the IEEE Access version has been\n  corrected in this version. Link to the IEEE Access paper:\n  https://doi.org/10.1109/ACCESS.2021.3058423", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3058423", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are a new type of image sensors that output changes in light\nintensity (events) instead of absolute intensity values. They have a very high\ntemporal resolution and a high dynamic range. In this paper, we propose a\nmethod to detect and decode binary square markers using an event camera. We\ndetect the edges of the markers by detecting line segments in an image created\nfrom events in the current packet. The line segments are combined to form\nmarker candidates. The bit value of marker cells is decoded using the events on\ntheir borders. To the best of our knowledge, no other approach exists for\ndetecting square binary markers directly from an event camera using only the\nCPU unit in real-time. Experimental results show that the performance of our\nproposal is much superior to the one from the RGB ArUco marker detector. The\nproposed method can achieve the real-time performance on a single CPU thread.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 17:34:47 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 13:53:49 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 21:23:51 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sarmadi", "Hamid", ""], ["Mu\u00f1oz-Salinas", "Rafael", ""], ["Olivares-Mendez", "Miguel A.", ""], ["Medina-Carnicer", "Rafael", ""]]}, {"id": "2012.06523", "submitter": "Jan Kronenberger H", "authors": "Jan Kronenberger and Anselm Haselhoff", "title": "Dependency Decomposition and a Reject Option for Explainable Models", "comments": "Accepted at CVPR 2019 Workshop \"DThree19: Dependable Deep Detectors\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deploying machine learning models in safety-related do-mains (e.g. autonomous\ndriving, medical diagnosis) demands for approaches that are explainable, robust\nagainst adversarial attacks and aware of the model uncertainty. Recent deep\nlearning models perform extremely well in various inference tasks, but the\nblack-box nature of these approaches leads to a weakness regarding the three\nrequirements mentioned above. Recent advances offer methods to visualize\nfeatures, describe attribution of the input (e.g.heatmaps), provide textual\nexplanations or reduce dimensionality. However,are explanations for\nclassification tasks dependent or are they independent of each other? For\nin-stance, is the shape of an object dependent on the color? What is the effect\nof using the predicted class for generating explanations and vice versa? In the\ncontext of explainable deep learning models, we present the first analysis of\ndependencies regarding the probability distribution over the desired image\nclassification outputs and the explaining variables (e.g. attributes, texts,\nheatmaps). Therefore, we perform an Explanation Dependency Decomposition (EDD).\nWe analyze the implications of the different dependencies and propose two ways\nof generating the explanation. Finally, we use the explanation to verify\n(accept or reject) the prediction\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 17:39:33 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Kronenberger", "Jan", ""], ["Haselhoff", "Anselm", ""]]}, {"id": "2012.06531", "submitter": "Paolo Soda", "authors": "Paolo Soda, Natascha Claudia D'Amico, Jacopo Tessadori, Giovanni\n  Valbusa, Valerio Guarrasi, Chandra Bortolotto, Muhammad Usman Akbar, Rosa\n  Sicilia, Ermanno Cordelli, Deborah Fazzini, Michaela Cellina, Giancarlo\n  Oliva, Giovanni Callea, Silvia Panella, Maurizio Cariati, Diletta Cozzi,\n  Vittorio Miele, Elvira Stellato, Gian Paolo Carrafiello, Giulia Castorani,\n  Annalisa Simeone, Lorenzo Preda, Giulio Iannello, Alessio Del Bue, Fabio\n  Tedoldi, Marco Al\\`i, Diego Sona and Sergio Papa", "title": "AIforCOVID: predicting the clinical outcomes in patients with COVID-19\n  applying AI to chest-X-rays. An Italian multicentre study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent epidemiological data report that worldwide more than 53 million people\nhave been infected by SARS-CoV-2, resulting in 1.3 million deaths. The disease\nhas been spreading very rapidly and few months after the identification of the\nfirst infected, shortage of hospital resources quickly became a problem. In\nthis work we investigate whether chest X-ray (CXR) can be used as a possible\ntool for the early identification of patients at risk of severe outcome, like\nintensive care or death. CXR is a radiological technique that compared to\ncomputed tomography (CT) it is simpler, faster, more widespread and it induces\nlower radiation dose. We present a dataset including data collected from 820\npatients by six Italian hospitals in spring 2020 during the first COVID-19\nemergency. The dataset includes CXR images, several clinical attributes and\nclinical outcomes. We investigate the potential of artificial intelligence to\npredict the prognosis of such patients, distinguishing between severe and mild\ncases, thus offering a baseline reference for other researchers and\npractitioners. To this goal, we present three approaches that use features\nextracted from CXR images, either handcrafted or automatically by convolutional\nneuronal networks, which are then integrated with the clinical data. Exhaustive\nevaluation shows promising performance both in 10-fold and leave-one-centre-out\ncross-validation, implying that clinical data and images have the potential to\nprovide useful information for the management of patients and hospital\nresources.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 18:03:08 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Soda", "Paolo", ""], ["D'Amico", "Natascha Claudia", ""], ["Tessadori", "Jacopo", ""], ["Valbusa", "Giovanni", ""], ["Guarrasi", "Valerio", ""], ["Bortolotto", "Chandra", ""], ["Akbar", "Muhammad Usman", ""], ["Sicilia", "Rosa", ""], ["Cordelli", "Ermanno", ""], ["Fazzini", "Deborah", ""], ["Cellina", "Michaela", ""], ["Oliva", "Giancarlo", ""], ["Callea", "Giovanni", ""], ["Panella", "Silvia", ""], ["Cariati", "Maurizio", ""], ["Cozzi", "Diletta", ""], ["Miele", "Vittorio", ""], ["Stellato", "Elvira", ""], ["Carrafiello", "Gian Paolo", ""], ["Castorani", "Giulia", ""], ["Simeone", "Annalisa", ""], ["Preda", "Lorenzo", ""], ["Iannello", "Giulio", ""], ["Del Bue", "Alessio", ""], ["Tedoldi", "Fabio", ""], ["Al\u00ec", "Marco", ""], ["Sona", "Diego", ""], ["Papa", "Sergio", ""]]}, {"id": "2012.06547", "submitter": "Akshay Gadi Patil", "authors": "Akshay Gadi Patil, Manyi Li, Matthew Fisher, Manolis Savva, Hao Zhang", "title": "LayoutGMN: Neural Graph Matching for Structural Layout Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep neural network to predict structural similarity between 2D\nlayouts by leveraging Graph Matching Networks (GMN). Our network, coined\nLayoutGMN, learns the layout metric via neural graph matching, using an\nattention-based GMN designed under a triplet network setting. To train our\nnetwork, we utilize weak labels obtained by pixel-wise Intersection-over-Union\n(IoUs) to define the triplet loss. Importantly, LayoutGMN is built with a\nstructural bias which can effectively compensate for the lack of structure\nawareness in IoUs. We demonstrate this on two prominent forms of layouts, viz.,\nfloorplans and UI designs, via retrieval experiments on large-scale datasets.\nIn particular, retrieval results by our network better match human judgement of\nstructural layout similarity compared to both IoUs and other baselines\nincluding a state-of-the-art method based on graph neural networks and image\nconvolution. In addition, LayoutGMN is the first deep model to offer both\nmetric learning of structural layout similarity and structural matching between\nlayout elements.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 18:24:18 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 02:58:30 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Patil", "Akshay Gadi", ""], ["Li", "Manyi", ""], ["Fisher", "Matthew", ""], ["Savva", "Manolis", ""], ["Zhang", "Hao", ""]]}, {"id": "2012.06563", "submitter": "Aythami Morales", "authors": "Luis Felipe Gomez-Gomez and Aythami Morales and Julian Fierrez and\n  Juan Rafael Orozco-Arroyave", "title": "Exploring Facial Expressions and Affective Domains for Parkinson\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Parkinson's Disease (PD) is a neurological disorder that affects facial\nmovements and non-verbal communication. Patients with PD present a reduction in\nfacial movements called hypomimia which is evaluated in item 3.2 of the\nMDS-UPDRS-III scale. In this work, we propose to use facial expression analysis\nfrom face images based on affective domains to improve PD detection. We propose\ndifferent domain adaptation techniques to exploit the latest advances in face\nrecognition and Face Action Unit (FAU) detection. The principal contributions\nof this work are: (1) a novel framework to exploit deep face architectures to\nmodel hypomimia in PD patients; (2) we experimentally compare PD detection\nbased on single images vs. image sequences while the patients are evoked\nvarious face expressions; (3) we explore different domain adaptation techniques\nto exploit existing models initially trained either for Face Recognition or to\ndetect FAUs for the automatic discrimination between PD patients and healthy\nsubjects; and (4) a new approach to use triplet-loss learning to improve\nhypomimia modeling and PD detection. The results on real face images from PD\npatients show that we are able to properly model evoked emotions using image\nsequences (neutral, onset-transition, apex, offset-transition, and neutral)\nwith accuracy improvements up to 5.5% (from 72.9% to 78.4%) with respect to\nsingle-image PD detection. We also show that our proposed affective-domain\nadaptation provides improvements in PD detection up to 8.9% (from 78.4% to\n87.3% detection accuracy).\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 18:48:53 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Gomez-Gomez", "Luis Felipe", ""], ["Morales", "Aythami", ""], ["Fierrez", "Julian", ""], ["Orozco-Arroyave", "Juan Rafael", ""]]}, {"id": "2012.06567", "submitter": "Yi Zhu", "authors": "Yi Zhu, Xinyu Li, Chunhui Liu, Mohammadreza Zolfaghari, Yuanjun Xiong,\n  Chongruo Wu, Zhi Zhang, Joseph Tighe, R. Manmatha, Mu Li", "title": "A Comprehensive Study of Deep Video Action Recognition", "comments": "Technical report. Code and model zoo can be found at\n  https://cv.gluon.ai/model_zoo/action_recognition.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video action recognition is one of the representative tasks for video\nunderstanding. Over the last decade, we have witnessed great advancements in\nvideo action recognition thanks to the emergence of deep learning. But we also\nencountered new challenges, including modeling long-range temporal information\nin videos, high computation costs, and incomparable results due to datasets and\nevaluation protocol variances. In this paper, we provide a comprehensive survey\nof over 200 existing papers on deep learning for video action recognition. We\nfirst introduce the 17 video action recognition datasets that influenced the\ndesign of models. Then we present video action recognition models in\nchronological order: starting with early attempts at adapting deep learning,\nthen to the two-stream networks, followed by the adoption of 3D convolutional\nkernels, and finally to the recent compute-efficient models. In addition, we\nbenchmark popular methods on several representative datasets and release code\nfor reproducibility. In the end, we discuss open problems and shed light on\nopportunities for video action recognition to facilitate new research ideas.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 18:54:08 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Zhu", "Yi", ""], ["Li", "Xinyu", ""], ["Liu", "Chunhui", ""], ["Zolfaghari", "Mohammadreza", ""], ["Xiong", "Yuanjun", ""], ["Wu", "Chongruo", ""], ["Zhang", "Zhi", ""], ["Tighe", "Joseph", ""], ["Manmatha", "R.", ""], ["Li", "Mu", ""]]}, {"id": "2012.06568", "submitter": "Xuwang Yin", "authors": "Xuwang Yin, Shiying Li, Gustavo K. Rohde", "title": "Analyzing and Improving Generative Adversarial Training for Generative\n  Modeling and Out-of-Distribution Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial training (GAT) is a recently introduced adversarial\ndefense method. Previous works have focused on empirical evaluations of its\napplication to training robust predictive models. In this paper we focus on\ntheoretical understanding of the GAT method and extending its application to\ngenerative modeling and out-of-distribution detection. We analyze the optimal\nsolutions of the maximin formulation employed by the GAT objective, and make a\ncomparative analysis of the minimax formulation employed by GANs. We use\ntheoretical analysis and 2D simulations to understand the convergence property\nof the training algorithm. Based on these results, we develop an incremental\ngenerative training algorithm, and conduct comprehensive evaluations of the\nalgorithm's application to image generation and adversarial out-of-distribution\ndetection. Our results suggest that generative adversarial training is a\npromising new direction for the above applications.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 18:54:34 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Yin", "Xuwang", ""], ["Li", "Shiying", ""], ["Rohde", "Gustavo K.", ""]]}, {"id": "2012.06573", "submitter": "Alexis Marchal", "authors": "Alexis Marchal", "title": "Risk & returns around FOMC press conferences: a novel perspective from\n  computer vision", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I propose a new tool to characterize the resolution of uncertainty around\nFOMC press conferences. It relies on the construction of a measure capturing\nthe level of discussion complexity between the Fed Chair and reporters during\nthe Q&A sessions. I show that complex discussions are associated with higher\nequity returns and a drop in realized volatility. The method creates an\nattention score by quantifying how much the Chair needs to rely on reading\ninternal documents to be able to answer a question. This is accomplished by\nbuilding a novel dataset of video images of the press conferences and\nleveraging recent deep learning algorithms from computer vision. This\nalternative data provides new information on nonverbal communication that\ncannot be extracted from the widely analyzed FOMC transcripts. This paper can\nbe seen as a proof of concept that certain videos contain valuable information\nfor the study of financial markets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 18:59:47 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 16:31:18 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Marchal", "Alexis", ""]]}, {"id": "2012.06575", "submitter": "Robin Chan", "authors": "Robin Chan, Matthias Rottmann, Hanno Gottschalk", "title": "Entropy Maximization and Meta Classification for Out-Of-Distribution\n  Detection in Semantic Segmentation", "comments": "18 pages, 20 figures, ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) for the semantic segmentation of images are\nusually trained to operate on a predefined closed set of object classes. This\nis in contrast to the \"open world\" setting where DNNs are envisioned to be\ndeployed to. From a functional safety point of view, the ability to detect\nso-called \"out-of-distribution\" (OoD) samples, i.e., objects outside of a DNN's\nsemantic space, is crucial for many applications such as automated driving. A\nnatural baseline approach to OoD detection is to threshold on the pixel-wise\nsoftmax entropy. We present a two-step procedure that significantly improves\nthat approach. Firstly, we utilize samples from the COCO dataset as OoD proxy\nand introduce a second training objective to maximize the softmax entropy on\nthese samples. Starting from pretrained semantic segmentation networks we\nre-train a number of DNNs on different in-distribution datasets and\nconsistently observe improved OoD detection performance when evaluating on\ncompletely disjoint OoD datasets. Secondly, we perform a transparent\npost-processing step to discard false positive OoD samples by so-called \"meta\nclassification\". To this end, we apply linear models to a set of hand-crafted\nmetrics derived from the DNN's softmax probabilities. In our experiments we\nconsistently observe a clear additional gain in OoD detection performance,\ncutting down the number of detection errors by up to 52% when comparing the\nbest baseline with our results. We achieve this improvement sacrificing only\nmarginally in original segmentation performance. Therefore, our method\ncontributes to safer DNNs with more reliable overall system performance.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 11:01:06 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 11:37:07 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Chan", "Robin", ""], ["Rottmann", "Matthias", ""], ["Gottschalk", "Hanno", ""]]}, {"id": "2012.06628", "submitter": "Zuoyue Li", "authors": "Zuoyue Li, Zhenqiang Li, Zhaopeng Cui, Rongjun Qin, Marc Pollefeys,\n  Martin R. Oswald", "title": "Sat2Vid: Street-view Panoramic Video Synthesis from a Single Satellite\n  Image", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel method for synthesizing both temporally and geometrically\nconsistent street-view panoramic video from a single satellite image and camera\ntrajectory. Existing cross-view synthesis approaches focus on images, while\nvideo synthesis in such a case has not yet received enough attention. For\ngeometrical and temporal consistency, our approach explicitly creates a 3D\npoint cloud representation of the scene and maintains dense 3D-2D\ncorrespondences across frames that reflect the geometric scene configuration\ninferred from the satellite view. As for synthesis in the 3D space, we\nimplement a cascaded network architecture with two hourglass modules to\ngenerate point-wise coarse and fine features from semantics and per-class\nlatent vectors, followed by projection to frames and an upsampling module to\nobtain the final realistic video. By leveraging computed correspondences, the\nproduced street-view video frames adhere to the 3D geometric scene structure\nand maintain temporal consistency. Qualitative and quantitative experiments\ndemonstrate superior results compared to other state-of-the-art synthesis\napproaches that either lack temporal consistency or realistic appearance. To\nthe best of our knowledge, our work is the first one to synthesize cross-view\nimages to video.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 20:22:38 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 08:43:00 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 03:02:44 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Li", "Zuoyue", ""], ["Li", "Zhenqiang", ""], ["Cui", "Zhaopeng", ""], ["Qin", "Rongjun", ""], ["Pollefeys", "Marc", ""], ["Oswald", "Martin R.", ""]]}, {"id": "2012.06650", "submitter": "Manyi Li", "authors": "Manyi Li, Hao Zhang", "title": "D$^2$IM-Net: Learning Detail Disentangled Implicit Fields from Single\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first single-view 3D reconstruction network aimed at\nrecovering geometric details from an input image which encompass both\ntopological shape structures and surface features. Our key idea is to train the\nnetwork to learn a detail disentangled reconstruction consisting of two\nfunctions, one implicit field representing the coarse 3D shape and the other\ncapturing the details. Given an input image, our network, coined D$^2$IM-Net,\nencodes it into global and local features which are respectively fed into two\ndecoders. The base decoder uses the global features to reconstruct a coarse\nimplicit field, while the detail decoder reconstructs, from the local features,\ntwo displacement maps, defined over the front and back sides of the captured\nobject. The final 3D reconstruction is a fusion between the base shape and the\ndisplacement maps, with three losses enforcing the recovery of coarse shape,\noverall structure, and surface details via a novel Laplacian term.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 21:42:52 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 13:16:00 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Li", "Manyi", ""], ["Zhang", "Hao", ""]]}, {"id": "2012.06711", "submitter": "Meng Rongye", "authors": "Rongye Meng, Sanping Zhou, Xingyu Wan, Mengliu Li, Jinjun Wang", "title": "Teacher-Student Asynchronous Learning with Multi-Source Consistency for\n  Facial Landmark Detection", "comments": "second version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the high annotation cost of large-scale facial landmark detection\ntasks in videos, a semi-supervised paradigm that uses self-training for mining\nhigh-quality pseudo-labels to participate in training has been proposed by\nresearchers. However, self-training based methods often train with a gradually\nincreasing number of samples, whose performances vary a lot depending on the\nnumber of pseudo-labeled samples added.\n  In this paper, we propose a teacher-student asynchronous learning~(TSAL)\nframework based on the multi-source supervision signal consistency criterion,\nwhich implicitly mines pseudo-labels through consistency constraints.\nSpecifically, the TSAL framework contains two models with exactly the same\nstructure. The radical student uses multi-source supervision signals from the\nsame task to update parameters, while the calm teacher uses a single-source\nsupervision signal to update parameters. In order to reasonably absorb\nstudent's suggestions, teacher's parameters are updated again through recursive\naverage filtering. The experimental results prove that asynchronous-learning\nframework can effectively filter noise in multi-source supervision signals,\nthereby mining the pseudo-labels which are more significant for network\nparameter updating. And extensive experiments on 300W, AFLW, and 300VW\nbenchmarks show that the TSAL framework achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 03:23:30 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Meng", "Rongye", ""], ["Zhou", "Sanping", ""], ["Wan", "Xingyu", ""], ["Li", "Mengliu", ""], ["Wang", "Jinjun", ""]]}, {"id": "2012.06718", "submitter": "Gabriel Hope", "authors": "Gabriel Hope, Madina Abdrakhmanova, Xiaoyin Chen, Michael C. Hughes,\n  Michael C. Hughes and Erik B. Sudderth", "title": "Learning Consistent Deep Generative Models from Sparse Data via\n  Prediction Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a new framework for learning variational autoencoders and other\ndeep generative models that balances generative and discriminative goals. Our\nframework optimizes model parameters to maximize a variational lower bound on\nthe likelihood of observed data, subject to a task-specific prediction\nconstraint that prevents model misspecification from leading to inaccurate\npredictions. We further enforce a consistency constraint, derived naturally\nfrom the generative model, that requires predictions on reconstructed data to\nmatch those on the original data. We show that these two contributions --\nprediction constraints and consistency constraints -- lead to promising image\nclassification performance, especially in the semi-supervised scenario where\ncategory labels are sparse but unlabeled data is plentiful. Our approach\nenables advances in generative modeling to directly boost semi-supervised\nclassification performance, an ability we demonstrate by augmenting deep\ngenerative models with latent variables capturing spatial transformations.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 04:18:50 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Hope", "Gabriel", ""], ["Abdrakhmanova", "Madina", ""], ["Chen", "Xiaoyin", ""], ["Hughes", "Michael C.", ""], ["Hughes", "Michael C.", ""], ["Sudderth", "Erik B.", ""]]}, {"id": "2012.06722", "submitter": "Qihang Yu", "authors": "Qihang Yu, Jianming Zhang, He Zhang, Yilin Wang, Zhe Lin, Ning Xu,\n  Yutong Bai, Alan Yuille", "title": "Mask Guided Matting via Progressive Refinement Network", "comments": "CVPR 2021, code available at https://github.com/yucornetto/MGMatting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Mask Guided (MG) Matting, a robust matting framework that takes a\ngeneral coarse mask as guidance. MG Matting leverages a network (PRN) design\nwhich encourages the matting model to provide self-guidance to progressively\nrefine the uncertain regions through the decoding process. A series of guidance\nmask perturbation operations are also introduced in the training to further\nenhance its robustness to external guidance. We show that PRN can generalize to\nunseen types of guidance masks such as trimap and low-quality alpha matte,\nmaking it suitable for various application pipelines. In addition, we revisit\nthe foreground color prediction problem for matting and propose a surprisingly\nsimple improvement to address the dataset issue. Evaluation on real and\nsynthetic benchmarks shows that MG Matting achieves state-of-the-art\nperformance using various types of guidance inputs. Code and models are\navailable at https://github.com/yucornetto/MGMatting.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 04:26:14 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 00:57:47 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Yu", "Qihang", ""], ["Zhang", "Jianming", ""], ["Zhang", "He", ""], ["Wang", "Yilin", ""], ["Lin", "Zhe", ""], ["Xu", "Ning", ""], ["Bai", "Yutong", ""], ["Yuille", "Alan", ""]]}, {"id": "2012.06734", "submitter": "Yuliang Guo", "authors": "Yuliang Guo, Zhong Li, Zekun Li, Xiangyu Du, Shuxue Quan, Yi Xu", "title": "PoP-Net: Pose over Parts Network for Multi-Person 3D Pose Estimation\n  from a Depth Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, a real-time method called PoP-Net is proposed to predict\nmulti-person 3D poses from a depth image. PoP-Net learns to predict bottom-up\npart detection maps and top-down global poses in a single-shot framework. A\nsimple and effective fusion process is applied to fuse the global poses and\npart detection. Specifically, a new part-level representation, called Truncated\nPart Displacement Field (TPDF), is introduced. It drags low-precision global\nposes towards more accurate part locations while maintaining the advantage of\nglobal poses in handling severe occlusion and truncation cases. A mode\nselection scheme is developed to automatically resolve the conflict between\nglobal poses and local detection. Finally, due to the lack of high-quality\ndepth datasets for developing and evaluating multi-person 3D pose estimation\nmethods, a comprehensive depth dataset with 3D pose labels is released. The\ndataset is designed to enable effective multi-person and background data\naugmentation such that the developed models are more generalizable towards\nuncontrolled real-world multi-person scenarios. We show that PoP-Net has\nsignificant advantages in efficiency for multi-person processing and achieves\nthe state-of-the-art results both on the released challenging dataset and on\nthe widely used ITOP dataset.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 05:32:25 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Guo", "Yuliang", ""], ["Li", "Zhong", ""], ["Li", "Zekun", ""], ["Du", "Xiangyu", ""], ["Quan", "Shuxue", ""], ["Xu", "Yi", ""]]}, {"id": "2012.06735", "submitter": "Yu Yin", "authors": "Yu Yin, Joseph P. Robinson, Yun Fu", "title": "Multimodal In-bed Pose and Shape Estimation under the Blankets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans spend vast hours in bed -- about one-third of the lifetime on average.\nBesides, a human at rest is vital in many healthcare applications. Typically,\nhumans are covered by a blanket when resting, for which we propose a multimodal\napproach to uncover the subjects so their bodies at rest can be viewed without\nthe occlusion of the blankets above. We propose a pyramid scheme to effectively\nfuse the different modalities in a way that best leverages the knowledge\ncaptured by the multimodal sensors. Specifically, the two most informative\nmodalities (i.e., depth and infrared images) are first fused to generate good\ninitial pose and shape estimation. Then pressure map and RGB images are further\nfused one by one to refine the result by providing occlusion-invariant\ninformation for the covered part, and accurate shape information for the\nuncovered part, respectively. However, even with multimodal data, the task of\ndetecting human bodies at rest is still very challenging due to the extreme\nocclusion of bodies. To further reduce the negative effects of the occlusion\nfrom blankets, we employ an attention-based reconstruction module to generate\nuncovered modalities, which are further fused to update current estimation via\na cyclic fashion. Extensive experiments validate the superiority of the\nproposed model over others.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 05:35:23 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Yin", "Yu", ""], ["Robinson", "Joseph P.", ""], ["Fu", "Yun", ""]]}, {"id": "2012.06737", "submitter": "Zijian Kuang", "authors": "Zijian Kuang and Xinran Tie", "title": "Computer Vision and Normalizing Flow Based Defect Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface defect detection is essential and necessary for controlling the\nqualities of the products during manufacturing. The challenges in this complex\ntask include: 1) collecting defective samples and manually labeling for\ntraining is time-consuming; 2) the defects' characteristics are difficult to\ndefine as new types of defect can happen all the time; 3) and the real-world\nproduct images contain lots of background noise. In this paper, we present a\ntwo-stage defect detection network based on the object detection model YOLO,\nand the normalizing flow-based defect detection model DifferNet. Our model has\nhigh robustness and performance on defect detection using real-world video\nclips taken from a production line monitoring system. The normalizing\nflow-based anomaly detection model only requires a small number of good samples\nfor training and then perform defect detection on the product images detected\nby YOLO. The model we invent employs two novel strategies: 1) a two-stage\nnetwork using YOLO and a normalizing flow-based model to perform product defect\ndetection, 2) multi-scale image transformations are implemented to solve the\nissue product image cropped by YOLO includes many background noise. Besides,\nextensive experiments are conducted on a new dataset collected from the\nreal-world factory production line. We demonstrate that our proposed model can\nlearn on a small number of defect-free samples of single or multiple product\ntypes. The dataset will also be made public to encourage further studies and\nresearch in surface defect detection.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 05:38:21 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Kuang", "Zijian", ""], ["Tie", "Xinran", ""]]}, {"id": "2012.06739", "submitter": "Sandeep Chinchali", "authors": "Sandeep Chinchali, Evgenya Pergament, Manabu Nakanoya, Eyal Cidon,\n  Edward Zhang, Dinesh Bharadia, Marco Pavone, and Sachin Katti", "title": "Sampling Training Data for Continual Learning Between Robots and the\n  Cloud", "comments": "International Symposium on Experimental Robotics (ISER) 2020, Malta", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.DC cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today's robotic fleets are increasingly measuring high-volume video and LIDAR\nsensory streams, which can be mined for valuable training data, such as rare\nscenes of road construction sites, to steadily improve robotic perception\nmodels. However, re-training perception models on growing volumes of rich\nsensory data in central compute servers (or the \"cloud\") places an enormous\ntime and cost burden on network transfer, cloud storage, human annotation, and\ncloud computing resources. Hence, we introduce HarvestNet, an intelligent\nsampling algorithm that resides on-board a robot and reduces system bottlenecks\nby only storing rare, useful events to steadily improve perception models\nre-trained in the cloud. HarvestNet significantly improves the accuracy of\nmachine-learning models on our novel dataset of road construction sites, field\ntesting of self-driving cars, and streaming face recognition, while reducing\ncloud storage, dataset annotation time, and cloud compute time by between\n65.7-81.3%. Further, it is between 1.05-2.58x more accurate than baseline\nalgorithms and scalably runs on embedded deep learning hardware. We provide a\nsuite of compute-efficient perception models for the Google Edge Tensor\nProcessing Unit (TPU), an extended technical report, and a novel video dataset\nto the research community at https://sites.google.com/view/harvestnet.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 05:52:33 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Chinchali", "Sandeep", ""], ["Pergament", "Evgenya", ""], ["Nakanoya", "Manabu", ""], ["Cidon", "Eyal", ""], ["Zhang", "Edward", ""], ["Bharadia", "Dinesh", ""], ["Pavone", "Marco", ""], ["Katti", "Sachin", ""]]}, {"id": "2012.06746", "submitter": "Yoon Gyo Jung", "authors": "Yoon Gyo Jung, Jaewoo Park, Cheng Yaw Low, Leslie Ching Ow Tiong,\n  Andrew Beng Jin Teoh", "title": "Periocular in the Wild Embedding Learning with Cross-Modal Consistent\n  Knowledge Distillation", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Periocular biometric, or peripheral area of ocular, is a collaborative\nalternative to face, especially if a face is occluded or masked. In practice,\nsole periocular biometric captures least salient facial features, thereby\nsuffering from intra-class compactness and inter-class dispersion issues\nparticularly in the wild environment. To address these problems, we transfer\nuseful information from face to support periocular modality by means of\nknowledge distillation (KD) for embedding learning. However, applying typical\nKD techniques to heterogeneous modalities directly is suboptimal. We put\nforward in this paper a deep face-to-periocular distillation networks, coined\nas cross-modal consistent knowledge distillation (CM-CKD) henceforward. The\nthree key ingredients of CM-CKD are (1) shared-weight networks, (2) consistent\nbatch normalization, and (3) a bidirectional consistency distillation for face\nand periocular through an effectual CKD loss. To be more specific, we leverage\nface modality for periocular embedding learning, but only periocular images are\ntargeted for identification or verification tasks. Extensive experiments on six\nconstrained and unconstrained periocular datasets disclose that the\nCM-CKD-learned periocular embeddings extend identification and verification\nperformance by 50% in terms of relative performance gain computed based upon\nface and periocular baselines. The experiments also reveal that the\nCM-CKD-learned periocular features enjoy better subject-wise cluster\nseparation, thereby refining the overall accuracy performance.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 07:12:21 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Jung", "Yoon Gyo", ""], ["Park", "Jaewoo", ""], ["Low", "Cheng Yaw", ""], ["Tiong", "Leslie Ching Ow", ""], ["Teoh", "Andrew Beng Jin", ""]]}, {"id": "2012.06760", "submitter": "Parvez Ahmad", "authors": "Saqib Qamar, Parvez Ahmad, Linlin Shen", "title": "HI-Net: Hyperdense Inception 3D UNet for Brain Tumor Segmentation", "comments": "Accepted for MICCAI BraTS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The brain tumor segmentation task aims to classify tissue into the whole\ntumor (WT), tumor core (TC), and enhancing tumor (ET) classes using multimodel\nMRI images. Quantitative analysis of brain tumors is critical for clinical\ndecision making. While manual segmentation is tedious, time-consuming, and\nsubjective, this task is at the same time very challenging to automatic\nsegmentation methods. Thanks to the powerful learning ability, convolutional\nneural networks (CNNs), mainly fully convolutional networks, have shown\npromising brain tumor segmentation. This paper further boosts the performance\nof brain tumor segmentation by proposing hyperdense inception 3D UNet (HI-Net),\nwhich captures multi-scale information by stacking factorization of 3D weighted\nconvolutional layers in the residual inception block. We use hyper dense\nconnections among factorized convolutional layers to extract more contexual\ninformation, with the help of features reusability. We use a dice loss function\nto cope with class imbalances. We validate the proposed architecture on the\nmulti-modal brain tumor segmentation challenges (BRATS) 2020 testing dataset.\nPreliminary results on the BRATS 2020 testing set show that achieved by our\nproposed approach, the dice (DSC) scores of ET, WT, and TC are 0.79457,\n0.87494, and 0.83712, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 09:09:04 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Qamar", "Saqib", ""], ["Ahmad", "Parvez", ""], ["Shen", "Linlin", ""]]}, {"id": "2012.06765", "submitter": "Sergio Naval Marimont", "authors": "Sergio Naval Marimont and Giacomo Tarroni", "title": "Anomaly detection through latent space restoration using\n  vector-quantized variational autoencoders", "comments": "4 Pages, 4 Figures. Submitted to ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an out-of-distribution detection method that combines density and\nrestoration-based approaches using Vector-Quantized Variational Auto-Encoders\n(VQ-VAEs). The VQ-VAE model learns to encode images in a categorical latent\nspace. The prior distribution of latent codes is then modelled using an\nAuto-Regressive (AR) model. We found that the prior probability estimated by\nthe AR model can be useful for unsupervised anomaly detection and enables the\nestimation of both sample and pixel-wise anomaly scores. The sample-wise score\nis defined as the negative log-likelihood of the latent variables above a\nthreshold selecting highly unlikely codes. Additionally, out-of-distribution\nimages are restored into in-distribution images by replacing unlikely latent\ncodes with samples from the prior model and decoding to pixel space. The\naverage L1 distance between generated restorations and original image is used\nas pixel-wise anomaly score. We tested our approach on the MOOD challenge\ndatasets, and report higher accuracies compared to a standard\nreconstruction-based approach with VAEs.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 09:19:59 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Marimont", "Sergio Naval", ""], ["Tarroni", "Giacomo", ""]]}, {"id": "2012.06769", "submitter": "Radu P Horaud", "authors": "Georgios D. Evangelidis, Miles Hansard, and Radu Horaud", "title": "Fusion of Range and Stereo Data for High-Resolution Scene-Modeling", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  37(11), 2015", "doi": "10.1109/TPAMI.2015.2400465", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the problem of range-stereo fusion, for the construction\nof high-resolution depth maps. In particular, we combine low-resolution depth\ndata with high-resolution stereo data, in a maximum a posteriori (MAP)\nformulation. Unlike existing schemes that build on MRF optimizers, we infer the\ndisparity map from a series of local energy minimization problems that are\nsolved hierarchically, by growing sparse initial disparities obtained from the\ndepth data. The accuracy of the method is not compromised, owing to three\nproperties of the data-term in the energy function. Firstly, it incorporates a\nnew correlation function that is capable of providing refined correlations and\ndisparities, via subpixel correction. Secondly, the correlation scores rely on\nan adaptive cost aggregation step, based on the depth data. Thirdly, the stereo\nand depth likelihoods are adaptively fused, based on the scene texture and\ncamera geometry. These properties lead to a more selective growing process\nwhich, unlike previous seed-growing methods, avoids the tendency to propagate\nincorrect disparities. The proposed method gives rise to an intrinsically\nefficient algorithm, which runs at 3FPS on 2.0MP images on a standard desktop\ncomputer. The strong performance of the new method is established both by\nquantitative comparisons with state-of-the-art methods, and by qualitative\ncomparisons using real depth-stereo data-sets.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 09:37:42 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Evangelidis", "Georgios D.", ""], ["Hansard", "Miles", ""], ["Horaud", "Radu", ""]]}, {"id": "2012.06771", "submitter": "Awadelrahman M. A. Ahmed Mr.", "authors": "Awadelrahman M. A. Ali Ahmed (University of Oslo)", "title": "Generative Adversarial Networks for Automatic Polyp Segmentation", "comments": "MediaEval20, Multimedia Evaluation Workshop, December 14-15 2020,\n  Online", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper aims to contribute in bench-marking the automatic polyp\nsegmentation problem using generative adversarial networks framework.\nPerceiving the problem as an image-to-image translation task, conditional\ngenerative adversarial networks are utilized to generate masks conditioned by\nthe images as inputs. Both generator and discriminator are convolution neural\nnetworks based. The model achieved 0.4382 on Jaccard index and 0.611 as F2\nscore.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 09:48:08 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Ahmed", "Awadelrahman M. A. Ali", "", "University of Oslo"]]}, {"id": "2012.06772", "submitter": "Radu P Horaud", "authors": "Radu Horaud, Miles Hansard, Georgios Evangelidis and Clement Menier", "title": "An Overview of Depth Cameras and Range Scanners Based on Time-of-Flight\n  Technologies", "comments": null, "journal-ref": "Machine Vision and Applications, 27(7), 2016", "doi": "10.1007/s00138-016-0784-4", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time-of-flight (TOF) cameras are sensors that can measure the depths of\nscene-points, by illuminating the scene with a controlled laser or LED source,\nand then analyzing the reflected light. In this paper, we will first describe\nthe underlying measurement principles of time-of-flight cameras, including: (i)\npulsed-light cameras, which measure directly the time taken for a light pulse\nto travel from the device to the object and back again, and (ii)\ncontinuous-wave modulated-light cameras, which measure the phase difference\nbetween the emitted and received signals, and hence obtain the travel time\nindirectly. We review the main existing designs, including prototypes as well\nas commercially available devices. We also review the relevant camera\ncalibration principles, and how they are applied to TOF devices. Finally, we\ndiscuss the benefits and challenges of combined TOF and color camera systems.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 09:48:52 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Horaud", "Radu", ""], ["Hansard", "Miles", ""], ["Evangelidis", "Georgios", ""], ["Menier", "Clement", ""]]}, {"id": "2012.06777", "submitter": "Dr. Suryansh Kumar", "authors": "Berk Kaya, Suryansh Kumar, Carlos Oliveira, Vittorio Ferrari, Luc Van\n  Gool", "title": "Uncalibrated Neural Inverse Rendering for Photometric Stereo of General\n  Surfaces", "comments": "Accepted for publication at CVPR 2021. Document info: 18 pages, 21\n  Figures, 5 tables. (Minor typo corrected)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an uncalibrated deep neural network framework for the\nphotometric stereo problem. For training models to solve the problem, existing\nneural network-based methods either require exact light directions or\nground-truth surface normals of the object or both. However, in practice, it is\nchallenging to procure both of this information precisely, which restricts the\nbroader adoption of photometric stereo algorithms for vision application. To\nbypass this difficulty, we propose an uncalibrated neural inverse rendering\napproach to this problem. Our method first estimates the light directions from\nthe input images and then optimizes an image reconstruction loss to calculate\nthe surface normals, bidirectional reflectance distribution function value, and\ndepth. Additionally, our formulation explicitly models the concave and convex\nparts of a complex surface to consider the effects of interreflections in the\nimage formation process. Extensive evaluation of the proposed method on the\nchallenging subjects generally shows comparable or better results than the\nsupervised and classical approaches.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 10:33:08 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 13:26:46 GMT"}, {"version": "v3", "created": "Sat, 17 Apr 2021 22:10:57 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kaya", "Berk", ""], ["Kumar", "Suryansh", ""], ["Oliveira", "Carlos", ""], ["Ferrari", "Vittorio", ""], ["Van Gool", "Luc", ""]]}, {"id": "2012.06785", "submitter": "Matthieu Lin", "authors": "Matthieu Lin and Chuming Li and Xingyuan Bu and Ming Sun and Chen Lin\n  and Junjie Yan and Wanli Ouyang and Zhidong Deng", "title": "DETR for Crowd Pedestrian Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pedestrian detection in crowd scenes poses a challenging problem due to the\nheuristic defined mapping from anchors to pedestrians and the conflict between\nNMS and highly overlapped pedestrians. The recently proposed end-to-end\ndetectors(ED), DETR and deformable DETR, replace hand designed components such\nas NMS and anchors using the transformer architecture, which gets rid of\nduplicate predictions by computing all pairwise interactions between queries.\nInspired by these works, we explore their performance on crowd pedestrian\ndetection. Surprisingly, compared to Faster-RCNN with FPN, the results are\nopposite to those obtained on COCO. Furthermore, the bipartite match of ED\nharms the training efficiency due to the large ground truth number in crowd\nscenes. In this work, we identify the underlying motives driving ED's poor\nperformance and propose a new decoder to address them. Moreover, we design a\nmechanism to leverage the less occluded visible parts of pedestrian\nspecifically for ED, and achieve further improvements. A faster bipartite match\nalgorithm is also introduced to make ED training on crowd dataset more\npractical. The proposed detector PED(Pedestrian End-to-end Detector)\noutperforms both previous EDs and the baseline Faster-RCNN on CityPersons and\nCrowdHuman. It also achieves comparable performance with state-of-the-art\npedestrian detection methods. Code will be released soon.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 11:02:05 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 06:30:10 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 09:46:22 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Lin", "Matthieu", ""], ["Li", "Chuming", ""], ["Bu", "Xingyuan", ""], ["Sun", "Ming", ""], ["Lin", "Chen", ""], ["Yan", "Junjie", ""], ["Ouyang", "Wanli", ""], ["Deng", "Zhidong", ""]]}, {"id": "2012.06789", "submitter": "Saisubramaniam Gopalakrishnan", "authors": "Saisubramaniam Gopalakrishnan, Pranshu Ranjan Singh, Haytham Fayek,\n  Savitha Ramasamy, Arulmurugan Ambikapathi", "title": "Knowledge Capture and Replay for Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have shown promise in several domains, and the learned\ndata (task) specific information is implicitly stored in the network\nparameters. Extraction and utilization of encoded knowledge representations are\nvital when data is no longer available in the future, especially in a continual\nlearning scenario. In this work, we introduce {\\em flashcards}, which are\nvisual representations that {\\em capture} the encoded knowledge of a network as\na recursive function of predefined random image patterns. In a continual\nlearning scenario, flashcards help to prevent catastrophic forgetting and\nconsolidating knowledge of all the previous tasks. Flashcards need to be\nconstructed only before learning the subsequent task, and hence, independent of\nthe number of tasks trained before. We demonstrate the efficacy of flashcards\nin capturing learned knowledge representation (as an alternative to the\noriginal dataset) and empirically validate on a variety of continual learning\ntasks: reconstruction, denoising, task-incremental learning, and new-instance\nlearning classification, using several heterogeneous benchmark datasets.\nExperimental evidence indicates that: (i) flashcards as a replay strategy is {\n\\em task agnostic}, (ii) performs better than generative replay, and (iii) is\non par with episodic replay without additional memory overhead.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 11:24:45 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 14:17:52 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Gopalakrishnan", "Saisubramaniam", ""], ["Singh", "Pranshu Ranjan", ""], ["Fayek", "Haytham", ""], ["Ramasamy", "Savitha", ""], ["Ambikapathi", "Arulmurugan", ""]]}, {"id": "2012.06793", "submitter": "Deng Weijian", "authors": "Weijian Deng, Joshua Marsh, Stephen Gould, Liang Zheng", "title": "Fine-grained Classification via Categorical Memory Networks", "comments": "10 pages, 9 figures, 7 tables; this version is not fully edited and\n  will be updated soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the desire to exploit patterns shared across classes, we present\na simple yet effective class-specific memory module for fine-grained feature\nlearning. The memory module stores the prototypical feature representation for\neach category as a moving average. We hypothesize that the combination of\nsimilarities with respect to each category is itself a useful discriminative\ncue. To detect these similarities, we use attention as a querying mechanism.\nThe attention scores with respect to each class prototype are used as weights\nto combine prototypes via weighted sum, producing a uniquely tailored response\nfeature representation for a given input. The original and response features\nare combined to produce an augmented feature for classification. We integrate\nour class-specific memory module into a standard convolutional neural network,\nyielding a Categorical Memory Network. Our memory module significantly improves\naccuracy over baseline CNNs, achieving competitive accuracy with\nstate-of-the-art methods on four benchmarks, including CUB-200-2011, Stanford\nCars, FGVC Aircraft, and NABirds.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 11:50:13 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Deng", "Weijian", ""], ["Marsh", "Joshua", ""], ["Gould", "Stephen", ""], ["Zheng", "Liang", ""]]}, {"id": "2012.06800", "submitter": "Srinivas Anumasa", "authors": "Srinivas Anumasa, P.K. Srijith", "title": "Delay Differential Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural ordinary differential equations (NODEs) treat computation of\nintermediate feature vectors as trajectories of ordinary differential equation\nparameterized by a neural network. In this paper, we propose a novel model,\ndelay differential neural networks (DDNN), inspired by delay differential\nequations (DDEs). The proposed model considers the derivative of the hidden\nfeature vector as a function of the current feature vector and past feature\nvectors (history). The function is modelled as a neural network and\nconsequently, it leads to continuous depth alternatives to many recent ResNet\nvariants. We propose two different DDNN architectures, depending on the way\ncurrent and past feature vectors are considered. For training DDNNs, we provide\na memory-efficient adjoint method for computing gradients and back-propagate\nthrough the network. DDNN improves the data efficiency of NODE by further\nreducing the number of parameters without affecting the generalization\nperformance. Experiments conducted on synthetic and real-world image\nclassification datasets such as Cifar10 and Cifar100 show the effectiveness of\nthe proposed models.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 12:20:54 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Anumasa", "Srinivas", ""], ["Srijith", "P. K.", ""]]}, {"id": "2012.06815", "submitter": "Xinyu Zhang", "authors": "Bin Yan, Xinyu Zhang, Dong Wang, Huchuan Lu, Xiaoyun Yang", "title": "Alpha-Refine: Boosting Tracking Performance by Precise Bounding Box\n  Estimation", "comments": "arXiv admin note: Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual object tracking aims to precisely estimate the bounding box for the\ngiven target, which is a challenging problem due to factors such as deformation\nand occlusion. Many recent trackers adopt the multiple-stage tracking strategy\nto improve the quality of bounding box estimation. These methods first coarsely\nlocate the target and then refine the initial prediction in the following\nstages. However, existing approaches still suffer from limited precision, and\nthe coupling of different stages severely restricts the method's\ntransferability. This work proposes a novel, flexible, and accurate refinement\nmodule called Alpha-Refine (AR), which can significantly improve the base\ntrackers' box estimation quality. By exploring a series of design options, we\nconclude that the key to successful refinement is extracting and maintaining\ndetailed spatial information as much as possible. Following this principle,\nAlpha-Refine adopts a pixel-wise correlation, a corner prediction head, and an\nauxiliary mask head as the core components. Comprehensive experiments on\nTrackingNet, LaSOT, GOT-10K, and VOT2020 benchmarks with multiple base trackers\nshow that our approach significantly improves the base trackers' performance\nwith little extra latency. The proposed Alpha-Refine method leads to a series\nof strengthened trackers, among which the ARSiamRPN (AR strengthened SiamRPNpp)\nand the ARDiMP50 (ARstrengthened DiMP50) achieve good efficiency-precision\ntrade-off, while the ARDiMPsuper (AR strengthened DiMP-super) achieves very\ncompetitive performance at a real-time speed. Code and pretrained models are\navailable at https://github.com/MasterBin-IIAU/AlphaRefine.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 13:33:25 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 03:53:00 GMT"}, {"version": "v3", "created": "Sun, 11 Jul 2021 07:37:19 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Yan", "Bin", ""], ["Zhang", "Xinyu", ""], ["Wang", "Dong", ""], ["Lu", "Huchuan", ""], ["Yang", "Xiaoyun", ""]]}, {"id": "2012.06838", "submitter": "Almabrok Essa", "authors": "Almabrok Essa and Vijayan Asari", "title": "High Order Local Directional Pattern Based Pyramidal Multi-structure for\n  Robust Face Recognition", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Derived from a general definition of texture in a local neighborhood, local\ndirectional pattern (LDP) encodes the directional information in the small\nlocal 3x3 neighborhood of a pixel, which may fail to extract detailed\ninformation especially during changes in the input image due to illumination\nvariations. Therefore, in this paper we introduce a novel feature extraction\ntechnique that calculates the nth order direction variation patterns, named\nhigh order local directional pattern (HOLDP). The proposed HOLDP can capture\nmore detailed discriminative information than the conventional LDP. Unlike the\nLDP operator, our proposed technique extracts nth order local information by\nencoding various distinctive spatial relationships from each neighborhood layer\nof a pixel in the pyramidal multi-structure way. Then we concatenate the\nfeature vector of each neighborhood layer to form the final HOLDP feature\nvector. The performance evaluation of the proposed HOLDP algorithm is conducted\non several publicly available face databases and observed the superiority of\nHOLDP under extreme illumination conditions.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 15:13:07 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Essa", "Almabrok", ""], ["Asari", "Vijayan", ""]]}, {"id": "2012.06843", "submitter": "Can Zhang", "authors": "Can Zhang, Hong Liu, Wei Guo, Mang Ye", "title": "Multi-Scale Cascading Network with Compact Feature Learning for\n  RGB-Infrared Person Re-Identification", "comments": "8 pages, 5 figures, ICPR2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB-Infrared person re-identification (RGB-IR Re-ID) aims to match persons\nfrom heterogeneous images captured by visible and thermal cameras, which is of\ngreat significance in the surveillance system under poor light conditions.\nFacing great challenges in complex variances including conventional\nsingle-modality and additional inter-modality discrepancies, most of the\nexisting RGB-IR Re-ID methods propose to impose constraints in image level,\nfeature level or a hybrid of both. Despite the better performance of hybrid\nconstraints, they are usually implemented with heavy network architecture. As a\nmatter of fact, previous efforts contribute more as pioneering works in new\ncross-modal Re-ID area while leaving large space for improvement. This can be\nmainly attributed to: (1) lack of abundant person image pairs from different\nmodalities for training, and (2) scarcity of salient modality-invariant\nfeatures especially on coarse representations for effective matching. To\naddress these issues, a novel Multi-Scale Part-Aware Cascading framework\n(MSPAC) is formulated by aggregating multi-scale fine-grained features from\npart to global in a cascading manner, which results in a unified representation\ncontaining rich and enhanced semantic features. Furthermore, a marginal\nexponential centre (MeCen) loss is introduced to jointly eliminate mixed\nvariances from intra- and inter-modal examples. Cross-modality correlations can\nthus be efficiently explored on salient features for distinctive\nmodality-invariant feature learning. Extensive experiments are conducted to\ndemonstrate that the proposed method outperforms all the state-of-the-art by a\nlarge margin.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 15:39:11 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zhang", "Can", ""], ["Liu", "Hong", ""], ["Guo", "Wei", ""], ["Ye", "Mang", ""]]}, {"id": "2012.06858", "submitter": "David Mallas\\'en Quintana", "authors": "David Mallas\\'en Quintana, Alberto Antonio del Barrio Garc\\'ia and\n  Manuel Prieto Mat\\'ias", "title": "LiveChess2FEN: a Framework for Classifying Chess Pieces based on CNNs", "comments": "The complete source code of the LiveChess2FEN framework is publicly\n  available with an open-source license in our GitHub repository:\n  https://github.com/davidmallasen/LiveChess2FEN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic digitization of chess games using computer vision is a significant\ntechnological challenge. This problem is of much interest for tournament\norganizers and amateur or professional players to broadcast their\nover-the-board (OTB) games online or analyze them using chess engines. Previous\nwork has shown promising results, but the recognition accuracy and the latency\nof state-of-the-art techniques still need further enhancements to allow their\npractical and affordable deployment. We have investigated how to implement them\non an Nvidia Jetson Nano single-board computer effectively. Our first\ncontribution has been accelerating the chessboard's detection algorithm.\nSubsequently, we have analyzed different Convolutional Neural Networks for\nchess piece classification and how to map them efficiently on our embedded\nplatform. Notably, we have implemented a functional framework that\nautomatically digitizes a chess position from an image in less than 1 second,\nwith 92% accuracy when classifying the pieces and 95% when detecting the board.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 16:48:40 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Quintana", "David Mallas\u00e9n", ""], ["Garc\u00eda", "Alberto Antonio del Barrio", ""], ["Mat\u00edas", "Manuel Prieto", ""]]}, {"id": "2012.06859", "submitter": "Savas Ozkan", "authors": "Savas Ozkan, Gozde Bozdagi Akar", "title": "Spectral Unmixing With Multinomial Mixture Kernel and Wasserstein\n  Generative Adversarial Loss", "comments": "AI for Earth Sciences Workshop at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study proposes a novel framework for spectral unmixing by using 1D\nconvolution kernels and spectral uncertainty. High-level representations are\ncomputed from data, and they are further modeled with the Multinomial Mixture\nModel to estimate fractions under severe spectral uncertainty. Furthermore, a\nnew trainable uncertainty term based on a nonlinear neural network model is\nintroduced in the reconstruction step. All uncertainty models are optimized by\nWasserstein Generative Adversarial Network (WGAN) to improve stability and\ncapture uncertainty. Experiments are performed on both real and synthetic\ndatasets. The results validate that the proposed method obtains\nstate-of-the-art performance, especially for the real datasets compared to the\nbaselines. Project page at: https://github.com/savasozkan/dscn.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 16:49:01 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Ozkan", "Savas", ""], ["Akar", "Gozde Bozdagi", ""]]}, {"id": "2012.06873", "submitter": "Chun-Hung Chao", "authors": "Chun-Hung Chao, Hsien-Tzu Cheng, Tsung-Ying Ho, Le Lu, and Min Sun", "title": "Interactive Radiotherapy Target Delineation with 3D-Fused Context\n  Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Gross tumor volume (GTV) delineation on tomography medical imaging is crucial\nfor radiotherapy planning and cancer diagnosis. Convolutional neural networks\n(CNNs) has been predominated on automatic 3D medical segmentation tasks,\nincluding contouring the radiotherapy target given 3D CT volume. While CNNs may\nprovide feasible outcome, in clinical scenario, double-check and prediction\nrefinement by experts is still necessary because of CNNs' inconsistent\nperformance on unexpected patient cases. To provide experts an efficient way to\nmodify the CNN predictions without retrain the model, we propose 3D-fused\ncontext propagation, which propagates any edited slice to the whole 3D volume.\nBy considering the high-level feature maps, the radiation oncologists would\nonly required to edit few slices to guide the correction and refine the whole\nprediction volume. Specifically, we leverage the backpropagation for activation\ntechnique to convey the user editing information backwardly to the latent space\nand generate new prediction based on the updated and original feature. During\nthe interaction, our proposed approach reuses the extant extracted features and\ndoes not alter the existing 3D CNN model architectures, avoiding the\nperturbation on other predictions. The proposed method is evaluated on two\npublished radiotherapy target contouring datasets of nasopharyngeal and\nesophageal cancer. The experimental results demonstrate that our proposed\nmethod is able to further effectively improve the existing segmentation\nprediction from different model architectures given oncologists' interactive\ninputs.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 17:46:20 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Chao", "Chun-Hung", ""], ["Cheng", "Hsien-Tzu", ""], ["Ho", "Tsung-Ying", ""], ["Lu", "Le", ""], ["Sun", "Min", ""]]}, {"id": "2012.06875", "submitter": "Jianan Chen", "authors": "Jianan Chen, Helen M. C. Cheung, Laurent Milot, Anne L. Martel", "title": "AMINN: Autoencoder-based Multiple Instance Neural Network Improves\n  Outcome Prediction of Multifocal Liver Metastases", "comments": "Early accepted by MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Colorectal cancer is one of the most common and lethal cancers and colorectal\ncancer liver metastases (CRLM) is the major cause of death in patients with\ncolorectal cancer. Multifocality occurs frequently in CRLM, but is relatively\nunexplored in CRLM outcome prediction. Most existing clinical and imaging\nbiomarkers do not take the imaging features of all multifocal lesions into\naccount. In this paper, we present an end-to-end autoencoder-based multiple\ninstance neural network (AMINN) for the prediction of survival outcomes in\nmultifocal CRLM patients using radiomic features extracted from\ncontrast-enhanced MRIs. Specifically, we jointly train an autoencoder to\nreconstruct input features and a multiple instance network to make predictions\nby aggregating information from all tumour lesions of a patient. Also, we\nincorporate a two-step normalization technique to improve the training of deep\nneural networks, built on the observation that the distributions of radiomic\nfeatures are almost always severely skewed. Experimental results empirically\nvalidated our hypothesis that incorporating imaging features of all lesions\nimproves outcome prediction for multifocal cancer. The proposed AMINN framework\nachieved an area under the ROC curve (AUC) of 0.70, which is 11.4% higher than\nthe best baseline method. A risk score based on the outputs of AMINN achieved\nsuperior prediction in our multifocal CRLM cohort. The effectiveness of\nincorporating all lesions and applying two-step normalization is demonstrated\nby a series of ablation studies. A Keras implementation of AMINN is released.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 17:52:14 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 21:04:00 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chen", "Jianan", ""], ["Cheung", "Helen M. C.", ""], ["Milot", "Laurent", ""], ["Martel", "Anne L.", ""]]}, {"id": "2012.06907", "submitter": "Wang Zhou", "authors": "Wang Zhou, Levente J. Klein, Siyuan Lu", "title": "PAIRS AutoGeo: an Automated Machine Learning Framework for Massive\n  Geospatial Data", "comments": null, "journal-ref": "IEEE International Conference on Big Data (IEEE BigData 2020)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An automated machine learning framework for geospatial data named PAIRS\nAutoGeo is introduced on IBM PAIRS Geoscope big data and analytics platform.\nThe framework simplifies the development of industrial machine learning\nsolutions leveraging geospatial data to the extent that the user inputs are\nminimized to merely a text file containing labeled GPS coordinates. PAIRS\nAutoGeo automatically gathers required data at the location coordinates,\nassembles the training data, performs quality check, and trains multiple\nmachine learning models for subsequent deployment. The framework is validated\nusing a realistic industrial use case of tree species classification.\nOpen-source tree species data are used as the input to train a random forest\nclassifier and a modified ResNet model for 10-way tree species classification\nbased on aerial imagery, which leads to an accuracy of $59.8\\%$ and $81.4\\%$,\nrespectively. This use case exemplifies how PAIRS AutoGeo enables users to\nleverage machine learning without extensive geospatial expertise.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 21:12:41 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zhou", "Wang", ""], ["Klein", "Levente J.", ""], ["Lu", "Siyuan", ""]]}, {"id": "2012.06908", "submitter": "Tianlong Chen", "authors": "Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang,\n  Michael Carbin, Zhangyang Wang", "title": "The Lottery Tickets Hypothesis for Supervised and Self-supervised\n  Pre-training in Computer Vision Models", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computer vision world has been re-gaining enthusiasm in various\npre-trained models, including both classical ImageNet supervised pre-training\nand recently emerged self-supervised pre-training such as simCLR and MoCo.\nPre-trained weights often boost a wide range of downstream tasks including\nclassification, detection, and segmentation. Latest studies suggest that\npre-training benefits from gigantic model capacity. We are hereby curious and\nask: after pre-training, does a pre-trained model indeed have to stay large for\nits downstream transferability?\n  In this paper, we examine supervised and self-supervised pre-trained models\nthrough the lens of the lottery ticket hypothesis (LTH). LTH identifies highly\nsparse matching subnetworks that can be trained in isolation from (nearly)\nscratch yet still reach the full models' performance. We extend the scope of\nLTH and question whether matching subnetworks still exist in pre-trained\ncomputer vision models, that enjoy the same downstream transfer performance.\nOur extensive experiments convey an overall positive message: from all\npre-trained weights obtained by ImageNet classification, simCLR, and MoCo, we\nare consistently able to locate such matching subnetworks at 59.04% to 96.48%\nsparsity that transfer universally to multiple downstream tasks, whose\nperformance see no degradation compared to using full pre-trained weights.\nFurther analyses reveal that subnetworks found from different pre-training tend\nto yield diverse mask structures and perturbation sensitivities. We conclude\nthat the core LTH observations remain generally relevant in the pre-training\nparadigm of computer vision, but more delicate discussions are needed in some\ncases. Codes and pre-trained models will be made available at:\nhttps://github.com/VITA-Group/CV_LTH_Pre-training.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 21:53:55 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 18:13:06 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Chen", "Tianlong", ""], ["Frankle", "Jonathan", ""], ["Chang", "Shiyu", ""], ["Liu", "Sijia", ""], ["Zhang", "Yang", ""], ["Carbin", "Michael", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2012.06917", "submitter": "Aditya Singh", "authors": "Aditya Singh, Alessandro Bay and Andrea Mirabile", "title": "Assessing The Importance Of Colours For CNNs In Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans rely heavily on shapes as a primary cue for object recognition. As\nsecondary cues, colours and textures are also beneficial in this regard.\nConvolutional neural networks (CNNs), an imitation of biological neural\nnetworks, have been shown to exhibit conflicting properties. Some studies\nindicate that CNNs are biased towards textures whereas, another set of studies\nsuggests shape bias for a classification task. However, they do not discuss the\nrole of colours, implying its possible humble role in the task of object\nrecognition. In this paper, we empirically investigate the importance of\ncolours in object recognition for CNNs. We are able to demonstrate that CNNs\noften rely heavily on colour information while making a prediction. Our results\nshow that the degree of dependency on colours tend to vary from one dataset to\nanother. Moreover, networks tend to rely more on colours if trained from\nscratch. Pre-training can allow the model to be less colour dependent. To\nfacilitate these findings, we follow the framework often deployed in\nunderstanding role of colours in object recognition for humans. We evaluate a\nmodel trained with congruent images (images in original colours eg. red\nstrawberries) on congruent, greyscale, and incongruent images (images in\nunnatural colours eg. blue strawberries). We measure and analyse network's\npredictive performance (top-1 accuracy) under these different stylisations. We\nutilise standard datasets of supervised image classification and fine-grained\nimage classification in our experiments.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 22:55:06 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Singh", "Aditya", ""], ["Bay", "Alessandro", ""], ["Mirabile", "Andrea", ""]]}, {"id": "2012.06940", "submitter": "Jinsong Zhang", "authors": "Jinsong Zhang, Xingzi Liu, Kun Li", "title": "Human Pose Transfer by Adaptive Hierarchical Deformation", "comments": "13 pages, 10 figures. Code is available at\n  https://github.com/Zhangjinso/PINet_PG", "journal-ref": "Computer Graphics Forum (2020), Volume 39, Issue 7", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose transfer, as a misaligned image generation task, is very\nchallenging. Existing methods cannot effectively utilize the input information,\nwhich often fail to preserve the style and shape of hair and clothes. In this\npaper, we propose an adaptive human pose transfer network with two hierarchical\ndeformation levels. The first level generates human semantic parsing aligned\nwith the target pose, and the second level generates the final textured person\nimage in the target pose with the semantic guidance. To avoid the drawback of\nvanilla convolution that treats all the pixels as valid information, we use\ngated convolution in both two levels to dynamically select the important\nfeatures and adaptively deform the image layer by layer. Our model has very few\nparameters and is fast to converge. Experimental results demonstrate that our\nmodel achieves better performance with more consistent hair, face and clothes\nwith fewer parameters than state-of-the-art methods. Furthermore, our method\ncan be applied to clothing texture transfer.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 01:49:26 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zhang", "Jinsong", ""], ["Liu", "Xingzi", ""], ["Li", "Kun", ""]]}, {"id": "2012.06946", "submitter": "Jianfeng Wang", "authors": "Jianfeng Wang and Xiaowei Hu and Pengchuan Zhang and Xiujun Li and\n  Lijuan Wang and Lei Zhang and Jianfeng Gao and Zicheng Liu", "title": "MiniVLM: A Smaller and Faster Vision-Language Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent vision-language (VL) studies have shown remarkable progress by\nlearning generic representations from massive image-text pairs with transformer\nmodels and then fine-tuning on downstream VL tasks. While existing research has\nbeen focused on achieving high accuracy with large pre-trained models, building\na lightweight model is of great value in practice but is less explored. In this\npaper, we propose a smaller and faster VL model, MiniVLM, which can be\nfinetuned with good performance on various downstream tasks like its larger\ncounterpart. MiniVLM consists of two modules, a vision feature extractor and a\ntransformer-based vision-language fusion module. We design a Two-stage\nEfficient feature Extractor (TEE), inspired by the one-stage EfficientDet\nnetwork, to significantly reduce the time cost of visual feature extraction by\n$95\\%$, compared to a baseline model. We adopt the MiniLM structure to reduce\nthe computation cost of the transformer module after comparing different\ncompact BERT models. In addition, we improve the MiniVLM pre-training by adding\n$7M$ Open Images data, which are pseudo-labeled by a state-of-the-art\ncaptioning model. We also pre-train with high-quality image tags obtained from\na strong tagging model to enhance cross-modality alignment. The large models\nare used offline without adding any overhead in fine-tuning and inference. With\nthe above design choices, our MiniVLM reduces the model size by $73\\%$ and the\ninference time cost by $94\\%$ while being able to retain $94-97\\%$ of the\naccuracy on multiple VL tasks. We hope that MiniVLM helps ease the use of the\nstate-of-the-art VL research for on-the-edge applications.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 03:02:06 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Wang", "Jianfeng", ""], ["Hu", "Xiaowei", ""], ["Zhang", "Pengchuan", ""], ["Li", "Xiujun", ""], ["Wang", "Lijuan", ""], ["Zhang", "Lei", ""], ["Gao", "Jianfeng", ""], ["Liu", "Zicheng", ""]]}, {"id": "2012.06948", "submitter": "Michael Zhang", "authors": "Michael Zhang, Xiaotian Cheng, Daniel Copeland, Arjun Desai, Melody Y.\n  Guan, Gabriel A. Brat, and Serena Yeung", "title": "Using Computer Vision to Automate Hand Detection and Tracking of Surgeon\n  Movements in Videos of Open Surgery", "comments": "AMIA 2020 Annual Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Open, or non-laparoscopic surgery, represents the vast majority of all\noperating room procedures, but few tools exist to objectively evaluate these\ntechniques at scale. Current efforts involve human expert-based visual\nassessment. We leverage advances in computer vision to introduce an automated\napproach to video analysis of surgical execution. A state-of-the-art\nconvolutional neural network architecture for object detection was used to\ndetect operating hands in open surgery videos. Automated assessment was\nexpanded by combining model predictions with a fast object tracker to enable\nsurgeon-specific hand tracking. To train our model, we used publicly available\nvideos of open surgery from YouTube and annotated these with spatial bounding\nboxes of operating hands. Our model's spatial detections of operating hands\nsignificantly outperforms the detections achieved using pre-existing\nhand-detection datasets, and allow for insights into intra-operative movement\npatterns and economy of motion.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 03:10:09 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zhang", "Michael", ""], ["Cheng", "Xiaotian", ""], ["Copeland", "Daniel", ""], ["Desai", "Arjun", ""], ["Guan", "Melody Y.", ""], ["Brat", "Gabriel A.", ""], ["Yeung", "Serena", ""]]}, {"id": "2012.06951", "submitter": "Qi Qi", "authors": "Qi Qi, Yi Xu, Rong Jin, Wotao Yin, Tianbao Yang", "title": "Attentional Biased Stochastic Gradient for Imbalanced Classification", "comments": "29pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a simple yet effective method (ABSGD) for\naddressing the data imbalance issue in deep learning. Our method is a simple\nmodification to momentum SGD where we leverage an attentional mechanism to\nassign an individual importance weight to each gradient in the mini-batch.\nUnlike many existing heuristic-driven methods for tackling data imbalance, our\nmethod is grounded in {\\it theoretically justified distributionally robust\noptimization (DRO)}, which is guaranteed to converge to a stationary point of\nan information-regularized DRO problem. The individual-level weight of a\nsampled data is systematically proportional to the exponential of a scaled loss\nvalue of the data, where the scaling factor is interpreted as the\nregularization parameter in the framework of information-regularized DRO.\nCompared with existing class-level weighting schemes, our method can capture\nthe diversity between individual examples within each class. Compared with\nexisting individual-level weighting methods using meta-learning that require\nthree backward propagations for computing mini-batch stochastic gradients, our\nmethod is more efficient with only one backward propagation at each iteration\nas in standard deep learning methods. To balance between the learning of\nfeature extraction layers and the learning of the classifier layer, we employ a\ntwo-stage method that uses SGD for pretraining followed by ABSGD for learning a\nrobust classifier and finetuning lower layers. Our empirical studies on several\nbenchmark datasets demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 03:41:52 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 02:29:33 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Qi", "Qi", ""], ["Xu", "Yi", ""], ["Jin", "Rong", ""], ["Yin", "Wotao", ""], ["Yang", "Tianbao", ""]]}, {"id": "2012.06956", "submitter": "Zifeng Wang", "authors": "Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy,\n  Stratis Ioannidis", "title": "Learn-Prune-Share for Lifelong Learning", "comments": "Accepted to the IEEE International Conference on Data Mining 2020\n  (ICDM'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In lifelong learning, we wish to maintain and update a model (e.g., a neural\nnetwork classifier) in the presence of new classification tasks that arrive\nsequentially. In this paper, we propose a learn-prune-share (LPS) algorithm\nwhich addresses the challenges of catastrophic forgetting, parsimony, and\nknowledge reuse simultaneously. LPS splits the network into task-specific\npartitions via an ADMM-based pruning strategy. This leads to no forgetting,\nwhile maintaining parsimony. Moreover, LPS integrates a novel selective\nknowledge sharing scheme into this ADMM optimization framework. This enables\nadaptive knowledge sharing in an end-to-end fashion. Comprehensive experimental\nresults on two lifelong learning benchmark datasets and a challenging\nreal-world radio frequency fingerprinting dataset are provided to demonstrate\nthe effectiveness of our approach. Our experiments show that LPS consistently\noutperforms multiple state-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 04:05:16 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Wang", "Zifeng", ""], ["Jian", "Tong", ""], ["Chowdhury", "Kaushik", ""], ["Wang", "Yanzhi", ""], ["Dy", "Jennifer", ""], ["Ioannidis", "Stratis", ""]]}, {"id": "2012.06964", "submitter": "Bolin Lai", "authors": "Bolin Lai, Yuhsuan Wu, Xiaoyu Bai, Xiao-Yun Zhou, Peng Wang, Jinzheng\n  Cai, Yuankai Huo, Lingyun Huang, Yong Xia, Jing Xiao, Le Lu, Heping Hu, Adam\n  Harrison", "title": "Fully-Automated Liver Tumor Localization and Characterization from\n  Multi-Phase MR Volumes Using Key-Slice ROI Parsing: A Physician-Inspired\n  Approach", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using radiological scans to identify liver tumors is crucial for proper\npatient treatment. This is highly challenging, as top radiologists only achieve\nF1 scores of roughly 80% (hepatocellular carcinoma (HCC) vs. others) with only\nmoderate inter-rater agreement, even when using multi-phase magnetic resonance\n(MR) imagery. Thus, there is great impetus for computer-aided diagnosis (CAD)\nsolutions. A critical challenge is to robustly parse a 3D MR volume to localize\ndiagnosable regions of interest (ROI), especially for edge cases. In this\npaper, we break down this problem using a key-slice parser (KSP), which\nemulates physician workflows by first identifying key slices and then\nlocalizing their corresponding key ROIs. To achieve robustness, the KSP also\nuses curve-parsing and detection confidence re-weighting. We evaluate our\napproach on the largest multi-phase MR liver lesion test dataset to date (430\nbiopsy-confirmed patients). Experiments demonstrate that our KSP can localize\ndiagnosable ROIs with high reliability: 87% patients have an average 3D overlap\nof >= 40% with the ground truth compared to only 79% using the best tested\ndetector. When coupled with a classifier, we achieve an HCC vs. others F1 score\nof 0.801, providing a fully-automated CAD performance comparable to top human\nphysicians.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 05:23:33 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 19:47:40 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 04:04:47 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Lai", "Bolin", ""], ["Wu", "Yuhsuan", ""], ["Bai", "Xiaoyu", ""], ["Zhou", "Xiao-Yun", ""], ["Wang", "Peng", ""], ["Cai", "Jinzheng", ""], ["Huo", "Yuankai", ""], ["Huang", "Lingyun", ""], ["Xia", "Yong", ""], ["Xiao", "Jing", ""], ["Lu", "Le", ""], ["Hu", "Heping", ""], ["Harrison", "Adam", ""]]}, {"id": "2012.06973", "submitter": "Chirag Kyal", "authors": "Chirag Kyal", "title": "Spontaneous Emotion Recognition from Facial Thermal Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the key research areas in computer vision addressed by a vast number\nof publications is the processing and understanding of images containing human\nfaces. The most often addressed tasks include face detection, facial landmark\nlocalization, face recognition and facial expression analysis. Other, more\nspecialized tasks such as affective computing, the extraction of vital signs\nfrom videos or analysis of social interaction usually require one or several of\nthe aforementioned tasks that have to be performed. In our work, we analyze\nthat a large number of tasks for facial image processing in thermal infrared\nimages that are currently solved using specialized rule-based methods or not\nsolved at all can be addressed with modern learning-based approaches. We have\nused USTC-NVIE database for training of a number of machine learning algorithms\nfor facial landmark localization.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 05:55:19 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Kyal", "Chirag", ""]]}, {"id": "2012.06977", "submitter": "Wenhao Wu", "authors": "Wenhao Wu, Dongliang He, Tianwei Lin, Fu Li, Chuang Gan, Errui Ding", "title": "MVFNet: Multi-View Fusion Network for Efficient Video Recognition", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventionally, spatiotemporal modeling network and its complexity are the\ntwo most concentrated research topics in video action recognition. Existing\nstate-of-the-art methods have achieved excellent accuracy regardless of the\ncomplexity meanwhile efficient spatiotemporal modeling solutions are slightly\ninferior in performance. In this paper, we attempt to acquire both efficiency\nand effectiveness simultaneously. First of all, besides traditionally treating\nH x W x T video frames as space-time signal (viewing from the Height-Width\nspatial plane), we propose to also model video from the other two Height-Time\nand Width-Time planes, to capture the dynamics of video thoroughly. Secondly,\nour model is designed based on 2D CNN backbones and model complexity is well\nkept in mind by design. Specifically, we introduce a novel multi-view fusion\n(MVF) module to exploit video dynamics using separable convolution for\nefficiency. It is a plug-and-play module and can be inserted into off-the-shelf\n2D CNNs to form a simple yet effective model called MVFNet. Moreover, MVFNet\ncan be thought of as a generalized video modeling framework and it can\nspecialize to be existing methods such as C2D, SlowOnly, and TSM under\ndifferent settings. Extensive experiments are conducted on popular benchmarks\n(i.e., Something-Something V1 & V2, Kinetics, UCF-101, and HMDB-51) to show its\nsuperiority. The proposed MVFNet can achieve state-of-the-art performance with\n2D CNN's complexity.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 06:34:18 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 06:09:48 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Wu", "Wenhao", ""], ["He", "Dongliang", ""], ["Lin", "Tianwei", ""], ["Li", "Fu", ""], ["Gan", "Chuang", ""], ["Ding", "Errui", ""]]}, {"id": "2012.06980", "submitter": "Xiaojuan Qi", "authors": "Xiaojuan Qi, Zhengzhe Liu, Renjie Liao, Philip H.S. Torr, Raquel\n  Urtasun, Jiaya Jia", "title": "GeoNet++: Iterative Geometric Neural Network with Edge-Aware Refinement\n  for Joint Depth and Surface Normal Estimation", "comments": "TPAMI 2020. Code available: https://github.com/xjqi/GeoNet", "journal-ref": null, "doi": "10.1109/TPAMI.2020.3020800", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a geometric neural network with edge-aware\nrefinement (GeoNet++) to jointly predict both depth and surface normal maps\nfrom a single image. Building on top of two-stream CNNs, GeoNet++ captures the\ngeometric relationships between depth and surface normals with the proposed\ndepth-to-normal and normal-to-depth modules. In particular, the\n\"depth-to-normal\" module exploits the least square solution of estimating\nsurface normals from depth to improve their quality, while the\n\"normal-to-depth\" module refines the depth map based on the constraints on\nsurface normals through kernel regression. Boundary information is exploited\nvia an edge-aware refinement module. GeoNet++ effectively predicts depth and\nsurface normals with strong 3D consistency and sharp boundaries resulting in\nbetter reconstructed 3D scenes. Note that GeoNet++ is generic and can be used\nin other depth/normal prediction frameworks to improve the quality of 3D\nreconstruction and pixel-wise accuracy of depth and surface normals.\nFurthermore, we propose a new 3D geometric metric (3DGM) for evaluating depth\nprediction in 3D. In contrast to current metrics that focus on evaluating\npixel-wise error/accuracy, 3DGM measures whether the predicted depth can\nreconstruct high-quality 3D surface normals. This is a more natural metric for\nmany 3D application domains. Our experiments on NYUD-V2 and KITTI datasets\nverify that GeoNet++ produces fine boundary details, and the predicted depth\ncan be used to reconstruct high-quality 3D surfaces. Code has been made\npublicly available.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 06:48:01 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Qi", "Xiaojuan", ""], ["Liu", "Zhengzhe", ""], ["Liao", "Renjie", ""], ["Torr", "Philip H. S.", ""], ["Urtasun", "Raquel", ""], ["Jia", "Jiaya", ""]]}, {"id": "2012.06983", "submitter": "Yi Zhang", "authors": "Yi Zhang, Hu Chen, Wenjun Xia, Yang Chen, Baodong Liu, Yan Liu,\n  Huaiqiang Sun, and Jiliu Zhou", "title": "LEARN++: Recurrent Dual-Domain Reconstruction Network for Compressed\n  Sensing CT", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compressed sensing (CS) computed tomography has been proven to be important\nfor several clinical applications, such as sparse-view computed tomography\n(CT), digital tomosynthesis and interior tomography. Traditional compressed\nsensing focuses on the design of handcrafted prior regularizers, which are\nusually image-dependent and time-consuming. Inspired by recently proposed deep\nlearning-based CT reconstruction models, we extend the state-of-the-art LEARN\nmodel to a dual-domain version, dubbed LEARN++. Different from existing\niteration unrolling methods, which only involve projection data in the data\nconsistency layer, the proposed LEARN++ model integrates two parallel and\ninteractive subnetworks to perform image restoration and sinogram inpainting\noperations on both the image and projection domains simultaneously, which can\nfully explore the latent relations between projection data and reconstructed\nimages. The experimental results demonstrate that the proposed LEARN++ model\nachieves competitive qualitative and quantitative results compared to several\nstate-of-the-art methods in terms of both artifact reduction and detail\npreservation.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 07:00:50 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zhang", "Yi", ""], ["Chen", "Hu", ""], ["Xia", "Wenjun", ""], ["Chen", "Yang", ""], ["Liu", "Baodong", ""], ["Liu", "Yan", ""], ["Sun", "Huaiqiang", ""], ["Zhou", "Jiliu", ""]]}, {"id": "2012.06985", "submitter": "Raviteja Vemulapalli", "authors": "Xiangyun Zhao, Raviteja Vemulapalli, Philip Mansfield, Boqing Gong,\n  Bradley Green, Lior Shapira, Ying Wu", "title": "Contrastive Learning for Label-Efficient Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Collecting labeled data for the task of semantic segmentation is expensive\nand time-consuming, as it requires dense pixel-level annotations. While recent\nConvolutional Neural Network (CNN) based semantic segmentation approaches have\nachieved impressive results by using large amounts of labeled training data,\ntheir performance drops significantly as the amount of labeled data decreases.\nThis happens because deep CNNs trained with the de facto cross-entropy loss can\neasily overfit to small amounts of labeled data. To address this issue, we\npropose a simple and effective contrastive learning-based training strategy in\nwhich we first pretrain the network using a pixel-wise, label-based contrastive\nloss, and then fine-tune it using the cross-entropy loss. This approach\nincreases intra-class compactness and inter-class separability, thereby\nresulting in a better pixel classifier. We demonstrate the effectiveness of the\nproposed training strategy using the Cityscapes and PASCAL VOC 2012\nsegmentation datasets. Our results show that pretraining with the proposed\ncontrastive loss results in large performance gains (more than 20% absolute\nimprovement in some settings) when the amount of labeled data is limited. In\nmany settings, the proposed contrastive pretraining strategy, which does not\nuse any additional data, is able to match or outperform the widely-used\nImageNet pretraining strategy that uses more than a million additional labeled\nimages.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 07:05:39 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 05:00:28 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 00:33:07 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Zhao", "Xiangyun", ""], ["Vemulapalli", "Raviteja", ""], ["Mansfield", "Philip", ""], ["Gong", "Boqing", ""], ["Green", "Bradley", ""], ["Shapira", "Lior", ""], ["Wu", "Ying", ""]]}, {"id": "2012.06995", "submitter": "Fangrui Lv", "authors": "Shuang Li, Fangrui Lv, Binhui Xie, Chi Harold Liu, Jian Liang, Chen\n  Qin", "title": "Bi-Classifier Determinacy Maximization for Unsupervised Domain\n  Adaptation", "comments": "Accepted as AAAI 2021. The code is publicly available at\n  https://github.com/BIT-DA/BCDM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation challenges the problem of transferring\nknowledge from a well-labelled source domain to an unlabelled target domain.\nRecently,adversarial learning with bi-classifier has been proven effective in\npushing cross-domain distributions close. Prior approaches typically leverage\nthe disagreement between bi-classifier to learn transferable representations,\nhowever, they often neglect the classifier determinacy in the target domain,\nwhich could result in a lack of feature discriminability. In this paper, we\npresent a simple yet effective method, namely Bi-Classifier Determinacy\nMaximization(BCDM), to tackle this problem. Motivated by the observation that\ntarget samples cannot always be separated distinctly by the decision boundary,\nhere in the proposed BCDM, we design a novel classifier determinacy disparity\n(CDD) metric, which formulates classifier discrepancy as the class relevance of\ndistinct target predictions and implicitly introduces constraint on the target\nfeature discriminability. To this end, the BCDM can generate discriminative\nrepresentations by encouraging target predictive outputs to be consistent and\ndetermined, meanwhile, preserve the diversity of predictions in an adversarial\nmanner. Furthermore, the properties of CDD as well as the theoretical\nguarantees of BCDM's generalization bound are both elaborated. Extensive\nexperiments show that BCDM compares favorably against the existing\nstate-of-the-art domain adaptation methods.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 07:55:39 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Li", "Shuang", ""], ["Lv", "Fangrui", ""], ["Xie", "Binhui", ""], ["Liu", "Chi Harold", ""], ["Liang", "Jian", ""], ["Qin", "Chen", ""]]}, {"id": "2012.07002", "submitter": "Jihua Zhu", "authors": "Yanlin Ma, Jihua Zhu, Zhongyu Li, Zhiqiang Tian, Yaochen Li", "title": "Effective multi-view registration of point sets based on student's t\n  mixture model", "comments": "11pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, Expectation-maximization (EM) algorithm has been introduced as an\neffective means to solve multi-view registration problem. Most of the previous\nmethods assume that each data point is drawn from the Gaussian Mixture Model\n(GMM), which is difficult to deal with the noise with heavy-tail or outliers.\nAccordingly, this paper proposed an effective registration method based on\nStudent's t Mixture Model (StMM). More specially, we assume that each data\npoint is drawn from one unique StMM, where its nearest neighbors (NNs) in other\npoint sets are regarded as the t-distribution centroids with equal covariances,\nmembership probabilities, and fixed degrees of freedom. Based on this\nassumption, the multi-view registration problem is formulated into the\nmaximization of the likelihood function including all rigid transformations.\nSubsequently, the EM algorithm is utilized to optimize rigid transformations as\nwell as the only t-distribution covariance for multi-view registration. Since\nonly a few model parameters require to be optimized, the proposed method is\nmore likely to obtain the desired registration results. Besides, all\nt-distribution centroids can be obtained by the NN search method, it is very\nefficient to achieve multi-view registration. What's more, the t-distribution\ntakes the noise with heavy-tail into consideration, which makes the proposed\nmethod be inherently robust to noises and outliers. Experimental results tested\non benchmark data sets illustrate its superior performance on robustness and\naccuracy over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 08:27:29 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Ma", "Yanlin", ""], ["Zhu", "Jihua", ""], ["Li", "Zhongyu", ""], ["Tian", "Zhiqiang", ""], ["Li", "Yaochen", ""]]}, {"id": "2012.07007", "submitter": "Xiaodong Cun", "authors": "Xiaodong Cun and Chi-Man Pun", "title": "Split then Refine: Stacked Attention-guided ResUNets for Blind Single\n  Image Visible Watermark Removal", "comments": "AAAI21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital watermark is a commonly used technique to protect the copyright of\nmedias. Simultaneously, to increase the robustness of watermark, attacking\ntechnique, such as watermark removal, also gets the attention from the\ncommunity. Previous watermark removal methods require to gain the watermark\nlocation from users or train a multi-task network to recover the background\nindiscriminately. However, when jointly learning, the network performs better\non watermark detection than recovering the texture. Inspired by this\nobservation and to erase the visible watermarks blindly, we propose a novel\ntwo-stage framework with a stacked attention-guided ResUNets to simulate the\nprocess of detection, removal and refinement. In the first stage, we design a\nmulti-task network called SplitNet. It learns the basis features for three\nsub-tasks altogether while the task-specific features separately use multiple\nchannel attentions. Then, with the predicted mask and coarser restored image,\nwe design RefineNet to smooth the watermarked region with a mask-guided spatial\nattention. Besides network structure, the proposed algorithm also combines\nmultiple perceptual losses for better quality both visually and numerically. We\nextensively evaluate our algorithm over four different datasets under various\nsettings and the experiments show that our approach outperforms other\nstate-of-the-art methods by a large margin. The code is available at\nhttp://github.com/vinthony/deep-blind-watermark-removal.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 09:05:37 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Cun", "Xiaodong", ""], ["Pun", "Chi-Man", ""]]}, {"id": "2012.07033", "submitter": "Zhengxiong Luo", "authors": "Zhengxiong Luo, Zhicheng Wang, Yuanhao Cai, Guanan Wang, Yan Huang,\n  Liang Wang, Erjin Zhou, Tieniu Tan, Jian Sun", "title": "Efficient Human Pose Estimation by Learning Deeply Aggregated\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose an efficient human pose estimation network (DANet)\nby learning deeply aggregated representations. Most existing models explore\nmulti-scale information mainly from features with different spatial sizes.\nPowerful multi-scale representations usually rely on the cascaded pyramid\nframework. This framework largely boosts the performance but in the meanwhile\nmakes networks very deep and complex. Instead, we focus on exploiting\nmulti-scale information from layers with different receptive-field sizes and\nthen making full of use this information by improving the fusion method.\nSpecifically, we propose an orthogonal attention block (OAB) and a second-order\nfusion unit (SFU). The OAB learns multi-scale information from different layers\nand enhances them by encouraging them to be diverse. The SFU adaptively selects\nand fuses diverse multi-scale information and suppress the redundant ones. This\ncould maximize the effective information in final fused representations. With\nthe help of OAB and SFU, our single pyramid network may be able to generate\ndeeply aggregated representations that contain even richer multi-scale\ninformation and have a larger representing capacity than that of cascaded\nnetworks. Thus, our networks could achieve comparable or even better accuracy\nwith much smaller model complexity. Specifically, our \\mbox{DANet-72} achieves\n$70.5$ in AP score on COCO test-dev set with only $1.0G$ FLOPs. Its speed on a\nCPU platform achieves $58$ Persons-Per-Second~(PPS).\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 10:58:07 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 02:48:52 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Luo", "Zhengxiong", ""], ["Wang", "Zhicheng", ""], ["Cai", "Yuanhao", ""], ["Wang", "Guanan", ""], ["Huang", "Yan", ""], ["Wang", "Liang", ""], ["Zhou", "Erjin", ""], ["Tan", "Tieniu", ""], ["Sun", "Jian", ""]]}, {"id": "2012.07038", "submitter": "Christina Petschnigg", "authors": "Christina Petschnigg and Juergen Pilz", "title": "Uncertainty Estimation in Deep Neural Networks for Point Cloud\n  Segmentation in Factory Planning", "comments": "17 pages, 5 figures, submitted to MDPI Modelling journal for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The digital factory provides undoubtedly a great potential for future\nproduction systems in terms of efficiency and effectivity. A key aspect on the\nway to realize the digital copy of a real factory is the understanding of\ncomplex indoor environments on the basis of 3D data. In order to generate an\naccurate factory model including the major components, i.e. building parts,\nproduct assets and process details, the 3D data collected during digitalization\ncan be processed with advanced methods of deep learning. In this work, we\npropose a fully Bayesian and an approximate Bayesian neural network for point\ncloud segmentation. This allows us to analyze how different ways of estimating\nuncertainty in these networks improve segmentation results on raw 3D point\nclouds. We achieve superior model performance for both, the Bayesian and the\napproximate Bayesian model compared to the frequentist one. This performance\ndifference becomes even more striking when incorporating the networks'\nuncertainty in their predictions. For evaluation we use the scientific data set\nS3DIS as well as a data set, which was collected by the authors at a German\nautomotive production plant. The methods proposed in this work lead to more\naccurate segmentation results and the incorporation of uncertainty information\nmakes this approach especially applicable to safety critical applications.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 11:18:52 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Petschnigg", "Christina", ""], ["Pilz", "Juergen", ""]]}, {"id": "2012.07042", "submitter": "Xiangde Luo", "authors": "Xiangde Luo, Wenjun Liao, Jieneng Chen, Tao Song, Yinan Chen, Shichuan\n  Zhang, Nianyong Chen, Guotai Wang, Shaoting Zhang", "title": "Efficient Semi-Supervised Gross Target Volume of Nasopharyngeal\n  Carcinoma Segmentation via Uncertainty Rectified Pyramid Consistency", "comments": "13 pages, provisional accept by MICCAI2021,code\n  at:https://github.com/HiLab-git/SSL4MIS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Gross Target Volume (GTV) segmentation plays an irreplaceable role in\nradiotherapy planning for Nasopharyngeal Carcinoma (NPC). Despite that\nConvolutional Neural Networks (CNN) have achieved good performance for this\ntask, they rely on a large set of labeled images for training, which is\nexpensive and time-consuming to acquire. In this paper, we propose a novel\nframework with Uncertainty Rectified Pyramid Consistency (URPC) regularization\nfor semi-supervised NPC GTV segmentation. Concretely, we extend a backbone\nsegmentation network to produce pyramid predictions at different scales. The\npyramid predictions network (PPNet) is supervised by the ground truth of\nlabeled images and a multi-scale consistency loss for unlabeled images,\nmotivated by the fact that prediction at different scales for the same input\nshould be similar and consistent. However, due to the different resolution of\nthese predictions, encouraging them to be consistent at each pixel directly has\nlow robustness and may lose some fine details. To address this problem, we\nfurther design a novel uncertainty rectifying module to enable the framework to\ngradually learn from meaningful and reliable consensual regions at different\nscales. Experimental results on a dataset with 258 NPC MR images showed that\nwith only 10% or 20% images labeled, our method largely improved the\nsegmentation performance by leveraging the unlabeled images, and it also\noutperformed five state-of-the-art semi-supervised segmentation methods.\nMoreover, when only 50% images labeled, URPC achieved an average Dice score of\n82.74% that was close to fully supervised learning.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 11:45:00 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 07:16:26 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 06:04:41 GMT"}, {"version": "v4", "created": "Fri, 4 Jun 2021 03:31:58 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Luo", "Xiangde", ""], ["Liao", "Wenjun", ""], ["Chen", "Jieneng", ""], ["Song", "Tao", ""], ["Chen", "Yinan", ""], ["Zhang", "Shichuan", ""], ["Chen", "Nianyong", ""], ["Wang", "Guotai", ""], ["Zhang", "Shaoting", ""]]}, {"id": "2012.07043", "submitter": "WenHui Lei", "authors": "Wenhui Lei, Wei Xu, Ran Gu, Hao Fu, Shaoting Zhang, Guotai Wang", "title": "Contrastive Learning of Relative Position Regression for One-Shot Object\n  Localization in 3D Medical Images", "comments": "Early accepted by MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning networks have shown promising performance for accurate object\nlocalization in medial images, but require large amount of annotated data for\nsupervised training, which is expensive and expertise burdensome. To address\nthis problem, we present a one-shot framework for organ and landmark\nlocalization in volumetric medical images, which does not need any annotation\nduring the training stage and could be employed to locate any landmarks or\norgans in test images given a support (reference) image during the inference\nstage. Our main idea comes from that tissues and organs from different human\nbodies have a similar relative position and context. Therefore, we could\npredict the relative positions of their non-local patches, thus locate the\ntarget organ. Our framework is composed of three parts: (1) A projection\nnetwork trained to predict the 3D offset between any two patches from the same\nvolume, where human annotations are not required. In the inference stage, it\ntakes one given landmark in a reference image as a support patch and predicts\nthe offset from a random patch to the corresponding landmark in the test\n(query) volume. (2) A coarse-to-fine framework contains two projection\nnetworks, providing more accurate localization of the target. (3) Based on the\ncoarse-to-fine model, we transfer the organ boundingbox (B-box) detection to\nlocating six extreme points along x, y and z directions in the query volume.\nExperiments on multi-organ localization from head-and-neck (HaN) CT volumes\nshowed that our method acquired competitive performance in real time, which is\nmore accurate and 10^5 times faster than template matching methods with the\nsame setting. Code is available: https://github.com/LWHYC/RPR-Loc.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 11:54:19 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 13:13:27 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Lei", "Wenhui", ""], ["Xu", "Wei", ""], ["Gu", "Ran", ""], ["Fu", "Hao", ""], ["Zhang", "Shaoting", ""], ["Wang", "Guotai", ""]]}, {"id": "2012.07049", "submitter": "Jinsong Zhang", "authors": "Kun Li, Jinsong Zhang, Yebin Liu, Yu-Kun Lai, Qionghai Dai", "title": "PoNA: Pose-guided Non-local Attention for Human Pose Transfer", "comments": "16 pages, 14 figures", "journal-ref": "IEEE Transactions on Image Processing (2020), Volume 29", "doi": "10.1109/TIP.2020.3029455", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose transfer, which aims at transferring the appearance of a given\nperson to a target pose, is very challenging and important in many\napplications. Previous work ignores the guidance of pose features or only uses\nlocal attention mechanism, leading to implausible and blurry results. We\npropose a new human pose transfer method using a generative adversarial network\n(GAN) with simplified cascaded blocks. In each block, we propose a pose-guided\nnon-local attention (PoNA) mechanism with a long-range dependency scheme to\nselect more important regions of image features to transfer. We also design\npre-posed image-guided pose feature update and post-posed pose-guided image\nfeature update to better utilize the pose and image features. Our network is\nsimple, stable, and easy to train. Quantitative and qualitative results on\nMarket-1501 and DeepFashion datasets show the efficacy and efficiency of our\nmodel. Compared with state-of-the-art methods, our model generates sharper and\nmore realistic images with rich details, while having fewer parameters and\nfaster speed. Furthermore, our generated images can help to alleviate data\ninsufficiency for person re-identification.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 12:38:29 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Li", "Kun", ""], ["Zhang", "Jinsong", ""], ["Liu", "Yebin", ""], ["Lai", "Yu-Kun", ""], ["Dai", "Qionghai", ""]]}, {"id": "2012.07061", "submitter": "Jiayi Ji", "authors": "Jiayi Ji, Yunpeng Luo, Xiaoshuai Sun, Fuhai Chen, Gen Luo, Yongjian\n  Wu, Yue Gao, Rongrong Ji", "title": "Improving Image Captioning by Leveraging Intra- and Inter-layer Global\n  Representation in Transformer Network", "comments": "Accepted at AAAI 2021 (preprint version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformer-based architectures have shown great success in image captioning,\nwhere object regions are encoded and then attended into the vectorial\nrepresentations to guide the caption decoding. However, such vectorial\nrepresentations only contain region-level information without considering the\nglobal information reflecting the entire image, which fails to expand the\ncapability of complex multi-modal reasoning in image captioning. In this paper,\nwe introduce a Global Enhanced Transformer (termed GET) to enable the\nextraction of a more comprehensive global representation, and then adaptively\nguide the decoder to generate high-quality captions. In GET, a Global Enhanced\nEncoder is designed for the embedding of the global feature, and a Global\nAdaptive Decoder are designed for the guidance of the caption generation. The\nformer models intra- and inter-layer global representation by taking advantage\nof the proposed Global Enhanced Attention and a layer-wise fusion module. The\nlatter contains a Global Adaptive Controller that can adaptively fuse the\nglobal information into the decoder to guide the caption generation. Extensive\nexperiments on MS COCO dataset demonstrate the superiority of our GET over many\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 13:38:58 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Ji", "Jiayi", ""], ["Luo", "Yunpeng", ""], ["Sun", "Xiaoshuai", ""], ["Chen", "Fuhai", ""], ["Luo", "Gen", ""], ["Wu", "Yongjian", ""], ["Gao", "Yue", ""], ["Ji", "Rongrong", ""]]}, {"id": "2012.07072", "submitter": "Muhammad Afifi", "authors": "Mohamed Afifi, Yara Ali, Karim Amer, Mahmoud Shaker, Mohamed Elhelw", "title": "Robust Real-Time Pedestrian Detection on Embedded Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of pedestrians on embedded devices, such as those on-board of\nrobots and drones, has many applications including road intersection\nmonitoring, security, crowd monitoring and surveillance, to name a few.\nHowever, the problem can be challenging due to continuously-changing camera\nviewpoint and varying object appearances as well as the need for lightweight\nalgorithms suitable for embedded systems. This paper proposes a robust\nframework for pedestrian detection in many footages. The framework performs\nfine and coarse detections on different image regions and exploits temporal and\nspatial characteristics to attain enhanced accuracy and real time performance\non embedded boards. The framework uses the Yolo-v3 object detection [1] as its\nbackbone detector and runs on the Nvidia Jetson TX2 embedded board, however\nother detectors and/or boards can be used as well. The performance of the\nframework is demonstrated on two established datasets and its achievement of\nthe second place in CVPR 2019 Embedded Real-Time Inference (ERTI) Challenge.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 14:43:30 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Afifi", "Mohamed", ""], ["Ali", "Yara", ""], ["Amer", "Karim", ""], ["Shaker", "Mahmoud", ""], ["Elhelw", "Mohamed", ""]]}, {"id": "2012.07079", "submitter": "Narinder Punn", "authors": "Narinder Singh Punn, Sonali Agarwal", "title": "CHS-Net: A Deep learning approach for hierarchical segmentation of\n  COVID-19 infected CT images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The pandemic of novel severe acute respiratory syndrome coronavirus 2\n(SARS-CoV-2) also known as COVID-19 has been spreading worldwide, causing\nrampant loss of lives. Medical imaging such as computed tomography (CT), X-ray,\netc., plays a significant role in diagnosing the patients by presenting the\nexcellent details about the structure of the organs. However, for any\nradiologist analyzing such scans is a tedious and time-consuming task. The\nemerging deep learning technologies have displayed its strength in analyzing\nsuch scans to aid in the faster diagnosis of the diseases and viruses such as\nCOVID-19. In the present article, an automated deep learning based model,\nCOVID-19 hierarchical segmentation network (CHS-Net) is proposed that functions\nas a semantic hierarchical segmenter to identify the COVID-19 infected regions\nfrom lungs contour via CT medical imaging. The CHS-Net is developed with the\ntwo cascaded residual attention inception U-Net (RAIU-Net) models where first\ngenerates lungs contour maps and second generates COVID-19 infected regions.\nRAIU-Net comprises of a residual inception U-Net model with spectral spatial\nand depth attention network (SSD), consisting of contraction and expansion\nphases of depthwise separable convolutions and hybrid pooling (max and spectral\npooling) to efficiently encode and decode the semantic and varying resolution\ninformation. The CHS-Net is trained with the segmentation loss function that is\nthe weighted average of binary cross entropy loss and dice loss to penalize\nfalse negative and false positive predictions. The approach is compared with\nthe recently proposed research works on the basis of standard metrics. With\nextensive trials, it is observed that the proposed approach outperformed the\nrecently proposed approaches and effectively segments the COVID-19 infected\nregions in the lungs.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 15:02:05 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 14:21:52 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2021 18:15:12 GMT"}, {"version": "v4", "created": "Fri, 30 Apr 2021 11:03:07 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Punn", "Narinder Singh", ""], ["Agarwal", "Sonali", ""]]}, {"id": "2012.07086", "submitter": "Wenqiang Zhang", "authors": "Wenqiang Zhang, Jiemin Fang, Xinggang Wang, Wenyu Liu", "title": "EfficientPose: Efficient Human Pose Estimation with Neural Architecture\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation from image and video is a vital task in many multimedia\napplications. Previous methods achieve great performance but rarely take\nefficiency into consideration, which makes it difficult to implement the\nnetworks on resource-constrained devices. Nowadays real-time multimedia\napplications call for more efficient models for better interactions. Moreover,\nmost deep neural networks for pose estimation directly reuse the networks\ndesigned for image classification as the backbone, which are not yet optimized\nfor the pose estimation task. In this paper, we propose an efficient framework\ntargeted at human pose estimation including two parts, the efficient backbone\nand the efficient head. By implementing the differentiable neural architecture\nsearch method, we customize the backbone network design for pose estimation and\nreduce the computation cost with negligible accuracy degradation. For the\nefficient head, we slim the transposed convolutions and propose a spatial\ninformation correction module to promote the performance of the final\nprediction. In experiments, we evaluate our networks on the MPII and COCO\ndatasets. Our smallest model has only 0.65 GFLOPs with 88.1% PCKh@0.5 on MPII\nand our large model has only 2 GFLOPs while its accuracy is competitive with\nthe state-of-the-art large model, i.e., HRNet with 9.5 GFLOPs.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 15:38:38 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zhang", "Wenqiang", ""], ["Fang", "Jiemin", ""], ["Wang", "Xinggang", ""], ["Liu", "Wenyu", ""]]}, {"id": "2012.07098", "submitter": "Erkut Erdem", "authors": "Begum Citamak and Ozan Caglayan and Menekse Kuyu and Erkut Erdem and\n  Aykut Erdem and Pranava Madhyastha and Lucia Specia", "title": "MSVD-Turkish: A Comprehensive Multimodal Dataset for Integrated Vision\n  and Language Research in Turkish", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic generation of video descriptions in natural language, also called\nvideo captioning, aims to understand the visual content of the video and\nproduce a natural language sentence depicting the objects and actions in the\nscene. This challenging integrated vision and language problem, however, has\nbeen predominantly addressed for English. The lack of data and the linguistic\nproperties of other languages limit the success of existing approaches for such\nlanguages. In this paper we target Turkish, a morphologically rich and\nagglutinative language that has very different properties compared to English.\nTo do so, we create the first large scale video captioning dataset for this\nlanguage by carefully translating the English descriptions of the videos in the\nMSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In\naddition to enabling research in video captioning in Turkish, the parallel\nEnglish-Turkish descriptions also enables the study of the role of video\ncontext in (multimodal) machine translation. In our experiments, we build\nmodels for both video captioning and multimodal machine translation and\ninvestigate the effect of different word segmentation approaches and different\nneural architectures to better address the properties of Turkish. We hope that\nthe MSVD-Turkish dataset and the results reported in this work will lead to\nbetter video captioning and multimodal machine translation models for Turkish\nand other morphology rich and agglutinative languages.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 16:51:35 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Citamak", "Begum", ""], ["Caglayan", "Ozan", ""], ["Kuyu", "Menekse", ""], ["Erdem", "Erkut", ""], ["Erdem", "Aykut", ""], ["Madhyastha", "Pranava", ""], ["Specia", "Lucia", ""]]}, {"id": "2012.07101", "submitter": "Kun Zhang", "authors": "Kun Zhang, Rui Wu, Ping Yao, Kai Deng, Ding Li, Renbiao Liu,\n  Chuanguang Yang, Ge Chen, Min Du, Tianyao Zheng", "title": "Learning Heatmap-Style Jigsaw Puzzles Provides Good Pretraining for 2D\n  Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The target of 2D human pose estimation is to locate the keypoints of body\nparts from input 2D images. State-of-the-art methods for pose estimation\nusually construct pixel-wise heatmaps from keypoints as labels for learning\nconvolution neural networks, which are usually initialized randomly or using\nclassification models on ImageNet as their backbones. We note that 2D pose\nestimation task is highly dependent on the contextual relationship between\nimage patches, thus we introduce a self-supervised method for pretraining 2D\npose estimation networks. Specifically, we propose Heatmap-Style Jigsaw Puzzles\n(HSJP) problem as our pretext-task, whose target is to learn the location of\neach patch from an image composed of shuffled patches. During our pretraining\nprocess, we only use images of person instances in MS-COCO, rather than\nintroducing extra and much larger ImageNet dataset. A heatmap-style label for\npatch location is designed and our learning process is in a non-contrastive\nway. The weights learned by HSJP pretext task are utilised as backbones of 2D\nhuman pose estimator, which are then finetuned on MS-COCO human keypoints\ndataset. With two popular and strong 2D human pose estimators, HRNet and\nSimpleBaseline, we evaluate mAP score on both MS-COCO validation and test-dev\ndatasets. Our experiments show that downstream pose estimators with our\nself-supervised pretraining obtain much better performance than those trained\nfrom scratch, and are comparable to those using ImageNet classification models\nas their initial backbones.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 17:04:29 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zhang", "Kun", ""], ["Wu", "Rui", ""], ["Yao", "Ping", ""], ["Deng", "Kai", ""], ["Li", "Ding", ""], ["Liu", "Renbiao", ""], ["Yang", "Chuanguang", ""], ["Chen", "Ge", ""], ["Du", "Min", ""], ["Zheng", "Tianyao", ""]]}, {"id": "2012.07110", "submitter": "Marco Schreyer", "authors": "Marco Schreyer, Chistian Schulze, Damian Borth", "title": "Leaking Sensitive Financial Accounting Data in Plain Sight using Deep\n  Autoencoder Neural Networks", "comments": "8 pages (excl. appendix), 4 Figures, 2 Tables, AAAI-21 Workshop on\n  Knowledge Discovery from Unstructured Data in Financial Services, this paper\n  is the initial accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Nowadays, organizations collect vast quantities of sensitive information in\n`Enterprise Resource Planning' (ERP) systems, such as accounting relevant\ntransactions, customer master data, or strategic sales price information. The\nleakage of such information poses a severe threat for companies as the number\nof incidents and the reputational damage to those experiencing them continue to\nincrease. At the same time, discoveries in deep learning research revealed that\nmachine learning models could be maliciously misused to create new attack\nvectors. Understanding the nature of such attacks becomes increasingly\nimportant for the (internal) audit and fraud examination practice. The creation\nof such an awareness holds in particular for the fraudulent data leakage using\ndeep learning-based steganographic techniques that might remain undetected by\nstate-of-the-art `Computer Assisted Audit Techniques' (CAATs). In this work, we\nintroduce a real-world `threat model' designed to leak sensitive accounting\ndata. In addition, we show that a deep steganographic process, constituted by\nthree neural networks, can be trained to hide such data in unobtrusive\n`day-to-day' images. Finally, we provide qualitative and quantitative\nevaluations on two publicly available real-world payment datasets.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 17:29:53 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Schreyer", "Marco", ""], ["Schulze", "Chistian", ""], ["Borth", "Damian", ""]]}, {"id": "2012.07122", "submitter": "Jie Yang", "authors": "Jie Yang, Yong Shi, Zhiquan Qi", "title": "DFR: Deep Feature Reconstruction for Unsupervised Anomaly Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic detecting anomalous regions in images of objects or textures\nwithout priors of the anomalies is challenging, especially when the anomalies\nappear in very small areas of the images, making difficult-to-detect visual\nvariations, such as defects on manufacturing products. This paper proposes an\neffective unsupervised anomaly segmentation approach that can detect and\nsegment out the anomalies in small and confined regions of images. Concretely,\nwe develop a multi-scale regional feature generator that can generate multiple\nspatial context-aware representations from pre-trained deep convolutional\nnetworks for every subregion of an image. The regional representations not only\ndescribe the local characteristics of corresponding regions but also encode\ntheir multiple spatial context information, making them discriminative and very\nbeneficial for anomaly detection. Leveraging these descriptive regional\nfeatures, we then design a deep yet efficient convolutional autoencoder and\ndetect anomalous regions within images via fast feature reconstruction. Our\nmethod is simple yet effective and efficient. It advances the state-of-the-art\nperformances on several benchmark datasets and shows great potential for real\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 18:30:51 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Yang", "Jie", ""], ["Shi", "Yong", ""], ["Qi", "Zhiquan", ""]]}, {"id": "2012.07123", "submitter": "Emanuela Haller", "authors": "Emanuela Haller, Adina Magda Florea and Marius Leordeanu", "title": "Iterative Knowledge Exchange Between Deep Learning and Space-Time\n  Spectral Clustering for Unsupervised Segmentation in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dual system for unsupervised object segmentation in video, which\nbrings together two modules with complementary properties: a space-time graph\nthat discovers objects in videos and a deep network that learns powerful object\nfeatures. The system uses an iterative knowledge exchange policy. A novel\nspectral space-time clustering process on the graph produces unsupervised\nsegmentation masks passed to the network as pseudo-labels. The net learns to\nsegment in single frames what the graph discovers in video and passes back to\nthe graph strong image-level features that improve its node-level features in\nthe next iteration. Knowledge is exchanged for several cycles until\nconvergence. The graph has one node per each video pixel, but the object\ndiscovery is fast. It uses a novel power iteration algorithm computing the main\nspace-time cluster as the principal eigenvector of a special Feature-Motion\nmatrix without actually computing the matrix. The thorough experimental\nanalysis validates our theoretical claims and proves the effectiveness of the\ncyclical knowledge exchange. We also perform experiments on the supervised\nscenario, incorporating features pretrained with human supervision. We achieve\nstate-of-the-art level on unsupervised and supervised scenarios on four\nchallenging datasets: DAVIS, SegTrack, YouTube-Objects, and DAVSOD.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 18:36:18 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Haller", "Emanuela", ""], ["Florea", "Adina Magda", ""], ["Leordeanu", "Marius", ""]]}, {"id": "2012.07128", "submitter": "Aniketh Manjunath", "authors": "Aniketh Manjunath, Subramanya Jois, and Chandra Sekhar Seelamantula", "title": "Robust Segmentation of Optic Disc and Cup from Fundus Images Using Deep\n  Neural Networks", "comments": "12 pages, 10 figures, 8 tables; Submitted To IEEE Transactions On\n  Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optic disc (OD) and optic cup (OC) are regions of prominent clinical interest\nin a retinal fundus image. They are the primary indicators of a glaucomatous\ncondition. With the advent and success of deep learning for healthcare\nresearch, several approaches have been proposed for the segmentation of\nimportant features in retinal fundus images. We propose a novel approach for\nthe simultaneous segmentation of the OD and OC using a residual encoder-decoder\nnetwork (REDNet) based regional convolutional neural network (RCNN). The\nRED-RCNN is motivated by the Mask RCNN (MRCNN). Performance comparisons with\nthe state-of-the-art techniques and extensive validations on standard publicly\navailable fundus image datasets show that RED-RCNN has superior performance\ncompared with MRCNN. RED-RCNN results in Sensitivity, Specificity, Accuracy,\nPrecision, Dice and Jaccard indices of 95.64%, 99.9%, 99.82%, 95.68%, 95.64%,\n91.65%, respectively, for OD segmentation, and 91.44%, 99.87%, 99.83%, 85.67%,\n87.48%, 78.09%, respectively, for OC segmentation. Further, we perform\ntwo-stage glaucoma severity grading using the cup-to-disc ratio (CDR) computed\nbased on the obtained OD/OC segmentation. The superior segmentation performance\nof RED-RCNN over MRCNN translates to higher accuracy in glaucoma severity\ngrading.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 19:24:53 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Manjunath", "Aniketh", ""], ["Jois", "Subramanya", ""], ["Seelamantula", "Chandra Sekhar", ""]]}, {"id": "2012.07131", "submitter": "Zheng Dong", "authors": "Zheng Dong, Ke Xu, Yin Yang, Hujun Bao, Weiwei Xu, Rynson W.H. Lau", "title": "Location-aware Single Image Reflection Removal", "comments": "10 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel location-aware deep learning-based single image\nreflection removal method. Our network has a reflection detection module to\nregress a probabilistic reflection confidence map, taking multi-scale Laplacian\nfeatures as inputs. This probabilistic map tells whether a region is\nreflection-dominated or transmission-dominated. The novelty is that we use the\nreflection confidence map as the cues for the network to learn how to encode\nthe reflection information adaptively and control the feature flow when\npredicting reflection and transmission layers. The integration of location\ninformation into the network significantly improves the quality of reflection\nremoval results. Besides, a set of learnable Laplacian kernel parameters is\nintroduced to facilitate the extraction of discriminative Laplacian features\nfor reflection detection. We design our network as a recurrent network to\nprogressively refine each iteration's reflection removal results. Extensive\nexperiments verify the superior performance of the proposed method over\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 19:34:35 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Dong", "Zheng", ""], ["Xu", "Ke", ""], ["Yang", "Yin", ""], ["Bao", "Hujun", ""], ["Xu", "Weiwei", ""], ["Lau", "Rynson W. H.", ""]]}, {"id": "2012.07138", "submitter": "Hongming Zhang", "authors": "Hongming Zhang, Yintong Huo, Xinran Zhao, Yangqiu Song, Dan Roth", "title": "Learning Contextual Causality from Time-consecutive Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Causality knowledge is crucial for many artificial intelligence systems.\nConventional textual-based causality knowledge acquisition methods typically\nrequire laborious and expensive human annotations. As a result, their scale is\noften limited. Moreover, as no context is provided during the annotation, the\nresulting causality knowledge records (e.g., ConceptNet) typically do not take\nthe context into consideration. To explore a more scalable way of acquiring\ncausality knowledge, in this paper, we jump out of the textual domain and\ninvestigate the possibility of learning contextual causality from the visual\nsignal. Compared with pure text-based approaches, learning causality from the\nvisual signal has the following advantages: (1) Causality knowledge belongs to\nthe commonsense knowledge, which is rarely expressed in the text but rich in\nvideos; (2) Most events in the video are naturally time-ordered, which provides\na rich resource for us to mine causality knowledge from; (3) All the objects in\nthe video can be used as context to study the contextual property of causal\nrelations. In detail, we first propose a high-quality dataset Vis-Causal and\nthen conduct experiments to demonstrate that with good language and visual\nrepresentation models as well as enough training signals, it is possible to\nautomatically discover meaningful causal knowledge from the videos. Further\nanalysis also shows that the contextual property of causal relations indeed\nexists, taking which into consideration might be crucial if we want to use the\ncausality knowledge in real applications, and the visual signal could serve as\na good resource for learning such contextual causality.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 20:24:48 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zhang", "Hongming", ""], ["Huo", "Yintong", ""], ["Zhao", "Xinran", ""], ["Song", "Yangqiu", ""], ["Roth", "Dan", ""]]}, {"id": "2012.07139", "submitter": "Niclas V\\\"odisch", "authors": "David Dodel, Michael Sch\\\"otz, Niclas V\\\"odisch", "title": "FSOCO: The Formula Student Objects in Context Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the FSOCO dataset, a collaborative dataset for\nvision-based cone detection systems in Formula Student Driverless competitions.\nIt contains human annotated ground truth labels for both bounding boxes and\ninstance-wise segmentation masks. The data buy-in philosophy of FSOCO asks\nstudent teams to contribute to the database first before being granted access\nensuring continuous growth. By providing clear labeling guidelines and tools\nfor a sophisticated raw image selection, new annotations are guaranteed to meet\nthe desired quality. The effectiveness of the approach is shown by comparing\nprediction results of a network trained on FSOCO and its unregulated\npredecessor. The FSOCO dataset can be found at fsoco-dataset.com.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 20:24:48 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 09:19:44 GMT"}, {"version": "v3", "created": "Tue, 25 May 2021 16:34:19 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Dodel", "David", ""], ["Sch\u00f6tz", "Michael", ""], ["V\u00f6disch", "Niclas", ""]]}, {"id": "2012.07175", "submitter": "Chuqing Hu", "authors": "Lang Su, Chuqing Hu, Guofa Li, Dongpu Cao", "title": "MSAF: Multimodal Split Attention Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal learning mimics the reasoning process of the human multi-sensory\nsystem, which is used to perceive the surrounding world. While making a\nprediction, the human brain tends to relate crucial cues from multiple sources\nof information. In this work, we propose a novel multimodal fusion module that\nlearns to emphasize more contributive features across all modalities.\nSpecifically, the proposed Multimodal Split Attention Fusion (MSAF) module\nsplits each modality into channel-wise equal feature blocks and creates a joint\nrepresentation that is used to generate soft attention for each channel across\nthe feature blocks. Further, the MSAF module is designed to be compatible with\nfeatures of various spatial dimensions and sequence lengths, suitable for both\nCNNs and RNNs. Thus, MSAF can be easily added to fuse features of any unimodal\nnetworks and utilize existing pretrained unimodal model weights. To demonstrate\nthe effectiveness of our fusion module, we design three multimodal networks\nwith MSAF for emotion recognition, sentiment analysis, and action recognition\ntasks. Our approach achieves competitive results in each task and outperforms\nother application-specific networks and multimodal fusion benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 22:42:41 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 14:24:23 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Su", "Lang", ""], ["Hu", "Chuqing", ""], ["Li", "Guofa", ""], ["Cao", "Dongpu", ""]]}, {"id": "2012.07176", "submitter": "Reza Esfandiarpoor", "authors": "Reza Esfandiarpoor, Amy Pu, Mohsen Hajabdollahi, Stephen H. Bach", "title": "Extended Few-Shot Learning: Exploiting Existing Resources for Novel\n  Tasks", "comments": "Added the new version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many practical few-shot learning problems, even though labeled examples\nare scarce, there are abundant auxiliary datasets that potentially contain\nuseful information. We propose the problem of extended few-shot learning to\nstudy these scenarios. We then introduce a framework to address the challenges\nof efficiently selecting and effectively using auxiliary data in few-shot image\nclassification. Given a large auxiliary dataset and a notion of semantic\nsimilarity among classes, we automatically select pseudo shots, which are\nlabeled examples from other classes related to the target task. We show that\nnaive approaches, such as (1) modeling these additional examples the same as\nthe target task examples or (2) using them to learn features via transfer\nlearning, only increase accuracy by a modest amount. Instead, we propose a\nmasking module that adjusts the features of auxiliary data to be more similar\nto those of the target classes. We show that this masking module performs\nbetter than naively modeling the support examples and transfer learning by 4.68\nand 6.03 percentage points, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 22:45:44 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 15:53:04 GMT"}, {"version": "v3", "created": "Sat, 3 Jul 2021 19:47:58 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Esfandiarpoor", "Reza", ""], ["Pu", "Amy", ""], ["Hajabdollahi", "Mohsen", ""], ["Bach", "Stephen H.", ""]]}, {"id": "2012.07177", "submitter": "Golnaz Ghiasi", "authors": "Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin\n  D. Cubuk, Quoc V. Le, Barret Zoph", "title": "Simple Copy-Paste is a Strong Data Augmentation Method for Instance\n  Segmentation", "comments": "Accepted at CVPR 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building instance segmentation models that are data-efficient and can handle\nrare object categories is an important challenge in computer vision. Leveraging\ndata augmentations is a promising direction towards addressing this challenge.\nHere, we perform a systematic study of the Copy-Paste augmentation ([13, 12])\nfor instance segmentation where we randomly paste objects onto an image. Prior\nstudies on Copy-Paste relied on modeling the surrounding visual context for\npasting the objects. However, we find that the simple mechanism of pasting\nobjects randomly is good enough and can provide solid gains on top of strong\nbaselines. Furthermore, we show Copy-Paste is additive with semi-supervised\nmethods that leverage extra data through pseudo labeling (e.g. self-training).\nOn COCO instance segmentation, we achieve 49.1 mask AP and 57.3 box AP, an\nimprovement of +0.6 mask AP and +1.5 box AP over the previous state-of-the-art.\nWe further demonstrate that Copy-Paste can lead to significant improvements on\nthe LVIS benchmark. Our baseline model outperforms the LVIS 2020 Challenge\nwinning entry by +3.6 mask AP on rare categories.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 22:59:45 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 05:13:04 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Ghiasi", "Golnaz", ""], ["Cui", "Yin", ""], ["Srinivas", "Aravind", ""], ["Qian", "Rui", ""], ["Lin", "Tsung-Yi", ""], ["Cubuk", "Ekin D.", ""], ["Le", "Quoc V.", ""], ["Zoph", "Barret", ""]]}, {"id": "2012.07181", "submitter": "Chenglin Yang", "authors": "Chenglin Yang, Yilin Wang, Jianming Zhang, He Zhang, Zhe Lin, Alan\n  Yuille", "title": "Meticulous Object Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with common image segmentation tasks targeted at low-resolution\nimages, higher resolution detailed image segmentation receives much less\nattention. In this paper, we propose and study a task named Meticulous Object\nSegmentation (MOS), which is focused on segmenting well-defined foreground\nobjects with elaborate shapes in high resolution images (e.g. 2k - 4k). To this\nend, we propose the MeticulousNet which leverages a dedicated decoder to\ncapture the object boundary details. Specifically, we design a Hierarchical\nPoint-wise Refining (HierPR) block to better delineate object boundaries, and\nreformulate the decoding process as a recursive coarse to fine refinement of\nthe object mask. To evaluate segmentation quality near object boundaries, we\npropose the Meticulosity Quality (MQ) score considering both the mask coverage\nand boundary precision. In addition, we collect a MOS benchmark dataset\nincluding 600 high quality images with complex objects. We provide\ncomprehensive empirical evidence showing that MeticulousNet can reveal\npixel-accurate segmentation boundaries and is superior to state-of-the-art\nmethods for high resolution object segmentation tasks.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 23:38:40 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Yang", "Chenglin", ""], ["Wang", "Yilin", ""], ["Zhang", "Jianming", ""], ["Zhang", "He", ""], ["Lin", "Zhe", ""], ["Yuille", "Alan", ""]]}, {"id": "2012.07192", "submitter": "Liang Lin", "authors": "Qingxing Cao and Bailin Li and Xiaodan Liang and Keze Wang and Liang\n  Lin", "title": "Knowledge-Routed Visual Question Reasoning: Challenges for Deep\n  Representation Embedding", "comments": "To appear in TNNLS 2021. Considering that a desirable VQA model\n  should correctly perceive the image context, understand the question, and\n  incorporate its learned knowledge, our proposed dataset aims to cutoff the\n  shortcut learning exploited by the current deep embedding models and push the\n  research boundary of the knowledge-based visual question reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though beneficial for encouraging the Visual Question Answering (VQA) models\nto discover the underlying knowledge by exploiting the input-output correlation\nbeyond image and text contexts, the existing knowledge VQA datasets are mostly\nannotated in a crowdsource way, e.g., collecting questions and external reasons\nfrom different users via the internet. In addition to the challenge of\nknowledge reasoning, how to deal with the annotator bias also remains unsolved,\nwhich often leads to superficial over-fitted correlations between questions and\nanswers. To address this issue, we propose a novel dataset named\nKnowledge-Routed Visual Question Reasoning for VQA model evaluation.\nConsidering that a desirable VQA model should correctly perceive the image\ncontext, understand the question, and incorporate its learned knowledge, our\nproposed dataset aims to cutoff the shortcut learning exploited by the current\ndeep embedding models and push the research boundary of the knowledge-based\nvisual question reasoning. Specifically, we generate the question-answer pair\nbased on both the Visual Genome scene graph and an external knowledge base with\ncontrolled programs to disentangle the knowledge from other biases. The\nprograms can select one or two triplets from the scene graph or knowledge base\nto push multi-step reasoning, avoid answer ambiguity, and balanced the answer\ndistribution. In contrast to the existing VQA datasets, we further imply the\nfollowing two major constraints on the programs to incorporate knowledge\nreasoning: i) multiple knowledge triplets can be related to the question, but\nonly one knowledge relates to the image object. This can enforce the VQA model\nto correctly perceive the image instead of guessing the knowledge based on the\ngiven question solely; ii) all questions are based on different knowledge, but\nthe candidate answers are the same for both the training and test sets.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 00:33:44 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Cao", "Qingxing", ""], ["Li", "Bailin", ""], ["Liang", "Xiaodan", ""], ["Wang", "Keze", ""], ["Lin", "Liang", ""]]}, {"id": "2012.07206", "submitter": "Kumar Abhishek", "authors": "Zahra Mirikharaji, Kumar Abhishek, Saeed Izadi, Ghassan Hamarneh", "title": "D-LEMA: Deep Learning Ensembles from Multiple Annotations -- Application\n  to Skin Lesion Segmentation", "comments": "Accepted at the CVPR 2021 Sixth ISIC Skin Image Analysis Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image segmentation annotations suffer from inter- and intra-observer\nvariations even among experts due to intrinsic differences in human annotators\nand ambiguous boundaries. Leveraging a collection of annotators' opinions for\nan image is an interesting way of estimating a gold standard. Although training\ndeep models in a supervised setting with a single annotation per image has been\nextensively studied, generalizing their training to work with datasets\ncontaining multiple annotations per image remains a fairly unexplored problem.\nIn this paper, we propose an approach to handle annotators' disagreements when\ntraining a deep model. To this end, we propose an ensemble of Bayesian fully\nconvolutional networks (FCNs) for the segmentation task by considering two\nmajor factors in the aggregation of multiple ground truth annotations: (1)\nhandling contradictory annotations in the training data originating from\ninter-annotator disagreements and (2) improving confidence calibration through\nthe fusion of base models' predictions. We demonstrate the superior performance\nof our approach on the ISIC Archive and explore the generalization performance\nof our proposed method by cross-dataset evaluation on the PH2 and DermoFit\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 01:51:22 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 01:31:40 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Mirikharaji", "Zahra", ""], ["Abhishek", "Kumar", ""], ["Izadi", "Saeed", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "2012.07208", "submitter": "Johan \\\"Ofverstedt", "authors": "Johan \\\"Ofverstedt, Joakim Lindblad, Nata\\v{s}a Sladoje", "title": "INSPIRE: Intensity and Spatial Information-Based Deformable Image\n  Registration", "comments": "13 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present INSPIRE, a top-performing general-purpose method for deformable\nimage registration. INSPIRE extends our existing symmetric registration\nframework based on distances combining intensity and spatial information to an\nelastic B-splines based transformation model. We also present several\ntheoretical and algorithmic improvements which provide high computational\nefficiency and thereby applicability of the framework in a wide range of real\nscenarios. We show that the proposed method delivers both highly accurate as\nwell as stable and robust registration results. We evaluate the method on a\nsynthetic dataset created from retinal images, consisting of thin networks of\nvessels, where INSPIRE exhibits excellent performance, substantially\noutperforming the reference methods. We also evaluate the method on four\nbenchmark datasets of 3D images of brains, for a total of 2088 pairwise\nregistrations; a comparison with 15 other state-of-the-art methods reveals that\nINSPIRE provides the best overall performance. Code is available at\ngithub.com/MIDA-group/inspire.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 01:51:59 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["\u00d6fverstedt", "Johan", ""], ["Lindblad", "Joakim", ""], ["Sladoje", "Nata\u0161a", ""]]}, {"id": "2012.07236", "submitter": "Fan Lyu", "authors": "Fan Lyu, Shuai Wang, Wei Feng, Zihan Ye, Fuyuan Hu, Song Wang", "title": "Multi-Domain Multi-Task Rehearsal for Lifelong Learning", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rehearsal, seeking to remind the model by storing old knowledge in lifelong\nlearning, is one of the most effective ways to mitigate catastrophic\nforgetting, i.e., biased forgetting of previous knowledge when moving to new\ntasks. However, the old tasks of the most previous rehearsal-based methods\nsuffer from the unpredictable domain shift when training the new task. This is\nbecause these methods always ignore two significant factors. First, the Data\nImbalance between the new task and old tasks that makes the domain of old tasks\nprone to shift. Second, the Task Isolation among all tasks will make the domain\nshift toward unpredictable directions; To address the unpredictable domain\nshift, in this paper, we propose Multi-Domain Multi-Task (MDMT) rehearsal to\ntrain the old tasks and new task parallelly and equally to break the isolation\namong tasks. Specifically, a two-level angular margin loss is proposed to\nencourage the intra-class/task compactness and inter-class/task discrepancy,\nwhich keeps the model from domain chaos. In addition, to further address domain\nshift of the old tasks, we propose an optional episodic distillation loss on\nthe memory to anchor the knowledge for each old task. Experiments on benchmark\ndatasets validate the proposed approach can effectively mitigate the\nunpredictable domain shift.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 03:36:25 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Lyu", "Fan", ""], ["Wang", "Shuai", ""], ["Feng", "Wei", ""], ["Ye", "Zihan", ""], ["Hu", "Fuyuan", ""], ["Wang", "Song", ""]]}, {"id": "2012.07237", "submitter": "Muyi Sun", "authors": "Muyi Sun, Zeyi Yao, Guanhong Zhang", "title": "Accurate Cell Segmentation in Digital Pathology Images via Attention\n  Enforced Networks", "comments": "6 pages. Accepted by ICPR2020 in the first round", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic cell segmentation is an essential step in the pipeline of\ncomputer-aided diagnosis (CAD), such as the detection and grading of breast\ncancer. Accurate segmentation of cells can not only assist the pathologists to\nmake a more precise diagnosis, but also save much time and labor. However, this\ntask suffers from stain variation, cell inhomogeneous intensities, background\nclutters and cells from different tissues. To address these issues, we propose\nan Attention Enforced Network (AENet), which is built on spatial attention\nmodule and channel attention module, to integrate local features with global\ndependencies and weight effective channels adaptively. Besides, we introduce a\nfeature fusion branch to bridge high-level and low-level features. Finally, the\nmarker controlled watershed algorithm is applied to post-process the predicted\nsegmentation maps for reducing the fragmented regions. In the test stage, we\npresent an individual color normalization method to deal with the stain\nvariation problem. We evaluate this model on the MoNuSeg dataset. The\nquantitative comparisons against several prior methods demonstrate the\nsuperiority of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 03:39:33 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 09:41:49 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Sun", "Muyi", ""], ["Yao", "Zeyi", ""], ["Zhang", "Guanhong", ""]]}, {"id": "2012.07241", "submitter": "Mingyue Yang", "authors": "Mingyue Yang, Yuxin Wen, Weikai Chen, Yongwei Chen, Kui Jia", "title": "Deep Optimized Priors for 3D Shape Modeling and Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many learning-based approaches have difficulty scaling to unseen data, as the\ngenerality of its learned prior is limited to the scale and variations of the\ntraining samples. This holds particularly true with 3D learning tasks, given\nthe sparsity of 3D datasets available. We introduce a new learning framework\nfor 3D modeling and reconstruction that greatly improves the generalization\nability of a deep generator. Our approach strives to connect the good ends of\nboth learning-based and optimization-based methods. In particular, unlike the\ncommon practice that fixes the pre-trained priors at test time, we propose to\nfurther optimize the learned prior and latent code according to the input\nphysical measurements after the training. We show that the proposed strategy\neffectively breaks the barriers constrained by the pre-trained priors and could\nlead to high-quality adaptation to unseen data. We realize our framework using\nthe implicit surface representation and validate the efficacy of our approach\nin a variety of challenging tasks that take highly sparse or collapsed\nobservations as input. Experimental results show that our approach compares\nfavorably with the state-of-the-art methods in terms of both generality and\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 03:56:31 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Yang", "Mingyue", ""], ["Wen", "Yuxin", ""], ["Chen", "Weikai", ""], ["Chen", "Yongwei", ""], ["Jia", "Kui", ""]]}, {"id": "2012.07248", "submitter": "Bo Pang", "authors": "Bo Pang, Yizhuo Li, Jiefeng Li, Muchen Li, Hanwen Cao, Cewu Lu", "title": "TDAF: Top-Down Attention Framework for Vision Tasks", "comments": "Conference paper in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Human attention mechanisms often work in a top-down manner, yet it is not\nwell explored in vision research. Here, we propose the Top-Down Attention\nFramework (TDAF) to capture top-down attentions, which can be easily adopted in\nmost existing models. The designed Recursive Dual-Directional Nested Structure\nin it forms two sets of orthogonal paths, recursive and structural ones, where\nbottom-up spatial features and top-down attention features are extracted\nrespectively. Such spatial and attention features are nested deeply, therefore,\nthe proposed framework works in a mixed top-down and bottom-up manner.\nEmpirical evidence shows that our TDAF can capture effective stratified\nattention information and boost performance. ResNet with TDAF achieves 2.0%\nimprovements on ImageNet. For object detection, the performance is improved by\n2.7% AP over FCOS. For pose estimation, TDAF improves the baseline by 1.6%. And\nfor action recognition, the 3D-ResNet adopting TDAF achieves improvements of\n1.7% accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 04:19:13 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Pang", "Bo", ""], ["Li", "Yizhuo", ""], ["Li", "Jiefeng", ""], ["Li", "Muchen", ""], ["Cao", "Hanwen", ""], ["Lu", "Cewu", ""]]}, {"id": "2012.07261", "submitter": "Mingchao Li", "authors": "Mingchao Li, Yuhan Zhang, Zexuan Ji, Keren Xie, Songtao Yuan, Qinghuai\n  Liu and Qiang Chen", "title": "IPN-V2 and OCTA-500: Methodology and Dataset for Retinal Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical coherence tomography angiography (OCTA) is a novel imaging modality\nthat allows a micron-level resolution to present the three-dimensional\nstructure of the retinal vascular. In our previous work, a 3D-to-2D image\nprojection network (IPN) was proposed for retinal vessel (RV) and foveal\navascular zone (FAZ) segmentations in OCTA images. One of its advantages is\nthat the segmentation results are directly from the original volumes without\nusing any projection images and retinal layer segmentation. In this work, we\npropose image projection network V2 (IPN-V2), extending IPN by adding a plane\nperceptron to enhance the perceptron ability in the horizontal direction. We\nalso propose IPN-V2+, as a supplement of the IPN-V2, by introducing a global\nretraining process to overcome the \"checkerboard effect\". Besides, we propose a\nnew multi-modality dataset, dubbed OCTA-500. It contains 500 subjects with two\nfield of view (FOV) types, including OCT and OCTA volumes, six types of\nprojections, four types of text labels and two types of pixel-level labels. The\ndataset contains more than 360K images with a size of about 80GB. To the best\nof our knowledge, it is currently the largest OCTA dataset with the abundant\ninformation. Finally, we perform a thorough evaluation of the performance of\nIPN-V2 on the OCTA-500 dataset. The experimental results demonstrate that our\nproposed IPN-V2 performs better than IPN and other deep learning methods in RV\nsegmentation and FAZ segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 05:20:29 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Li", "Mingchao", ""], ["Zhang", "Yuhan", ""], ["Ji", "Zexuan", ""], ["Xie", "Keren", ""], ["Yuan", "Songtao", ""], ["Liu", "Qinghuai", ""], ["Chen", "Qiang", ""]]}, {"id": "2012.07262", "submitter": "Jiafa He", "authors": "Jiafa He, Chengwei Pan, Can Yang, Ming Zhang, Yang Wang, Xiaowei Zhou\n  and Yizhou Yu", "title": "Learning Hybrid Representations for Automatic 3D Vessel Centerline\n  Extraction", "comments": null, "journal-ref": "MICCAI 2020", "doi": "10.1007/978-3-030-59725-2_3", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic blood vessel extraction from 3D medical images is crucial for\nvascular disease diagnoses. Existing methods based on convolutional neural\nnetworks (CNNs) may suffer from discontinuities of extracted vessels when\nsegmenting such thin tubular structures from 3D images. We argue that\npreserving the continuity of extracted vessels requires to take into account\nthe global geometry. However, 3D convolutions are computationally inefficient,\nwhich prohibits the 3D CNNs from sufficiently large receptive fields to capture\nthe global cues in the entire image. In this work, we propose a hybrid\nrepresentation learning approach to address this challenge. The main idea is to\nuse CNNs to learn local appearances of vessels in image crops while using\nanother point-cloud network to learn the global geometry of vessels in the\nentire image. In inference, the proposed approach extracts local segments of\nvessels using CNNs, classifies each segment based on global geometry using the\npoint-cloud network, and finally connects all the segments that belong to the\nsame vessel using the shortest-path algorithm. This combination results in an\nefficient, fully-automatic and template-free approach to centerline extraction\nfrom 3D images. We validate the proposed approach on CTA datasets and\ndemonstrate its superior performance compared to both traditional and CNN-based\nbaselines.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 05:22:49 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["He", "Jiafa", ""], ["Pan", "Chengwei", ""], ["Yang", "Can", ""], ["Zhang", "Ming", ""], ["Wang", "Yang", ""], ["Zhou", "Xiaowei", ""], ["Yu", "Yizhou", ""]]}, {"id": "2012.07287", "submitter": "Pedro Savarese", "authors": "Pedro Savarese and Sunnie S. Y. Kim and Michael Maire and Greg\n  Shakhnarovich and David McAllester", "title": "Information-Theoretic Segmentation by Inpainting Error Maximization", "comments": "Published as a conference paper at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study image segmentation from an information-theoretic perspective,\nproposing a novel adversarial method that performs unsupervised segmentation by\npartitioning images into maximally independent sets. More specifically, we\ngroup image pixels into foreground and background, with the goal of minimizing\npredictability of one set from the other. An easily computed loss drives a\ngreedy search process to maximize inpainting error over these partitions. Our\nmethod does not involve training deep networks, is computationally cheap,\nclass-agnostic, and even applicable in isolation to a single unlabeled image.\nExperiments demonstrate that it achieves a new state-of-the-art in unsupervised\nsegmentation quality, while being substantially faster and more general than\ncompeting approaches.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 06:42:27 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 20:22:13 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 17:28:51 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Savarese", "Pedro", ""], ["Kim", "Sunnie S. Y.", ""], ["Maire", "Michael", ""], ["Shakhnarovich", "Greg", ""], ["McAllester", "David", ""]]}, {"id": "2012.07288", "submitter": "Haitian Zheng", "authors": "Haitian Zheng, Zhe Lin, Jingwan Lu, Scott Cohen, Jianming Zhang, Ning\n  Xu, Jiebo Luo", "title": "Semantic Layout Manipulation with High-Resolution Sparse Attention", "comments": "22 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We tackle the problem of semantic image layout manipulation, which aims to\nmanipulate an input image by editing its semantic label map. A core problem of\nthis task is how to transfer visual details from the input images to the new\nsemantic layout while making the resulting image visually realistic. Recent\nwork on learning cross-domain correspondence has shown promising results for\nglobal layout transfer with dense attention-based warping. However, this method\ntends to lose texture details due to the resolution limitation and the lack of\nsmoothness constraint of correspondence. To adapt this paradigm for the layout\nmanipulation task, we propose a high-resolution sparse attention module that\neffectively transfers visual details to new layouts at a resolution up to\n512x512. To further improve visual quality, we introduce a novel generator\narchitecture consisting of a semantic encoder and a two-stage decoder for\ncoarse-to-fine synthesis. Experiments on the ADE20k and Places365 datasets\ndemonstrate that our proposed approach achieves substantial improvements over\nthe existing inpainting and layout manipulation methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 06:50:43 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 00:00:29 GMT"}, {"version": "v3", "created": "Fri, 16 Apr 2021 20:09:17 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zheng", "Haitian", ""], ["Lin", "Zhe", ""], ["Lu", "Jingwan", ""], ["Cohen", "Scott", ""], ["Zhang", "Jianming", ""], ["Xu", "Ning", ""], ["Luo", "Jiebo", ""]]}, {"id": "2012.07290", "submitter": "Chaozheng Wu", "authors": "Chaozheng Wu, Lin Sun, Xun Xu, Kui Jia", "title": "Learning Category-level Shape Saliency via Deep Implicit Surface\n  Networks", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper is motivated from a fundamental curiosity on what defines a\ncategory of object shapes. For example, we may have the common knowledge that a\nplane has wings, and a chair has legs. Given the large shape variations among\ndifferent instances of a same category, we are formally interested in\ndeveloping a quantity defined for individual points on a continuous object\nsurface; the quantity specifies how individual surface points contribute to the\nformation of the shape as the category. We term such a quantity as\ncategory-level shape saliency or shape saliency for short. Technically, we\npropose to learn saliency maps for shape instances of a same category from a\ndeep implicit surface network; sensible saliency scores for sampled points in\nthe implicit surface field are predicted by constraining the capacity of input\nlatent code. We also enhance the saliency prediction with an additional loss of\ncontrastive training. We expect such learned surface maps of shape saliency to\nhave the properties of smoothness, symmetry, and semantic representativeness.\nWe verify these properties by comparing our method with alternative ways of\nsaliency computation. Notably, we show that by leveraging the learned shape\nsaliency, we are able to reconstruct either category-salient or\ninstance-specific parts of object surfaces; semantic representativeness of the\nlearned saliency is also reflected in its efficacy to guide the selection of\nsurface points for better point cloud classification.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 06:54:42 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Wu", "Chaozheng", ""], ["Sun", "Lin", ""], ["Xu", "Xun", ""], ["Jia", "Kui", ""]]}, {"id": "2012.07297", "submitter": "Jian Liang", "authors": "Jian Liang and Dapeng Hu and Yunbo Wang and Ran He and Jiashi Feng", "title": "Source Data-absent Unsupervised Domain Adaptation through Hypothesis\n  Transfer and Labeling Transfer", "comments": "More interesting results are further shown in\n  https://github.com/tim-learn/SHOT-plus/blob/master/supp/shot%2B%2B_supp.pdf.\n  arXiv admin note: text overlap with arXiv:2002.08546", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims to transfer knowledge from a\nrelated but different well-labeled source domain to a new unlabeled target\ndomain. Most existing UDA methods require access to the source data, and thus\nare not applicable when the data are confidential and not shareable due to\nprivacy concerns. This paper aims to tackle a realistic setting with only a\nclassification model available trained over, instead of accessing to, the\nsource data. To effectively utilize the source model for adaptation, we propose\na novel approach called Source HypOthesis Transfer (SHOT), which learns the\nfeature extraction module for the target domain by fitting the target data\nfeatures to the frozen source classification module (representing\nclassification hypothesis). Specifically, SHOT exploits both information\nmaximization and self-supervised learning for the feature extraction module\nlearning to ensure the target features are implicitly aligned with the features\nof unseen source data via the same hypothesis. Furthermore, we propose a new\nlabeling transfer strategy, which separates the target data into two splits\nbased on the confidence of predictions (labeling information), and then employ\nsemi-supervised learning to improve the accuracy of less-confident predictions\nin the target domain. We denote labeling transfer as SHOT++ if the predictions\nare obtained by SHOT. Extensive experiments on both digit classification and\nobject recognition tasks show that SHOT and SHOT++ achieve results surpassing\nor comparable to the state-of-the-arts, demonstrating the effectiveness of our\napproaches for various visual domain adaptation problems. Code will be\navailable at \\url{https://github.com/tim-learn/SHOT-plus}.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 07:28:50 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 09:32:39 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Liang", "Jian", ""], ["Hu", "Dapeng", ""], ["Wang", "Yunbo", ""], ["He", "Ran", ""], ["Feng", "Jiashi", ""]]}, {"id": "2012.07304", "submitter": "Srishti Goel Ms.", "authors": "Neeraj Kumar, Srishti Goel, Ankur Narang, Brejesh Lall", "title": "Multi Modal Adaptive Normalization for Audio to Video Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Speech-driven facial video generation has been a complex problem due to its\nmulti-modal aspects namely audio and video domain. The audio comprises lots of\nunderlying features such as expression, pitch, loudness, prosody(speaking\nstyle) and facial video has lots of variability in terms of head movement, eye\nblinks, lip synchronization and movements of various facial action units along\nwith temporal smoothness. Synthesizing highly expressive facial videos from the\naudio input and static image is still a challenging task for generative\nadversarial networks. In this paper, we propose a multi-modal adaptive\nnormalization(MAN) based architecture to synthesize a talking person video of\narbitrary length using as input: an audio signal and a single image of a\nperson. The architecture uses the multi-modal adaptive normalization, keypoint\nheatmap predictor, optical flow predictor and class activation map[58] based\nlayers to learn movements of expressive facial components and hence generates a\nhighly expressive talking-head video of the given person. The multi-modal\nadaptive normalization uses the various features of audio and video such as Mel\nspectrogram, pitch, energy from audio signals and predicted keypoint\nheatmap/optical flow and a single image to learn the respective affine\nparameters to generate highly expressive video. Experimental evaluation\ndemonstrates superior performance of the proposed method as compared to\nRealistic Speech-Driven Facial Animation with GANs(RSDGAN) [53], Speech2Vid\n[10], and other approaches, on multiple quantitative metrics including: SSIM\n(structural similarity index), PSNR (peak signal to noise ratio), CPBD (image\nsharpness), WER(word error rate), blinks/sec and LMD(landmark distance).\nFurther, qualitative evaluation and Online Turing tests demonstrate the\nefficacy of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 07:39:45 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Kumar", "Neeraj", ""], ["Goel", "Srishti", ""], ["Narang", "Ankur", ""], ["Lall", "Brejesh", ""]]}, {"id": "2012.07315", "submitter": "Silas {\\O}rting", "authors": "Silas Nyboe {\\O}rting, Hans Jacob Teglbj{\\ae}rg Stephensen, Jon\n  Sporring", "title": "Morphology on categorical distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The categorical distribution is a natural representation of uncertainty in\nmulti-class segmentations. In the two-class case the categorical distribution\nreduces to the Bernoulli distribution, for which grayscale morphology provides\na range of useful operations. In the general case, applying morphological\noperations on uncertain multi-class segmentations is not straightforward as an\nimage of categorical distributions is not a complete lattice. Although\nmorphology on color images has received wide attention, this is not so for\ncolor-coded or categorical images and even less so for images of categorical\ndistributions. In this work, we establish a set of requirements for morphology\non categorical distributions by combining classic morphology with a\nprobabilistic view. We then define operators respecting these requirements,\nintroduce protected operations on categorical distributions and illustrate the\nutility of these operators on two example tasks: modeling annotator bias in\nbrain tumor segmentations and segmenting vesicle instances from the predictions\nof a multi-class U-Net.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 07:54:00 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["\u00d8rting", "Silas Nyboe", ""], ["Stephensen", "Hans Jacob Teglbj\u00e6rg", ""], ["Sporring", "Jon", ""]]}, {"id": "2012.07332", "submitter": "Martin Charachon", "authors": "Martin Charachon, C\\'eline Hudelot, Paul-Henry Courn\\`ede, Camille\n  Ruppli, Roberto Ardon", "title": "Combining Similarity and Adversarial Learning to Generate Visual\n  Explanation: Application to Medical Image Classification", "comments": "To be published in ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Explaining decisions of black-box classifiers is paramount in sensitive\ndomains such as medical imaging since clinicians confidence is necessary for\nadoption. Various explanation approaches have been proposed, among which\nperturbation based approaches are very promising. Within this class of methods,\nwe leverage a learning framework to produce our visual explanations method.\nFrom a given classifier, we train two generators to produce from an input image\nthe so called similar and adversarial images. The similar image shall be\nclassified as the input image whereas the adversarial shall not. Visual\nexplanation is built as the difference between these two generated images.\nUsing metrics from the literature, our method outperforms state-of-the-art\napproaches. The proposed approach is model-agnostic and has a low computation\nburden at prediction time. Thus, it is adapted for real-time systems. Finally,\nwe show that random geometric augmentations applied to the original image play\na regularization role that improves several previously proposed explanation\nmethods. We validate our approach on a large chest X-ray database.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 08:34:12 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Charachon", "Martin", ""], ["Hudelot", "C\u00e9line", ""], ["Courn\u00e8de", "Paul-Henry", ""], ["Ruppli", "Camille", ""], ["Ardon", "Roberto", ""]]}, {"id": "2012.07333", "submitter": "Chao Zeng", "authors": "Chao Zeng, Sam Kwong", "title": "Intrinsic Image Captioning Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The image captioning task is about to generate suitable descriptions from\nimages. For this task there can be several challenges such as accuracy, fluency\nand diversity. However there are few metrics that can cover all these\nproperties while evaluating results of captioning models.In this paper we first\nconduct a comprehensive investigation on contemporary metrics. Motivated by the\nauto-encoder mechanism and the research advances of word embeddings we propose\na learning based metrics for image captioning, which we call Intrinsic Image\nCaptioning Evaluation(I2CE). We select several state-of-the-art image\ncaptioning models and test their performances on MS COCO dataset with respects\nto both contemporary metrics and the proposed I2CE. Experiment results show\nthat our proposed method can keep robust performance and give more flexible\nscores to candidate captions when encountered with semantic similar expression\nor less aligned semantics. On this concern the proposed metric could serve as a\nnovel indicator on the intrinsic information between captions, which may be\ncomplementary to the existing ones.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 08:36:05 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zeng", "Chao", ""], ["Kwong", "Sam", ""]]}, {"id": "2012.07340", "submitter": "Radu P Horaud", "authors": "Diana Mateus, Radu Horaud, David Knossow, Fabio Cuzzolin and Edmond\n  Boyer", "title": "Articulated Shape Matching Using Laplacian Eigenfunctions and\n  Unsupervised Point Registration", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition, 2008", "doi": "10.1109/CVPR.2008.4587538", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Matching articulated shapes represented by voxel-sets reduces to maximal\nsub-graph isomorphism when each set is described by a weighted graph. Spectral\ngraph theory can be used to map these graphs onto lower dimensional spaces and\nmatch shapes by aligning their embeddings in virtue of their invariance to\nchange of pose. Classical graph isomorphism schemes relying on the ordering of\nthe eigenvalues to align the eigenspaces fail when handling large data-sets or\nnoisy data. We derive a new formulation that finds the best alignment between\ntwo congruent $K$-dimensional sets of points by selecting the best subset of\neigenfunctions of the Laplacian matrix. The selection is done by matching\neigenfunction signatures built with histograms, and the retained set provides a\nsmart initialization for the alignment problem with a considerable impact on\nthe overall performance. Dense shape matching casted into graph matching\nreduces then, to point registration of embeddings under orthogonal\ntransformations; the registration is solved using the framework of unsupervised\nclustering and the EM algorithm. Maximal subset matching of non identical\nshapes is handled by defining an appropriate outlier class. Experimental\nresults on challenging examples show how the algorithm naturally treats changes\nof topology, shape variations and different sampling densities.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 08:49:25 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Mateus", "Diana", ""], ["Horaud", "Radu", ""], ["Knossow", "David", ""], ["Cuzzolin", "Fabio", ""], ["Boyer", "Edmond", ""]]}, {"id": "2012.07350", "submitter": "Xuan Jin", "authors": "Xuan Jin, Wei Su, Rong Zhang, Yuan He, Hui Xue", "title": "The Open Brands Dataset: Unified brand detection and recognition at\n  scale", "comments": "ICASSP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intellectual property protection(IPP) have received more and more attention\nrecently due to the development of the global e-commerce platforms. brand\nrecognition plays a significant role in IPP. Recent studies for brand\nrecognition and detection are based on small-scale datasets that are not\ncomprehensive enough when exploring emerging deep learning techniques.\nMoreover, it is challenging to evaluate the true performance of brand detection\nmethods in realistic and open scenes. In order to tackle these problems, we\nfirst define the special issues of brand detection and recognition compared\nwith generic object detection. Second, a novel brands benchmark called \"Open\nBrands\" is established. The dataset contains 1,437,812 images which have brands\nand 50,000 images without any brand. The part with brands in Open Brands\ncontains 3,113,828 instances annotated in 3 dimensions: 4 types, 559 brands and\n1216 logos. To the best of our knowledge, it is the largest dataset for brand\ndetection and recognition with rich annotations. We provide in-depth\ncomprehensive statistics about the dataset, validate the quality of the\nannotations and study how the performance of many modern models evolves with an\nincreasing amount of training data. Third, we design a network called \"Brand\nNet\" to handle brand recognition. Brand Net gets state-of-art mAP on Open Brand\ncompared with existing detection methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 09:06:42 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Jin", "Xuan", ""], ["Su", "Wei", ""], ["Zhang", "Rong", ""], ["He", "Yuan", ""], ["Xue", "Hui", ""]]}, {"id": "2012.07356", "submitter": "Xiaoyang Lyu", "authors": "Xiaoyang Lyu, Liang Liu, Mengmeng Wang, Xin Kong, Lina Liu, Yong Liu,\n  Xinxin Chen, Yi Yuan", "title": "HR-Depth: High Resolution Self-Supervised Monocular Depth Estimation", "comments": "9 pages, 5 figures. Accepted to 35th AAAI Conference on Artificial\n  Intelligence (AAAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised learning shows great potential in monoculardepth estimation,\nusing image sequences as the only source ofsupervision. Although people try to\nuse the high-resolutionimage for depth estimation, the accuracy of prediction\nhasnot been significantly improved. In this work, we find thecore reason comes\nfrom the inaccurate depth estimation inlarge gradient regions, making the\nbilinear interpolation er-ror gradually disappear as the resolution increases.\nTo obtainmore accurate depth estimation in large gradient regions, itis\nnecessary to obtain high-resolution features with spatialand semantic\ninformation. Therefore, we present an improvedDepthNet, HR-Depth, with two\neffective strategies: (1) re-design the skip-connection in DepthNet to get\nbetter high-resolution features and (2) propose feature fusion\nSqueeze-and-Excitation(fSE) module to fuse feature more efficiently.Using\nResnet-18 as the encoder, HR-Depth surpasses all pre-vious\nstate-of-the-art(SoTA) methods with the least param-eters at both high and low\nresolution. Moreover, previousstate-of-the-art methods are based on fairly\ncomplex and deepnetworks with a mass of parameters which limits their\nrealapplications. Thus we also construct a lightweight networkwhich uses\nMobileNetV3 as encoder. Experiments show thatthe lightweight network can\nperform on par with many largemodels like Monodepth2 at high-resolution with\nonly20%parameters. All codes and models will be available at\nhttps://github.com/shawLyu/HR-Depth.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 09:15:15 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Lyu", "Xiaoyang", ""], ["Liu", "Liang", ""], ["Wang", "Mengmeng", ""], ["Kong", "Xin", ""], ["Liu", "Lina", ""], ["Liu", "Yong", ""], ["Chen", "Xinxin", ""], ["Yuan", "Yi", ""]]}, {"id": "2012.07386", "submitter": "Marylou Gabri\\'e", "authors": "Hannah Lawrence, David A. Barmherzig, Henry Li, Michael Eickenberg and\n  Marylou Gabri\\'e", "title": "Phase Retrieval with Holography and Untrained Priors: Tackling the\n  Challenges of Low-Photon Nanoscale Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.optics stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase retrieval is the inverse problem of recovering a signal from\nmagnitude-only Fourier measurements, and underlies numerous imaging modalities,\nsuch as Coherent Diffraction Imaging (CDI). A variant of this setup, known as\nholography, includes a reference object that is placed adjacent to the specimen\nof interest before measurements are collected. The resulting inverse problem,\nknown as holographic phase retrieval, is well-known to have improved problem\nconditioning relative to the original. This innovation, i.e. Holographic CDI,\nbecomes crucial at the nanoscale, where imaging specimens such as viruses,\nproteins, and crystals require low-photon measurements. This data is highly\ncorrupted by Poisson shot noise, and often lacks low-frequency content as well.\nIn this work, we introduce a dataset-free deep learning framework for\nholographic phase retrieval adapted to these challenges. The key ingredients of\nour approach are the explicit and flexible incorporation of the physical\nforward model into an automatic differentiation procedure, the Poisson\nlog-likelihood objective function, and an optional untrained deep image prior.\nWe perform extensive evaluation under realistic conditions. Compared to\ncompeting classical methods, our method recovers signal from higher noise\nlevels and is more resilient to suboptimal reference design, as well as to\nlarge missing regions of low frequencies in the observations. Finally, we show\nthat these properties carry over to experimental data acquired on optical\nwavelengths. To the best of our knowledge, this is the first work to consider a\ndataset-free machine learning approach for holographic phase retrieval.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 10:15:07 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 16:46:13 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 03:48:17 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Lawrence", "Hannah", ""], ["Barmherzig", "David A.", ""], ["Li", "Henry", ""], ["Eickenberg", "Michael", ""], ["Gabri\u00e9", "Marylou", ""]]}, {"id": "2012.07403", "submitter": "Andrey Nechaevskiy V.", "authors": "Alexander Uzhinskiy (1), Gennady Ososkov (1), Pavel Goncharov (1),\n  Andrey Nechaevskiy (1), Artem Smetanin (2) ((1) Joint Institute for Nuclear\n  Research, Dubna, Moscow region, Russia, (2) ITMO University, Saint\n  Petersburg, Russia)", "title": "One-Shot Learning with Triplet Loss for Vegetation Classification Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Triplet loss function is one of the options that can significantly improve\nthe accuracy of the One-shot Learning tasks. Starting from 2015, many projects\nuse Siamese networks and this kind of loss for face recognition and object\nclassification. In our research, we focused on two tasks related to vegetation.\nThe first one is plant disease detection on 25 classes of five crops (grape,\ncotton, wheat, cucumbers, and corn). This task is motivated because harvest\nlosses due to diseases is a serious problem for both large farming structures\nand rural families. The second task is the identification of moss species (5\nclasses). Mosses are natural bioaccumulators of pollutants; therefore, they are\nused in environmental monitoring programs. The identification of moss species\nis an important step in the sample preprocessing. In both tasks, we used\nself-collected image databases. We tried several deep learning architectures\nand approaches. Our Siamese network architecture with a triplet loss function\nand MobileNetV2 as a base network showed the most impressive results in both\nabove-mentioned tasks. The average accuracy for plant disease detection\namounted to over 97.8% and 97.6% for moss species classification.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 10:44:22 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 19:34:55 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Uzhinskiy", "Alexander", ""], ["Ososkov", "Gennady", ""], ["Goncharov", "Pavel", ""], ["Nechaevskiy", "Andrey", ""], ["Smetanin", "Artem", ""]]}, {"id": "2012.07427", "submitter": "Nando Metzger", "authors": "Nando Metzger", "title": "DSM Refinement with Deep Encoder-Decoder Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D city models can be generated from aerial images. However, the calculated\nDSMs suffer from noise, artefacts, and data holes that have to be manually\ncleaned up in a time-consuming process. This work presents an approach that\nautomatically refines such DSMs. The key idea is to teach a neural network the\ncharacteristics of urban area from reference data. In order to achieve this\ngoal, a loss function consisting of an L1 norm and a feature loss is proposed.\nThese features are constructed using a pre-trained image classification\nnetwork. To learn to update the height maps, the network architecture is set up\nbased on the concept of deep residual learning and an encoder-decoder\nstructure. The results show that this combination is highly effective in\npreserving the relevant geometric structures while removing the undesired\nartefacts and noise.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 11:27:42 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Metzger", "Nando", ""]]}, {"id": "2012.07430", "submitter": "Vajira Thambawita", "authors": "Vajira Thambawita, Steven Hicks, P{\\aa}l Halvorsen, Michael A. Riegler", "title": "Pyramid-Focus-Augmentation: Medical Image Segmentation with Step-Wise\n  Focus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Segmentation of findings in the gastrointestinal tract is a challenging but\nalso an important task which is an important building stone for sufficient\nautomatic decision support systems. In this work, we present our solution for\nthe Medico 2020 task, which focused on the problem of colon polyp segmentation.\nWe present our simple but efficient idea of using an augmentation method that\nuses grids in a pyramid-like manner (large to small) for segmentation. Our\nresults show that the proposed methods work as indented and can also lead to\ncomparable results when competing with other methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 11:34:29 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Thambawita", "Vajira", ""], ["Hicks", "Steven", ""], ["Halvorsen", "P\u00e5l", ""], ["Riegler", "Michael A.", ""]]}, {"id": "2012.07462", "submitter": "David Alexandre", "authors": "David Alexandre and Hsueh-Ming Hang", "title": "Learned Video Codec with Enriched Reconstruction for CLIC P-frame Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a learning-based video codec, specifically used for\nChallenge on Learned Image Compression (CLIC, CVPRWorkshop) 2020 P-frame\ncoding. More specifically, we designed a compressor network with Refine-Net for\ncoding residual signals and motion vectors. Also, for motion estimation, we\nintroduced a hierarchical, attention-based ME-Net. To verify our design, we\nconducted an extensive ablation study on our modules and different input\nformats. Our video codec demonstrates its performance by using the perfect\nreference frame at the decoder side specified by the CLIC P-frame Challenge.\nThe experimental result shows that our proposed codec is very competitive with\nthe Challenge top performers in terms of quality metrics.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 12:32:46 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Alexandre", "David", ""], ["Hang", "Hsueh-Ming", ""]]}, {"id": "2012.07477", "submitter": "Jiuwen Zhu", "authors": "Jiuwen Zhu, Yuexiang Li, S. Kevin Zhou", "title": "Aggregative Self-Supervised Feature Learning from a Limited Sample", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning (SSL) is an efficient approach that addresses the\nissue of limited training data and annotation shortage. The key part in SSL is\nits proxy task that defines the supervisory signals and drives the learning\ntoward effective feature representations. However, most SSL approaches usually\nfocus on a single proxy task, which greatly limits the expressive power of the\nlearned features and therefore deteriorates the network generalization\ncapacity. In this regard, we hereby propose two strategies of aggregation in\nterms of complementarity of various forms to boost the robustness of\nself-supervised learned features. We firstly propose a principled framework of\nmulti-task aggregative self-supervised learning from a limited sample to form a\nunified representation, with an intent of exploiting feature complementarity\namong different tasks. Then, in self-aggregative SSL, we propose to\nself-complement an existing proxy task with an auxiliary loss function based on\na linear centered kernel alignment metric, which explicitly promotes the\nexploring of where are uncovered by the features learned from a proxy task at\nhand to further boost the modeling capability. Our extensive experiments on 2D\nnatural image and 3D medical image classification tasks under limited data and\nannotation scenarios confirm that the proposed aggregation strategies\nsuccessfully boost the classification accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 12:49:37 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 09:32:53 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 01:27:18 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zhu", "Jiuwen", ""], ["Li", "Yuexiang", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2012.07489", "submitter": "Shipra Jain", "authors": "Shipra Jain, Danda Paudel Pani, Martin Danelljan, Luc Van Gool", "title": "Scaling Semantic Segmentation Beyond 1K Classes on a Single GPU", "comments": "second version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art object detection and image classification methods can\nperform impressively on more than 9k and 10k classes, respectively. In\ncontrast, the number of classes in semantic segmentation datasets is relatively\nlimited. This is not surprising when the restrictions caused by the lack of\nlabeled data and high computation demand for segmentation are considered. In\nthis paper, we propose a novel training methodology to train and scale the\nexisting semantic segmentation models for a large number of semantic classes\nwithout increasing the memory overhead. In our embedding-based scalable\nsegmentation approach, we reduce the space complexity of the segmentation\nmodel's output from O(C) to O(1), propose an approximation method for\nground-truth class probability, and use it to compute cross-entropy loss. The\nproposed approach is general and can be adopted by any state-of-the-art\nsegmentation model to gracefully scale it for any number of semantic classes\nwith only one GPU. Our approach achieves similar, and in some cases, even\nbetter mIoU for Cityscapes, Pascal VOC, ADE20k, COCO-Stuff10k datasets when\nadopted to DeeplabV3+ model with different backbones. We demonstrate a clear\nbenefit of our approach on a dataset with 1284 classes, bootstrapped from LVIS\nand COCO annotations, with three times better mIoU than the DeeplabV3+ model.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 13:12:38 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 16:38:34 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Jain", "Shipra", ""], ["Pani", "Danda Paudel", ""], ["Danelljan", "Martin", ""], ["Van Gool", "Luc", ""]]}, {"id": "2012.07495", "submitter": "Alain Tremeau Prof", "authors": "Alain Tremeau, Sixiang Xu and Damien Muselet", "title": "Deep Learning for Material recognition: most recent advances and open\n  challenges", "comments": "Paper presented as Invited paper at the International Conference on\n  Big Data, Machine Learning and Applications (BIGDML), Silchar, India,\n  December 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing material from color images is still a challenging problem today.\nWhile deep neural networks provide very good results on object recognition and\nhas been the topic of a huge amount of papers in the last decade, their\nadaptation to material images still requires some works to reach equivalent\naccuracies. Nevertheless, recent studies achieve very good results in material\nrecognition with deep learning and we propose, in this paper, to review most of\nthem by focusing on three aspects: material image datasets, influence of the\ncontext and ad hoc descriptors for material appearance. Every aspect is\nintroduced by a systematic manner and results from representative works are\ncited. We also present our own studies in this area and point out some open\nchallenges for future works.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 13:27:25 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Tremeau", "Alain", ""], ["Xu", "Sixiang", ""], ["Muselet", "Damien", ""]]}, {"id": "2012.07498", "submitter": "Wenbin Zhao", "authors": "Wenbin Zhao, Jiabao Lei, Yuxin Wen, Jianguo Zhang, Kui Jia", "title": "Sign-Agnostic Implicit Learning of Surface Self-Similarities for Shape\n  Modeling and Reconstruction from Raw Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape modeling and reconstruction from raw point clouds of objects stand as a\nfundamental challenge in vision and graphics research. Classical methods\nconsider analytic shape priors; however, their performance degraded when the\nscanned points deviate from the ideal conditions of cleanness and completeness.\nImportant progress has been recently made by data-driven approaches, which\nlearn global and/or local models of implicit surface representations from\nauxiliary sets of training shapes. Motivated from a universal phenomenon that\nself-similar shape patterns of local surface patches repeat across the entire\nsurface of an object, we aim to push forward the data-driven strategies and\npropose to learn a local implicit surface network for a shared, adaptive\nmodeling of the entire surface for a direct surface reconstruction from raw\npoint cloud; we also enhance the leveraging of surface self-similarities by\nimproving correlations among the optimized latent codes of individual surface\npatches. Given that orientations of raw points could be unavailable or noisy,\nwe extend sign agnostic learning into our local implicit model, which enables\nour recovery of signed implicit fields of local surfaces from the unsigned\ninputs. We term our framework as Sign-Agnostic Implicit Learning of Surface\nSelf-Similarities (SAIL-S3). With a global post-optimization of local sign\nflipping, SAIL-S3 is able to directly model raw, un-oriented point clouds and\nreconstruct high-quality object surfaces. Experiments show its superiority over\nexisting methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 13:33:22 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 02:35:45 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 13:38:40 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zhao", "Wenbin", ""], ["Lei", "Jiabao", ""], ["Wen", "Yuxin", ""], ["Zhang", "Jianguo", ""], ["Jia", "Kui", ""]]}, {"id": "2012.07504", "submitter": "Kira Maag", "authors": "Kira Maag, Matthias Rottmann, Serin Varghese, Fabian Hueger, Peter\n  Schlicht and Hanno Gottschalk", "title": "Improving Video Instance Segmentation by Light-weight Temporal\n  Uncertainty Estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation with neural networks is an essential task in\nenvironment perception. In many works, it has been observed that neural\nnetworks can predict false positive instances with high confidence values and\ntrue positives with low ones. Thus, it is important to accurately model the\nuncertainties of neural networks in order to prevent safety issues and foster\ninterpretability. In applications such as automated driving, the reliability of\nneural networks is of highest interest. In this paper, we present a\ntime-dynamic approach to model uncertainties of instance segmentation networks\nand apply this to the detection of false positives as well as the estimation of\nprediction quality. The availability of image sequences in online applications\nallows for tracking instances over multiple frames. Based on an instances\nhistory of shape and uncertainty information, we construct temporal\ninstance-wise aggregated metrics. The latter are used as input to\npost-processing models that estimate the prediction quality in terms of\ninstance-wise intersection over union. The proposed method only requires a\nreadily trained neural network (that may operate on single frames) and video\nsequence input. In our experiments, we further demonstrate the use of the\nproposed method by replacing the traditional score value from object detection\nand thereby improving the overall performance of the instance segmentation\nnetwork.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 13:39:05 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 12:17:30 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Maag", "Kira", ""], ["Rottmann", "Matthias", ""], ["Varghese", "Serin", ""], ["Hueger", "Fabian", ""], ["Schlicht", "Peter", ""], ["Gottschalk", "Hanno", ""]]}, {"id": "2012.07508", "submitter": "Dong Wang", "authors": "Dong Wang, Di Hu, Xingjian Li, Dejing Dou", "title": "Temporal Relational Modeling with Self-Supervision for Action\n  Segmentation", "comments": "Accepted by the Thirty-Fifth AAAI Conference on Artificial\n  Intelligence (AAAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Temporal relational modeling in video is essential for human action\nunderstanding, such as action recognition and action segmentation. Although\nGraph Convolution Networks (GCNs) have shown promising advantages in relation\nreasoning on many tasks, it is still a challenge to apply graph convolution\nnetworks on long video sequences effectively. The main reason is that large\nnumber of nodes (i.e., video frames) makes GCNs hard to capture and model\ntemporal relations in videos. To tackle this problem, in this paper, we\nintroduce an effective GCN module, Dilated Temporal Graph Reasoning Module\n(DTGRM), designed to model temporal relations and dependencies between video\nframes at various time spans. In particular, we capture and model temporal\nrelations via constructing multi-level dilated temporal graphs where the nodes\nrepresent frames from different moments in video. Moreover, to enhance temporal\nreasoning ability of the proposed model, an auxiliary self-supervised task is\nproposed to encourage the dilated temporal graph reasoning module to find and\ncorrect wrong temporal relations in videos. Our DTGRM model outperforms\nstate-of-the-art action segmentation models on three challenging datasets:\n50Salads, Georgia Tech Egocentric Activities (GTEA), and the Breakfast dataset.\nThe code is available at https://github.com/redwang/DTGRM.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 13:41:28 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Wang", "Dong", ""], ["Hu", "Di", ""], ["Li", "Xingjian", ""], ["Dou", "Dejing", ""]]}, {"id": "2012.07536", "submitter": "Pinelopi Papalampidi", "authors": "Pinelopi Papalampidi, Frank Keller, Mirella Lapata", "title": "Movie Summarization via Sparse Graph Construction", "comments": "Accepted at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We summarize full-length movies by creating shorter videos containing their\nmost informative scenes. We explore the hypothesis that a summary can be\ncreated by assembling scenes which are turning points (TPs), i.e., key events\nin a movie that describe its storyline. We propose a model that identifies TP\nscenes by building a sparse movie graph that represents relations between\nscenes and is constructed using multimodal information. According to human\njudges, the summaries created by our approach are more informative and\ncomplete, and receive higher ratings, than the outputs of sequence-based models\nand general-purpose summarization algorithms. The induced graphs are\ninterpretable, displaying different topology for different movie genres.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 13:54:34 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Papalampidi", "Pinelopi", ""], ["Keller", "Frank", ""], ["Lapata", "Mirella", ""]]}, {"id": "2012.07541", "submitter": "Guangyao Zhai", "authors": "Guangyao Zhai, Xin Kong, Jinhao Cui, Yong Liu, and Zhen Yang", "title": "FlowMOT: 3D Multi-Object Tracking by Scene Flow Association", "comments": "Internship technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most end-to-end Multi-Object Tracking (MOT) methods face the problems of low\naccuracy and poor generalization ability. Although traditional filter-based\nmethods can achieve better results, they are difficult to be endowed with\noptimal hyperparameters and often fail in varying scenarios. To alleviate these\ndrawbacks, we propose a LiDAR-based 3D MOT framework named FlowMOT, which\nintegrates point-wise motion information with the traditional matching\nalgorithm, enhancing the robustness of the motion prediction. We firstly\nutilize a scene flow estimation network to obtain implicit motion information\nbetween two adjacent frames and calculate the predicted detection for each old\ntracklet in the previous frame. Then we use Hungarian algorithm to generate\noptimal matching relations with the ID propagation strategy to finish the\ntracking task. Experiments on KITTI MOT dataset show that our approach\noutperforms recent end-to-end methods and achieves competitive performance with\nthe state-of-the-art filter-based method. In addition, ours can work steadily\nin the various-speed scenarios where the filter-based methods may fail.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 14:03:48 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 13:18:56 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 10:36:56 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Zhai", "Guangyao", ""], ["Kong", "Xin", ""], ["Cui", "Jinhao", ""], ["Liu", "Yong", ""], ["Yang", "Zhen", ""]]}, {"id": "2012.07596", "submitter": "Mariana Da Silva", "authors": "Mariana da Silva, Kara Garcia, Carole H. Sudre, Cher Bass, M. Jorge\n  Cardoso, Emma Robinson", "title": "Biomechanical modelling of brain atrophy through deep learning", "comments": "Submitted to Medical Imaging Meets NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV q-bio.TO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a proof-of-concept, deep learning (DL) based, differentiable\nbiomechanical model of realistic brain deformations. Using prescribed maps of\nlocal atrophy and growth as input, the network learns to deform images\naccording to a Neo-Hookean model of tissue deformation. The tool is validated\nusing longitudinal brain atrophy data from the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) dataset, and we demonstrate that the trained model is capable\nof rapidly simulating new brain deformations with minimal residuals. This\nmethod has the potential to be used in data augmentation or for the exploration\nof different causal hypotheses reflecting brain growth and atrophy.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 14:40:47 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["da Silva", "Mariana", ""], ["Garcia", "Kara", ""], ["Sudre", "Carole H.", ""], ["Bass", "Cher", ""], ["Cardoso", "M. Jorge", ""], ["Robinson", "Emma", ""]]}, {"id": "2012.07599", "submitter": "Rahat Zaman", "authors": "Md. Rahat-uz-Zaman and Shadmaan Hye", "title": "Agglomerative Clustering of Handwritten Numerals to Determine Similarity\n  of Different Languages", "comments": "Submitted to the 22nd International Conference on Computer and\n  Information Technology (ICCIT), 18-20 December, 2019, 6 pages, 5 figures and\n  1 table", "journal-ref": null, "doi": "10.1109/ICCIT48885.2019.9038550", "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwritten numerals of different languages have various characteristics.\nSimilarities and dissimilarities of the languages can be measured by analyzing\nthe extracted features of the numerals. Handwritten numeral datasets are\navailable and accessible for many renowned languages of different regions. In\nthis paper, several handwritten numeral datasets of different languages are\ncollected. Then they are used to find the similarity among those written\nlanguages through determining and comparing the similitude of each handwritten\nnumerals. This will help to find which languages have the same or adjacent\nparent language. Firstly, a similarity measure of two numeral images is\nconstructed with a Siamese network. Secondly, the similarity of the numeral\ndatasets is determined with the help of the Siamese network and a new random\nsample with replacement similarity averaging technique. Finally, an\nagglomerative clustering is done based on the similarities of each dataset.\nThis clustering technique shows some very interesting properties of the\ndatasets. The property focused in this paper is the regional resemblance of the\ndatasets. By analyzing the clusters, it becomes easy to identify which\nlanguages are originated from similar regions.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 04:36:25 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Rahat-uz-Zaman", "Md.", ""], ["Hye", "Shadmaan", ""]]}, {"id": "2012.07616", "submitter": "Yang Liu", "authors": "Yang Liu, Zhen Zhu, and Xiang Bai", "title": "WDNet: Watermark-Decomposition Network for Visible Watermark Removal", "comments": "To appear in WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visible watermarks are widely-used in images to protect copyright ownership.\nAnalyzing watermark removal helps to reinforce the anti-attack techniques in an\nadversarial way. Current removal methods normally leverage image-to-image\ntranslation techniques. Nevertheless, the uncertainty of the size, shape, color\nand transparency of the watermarks set a huge barrier for these methods. To\ncombat this, we combine traditional watermarked image decomposition into a\ntwo-stage generator, called Watermark-Decomposition Network (WDNet), where the\nfirst stage predicts a rough decomposition from the whole watermarked image and\nthe second stage specifically centers on the watermarked area to refine the\nremoval results. The decomposition formulation enables WDNet to separate\nwatermarks from the images rather than simply removing them. We further show\nthat these separated watermarks can serve as extra nutrients for building a\nlarger training dataset and further improving removal performance. Besides, we\nconstruct a large-scale dataset named CLWD, which mainly contains colored\nwatermarks, to fill the vacuum of colored watermark removal dataset. Extensive\nexperiments on the public gray-scale dataset LVW and CLWD consistently show\nthat the proposed WDNet outperforms the state-of-the-art approaches both in\naccuracy and efficiency. The code and CLWD dataset are publicly available at\nhttps://github.com/MRUIL/WDNet.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 15:07:35 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 03:57:21 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Liu", "Yang", ""], ["Zhu", "Zhen", ""], ["Bai", "Xiang", ""]]}, {"id": "2012.07620", "submitter": "Xuanmeng Zhang", "authors": "Xuanmeng Zhang, Minyue Jiang, Zhedong Zheng, Xiao Tan, Errui Ding, Yi\n  Yang", "title": "Understanding Image Retrieval Re-Ranking: A Graph Neural Network\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The re-ranking approach leverages high-confidence retrieved samples to refine\nretrieval results, which have been widely adopted as a post-processing tool for\nimage retrieval tasks. However, we notice one main flaw of re-ranking, i.e.,\nhigh computational complexity, which leads to an unaffordable time cost for\nreal-world applications. In this paper, we revisit re-ranking and demonstrate\nthat re-ranking can be reformulated as a high-parallelism Graph Neural Network\n(GNN) function. In particular, we divide the conventional re-ranking process\ninto two phases, i.e., retrieving high-quality gallery samples and updating\nfeatures. We argue that the first phase equals building the k-nearest neighbor\ngraph, while the second phase can be viewed as spreading the message within the\ngraph. In practice, GNN only needs to concern vertices with the connected\nedges. Since the graph is sparse, we can efficiently update the vertex\nfeatures. On the Market-1501 dataset, we accelerate the re-ranking processing\nfrom 89.2s to 9.4ms with one K40m GPU, facilitating the real-time\npost-processing. Similarly, we observe that our method achieves comparable or\neven better retrieval results on the other four image retrieval benchmarks,\ni.e., VeRi-776, Oxford-5k, Paris-6k and University-1652, with limited time\ncost. Our code is publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 15:12:36 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 05:09:06 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Zhang", "Xuanmeng", ""], ["Jiang", "Minyue", ""], ["Zheng", "Zhedong", ""], ["Tan", "Xiao", ""], ["Ding", "Errui", ""], ["Yang", "Yi", ""]]}, {"id": "2012.07630", "submitter": "Chen Zuge", "authors": "Kehe WU, Zuge Chen, Qi MA, Xiaoliang Zhang, Wei Li", "title": "Decoupled Self Attention for Accurate One Stage Object Detection", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the scale of object detection dataset is smaller than that of image\nrecognition dataset ImageNet, transfer learning has become a basic training\nmethod for deep learning object detection models, which will pretrain the\nbackbone network of object detection model on ImageNet dataset to extract\nfeatures for classification and localization subtasks. However, the\nclassification task focuses on the salient region features of object, while the\nlocation task focuses on the edge features of object, so there is certain\ndeviation between the features extracted by pretrained backbone network and the\nfeatures used for localization task. In order to solve this problem, a\ndecoupled self attention(DSA) module is proposed for one stage object detection\nmodels in this paper. DSA includes two decoupled self-attention branches, so it\ncan extract appropriate features for different tasks. It is located between FPN\nand head networks of subtasks, so it is used to extract global features based\non FPN fused features for different tasks independently. Although the network\nof DSA module is simple, but it can effectively improve the performance of\nobject detection, also it can be easily embedded in many detection models. Our\nexperiments are based on the representative one-stage detection model\nRetinaNet. In COCO dataset, when ResNet50 and ResNet101 are used as backbone\nnetworks, the detection performances can be increased by 0.4% AP and 0.5% AP\nrespectively. When DSA module and object confidence task are applied in\nRetinaNet together, the detection performances based on ResNet50 and ResNet101\ncan be increased by 1.0% AP and 1.4% AP respectively. The experiment results\nshow the effectiveness of DSA module. Code is at:\nhttps://github.com/chenzuge1/DSANet.git.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 15:19:30 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 06:47:27 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["WU", "Kehe", ""], ["Chen", "Zuge", ""], ["MA", "Qi", ""], ["Zhang", "Xiaoliang", ""], ["Li", "Wei", ""]]}, {"id": "2012.07653", "submitter": "Ivan Konovalenko", "authors": "Ivan A. Konovalenko, Anna A. Smagina, Dmitry P. Nikolaev and Petr P.\n  Nikolaev", "title": "ProLab: perceptually uniform projective colour coordinate system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose proLab: a new colour coordinate system derived as a\n3D projective transformation of CIE XYZ. We show that proLab is far ahead of\nthe widely used CIELAB coordinate system (though inferior to the modern\nCAM16-UCS) according to perceptual uniformity evaluated by the STRESS metric in\nreference to the CIEDE2000 colour difference formula. At the same time, angular\nerrors of chromaticity estimation that are standard for linear colour spaces\ncan also be used in proLab since projective transformations preserve the\nlinearity of manifolds. Unlike in linear spaces, angular errors for different\nhues are normalized according to human colour discrimination thresholds within\nproLab. We also demonstrate that shot noise in proLab is more homoscedastic\nthan in CAM16-UCS or other standard colour spaces. This makes proLab a\nconvenient coordinate system in which to perform linear colour analysis.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 15:49:57 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 18:46:46 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Konovalenko", "Ivan A.", ""], ["Smagina", "Anna A.", ""], ["Nikolaev", "Dmitry P.", ""], ["Nikolaev", "Petr P.", ""]]}, {"id": "2012.07655", "submitter": "Walid Hariri", "authors": "Walid Hariri, Ali Narin", "title": "Deep Neural Networks for COVID-19 Detection and Diagnosis using Images\n  and Acoustic-based Techniques: A Recent Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The new coronavirus disease (COVID-19) has been declared a pandemic since\nMarch 2020 by the World Health Organization. It consists of an emerging viral\ninfection with respiratory tropism that could develop atypical pneumonia.\nExperts emphasize the importance of early detection of those who have the\nCOVID-19 virus. In this way, patients will be isolated from other people and\nthe spread of the virus can be prevented. For this reason, it has become an\narea of interest to develop early diagnosis and detection methods to ensure a\nrapid treatment process and prevent the virus from spreading. Since the\nstandard testing system is time-consuming and not available for everyone,\nalternative early-screening techniques have become an urgent need. In this\nstudy, the approaches used in the detection of COVID-19 based on deep learning\n(DL) algorithms, which have been popular in recent years, have been\ncomprehensively discussed. The advantages and disadvantages of different\napproaches used in literature are examined in detail. The Computed Tomography\nof the chest and X-ray images give a rich representation of the patient's lung\nthat is less time-consuming and allows an efficient viral pneumonia detection\nusing the DL algorithms. The first step is the pre-processing of these images\nto remove noise. Next, deep features are extracted using multiple types of deep\nmodels (pre-trained models, generative models, generic neural networks, etc.).\nFinally, the classification is performed using the obtained features to decide\nwhether the patient is infected by coronavirus or it is another lung disease.\nIn this study, we also give a brief review of the latest applications of cough\nanalysis to early screen the COVID-19, and human mobility estimation to limit\nits spread.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 19:52:12 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 19:20:31 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 19:41:59 GMT"}, {"version": "v4", "created": "Sat, 1 May 2021 12:48:31 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Hariri", "Walid", ""], ["Narin", "Ali", ""]]}, {"id": "2012.07657", "submitter": "Alexandros Haliassos", "authors": "Alexandros Haliassos, Konstantinos Vougioukas, Stavros Petridis, Maja\n  Pantic", "title": "Lips Don't Lie: A Generalisable and Robust Approach to Face Forgery\n  Detection", "comments": "Accepted at CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although current deep learning-based face forgery detectors achieve\nimpressive performance in constrained scenarios, they are vulnerable to samples\ncreated by unseen manipulation methods. Some recent works show improvements in\ngeneralisation but rely on cues that are easily corrupted by common\npost-processing operations such as compression. In this paper, we propose\nLipForensics, a detection approach capable of both generalising to novel\nmanipulations and withstanding various distortions. LipForensics targets\nhigh-level semantic irregularities in mouth movements, which are common in many\ngenerated videos. It consists in first pretraining a spatio-temporal network to\nperform visual speech recognition (lipreading), thus learning rich internal\nrepresentations related to natural mouth motion. A temporal network is\nsubsequently finetuned on fixed mouth embeddings of real and forged data in\norder to detect fake videos based on mouth movements without overfitting to\nlow-level, manipulation-specific artefacts. Extensive experiments show that\nthis simple approach significantly surpasses the state-of-the-art in terms of\ngeneralisation to unseen manipulations and robustness to perturbations, as well\nas shed light on the factors responsible for its performance.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 15:53:56 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 10:24:56 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Haliassos", "Alexandros", ""], ["Vougioukas", "Konstantinos", ""], ["Petridis", "Stavros", ""], ["Pantic", "Maja", ""]]}, {"id": "2012.07658", "submitter": "Sonal Thakkar", "authors": "Weixin (Angela) Wu, Sonal Thakkar, Will Hawkins, Hossein Vahabi,\n  Alberto Todeschini", "title": "High-resolution global irrigation prediction with Sentinel-2 30m data", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  An accurate and precise understanding of global irrigation usage is crucial\nfor a variety of climate science efforts. Irrigation is highly\nenergy-intensive, and as population growth continues at its current pace,\nincreases in crop need and water usage will have an impact on climate change.\nPrecise irrigation data can help with monitoring water usage and optimizing\nagricultural yield, particularly in developing countries. Irrigation data, in\ntandem with precipitation data, can be used to predict water budgets as well as\nclimate and weather modeling. With our research, we produce an irrigation\nprediction model that combines unsupervised clustering of Normalized Difference\nVegetation Index (NDVI) temporal signatures with a precipitation heuristic to\nlabel the months that irrigation peaks for each cropland cluster in a given\nyear. We have developed a novel irrigation model and Python package\n(\"Irrigation30\") to generate 30m resolution irrigation predictions of cropland\nworldwide. With a small crowdsourced test set of cropland coordinates and\nirrigation labels, using a fraction of the resources used by the\nstate-of-the-art NASA-funded GFSAD30 project with irrigation data limited to\nIndia and Australia, our model was able to achieve consistency scores in excess\nof 97\\% and an accuracy of 92\\% in a small geo-diverse randomly sampled test\nset.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 17:26:43 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Weixin", "", "", "Angela"], ["Wu", "", ""], ["Thakkar", "Sonal", ""], ["Hawkins", "Will", ""], ["Vahabi", "Hossein", ""], ["Todeschini", "Alberto", ""]]}, {"id": "2012.07688", "submitter": "Xin Li", "authors": "Xin Li, Xiangrui Li, Deng Pan, Dongxiao Zhu", "title": "Improving Adversarial Robustness via Probabilistically Compact Loss with\n  Logit Constraints", "comments": "To appear in the proceedings of Thirty-Five AAAI Conference on\n  Artificial Intelligence (AAAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have achieved state-of-the-art\nperformance on various tasks in computer vision. However, recent studies\ndemonstrate that these models are vulnerable to carefully crafted adversarial\nsamples and suffer from a significant performance drop when predicting them.\nMany methods have been proposed to improve adversarial robustness (e.g.,\nadversarial training and new loss functions to learn adversarially robust\nfeature representations). Here we offer a unique insight into the predictive\nbehavior of CNNs that they tend to misclassify adversarial samples into the\nmost probable false classes. This inspires us to propose a new\nProbabilistically Compact (PC) loss with logit constraints which can be used as\na drop-in replacement for cross-entropy (CE) loss to improve CNN's adversarial\nrobustness. Specifically, PC loss enlarges the probability gaps between true\nclass and false classes meanwhile the logit constraints prevent the gaps from\nbeing melted by a small perturbation. We extensively compare our method with\nthe state-of-the-art using large scale datasets under both white-box and\nblack-box attacks to demonstrate its effectiveness. The source codes are\navailable from the following url: https://github.com/xinli0928/PC-LC.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 16:40:53 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Li", "Xin", ""], ["Li", "Xiangrui", ""], ["Pan", "Deng", ""], ["Zhu", "Dongxiao", ""]]}, {"id": "2012.07717", "submitter": "Lorenzo Porzi", "authors": "Lorenzo Porzi, Samuel Rota Bul\\`o, Peter Kontschieder", "title": "Improving Panoptic Segmentation at All Scales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crop-based training strategies decouple training resolution from GPU memory\nconsumption, allowing the use of large-capacity panoptic segmentation networks\non multi-megapixel images. Using crops, however, can introduce a bias towards\ntruncating or missing large objects. To address this, we propose a novel\ncrop-aware bounding box regression loss (CABB loss), which promotes predictions\nto be consistent with the visible parts of the cropped objects, while not\nover-penalizing them for extending outside of the crop. We further introduce a\nnovel data sampling and augmentation strategy which improves generalization\nacross scales by counteracting the imbalanced distribution of object sizes.\nCombining these two contributions with a carefully designed, top-down panoptic\nsegmentation architecture, we obtain new state-of-the-art results on the\nchallenging Mapillary Vistas (MVD), Indian Driving and Cityscapes datasets,\nsurpassing the previously best approach on MVD by +4.5% PQ and +5.2% mAP.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 17:11:00 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 13:31:57 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Porzi", "Lorenzo", ""], ["Bul\u00f2", "Samuel Rota", ""], ["Kontschieder", "Peter", ""]]}, {"id": "2012.07719", "submitter": "Dongxiao Zhang", "authors": "Qiang Zheng and Dongxiao Zhang", "title": "Digital rock reconstruction with user-defined properties using\n  conditional generative adversarial networks", "comments": "36 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty is ubiquitous with flow in subsurface rocks because of their\ninherent heterogeneity and lack of in-situ measurements. To complete\nuncertainty analysis in a multi-scale manner, it is a prerequisite to provide\nsufficient rock samples. Even though the advent of digital rock technology\noffers opportunities to reproduce rocks, it still cannot be utilized to provide\nmassive samples due to its high cost, thus leading to the development of\ndiversified mathematical methods. Among them, two-point statistics (TPS) and\nmulti-point statistics (MPS) are commonly utilized, which feature incorporating\nlow-order and high-order statistical information, respectively. Recently,\ngenerative adversarial networks (GANs) are becoming increasingly popular since\nthey can reproduce training images with excellent visual and consequent\ngeologic realism. However, standard GANs can only incorporate information from\ndata, while leaving no interface for user-defined properties, and thus may\nlimit the representativeness of reconstructed samples. In this study, we\npropose conditional GANs for digital rock reconstruction, aiming to reproduce\nsamples not only similar to the real training data, but also satisfying\nuser-specified properties. In fact, the proposed framework can realize the\ntargets of MPS and TPS simultaneously by incorporating high-order information\ndirectly from rock images with the GANs scheme, while preserving low-order\ncounterparts through conditioning. We conduct three reconstruction experiments,\nand the results demonstrate that rock type, rock porosity, and correlation\nlength can be successfully conditioned to affect the reconstructed rock images.\nFurthermore, in contrast to existing GANs, the proposed conditioning enables\nlearning of multiple rock types simultaneously, and thus invisibly saves\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 10:55:58 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 06:32:43 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Zheng", "Qiang", ""], ["Zhang", "Dongxiao", ""]]}, {"id": "2012.07773", "submitter": "Amir Rasouli", "authors": "Amir Rasouli, Tiffany Yau, Peter Lakner, Saber Malekmohammadi, Mohsen\n  Rohani, Jun Luo", "title": "PePScenes: A Novel Dataset and Baseline for Pedestrian Action Prediction\n  in 3D", "comments": "1 Figure, 2 Table. ML4AD at NeurIPS, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the behavior of road users, particularly pedestrians, is vital for\nsafe motion planning in the context of autonomous driving systems.\nTraditionally, pedestrian behavior prediction has been realized in terms of\nforecasting future trajectories. However, recent evidence suggests that\npredicting higher-level actions, such as crossing the road, can help improve\ntrajectory forecasting and planning tasks accordingly. There are a number of\nexisting datasets that cater to the development of pedestrian action prediction\nalgorithms, however, they lack certain characteristics, such as bird's eye view\nsemantic map information, 3D locations of objects in the scene, etc., which are\ncrucial in the autonomous driving context. To this end, we propose a new\npedestrian action prediction dataset created by adding per-frame 2D/3D bounding\nbox and behavioral annotations to the popular autonomous driving dataset,\nnuScenes. In addition, we propose a hybrid neural network architecture that\nincorporates various data modalities for predicting pedestrian crossing action.\nBy evaluating our model on the newly proposed dataset, the contribution of\ndifferent data modalities to the prediction task is revealed. The dataset is\navailable at https://github.com/huawei-noah/PePScenes.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 18:13:44 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Rasouli", "Amir", ""], ["Yau", "Tiffany", ""], ["Lakner", "Peter", ""], ["Malekmohammadi", "Saber", ""], ["Rohani", "Mohsen", ""], ["Luo", "Jun", ""]]}, {"id": "2012.07788", "submitter": "Niklas Muennighoff", "authors": "Niklas Muennighoff", "title": "Vilio: State-of-the-art Visio-Linguistic Models applied to Hateful Memes", "comments": "Presented at NIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents Vilio, an implementation of state-of-the-art\nvisio-linguistic models and their application to the Hateful Memes Dataset. The\nimplemented models have been fitted into a uniform code-base and altered to\nyield better performance. The goal of Vilio is to provide a user-friendly\nstarting point for any visio-linguistic problem. An ensemble of 5 different V+L\nmodels implemented in Vilio achieves 2nd place in the Hateful Memes Challenge\nout of 3,300 participants. The code is available at\nhttps://github.com/Muennighoff/vilio.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 18:25:03 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Muennighoff", "Niklas", ""]]}, {"id": "2012.07791", "submitter": "V\\'itor Albiero", "authors": "V\\'itor Albiero, Xingyu Chen, Xi Yin, Guan Pang, Tal Hassner", "title": "img2pose: Face Alignment and Detection via 6DoF, Face Pose Estimation", "comments": "To appear in CVPR 2021. Joint first authorship: V\\'itor Albiero and\n  Xingyu Chen", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose real-time, six degrees of freedom (6DoF), 3D face pose estimation\nwithout face detection or landmark localization. We observe that estimating the\n6DoF rigid transformation of a face is a simpler problem than facial landmark\ndetection, often used for 3D face alignment. In addition, 6DoF offers more\ninformation than face bounding box labels. We leverage these observations to\nmake multiple contributions: (a) We describe an easily trained, efficient,\nFaster R-CNN--based model which regresses 6DoF pose for all faces in the photo,\nwithout preliminary face detection. (b) We explain how pose is converted and\nkept consistent between the input photo and arbitrary crops created while\ntraining and evaluating our model. (c) Finally, we show how face poses can\nreplace detection bounding box training labels. Tests on AFLW2000-3D and BIWI\nshow that our method runs at real-time and outperforms state of the art (SotA)\nface pose estimators. Remarkably, our method also surpasses SotA models of\ncomparable complexity on the WIDER FACE detection benchmark, despite not been\noptimized on bounding box labels.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 18:26:20 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 22:33:36 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Albiero", "V\u00edtor", ""], ["Chen", "Xingyu", ""], ["Yin", "Xi", ""], ["Pang", "Guan", ""], ["Hassner", "Tal", ""]]}, {"id": "2012.07810", "submitter": "Soumyadip Sengupta", "authors": "Shanchuan Lin, Andrey Ryabtsev, Soumyadip Sengupta, Brian Curless,\n  Steve Seitz, and Ira Kemelmacher-Shlizerman", "title": "Real-Time High-Resolution Background Matting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a real-time, high-resolution background replacement technique\nwhich operates at 30fps in 4K resolution, and 60fps for HD on a modern GPU. Our\ntechnique is based on background matting, where an additional frame of the\nbackground is captured and used in recovering the alpha matte and the\nforeground layer. The main challenge is to compute a high-quality alpha matte,\npreserving strand-level hair details, while processing high-resolution images\nin real-time. To achieve this goal, we employ two neural networks; a base\nnetwork computes a low-resolution result which is refined by a second network\noperating at high-resolution on selective patches. We introduce two largescale\nvideo and image matting datasets: VideoMatte240K and PhotoMatte13K/85. Our\napproach yields higher quality results compared to the previous\nstate-of-the-art in background matting, while simultaneously yielding a\ndramatic boost in both speed and resolution.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 18:43:32 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Lin", "Shanchuan", ""], ["Ryabtsev", "Andrey", ""], ["Sengupta", "Soumyadip", ""], ["Curless", "Brian", ""], ["Seitz", "Steve", ""], ["Kemelmacher-Shlizerman", "Ira", ""]]}, {"id": "2012.07842", "submitter": "Srishti Goel Ms.", "authors": "Neeraj Kumar, Srishti Goel, Ankur Narang, Mujtaba Hasan", "title": "Robust One Shot Audio to Video Generation", "comments": "Accepted in CVPR Deep Vision 2020. arXiv admin note: text overlap\n  with arXiv:2012.07304", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Audio to Video generation is an interesting problem that has numerous\napplications across industry verticals including film making, multi-media,\nmarketing, education and others. High-quality video generation with expressive\nfacial movements is a challenging problem that involves complex learning steps\nfor generative adversarial networks. Further, enabling one-shot learning for an\nunseen single image increases the complexity of the problem while\nsimultaneously making it more applicable to practical scenarios. In the paper,\nwe propose a novel approach OneShotA2V to synthesize a talking person video of\narbitrary length using as input: an audio signal and a single unseen image of a\nperson. OneShotA2V leverages curriculum learning to learn movements of\nexpressive facial components and hence generates a high-quality talking-head\nvideo of the given person. Further, it feeds the features generated from the\naudio input directly into a generative adversarial network and it adapts to any\ngiven unseen selfie by applying fewshot learning with only a few output\nupdation epochs. OneShotA2V leverages spatially adaptive normalization based\nmulti-level generator and multiple multi-level discriminators based\narchitecture. The input audio clip is not restricted to any specific language,\nwhich gives the method multilingual applicability. Experimental evaluation\ndemonstrates superior performance of OneShotA2V as compared to Realistic\nSpeech-Driven Facial Animation with GANs(RSDGAN) [43], Speech2Vid [8], and\nother approaches, on multiple quantitative metrics including: SSIM (structural\nsimilarity index), PSNR (peak signal to noise ratio) and CPBD (image\nsharpness). Further, qualitative evaluation and Online Turing tests demonstrate\nthe efficacy of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 10:50:05 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Kumar", "Neeraj", ""], ["Goel", "Srishti", ""], ["Narang", "Ankur", ""], ["Hasan", "Mujtaba", ""]]}, {"id": "2012.07890", "submitter": "Rui Fan", "authors": "Rui Fan, Hengli Wang, Peide Cai, Jin Wu, Mohammud Junaid Bocus, Lei\n  Qiao and Ming Liu", "title": "Learning Collision-Free Space Detection from Stereo Images: Homography\n  Matrix Brings Better Data Augmentation", "comments": "accepted to IEEE/ASME Transactions on Mechatronics", "journal-ref": null, "doi": "10.1109/TMECH.2021.3061077", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collision-free space detection is a critical component of autonomous vehicle\nperception. The state-of-the-art algorithms are typically based on supervised\nlearning. The performance of such approaches is always dependent on the quality\nand amount of labeled training data. Additionally, it remains an open challenge\nto train deep convolutional neural networks (DCNNs) using only a small quantity\nof training samples. Therefore, this paper mainly explores an effective\ntraining data augmentation approach that can be employed to improve the overall\nDCNN performance, when additional images captured from different views are\navailable. Due to the fact that the pixels of the collision-free space\n(generally regarded as a planar surface) between two images captured from\ndifferent views can be associated by a homography matrix, the scenario of the\ntarget image can be transformed into the reference view. This provides a simple\nbut effective way of generating training data from additional multi-view\nimages. Extensive experimental results, conducted with six state-of-the-art\nsemantic segmentation DCNNs on three datasets, demonstrate the effectiveness of\nour proposed training data augmentation algorithm for enhancing collision-free\nspace detection performance. When validated on the KITTI road benchmark, our\napproach provides the best results for stereo vision-based collision-free space\ndetection.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 19:14:35 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 04:47:05 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 21:22:09 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Fan", "Rui", ""], ["Wang", "Hengli", ""], ["Cai", "Peide", ""], ["Wu", "Jin", ""], ["Bocus", "Mohammud Junaid", ""], ["Qiao", "Lei", ""], ["Liu", "Ming", ""]]}, {"id": "2012.07916", "submitter": "Nima Karimian", "authors": "Kavya Dayananda and Nima Karimian", "title": "When Physical Unclonable Function Meets Biometrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the Covid-19 pandemic grips the world, healthcare systems are being\nreshaped, where the e-health concepts become more likely to be accepted.\nWearable devices often carry sensitive information from users which are exposed\nto security and privacy risks. Moreover, users have always had the concern of\nbeing counterfeited between the fabrication process and vendors' storage.\nHence, not only securing personal data is becoming a crucial obligation, but\nalso device verification is another challenge. To address biometrics\nauthentication and physically unclonable functions (PUFs) need to be put in\nplace to mitigate the security and privacy of the users. Among biometrics\nmodalities, Electrocardiogram (ECG) based biometric has become popular as it\ncan authenticate patients and monitor the patient's vital signs. However,\nresearchers have recently started to study the vulnerabilities of the ECG\nbiometric systems and tried to address the issues of spoofing. Moreover, most\nof the wearable is enabled with CPU and memories. Thus, volatile memory-based\n(NVM) PUF can be easily placed in the device to avoid counterfeit. However,\nmany research challenged the unclonability characteristics of PUFs. Thus, a\ncareful study on these attacks should be sufficient to address the need. In\nthis paper, our aim is to provide a comprehensive study on the state-of-the-art\ndevelopments papers based on biometrics enabled hardware security.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 20:00:40 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Dayananda", "Kavya", ""], ["Karimian", "Nima", ""]]}, {"id": "2012.07947", "submitter": "Fakai Wang", "authors": "Fakai Wang, Kang Zheng, Le Lu, Jing Xiao, Min Wu and Shun Miao", "title": "Automatic Vertebra Localization and Identification in CT by Spine\n  Rectification and Anatomically-constrained Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate vertebra localization and identification are required in many\nclinical applications of spine disorder diagnosis and surgery planning.\nHowever, significant challenges are posed in this task by highly varying\npathologies (such as vertebral compression fracture, scoliosis, and vertebral\nfixation) and imaging conditions (such as limited field of view and metal\nstreak artifacts). This paper proposes a robust and accurate method that\neffectively exploits the anatomical knowledge of the spine to facilitate\nvertebra localization and identification. A key point localization model is\ntrained to produce activation maps of vertebra centers. They are then\nre-sampled along the spine centerline to produce spine-rectified activation\nmaps, which are further aggregated into 1-D activation signals. Following this,\nan anatomically-constrained optimization module is introduced to jointly search\nfor the optimal vertebra centers under a soft constraint that regulates the\ndistance between vertebrae and a hard constraint on the consecutive vertebra\nindices. When being evaluated on a major public benchmark of 302 highly\npathological CT images, the proposed method reports the state of the art\nidentification (id.) rate of 97.4%, and outperforms the best competing method\nof 94.7% id. rate by reducing the relative id. error rate by half.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 21:26:48 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Wang", "Fakai", ""], ["Zheng", "Kang", ""], ["Lu", "Le", ""], ["Xiao", "Jing", ""], ["Wu", "Min", ""], ["Miao", "Shun", ""]]}, {"id": "2012.07950", "submitter": "Reda Abdellah Kamraoui", "authors": "Reda Abdellah Kamraoui, Vinh-Thong Ta, Thomas Tourdias, Boris\n  Mansencal, Jos\\'e V Manjon, Pierrick Coup\\'e", "title": "DeepLesionBrain: Towards a broader deep-learning generalization for\n  multiple sclerosis lesion segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recently, segmentation methods based on Convolutional Neural Networks (CNNs)\nshowed promising performance in automatic Multiple Sclerosis (MS) lesions\nsegmentation. These techniques have even outperformed human experts in\ncontrolled evaluation conditions such as Longitudinal MS Lesion Segmentation\nChallenge (ISBI Challenge). However state-of-the-art approaches trained to\nperform well on highly-controlled datasets fail to generalize on clinical data\nfrom unseen datasets. Instead of proposing another improvement of the\nsegmentation accuracy, we propose a novel method robust to domain shift and\nperforming well on unseen datasets, called DeepLesionBrain (DLB). This\ngeneralization property results from three main contributions. First, DLB is\nbased on a large group of compact 3D CNNs. This spatially distributed strategy\nensures a robust prediction despite the risk of generalization failure of some\nindividual networks. Second, DLB includes a new image quality data augmentation\nto reduce dependency to training data specificity (e.g., acquisition protocol).\nFinally, to learn a more generalizable representation of MS lesions, we propose\na hierarchical specialization learning (HSL). HSL is performed by pre-training\na generic network over the whole brain, before using its weights as\ninitialization to locally specialized networks. By this end, DLB learns both\ngeneric features extracted at global image level and specific features\nextracted at local image level. DLB generalization was validated in\ncross-dataset experiments on MSSEG'16, ISBI challenge, and in-house datasets.\nDuring experiments, DLB showed higher segmentation accuracy, better\nsegmentation consistency and greater generalization performance compared to\nstate-of-the-art methods. Therefore, DLB offers a robust framework well-suited\nfor clinical practice.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 21:33:53 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 17:00:12 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Kamraoui", "Reda Abdellah", ""], ["Ta", "Vinh-Thong", ""], ["Tourdias", "Thomas", ""], ["Mansencal", "Boris", ""], ["Manjon", "Jos\u00e9 V", ""], ["Coup\u00e9", "Pierrick", ""]]}, {"id": "2012.07962", "submitter": "Michalis Lazarou Mr", "authors": "Michalis Lazarou, Yannis Avrithis, Tania Stathaki", "title": "Iterative label cleaning for transductive and semi-supervised few-shot\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning amounts to learning representations and acquiring knowledge\nsuch that novel tasks may be solved with both supervision and data being\nlimited. Improved performance is possible by transductive inference, where the\nentire test set is available concurrently, and semi-supervised learning, where\nmore unlabeled data is available. These problems are closely related because\nthere is little or no adaptation of the representation in novel tasks.\n  Focusing on these two settings, we introduce a new algorithm that leverages\nthe manifold structure of the labeled and unlabeled data distribution to\npredict pseudo-labels, while balancing over classes and using the loss value\ndistribution of a limited-capacity classifier to select the cleanest labels,\niterately improving the quality of pseudo-labels. Our solution sets new state\nof the art on four benchmark datasets, namely \\emph{mini}ImageNet,\n\\emph{tiered}ImageNet, CUB and CIFAR-FS, while being robust over feature space\npre-processing and the quantity of available data.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 21:54:11 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Lazarou", "Michalis", ""], ["Avrithis", "Yannis", ""], ["Stathaki", "Tania", ""]]}, {"id": "2012.07968", "submitter": "Jun Jet Tai Jet", "authors": "Jun Jet Tai, Mauro S. Innocente, Owais Mehmood", "title": "FasteNet: A Fast Railway Fastener Detector", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, a novel high-speed railway fastener detector is introduced.\nThis fully convolutional network, dubbed FasteNet, foregoes the notion of\nbounding boxes and performs detection directly on a predicted saliency map.\nFastenet uses transposed convolutions and skip connections, the effective\nreceptive field of the network is 1.5$\\times$ larger than the average size of a\nfastener, enabling the network to make predictions with high confidence,\nwithout sacrificing output resolution. In addition, due to the saliency map\napproach, the network is able to vote for the presence of a fastener up to 30\ntimes per fastener, boosting prediction accuracy. Fastenet is capable of\nrunning at 110 FPS on an Nvidia GTX 1080, while taking in inputs of\n1600$\\times$512 with an average of 14 fasteners per image. Our source is open\nhere: https://github.com/jjshoots/DL\\_FasteNet.git\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 22:04:47 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Tai", "Jun Jet", ""], ["Innocente", "Mauro S.", ""], ["Mehmood", "Owais", ""]]}, {"id": "2012.07999", "submitter": "ShahRukh Athar", "authors": "ShahRukh Athar, Albert Pumarola, Francesc Moreno-Noguer, Dimitris\n  Samaras", "title": "FaceDet3D: Facial Expressions with 3D Geometric Detail Prediction", "comments": "Fixed errors in acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Facial Expressions induce a variety of high-level details on the 3D face\ngeometry. For example, a smile causes the wrinkling of cheeks or the formation\nof dimples, while being angry often causes wrinkling of the forehead. Morphable\nModels (3DMMs) of the human face fail to capture such fine details in their\nPCA-based representations and consequently cannot generate such details when\nused to edit expressions. In this work, we introduce FaceDet3D, a\nfirst-of-its-kind method that generates - from a single image - geometric\nfacial details that are consistent with any desired target expression. The\nfacial details are represented as a vertex displacement map and used then by a\nNeural Renderer to photo-realistically render novel images of any single image\nin any desired expression and view. The project website is:\nhttp://shahrukhathar.github.io/2020/12/14/FaceDet3D.html\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 23:07:38 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 18:39:25 GMT"}, {"version": "v3", "created": "Wed, 23 Dec 2020 18:22:48 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Athar", "ShahRukh", ""], ["Pumarola", "Albert", ""], ["Moreno-Noguer", "Francesc", ""], ["Samaras", "Dimitris", ""]]}, {"id": "2012.08011", "submitter": "Danish Syed", "authors": "Danish Syed, Naman Gandhi, Arushi Arora and Nilesh Kadam", "title": "DeepGamble: Towards unlocking real-time player intelligence using\n  multi-layer instance segmentation and attribute detection", "comments": "2020 19th IEEE International Conference on Machine Learning and\n  Applications (ICMLA)", "journal-ref": null, "doi": "10.1109/ICMLA51294.2020.00067", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Annually the gaming industry spends approximately $15 billion in marketing\nreinvestment. However, this amount is spent without any consideration for the\nskill and luck of the player. For a casino, an unskilled player could fetch ~4\ntimes more revenue than a skilled player. This paper describes a video\nrecognition system that is based on an extension of the Mask R-CNN model. Our\nsystem digitizes the game of blackjack by detecting cards and player bets in\nreal-time and processes decisions they took in order to create accurate player\npersonas. Our proposed supervised learning approach consists of a specialized\nthree-stage pipeline that takes images from two viewpoints of the casino table\nand does instance segmentation to generate masks on proposed regions of\ninterest. These predicted masks along with derivative features are used to\nclassify image attributes that are passed onto the next stage to assimilate the\ngameplay understanding. Our end-to-end model yields an accuracy of ~95% for the\nmain bet detection and ~97% for card detection in a controlled environment\ntrained using transfer learning approach with 900 training examples. Our\napproach is generalizable and scalable and shows promising results in varied\ngaming scenarios and test data. Such granular level gathered data, helped in\nunderstanding player's deviation from optimum strategy and thereby separate the\nskill of the player from the luck of the game. Our system also assesses the\nlikelihood of card counting by correlating the player's betting pattern to the\ndeck's scaled count. Such a system lets casinos flag fraudulent activity and\ncalculate expected personalized profitability for each player and tailor their\nmarketing reinvestment decisions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 23:46:26 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Syed", "Danish", ""], ["Gandhi", "Naman", ""], ["Arora", "Arushi", ""], ["Kadam", "Nilesh", ""]]}, {"id": "2012.08026", "submitter": "Miaowei Wang", "authors": "Miaowei Wang, Alexander William Mohacey, Hongyu Wang, James Apfel", "title": "Classification of Smoking and Calling using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since 2014, very deep convolutional neural networks have been proposed and\nbecome the must-have weapon for champions in all kinds of competition. In this\nreport, a pipeline is introduced to perform the classification of smoking and\ncalling by modifying the pretrained inception V3. Brightness enhancing based on\ndeep learning is implemented to improve the classification of this\nclassification task along with other useful training tricks. Based on the\nquality and quantity results, it can be concluded that this pipeline with small\nbiased samples is practical and useful with high accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 00:59:57 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Wang", "Miaowei", ""], ["Mohacey", "Alexander William", ""], ["Wang", "Hongyu", ""], ["Apfel", "James", ""]]}, {"id": "2012.08041", "submitter": "Xinyu Li", "authors": "Xinyu Li, Chunhui Liu, Bing Shuai, Yi Zhu, Hao Chen, Joseph Tighe", "title": "NUTA: Non-uniform Temporal Aggregation for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the world of action recognition research, one primary focus has been on\nhow to construct and train networks to model the spatial-temporal volume of an\ninput video. These methods typically uniformly sample a segment of an input\nclip (along the temporal dimension). However, not all parts of a video are\nequally important to determine the action in the clip. In this work, we focus\ninstead on learning where to extract features, so as to focus on the most\ninformative parts of the video. We propose a method called the non-uniform\ntemporal aggregation (NUTA), which aggregates features only from informative\ntemporal segments. We also introduce a synchronization method that allows our\nNUTA features to be temporally aligned with traditional uniformly sampled video\nfeatures, so that both local and clip-level features can be combined. Our model\nhas achieved state-of-the-art performance on four widely used large-scale\naction-recognition datasets (Kinetics400, Kinetics700, Something-something V2\nand Charades). In addition, we have created a visualization to illustrate how\nthe proposed NUTA method selects only the most relevant parts of a video clip.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 02:03:37 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Li", "Xinyu", ""], ["Liu", "Chunhui", ""], ["Shuai", "Bing", ""], ["Zhu", "Yi", ""], ["Chen", "Hao", ""], ["Tighe", "Joseph", ""]]}, {"id": "2012.08048", "submitter": "Rui Li", "authors": "Rui Li, Qing Mao, Pei Wang, Xiantuo He, Yu Zhu, Jinqiu Sun, Yanning\n  Zhang", "title": "Semantic-Guided Representation Enhancement for Self-supervised Monocular\n  Trained Depth Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised depth estimation has shown its great effectiveness in\nproducing high quality depth maps given only image sequences as input. However,\nits performance usually drops when estimating on border areas or objects with\nthin structures due to the limited depth representation ability. In this paper,\nwe address this problem by proposing a semantic-guided depth representation\nenhancement method, which promotes both local and global depth feature\nrepresentations by leveraging rich contextual information. In stead of a single\ndepth network as used in conventional paradigms, we propose an extra semantic\nsegmentation branch to offer extra contextual features for depth estimation.\nBased on this framework, we enhance the local feature representation by\nsampling and feeding the point-based features that locate on the semantic edges\nto an individual Semantic-guided Edge Enhancement module (SEEM), which is\nspecifically designed for promoting depth estimation on the challenging\nsemantic borders. Then, we improve the global feature representation by\nproposing a semantic-guided multi-level attention mechanism, which enhances the\nsemantic and depth features by exploring pixel-wise correlations in the\nmulti-level depth decoding scheme. Extensive experiments validate the distinct\nsuperiority of our method in capturing highly accurate depth on the challenging\nimage areas such as semantic category borders and thin objects. Both\nquantitative and qualitative experiments on KITTI show that our method\noutperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 02:24:57 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Li", "Rui", ""], ["Mao", "Qing", ""], ["Wang", "Pei", ""], ["He", "Xiantuo", ""], ["Zhu", "Yu", ""], ["Sun", "Jinqiu", ""], ["Zhang", "Yanning", ""]]}, {"id": "2012.08051", "submitter": "Jose Dolz", "authors": "Jose Dolz, Christian Desrosiers, Ismail Ben Ayed", "title": "Teach me to segment with mixed supervision: Confident students become\n  masters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep segmentation neural networks require large training datasets with\npixel-wise segmentations, which are expensive to obtain in practice. Mixed\nsupervision could mitigate this difficulty, with a small fraction of the data\ncontaining complete pixel-wise annotations, while the rest being less\nsupervised, e.g., only a handful of pixels are labeled. In this work, we\npropose a dual-branch architecture, where the upper branch (teacher) receives\nstrong annotations, while the bottom one (student) is driven by limited\nsupervision and guided by the upper branch. In conjunction with a standard\ncross-entropy over the labeled pixels, our novel formulation integrates two\nimportant terms: (i) a Shannon entropy loss defined over the less-supervised\nimages, which encourages confident student predictions at the bottom branch;\nand (ii) a Kullback-Leibler (KL) divergence, which transfers the knowledge from\nthe predictions generated by the strongly supervised branch to the\nless-supervised branch, and guides the entropy (student-confidence) term to\navoid trivial solutions. Very interestingly, we show that the synergy between\nthe entropy and KL divergence yields substantial improvements in performances.\nFurthermore, we discuss an interesting link between Shannon-entropy\nminimization and standard pseudo-mask generation and argue that the former\nshould be preferred over the latter for leveraging information from unlabeled\npixels. Through a series of quantitative and qualitative experiments, we show\nthe effectiveness of the proposed formulation in segmenting the left-ventricle\nendocardium in MRI images. We demonstrate that our method significantly\noutperforms other strategies to tackle semantic segmentation within a\nmixed-supervision framework. More interestingly, and in line with recent\nobservations in classification, we show that the branch trained with reduced\nsupervision largely outperforms the teacher.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 02:51:36 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Dolz", "Jose", ""], ["Desrosiers", "Christian", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "2012.08054", "submitter": "Liang Liao", "authors": "Liang Liao, Jing Xiao, Zheng Wang, Chia-Wen Lin, Shin'ichi Satoh", "title": "Image Inpainting Guided by Coherence Priors of Semantics and Textures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing inpainting methods have achieved promising performance in recovering\ndefected images of specific scenes. However, filling holes involving multiple\nsemantic categories remains challenging due to the obscure semantic boundaries\nand the mixture of different semantic textures. In this paper, we introduce\ncoherence priors between the semantics and textures which make it possible to\nconcentrate on completing separate textures in a semantic-wise manner.\nSpecifically, we adopt a multi-scale joint optimization framework to first\nmodel the coherence priors and then accordingly interleavingly optimize image\ninpainting and semantic segmentation in a coarse-to-fine manner. A\nSemantic-Wise Attention Propagation (SWAP) module is devised to refine\ncompleted image textures across scales by exploring non-local semantic\ncoherence, which effectively mitigates mix-up of textures. We also propose two\ncoherence losses to constrain the consistency between the semantics and the\ninpainted image in terms of the overall structure and detailed textures.\nExperimental results demonstrate the superiority of our proposed method for\nchallenging cases with complex holes.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 02:59:37 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Liao", "Liang", ""], ["Xiao", "Jing", ""], ["Wang", "Zheng", ""], ["Lin", "Chia-Wen", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "2012.08055", "submitter": "Feixiang Lu", "authors": "Feixiang Lu, Zongdai Liu, Hui Miao, Peng Wang, Liangjun Zhang, Ruigang\n  Yang, Dinesh Manocha, Bin Zhou", "title": "Fine-Grained Vehicle Perception via 3D Part-Guided Visual Data\n  Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Holistically understanding an object and its 3D movable parts through visual\nperception models is essential for enabling an autonomous agent to interact\nwith the world. For autonomous driving, the dynamics and states of vehicle\nparts such as doors, the trunk, and the bonnet can provide meaningful semantic\ninformation and interaction states, which are essential to ensuring the safety\nof the self-driving vehicle. Existing visual perception models mainly focus on\ncoarse parsing such as object bounding box detection or pose estimation and\nrarely tackle these situations. In this paper, we address this important\nautonomous driving problem by solving three critical issues. First, to deal\nwith data scarcity, we propose an effective training data generation process by\nfitting a 3D car model with dynamic parts to vehicles in real images before\nreconstructing human-vehicle interaction (VHI) scenarios. Our approach is fully\nautomatic without any human interaction, which can generate a large number of\nvehicles in uncommon states (VUS) for training deep neural networks (DNNs).\nSecond, to perform fine-grained vehicle perception, we present a multi-task\nnetwork for VUS parsing and a multi-stream network for VHI parsing. Third, to\nquantitatively evaluate the effectiveness of our data augmentation approach, we\nbuild the first VUS dataset in real traffic scenarios (e.g., getting on/out or\nplacing/removing luggage). Experimental results show that our approach advances\nother baseline methods in 2D detection and instance segmentation by a big\nmargin (over 8%). In addition, our network yields large improvements in\ndiscovering and understanding these uncommon cases. Moreover, we have released\nthe source code, the dataset, and the trained model on Github\n(https://github.com/zongdai/EditingForDNN).\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 03:03:38 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 09:04:58 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Lu", "Feixiang", ""], ["Liu", "Zongdai", ""], ["Miao", "Hui", ""], ["Wang", "Peng", ""], ["Zhang", "Liangjun", ""], ["Yang", "Ruigang", ""], ["Manocha", "Dinesh", ""], ["Zhou", "Bin", ""]]}, {"id": "2012.08072", "submitter": "Qicheng Lao", "authors": "Qicheng Lao, Xiang Jiang, Mohammad Havaei", "title": "Hypothesis Disparity Regularized Mutual Information Maximization", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hypothesis disparity regularized mutual information\nmaximization~(HDMI) approach to tackle unsupervised hypothesis transfer -- as\nan effort towards unifying hypothesis transfer learning (HTL) and unsupervised\ndomain adaptation (UDA) -- where the knowledge from a source domain is\ntransferred solely through hypotheses and adapted to the target domain in an\nunsupervised manner. In contrast to the prevalent HTL and UDA approaches that\ntypically use a single hypothesis, HDMI employs multiple hypotheses to leverage\nthe underlying distributions of the source and target hypotheses. To better\nutilize the crucial relationship among different hypotheses -- as opposed to\nunconstrained optimization of each hypothesis independently -- while adapting\nto the unlabeled target domain through mutual information maximization, HDMI\nincorporates a hypothesis disparity regularization that coordinates the target\nhypotheses jointly learn better target representations while preserving more\ntransferable source knowledge with better-calibrated prediction uncertainty.\nHDMI achieves state-of-the-art adaptation performance on benchmark datasets for\nUDA in the context of HTL, without the need to access the source data during\nthe adaptation.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 03:39:16 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Lao", "Qicheng", ""], ["Jiang", "Xiang", ""], ["Havaei", "Mohammad", ""]]}, {"id": "2012.08096", "submitter": "Lu Chen", "authors": "Lu Chen, Jiao Sun, Wei Xu", "title": "FAWA: Fast Adversarial Watermark Attack on Optical Character Recognition\n  (OCR) Systems", "comments": "16 pages, ECML/PKDD 2020 research trace", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) significantly improved the accuracy of optical\ncharacter recognition (OCR) and inspired many important applications.\nUnfortunately, OCRs also inherit the vulnerabilities of DNNs under adversarial\nexamples. Different from colorful vanilla images, text images usually have\nclear backgrounds. Adversarial examples generated by most existing adversarial\nattacks are unnatural and pollute the background severely. To address this\nissue, we propose the Fast Adversarial Watermark Attack (FAWA) against\nsequence-based OCR models in the white-box manner. By disguising the\nperturbations as watermarks, we can make the resulting adversarial images\nappear natural to human eyes and achieve a perfect attack success rate. FAWA\nworks with either gradient-based or optimization-based perturbation generation.\nIn both letter-level and word-level attacks, our experiments show that in\naddition to natural appearance, FAWA achieves a 100% attack success rate with\n60% less perturbations and 78% fewer iterations on average. In addition, we\nfurther extend FAWA to support full-color watermarks, other languages, and even\nthe OCR accuracy-enhancing mechanism.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 05:19:54 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Chen", "Lu", ""], ["Sun", "Jiao", ""], ["Xu", "Wei", ""]]}, {"id": "2012.08097", "submitter": "Shentong Mo", "authors": "Shentong Mo, Xiaoqing Tan, Jingfei Xia, Pinxu Ren", "title": "Towards Improving Spatiotemporal Action Recognition in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatiotemporal action recognition deals with locating and classifying actions\nin videos. Motivated by the latest state-of-the-art real-time object detector\nYou Only Watch Once (YOWO), we aim to modify its structure to increase action\ndetection precision and reduce computational time. Specifically, we propose\nfour novel approaches in attempts to improve YOWO and address the imbalanced\nclass issue in videos by modifying the loss function. We consider two\nmoderate-sized datasets to apply our modification of YOWO - the popular\nJoint-annotated Human Motion Data Base (J-HMDB-21) and a private dataset of\nrestaurant video footage provided by a Carnegie Mellon University-based\nstartup, Agot.AI. The latter involves fast-moving actions with small objects as\nwell as unbalanced data classes, making the task of action localization more\nchallenging. We implement our proposed methods in the GitHub repository\nhttps://github.com/stoneMo/YOWOv2.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 05:21:50 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Mo", "Shentong", ""], ["Tan", "Xiaoqing", ""], ["Xia", "Jingfei", ""], ["Ren", "Pinxu", ""]]}, {"id": "2012.08099", "submitter": "William Diggin", "authors": "William Diggin and Michael Diggin", "title": "Fast 3D Image Moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An algorithm to efficiently compute the moments of volumetric images is\ndisclosed. The approach demonstrates a reduction in processing time by reducing\nthe computational complexity significantly. Specifically, the algorithm reduces\nmultiplicative complexity from O(n^3) to O(n). Several 2D projection images of\nthe 3D volume are generated. The algorithm computes a set of 2D moments from\nthose 2D images. Those 2D moments are then used to derive the 3D volumetric\nmoments. Examples of use in MRI or CT and related analysis demonstrates the\nbenefit of the Discrete Projection Moment Algorithm. The approach is also\nuseful in computing the moments of a 3D object using a small set of 2D\ntomographic images of that object.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 05:24:12 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Diggin", "William", ""], ["Diggin", "Michael", ""]]}, {"id": "2012.08103", "submitter": "Soo Ye Kim", "authors": "Soo Ye Kim, Hyeonjun Sim, Munchurl Kim", "title": "KOALAnet: Blind Super-Resolution using Kernel-Oriented Adaptive Local\n  Adjustment", "comments": "The first two authors contributed equally to this work. Accepted to\n  CVPR 2021 (camera-ready version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind super-resolution (SR) methods aim to generate a high quality high\nresolution image from a low resolution image containing unknown degradations.\nHowever, natural images contain various types and amounts of blur: some may be\ndue to the inherent degradation characteristics of the camera, but some may\neven be intentional, for aesthetic purposes (e.g. Bokeh effect). In the case of\nthe latter, it becomes highly difficult for SR methods to disentangle the blur\nto remove, and that to leave as is. In this paper, we propose a novel blind SR\nframework based on kernel-oriented adaptive local adjustment (KOALA) of SR\nfeatures, called KOALAnet, which jointly learns spatially-variant degradation\nand restoration kernels in order to adapt to the spatially-variant blur\ncharacteristics in real images. Our KOALAnet outperforms recent blind SR\nmethods for synthesized LR images obtained with randomized degradations, and we\nfurther show that the proposed KOALAnet produces the most natural results for\nartistic photographs with intentional blur, which are not over-sharpened, by\neffectively handling images mixed with in-focus and out-of-focus areas.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 05:54:05 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 05:54:27 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 04:43:11 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Kim", "Soo Ye", ""], ["Sim", "Hyeonjun", ""], ["Kim", "Munchurl", ""]]}, {"id": "2012.08129", "submitter": "Yi-Hsin Chen", "authors": "Cheng-Hsun Lei, Yi-Hsin Chen, Wen-Hsiao Peng, Wei-Chen Chiu", "title": "Class-incremental Learning with Rectified Feature-Graph Preservation", "comments": "Accepted by ACCV 2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address the problem of distillation-based class-incremental\nlearning with a single head. A central theme of this task is to learn new\nclasses that arrive in sequential phases over time while keeping the model's\ncapability of recognizing seen classes with only limited memory for preserving\nseen data samples. Many regularization strategies have been proposed to\nmitigate the phenomenon of catastrophic forgetting. To understand better the\nessence of these regularizations, we introduce a feature-graph preservation\nperspective. Insights into their merits and faults motivate our\nweighted-Euclidean regularization for old knowledge preservation. We further\npropose rectified cosine normalization and show how it can work with binary\ncross-entropy to increase class separation for effective learning of new\nclasses. Experimental results on both CIFAR-100 and ImageNet datasets\ndemonstrate that our method outperforms the state-of-the-art approaches in\nreducing classification error, easing catastrophic forgetting, and encouraging\nevenly balanced accuracy over different classes. Our project page is at :\nhttps://github.com/yhchen12101/FGP-ICL.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 07:26:04 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 09:06:04 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Lei", "Cheng-Hsun", ""], ["Chen", "Yi-Hsin", ""], ["Peng", "Wen-Hsiao", ""], ["Chiu", "Wei-Chen", ""]]}, {"id": "2012.08131", "submitter": "Xinhan Di", "authors": "Xinhan Di, Pengqian Yu, Danfeng Yang, Hong Zhu, Changyu Sun, YinDong\n  Liu", "title": "Deep Layout of Custom-size Furniture through Multiple-domain Learning", "comments": "Submitted to CV Conference. arXiv admin note: text overlap with\n  arXiv:2006.13527", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a multiple-domain model for producing a custom-size\nfurniture layout in the interior scene. This model is aimed to support\nprofessional interior designers to produce interior decoration solutions with\ncustom-size furniture more quickly. The proposed model combines a deep layout\nmodule, a domain attention module, a dimensional domain transfer module, and a\ncustom-size module in the end-end training. Compared with the prior work on\nscene synthesis, our proposed model enhances the ability of auto-layout of\ncustom-size furniture in the interior room. We conduct our experiments on a\nreal-world interior layout dataset that contains $710,700$ designs from\nprofessional designers. Our numerical results demonstrate that the proposed\nmodel yields higher-quality layouts of custom-size furniture in comparison with\nthe state-of-art model.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 07:32:13 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Di", "Xinhan", ""], ["Yu", "Pengqian", ""], ["Yang", "Danfeng", ""], ["Zhu", "Hong", ""], ["Sun", "Changyu", ""], ["Liu", "YinDong", ""]]}, {"id": "2012.08143", "submitter": "Nicolas Wagner", "authors": "Nicolas Wagner, Ulrich Schwanecke", "title": "NeuralQAAD: An Efficient Differentiable Framework for High Resolution\n  Point Cloud Compression", "comments": "Prepublication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we propose NeuralQAAD, a differentiable point cloud\ncompression framework that is fast, robust to sampling, and applicable to high\nresolutions. Previous work that is able to handle complex and non-smooth\ntopologies is hardly scaleable to more than just a few thousand points. We\ntackle the task with a novel neural network architecture characterized by\nweight sharing and autodecoding. Our architecture uses parameters much more\nefficiently than previous work, allowing us to be deeper and scalable.\nFuthermore, we show that the currently only tractable training criterion for\npoint cloud compression, the Chamfer distance, performances poorly for high\nresolutions. To overcome this issue, we pair our architecture with a new\ntraining procedure based upon a quadratic assignment problem (QAP) for which we\nstate two approximation algorithms. We solve the QAP in parallel to gradient\ndescent. This procedure acts as a surrogate loss and allows to implicitly\nminimize the more expressive Earth Movers Distance (EMD) even for point clouds\nwith way more than $10^6$ points. As evaluating the EMD on high resolution\npoint clouds is intractable, we propose a divide-and-conquer approach based on\nk-d trees, the EM-kD, as a scaleable and fast but still reliable upper bound\nfor the EMD. NeuralQAAD is demonstrated on COMA, D-FAUST, and Skulls to\nsignificantly outperform the current state-of-the-art visually and in terms of\nthe EM-kD. Skulls is a novel dataset of skull CT-scans which we will make\npublicly available together with our implementation of NeuralQAAD.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 08:18:38 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Wagner", "Nicolas", ""], ["Schwanecke", "Ulrich", ""]]}, {"id": "2012.08149", "submitter": "Wei Xu", "authors": "Wei Xu, Dingkang Liang, Yixiao Zheng, Zhanyu Ma", "title": "Dilated-Scale-Aware Attention ConvNet For Multi-Class Object Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object counting aims to estimate the number of objects in images. The leading\ncounting approaches focus on the single category counting task and achieve\nimpressive performance. Note that there are multiple categories of objects in\nreal scenes. Multi-class object counting expands the scope of application of\nobject counting task. The multi-target detection task can achieve multi-class\nobject counting in some scenarios. However, it requires the dataset annotated\nwith bounding boxes. Compared with the point annotations in mainstream object\ncounting issues, the coordinate box-level annotations are more difficult to\nobtain. In this paper, we propose a simple yet efficient counting network based\non point-level annotations. Specifically, we first change the traditional\noutput channel from one to the number of categories to achieve multiclass\ncounting. Since all categories of objects use the same feature extractor in our\nproposed framework, their features will interfere mutually in the shared\nfeature space. We further design a multi-mask structure to suppress harmful\ninteraction among objects. Extensive experiments on the challenging benchmarks\nillustrate that the proposed method achieves state-of-the-art counting\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 08:38:28 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Xu", "Wei", ""], ["Liang", "Dingkang", ""], ["Zheng", "Yixiao", ""], ["Ma", "Zhanyu", ""]]}, {"id": "2012.08158", "submitter": "Michael Gadermayr", "authors": "Michael Gadermayr, Maximilian Tschuchnig, Lea Maria Stangassinger,\n  Christina Kreutzer, Sebastien Couillard-Despres, Gertie Janneke Oostingh,\n  Anton Hittmair", "title": "Frozen-to-Paraffin: Categorization of Histological Frozen Sections by\n  the Aid of Paraffin Sections and Generative Adversarial Networks", "comments": "Accepted: MICCAI SASHIMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In contrast to paraffin sections, frozen sections can be quickly generated\nduring surgical interventions. This procedure allows surgeons to wait for\nhistological findings during the intervention to base intra-operative decisions\non the outcome of the histology. However, compared to paraffin sections, the\nquality of frozen sections is typically lower, leading to a higher ratio of\nmiss-classification. In this work, we investigated the effect of the section\ntype on automated decision support approaches for classification of thyroid\ncancer. This was enabled by a data set consisting of pairs of sections for\nindividual patients. Moreover, we investigated, whether a frozen-to-paraffin\ntranslation could help to optimize classification scores. Finally, we propose a\nspecific data augmentation strategy to deal with a small amount of training\ndata and to increase classification accuracy even further.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 09:09:15 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 07:31:28 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Gadermayr", "Michael", ""], ["Tschuchnig", "Maximilian", ""], ["Stangassinger", "Lea Maria", ""], ["Kreutzer", "Christina", ""], ["Couillard-Despres", "Sebastien", ""], ["Oostingh", "Gertie Janneke", ""], ["Hittmair", "Anton", ""]]}, {"id": "2012.08168", "submitter": "Haiyu Wu", "authors": "Fukang Tian, Haiyu Wu, Bo Xu", "title": "Research on All-content Text Recognition Method for Financial Ticket\n  Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of the economy, the number of financial tickets\nincreases rapidly. The traditional manual invoice reimbursement and financial\naccounting system bring more and more burden to financial accountants.\nTherefore, based on the research and analysis of a large number of real\nfinancial ticket data, we designed an accurate and efficient all contents text\ndetection and recognition method based on deep learning. This method has higher\nrecognition accuracy and recall rate and can meet the actual requirements of\nfinancial accounting work. In addition, we propose a Financial Ticket Character\nRecognition Framework (FTCRF). According to the characteristics of Chinese\ncharacter recognition, this framework contains a two-step information\nextraction method, which can improve the speed of Chinese character\nrecognition. The experimental results show that the average recognition\naccuracy of this method is 91.75\\% for character sequence and 87\\% for the\nwhole ticket. The availability and effectiveness of this method are verified by\na commercial application system, which significantly improves the efficiency of\nthe financial accounting system.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 09:39:32 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Tian", "Fukang", ""], ["Wu", "Haiyu", ""], ["Xu", "Bo", ""]]}, {"id": "2012.08191", "submitter": "Tom Monnier", "authors": "Tom Monnier, Mathieu Aubry", "title": "docExtractor: An off-the-shelf historical document element extraction", "comments": "Accepted at 2020 17th International Conference on Frontiers in\n  Handwriting Recognition (ICFHR) (oral). Project webpage:\n  http://imagine.enpc.fr/~monniert/docExtractor/", "journal-ref": null, "doi": "10.1109/ICFHR2020.2020.00027", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present docExtractor, a generic approach for extracting visual elements\nsuch as text lines or illustrations from historical documents without requiring\nany real data annotation. We demonstrate it provides high-quality performances\nas an off-the-shelf system across a wide variety of datasets and leads to\nresults on par with state-of-the-art when fine-tuned. We argue that the\nperformance obtained without fine-tuning on a specific dataset is critical for\napplications, in particular in digital humanities, and that the line-level page\nsegmentation we address is the most relevant for a general purpose element\nextraction engine. We rely on a fast generator of rich synthetic documents and\ndesign a fully convolutional network, which we show to generalize better than a\ndetection-based approach. Furthermore, we introduce a new public dataset dubbed\nIlluHisDoc dedicated to the fine evaluation of illustration segmentation in\nhistorical documents.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 10:19:18 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Monnier", "Tom", ""], ["Aubry", "Mathieu", ""]]}, {"id": "2012.08195", "submitter": "Darya Trofimova", "authors": "Darya Trofimova, Tim Adler, Lisa Kausch, Lynton Ardizzone, Klaus\n  Maier-Hein, Ulrich K\\\"othe, Carsten Rother and Lena Maier-Hein", "title": "Representing Ambiguity in Registration Problems with Conditional\n  Invertible Neural Networks", "comments": "The paper got accepted at Medical Imaging Meets NeurIPS Workshop at\n  Neural Information Processing Systems 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration is the basis for many applications in the fields of\nmedical image computing and computer assisted interventions. One example is the\nregistration of 2D X-ray images with preoperative three-dimensional computed\ntomography (CT) images in intraoperative surgical guidance systems. Due to the\nhigh safety requirements in medical applications, estimating registration\nuncertainty is of a crucial importance in such a scenario. However, previously\nproposed methods, including classical iterative registration methods and deep\nlearning-based methods have one characteristic in common: They lack the\ncapacity to represent the fact that a registration problem may be inherently\nambiguous, meaning that multiple (substantially different) plausible solutions\nexist. To tackle this limitation, we explore the application of invertible\nneural networks (INN) as core component of a registration methodology. In the\nproposed framework, INNs enable going beyond point estimates as network output\nby representing the possible solutions to a registration problem by a\nprobability distribution that encodes different plausible solutions via\nmultiple modes. In a first feasibility study, we test the approach for a 2D 3D\nregistration setting by registering spinal CT volumes to X-ray images. To this\nend, we simulate the X-ray images taken by a C-Arm with multiple orientations\nusing the principle of digitially reconstructed radiographs (DRRs). Due to the\nsymmetry of human spine, there are potentially multiple substantially different\nposes of the C-Arm that can lead to similar projections. The hypothesis of this\nwork is that the proposed approach is able to identify multiple solutions in\nsuch ambiguous registration problems.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 10:28:41 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Trofimova", "Darya", ""], ["Adler", "Tim", ""], ["Kausch", "Lisa", ""], ["Ardizzone", "Lynton", ""], ["Maier-Hein", "Klaus", ""], ["K\u00f6the", "Ulrich", ""], ["Rother", "Carsten", ""], ["Maier-Hein", "Lena", ""]]}, {"id": "2012.08197", "submitter": "Norman M\\\"uller", "authors": "Norman M\\\"uller, Yu-Shiang Wong, Niloy J. Mitra, Angela Dai and\n  Matthias Nie{\\ss}ner", "title": "Seeing Behind Objects for 3D Multi-Object Tracking in RGB-D Sequences", "comments": "Video: https://youtu.be/F2zs9AMRxeg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-object tracking from RGB-D video sequences is a challenging problem due\nto the combination of changing viewpoints, motion, and occlusions over time. We\nobserve that having the complete geometry of objects aids in their tracking,\nand thus propose to jointly infer the complete geometry of objects as well as\ntrack them, for rigidly moving objects over time. Our key insight is that\ninferring the complete geometry of the objects significantly helps in tracking.\nBy hallucinating unseen regions of objects, we can obtain additional\ncorrespondences between the same instance, thus providing robust tracking even\nunder strong change of appearance. From a sequence of RGB-D frames, we detect\nobjects in each frame and learn to predict their complete object geometry as\nwell as a dense correspondence mapping into a canonical space. This allows us\nto derive 6DoF poses for the objects in each frame, along with their\ncorrespondence between frames, providing robust object tracking across the\nRGB-D sequence. Experiments on both synthetic and real-world RGB-D data\ndemonstrate that we achieve state-of-the-art performance on dynamic object\ntracking. Furthermore, we show that our object completion significantly helps\ntracking, providing an improvement of $6.5\\%$ in mean MOTA.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 10:33:21 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 14:39:51 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["M\u00fcller", "Norman", ""], ["Wong", "Yu-Shiang", ""], ["Mitra", "Niloy J.", ""], ["Dai", "Angela", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2012.08205", "submitter": "Tobias Scheck", "authors": "Tobias Scheck, Ana Perez Grassi, Gangolf Hirtz", "title": "Unsupervised Domain Adaptation from Synthetic to Real Images for\n  Anchorless Object Detection", "comments": "Paper accepted in VISAPP 2021", "journal-ref": null, "doi": "10.5220/0010202503190327", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Synthetic images are one of the most promising solutions to avoid high costs\nassociated with generating annotated datasets to train supervised convolutional\nneural networks (CNN). However, to allow networks to generalize knowledge from\nsynthetic to real images, domain adaptation methods are necessary. This paper\nimplements unsupervised domain adaptation (UDA) methods on an anchorless object\ndetector. Given their good performance, anchorless detectors are increasingly\nattracting attention in the field of object detection. While their results are\ncomparable to the well-established anchor-based methods, anchorless detectors\nare considerably faster. In our work, we use CenterNet, one of the most recent\nanchorless architectures, for a domain adaptation problem involving synthetic\nimages. Taking advantage of the architecture of anchorless detectors, we\npropose to adjust two UDA methods, viz., entropy minimization and maximum\nsquares loss, originally developed for segmentation, to object detection. Our\nresults show that the proposed UDA methods can increase the mAPfrom61 %to69\n%with respect to direct transfer on the considered anchorless detector. The\ncode is available: https://github.com/scheckmedia/centernet-uda.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 10:51:43 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Scheck", "Tobias", ""], ["Grassi", "Ana Perez", ""], ["Hirtz", "Gangolf", ""]]}, {"id": "2012.08216", "submitter": "Denys Rozumnyi", "authors": "Denys Rozumnyi, Jiri Matas, Filip Sroubek, Marc Pollefeys, Martin R.\n  Oswald", "title": "FMODetect: Robust Detection and Trajectory Estimation of Fast Moving\n  Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose the first learning-based approach for detection and trajectory\nestimation of fast moving objects. Such objects are highly blurred and move\nover large distances within one video frame. Fast moving objects are associated\nwith a deblurring and matting problem, also called deblatting. Instead of\nsolving the complex deblatting problem jointly, we split the problem into\nmatting and deblurring and solve them separately. The proposed method first\ndetects all fast moving objects as a truncated distance function to the\ntrajectory. Subsequently, a matting and fitting network for each detected\nobject estimates the object trajectory and its blurred appearance without\nbackground. For the sharp appearance estimation, we propose an energy\nminimization based deblurring. The state-of-the-art methods are outperformed in\nterms of trajectory estimation and sharp appearance reconstruction. Compared to\nother methods, such as deblatting, the inference is of several orders of\nmagnitude faster and allows applications such as real-time fast moving object\ndetection and retrieval in large video collections.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 11:05:34 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Rozumnyi", "Denys", ""], ["Matas", "Jiri", ""], ["Sroubek", "Filip", ""], ["Pollefeys", "Marc", ""], ["Oswald", "Martin R.", ""]]}, {"id": "2012.08226", "submitter": "Minsu Kim", "authors": "Minsu Kim, Sunghun Joung, Seungryong Kim, JungIn Park, Ig-Jae Kim,\n  Kwanghoon Sohn", "title": "Cross-Domain Grouping and Alignment for Domain Adaptive Semantic\n  Segmentation", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing techniques to adapt semantic segmentation networks across the source\nand target domains within deep convolutional neural networks (CNNs) deal with\nall the samples from the two domains in a global or category-aware manner. They\ndo not consider an inter-class variation within the target domain itself or\nestimated category, providing the limitation to encode the domains having a\nmulti-modal data distribution. To overcome this limitation, we introduce a\nlearnable clustering module, and a novel domain adaptation framework called\ncross-domain grouping and alignment. To cluster the samples across domains with\nan aim to maximize the domain alignment without forgetting precise segmentation\nability on the source domain, we present two loss functions, in particular, for\nencouraging semantic consistency and orthogonality among the clusters. We also\npresent a loss so as to solve a class imbalance problem, which is the other\nlimitation of the previous methods. Our experiments show that our method\nconsistently boosts the adaptation performance in semantic segmentation,\noutperforming the state-of-the-arts on various domain adaptation settings.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 11:36:21 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 07:07:25 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Kim", "Minsu", ""], ["Joung", "Sunghun", ""], ["Kim", "Seungryong", ""], ["Park", "JungIn", ""], ["Kim", "Ig-Jae", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "2012.08228", "submitter": "Yi Zhou", "authors": "Yi Zhou, Hongdong Li, Laurent Kneip", "title": "Canny-VO: Visual Odometry with RGB-D Cameras based on Geometric 3D-2D\n  Edge Alignment", "comments": null, "journal-ref": "IEEE Transactions on Robotics ( Volume: 35, Issue: 1, Feb. 2019)", "doi": "10.1109/TRO.2018.2875382", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The present paper reviews the classical problem of free-form curve\nregistration and applies it to an efficient RGBD visual odometry system called\nCanny-VO, as it efficiently tracks all Canny edge features extracted from the\nimages. Two replacements for the distance transformation commonly used in edge\nregistration are proposed: Approximate Nearest Neighbour Fields and Oriented\nNearest Neighbour Fields. 3D2D edge alignment benefits from these alternative\nformulations in terms of both efficiency and accuracy. It removes the need for\nthe more computationally demanding paradigms of datato-model registration,\nbilinear interpolation, and sub-gradient computation. To ensure robustness of\nthe system in the presence of outliers and sensor noise, the registration is\nformulated as a maximum a posteriori problem, and the resulting weighted least\nsquares objective is solved by the iteratively re-weighted least squares\nmethod. A variety of robust weight functions are investigated and the optimal\nchoice is made based on the statistics of the residual errors. Efficiency is\nfurthermore boosted by an adaptively sampled definition of the nearest\nneighbour fields. Extensive evaluations on public SLAM benchmark sequences\ndemonstrate state-of-the-art performance and an advantage over classical\nEuclidean distance fields.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 11:42:17 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 02:57:47 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Zhou", "Yi", ""], ["Li", "Hongdong", ""], ["Kneip", "Laurent", ""]]}, {"id": "2012.08236", "submitter": "Peisen Zhao", "authors": "Chen Ju, Peisen Zhao, Ya Zhang, Yanfeng Wang, Qi Tian", "title": "Point-Level Temporal Action Localization: Bridging Fully-supervised\n  Proposals to Weakly-supervised Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point-Level temporal action localization (PTAL) aims to localize actions in\nuntrimmed videos with only one timestamp annotation for each action instance.\nExisting methods adopt the frame-level prediction paradigm to learn from the\nsparse single-frame labels. However, such a framework inevitably suffers from a\nlarge solution space. This paper attempts to explore the proposal-based\nprediction paradigm for point-level annotations, which has the advantage of\nmore constrained solution space and consistent predictions among neighboring\nframes. The point-level annotations are first used as the keypoint supervision\nto train a keypoint detector. At the location prediction stage, a simple but\neffective mapper module, which enables back-propagation of training errors, is\nthen introduced to bridge the fully-supervised framework with weak supervision.\nTo our best of knowledge, this is the first work to leverage the\nfully-supervised paradigm for the point-level setting. Experiments on THUMOS14,\nBEOID, and GTEA verify the effectiveness of our proposed method both\nquantitatively and qualitatively, and demonstrate that our method outperforms\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 12:11:48 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Ju", "Chen", ""], ["Zhao", "Peisen", ""], ["Zhang", "Ya", ""], ["Wang", "Yanfeng", ""], ["Tian", "Qi", ""]]}, {"id": "2012.08241", "submitter": "Yang He", "authors": "Yang He and Maximilian Zenk and Mario Fritz", "title": "CosSGD: Nonlinear Quantization for Communication-efficient Federated\n  Learning", "comments": "Source code will be released when published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning facilitates learning across clients without transferring\nlocal data on these clients to a central server. Despite the success of the\nfederated learning method, it remains to improve further w.r.t communicating\nthe most critical information to update a model under limited communication\nconditions, which can benefit this learning scheme into a wide range of\napplication scenarios. In this work, we propose a nonlinear quantization for\ncompressed stochastic gradient descent, which can be easily utilized in\nfederated learning. Based on the proposed quantization, our system\nsignificantly reduces the communication cost by up to three orders of\nmagnitude, while maintaining convergence and accuracy of the training process\nto a large extent. Extensive experiments are conducted on image classification\nand brain tumor semantic segmentation using the MNIST, CIFAR-10 and BraTS\ndatasets where we show state-of-the-art effectiveness and impressive\ncommunication efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 12:20:28 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["He", "Yang", ""], ["Zenk", "Maximilian", ""], ["Fritz", "Mario", ""]]}, {"id": "2012.08243", "submitter": "Radu P Horaud", "authors": "Andrei Zaharescu and Radu Horaud", "title": "Robust Factorization Methods Using a Gaussian/Uniform Mixture Model", "comments": null, "journal-ref": "International Journal of Computer Vision, 81(3), 2009", "doi": "10.1007/s11263-008-0169-x", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we address the problem of building a class of robust\nfactorization algorithms that solve for the shape and motion parameters with\nboth affine (weak perspective) and perspective camera models. We introduce a\nGaussian/uniform mixture model and its associated EM algorithm. This allows us\nto address robust parameter estimation within a data clustering approach. We\npropose a robust technique that works with any affine factorization method and\nmakes it robust to outliers. In addition, we show how such a framework can be\nfurther embedded into an iterative perspective factorization scheme. We carry\nout a large number of experiments to validate our algorithms and to compare\nthem with existing ones. We also compare our approach with factorization\nmethods that use M-estimators.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 12:21:33 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Zaharescu", "Andrei", ""], ["Horaud", "Radu", ""]]}, {"id": "2012.08248", "submitter": "Liran Azaria", "authors": "Liran Azaria and Dan Raviv", "title": "Geometry Enhancements from Visual Content: Going Beyond Ground Truth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a new cyclic architecture that extracts high-frequency\npatterns from images and re-insert them as geometric features. This procedure\nallows us to enhance the resolution of low-cost depth sensors capturing fine\ndetails on the one hand and being loyal to the scanned ground truth on the\nother. We present state-of-the-art results for depth super-resolution tasks and\nas well as visually attractive, enhanced generated 3D models.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 12:28:44 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Azaria", "Liran", ""], ["Raviv", "Dan", ""]]}, {"id": "2012.08261", "submitter": "Michail Christos Doukas", "authors": "Michail Christos Doukas, Stefanos Zafeiriou, Viktoriia Sharmanska", "title": "HeadGAN: One-shot Neural Head Synthesis and Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent attempts to solve the problem of head reenactment using a single\nreference image have shown promising results. However, most of them either\nperform poorly in terms of photo-realism, or fail to meet the identity\npreservation problem, or do not fully transfer the driving pose and expression.\nWe propose HeadGAN, a novel system that conditions synthesis on 3D face\nrepresentations, which can be extracted from any driving video and adapted to\nthe facial geometry of any reference image, disentangling identity from\nexpression. We further improve mouth movements, by utilising audio features as\na complementary input. The 3D face representation enables HeadGAN to be further\nused as an efficient method for compression and reconstruction and a tool for\nexpression and pose editing.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 12:51:32 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 15:19:28 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Doukas", "Michail Christos", ""], ["Zafeiriou", "Stefanos", ""], ["Sharmanska", "Viktoriia", ""]]}, {"id": "2012.08270", "submitter": "Lina Liu", "authors": "Lina Liu, Xibin Song, Xiaoyang Lyu, Junwei Diao, Mengmeng Wang, Yong\n  Liu, Liangjun Zhang", "title": "FCFR-Net: Feature Fusion based Coarse-to-Fine Residual Learning for\n  Monocular Depth Completion", "comments": "9 pages, 5 figures. Accepted to 35th AAAI Conference on Artificial\n  Intelligence (AAAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth completion aims to recover a dense depth map from a sparse depth map\nwith the corresponding color image as input. Recent approaches mainly formulate\nthe depth completion as a one-stage end-to-end learning task, which outputs\ndense depth maps directly. However, the feature extraction and supervision in\none-stage frameworks are insufficient, limiting the performance of these\napproaches. To address this problem, we propose a novel end-to-end residual\nlearning framework, which formulates the depth completion as a two-stage\nlearning task, i.e., a sparse-to-coarse stage and a coarse-to-fine stage.\nFirst, a coarse dense depth map is obtained by a simple CNN framework. Then, a\nrefined depth map is further obtained using a residual learning strategy in the\ncoarse-to-fine stage with coarse depth map and color image as input. Specially,\nin the coarse-to-fine stage, a channel shuffle extraction operation is utilized\nto extract more representative features from color image and coarse depth map,\nand an energy based fusion operation is exploited to effectively fuse these\nfeatures obtained by channel shuffle operation, thus leading to more accurate\nand refined depth maps. We achieve SoTA performance in RMSE on KITTI benchmark.\nExtensive experiments on other datasets future demonstrate the superiority of\nour approach over current state-of-the-art depth completion approaches.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 13:09:56 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Liu", "Lina", ""], ["Song", "Xibin", ""], ["Lyu", "Xiaoyang", ""], ["Diao", "Junwei", ""], ["Wang", "Mengmeng", ""], ["Liu", "Yong", ""], ["Zhang", "Liangjun", ""]]}, {"id": "2012.08274", "submitter": "Anton\\'in Vobeck\\'y", "authors": "Anton\\'in Vobeck\\'y, David Hurych, Michal U\\v{r}i\\v{c}\\'a\\v{r},\n  Patrick P\\'erez, and Josef \\v{S}ivic", "title": "Artificial Dummies for Urban Dataset Augmentation", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing datasets for training pedestrian detectors in images suffer from\nlimited appearance and pose variation. The most challenging scenarios are\nrarely included because they are too difficult to capture due to safety\nreasons, or they are very unlikely to happen. The strict safety requirements in\nassisted and autonomous driving applications call for an extra high detection\naccuracy also in these rare situations. Having the ability to generate people\nimages in arbitrary poses, with arbitrary appearances and embedded in different\nbackground scenes with varying illumination and weather conditions, is a\ncrucial component for the development and testing of such applications. The\ncontributions of this paper are three-fold. First, we describe an augmentation\nmethod for controlled synthesis of urban scenes containing people, thus\nproducing rare or never-seen situations. This is achieved with a data generator\n(called DummyNet) with disentangled control of the pose, the appearance, and\nthe target background scene. Second, the proposed generator relies on novel\nnetwork architecture and associated loss that takes into account the\nsegmentation of the foreground person and its composition into the background\nscene. Finally, we demonstrate that the data generated by our DummyNet improve\nperformance of several existing person detectors across various datasets as\nwell as in challenging situations, such as night-time conditions, where only a\nlimited amount of training data is available. In the setup with only day-time\ndata available, we improve the night-time detector by $17\\%$ log-average miss\nrate over the detector trained with the day-time data only.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 13:17:25 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Vobeck\u00fd", "Anton\u00edn", ""], ["Hurych", "David", ""], ["U\u0159i\u010d\u00e1\u0159", "Michal", ""], ["P\u00e9rez", "Patrick", ""], ["\u0160ivic", "Josef", ""]]}, {"id": "2012.08278", "submitter": "Rui Gong", "authors": "Rui Gong, Yuhua Chen, Danda Pani Paudel, Yawei Li, Ajad Chhatkuli, Wen\n  Li, Dengxin Dai, Luc Van Gool", "title": "Cluster, Split, Fuse, and Update: Meta-Learning for Open Compound Domain\n  Adaptive Semantic Segmentation", "comments": "18 pages, 8 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open compound domain adaptation (OCDA) is a domain adaptation setting, where\ntarget domain is modeled as a compound of multiple unknown homogeneous domains,\nwhich brings the advantage of improved generalization to unseen domains. In\nthis work, we propose a principled meta-learning based approach to OCDA for\nsemantic segmentation, MOCDA, by modeling the unlabeled target domain\ncontinuously. Our approach consists of four key steps. First, we cluster target\ndomain into multiple sub-target domains by image styles, extracted in an\nunsupervised manner. Then, different sub-target domains are split into\nindependent branches, for which batch normalization parameters are learnt to\ntreat them independently. A meta-learner is thereafter deployed to learn to\nfuse sub-target domain-specific predictions, conditioned upon the style code.\nMeanwhile, we learn to online update the model by model-agnostic meta-learning\n(MAML) algorithm, thus to further improve generalization. We validate the\nbenefits of our approach by extensive experiments on synthetic-to-real\nknowledge transfer benchmark datasets, where we achieve the state-of-the-art\nperformance in both compound and open domains.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 13:21:54 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Gong", "Rui", ""], ["Chen", "Yuhua", ""], ["Paudel", "Danda Pani", ""], ["Li", "Yawei", ""], ["Chhatkuli", "Ajad", ""], ["Li", "Wen", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "2012.08282", "submitter": "Goldie Nejat Professor", "authors": "Daniel Dworakowski, and Goldie Nejat", "title": "Robots Understanding Contextual Information in Human-Centered\n  Environments using Weakly Supervised Mask Data Distillation", "comments": "14 pages and 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual information in human environments, such as signs, symbols, and\nobjects provide important information for robots to use for exploration and\nnavigation. To identify and segment contextual information from complex images\nobtained in these environments, data-driven methods such as Convolutional\nNeural Networks (CNNs) are used. However, these methods require large amounts\nof human labeled data which are slow and time-consuming to obtain. Weakly\nsupervised methods address this limitation by generating pseudo segmentation\nlabels (PSLs). In this paper, we present the novel Weakly Supervised Mask Data\nDistillation (WeSuperMaDD) architecture for autonomously generating PSLs using\nCNNs not specifically trained for the task of context segmentation; i.e., CNNs\ntrained for object classification, image captioning, etc. WeSuperMaDD uniquely\ngenerates PSLs using learned image features from sparse and limited diversity\ndata; common in robot navigation tasks in human-centred environments (malls,\ngrocery stores). Our proposed architecture uses a new mask refinement system\nwhich automatically searches for the PSL with the fewest foreground pixels that\nsatisfies cost constraints. This removes the need for handcrafted heuristic\nrules. Extensive experiments successfully validated the performance of\nWeSuperMaDD in generating PSLs for datasets with text of various scales, fonts,\nand perspectives in multiple indoor/outdoor environments. A comparison with\nNaive, GrabCut, and Pyramid methods found a significant improvement in label\nand segmentation quality. Moreover, a context segmentation CNN trained using\nthe WeSuperMaDD architecture achieved measurable improvements in accuracy\ncompared to one trained with Naive PSLs. Our method also had comparable\nperformance to existing state-of-the-art text detection and segmentation\nmethods on real datasets without requiring segmentation labels for training.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 13:24:31 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Dworakowski", "Daniel", ""], ["Nejat", "Goldie", ""]]}, {"id": "2012.08290", "submitter": "Ron Zhu", "authors": "Ron Zhu", "title": "Enhance Multimodal Transformer With External Label And In-Domain\n  Pretrain: Hateful Meme Challenge Winning Solution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hateful meme detection is a new research area recently brought out that\nrequires both visual, linguistic understanding of the meme and some background\nknowledge to performing well on the task. This technical report summarises the\nfirst place solution of the Hateful Meme Detection Challenge 2020, which\nextending state-of-the-art visual-linguistic transformers to tackle this\nproblem. At the end of the report, we also point out the shortcomings and\npossible directions for improving the current methodology.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 13:57:21 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Zhu", "Ron", ""]]}, {"id": "2012.08317", "submitter": "Huahong Zhang", "authors": "Huahong Zhang and Ipek Oguz", "title": "Multiple Sclerosis Lesion Segmentation -- A Survey of Supervised\n  CNN-Based Methods", "comments": "Accepted by BrainLes 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lesion segmentation is a core task for quantitative analysis of MRI scans of\nMultiple Sclerosis patients. The recent success of deep learning techniques in\na variety of medical image analysis applications has renewed community interest\nin this challenging problem and led to a burst of activity for new algorithm\ndevelopment. In this survey, we investigate the supervised CNN-based methods\nfor MS lesion segmentation. We decouple these reviewed works into their\nalgorithmic components and discuss each separately. For methods that provide\nevaluations on public benchmark datasets, we report comparisons between their\nresults.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 19:05:41 GMT"}, {"version": "v2", "created": "Sat, 26 Dec 2020 23:54:04 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhang", "Huahong", ""], ["Oguz", "Ipek", ""]]}, {"id": "2012.08323", "submitter": "Tianyi Wei", "authors": "Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Hanqing Zhao,\n  Weiming Zhang, Nenghai Yu", "title": "Improved Image Matting via Real-time User Clicks and Uncertainty\n  Estimation", "comments": "Accepted by IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image matting is a fundamental and challenging problem in computer vision and\ngraphics. Most existing matting methods leverage a user-supplied trimap as an\nauxiliary input to produce good alpha matte. However, obtaining high-quality\ntrimap itself is arduous, thus restricting the application of these methods.\nRecently, some trimap-free methods have emerged, however, the matting quality\nis still far behind the trimap-based methods. The main reason is that, without\nthe trimap guidance in some cases, the target network is ambiguous about which\nis the foreground target. In fact, choosing the foreground is a subjective\nprocedure and depends on the user's intention. To this end, this paper proposes\nan improved deep image matting framework which is trimap-free and only needs\nseveral user click interactions to eliminate the ambiguity. Moreover, we\nintroduce a new uncertainty estimation module that can predict which parts need\npolishing and a following local refinement module. Based on the computation\nbudget, users can choose how many local parts to improve with the uncertainty\nguidance. Quantitative and qualitative results show that our method performs\nbetter than existing trimap-free methods and comparably to state-of-the-art\ntrimap-based methods with minimal user effort.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 14:32:36 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 07:14:12 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Wei", "Tianyi", ""], ["Chen", "Dongdong", ""], ["Zhou", "Wenbo", ""], ["Liao", "Jing", ""], ["Zhao", "Hanqing", ""], ["Zhang", "Weiming", ""], ["Yu", "Nenghai", ""]]}, {"id": "2012.08333", "submitter": "Weronika Hryniewska", "authors": "Weronika Hryniewska, Przemys{\\l}aw Bombi\\'nski, Patryk Szatkowski,\n  Paulina Tomaszewska, Artur Przelaskowski, Przemys{\\l}aw Biecek", "title": "Checklist for responsible deep learning modeling of medical images based\n  on COVID-19 detection studies", "comments": null, "journal-ref": "Pattern Recognition 118 (2021) 108035", "doi": "10.1016/j.patcog.2021.108035", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sudden outbreak and uncontrolled spread of COVID-19 disease is one of the\nmost important global problems today. In a short period of time, it has led to\nthe development of many deep neural network models for COVID-19 detection with\nmodules for explainability. In this work, we carry out a systematic analysis of\nvarious aspects of proposed models. Our analysis revealed numerous mistakes\nmade at different stages of data acquisition, model development, and\nexplanation construction. In this work, we overview the approaches proposed in\nthe surveyed Machine Learning articles and indicate typical errors emerging\nfrom the lack of deep understanding of the radiography domain. We present the\nperspective of both: experts in the field - radiologists and deep learning\nengineers dealing with model explanations. The final result is a proposed\nchecklist with the minimum conditions to be met by a reliable COVID-19\ndiagnostic model.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 18:42:46 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 10:20:16 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 23:00:13 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Hryniewska", "Weronika", ""], ["Bombi\u0144ski", "Przemys\u0142aw", ""], ["Szatkowski", "Patryk", ""], ["Tomaszewska", "Paulina", ""], ["Przelaskowski", "Artur", ""], ["Biecek", "Przemys\u0142aw", ""]]}, {"id": "2012.08334", "submitter": "Nikita Durasov", "authors": "Nikita Durasov, Timur Bagautdinov, Pierre Baque, Pascal Fua", "title": "Masksembles for Uncertainty Estimation", "comments": null, "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2021, pp. 13539-13548", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks have amply demonstrated their prowess but estimating the\nreliability of their predictions remains challenging. Deep Ensembles are widely\nconsidered as being one of the best methods for generating uncertainty\nestimates but are very expensive to train and evaluate. MC-Dropout is another\npopular alternative, which is less expensive, but also less reliable. Our\ncentral intuition is that there is a continuous spectrum of ensemble-like\nmodels of which MC-Dropout and Deep Ensembles are extreme examples. The first\nuses an effectively infinite number of highly correlated models while the\nsecond relies on a finite number of independent models.\n  To combine the benefits of both, we introduce Masksembles. Instead of\nrandomly dropping parts of the network as in MC-dropout, Masksemble relies on a\nfixed number of binary masks, which are parameterized in a way that allows to\nchange correlations between individual models. Namely, by controlling the\noverlap between the masks and their density one can choose the optimal\nconfiguration for the task at hand. This leads to a simple and easy to\nimplement method with performance on par with Ensembles at a fraction of the\ncost. We experimentally validate Masksembles on two widely used datasets,\nCIFAR10 and ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 14:39:57 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 17:45:05 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Durasov", "Nikita", ""], ["Bagautdinov", "Timur", ""], ["Baque", "Pierre", ""], ["Fua", "Pascal", ""]]}, {"id": "2012.08375", "submitter": "Elahe Arani", "authors": "Hemang Chawla, Matti Jukola, Shabbir Marzban, Elahe Arani and Bahram\n  Zonooz", "title": "Practical Auto-Calibration for Spatial Scene-Understanding from\n  Crowdsourced Dashcamera Videos", "comments": "Accepted at 16th International Conference on Computer Vision Theory\n  and Applications (VISAP, 2021)", "journal-ref": null, "doi": "10.5220/0010255808690880", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial scene-understanding, including dense depth and ego-motion estimation,\nis an important problem in computer vision for autonomous vehicles and advanced\ndriver assistance systems. Thus, it is beneficial to design perception modules\nthat can utilize crowdsourced videos collected from arbitrary vehicular onboard\nor dashboard cameras. However, the intrinsic parameters corresponding to such\ncameras are often unknown or change over time. Typical manual calibration\napproaches require objects such as a chessboard or additional scene-specific\ninformation. On the other hand, automatic camera calibration does not have such\nrequirements. Yet, the automatic calibration of dashboard cameras is\nchallenging as forward and planar navigation results in critical motion\nsequences with reconstruction ambiguities. Structure reconstruction of complete\nvisual-sequences that may contain tens of thousands of images is also\ncomputationally untenable. Here, we propose a system for practical monocular\nonboard camera auto-calibration from crowdsourced videos. We show the\neffectiveness of our proposed system on the KITTI raw, Oxford RobotCar, and the\ncrowdsourced D$^2$-City datasets in varying conditions. Finally, we demonstrate\nits application for accurate monocular dense depth and ego-motion estimation on\nuncalibrated videos.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 15:38:17 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Chawla", "Hemang", ""], ["Jukola", "Matti", ""], ["Marzban", "Shabbir", ""], ["Arani", "Elahe", ""], ["Zonooz", "Bahram", ""]]}, {"id": "2012.08385", "submitter": "Rui Gong", "authors": "Rui Gong, Dengxin Dai, Yuhua Chen, Wen Li, Luc Van Gool", "title": "mDALU: Multi-Source Domain Adaptation and Label Unification with Partial\n  Datasets", "comments": "17 pages, 10 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition advances very rapidly these days. One challenge is to\ngeneralize existing methods to new domains, to more classes and/or to new data\nmodalities. In order to avoid annotating one dataset for each of these new\ncases, one needs to combine and reuse existing datasets that may belong to\ndifferent domains, have partial annotations, and/or have different data\nmodalities. This paper treats this task as a multi-source domain adaptation and\nlabel unification (mDALU) problem and proposes a novel method for it. Our\nmethod consists of a partially-supervised adaptation stage and a\nfully-supervised adaptation stage. In the former, partial knowledge is\ntransferred from multiple source domains to the target domain and fused\ntherein. Negative transfer between unmatched label space is mitigated via three\nnew modules: domain attention, uncertainty maximization and attention-guided\nadversarial alignment. In the latter, knowledge is transferred in the unified\nlabel space after a label completion process with pseudo-labels. We verify the\nmethod on three different tasks, image classification, 2D semantic image\nsegmentation, and joint 2D-3D semantic segmentation. Extensive experiments show\nthat our method outperforms all competing methods significantly.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 15:58:03 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Gong", "Rui", ""], ["Dai", "Dengxin", ""], ["Chen", "Yuhua", ""], ["Li", "Wen", ""], ["Van Gool", "Luc", ""]]}, {"id": "2012.08392", "submitter": "Jan Kristanto Wibisono", "authors": "Jan Kristanto Wibisono and Hsueh-Ming Hang", "title": "FINED: Fast Inference Network for Edge Detection", "comments": "Submitted to ICME 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the design of lightweight deep learning-based edge\ndetection. The deep learning technology offers a significant improvement on the\nedge detection accuracy. However, typical neural network designs have very high\nmodel complexity, which prevents it from practical usage. In contrast, we\npropose a Fast Inference Network for Edge Detection (FINED), which is a\nlightweight neural net dedicated to edge detection. By carefully choosing\nproper components for edge detection purpose, we can achieve the\nstate-of-the-art accuracy in edge detection while significantly reducing its\ncomplexity. Another key contribution in increasing the inferencing speed is\nintroducing the training helper concept. The extra subnetworks (training\nhelper) are employed in training but not used in inferencing. It can further\nreduce the model complexity and yet maintain the same level of accuracy. Our\nexperiments show that our systems outperform all the current edge detectors at\nabout the same model (parameter) size.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 16:08:46 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Wibisono", "Jan Kristanto", ""], ["Hang", "Hsueh-Ming", ""]]}, {"id": "2012.08398", "submitter": "Deepak Ravikumar", "authors": "Deepak Ravikumar, Sangamesh Kodge, Isha Garg, Kaushik Roy", "title": "Exploring Vicinal Risk Minimization for Lightweight Out-of-Distribution\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have found widespread adoption in solving complex tasks\nranging from image recognition to natural language processing. However, these\nnetworks make confident mispredictions when presented with data that does not\nbelong to the training distribution, i.e. out-of-distribution (OoD) samples. In\nthis paper we explore whether the property of Vicinal Risk Minimization (VRM)\nto smoothly interpolate between different class boundaries helps to train\nbetter OoD detectors. We apply VRM to existing OoD detection techniques and\nshow their improved performance. We observe that existing OoD detectors have\nsignificant memory and compute overhead, hence we leverage VRM to develop an\nOoD detector with minimal overheard. Our detection method introduces an\nauxiliary class for classifying OoD samples. We utilize mixup in two ways to\nimplement Vicinal Risk Minimization. First, we perform mixup within the same\nclass and second, we perform mixup with Gaussian noise when training the\nauxiliary class. Our method achieves near competitive performance with\nsignificantly less compute and memory overhead when compared to existing OoD\ndetection techniques. This facilitates the deployment of OoD detection on edge\ndevices and expands our understanding of Vicinal Risk Minimization for use in\ntraining OoD detectors.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 16:13:33 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Ravikumar", "Deepak", ""], ["Kodge", "Sangamesh", ""], ["Garg", "Isha", ""], ["Roy", "Kaushik", ""]]}, {"id": "2012.08408", "submitter": "Zhuonan Liang", "authors": "Zhuonan Liang, Ziheng Liu, Huaze Shi, Yunlong Chen, Yanbin Cai, Yating\n  Liang, Yafan Feng, Yuqing Yang, Jing Zhang, Peng Fu", "title": "SPOC learner's final grade prediction based on a novel sampling batch\n  normalization embedded neural network method", "comments": "11 pages, 5 figures, ICAIS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent years have witnessed the rapid growth of Small Private Online Courses\n(SPOC) which is able to highly customized and personalized to adapt variable\neducational requests, in which machine learning techniques are explored to\nsummarize and predict the learner's performance, mostly focus on the final\ngrade. However, the problem is that the final grade of learners on SPOC is\ngenerally seriously imbalance which handicaps the training of prediction model.\nTo solve this problem, a sampling batch normalization embedded deep neural\nnetwork (SBNEDNN) method is developed in this paper. First, a combined\nindicator is defined to measure the distribution of the data, then a rule is\nestablished to guide the sampling process. Second, the batch normalization (BN)\nmodified layers are embedded into full connected neural network to solve the\ndata imbalanced problem. Experimental results with other three deep learning\nmethods demonstrates the superiority of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 16:36:42 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Liang", "Zhuonan", ""], ["Liu", "Ziheng", ""], ["Shi", "Huaze", ""], ["Chen", "Yunlong", ""], ["Cai", "Yanbin", ""], ["Liang", "Yating", ""], ["Feng", "Yafan", ""], ["Yang", "Yuqing", ""], ["Zhang", "Jing", ""], ["Fu", "Peng", ""]]}, {"id": "2012.08419", "submitter": "Tarasha Khurana", "authors": "Tarasha Khurana, Achal Dave, Deva Ramanan", "title": "Detecting Invisible People", "comments": "Project page: http://www.cs.cmu.edu/~tkhurana/invisible.htm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular object detection and tracking have improved drastically in recent\nyears, but rely on a key assumption: that objects are visible to the camera.\nMany offline tracking approaches reason about occluded objects post-hoc, by\nlinking together tracklets after the object re-appears, making use of\nreidentification (ReID). However, online tracking in embodied robotic agents\n(such as a self-driving vehicle) fundamentally requires object permanence,\nwhich is the ability to reason about occluded objects before they re-appear. In\nthis work, we re-purpose tracking benchmarks and propose new metrics for the\ntask of detecting invisible objects, focusing on the illustrative case of\npeople. We demonstrate that current detection and tracking systems perform\ndramatically worse on this task. We introduce two key innovations to recover\nmuch of this performance drop. We treat occluded object detection in temporal\nsequences as a short-term forecasting challenge, bringing to bear tools from\ndynamic sequence prediction. Second, we build dynamic models that explicitly\nreason in 3D, making use of observations produced by state-of-the-art monocular\ndepth estimation networks. To our knowledge, ours is the first work to\ndemonstrate the effectiveness of monocular depth estimation for the task of\ntracking and detecting occluded objects. Our approach strongly improves by\n11.4% over the baseline in ablations and by 5.0% over the state-of-the-art in\nF1 score.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 16:54:45 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Khurana", "Tarasha", ""], ["Dave", "Achal", ""], ["Ramanan", "Deva", ""]]}, {"id": "2012.08451", "submitter": "Tanasai Sucontphunt", "authors": "Tanasai Sucontphunt", "title": "Geometric Surface Image Prediction for Image Recognition Enhancement", "comments": null, "journal-ref": "SmartCom 2020 (LNCS)", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a method to predict a geometric surface image from a\nphotograph to assist in image recognition. To recognize objects, several images\nfrom different conditions are required for training a model or fine-tuning a\npre-trained model. In this work, a geometric surface image is introduced as a\nbetter representation than its color image counterpart to overcome lighting\nconditions. The surface image is predicted from a color image. To do so, the\ngeometric surface image together with its color photographs are firstly trained\nwith Generative Adversarial Networks (GAN) model. The trained generator model\nis then used to predict the geometric surface image from the input color image.\nThe evaluation on a case study of an amulet recognition shows that the\npredicted geometric surface images contain less ambiguity than their color\nimages counterpart under different lighting conditions and can be used\neffectively for assisting in image recognition task.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 17:44:37 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Sucontphunt", "Tanasai", ""]]}, {"id": "2012.08501", "submitter": "Qingfu Wan", "authors": "Qingfu Wan, Oliver Lu", "title": "NAPA: Neural Art Human Pose Amplifier", "comments": "Tech Report; Graduate Course Project Report; Code, datasets and video\n  released", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This is the project report for CSCI-GA.2271-001. We target human pose\nestimation in artistic images. For this goal, we design an end-to-end system\nthat uses neural style transfer for pose regression. We collect a 277-style set\nfor arbitrary style transfer and build an artistic 281-image test set. We\ndirectly run pose regression on the test set and show promising results. For\npose regression, we propose a 2d-induced bone map from which pose is lifted. To\nhelp such a lifting, we additionally annotate the pseudo 3d labels of the full\nin-the-wild MPII dataset. Further, we append another style transfer as self\nsupervision to improve 2d. We perform extensive ablation studies to analyze the\nintroduced features. We also compare end-to-end with per-style training and\nallude to the tradeoff between style transfer and pose regression. Lastly, we\ngeneralize our model to the real-world human dataset and show its potentiality\nas a generic pose model. We explain the theoretical foundation in Appendix. We\nrelease code at https://github.com/strawberryfg/NAPA-NST-HPE, data, and video.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 18:51:19 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Wan", "Qingfu", ""], ["Lu", "Oliver", ""]]}, {"id": "2012.08503", "submitter": "Michelle Guo", "authors": "Michelle Guo, Alireza Fathi, Jiajun Wu, Thomas Funkhouser", "title": "Object-Centric Neural Scene Rendering", "comments": "Summary Video: https://youtu.be/NtR7xgxSL1U Project Webpage:\n  https://shellguo.com/osf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for composing photorealistic scenes from captured images\nof objects. Our work builds upon neural radiance fields (NeRFs), which\nimplicitly model the volumetric density and directionally-emitted radiance of a\nscene. While NeRFs synthesize realistic pictures, they only model static scenes\nand are closely tied to specific imaging conditions. This property makes NeRFs\nhard to generalize to new scenarios, including new lighting or new arrangements\nof objects. Instead of learning a scene radiance field as a NeRF does, we\npropose to learn object-centric neural scattering functions (OSFs), a\nrepresentation that models per-object light transport implicitly using a\nlighting- and view-dependent neural network. This enables rendering scenes even\nwhen objects or lights move, without retraining. Combined with a volumetric\npath tracing procedure, our framework is capable of rendering both intra- and\ninter-object light transport effects including occlusions, specularities,\nshadows, and indirect illumination. We evaluate our approach on scene\ncomposition and show that it generalizes to novel illumination conditions,\nproducing photorealistic, physically accurate renderings of multi-object\nscenes.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 18:55:02 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Guo", "Michelle", ""], ["Fathi", "Alireza", ""], ["Wu", "Jiajun", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "2012.08508", "submitter": "David Ding", "authors": "David Ding, Felix Hill, Adam Santoro, Malcolm Reynolds, Matt Botvinick", "title": "Attention over learned object embeddings enables complex visual\n  reasoning", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have achieved success in a wide array of perceptual tasks but\noften fail at tasks involving both perception and higher-level reasoning. On\nthese more challenging tasks, bespoke approaches (such as modular symbolic\ncomponents, independent dynamics models or semantic parsers) targeted towards\nthat specific type of task have typically performed better. The downside to\nthese targeted approaches, however, is that they can be more brittle than\ngeneral-purpose neural networks, requiring significant modification or even\nredesign according to the particular task at hand. Here, we propose a more\ngeneral neural-network-based approach to dynamic visual reasoning problems that\nobtains state-of-the-art performance on three different domains, in each case\noutperforming bespoke modular approaches tailored specifically to the task. Our\nmethod relies on learned object-centric representations, self-attention and\nself-supervised dynamics learning, and all three elements together are required\nfor strong performance to emerge. The success of this combination suggests that\nthere may be no need to trade off flexibility for performance on problems\ninvolving spatio-temporal or causal-style reasoning. With the right soft biases\nand learning objectives in a neural network we may be able to attain the best\nof both worlds.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 18:57:40 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 17:58:42 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Ding", "David", ""], ["Hill", "Felix", ""], ["Santoro", "Adam", ""], ["Reynolds", "Malcolm", ""], ["Botvinick", "Matt", ""]]}, {"id": "2012.08510", "submitter": "Bo He", "authors": "Bo He, Xitong Yang, Zuxuan Wu, Hao Chen, Ser-Nam Lim, Abhinav\n  Shrivastava", "title": "GTA: Global Temporal Attention for Video Action Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-attention learns pairwise interactions to model long-range dependencies,\nyielding great improvements for video action recognition. In this paper, we\nseek a deeper understanding of self-attention for temporal modeling in videos.\nWe first demonstrate that the entangled modeling of spatio-temporal information\nby flattening all pixels is sub-optimal, failing to capture temporal\nrelationships among frames explicitly. To this end, we introduce Global\nTemporal Attention (GTA), which performs global temporal attention on top of\nspatial attention in a decoupled manner. We apply GTA on both pixels and\nsemantically similar regions to capture temporal relationships at different\nlevels of spatial granularity. Unlike conventional self-attention that computes\nan instance-specific attention matrix, GTA directly learns a global attention\nmatrix that is intended to encode temporal structures that generalize across\ndifferent samples. We further augment GTA with a cross-channel multi-head\nfashion to exploit channel interactions for better temporal modeling. Extensive\nexperiments on 2D and 3D networks demonstrate that our approach consistently\nenhances temporal modeling and provides state-of-the-art performance on three\nvideo action recognition datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 18:58:21 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 18:16:52 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["He", "Bo", ""], ["Yang", "Xitong", ""], ["Wu", "Zuxuan", ""], ["Chen", "Hao", ""], ["Lim", "Ser-Nam", ""], ["Shrivastava", "Abhinav", ""]]}, {"id": "2012.08512", "submitter": "Tarun Kalluri", "authors": "Tarun Kalluri, Deepak Pathak, Manmohan Chandraker, Du Tran", "title": "FLAVR: Flow-Agnostic Video Representations for Fast Frame Interpolation", "comments": "Project webpage: https://tarun005.github.io/FLAVR/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A majority of methods for video frame interpolation compute bidirectional\noptical flow between adjacent frames of a video, followed by a suitable warping\nalgorithm to generate the output frames. However, approaches relying on optical\nflow often fail to model occlusions and complex non-linear motions directly\nfrom the video and introduce additional bottlenecks unsuitable for widespread\ndeployment. We address these limitations with FLAVR, a flexible and efficient\narchitecture that uses 3D space-time convolutions to enable end-to-end learning\nand inference for video frame interpolation. Our method efficiently learns to\nreason about non-linear motions, complex occlusions and temporal abstractions,\nresulting in improved performance on video interpolation, while requiring no\nadditional inputs in the form of optical flow or depth maps. Due to its\nsimplicity, FLAVR can deliver 3x faster inference speed compared to the current\nmost accurate method on multi-frame interpolation without losing interpolation\naccuracy. In addition, we evaluate FLAVR on a wide range of challenging\nsettings and consistently demonstrate superior qualitative and quantitative\nresults compared with prior methods on various popular benchmarks including\nVimeo-90K, UCF101, DAVIS, Adobe, and GoPro. Finally, we demonstrate that FLAVR\nfor video frame interpolation can serve as a useful self-supervised pretext\ntask for action recognition, optical flow estimation, and motion magnification.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 18:59:30 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 18:53:49 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Kalluri", "Tarun", ""], ["Pathak", "Deepak", ""], ["Chandraker", "Manmohan", ""], ["Tran", "Du", ""]]}, {"id": "2012.08514", "submitter": "Xinhan Di", "authors": "Xinhan Di, Pengqian Yu, Danfeng Yang, Hong Zhu, Changyu Sun, YinDong\n  Liu", "title": "End-to-end Generative Floor-plan and Layout with Attributes and Relation\n  Graph", "comments": "Submitted to CV Conference. arXiv admin note: text overlap with\n  arXiv:2006.13527. text overlap with arXiv:2012.08131", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an end-end model for producing furniture layout for\ninterior scene synthesis from the random vector. This proposed model is aimed\nto support professional interior designers to produce the interior decoration\nsolutions more quickly. The proposed model combines a conditional floor-plan\nmodule of the room, a conditional graphical floor-plan module of the room and a\nconditional layout module. As compared with the prior work on scene synthesis,\nour proposed three modules enhance the ability of auto-layout generation given\nthe dimensional category of the room. We conduct our experiments on the\nproposed real-world interior layout dataset that contains $191208$ designs from\nthe professional designers. Our numerical results demonstrate that the proposed\nmodel yields higher-quality layouts in comparison with the state-of-the-art\nmodel. The dataset and code are released\n\\href{https://github.com/CODE-SUBMIT/dataset3}{Dataset,Code}\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 07:37:05 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Di", "Xinhan", ""], ["Yu", "Pengqian", ""], ["Yang", "Danfeng", ""], ["Zhu", "Hong", ""], ["Sun", "Changyu", ""], ["Liu", "YinDong", ""]]}, {"id": "2012.08526", "submitter": "Fr\\'ed\\'eric Dreyer", "authors": "Fr\\'ed\\'eric A. Dreyer and Huilin Qu", "title": "Jet tagging in the Lund plane with graph networks", "comments": "23 pages, 12 figures, code available at\n  https://github.com/fdreyer/lundnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ph cs.CV cs.LG hep-ex", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The identification of boosted heavy particles such as top quarks or vector\nbosons is one of the key problems arising in experimental studies at the Large\nHadron Collider. In this article, we introduce LundNet, a novel jet tagging\nmethod which relies on graph neural networks and an efficient description of\nthe radiation patterns within a jet to optimally disentangle signatures of\nboosted objects from background events. We apply this framework to a number of\ndifferent benchmarks, showing significantly improved performance for top\ntagging compared to existing state-of-the-art algorithms. We study the\nrobustness of the LundNet taggers to non-perturbative and detector effects, and\nshow how kinematic cuts in the Lund plane can mitigate overfitting of the\nneural network to model-dependent contributions. Finally, we consider the\ncomputational complexity of this method and its scaling as a function of\nkinematic Lund plane cuts, showing an order of magnitude improvement in speed\nover previous graph-based taggers.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 19:00:01 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 12:08:25 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Dreyer", "Fr\u00e9d\u00e9ric A.", ""], ["Qu", "Huilin", ""]]}, {"id": "2012.08548", "submitter": "Jingru Tan", "authors": "Jingru Tan, Xin Lu, Gang Zhang, Changqing Yin, Quanquan Li", "title": "Equalization Loss v2: A New Gradient Balance Approach for Long-tailed\n  Object Detection", "comments": "CVPR2021, codes: https://github.com/tztztztztz/eqlv2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently proposed decoupled training methods emerge as a dominant paradigm\nfor long-tailed object detection. But they require an extra fine-tuning stage,\nand the disjointed optimization of representation and classifier might lead to\nsuboptimal results. However, end-to-end training methods, like equalization\nloss (EQL), still perform worse than decoupled training methods. In this paper,\nwe reveal the main issue in long-tailed object detection is the imbalanced\ngradients between positives and negatives, and find that EQL does not solve it\nwell. To address the problem of imbalanced gradients, we introduce a new\nversion of equalization loss, called equalization loss v2 (EQL v2), a novel\ngradient guided reweighing mechanism that re-balances the training process for\neach category independently and equally. Extensive experiments are performed on\nthe challenging LVIS benchmark. EQL v2 outperforms origin EQL by about 4 points\noverall AP with 14-18 points improvements on the rare categories. More\nimportantly, it also surpasses decoupled training methods. Without further\ntuning for the Open Images dataset, EQL v2 improves EQL by 7.3 points AP,\nshowing strong generalization ability. Codes have been released at\nhttps://github.com/tztztztztz/eqlv2\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 19:01:48 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 02:35:22 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Tan", "Jingru", ""], ["Lu", "Xin", ""], ["Zhang", "Gang", ""], ["Yin", "Changqing", ""], ["Li", "Quanquan", ""]]}, {"id": "2012.08573", "submitter": "Yimian Dai", "authors": "Yimian Dai and Yiquan Wu and Fei Zhou and Kobus Barnard", "title": "Attentional Local Contrast Networks for Infrared Small Target Detection", "comments": "Accepted by IEEE TGRS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To mitigate the issue of minimal intrinsic features for pure data-driven\nmethods, in this paper, we propose a novel model-driven deep network for\ninfrared small target detection, which combines discriminative networks and\nconventional model-driven methods to make use of both labeled data and the\ndomain knowledge. By designing a feature map cyclic shift scheme, we modularize\na conventional local contrast measure method as a depth-wise parameterless\nnonlinear feature refinement layer in an end-to-end network, which encodes\nrelatively long-range contextual interactions with clear physical\ninterpretability. To highlight and preserve the small target features, we also\nexploit a bottom-up attentional modulation integrating the smaller scale subtle\ndetails of low-level features into high-level features of deeper layers. We\nconduct detailed ablation studies with varying network depths to empirically\nverify the effectiveness and efficiency of the design of each component in our\nnetwork architecture. We also compare the performance of our network against\nother model-driven methods and deep networks on the open SIRST dataset as well.\nThe results suggest that our network yields a performance boost over its\ncompetitors. Our code, trained models, and results are available online.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 19:33:09 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Dai", "Yimian", ""], ["Wu", "Yiquan", ""], ["Zhou", "Fei", ""], ["Barnard", "Kobus", ""]]}, {"id": "2012.08588", "submitter": "Ivan Evtimov", "authors": "Ivan Evtimov, Pascal Sturmfels, Tadayoshi Kohno", "title": "FoggySight: A Scheme for Facial Lookup Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in deep learning algorithms have enabled better-than-human\nperformance on face recognition tasks. In parallel, private companies have been\nscraping social media and other public websites that tie photos to identities\nand have built up large databases of labeled face images. Searches in these\ndatabases are now being offered as a service to law enforcement and others and\ncarry a multitude of privacy risks for social media users. In this work, we\ntackle the problem of providing privacy from such face recognition systems. We\npropose and evaluate FoggySight, a solution that applies lessons learned from\nthe adversarial examples literature to modify facial photos in a\nprivacy-preserving manner before they are uploaded to social media.\nFoggySight's core feature is a community protection strategy where users acting\nas protectors of privacy for others upload decoy photos generated by\nadversarial machine learning algorithms. We explore different settings for this\nscheme and find that it does enable protection of facial privacy -- including\nagainst a facial recognition service with unknown internals.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 19:57:18 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Evtimov", "Ivan", ""], ["Sturmfels", "Pascal", ""], ["Kohno", "Tadayoshi", ""]]}, {"id": "2012.08606", "submitter": "Indrajit Kurmi", "authors": "Indrajit Kurmi, David C. Schedl, and Oliver Bimber", "title": "Pose Error Reduction for Focus Enhancement in Thermal Synthetic Aperture\n  Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Airborne optical sectioning, an effective aerial synthetic aperture imaging\ntechnique for revealing artifacts occluded by forests, requires precise\nmeasurements of drone poses. In this article we present a new approach for\nreducing pose estimation errors beyond the possibilities of conventional\nPerspective-n-Point solutions by considering the underlying optimization as a\nfocusing problem. We present an efficient image integration technique, which\nalso reduces the parameter search space to achieve realistic processing times,\nand improves the quality of resulting synthetic integral images.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 20:43:46 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Kurmi", "Indrajit", ""], ["Schedl", "David C.", ""], ["Bimber", "Oliver", ""]]}, {"id": "2012.08624", "submitter": "Quoc Dung Cao", "authors": "Quoc Dung Cao and Youngjun Choe", "title": "Post-Hurricane Damage Assessment Using Satellite Imagery and Geolocation\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaining timely and reliable situation awareness after hazard events such as a\nhurricane is crucial to emergency managers and first responders. One effective\nway to achieve that goal is through damage assessment. Recently, disaster\nresearchers have been utilizing imagery captured through satellites or drones\nto quantify the number of flooded/damaged buildings. In this paper, we propose\na mixed data approach, which leverages publicly available satellite imagery and\ngeolocation features of the affected area to identify damaged buildings after a\nhurricane. The method demonstrated significant improvement from performing a\nsimilar task using only imagery features, based on a case study of Hurricane\nHarvey affecting Greater Houston area in 2017. This result opens door to a wide\nrange of possibilities to unify the advancement in computer vision algorithms\nsuch as convolutional neural networks and traditional methods in damage\nassessment, for example, using flood depth or bare-earth topology. In this\nwork, a creative choice of the geolocation features was made to provide extra\ninformation to the imagery features, but it is up to the users to decide which\nother features can be included to model the physical behavior of the events,\ndepending on their domain knowledge and the type of disaster. The dataset\ncurated in this work is made openly available (DOI: 10.17603/ds2-3cca-f398).\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 21:30:19 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Cao", "Quoc Dung", ""], ["Choe", "Youngjun", ""]]}, {"id": "2012.08639", "submitter": "Gustau Camps-Valls", "authors": "Fatih Nar, Erdal Yilmaz, Gustau Camps-Valls", "title": "Sparsity-driven Digital Terrain Model Extraction", "comments": "Preprint. Paper published in IGARSS 2018 - 2018 IEEE International\n  Geoscience and Remote Sensing Symposium, Valencia, 2018, pp. 1316-1319", "journal-ref": null, "doi": "10.1109/IGARSS.2018.8517569", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We here introduce an automatic Digital Terrain Model (DTM) extraction method.\nThe proposed sparsity-driven DTM extractor (SD-DTM) takes a high-resolution\nDigital Surface Model (DSM) as an input and constructs a high-resolution DTM\nusing the variational framework. To obtain an accurate DTM, an iterative\napproach is proposed for the minimization of the target variational cost\nfunction. Accuracy of the SD-DTM is shown in a real-world DSM data set. We show\nthe efficiency and effectiveness of the approach both visually and\nquantitatively via residual plots in illustrative terrain types.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 12:29:01 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Nar", "Fatih", ""], ["Yilmaz", "Erdal", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.08640", "submitter": "Gustau Camps-Valls", "authors": "Jochem Verrelst, Juan Pablo Rivera, Anatoly Gitelson, Jesus Delegido,\n  Jos\\'e Moreno, Gustau Camps-Valls", "title": "Spectral band selection for vegetation properties retrieval using\n  Gaussian processes regression", "comments": null, "journal-ref": "International Journal of Applied Earth Observation and\n  Geoinformation Volume 52, October 2016, Pages 554-567", "doi": "10.1016/j.jag.2016.07.016", "report-no": null, "categories": "cs.CV eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With current and upcoming imaging spectrometers, automated band analysis\ntechniques are needed to enable efficient identification of most informative\nbands to facilitate optimized processing of spectral data into estimates of\nbiophysical variables. This paper introduces an automated spectral band\nanalysis tool (BAT) based on Gaussian processes regression (GPR) for the\nspectral analysis of vegetation properties. The GPR-BAT procedure sequentially\nbackwards removes the least contributing band in the regression model for a\ngiven variable until only one band is kept. GPR-BAT is implemented within the\nframework of the free ARTMO's MLRA (machine learning regression algorithms)\ntoolbox, which is dedicated to the transforming of optical remote sensing\nimages into biophysical products. GPR-BAT allows (1) to identify the most\ninformative bands in relating spectral data to a biophysical variable, and (2)\nto find the least number of bands that preserve optimized accurate predictions.\nThis study concludes that a wise band selection of hyperspectral data is\nstrictly required for optimal vegetation properties mapping.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 09:28:33 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Verrelst", "Jochem", ""], ["Rivera", "Juan Pablo", ""], ["Gitelson", "Anatoly", ""], ["Delegido", "Jesus", ""], ["Moreno", "Jos\u00e9", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.08641", "submitter": "Changyan Xiao", "authors": "Dieuthuy Pham, Minhtuan Ha and Changyan Xiao", "title": "A grid-point detection method based on U-net for a structured light\n  system", "comments": "http://airccse.org/csit/V10N16.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate detection of the feature points of the projected pattern plays an\nextremely important role in one-shot 3D reconstruction systems, especially for\nthe ones using a grid pattern. To solve this problem, this paper proposes a\ngrid-point detection method based on U-net. A specific dataset is designed that\nincludes the images captured with the two-shot imaging method and the ones\nacquired with the one-shot imaging method. Among them, the images in the first\ngroup after labeled as the ground truth images and the images captured at the\nsame pose with the one-shot method are cut into small patches with the size of\n64x64 pixels then feed to the training set. The remaining of the images in the\nsecond group is the test set. The experimental results show that our method can\nachieve a better detecting performance with higher accuracy in comparison with\nthe previous methods.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 09:33:43 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Pham", "Dieuthuy", ""], ["Ha", "Minhtuan", ""], ["Xiao", "Changyan", ""]]}, {"id": "2012.08642", "submitter": "Dhasarathy Parthasarathy", "authors": "Dhasarathy Parthasarathy, Anton Johansson", "title": "Does the dataset meet your expectations? Explaining sample\n  representation in image data", "comments": "Preprint of paper accepted at BNAIC/BeneLearn 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the behavior of a neural network model is adversely affected by a lack\nof diversity in training data, we present a method that identifies and explains\nsuch deficiencies. When a dataset is labeled, we note that annotations alone\nare capable of providing a human interpretable summary of sample diversity.\nThis allows explaining any lack of diversity as the mismatch found when\ncomparing the \\textit{actual} distribution of annotations in the dataset with\nan \\textit{expected} distribution of annotations, specified manually to capture\nessential label diversity. While, in many practical cases, labeling (samples\n$\\rightarrow$ annotations) is expensive, its inverse, simulation (annotations\n$\\rightarrow$ samples) can be cheaper. By mapping the expected distribution of\nannotations into test samples using parametric simulation, we present a method\nthat explains sample representation using the mismatch in diversity between\nsimulated and collected data. We then apply the method to examine a dataset of\ngeometric shapes to qualitatively and quantitatively explain sample\nrepresentation in terms of comprehensible aspects such as size, position, and\npixel brightness.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 18:16:28 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Parthasarathy", "Dhasarathy", ""], ["Johansson", "Anton", ""]]}, {"id": "2012.08643", "submitter": "Kasthuri Jayarajah", "authors": "Kasthuri Jayarajah, Dhanuja Wanniarachchige, Archan Misra", "title": "Enabling Collaborative Video Sensing at the Edge through Convolutional\n  Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Deep Neural Network (DNN) models have provided remarkable advances in\nmachine vision capabilities, their high computational complexity and model\nsizes present a formidable roadblock to deployment in AIoT-based sensing\napplications. In this paper, we propose a novel paradigm by which peer nodes in\na network can collaborate to improve their accuracy on person detection, an\nexemplar machine vision task. The proposed methodology requires no re-training\nof the DNNs and incurs minimal processing latency as it extracts scene\nsummaries from the collaborators and injects back into DNNs of the reference\ncameras, on-the-fly. Early results show promise with improvements in recall as\nhigh as 10% with a single collaborator, on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 06:29:09 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Jayarajah", "Kasthuri", ""], ["Wanniarachchige", "Dhanuja", ""], ["Misra", "Archan", ""]]}, {"id": "2012.08645", "submitter": "Tommaso Di Noto", "authors": "Tommaso Di Noto, Guillaume Marie, S\\'ebastien Tourbier, Yasser\n  Alem\\'an-G\\'omez, Guillaume Saliou, Meritxell Bach Cuadra, Patric Hagmann,\n  Jonas Richiardi", "title": "An anatomically-informed 3D CNN for brain aneurysm classification with\n  weak labels", "comments": "11 pages, 2 figures, Paper accepted at the \"Machine Learning in\n  Clinical Neuroimaging\" (MLCN) MICCAI workshop 2020 (https://mlcnws.com/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A commonly adopted approach to carry out detection tasks in medical imaging\nis to rely on an initial segmentation. However, this approach strongly depends\non voxel-wise annotations which are repetitive and time-consuming to draw for\nmedical experts. An interesting alternative to voxel-wise masks are so-called\n\"weak\" labels: these can either be coarse or oversized annotations that are\nless precise, but noticeably faster to create. In this work, we address the\ntask of brain aneurysm detection as a patch-wise binary classification with\nweak labels, in contrast to related studies that rather use supervised\nsegmentation methods and voxel-wise delineations. Our approach comes with the\nnon-trivial challenge of the data set creation: as for most focal diseases,\nanomalous patches (with aneurysm) are outnumbered by those showing no anomaly,\nand the two classes usually have different spatial distributions. To tackle\nthis frequent scenario of inherently imbalanced, spatially skewed data sets, we\npropose a novel, anatomically-driven approach by using a multi-scale and\nmulti-input 3D Convolutional Neural Network (CNN). We apply our model to 214\nsubjects (83 patients, 131 controls) who underwent Time-Of-Flight Magnetic\nResonance Angiography (TOF-MRA) and presented a total of 111 unruptured\ncerebral aneurysms. We compare two strategies for negative patch sampling that\nhave an increasing level of difficulty for the network and we show how this\nchoice can strongly affect the results. To assess whether the added spatial\ninformation helps improving performances, we compare our anatomically-informed\nCNN with a baseline, spatially-agnostic CNN. When considering the more\nrealistic and challenging scenario including vessel-like negative patches, the\nformer model attains the highest classification results (accuracy$\\simeq$95\\%,\nAUROC$\\simeq$0.95, AUPR$\\simeq$0.71), thus outperforming the baseline.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 08:00:54 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Di Noto", "Tommaso", ""], ["Marie", "Guillaume", ""], ["Tourbier", "S\u00e9bastien", ""], ["Alem\u00e1n-G\u00f3mez", "Yasser", ""], ["Saliou", "Guillaume", ""], ["Cuadra", "Meritxell Bach", ""], ["Hagmann", "Patric", ""], ["Richiardi", "Jonas", ""]]}, {"id": "2012.08655", "submitter": "Elian Malkin", "authors": "Elian Malkin, Arturo Deza, Tomaso Poggio", "title": "CUDA-Optimized real-time rendering of a Foveated Visual System", "comments": "16 pages, 13 figures, presented at the Shared Visual Representations\n  in Human and Machine Intelligence Workshop (SVRHM NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spatially-varying field of the human visual system has recently received\na resurgence of interest with the development of virtual reality (VR) and\nneural networks. The computational demands of high resolution rendering desired\nfor VR can be offset by savings in the periphery, while neural networks trained\nwith foveated input have shown perceptual gains in i.i.d and o.o.d\ngeneralization. In this paper, we present a technique that exploits the CUDA\nGPU architecture to efficiently generate Gaussian-based foveated images at high\ndefinition (1920x1080 px) in real-time (165 Hz), with a larger number of\npooling regions than previous Gaussian-based foveation algorithms by several\norders of magnitude, producing a smoothly foveated image that requires no\nfurther blending or stitching, and that can be well fit for any contrast\nsensitivity function. The approach described can be adapted from Gaussian\nblurring to any eccentricity-dependent image processing and our algorithm can\nmeet demand for experimentation to evaluate the role of spatially-varying\nprocessing across biological and artificial agents, so that foveation can be\nadded easily on top of existing systems rather than forcing their redesign\n(emulated foveated renderer). Altogether, this paper demonstrates how a GPU,\nwith a CUDA block-wise architecture, can be employed for radially-variant\nrendering, with opportunities for more complex post-processing to ensure a\nmetameric foveation scheme. Code is provided.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 22:43:04 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Malkin", "Elian", ""], ["Deza", "Arturo", ""], ["Poggio", "Tomaso", ""]]}, {"id": "2012.08662", "submitter": "Mohammad Zaki Zadeh", "authors": "Mohammad Zaki Zadeh, Ashwin Ramesh Babu, Ashish Jaiswal, Maria\n  Kyrarini, Morris Bell, Fillia Makedon", "title": "Automated system to measure Tandem Gait to assess executive functions in\n  children", "comments": null, "journal-ref": null, "doi": "10.1145/3453892.3453999", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As mobile technologies have become ubiquitous in recent years, computer-based\ncognitive tests have become more popular and efficient. In this work, we focus\non assessing motor function in children by analyzing their gait movements.\nAlthough there has been a lot of research on designing automated assessment\nsystems for gait analysis, most of these efforts use obtrusive wearable sensors\nfor measuring body movements. We have devised a computer vision-based\nassessment system that only requires a camera which makes it easier to employ\nin school or home environments. A dataset has been created with 27 children\nperforming the test. Furthermore in order to improve the accuracy of the\nsystem, a deep learning based model was pre-trained on NTU-RGB+D 120 dataset\nand then it was fine-tuned on our gait dataset. The results highlight the\nefficacy of proposed work for automating the assessment of children's\nperformances by achieving 76.61% classification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 23:12:13 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 18:40:25 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Zadeh", "Mohammad Zaki", ""], ["Babu", "Ashwin Ramesh", ""], ["Jaiswal", "Ashish", ""], ["Kyrarini", "Maria", ""], ["Bell", "Morris", ""], ["Makedon", "Fillia", ""]]}, {"id": "2012.08668", "submitter": "Rebecca Roelofs", "authors": "Rebecca Roelofs, Nicholas Cain, Jonathon Shlens, Michael C. Mozer", "title": "Mitigating Bias in Calibration Error Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building reliable machine learning systems requires that we correctly\nunderstand their level of confidence. Calibration measures the degree of\naccuracy in a model's confidence and most research in calibration focuses on\ntechniques to improve an empirical estimate of calibration error, ECE_bin. We\nintroduce a simulation framework that allows us to empirically show that\nECE_bin can systematically underestimate or overestimate the true calibration\nerror depending on the nature of model miscalibration, the size of the\nevaluation data set, and the number of bins. Critically, we find that ECE_bin\nis more strongly biased for perfectly calibrated models. We propose a simple\nalternative calibration error metric, ECE_sweep, in which the number of bins is\nchosen to be as large as possible while preserving monotonicity in the\ncalibration function. Evaluating our measure on distributions fit to neural\nnetwork confidence scores on CIFAR-10, CIFAR-100, and ImageNet, we show that\nECE_sweep produces a less biased estimator of calibration error and therefore\nshould be used by any researcher wishing to evaluate the calibration of models\ntrained on similar datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 23:28:06 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 19:25:00 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Roelofs", "Rebecca", ""], ["Cain", "Nicholas", ""], ["Shlens", "Jonathon", ""], ["Mozer", "Michael C.", ""]]}, {"id": "2012.08673", "submitter": "Linjie Li", "authors": "Linjie Li, Zhe Gan, Jingjing Liu", "title": "A Closer Look at the Robustness of Vision-and-Language Pre-trained\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large-scale pre-trained multimodal transformers, such as ViLBERT and UNITER,\nhave propelled the state of the art in vision-and-language (V+L) research to a\nnew level. Although achieving impressive performance on standard tasks, to\ndate, it still remains unclear how robust these pre-trained models are. To\ninvestigate, we conduct a host of thorough evaluations on existing pre-trained\nmodels over 4 different types of V+L specific model robustness: (i) Linguistic\nVariation; (ii) Logical Reasoning; (iii) Visual Content Manipulation; and (iv)\nAnswer Distribution Shift. Interestingly, by standard model finetuning,\npre-trained V+L models already exhibit better robustness than many\ntask-specific state-of-the-art methods. To further enhance model robustness, we\npropose Mango, a generic and efficient approach that learns a Multimodal\nAdversarial Noise GeneratOr in the embedding space to fool pre-trained V+L\nmodels. Differing from previous studies focused on one specific type of\nrobustness, Mango is task-agnostic, and enables universal performance lift for\npre-trained models over diverse tasks designed to evaluate broad aspects of\nrobustness. Comprehensive experiments demonstrate that Mango achieves new state\nof the art on 7 out of 9 robustness benchmarks, surpassing existing methods by\na significant margin. As the first comprehensive study on V+L robustness, this\nwork puts robustness of pre-trained models into sharper focus, pointing new\ndirections for future study.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 23:41:42 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 23:51:50 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Li", "Linjie", ""], ["Gan", "Zhe", ""], ["Liu", "Jingjing", ""]]}, {"id": "2012.08674", "submitter": "Zhe Gan", "authors": "Liqun Chen, Dong Wang, Zhe Gan, Jingjing Liu, Ricardo Henao, Lawrence\n  Carin", "title": "Wasserstein Contrastive Representation Distillation", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The primary goal of knowledge distillation (KD) is to encapsulate the\ninformation of a model learned from a teacher network into a student network,\nwith the latter being more compact than the former. Existing work, e.g., using\nKullback-Leibler divergence for distillation, may fail to capture important\nstructural knowledge in the teacher network and often lacks the ability for\nfeature generalization, particularly in situations when teacher and student are\nbuilt to address different classification tasks. We propose Wasserstein\nContrastive Representation Distillation (WCoRD), which leverages both primal\nand dual forms of Wasserstein distance for KD. The dual form is used for global\nknowledge transfer, yielding a contrastive learning objective that maximizes\nthe lower bound of mutual information between the teacher and the student\nnetworks. The primal form is used for local contrastive knowledge transfer\nwithin a mini-batch, effectively matching the distributions of features between\nthe teacher and the student networks. Experiments demonstrate that the proposed\nWCoRD method outperforms state-of-the-art approaches on privileged information\ndistillation, model compression and cross-modal transfer.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 23:43:28 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 00:14:56 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chen", "Liqun", ""], ["Wang", "Dong", ""], ["Gan", "Zhe", ""], ["Liu", "Jingjing", ""], ["Henao", "Ricardo", ""], ["Carin", "Lawrence", ""]]}, {"id": "2012.08678", "submitter": "Peter Washington", "authors": "Peter Washington, Haik Kalantarian, Jack Kent, Arman Husic, Aaron\n  Kline, Emilie Leblanc, Cathy Hou, Cezmi Mutlu, Kaitlyn Dunlap, Yordan Penev,\n  Maya Varma, Nate Stockham, Brianna Chrisman, Kelley Paskov, Min Woo Sun,\n  Jae-Yoon Jung, Catalin Voss, Nick Haber, Dennis P. Wall", "title": "Training an Emotion Detection Classifier using Frames from a Mobile\n  Therapeutic Game for Children with Developmental Disorders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Automated emotion classification could aid those who struggle to recognize\nemotion, including children with developmental behavioral conditions such as\nautism. However, most computer vision emotion models are trained on adult\naffect and therefore underperform on child faces. In this study, we designed a\nstrategy to gamify the collection and the labeling of child affect data in an\neffort to boost the performance of automatic child emotion detection to a level\ncloser to what will be needed for translational digital healthcare. We\nleveraged our therapeutic smartphone game, GuessWhat, which was designed in\nlarge part for children with developmental and behavioral conditions, to gamify\nthe secure collection of video data of children expressing a variety of\nemotions prompted by the game. Through a secure web interface gamifying the\nhuman labeling effort, we gathered and labeled 2,155 videos, 39,968 emotion\nframes, and 106,001 labels on all images. With this drastically expanded\npediatric emotion centric database (>30x larger than existing public pediatric\naffect datasets), we trained a pediatric emotion classification convolutional\nneural network (CNN) classifier of happy, sad, surprised, fearful, angry,\ndisgust, and neutral expressions in children. The classifier achieved 66.9%\nbalanced accuracy and 67.4% F1-score on the entirety of CAFE as well as 79.1%\nbalanced accuracy and 78.0% F1-score on CAFE Subset A, a subset containing at\nleast 60% human agreement on emotions labels. This performance is at least 10%\nhigher than all previously published classifiers, the best of which reached\n56.% balanced accuracy even when combining \"anger\" and \"disgust\" into a single\nclass. This work validates that mobile games designed for pediatric therapies\ncan generate high volumes of domain-relevant datasets to train state of the art\nclassifiers to perform tasks highly relevant to precision health efforts.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 00:08:51 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Washington", "Peter", ""], ["Kalantarian", "Haik", ""], ["Kent", "Jack", ""], ["Husic", "Arman", ""], ["Kline", "Aaron", ""], ["Leblanc", "Emilie", ""], ["Hou", "Cathy", ""], ["Mutlu", "Cezmi", ""], ["Dunlap", "Kaitlyn", ""], ["Penev", "Yordan", ""], ["Varma", "Maya", ""], ["Stockham", "Nate", ""], ["Chrisman", "Brianna", ""], ["Paskov", "Kelley", ""], ["Sun", "Min Woo", ""], ["Jung", "Jae-Yoon", ""], ["Voss", "Catalin", ""], ["Haber", "Nick", ""], ["Wall", "Dennis P.", ""]]}, {"id": "2012.08687", "submitter": "Jinshan Zeng", "authors": "Jinshan Zeng, Qi Chen, Yunxin Liu, Mingwen Wang, Yuan Yao", "title": "StrokeGAN: Reducing Mode Collapse in Chinese Font Generation via Stroke\n  Encoding", "comments": "10 pages, our codes and data are available at:\n  https://github.com/JinshanZeng/StrokeGAN", "journal-ref": "AAAI 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The generation of stylish Chinese fonts is an important problem involved in\nmany applications. Most of existing generation methods are based on the deep\ngenerative models, particularly, the generative adversarial networks (GAN)\nbased models. However, these deep generative models may suffer from the mode\ncollapse issue, which significantly degrades the diversity and quality of\ngenerated results. In this paper, we introduce a one-bit stroke encoding to\ncapture the key mode information of Chinese characters and then incorporate it\ninto CycleGAN, a popular deep generative model for Chinese font generation. As\na result we propose an efficient method called StrokeGAN, mainly motivated by\nthe observation that the stroke encoding contains amount of mode information of\nChinese characters. In order to reconstruct the one-bit stroke encoding of the\nassociated generated characters, we introduce a stroke-encoding reconstruction\nloss imposed on the discriminator. Equipped with such one-bit stroke encoding\nand stroke-encoding reconstruction loss, the mode collapse issue of CycleGAN\ncan be significantly alleviated, with an improved preservation of strokes and\ndiversity of generated characters. The effectiveness of StrokeGAN is\ndemonstrated by a series of generation tasks over nine datasets with different\nfonts. The numerical results demonstrate that StrokeGAN generally outperforms\nthe state-of-the-art methods in terms of content and recognition accuracies, as\nwell as certain stroke error, and also generates more realistic characters.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 01:36:19 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 01:41:46 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Zeng", "Jinshan", ""], ["Chen", "Qi", ""], ["Liu", "Yunxin", ""], ["Wang", "Mingwen", ""], ["Yao", "Yuan", ""]]}, {"id": "2012.08689", "submitter": "Zixiang Zhao", "authors": "Chengyang Liang, Zixiang Zhao, Junmin Liu, Jiangshe Zhang", "title": "Domain Adaptive Object Detection via Feature Separation and Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, adversarial-based domain adaptive object detection (DAOD) methods\nhave been developed rapidly. However, there are two issues that need to be\nresolved urgently. Firstly, numerous methods reduce the distributional shifts\nonly by aligning all the feature between the source and target domain, while\nignoring the private information of each domain. Secondly, DAOD should consider\nthe feature alignment on object existing regions in images. But redundancy of\nthe region proposals and background noise could reduce the domain\ntransferability. Therefore, we establish a Feature Separation and Alignment\nNetwork (FSANet) which consists of a gray-scale feature separation (GSFS)\nmodule, a local-global feature alignment (LGFA) module and a\nregion-instance-level alignment (RILA) module. The GSFS module decomposes the\ndistractive/shared information which is useless/useful for detection by a\ndual-stream framework, to focus on intrinsic feature of objects and resolve the\nfirst issue. Then, LGFA and RILA modules reduce the distributional shifts of\nthe multi-level features. Notably, scale-space filtering is exploited to\nimplement adaptive searching for regions to be aligned, and instance-level\nfeatures in each region are refined to reduce redundancy and noise mentioned in\nthe second issue. Various experiments on multiple benchmark datasets prove that\nour FSANet achieves better performance on the target domain detection and\nsurpasses the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 01:44:34 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Liang", "Chengyang", ""], ["Zhao", "Zixiang", ""], ["Liu", "Junmin", ""], ["Zhang", "Jiangshe", ""]]}, {"id": "2012.08697", "submitter": "Yaqi Liu", "authors": "Yaqi Liu and Chao Xia and Xiaobin Zhu and Shengwei Xu", "title": "Two-Stage Copy-Move Forgery Detection with Self Deep Matching and\n  Proposal SuperGlue", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copy-move forgery detection identifies a tampered image by detecting pasted\nand source regions in the same image. In this paper, we propose a novel\ntwo-stage framework specially for copy-move forgery detection. The first stage\nis a backbone self deep matching network, and the second stage is named as\nProposal SuperGlue. In the first stage, atrous convolution and skip matching\nare incorporated to enrich spatial information and leverage hierarchical\nfeatures. Spatial attention is built on self-correlation to reinforce the\nability to find appearance similar regions. In the second stage, Proposal\nSuperGlue is proposed to remove false-alarmed regions and remedy incomplete\nregions. Specifically, a proposal selection strategy is designed to enclose\nhighly suspected regions based on proposal generation and backbone score maps.\nThen, pairwise matching is conducted among candidate proposals by deep learning\nbased keypoint extraction and matching, i.e., SuperPoint and SuperGlue.\nIntegrated score map generation and refinement methods are designed to\nintegrate results of both stages and obtain optimized results. Our two-stage\nframework unifies end-to-end deep matching and keypoint matching by obtaining\nhighly suspected proposals, and opens a new gate for deep learning research in\ncopy-move forgery detection. Experiments on publicly available datasets\ndemonstrate the effectiveness of our two-stage framework.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 02:05:55 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Liu", "Yaqi", ""], ["Xia", "Chao", ""], ["Zhu", "Xiaobin", ""], ["Xu", "Shengwei", ""]]}, {"id": "2012.08707", "submitter": "Lijun Zhang", "authors": "Lijun Zhang, Xiao Liu, Erik Learned-Miller, Hui Guan", "title": "SID-NISM: A Self-supervised Low-light Image Enhancement Framework", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When capturing images in low-light conditions, the images often suffer from\nlow visibility, which not only degrades the visual aesthetics of images, but\nalso significantly degenerates the performance of many computer vision\nalgorithms. In this paper, we propose a self-supervised low-light image\nenhancement framework (SID-NISM), which consists of two components, a\nSelf-supervised Image Decomposition Network (SID-Net) and a Nonlinear\nIllumination Saturation Mapping function (NISM). As a self-supervised network,\nSID-Net could decompose the given low-light image into its reflectance,\nillumination and noise directly without any prior training or reference image,\nwhich distinguishes it from existing supervised-learning methods greatly. Then,\nthe decomposed illumination map will be enhanced by NISM. Having the restored\nillumination map, the enhancement can be achieved accordingly. Experiments on\nseveral public challenging low-light image datasets reveal that the images\nenhanced by SID-NISM are more natural and have less unexpected artifacts.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 02:33:13 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Zhang", "Lijun", ""], ["Liu", "Xiao", ""], ["Learned-Miller", "Erik", ""], ["Guan", "Hui", ""]]}, {"id": "2012.08721", "submitter": "Pengbo Liu", "authors": "Pengbo Liu, Hu Han, Yuanqi Du, Heqin Zhu, Yinhao Li, Feng Gu, Honghu\n  Xiao, Jun Li, Chunpeng Zhao, Li Xiao, Xinbao Wu and S.Kevin Zhou", "title": "Deep Learning to Segment Pelvic Bones: Large-scale CT Datasets and\n  Baseline Models", "comments": "Accepted by IPCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Purpose: Pelvic bone segmentation in CT has always been an essential step in\nclinical diagnosis and surgery planning of pelvic bone diseases. Existing\nmethods for pelvic bone segmentation are either hand-crafted or semi-automatic\nand achieve limited accuracy when dealing with image appearance variations due\nto the multi-site domain shift, the presence of contrasted vessels, coprolith\nand chyme, bone fractures, low dose, metal artifacts, etc. Due to the lack of a\nlarge-scale pelvic CT dataset with annotations, deep learning methods are not\nfully explored. Methods: In this paper, we aim to bridge the data gap by\ncurating a large pelvic CT dataset pooled from multiple sources and different\nmanufacturers, including 1, 184 CT volumes and over 320, 000 slices with\ndifferent resolutions and a variety of the above-mentioned appearance\nvariations. Then we propose for the first time, to the best of our knowledge,\nto learn a deep multi-class network for segmenting lumbar spine, sacrum, left\nhip, and right hip, from multiple-domain images simultaneously to obtain more\neffective and robust feature representations. Finally, we introduce a\npost-processing tool based on the signed distance function (SDF) to eliminate\nfalse predictions while retaining correctly predicted bone fragments. Results:\nExtensive experiments on our dataset demonstrate the effectiveness of our\nautomatic method, achieving an average Dice of 0.987 for a metal-free volume.\nSDF post-processor yields a decrease of 10.5% in hausdorff distance by\nmaintaining important bone fragments in post-processing phase. Conclusion: We\nbelieve this large-scale dataset will promote the development of the whole\ncommunity and plan to open source the images, annotations, codes, and trained\nbaseline models at https://github.com/ICT-MIRACLE-lab/CTPelvic1K.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 03:30:40 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 03:06:03 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Liu", "Pengbo", ""], ["Han", "Hu", ""], ["Du", "Yuanqi", ""], ["Zhu", "Heqin", ""], ["Li", "Yinhao", ""], ["Gu", "Feng", ""], ["Xiao", "Honghu", ""], ["Li", "Jun", ""], ["Zhao", "Chunpeng", ""], ["Xiao", "Li", ""], ["Wu", "Xinbao", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2012.08726", "submitter": "Ning Yu", "authors": "Ning Yu, Vladislav Skripniuk, Dingfan Chen, Larry Davis, Mario Fritz", "title": "Responsible Disclosure of Generative Models Using Scalable\n  Fingerprinting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.CY cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past six years, deep generative models have achieved a qualitatively\nnew level of performance. Generated data has become difficult, if not\nimpossible, to be distinguished from real data. While there are plenty of use\ncases that benefit from this technology, there are also strong concerns on how\nthis new technology can be misused to spoof sensors, generate deep fakes, and\nenable misinformation at scale. Unfortunately, current deep fake detection\nmethods are not sustainable, as the gap between real and fake continues to\nclose. In contrast, our work enables a responsible disclosure of such\nstate-of-the-art generative models, that allows researchers and companies to\nfingerprint their models, so that the generated samples containing a\nfingerprint can be accurately detected and attributed to a source. Our\ntechnique achieves this by an efficient and scalable ad-hoc generation of a\nlarge population of models with distinct fingerprints. Our recommended\noperation point uses a 128-bit fingerprint which in principle results in more\nthan $10^{36}$ identifiable models. Experiments show that our method fulfills\nkey properties of a fingerprinting mechanism and achieves effectiveness in deep\nfake detection and attribution.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 03:51:54 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 04:19:56 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 08:17:25 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 23:51:15 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yu", "Ning", ""], ["Skripniuk", "Vladislav", ""], ["Chen", "Dingfan", ""], ["Davis", "Larry", ""], ["Fritz", "Mario", ""]]}, {"id": "2012.08730", "submitter": "Yi Zhou", "authors": "Yi Zhou, Guillermo Gallego, Xiuyuan Lu, Siqi Liu, and Shaojie Shen", "title": "Event-based Motion Segmentation with Spatio-Temporal Graph Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying independently moving objects is an essential task for dynamic\nscene understanding. However, traditional cameras used in dynamic scenes may\nsuffer from motion blur or exposure artifacts due to their sampling principle.\nBy contrast, event-based cameras are novel bio-inspired sensors that offer\nadvantages to overcome such limitations. They report pixel-wise intensity\nchanges asynchronously, which enables them to acquire visual information at\nexactly the same rate as the scene dynamics. We have developed a method to\nidentify independently moving objects acquired with an event-based camera,\ni.e., to solve the event-based motion segmentation problem. This paper\ndescribes how to formulate the problem as a weakly-constrained multi-model\nfitting one via energy minimization, and how to jointly solve its two\nsubproblems -- event-cluster assignment (labeling) and motion model fitting --\nin an iterative manner, by exploiting the spatio-temporal structure of input\nevents in the form of a space-time graph. Experiments on available datasets\ndemonstrate the versatility of the method in scenes with different motion\npatterns and number of moving objects. The evaluation shows that the method\nperforms on par or better than the state of the art without having to\npredetermine the number of expected moving objects.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 04:06:02 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 00:08:17 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhou", "Yi", ""], ["Gallego", "Guillermo", ""], ["Lu", "Xiuyuan", ""], ["Liu", "Siqi", ""], ["Shen", "Shaojie", ""]]}, {"id": "2012.08732", "submitter": "Weiling Chen", "authors": "Tiesong Zhao, Yuting Lin, Yiwen Xu, Weiling Chen, Zhou Wang", "title": "Learning-Based Quality Assessment for Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image Super-Resolution (SR) techniques improve visual quality by enhancing\nthe spatial resolution of images. Quality evaluation metrics play a critical\nrole in comparing and optimizing SR algorithms, but current metrics achieve\nonly limited success, largely due to the lack of large-scale quality databases,\nwhich are essential for learning accurate and robust SR quality metrics. In\nthis work, we first build a large-scale SR image database using a novel\nsemi-automatic labeling approach, which allows us to label a large number of\nimages with manageable human workload. The resulting SR Image quality database\nwith Semi-Automatic Ratings (SISAR), so far the largest of SR-IQA database,\ncontains 8,400 images of 100 natural scenes. We train an end-to-end Deep Image\nSR Quality (DISQ) model by employing two-stream Deep Neural Networks (DNNs) for\nfeature extraction, followed by a feature fusion network for quality\nprediction. Experimental results demonstrate that the proposed method\noutperforms state-of-the-art metrics and achieves promising generalization\nperformance in cross-database tests. The SISAR database and DISQ model will be\nmade publicly available to facilitate reproducible research.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 04:06:27 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Zhao", "Tiesong", ""], ["Lin", "Yuting", ""], ["Xu", "Yiwen", ""], ["Chen", "Weiling", ""], ["Wang", "Zhou", ""]]}, {"id": "2012.08733", "submitter": "Kecheng Zheng", "authors": "Kecheng Zheng, Cuiling Lan, Wenjun Zeng, Zhizheng Zhang and Zheng-Jun\n  Zha", "title": "Exploiting Sample Uncertainty for Domain Adaptive Person\n  Re-Identification", "comments": "9 pages. Accepted to 35th AAAI Conference on Artificial Intelligence\n  (AAAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many unsupervised domain adaptive (UDA) person re-identification (ReID)\napproaches combine clustering-based pseudo-label prediction with feature\nfine-tuning. However, because of domain gap, the pseudo-labels are not always\nreliable and there are noisy/incorrect labels. This would mislead the feature\nrepresentation learning and deteriorate the performance. In this paper, we\npropose to estimate and exploit the credibility of the assigned pseudo-label of\neach sample to alleviate the influence of noisy labels, by suppressing the\ncontribution of noisy samples. We build our baseline framework using the mean\nteacher method together with an additional contrastive loss. We have observed\nthat a sample with a wrong pseudo-label through clustering in general has a\nweaker consistency between the output of the mean teacher model and the student\nmodel. Based on this finding, we propose to exploit the uncertainty (measured\nby consistency levels) to evaluate the reliability of the pseudo-label of a\nsample and incorporate the uncertainty to re-weight its contribution within\nvarious ReID losses, including the identity (ID) classification loss per\nsample, the triplet loss, and the contrastive loss. Our uncertainty-guided\noptimization brings significant improvement and achieves the state-of-the-art\nperformance on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 04:09:04 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 06:08:08 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Zheng", "Kecheng", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Zhang", "Zhizheng", ""], ["Zha", "Zheng-Jun", ""]]}, {"id": "2012.08751", "submitter": "Masaki Kitayama", "authors": "Masaki Kitayama, Hitoshi Kiya", "title": "Difficulty in estimating visual information from randomly sampled images", "comments": "accepted for publication in 2020 IEEE 9th Global Conference on\n  Consumer Electronics (GCCE 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we evaluate dimensionality reduction methods in terms of\ndifficulty in estimating visual information on original images from\ndimensionally reduced ones. Recently, dimensionality reduction has been\nreceiving attention as the process of not only reducing the number of random\nvariables, but also protecting visual information for privacy-preserving\nmachine learning. For such a reason, difficulty in estimating visual\ninformation is discussed. In particular, the random sampling method that was\nproposed for privacy-preserving machine learning, is compared with typical\ndimensionality reduction methods. In an image classification experiment, the\nrandom sampling method is demonstrated not only to have high difficulty, but\nalso to be comparable to other dimensionality reduction methods, while\nmaintaining the property of spatial information invariant.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 05:46:03 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Kitayama", "Masaki", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "2012.08769", "submitter": "Esther Bron", "authors": "Esther E. Bron, Stefan Klein, Janne M. Papma, Lize C. Jiskoot, Vikram\n  Venkatraghavan, Jara Linders, Pauline Aalten, Peter Paul De Deyn, Geert Jan\n  Biessels, Jurgen A.H.R. Claassen, Huub A.M. Middelkoop, Marion Smits, Wiro J.\n  Niessen, John C. van Swieten, Wiesje M. van der Flier, Inez H.G.B. Ramakers,\n  Aad van der Lugt", "title": "Cross-Cohort Generalizability of Deep and Conventional Machine Learning\n  for MRI-based Diagnosis and Prediction of Alzheimer's Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work validates the generalizability of MRI-based classification of\nAlzheimer's disease (AD) patients and controls (CN) to an external data set and\nto the task of prediction of conversion to AD in individuals with mild\ncognitive impairment (MCI). We used a conventional support vector machine (SVM)\nand a deep convolutional neural network (CNN) approach based on structural MRI\nscans that underwent either minimal pre-processing or more extensive\npre-processing into modulated gray matter (GM) maps. Classifiers were optimized\nand evaluated using cross-validation in the ADNI (334 AD, 520 CN). Trained\nclassifiers were subsequently applied to predict conversion to AD in ADNI MCI\npatients (231 converters, 628 non-converters) and in the independent Health-RI\nParelsnoer data set. From this multi-center study representing a tertiary\nmemory clinic population, we included 199 AD patients, 139 participants with\nsubjective cognitive decline, 48 MCI patients converting to dementia, and 91\nMCI patients who did not convert to dementia. AD-CN classification based on\nmodulated GM maps resulted in a similar AUC for SVM (0.940) and CNN (0.933).\nApplication to conversion prediction in MCI yielded significantly higher\nperformance for SVM (0.756) than for CNN (0.742). In external validation,\nperformance was slightly decreased. For AD-CN, it again gave similar AUCs for\nSVM (0.896) and CNN (0.876). For prediction in MCI, performances decreased for\nboth SVM (0.665) and CNN (0.702). Both with SVM and CNN, classification based\non modulated GM maps significantly outperformed classification based on\nminimally processed images. Deep and conventional classifiers performed equally\nwell for AD classification and their performance decreased only slightly when\napplied to the external cohort. We expect that this work on external validation\ncontributes towards translation of machine learning to clinical practice.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 07:09:30 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 09:19:36 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 11:32:37 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Bron", "Esther E.", ""], ["Klein", "Stefan", ""], ["Papma", "Janne M.", ""], ["Jiskoot", "Lize C.", ""], ["Venkatraghavan", "Vikram", ""], ["Linders", "Jara", ""], ["Aalten", "Pauline", ""], ["De Deyn", "Peter Paul", ""], ["Biessels", "Geert Jan", ""], ["Claassen", "Jurgen A. H. R.", ""], ["Middelkoop", "Huub A. M.", ""], ["Smits", "Marion", ""], ["Niessen", "Wiro J.", ""], ["van Swieten", "John C.", ""], ["van der Flier", "Wiesje M.", ""], ["Ramakers", "Inez H. G. B.", ""], ["van der Lugt", "Aad", ""]]}, {"id": "2012.08770", "submitter": "Shu Zhang", "authors": "Shu Zhang, Jincheng Xu, Yu-Chun Chen, Jiechao Ma, Zihao Li, Yizhou\n  Wang and Yizhou Yu", "title": "Revisiting 3D Context Modeling with Supervised Pre-training for\n  Universal Lesion Detection in CT Slices", "comments": "10 pages, 2 figures, MICCAI20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Universal lesion detection from computed tomography (CT) slices is important\nfor comprehensive disease screening. Since each lesion can locate in multiple\nadjacent slices, 3D context modeling is of great significance for developing\nautomated lesion detection algorithms. In this work, we propose a Modified\nPseudo-3D Feature Pyramid Network (MP3D FPN) that leverages depthwise separable\nconvolutional filters and a group transform module (GTM) to efficiently extract\n3D context enhanced 2D features for universal lesion detection in CT slices. To\nfacilitate faster convergence, a novel 3D network pre-training method is\nderived using solely large-scale 2D object detection dataset in the natural\nimage domain. We demonstrate that with the novel pre-training method, the\nproposed MP3D FPN achieves state-of-the-art detection performance on the\nDeepLesion dataset (3.48% absolute improvement in the sensitivity of FPs@0.5),\nsignificantly surpassing the baseline method by up to 6.06% (in MAP@0.5) which\nadopts 2D convolution for 3D context modeling. Moreover, the proposed 3D\npre-trained weights can potentially be used to boost the performance of other\n3D medical image analysis tasks.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 07:11:16 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Zhang", "Shu", ""], ["Xu", "Jincheng", ""], ["Chen", "Yu-Chun", ""], ["Ma", "Jiechao", ""], ["Li", "Zihao", ""], ["Wang", "Yizhou", ""], ["Yu", "Yizhou", ""]]}, {"id": "2012.08780", "submitter": "Maha Shadaydeh Dr", "authors": "Maha Shadaydeh, Lea Mueller, Dana Schneider, Martin Thuemmel, Thomas\n  Kessler, Joachim Denzler", "title": "Analysing the Direction of Emotional Influence in Nonverbal Dyadic\n  Communication: A Facial-Expression Study", "comments": "arXiv admin note: text overlap with arXiv:1810.12171", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying the direction of emotional influence in a dyadic dialogue is of\nincreasing interest in the psychological sciences with applications in\npsychotherapy, analysis of political interactions, or interpersonal conflict\nbehavior. Facial expressions are widely described as being automatic and thus\nhard to overtly influence. As such, they are a perfect measure for a better\nunderstanding of unintentional behavior cues about social-emotional cognitive\nprocesses. With this view, this study is concerned with the analysis of the\ndirection of emotional influence in dyadic dialogue based on facial expressions\nonly. We exploit computer vision capabilities along with causal inference\ntheory for quantitative verification of hypotheses on the direction of\nemotional influence, i.e., causal effect relationships, in dyadic dialogues. We\naddress two main issues. First, in a dyadic dialogue, emotional influence\noccurs over transient time intervals and with intensity and direction that are\nvariant over time. To this end, we propose a relevant interval selection\napproach that we use prior to causal inference to identify those transient\nintervals where causal inference should be applied. Second, we propose to use\nfine-grained facial expressions that are present when strong distinct facial\nemotions are not visible. To specify the direction of influence, we apply the\nconcept of Granger causality to the time series of facial expressions over\nselected relevant intervals. We tested our approach on newly, experimentally\nobtained data. Based on the quantitative verification of hypotheses on the\ndirection of emotional influence, we were able to show that the proposed\napproach is most promising to reveal the causal effect pattern in various\ninstructed interaction conditions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 07:52:35 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Shadaydeh", "Maha", ""], ["Mueller", "Lea", ""], ["Schneider", "Dana", ""], ["Thuemmel", "Martin", ""], ["Kessler", "Thomas", ""], ["Denzler", "Joachim", ""]]}, {"id": "2012.08803", "submitter": "Ricard Durall Lopez", "authors": "Ricard Durall, Kalun Ho, Franz-Josef Pfreundt and Janis Keuper", "title": "Latent Space Conditioning on Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative adversarial networks are the state of the art approach towards\nlearned synthetic image generation. Although early successes were mostly\nunsupervised, bit by bit, this trend has been superseded by approaches based on\nlabelled data. These supervised methods allow a much finer-grained control of\nthe output image, offering more flexibility and stability. Nevertheless, the\nmain drawback of such models is the necessity of annotated data. In this work,\nwe introduce an novel framework that benefits from two popular learning\ntechniques, adversarial training and representation learning, and takes a step\ntowards unsupervised conditional GANs. In particular, our approach exploits the\nstructure of a latent space (learned by the representation learning) and\nemploys it to condition the generative model. In this way, we break the\ntraditional dependency between condition and label, substituting the latter by\nunsupervised features coming from the latent space. Finally, we show that this\nnew technique is able to produce samples on demand keeping the quality of its\nsupervised counterpart.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 08:58:10 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Durall", "Ricard", ""], ["Ho", "Kalun", ""], ["Pfreundt", "Franz-Josef", ""], ["Keuper", "Janis", ""]]}, {"id": "2012.08804", "submitter": "Jianan Li", "authors": "Jianan Li, Xuemei Xie, Zhifu Zhao, Yuhan Cao, Qingzhe Pan and\n  Guangming Shi", "title": "Temporal Graph Modeling for Skeleton-based Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs), which model skeleton data as graphs,\nhave obtained remarkable performance for skeleton-based action recognition.\nParticularly, the temporal dynamic of skeleton sequence conveys significant\ninformation in the recognition task. For temporal dynamic modeling, GCN-based\nmethods only stack multi-layer 1D local convolutions to extract temporal\nrelations between adjacent time steps. With the repeat of a lot of local\nconvolutions, the key temporal information with non-adjacent temporal distance\nmay be ignored due to the information dilution. Therefore, these methods still\nremain unclear how to fully explore temporal dynamic of skeleton sequence. In\nthis paper, we propose a Temporal Enhanced Graph Convolutional Network (TE-GCN)\nto tackle this limitation. The proposed TE-GCN constructs temporal relation\ngraph to capture complex temporal dynamic. Specifically, the constructed\ntemporal relation graph explicitly builds connections between semantically\nrelated temporal features to model temporal relations between both adjacent and\nnon-adjacent time steps. Meanwhile, to further explore the sufficient temporal\ndynamic, multi-head mechanism is designed to investigate multi-kinds of\ntemporal relations. Extensive experiments are performed on two widely used\nlarge-scale datasets, NTU-60 RGB+D and NTU-120 RGB+D. And experimental results\nshow that the proposed model achieves the state-of-the-art performance by\nmaking contribution to temporal modeling for action recognition.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 09:02:47 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Li", "Jianan", ""], ["Xie", "Xuemei", ""], ["Zhao", "Zhifu", ""], ["Cao", "Yuhan", ""], ["Pan", "Qingzhe", ""], ["Shi", "Guangming", ""]]}, {"id": "2012.08824", "submitter": "Aleksei Shpilman", "authors": "Aleksandra Malysheva, Daniel Kudenko, Aleksei Shpilman", "title": "Learning to Run with Potential-Based Reward Shaping and Demonstrations\n  from Video Data", "comments": null, "journal-ref": null, "doi": "10.1109/ICARCV.2018.8581310", "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning to produce efficient movement behaviour for humanoid robots from\nscratch is a hard problem, as has been illustrated by the \"Learning to run\"\ncompetition at NIPS 2017. The goal of this competition was to train a\ntwo-legged model of a humanoid body to run in a simulated race course with\nmaximum speed. All submissions took a tabula rasa approach to reinforcement\nlearning (RL) and were able to produce relatively fast, but not optimal running\nbehaviour. In this paper, we demonstrate how data from videos of human running\n(e.g. taken from YouTube) can be used to shape the reward of the humanoid\nlearning agent to speed up the learning and produce a better result.\nSpecifically, we are using the positions of key body parts at regular time\nintervals to define a potential function for potential-based reward shaping\n(PBRS). Since PBRS does not change the optimal policy, this approach allows the\nRL agent to overcome sub-optimalities in the human movements that are shown in\nthe videos.\n  We present experiments in which we combine selected techniques from the top\nten approaches from the NIPS competition with further optimizations to create\nan high-performing agent as a baseline. We then demonstrate how video-based\nreward shaping improves the performance further, resulting in an RL agent that\nruns twice as fast as the baseline in 12 hours of training. We furthermore show\nthat our approach can overcome sub-optimal running behaviour in videos, with\nthe learned policy significantly outperforming that of the running agent from\nthe video.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 09:46:58 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Malysheva", "Aleksandra", ""], ["Kudenko", "Daniel", ""], ["Shpilman", "Aleksei", ""]]}, {"id": "2012.08859", "submitter": "Bert Moons", "authors": "Bert Moons, Parham Noorzad, Andrii Skliar, Giovanni Mariani, Dushyant\n  Mehta, Chris Lott, Tijmen Blankevoort", "title": "Distilling Optimal Neural Networks: Rapid Search in Diverse Spaces", "comments": "Main text 9 pages, Full text 21 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today, state-of-the-art Neural Architecture Search (NAS) methods cannot scale\nto many hardware platforms or scenarios at a low training costs and/or can only\nhandle non-diverse, heavily constrained architectural search-spaces. To solve\nthese issues, we present DONNA (Distilling Optimal Neural Network\nArchitectures), a novel pipeline for rapid and diverse NAS, that scales to many\nuser scenarios. In DONNA, a search consists of three phases. First, an accuracy\npredictor is built using blockwise knowledge distillation. This predictor\nenables searching across diverse networks with varying macro-architectural\nparameters such as layer types and attention mechanisms as well as across\nmicro-architectural parameters such as block repeats and expansion rates.\nSecond, a rapid evolutionary search phase finds a set of Pareto-optimal\narchitectures for any scenario using the accuracy predictor and on-device\nmeasurements. Third, optimal models are quickly finetuned to\ntraining-from-scratch accuracy. With this approach, DONNA is up to 100x faster\nthan MNasNet in finding state-of-the-art architectures on-device. Classifying\nImageNet, DONNA architectures are 20% faster than EfficientNet-B0 and\nMobileNetV2 on a Nvidia V100 GPU and 10% faster with 0.5% higher accuracy than\nMobileNetV2-1.4x on a Samsung S20 smartphone. In addition to NAS, DONNA is used\nfor search-space extension and exploration, as well as hardware-aware model\ncompression.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 11:00:19 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 08:14:26 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Moons", "Bert", ""], ["Noorzad", "Parham", ""], ["Skliar", "Andrii", ""], ["Mariani", "Giovanni", ""], ["Mehta", "Dushyant", ""], ["Lott", "Chris", ""], ["Blankevoort", "Tijmen", ""]]}, {"id": "2012.08890", "submitter": "Dan Jia", "authors": "Dan Jia and Mats Steinweg and Alexander Hermans and Bastian Leibe", "title": "Self-Supervised Person Detection in 2D Range Data using a Calibrated\n  Camera", "comments": "2021 IEEE International Conference on Robotics and Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is the essential building block of state-of-the-art person\ndetectors in 2D range data. However, only a few annotated datasets are\navailable for training and testing these deep networks, potentially limiting\ntheir performance when deployed in new environments or with different LiDAR\nmodels. We propose a method, which uses bounding boxes from an image-based\ndetector (e.g. Faster R-CNN) on a calibrated camera to automatically generate\ntraining labels (called pseudo-labels) for 2D LiDAR-based person detectors.\nThrough experiments on the JackRabbot dataset with two detector models, DROW3\nand DR-SPAAM, we show that self-supervised detectors, trained or fine-tuned\nwith pseudo-labels, outperform detectors trained only on a different dataset.\nCombined with robust training techniques, the self-supervised detectors reach a\nperformance close to the ones trained using manual annotations of the target\ndataset. Our method is an effective way to improve person detectors during\ndeployment without any additional labeling effort, and we release our source\ncode to support relevant robotic applications.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 12:10:04 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 13:30:48 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Jia", "Dan", ""], ["Steinweg", "Mats", ""], ["Hermans", "Alexander", ""], ["Leibe", "Bastian", ""]]}, {"id": "2012.08922", "submitter": "Zhichao Wu", "authors": "Zhichao Wu and Lei Guo and Hao Zhang and Dan Xu", "title": "Unsupervised Image Segmentation using Mutual Mean-Teaching", "comments": "5 figures, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image segmentation aims at assigning the pixels with similar\nfeature into a same cluster without annotation, which is an important task in\ncomputer vision. Due to lack of prior knowledge, most of existing model usually\nneed to be trained several times to obtain suitable results. To address this\nproblem, we propose an unsupervised image segmentation model based on the\nMutual Mean-Teaching (MMT) framework to produce more stable results. In\naddition, since the labels of pixels from two model are not matched, a label\nalignment algorithm based on the Hungarian algorithm is proposed to match the\ncluster labels. Experimental results demonstrate that the proposed model is\nable to segment various types of images and achieves better performance than\nthe existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 13:13:34 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Wu", "Zhichao", ""], ["Guo", "Lei", ""], ["Zhang", "Hao", ""], ["Xu", "Dan", ""]]}, {"id": "2012.08924", "submitter": "Onur G\\\"unl\\\"u Dr.-Ing.", "authors": "Onur G\\\"unl\\\"u and Rafael F. Schaefer", "title": "Secret Key Agreement with Physical Unclonable Functions: An Optimality\n  Summary", "comments": "To appear in MDPI Entropy Journal. arXiv admin note: text overlap\n  with arXiv:2002.11687", "journal-ref": null, "doi": "10.3390/e23010016", "report-no": null, "categories": "eess.SP cs.CR cs.CV cs.IT cs.MM math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address security and privacy problems for digital devices and biometrics\nfrom an information-theoretic optimality perspective, where a secret key is\ngenerated for authentication, identification, message encryption/decryption, or\nsecure computations. A physical unclonable function (PUF) is a promising\nsolution for local security in digital devices and this review gives the most\nrelevant summary for information theorists, coding theorists, and signal\nprocessing community members who are interested in optimal PUF constructions.\nLow-complexity signal processing methods such as transform coding that are\ndeveloped to make the information-theoretic analysis tractable are discussed.\nThe optimal trade-offs between the secret-key, privacy-leakage, and storage\nrates for multiple PUF measurements are given. Proposed optimal code\nconstructions that jointly design the vector quantizer and error-correction\ncode parameters are listed. These constructions include modern and algebraic\ncodes such as polar codes and convolutional codes, both of which can achieve\nsmall block-error probabilities at short block lengths, corresponding to a\nsmall number of PUF circuits. Open problems in the PUF literature from a signal\nprocessing, information theory, coding theory, and hardware complexity\nperspectives and their combinations are listed to stimulate further\nadvancements in the research on local privacy and security.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 13:21:20 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["G\u00fcnl\u00fc", "Onur", ""], ["Schaefer", "Rafael F.", ""]]}, {"id": "2012.08929", "submitter": "Dengqiang Jia", "authors": "Dengqiang Jia, Xiahai Zhuang", "title": "Learning-Based Algorithms for Vessel Tracking: A Review", "comments": "19 pages, 3 figures, 9 tables, accept by Computerized Medical Imaging\n  and Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing efficient vessel-tracking algorithms is crucial for imaging-based\ndiagnosis and treatment of vascular diseases. Vessel tracking aims to solve\nrecognition problems such as key (seed) point detection, centerline extraction,\nand vascular segmentation. Extensive image-processing techniques have been\ndeveloped to overcome the problems of vessel tracking that are mainly\nattributed to the complex morphologies of vessels and image characteristics of\nangiography. This paper presents a literature review on vessel-tracking\nmethods, focusing on machine-learning-based methods. First, the conventional\nmachine-learning-based algorithms are reviewed, and then, a general survey of\ndeep-learning-based frameworks is provided. On the basis of the reviewed\nmethods, the evaluation issues are introduced. The paper is concluded with\ndiscussions about the remaining exigencies and future research.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 13:31:51 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Jia", "Dengqiang", ""], ["Zhuang", "Xiahai", ""]]}, {"id": "2012.08932", "submitter": "Nishant Kumar", "authors": "Nishant Kumar, Stefan Gumhold", "title": "FuseVis: Interpreting neural networks for image fusion using per-pixel\n  saliency visualization", "comments": "30 pages, 9 figures, MDPI Journal (Computers)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image fusion helps in merging two or more images to construct a more\ninformative single fused image. Recently, unsupervised learning based\nconvolutional neural networks (CNN) have been utilized for different types of\nimage fusion tasks such as medical image fusion, infrared-visible image fusion\nfor autonomous driving as well as multi-focus and multi-exposure image fusion\nfor satellite imagery. However, it is challenging to analyze the reliability of\nthese CNNs for the image fusion tasks since no groundtruth is available. This\nled to the use of a wide variety of model architectures and optimization\nfunctions yielding quite different fusion results. Additionally, due to the\nhighly opaque nature of such neural networks, it is difficult to explain the\ninternal mechanics behind its fusion results. To overcome these challenges, we\npresent a novel real-time visualization tool, named FuseVis, with which the\nend-user can compute per-pixel saliency maps that examine the influence of the\ninput image pixels on each pixel of the fused image. We trained several image\nfusion based CNNs on medical image pairs and then using our FuseVis tool, we\nperformed case studies on a specific clinical application by interpreting the\nsaliency maps from each of the fusion methods. We specifically visualized the\nrelative influence of each input image on the predictions of the fused image\nand showed that some of the evaluated image fusion methods are better suited\nfor the specific clinical application. To the best of our knowledge, currently,\nthere is no approach for visual analysis of neural networks for image fusion.\nTherefore, this work opens up a new research direction to improve the\ninterpretability of deep fusion networks. The FuseVis tool can also be adapted\nin other deep neural network based image processing applications to make them\ninterpretable.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 10:03:02 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Kumar", "Nishant", ""], ["Gumhold", "Stefan", ""]]}, {"id": "2012.08933", "submitter": "Jessica Lundin PhD", "authors": "Jessica M. Lundin and Michael Sollami and Brian Lonsdorf and Alan Ross\n  and Owen Schoppe and David Woodward and S\\\"onke Rohde", "title": "Copyspace: Where to Write on Images?", "comments": "4th Workshop on Machine Learning for Creativity and Design, NeurIPS\n  2020, Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The placement of text over an image is an important part of producing\nhigh-quality visual designs. Automating this work by determining appropriate\nposition, orientation, and style for textual elements requires understanding\nthe contents of the background image. We refer to the search for aesthetic\nparameters of text rendered over images as \"copyspace detection\", noting that\nthis task is distinct from foreground-background separation. We have developed\nsolutions using one and two stage object detection methodologies trained on an\nexpertly labeled data. This workshop will examine such algorithms for copyspace\ndetection and demonstrate their application in generative design models and\npipelines such as Einstein Designer.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 01:58:39 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Lundin", "Jessica M.", ""], ["Sollami", "Michael", ""], ["Lonsdorf", "Brian", ""], ["Ross", "Alan", ""], ["Schoppe", "Owen", ""], ["Woodward", "David", ""], ["Rohde", "S\u00f6nke", ""]]}, {"id": "2012.08939", "submitter": "Divya Kothandaraman", "authors": "Divya Kothandaraman, Rohan Chandra, Dinesh Manocha", "title": "SS-SFDA : Self-Supervised Source-Free Domain Adaptation for Road\n  Segmentation in Hazardous Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for unsupervised road segmentation in adverse\nweather conditions such as rain or fog. This includes a new algorithm for\nsource-free domain adaptation (SFDA) using self-supervised learning. Moreover,\nour approach uses several techniques to address various challenges in SFDA and\nimprove performance, including online generation of pseudo-labels and\nself-attention as well as use of curriculum learning, entropy minimization and\nmodel distillation. We have evaluated the performance on $6$ datasets\ncorresponding to real and synthetic adverse weather conditions. Our method\noutperforms all prior works on unsupervised road segmentation and SFDA by at\nleast 10.26%, and improves the training time by 18-180x. Moreover, our\nself-supervised algorithm exhibits similar accuracy performance in terms of\nmIOU score as compared to prior supervised methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 09:19:03 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 09:33:13 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Kothandaraman", "Divya", ""], ["Chandra", "Rohan", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2012.08950", "submitter": "Chang Liu", "authors": "Chang Liu, Runzhong Wang, Zetian Jiang, Junchi Yan", "title": "Deep Reinforcement Learning of Graph Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph matching (GM) under node and pairwise constraints has been a building\nblock in areas from combinatorial optimization, data mining to computer vision,\nfor effective structural representation and association. We present a\nreinforcement learning solver for GM i.e. RGM that seeks the node\ncorrespondence between pairwise graphs, whereby the node embedding model on the\nassociation graph is learned to sequentially find the node-to-node matching.\nOur method differs from the previous deep graph matching model in the sense\nthat they are focused on the front-end feature extraction and affinity function\nlearning, while our method aims to learn the back-end decision making given the\naffinity objective function whether obtained by learning or not. Such an\nobjective function maximization setting naturally fits with the reinforcement\nlearning mechanism, of which the learning procedure is label-free. These\nfeatures make it more suitable for practical usage. Extensive experimental\nresults on both synthetic datasets, Willow Object dataset, Pascal VOC dataset,\nand QAPLIB showcase superior performance regarding both matching accuracy and\nefficiency. To our best knowledge, this is the first deep reinforcement\nlearning solver for graph matching.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 13:48:48 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 07:26:13 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Liu", "Chang", ""], ["Wang", "Runzhong", ""], ["Jiang", "Zetian", ""], ["Yan", "Junchi", ""]]}, {"id": "2012.08951", "submitter": "Nir Diamant", "authors": "Nir Diamant and Tal Mund and Ohad Menashe and Aviad Zabatani and Alex\n  M. Bronstein", "title": "SimuGAN: Unsupervised forward modeling and optimal design of a LIDAR\n  Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Energy-saving LIDAR camera for short distances estimates an object's distance\nusing temporally intensity-coded laser light pulses and calculates the maximum\ncorrelation with the back-scattered pulse.\n  Though on low power, the backs-scattered pulse is noisy and unstable, which\nleads to inaccurate and unreliable depth estimation.\n  To address this problem, we use GANs (Generative Adversarial Networks), which\nare two neural networks that can learn complicated class distributions through\nan adversarial process. We learn the LIDAR camera's hidden properties and\nbehavior, creating a novel, fully unsupervised forward model that simulates the\ncamera. Then, we use the model's differentiability to explore the camera\nparameter space and optimize those parameters in terms of depth, accuracy, and\nstability. To achieve this goal, we also propose a new custom loss function\ndesignated to the back-scattered code distribution's weaknesses and its\ncircular behavior. The results are demonstrated on both synthetic and real\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 13:52:10 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Diamant", "Nir", ""], ["Mund", "Tal", ""], ["Menashe", "Ohad", ""], ["Zabatani", "Aviad", ""], ["Bronstein", "Alex M.", ""]]}, {"id": "2012.08976", "submitter": "Dongxu Wei", "authors": "Dongxu Wei, Xiaowei Xu, Haibin Shen, Kejie Huang", "title": "C2F-FWN: Coarse-to-Fine Flow Warping Network for Spatial-Temporal\n  Consistent Motion Transfer", "comments": "This work is accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human video motion transfer (HVMT) aims to synthesize videos that one person\nimitates other persons' actions. Although existing GAN-based HVMT methods have\nachieved great success, they either fail to preserve appearance details due to\nthe loss of spatial consistency between synthesized and exemplary images, or\ngenerate incoherent video results due to the lack of temporal consistency among\nvideo frames. In this paper, we propose Coarse-to-Fine Flow Warping Network\n(C2F-FWN) for spatial-temporal consistent HVMT. Particularly, C2F-FWN utilizes\ncoarse-to-fine flow warping and Layout-Constrained Deformable Convolution\n(LC-DConv) to improve spatial consistency, and employs Flow Temporal\nConsistency (FTC) Loss to enhance temporal consistency. In addition, provided\nwith multi-source appearance inputs, C2F-FWN can support appearance attribute\nediting with great flexibility and efficiency. Besides public datasets, we also\ncollected a large-scale HVMT dataset named SoloDance for evaluation. Extensive\nexperiments conducted on our SoloDance dataset and the iPER dataset show that\nour approach outperforms state-of-art HVMT methods in terms of both spatial and\ntemporal consistency. Source code and the SoloDance dataset are available at\nhttps://github.com/wswdx/C2F-FWN.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 14:11:13 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Wei", "Dongxu", ""], ["Xu", "Xiaowei", ""], ["Shen", "Haibin", ""], ["Huang", "Kejie", ""]]}, {"id": "2012.09004", "submitter": "Zhengyan Tong", "authors": "Zhengyan Tong, Xuanhong Chen, Bingbing Ni, Xiaohang Wang", "title": "Sketch Generation with Drawing Process Guided by Vector Flow and\n  Grayscale", "comments": "This paper has been accepted for presentation at the Thirty-Fifth\n  AAAI Conference on Artificial Intelligence (AAAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel image-to-pencil translation method that could not only\ngenerate high-quality pencil sketches but also offer the drawing process.\nExisting pencil sketch algorithms are based on texture rendering rather than\nthe direct imitation of strokes, making them unable to show the drawing process\nbut only a final result. To address this challenge, we first establish a pencil\nstroke imitation mechanism. Next, we develop a framework with three branches to\nguide stroke drawing: the first branch guides the direction of the strokes, the\nsecond branch determines the shade of the strokes, and the third branch\nenhances the details further. Under this framework's guidance, we can produce a\npencil sketch by drawing one stroke every time. Our method is fully\ninterpretable. Comparison with existing pencil drawing algorithms shows that\nour method is superior to others in terms of texture quality, style, and user\nevaluation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 15:02:53 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Tong", "Zhengyan", ""], ["Chen", "Xuanhong", ""], ["Ni", "Bingbing", ""], ["Wang", "Xiaohang", ""]]}, {"id": "2012.09014", "submitter": "Jiahua Dong", "authors": "Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma and Lichen Wang", "title": "I3DOL: Incremental 3D Object Learning without Catastrophic Forgetting", "comments": "Accepted by Association for the Advancement of Artificial\n  Intelligence 2021 (AAAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object classification has attracted appealing attentions in academic\nresearches and industrial applications. However, most existing methods need to\naccess the training data of past 3D object classes when facing the common\nreal-world scenario: new classes of 3D objects arrive in a sequence. Moreover,\nthe performance of advanced approaches degrades dramatically for past learned\nclasses (i.e., catastrophic forgetting), due to the irregular and redundant\ngeometric structures of 3D point cloud data. To address these challenges, we\npropose a new Incremental 3D Object Learning (i.e., I3DOL) model, which is the\nfirst exploration to learn new classes of 3D object continually. Specifically,\nan adaptive-geometric centroid module is designed to construct discriminative\nlocal geometric structures, which can better characterize the irregular point\ncloud representation for 3D object. Afterwards, to prevent the catastrophic\nforgetting brought by redundant geometric information, a geometric-aware\nattention mechanism is developed to quantify the contributions of local\ngeometric structures, and explore unique 3D geometric characteristics with high\ncontributions for classes incremental learning. Meanwhile, a score fairness\ncompensation strategy is proposed to further alleviate the catastrophic\nforgetting caused by unbalanced data between past and new classes of 3D object,\nby compensating biased prediction for new classes in the validation phase.\nExperiments on 3D representative datasets validate the superiority of our I3DOL\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 15:17:51 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Dong", "Jiahua", ""], ["Cong", "Yang", ""], ["Sun", "Gan", ""], ["Ma", "Bingtao", ""], ["Wang", "Lichen", ""]]}, {"id": "2012.09020", "submitter": "Qing Wan", "authors": "Qing Wan, Yoonsuck Choe", "title": "AdjointBackMap: Reconstructing Effective Decision Hypersurfaces from CNN\n  Layers Using Adjoint Operators", "comments": "23 pages, 16 figures, 145MB. It may take some time to load", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are several effective methods in explaining the inner workings of\nconvolutional neural networks (CNNs). However, in general, finding the inverse\nof the function performed by CNNs as a whole is an ill-posed problem. In this\npaper, we propose a method based on adjoint operators to reconstruct, given an\narbitrary unit in the CNN (except for the first convolutional layer), its\neffective hypersurface in the input space that replicates that unit's decision\nsurface conditioned on a particular input image. Our results show that the\nhypersurface reconstructed this way, when multiplied by the original input\nimage, would give nearly the exact output value of that unit. We find that the\nCNN unit's decision surface is largely conditioned on the input, and this may\nexplain why adversarial inputs can effectively deceive CNNs.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 15:35:47 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 02:13:37 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Wan", "Qing", ""], ["Choe", "Yoonsuck", ""]]}, {"id": "2012.09027", "submitter": "Radu Horaud P", "authors": "Miles Hansard and Radu Horaud", "title": "A Differential Model of the Complex Cell", "comments": null, "journal-ref": "Neural Computation, 23(9), 2011", "doi": "10.1162/NECO_a_00163", "report-no": null, "categories": "q-bio.NC cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The receptive fields of simple cells in the visual cortex can be understood\nas linear filters. These filters can be modelled by Gabor functions, or by\nGaussian derivatives. Gabor functions can also be combined in an `energy model'\nof the complex cell response. This paper proposes an alternative model of the\ncomplex cell, based on Gaussian derivatives. It is most important to account\nfor the insensitivity of the complex response to small shifts of the image. The\nnew model uses a linear combination of the first few derivative filters, at a\nsingle position, to approximate the first derivative filter, at a series of\nadjacent positions. The maximum response, over all positions, gives a signal\nthat is insensitive to small shifts of the image. This model, unlike previous\napproaches, is based on the scale space theory of visual processing. In\nparticular, the complex cell is built from filters that respond to the \\twod\\\ndifferential structure of the image. The computational aspects of the new model\nare studied in one and two dimensions, using the steerability of the Gaussian\nderivatives. The response of the model to basic images, such as edges and\ngratings, is derived formally. The response to natural images is also\nevaluated, using statistical measures of shift insensitivity. The relevance of\nthe new model to the cortical image representation is discussed.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 10:23:23 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Hansard", "Miles", ""], ["Horaud", "Radu", ""]]}, {"id": "2012.09030", "submitter": "Nikola Popovic", "authors": "Nikola Popovic, Danda Pani Paudel, Thomas Probst, Guolei Sun, Luc Van\n  Gool", "title": "CompositeTasking: Understanding Images by Spatial Composition of Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define the concept of CompositeTasking as the fusion of multiple,\nspatially distributed tasks, for various aspects of image understanding.\nLearning to perform spatially distributed tasks is motivated by the frequent\navailability of only sparse labels across tasks, and the desire for a compact\nmulti-tasking network. To facilitate CompositeTasking, we introduce a novel\ntask conditioning model -- a single encoder-decoder network that performs\nmultiple, spatially varying tasks at once. The proposed network takes an image\nand a set of pixel-wise dense task requests as inputs, and performs the\nrequested prediction task for each pixel. Moreover, we also learn the\ncomposition of tasks that needs to be performed according to some\nCompositeTasking rules, which includes the decision of where to apply which\ntask. It not only offers us a compact network for multi-tasking, but also\nallows for task-editing. Another strength of the proposed method is\ndemonstrated by only having to supply sparse supervision per task. The obtained\nresults are on par with our baselines that use dense supervision and a\nmulti-headed multi-tasking design. The source code will be made publicly\navailable at www.github.com/nikola3794/composite-tasking.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 15:47:02 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 19:51:39 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Popovic", "Nikola", ""], ["Paudel", "Danda Pani", ""], ["Probst", "Thomas", ""], ["Sun", "Guolei", ""], ["Van Gool", "Luc", ""]]}, {"id": "2012.09036", "submitter": "Peihao Zhu", "authors": "Peihao Zhu, Rameen Abdal, Yipeng Qin, John Femiani, Peter Wonka", "title": "Improved StyleGAN Embedding: Where are the Good Latents?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  StyleGAN is able to produce photorealistic images that are almost\nindistinguishable from real ones. The reverse problem of finding an embedding\nfor a given image poses a challenge. Embeddings that reconstruct an image well\nare not always robust to editing operations. In this paper, we address the\nproblem of finding an embedding that both reconstructs images and also supports\nimage editing tasks. First, we introduce a new normalized space to analyze the\ndiversity and the quality of the reconstructed latent codes. This space can\nhelp answer the question of where good latent codes are located in latent\nspace. Second, we propose an improved embedding algorithm using a novel\nregularization method based on our analysis. Finally, we analyze the quality of\ndifferent embedding algorithms. We compare our results with the current\nstate-of-the-art methods and achieve a better trade-off between reconstruction\nquality and editing quality.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 18:01:24 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 00:01:21 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Zhu", "Peihao", ""], ["Abdal", "Rameen", ""], ["Qin", "Yipeng", ""], ["Femiani", "John", ""], ["Wonka", "Peter", ""]]}, {"id": "2012.09054", "submitter": "Qingjie Liu", "authors": "Huanyu Zhou and Qingjie Liu and Yunhong Wang", "title": "PGMAN: An Unsupervised Generative Multi-adversarial Network for\n  Pan-sharpening", "comments": "Code is available <https://github.com/zhysora/PGMAN>", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pan-sharpening aims at fusing a low-resolution (LR) multi-spectral (MS) image\nand a high-resolution (HR) panchromatic (PAN) image acquired by a satellite to\ngenerate an HR MS image. Many deep learning based methods have been developed\nin the past few years. However, since there are no intended HR MS images as\nreferences for learning, almost all of the existing methods down-sample the MS\nand PAN images and regard the original MS images as targets to form a\nsupervised setting for training. These methods may perform well on the\ndown-scaled images, however, they generalize poorly to the full-resolution\nimages. To conquer this problem, we design an unsupervised framework that is\nable to learn directly from the full-resolution images without any\npreprocessing. The model is built based on a novel generative multi-adversarial\nnetwork. We use a two-stream generator to extract the modality-specific\nfeatures from the PAN and MS images, respectively, and develop a\ndual-discriminator to preserve the spectral and spatial information of the\ninputs when performing fusion. Furthermore, a novel loss function is introduced\nto facilitate training under the unsupervised setting. Experiments and\ncomparisons with other state-of-the-art methods on GaoFen-2 and QuickBird\nimages demonstrate that the proposed method can obtain much better fusion\nresults on the full-resolution images.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 16:21:03 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 01:48:43 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Zhou", "Huanyu", ""], ["Liu", "Qingjie", ""], ["Wang", "Yunhong", ""]]}, {"id": "2012.09058", "submitter": "Massimiliano Mancini", "authors": "Massimiliano Mancini", "title": "Towards Recognizing New Semantic Concepts in New Visual Domains", "comments": "Ph.D. thesis. Sapienza University of Rome (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models heavily rely on large scale annotated datasets for\ntraining. Unfortunately, datasets cannot capture the infinite variability of\nthe real world, thus neural networks are inherently limited by the restricted\nvisual and semantic information contained in their training set. In this\nthesis, we argue that it is crucial to design deep architectures that can\noperate in previously unseen visual domains and recognize novel semantic\nconcepts. In the first part of the thesis, we describe different solutions to\nenable deep models to generalize to new visual domains, by transferring\nknowledge from a labeled source domain(s) to a domain (target) where no labeled\ndata are available. We will show how variants of batch-normalization (BN) can\nbe applied to different scenarios, from domain adaptation when source and\ntarget are mixtures of multiple latent domains, to domain generalization,\ncontinuous domain adaptation, and predictive domain adaptation, where\ninformation about the target domain is available only in the form of metadata.\nIn the second part of the thesis, we show how to extend the knowledge of a\npretrained deep model to new semantic concepts, without access to the original\ntraining set. We address the scenarios of sequential multi-task learning, using\ntransformed task-specific binary masks, open-world recognition, with end-to-end\ntraining and enforced clustering, and incremental class learning in semantic\nsegmentation, where we highlight and address the problem of the semantic shift\nof the background class. In the final part, we tackle a more challenging\nproblem: given images of multiple domains and semantic categories (with their\nattributes), how to build a model that recognizes images of unseen concepts in\nunseen domains? We also propose an approach based on domain and semantic mixing\nof inputs and features, which is a first, promising step towards solving this\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 16:23:40 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Mancini", "Massimiliano", ""]]}, {"id": "2012.09070", "submitter": "Olivier Rukundo", "authors": "Olivier Rukundo", "title": "Evaluation of deep learning-based myocardial infarction quantification\n  using Segment CMR software", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, the author evaluates the preliminary work related to\nautomating the quantification of the size of the myocardial infarction (MI)\nusing deep learning in Segment cardiovascular magnetic resonance (CMR)\nsoftware. Here, deep learning is used to automate the segmentation of\nmyocardial boundaries before triggering the automatic quantification of the\nsize of the MI using the expectation-maximization, weighted intensity, a priori\ninformation (EWA) algorithm incorporated in the Segment CMR software.\nExperimental evaluation of the size of the MI shows that more than 50 %\n(average infarct scar volume), 75% (average infarct scar percentage), and 65 %\n(average microvascular obstruction percentage) of the network-based results are\napproximately very close to the expert delineation-based results. Also, in an\nexperiment involving the visualization of myocardial and infarct contours, in\nall images of the selected stack, the network and expert-based results tie in\nterms of the number of infarcted and contoured images.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 16:49:50 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Rukundo", "Olivier", ""]]}, {"id": "2012.09071", "submitter": "Hao Chen", "authors": "Hao Chen, Yaohui Wang, Benoit Lagadec, Antitza Dantcheva, Francois\n  Bremond", "title": "Joint Generative and Contrastive Learning for Unsupervised Person\n  Re-identification", "comments": "CVPR 2021. Source code: https://github.com/chenhao2345/GCL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent self-supervised contrastive learning provides an effective approach\nfor unsupervised person re-identification (ReID) by learning invariance from\ndifferent views (transformed versions) of an input. In this paper, we\nincorporate a Generative Adversarial Network (GAN) and a contrastive learning\nmodule into one joint training framework. While the GAN provides online data\naugmentation for contrastive learning, the contrastive module learns\nview-invariant features for generation. In this context, we propose a\nmesh-based view generator. Specifically, mesh projections serve as references\ntowards generating novel views of a person. In addition, we propose a\nview-invariant loss to facilitate contrastive learning between original and\ngenerated views. Deviating from previous GAN-based unsupervised ReID methods\ninvolving domain adaptation, we do not rely on a labeled source dataset, which\nmakes our method more flexible. Extensive experimental results show that our\nmethod significantly outperforms state-of-the-art methods under both, fully\nunsupervised and unsupervised domain adaptive settings on several large scale\nReID datsets.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 16:49:57 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 10:52:24 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Chen", "Hao", ""], ["Wang", "Yaohui", ""], ["Lagadec", "Benoit", ""], ["Dantcheva", "Antitza", ""], ["Bremond", "Francois", ""]]}, {"id": "2012.09093", "submitter": "Huolin Xin", "authors": "Ruoqian Lin, Rui Zhang, Chunyang Wang, Xiao-Qing Yang, Huolin L. Xin", "title": "TEMImageNet Training Library and AtomSegNet Deep-Learning Models for\n  High-Precision Atom Segmentation, Localization, Denoising, and\n  Super-Resolution Processing of Atomic-Resolution Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Atom segmentation and localization, noise reduction and deblurring of\natomic-resolution scanning transmission electron microscopy (STEM) images with\nhigh precision and robustness is a challenging task. Although several\nconventional algorithms, such has thresholding, edge detection and clustering,\ncan achieve reasonable performance in some predefined sceneries, they tend to\nfail when interferences from the background are strong and unpredictable.\nParticularly, for atomic-resolution STEM images, so far there is no\nwell-established algorithm that is robust enough to segment or detect all\natomic columns when there is large thickness variation in a recorded image.\nHerein, we report the development of a training library and a deep learning\nmethod that can perform robust and precise atom segmentation, localization,\ndenoising, and super-resolution processing of experimental images. Despite\nusing simulated images as training datasets, the deep-learning model can\nself-adapt to experimental STEM images and shows outstanding performance in\natom detection and localization in challenging contrast conditions and the\nprecision consistently outperforms the state-of-the-art two-dimensional\nGaussian fit method. Taking a step further, we have deployed our deep-learning\nmodels to a desktop app with a graphical user interface and the app is free and\nopen-source. We have also built a TEM ImageNet project website for easy\nbrowsing and downloading of the training data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 17:21:23 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 06:14:18 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Lin", "Ruoqian", ""], ["Zhang", "Rui", ""], ["Wang", "Chunyang", ""], ["Yang", "Xiao-Qing", ""], ["Xin", "Huolin L.", ""]]}, {"id": "2012.09131", "submitter": "Amir M. Rahmani", "authors": "Amir M. Rahmani, Jocelyn Lai, Salar Jafarlou, Asal Yunusova, Alex. P.\n  Rivera, Sina Labbaf, Sirui Hu, Arman Anzanpour, Nikil Dutt, Ramesh Jain,\n  Jessica L. Borelli", "title": "Personal Mental Health Navigator: Harnessing the Power of Data, Personal\n  Models, and Health Cybernetics to Promote Psychological Well-being", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.CY cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, the regime of mental healthcare has followed an episodic\npsychotherapy model wherein patients seek care from a provider through a\nprescribed treatment plan developed over multiple provider visits. Recent\nadvances in wearable and mobile technology have generated increased interest in\ndigital mental healthcare that enables individuals to address episodic mental\nhealth symptoms. However, these efforts are typically reactive and\nsymptom-focused and do not provide comprehensive, wrap-around, customized\ntreatments that capture an individual's holistic mental health model as it\nunfolds over time. Recognizing that each individual is unique, we present the\nnotion of Personalized Mental Health Navigation (MHN): a therapist-in-the-loop,\ncybernetic goal-based system that deploys a continuous cyclic loop of\nmeasurement, estimation, guidance, to steer the individual's mental health\nstate towards a healthy zone. We outline the major components of MHN that is\npremised on the development of an individual's personal mental health state,\nholistically represented by a high-dimensional cover of multiple knowledge\nlayers such as emotion, biological patterns, sociology, behavior, and\ncognition. We demonstrate the feasibility of the personalized MHN approach via\na 12-month pilot case study for holistic stress management in college students\nand highlight an instance of a therapist-in-the-loop intervention using MHN for\nmonitoring, estimating, and proactively addressing moderately severe depression\nover a sustained period of time. We believe MHN paves the way to transform\nmental healthcare from the current passive, episodic, reactive process (where\nindividuals seek help to address symptoms that have already manifested) to a\ncontinuous and navigational paradigm that leverages a personalized model of the\nindividual, promising to deliver timely interventions to individuals in a\nholistic manner.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 18:34:09 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Rahmani", "Amir M.", ""], ["Lai", "Jocelyn", ""], ["Jafarlou", "Salar", ""], ["Yunusova", "Asal", ""], ["Rivera", "Alex. P.", ""], ["Labbaf", "Sina", ""], ["Hu", "Sirui", ""], ["Anzanpour", "Arman", ""], ["Dutt", "Nikil", ""], ["Jain", "Ramesh", ""], ["Borelli", "Jessica L.", ""]]}, {"id": "2012.09154", "submitter": "Huaju Liang", "authors": "Huaju Liang, Hongyang Bai and Tong Zhou", "title": "Exploration of Whether Skylight Polarization Patterns Contain\n  Three-dimensional Attitude Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our previous work has demonstrated that Rayleigh model, which is widely used\nin polarized skylight navigation to describe skylight polarization patterns,\ndoes not contain three-dimensional (3D) attitude information [1]. However, it\nis still necessary to further explore whether the skylight polarization\npatterns contain 3D attitude information. So, in this paper, a social spider\noptimization (SSO) method is proposed to estimate three Euler angles, which\nconsiders the difference of each pixel among polarization images based on\ntemplate matching (TM) to make full use of the captured polarization\ninformation. In addition, to explore this problem, we not only use angle of\npolarization (AOP) and degree of polarization (DOP) information, but also the\nlight intensity (LI) information. So, a sky model is established, which\ncombines Berry model and Hosek model to fully describe AOP, DOP, and LI\ninformation in the sky, and considers the influence of four neutral points,\nground albedo, atmospheric turbidity, and wavelength. The results of simulation\nshow that the SSO algorithm can estimate 3D attitude and the established sky\nmodel contains 3D attitude information. However, when there are measurement\nnoise or model error, the accuracy of 3D attitude estimation drops\nsignificantly. Especially in field experiment, it is very difficult to estimate\n3D attitude. Finally, the results are discussed in detail.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 12:10:29 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Liang", "Huaju", ""], ["Bai", "Hongyang", ""], ["Zhou", "Tong", ""]]}, {"id": "2012.09159", "submitter": "Zhiqin Chen", "authors": "Zhiqin Chen, Vladimir G. Kim, Matthew Fisher, Noam Aigerman, Hao\n  Zhang, Siddhartha Chaudhuri", "title": "DECOR-GAN: 3D Shape Detailization by Conditional Refinement", "comments": "CVPR 2021 (oral). Code: https://github.com/czq142857/DECOR-GAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep generative network for 3D shape detailization, akin to\nstylization with the style being geometric details. We address the challenge of\ncreating large varieties of high-resolution and detailed 3D geometry from a\nsmall set of exemplars by treating the problem as that of geometric detail\ntransfer. Given a low-resolution coarse voxel shape, our network refines it,\nvia voxel upsampling, into a higher-resolution shape enriched with geometric\ndetails. The output shape preserves the overall structure (or content) of the\ninput, while its detail generation is conditioned on an input \"style code\"\ncorresponding to a detailed exemplar. Our 3D detailization via conditional\nrefinement is realized by a generative adversarial network, coined DECOR-GAN.\nThe network utilizes a 3D CNN generator for upsampling coarse voxels and a 3D\nPatchGAN discriminator to enforce local patches of the generated model to be\nsimilar to those in the training detailed shapes. During testing, a style code\nis fed into the generator to condition the refinement. We demonstrate that our\nmethod can refine a coarse shape into a variety of detailed shapes with\ndifferent styles. The generated results are evaluated in terms of content\npreservation, plausibility, and diversity. Comprehensive ablation studies are\nconducted to validate our network designs. Code is available at\nhttps://github.com/czq142857/DECOR-GAN.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 18:52:10 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 03:04:08 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chen", "Zhiqin", ""], ["Kim", "Vladimir G.", ""], ["Fisher", "Matthew", ""], ["Aigerman", "Noam", ""], ["Zhang", "Hao", ""], ["Chaudhuri", "Siddhartha", ""]]}, {"id": "2012.09161", "submitter": "Yinbo Chen", "authors": "Yinbo Chen, Sifei Liu, Xiaolong Wang", "title": "Learning Continuous Image Representation with Local Implicit Image\n  Function", "comments": "CVPR 2021 (oral). Project page with videos and code:\n  https://yinboc.github.io/liif/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to represent an image? While the visual world is presented in a\ncontinuous manner, machines store and see the images in a discrete way with 2D\narrays of pixels. In this paper, we seek to learn a continuous representation\nfor images. Inspired by the recent progress in 3D reconstruction with implicit\nneural representation, we propose Local Implicit Image Function (LIIF), which\ntakes an image coordinate and the 2D deep features around the coordinate as\ninputs, predicts the RGB value at a given coordinate as an output. Since the\ncoordinates are continuous, LIIF can be presented in arbitrary resolution. To\ngenerate the continuous representation for images, we train an encoder with\nLIIF representation via a self-supervised task with super-resolution. The\nlearned continuous representation can be presented in arbitrary resolution even\nextrapolate to x30 higher resolution, where the training tasks are not\nprovided. We further show that LIIF representation builds a bridge between\ndiscrete and continuous representation in 2D, it naturally supports the\nlearning tasks with size-varied image ground-truths and significantly\noutperforms the method with resizing the ground-truths.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 18:56:50 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 13:33:26 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Chen", "Yinbo", ""], ["Liu", "Sifei", ""], ["Wang", "Xiaolong", ""]]}, {"id": "2012.09164", "submitter": "Hengshuang Zhao", "authors": "Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, Vladlen Koltun", "title": "Point Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-attention networks have revolutionized natural language processing and\nare making impressive strides in image analysis tasks such as image\nclassification and object detection. Inspired by this success, we investigate\nthe application of self-attention networks to 3D point cloud processing. We\ndesign self-attention layers for point clouds and use these to construct\nself-attention networks for tasks such as semantic scene segmentation, object\npart segmentation, and object classification. Our Point Transformer design\nimproves upon prior work across domains and tasks. For example, on the\nchallenging S3DIS dataset for large-scale semantic scene segmentation, the\nPoint Transformer attains an mIoU of 70.4% on Area 5, outperforming the\nstrongest prior model by 3.3 absolute percentage points and crossing the 70%\nmIoU threshold for the first time.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 18:58:56 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Zhao", "Hengshuang", ""], ["Jiang", "Li", ""], ["Jia", "Jiaya", ""], ["Torr", "Philip", ""], ["Koltun", "Vladlen", ""]]}, {"id": "2012.09165", "submitter": "Ji Hou", "authors": "Ji Hou, Benjamin Graham, Matthias Nie{\\ss}ner, Saining Xie", "title": "Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene\n  Contexts", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid progress in 3D scene understanding has come with growing demand for\ndata; however, collecting and annotating 3D scenes (e.g. point clouds) are\nnotoriously hard. For example, the number of scenes (e.g. indoor rooms) that\ncan be accessed and scanned might be limited; even given sufficient data,\nacquiring 3D labels (e.g. instance masks) requires intensive human labor. In\nthis paper, we explore data-efficient learning for 3D point cloud. As a first\nstep towards this direction, we propose Contrastive Scene Contexts, a 3D\npre-training method that makes use of both point-level correspondences and\nspatial contexts in a scene. Our method achieves state-of-the-art results on a\nsuite of benchmarks where training data or labels are scarce. Our study reveals\nthat exhaustive labelling of 3D point clouds might be unnecessary; and\nremarkably, on ScanNet, even using 0.1% of point labels, we still achieve 89%\n(instance segmentation) and 96% (semantic segmentation) of the baseline\nperformance that uses full annotations.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 18:59:26 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 08:49:47 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 18:33:54 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Hou", "Ji", ""], ["Graham", "Benjamin", ""], ["Nie\u00dfner", "Matthias", ""], ["Xie", "Saining", ""]]}, {"id": "2012.09216", "submitter": "Te-Lin Wu", "authors": "Te-Lin Wu, Shikhar Singh, Sayan Paul, Gully Burns, Nanyun Peng", "title": "MELINDA: A Multimodal Dataset for Biomedical Experiment Method\n  Classification", "comments": "In The Thirty-Fifth AAAI Conference on Artificial Intelligence\n  (AAAI-21), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new dataset, MELINDA, for Multimodal biomEdicaL experImeNt\nmethoD clAssification. The dataset is collected in a fully automated distant\nsupervision manner, where the labels are obtained from an existing curated\ndatabase, and the actual contents are extracted from papers associated with\neach of the records in the database. We benchmark various state-of-the-art NLP\nand computer vision models, including unimodal models which only take either\ncaption texts or images as inputs, and multimodal models. Extensive experiments\nand analysis show that multimodal models, despite outperforming unimodal ones,\nstill need improvements especially on a less-supervised way of grounding visual\nconcepts with languages, and better transferability to low resource domains. We\nrelease our dataset and the benchmarks to facilitate future research in\nmultimodal learning, especially to motivate targeted improvements for\napplications in scientific domains.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 19:11:36 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Wu", "Te-Lin", ""], ["Singh", "Shikhar", ""], ["Paul", "Sayan", ""], ["Burns", "Gully", ""], ["Peng", "Nanyun", ""]]}, {"id": "2012.09235", "submitter": "Mehdi Bahri", "authors": "Mehdi Bahri, Eimear O' Sullivan, Shunwang Gong, Feng Liu, Xiaoming\n  Liu, Michael M. Bronstein, Stefanos Zafeiriou", "title": "Shape My Face: Registering 3D Face Scans by Surface-to-Surface\n  Translation", "comments": "In review with International Journal of Computer Vision (IJCV) -\n  Revision 1 (minor revision)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Standard registration algorithms need to be independently applied to each\nsurface to register, following careful pre-processing and hand-tuning.\nRecently, learning-based approaches have emerged that reduce the registration\nof new scans to running inference with a previously-trained model. In this\npaper, we cast the registration task as a surface-to-surface translation\nproblem, and design a model to reliably capture the latent geometric\ninformation directly from raw 3D face scans. We introduce Shape-My-Face (SMF),\na powerful encoder-decoder architecture based on an improved point cloud\nencoder, a novel visual attention mechanism, graph convolutional decoders with\nskip connections, and a specialized mouth model that we smoothly integrate with\nthe mesh convolutions. Compared to the previous state-of-the-art learning\nalgorithms for non-rigid registration of face scans, SMF only requires the raw\ndata to be rigidly aligned (with scaling) with a pre-defined face template.\nAdditionally, our model provides topologically-sound meshes with minimal\nsupervision, offers faster training time, has orders of magnitude fewer\ntrainable parameters, is more robust to noise, and can generalize to previously\nunseen datasets. We extensively evaluate the quality of our registrations on\ndiverse data. We demonstrate the robustness and generalizability of our model\nwith in-the-wild face scans across different modalities, sensor types, and\nresolutions. Finally, we show that, by learning to register scans, SMF produces\na hybrid linear and non-linear morphable model. Manipulation of the latent\nspace of SMF allows for shape generation, and morphing applications such as\nexpression transfer in-the-wild. We train SMF on a dataset of human faces\ncomprising 9 large-scale databases on commodity hardware.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 20:02:36 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 15:25:41 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Bahri", "Mehdi", ""], ["Sullivan", "Eimear O'", ""], ["Gong", "Shunwang", ""], ["Liu", "Feng", ""], ["Liu", "Xiaoming", ""], ["Bronstein", "Michael M.", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2012.09237", "submitter": "Biagio Brattoli", "authors": "Biagio Brattoli, Uta Buechler, Michael Dorkenwald, Philipp Reiser,\n  Linard Filli, Fritjof Helmchen, Anna-Sophia Wahl, Bjoern Ommer", "title": "Unsupervised Behaviour Analysis and Magnification (uBAM) using Deep\n  Learning", "comments": "Published in Nature Machine Intelligence (2021),\n  https://rdcu.be/ch6pL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motor behaviour analysis is essential to biomedical research and clinical\ndiagnostics as it provides a non-invasive strategy for identifying motor\nimpairment and its change caused by interventions. State-of-the-art\ninstrumented movement analysis is time- and cost-intensive, since it requires\nplacing physical or virtual markers. Besides the effort required for marking\nkeypoints or annotations necessary for training or finetuning a detector, users\nneed to know the interesting behaviour beforehand to provide meaningful\nkeypoints. We introduce unsupervised behaviour analysis and magnification\n(uBAM), an automatic deep learning algorithm for analysing behaviour by\ndiscovering and magnifying deviations. A central aspect is unsupervised\nlearning of posture and behaviour representations to enable an objective\ncomparison of movement. Besides discovering and quantifying deviations in\nbehaviour, we also propose a generative model for visually magnifying subtle\nbehaviour differences directly in a video without requiring a detour via\nkeypoints or annotations. Essential for this magnification of deviations even\nacross different individuals is a disentangling of appearance and behaviour.\nEvaluations on rodents and human patients with neurological diseases\ndemonstrate the wide applicability of our approach. Moreover, combining\noptogenetic stimulation with our unsupervised behaviour analysis shows its\nsuitability as a non-invasive diagnostic tool correlating function to brain\nplasticity.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 20:07:36 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 13:07:54 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 13:52:14 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Brattoli", "Biagio", ""], ["Buechler", "Uta", ""], ["Dorkenwald", "Michael", ""], ["Reiser", "Philipp", ""], ["Filli", "Linard", ""], ["Helmchen", "Fritjof", ""], ["Wahl", "Anna-Sophia", ""], ["Ommer", "Bjoern", ""]]}, {"id": "2012.09242", "submitter": "Ran Cheng", "authors": "Ran Cheng, Christopher Agia, Yuan Ren, Xinhai Li, Liu Bingbing", "title": "S3CNet: A Sparse Semantic Scene Completion Network for LiDAR Point\n  Clouds", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the increasing reliance of self-driving and similar robotic systems on\nrobust 3D vision, the processing of LiDAR scans with deep convolutional neural\nnetworks has become a trend in academia and industry alike. Prior attempts on\nthe challenging Semantic Scene Completion task - which entails the inference of\ndense 3D structure and associated semantic labels from \"sparse\" representations\n- have been, to a degree, successful in small indoor scenes when provided with\ndense point clouds or dense depth maps often fused with semantic segmentation\nmaps from RGB images. However, the performance of these systems drop\ndrastically when applied to large outdoor scenes characterized by dynamic and\nexponentially sparser conditions. Likewise, processing of the entire sparse\nvolume becomes infeasible due to memory limitations and workarounds introduce\ncomputational inefficiency as practitioners are forced to divide the overall\nvolume into multiple equal segments and infer on each individually, rendering\nreal-time performance impossible. In this work, we formulate a method that\nsubsumes the sparsity of large-scale environments and present S3CNet, a sparse\nconvolution based neural network that predicts the semantically completed scene\nfrom a single, unified LiDAR point cloud. We show that our proposed method\noutperforms all counterparts on the 3D task, achieving state-of-the art results\non the SemanticKITTI benchmark. Furthermore, we propose a 2D variant of S3CNet\nwith a multi-view fusion strategy to complement our 3D network, providing\nrobustness to occlusions and extreme sparsity in distant regions. We conduct\nexperiments for the 2D semantic scene completion task and compare the results\nof our sparse 2D network against several leading LiDAR segmentation models\nadapted for bird's eye view segmentation on two open-source datasets.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 20:14:41 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Cheng", "Ran", ""], ["Agia", "Christopher", ""], ["Ren", "Yuan", ""], ["Li", "Xinhai", ""], ["Bingbing", "Liu", ""]]}, {"id": "2012.09243", "submitter": "Huan Wang", "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu", "title": "Neural Pruning via Growing Regularization", "comments": "Accepted by ICLR 2021", "journal-ref": "International Conference on Learning Representations (ICLR) 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization has long been utilized to learn sparsity in deep neural\nnetwork pruning. However, its role is mainly explored in the small penalty\nstrength regime. In this work, we extend its application to a new scenario\nwhere the regularization grows large gradually to tackle two central problems\nof pruning: pruning schedule and weight importance scoring. (1) The former\ntopic is newly brought up in this work, which we find critical to the pruning\nperformance while receives little research attention. Specifically, we propose\nan L2 regularization variant with rising penalty factors and show it can bring\nsignificant accuracy gains compared with its one-shot counterpart, even when\nthe same weights are removed. (2) The growing penalty scheme also brings us an\napproach to exploit the Hessian information for more accurate pruning without\nknowing their specific values, thus not bothered by the common Hessian\napproximation problems. Empirically, the proposed algorithms are easy to\nimplement and scalable to large datasets and networks in both structured and\nunstructured pruning. Their effectiveness is demonstrated with modern deep\nneural networks on the CIFAR and ImageNet datasets, achieving competitive\nresults compared to many state-of-the-art algorithms. Our code and trained\nmodels are publicly available at\nhttps://github.com/mingsuntse/regularization-pruning.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 20:16:28 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 19:37:45 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Wang", "Huan", ""], ["Qin", "Can", ""], ["Zhang", "Yulun", ""], ["Fu", "Yun", ""]]}, {"id": "2012.09250", "submitter": "Abdullah Sarhan", "authors": "Abdullah Sarhan, Jon Rokne, Reda Alhajj, and Andrew Crichton", "title": "Transfer Learning Through Weighted Loss Function and Group Normalization\n  for Vessel Segmentation from Retinal Images", "comments": "Accepted by ICPR. arXiv admin note: text overlap with\n  arXiv:2010.00583", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vascular structure of blood vessels is important in diagnosing retinal\nconditions such as glaucoma and diabetic retinopathy. Accurate segmentation of\nthese vessels can help in detecting retinal objects such as the optic disc and\noptic cup and hence determine if there are damages to these areas. Moreover,\nthe structure of the vessels can help in diagnosing glaucoma. The rapid\ndevelopment of digital imaging and computer-vision techniques has increased the\npotential for developing approaches for segmenting retinal vessels. In this\npaper, we propose an approach for segmenting retinal vessels that uses deep\nlearning along with transfer learning. We adapted the U-Net structure to use a\ncustomized InceptionV3 as the encoder and used multiple skip connections to\nform the decoder. Moreover, we used a weighted loss function to handle the\nissue of class imbalance in retinal images. Furthermore, we contributed a new\ndataset to this field. We tested our approach on six publicly available\ndatasets and a newly created dataset. We achieved an average accuracy of 95.60%\nand a Dice coefficient of 80.98%. The results obtained from comprehensive\nexperiments demonstrate the robustness of our approach to the segmentation of\nblood vessels in retinal images obtained from different sources. Our approach\nresults in greater segmentation accuracy than other approaches.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 20:34:48 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Sarhan", "Abdullah", ""], ["Rokne", "Jon", ""], ["Alhajj", "Reda", ""], ["Crichton", "Andrew", ""]]}, {"id": "2012.09259", "submitter": "Ajinkya Tejankar", "authors": "Ajinkya Tejankar, Soroush Abbasi Koohpayegani, Vipin Pillai, Paolo\n  Favaro, and Hamed Pirsiavash", "title": "ISD: Self-Supervised Learning by Iterative Similarity Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, contrastive learning has achieved great results in self-supervised\nlearning, where the main idea is to push two augmentations of an image\n(positive pairs) closer compared to other random images (negative pairs). We\nargue that not all random images are equal. Hence, we introduce a self\nsupervised learning algorithm where we use a soft similarity for the negative\nimages rather than a binary distinction between positive and negative pairs. We\niteratively distill a slowly evolving teacher model to the student model by\ncapturing the similarity of a query image to some random images and\ntransferring that knowledge to the student. We argue that our method is less\nconstrained compared to recent contrastive learning methods, so it can learn\nbetter features. Specifically, our method should handle unbalanced and\nunlabeled data better than existing contrastive learning methods, because the\nrandomly chosen negative set might include many samples that are semantically\nsimilar to the query image. In this case, our method labels them as highly\nsimilar while standard contrastive methods label them as negative pairs. Our\nmethod achieves comparable results to the state-of-the-art models. We also show\nthat our method performs better in the settings where the unlabeled data is\nunbalanced. Our code is available here: https://github.com/UMBCvision/ISD.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 20:50:17 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 14:20:59 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Tejankar", "Ajinkya", ""], ["Koohpayegani", "Soroush Abbasi", ""], ["Pillai", "Vipin", ""], ["Favaro", "Paolo", ""], ["Pirsiavash", "Hamed", ""]]}, {"id": "2012.09267", "submitter": "Homayoun Valafar", "authors": "Homayoun Valafar, Faramarz Valafar", "title": "Reduction in the complexity of 1D 1H-NMR spectra by the use of Frequency\n  to Information Transformation", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of 1H-NMR spectra is often hindered by large variations that occur\nduring the collection of these spectra. Large solvent and standard peaks, base\nline drift and negative peaks (due to improper phasing) are among some of these\nvariations. Furthermore, some instrument dependent alterations, such as\nincorrect shimming, are also embedded in the recorded spectrum. The\nunpredictable nature of these alterations of the signal has rendered the\nautomated and instrument independent computer analysis of these spectra\nunreliable. In this paper, a novel method of extracting the information content\nof a signal (in this paper, frequency domain 1H-NMR spectrum), called the\nfrequency-information transformation (FIT), is presented and compared to a\npreviously used method (SPUTNIK). FIT can successfully extract the relevant\ninformation to a pattern matching task present in a signal, while discarding\nthe remainder of a signal by transforming a Fourier transformed signal into an\ninformation spectrum (IS). This technique exhibits the ability of decreasing\nthe inter-class correlation coefficients while increasing the intra-class\ncorrelation coefficients. Different spectra of the same molecule, in other\nwords, will resemble more to each other while the spectra of different\nmolecules will look more different from each other. This feature allows easier\nautomated identification and analysis of molecules based on their spectral\nsignatures using computer algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 21:08:35 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Valafar", "Homayoun", ""], ["Valafar", "Faramarz", ""]]}, {"id": "2012.09279", "submitter": "Hao Tang", "authors": "Hao Tang, Xingwei Liu, Kun Han, Shanlin Sun, Narisu Bai, Xuming Chen,\n  Huang Qian, Yong Liu, Xiaohui Xie", "title": "Spatial Context-Aware Self-Attention Model For Multi-Organ Segmentation", "comments": "Accepted WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-organ segmentation is one of most successful applications of deep\nlearning in medical image analysis. Deep convolutional neural nets (CNNs) have\nshown great promise in achieving clinically applicable image segmentation\nperformance on CT or MRI images. State-of-the-art CNN segmentation models apply\neither 2D or 3D convolutions on input images, with pros and cons associated\nwith each method: 2D convolution is fast, less memory-intensive but inadequate\nfor extracting 3D contextual information from volumetric images, while the\nopposite is true for 3D convolution. To fit a 3D CNN model on CT or MRI images\non commodity GPUs, one usually has to either downsample input images or use\ncropped local regions as inputs, which limits the utility of 3D models for\nmulti-organ segmentation. In this work, we propose a new framework for\ncombining 3D and 2D models, in which the segmentation is realized through\nhigh-resolution 2D convolutions, but guided by spatial contextual information\nextracted from a low-resolution 3D model. We implement a self-attention\nmechanism to control which 3D features should be used to guide 2D segmentation.\nOur model is light on memory usage but fully equipped to take 3D contextual\ninformation into account. Experiments on multiple organ segmentation datasets\ndemonstrate that by taking advantage of both 2D and 3D models, our method is\nconsistently outperforms existing 2D and 3D models in organ segmentation\naccuracy, while being able to directly take raw whole-volume image data as\ninputs.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 21:39:53 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Tang", "Hao", ""], ["Liu", "Xingwei", ""], ["Han", "Kun", ""], ["Sun", "Shanlin", ""], ["Bai", "Narisu", ""], ["Chen", "Xuming", ""], ["Qian", "Huang", ""], ["Liu", "Yong", ""], ["Xie", "Xiaohui", ""]]}, {"id": "2012.09284", "submitter": "Tushar Agarwal", "authors": "Tushar Agarwal, Nithin Sugavanam and Emre Ertin", "title": "Sparse Signal Models for Data Augmentation in Deep Learning ATR", "comments": "12 pages, 5 figures, to be submitted to IEEE Transactions on\n  Geoscience and Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic Target Recognition (ATR) algorithms classify a given Synthetic\nAperture Radar (SAR) image into one of the known target classes using a set of\ntraining images available for each class. Recently, learning methods have shown\nto achieve state-of-the-art classification accuracy if abundant training data\nis available, sampled uniformly over the classes, and their poses. In this\npaper, we consider the task of ATR with a limited set of training images. We\npropose a data augmentation approach to incorporate domain knowledge and\nimprove the generalization power of a data-intensive learning algorithm, such\nas a Convolutional neural network (CNN). The proposed data augmentation method\nemploys a limited persistence sparse modeling approach, capitalizing on\ncommonly observed characteristics of wide-angle synthetic aperture radar (SAR)\nimagery. Specifically, we exploit the sparsity of the scattering centers in the\nspatial domain and the smoothly-varying structure of the scattering\ncoefficients in the azimuthal domain to solve the ill-posed problem of\nover-parametrized model fitting. Using this estimated model, we synthesize new\nimages at poses and sub-pixel translations not available in the given data to\naugment CNN's training data. The experimental results show that for the\ntraining data starved region, the proposed method provides a significant gain\nin the resulting ATR algorithm's generalization performance.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 21:46:33 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Agarwal", "Tushar", ""], ["Sugavanam", "Nithin", ""], ["Ertin", "Emre", ""]]}, {"id": "2012.09289", "submitter": "Mauricio Delbracio", "authors": "Mauricio Delbracio, Hossein Talebi, Peyman Milanfar", "title": "Projected Distribution Loss for Image Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Features obtained from object recognition CNNs have been widely used for\nmeasuring perceptual similarities between images. Such differentiable metrics\ncan be used as perceptual learning losses to train image enhancement models.\nHowever, the choice of the distance function between input and target features\nmay have a consequential impact on the performance of the trained model. While\nusing the norm of the difference between extracted features leads to limited\nhallucination of details, measuring the distance between distributions of\nfeatures may generate more textures; yet also more unrealistic details and\nartifacts. In this paper, we demonstrate that aggregating 1D-Wasserstein\ndistances between CNN activations is more reliable than the existing\napproaches, and it can significantly improve the perceptual performance of\nenhancement models. More explicitly, we show that in imaging applications such\nas denoising, super-resolution, demosaicing, deblurring and JPEG artifact\nremoval, the proposed learning loss outperforms the current state-of-the-art on\nreference-based perceptual losses. This means that the proposed learning loss\ncan be plugged into different imaging frameworks and produce perceptually\nrealistic results.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 22:13:03 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 15:31:44 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Delbracio", "Mauricio", ""], ["Talebi", "Hossein", ""], ["Milanfar", "Peyman", ""]]}, {"id": "2012.09290", "submitter": "Bingchen Liu", "authors": "Bingchen Liu, Yizhe Zhu, Kunpeng Song, Ahmed Elgammal", "title": "Self-Supervised Sketch-to-Image Synthesis", "comments": "AAAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagining a colored realistic image from an arbitrarily drawn sketch is one\nof the human capabilities that we eager machines to mimic. Unlike previous\nmethods that either requires the sketch-image pairs or utilize low-quantity\ndetected edges as sketches, we study the exemplar-based sketch-to-image (s2i)\nsynthesis task in a self-supervised learning manner, eliminating the necessity\nof the paired sketch data. To this end, we first propose an unsupervised method\nto efficiently synthesize line-sketches for general RGB-only datasets. With the\nsynthetic paired-data, we then present a self-supervised Auto-Encoder (AE) to\ndecouple the content/style features from sketches and RGB-images, and\nsynthesize images that are both content-faithful to the sketches and\nstyle-consistent to the RGB-images. While prior works employ either the\ncycle-consistence loss or dedicated attentional modules to enforce the\ncontent/style fidelity, we show AE's superior performance with pure\nself-supervisions. To further improve the synthesis quality in high resolution,\nwe also leverage an adversarial network to refine the details of synthetic\nimages. Extensive experiments on 1024*1024 resolution demonstrate a new\nstate-of-art-art performance of the proposed model on CelebA-HQ and Wiki-Art\ndatasets. Moreover, with the proposed sketch generator, the model shows a\npromising performance on style mixing and style transfer, which require\nsynthesized images to be both style-consistent and semantically meaningful. Our\ncode is available on\nhttps://github.com/odegeasslbc/Self-Supervised-Sketch-to-Image-Synthesis-PyTorch,\nand please visit https://create.playform.io/my-projects?mode=sketch for an\nonline demo of our model.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 22:14:06 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 20:40:27 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Liu", "Bingchen", ""], ["Zhu", "Yizhe", ""], ["Song", "Kunpeng", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "2012.09311", "submitter": "Mingze Xu", "authors": "Tianchen Zhao, Xiang Xu, Mingze Xu, Hui Ding, Yuanjun Xiong, Wei Xia", "title": "Learning Self-Consistency for Deepfake Detection", "comments": "ICCV 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new method to detect deepfake images using the cue of the source\nfeature inconsistency within the forged images. It is based on the hypothesis\nthat images' distinct source features can be preserved and extracted after\ngoing through state-of-the-art deepfake generation processes. We introduce a\nnovel representation learning approach, called pair-wise self-consistency\nlearning (PCL), for training ConvNets to extract these source features and\ndetect deepfake images. It is accompanied by a new image synthesis approach,\ncalled inconsistency image generator (I2G), to provide richly annotated\ntraining data for PCL. Experimental results on seven popular datasets show that\nour models improve averaged AUC over the state of the art from 96.45% to 98.05%\nin the in-dataset evaluation and from 86.03% to 92.18% in the cross-dataset\nevaluation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 23:06:56 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 18:05:20 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Zhao", "Tianchen", ""], ["Xu", "Xiang", ""], ["Xu", "Mingze", ""], ["Ding", "Hui", ""], ["Xiong", "Yuanjun", ""], ["Xia", "Wei", ""]]}, {"id": "2012.09322", "submitter": "Mauricio Delbracio", "authors": "Mauricio Delbracio, Ignacio Garcia-Dorado, Sungjoon Choi, Damien\n  Kelly, Peyman Milanfar", "title": "Polyblur: Removing mild blur by polynomial reblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a highly efficient blind restoration method to remove mild blur in\nnatural images. Contrary to the mainstream, we focus on removing slight blur\nthat is often present, damaging image quality and commonly generated by small\nout-of-focus, lens blur, or slight camera motion. The proposed algorithm first\nestimates image blur and then compensates for it by combining multiple\napplications of the estimated blur in a principled way. To estimate blur we\nintroduce a simple yet robust algorithm based on empirical observations about\nthe distribution of the gradient in sharp natural images. Our experiments show\nthat, in the context of mild blur, the proposed method outperforms traditional\nand modern blind deblurring methods and runs in a fraction of the time. Our\nmethod can be used to blindly correct blur before applying off-the-shelf deep\nsuper-resolution methods leading to superior results than other highly complex\nand computationally demanding techniques. The proposed method estimates and\nremoves mild blur from a 12MP image on a modern mobile phone in a fraction of a\nsecond.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 23:38:39 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 16:38:48 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Delbracio", "Mauricio", ""], ["Garcia-Dorado", "Ignacio", ""], ["Choi", "Sungjoon", ""], ["Kelly", "Damien", ""], ["Milanfar", "Peyman", ""]]}, {"id": "2012.09327", "submitter": "Gustavo Perez", "authors": "Gustavo Perez, Matteo Messa, Daniela Calzetti, Subhransu Maji, Dooseok\n  Jung, Angela Adamo, Mattia Siressi", "title": "StarcNet: Machine Learning for Star Cluster Identification", "comments": null, "journal-ref": null, "doi": "10.3847/1538-4357/abceba", "report-no": null, "categories": "astro-ph.GA astro-ph.IM astro-ph.SR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a machine learning (ML) pipeline to identify star clusters in the\nmulti{color images of nearby galaxies, from observations obtained with the\nHubble Space Telescope as part of the Treasury Project LEGUS (Legacy\nExtraGalactic Ultraviolet Survey). StarcNet (STAR Cluster classification\nNETwork) is a multi-scale convolutional neural network (CNN) which achieves an\naccuracy of 68.6% (4 classes)/86.0% (2 classes: cluster/non-cluster) for star\ncluster classification in the images of the LEGUS galaxies, nearly matching\nhuman expert performance. We test the performance of StarcNet by applying\npre-trained CNN model to galaxies not included in the training set, finding\naccuracies similar to the reference one. We test the effect of StarcNet\npredictions on the inferred cluster properties by comparing multi-color\nluminosity functions and mass-age plots from catalogs produced by StarcNet and\nby human-labeling; distributions in luminosity, color, and physical\ncharacteristics of star clusters are similar for the human and ML classified\nsamples. There are two advantages to the ML approach: (1) reproducibility of\nthe classifications: the ML algorithm's biases are fixed and can be measured\nfor subsequent analysis; and (2) speed of classification: the algorithm\nrequires minutes for tasks that humans require weeks to months to perform. By\nachieving comparable accuracy to human classifiers, StarcNet will enable\nextending classifications to a larger number of candidate samples than\ncurrently available, thus increasing significantly the statistics for cluster\nstudies.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 23:58:01 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Perez", "Gustavo", ""], ["Messa", "Matteo", ""], ["Calzetti", "Daniela", ""], ["Maji", "Subhransu", ""], ["Jung", "Dooseok", ""], ["Adamo", "Angela", ""], ["Siressi", "Mattia", ""]]}, {"id": "2012.09328", "submitter": "Brian Reily", "authors": "Brian Reily, Hao Zhang", "title": "Simultaneous View and Feature Selection for Collaborative Multi-Robot\n  Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative multi-robot perception provides multiple views of an\nenvironment, offering varying perspectives to collaboratively understand the\nenvironment even when individual robots have poor points of view or when\nocclusions are caused by obstacles. These multiple observations must be\nintelligently fused for accurate recognition, and relevant observations need to\nbe selected in order to allow unnecessary robots to continue on to observe\nother targets. This research problem has not been well studied in the\nliterature yet. In this paper, we propose a novel approach to collaborative\nmulti-robot perception that simultaneously integrates view selection, feature\nselection, and object recognition into a unified regularized optimization\nformulation, which uses sparsity-inducing norms to identify the robots with the\nmost representative views and the modalities with the most discriminative\nfeatures. As our optimization formulation is hard to solve due to the\nintroduced non-smooth norms, we implement a new iterative optimization\nalgorithm, which is guaranteed to converge to the optimal solution. We evaluate\nour approach through a case-study in simulation and on a physical multi-robot\nsystem. Experimental results demonstrate that our approach enables effective\ncollaborative perception through accurate object recognition and effective view\nand feature selection.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 00:01:05 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 21:41:43 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Reily", "Brian", ""], ["Zhang", "Hao", ""]]}, {"id": "2012.09329", "submitter": "Tiantu Xu", "authors": "Tiantu Xu, Kaiwen Shen, Yang Fu, Humphrey Shi, Felix Xiaozhu Lin", "title": "Clique: Spatiotemporal Object Re-identification at the City Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object re-identification (ReID) is a key application of city-scale cameras.\nWhile classic ReID tasks are often considered as image retrieval, we treat them\nas spatiotemporal queries for locations and times in which the target object\nappeared. Spatiotemporal reID is challenged by the accuracy limitation in\ncomputer vision algorithms and the colossal videos from city cameras. We\npresent Clique, a practical ReID engine that builds upon two new techniques:\n(1) Clique assesses target occurrences by clustering fuzzy object features\nextracted by ReID algorithms, with each cluster representing the general\nimpression of a distinct object to be matched against the input; (2) to search\nin videos, Clique samples cameras to maximize the spatiotemporal coverage and\nincrementally adds cameras for processing on demand. Through evaluation on 25\nhours of videos from 25 cameras, Clique reached a high accuracy of 0.87 (recall\nat 5) across 70 queries and runs at 830x of video realtime in achieving high\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 00:05:50 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Xu", "Tiantu", ""], ["Shen", "Kaiwen", ""], ["Fu", "Yang", ""], ["Shi", "Humphrey", ""], ["Lin", "Felix Xiaozhu", ""]]}, {"id": "2012.09333", "submitter": "Huai Chen", "authors": "Huai Chen, Jieyu Li, Renzhen Wang, Yijie Huang, Fanrui Meng, Deyu\n  Meng, Qing Peng, Lisheng Wang", "title": "Unsupervised Learning of Local Discriminative Representation for Medical\n  Images", "comments": "Accepted by IPMI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local discriminative representation is needed in many medical image analysis\ntasks such as identifying sub-types of lesion or segmenting detailed components\nof anatomical structures. However, the commonly applied supervised\nrepresentation learning methods require a large amount of annotated data, and\nunsupervised discriminative representation learning distinguishes different\nimages by learning a global feature, both of which are not suitable for\nlocalized medical image analysis tasks. In order to avoid the limitations of\nthese two methods, we introduce local discrimination into unsupervised\nrepresentation learning in this work. The model contains two branches: one is\nan embedding branch which learns an embedding function to disperse dissimilar\npixels over a low-dimensional hypersphere; and the other is a clustering branch\nwhich learns a clustering function to classify similar pixels into the same\ncluster. These two branches are trained simultaneously in a mutually beneficial\npattern, and the learnt local discriminative representations are able to well\nmeasure the similarity of local image regions. These representations can be\ntransferred to enhance various downstream tasks. Meanwhile, they can also be\napplied to cluster anatomical structures from unlabeled medical images under\nthe guidance of topological priors from simulation or other structures with\nsimilar topological characteristics. The effectiveness and usefulness of the\nproposed method are demonstrated by enhancing various downstream tasks and\nclustering anatomical structures in retinal images and chest X-ray images.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 00:20:23 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 12:30:47 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Chen", "Huai", ""], ["Li", "Jieyu", ""], ["Wang", "Renzhen", ""], ["Huang", "Yijie", ""], ["Meng", "Fanrui", ""], ["Meng", "Deyu", ""], ["Peng", "Qing", ""], ["Wang", "Lisheng", ""]]}, {"id": "2012.09340", "submitter": "Yiming Qian", "authors": "Yiming Qian, Hao Zhang, Yasutaka Furukawa", "title": "Roof-GAN: Learning to Generate Roof Geometry and Relations for\n  Residential Houses", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Roof-GAN, a novel generative adversarial network that\ngenerates structured geometry of residential roof structures as a set of roof\nprimitives and their relationships. Given the number of primitives, the\ngenerator produces a structured roof model as a graph, which consists of 1)\nprimitive geometry as raster images at each node, encoding facet segmentation\nand angles; 2) inter-primitive colinear/coplanar relationships at each edge;\nand 3) primitive geometry in a vector format at each node, generated by a novel\ndifferentiable vectorizer while enforcing the relationships. The discriminator\nis trained to assess the primitive raster geometry, the primitive\nrelationships, and the primitive vector geometry in a fully end-to-end\narchitecture. Qualitative and quantitative evaluations demonstrate the\neffectiveness of our approach in generating diverse and realistic roof models\nover the competing methods with a novel metric proposed in this paper for the\ntask of structured geometry generation. Code and data are available at\nhttps://github.com/yi-ming-qian/roofgan .\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 00:47:57 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 09:27:11 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Qian", "Yiming", ""], ["Zhang", "Hao", ""], ["Furukawa", "Yasutaka", ""]]}, {"id": "2012.09365", "submitter": "Chunhua Shen", "authors": "Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon\n  Chen, Chunhua Shen", "title": "Learning to Recover 3D Scene Shape from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Despite significant progress in monocular depth estimation in the wild,\nrecent state-of-the-art methods cannot be used to recover accurate 3D scene\nshape due to an unknown depth shift induced by shift-invariant reconstruction\nlosses used in mixed-data depth prediction training, and possible unknown\ncamera focal length. We investigate this problem in detail, and propose a\ntwo-stage framework that first predicts depth up to an unknown scale and shift\nfrom a single monocular image, and then use 3D point cloud encoders to predict\nthe missing depth shift and focal length that allow us to recover a realistic\n3D scene shape. In addition, we propose an image-level normalized regression\nloss and a normal-based geometry loss to enhance depth prediction models\ntrained on mixed datasets. We test our depth model on nine unseen datasets and\nachieve state-of-the-art performance on zero-shot dataset generalization. Code\nis available at: https://git.io/Depth\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 02:35:13 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Yin", "Wei", ""], ["Zhang", "Jianming", ""], ["Wang", "Oliver", ""], ["Niklaus", "Simon", ""], ["Mai", "Long", ""], ["Chen", "Simon", ""], ["Shen", "Chunhua", ""]]}, {"id": "2012.09372", "submitter": "Pengju Zhang", "authors": "Pengju Zhang, Yihong Wu, Jiagang Zhu", "title": "Semi-Global Shape-aware Network", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-local operations are usually used to capture long-range dependencies via\naggregating global context to each position recently. However, most of the\nmethods cannot preserve object shapes since they only focus on feature\nsimilarity but ignore proximity between central and other positions for\ncapturing long-range dependencies, while shape-awareness is beneficial to many\ncomputer vision tasks. In this paper, we propose a Semi-Global Shape-aware\nNetwork (SGSNet) considering both feature similarity and proximity for\npreserving object shapes when modeling long-range dependencies. A hierarchical\nway is taken to aggregate global context. In the first level, each position in\nthe whole feature map only aggregates contextual information in vertical and\nhorizontal directions according to both similarity and proximity. And then the\nresult is input into the second level to do the same operations. By this\nhierarchical way, each central position gains supports from all other\npositions, and the combination of similarity and proximity makes each position\ngain supports mostly from the same semantic object. Moreover, we also propose a\nlinear time algorithm for the aggregation of contextual information, where each\nof rows and columns in the feature map is treated as a binary tree to reduce\nsimilarity computation cost. Experiments on semantic segmentation and image\nretrieval show that adding SGSNet to existing networks gains solid improvements\non both accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 02:52:10 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Zhang", "Pengju", ""], ["Wu", "Yihong", ""], ["Zhu", "Jiagang", ""]]}, {"id": "2012.09373", "submitter": "Hongxiao Wang", "authors": "Hongxiao Wang, Hao Zheng, Jianxu Chen, Lin Yang, Yizhe Zhang, Danny Z.\n  Chen", "title": "Unlabeled Data Guided Semi-supervised Histopathology Image Segmentation", "comments": "Accepted paper for the 2020 IEEE International Conference on\n  Bioinformatics and Biomedicine (BIBM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic histopathology image segmentation is crucial to disease analysis.\nLimited available labeled data hinders the generalizability of trained models\nunder the fully supervised setting. Semi-supervised learning (SSL) based on\ngenerative methods has been proven to be effective in utilizing diverse image\ncharacteristics. However, it has not been well explored what kinds of generated\nimages would be more useful for model training and how to use such images. In\nthis paper, we propose a new data guided generative method for histopathology\nimage segmentation by leveraging the unlabeled data distributions. First, we\ndesign an image generation module. Image content and style are disentangled and\nembedded in a clustering-friendly space to utilize their distributions. New\nimages are synthesized by sampling and cross-combining contents and styles.\nSecond, we devise an effective data selection policy for judiciously sampling\nthe generated images: (1) to make the generated training set better cover the\ndataset, the clusters that are underrepresented in the original training set\nare covered more; (2) to make the training process more effective, we identify\nand oversample the images of \"hard cases\" in the data for which annotated\ntraining data may be scarce. Our method is evaluated on glands and nuclei\ndatasets. We show that under both the inductive and transductive settings, our\nSSL method consistently boosts the performance of common segmentation models\nand attains state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 02:54:19 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Wang", "Hongxiao", ""], ["Zheng", "Hao", ""], ["Chen", "Jianxu", ""], ["Yang", "Lin", ""], ["Zhang", "Yizhe", ""], ["Chen", "Danny Z.", ""]]}, {"id": "2012.09378", "submitter": "Ziwei Wang", "authors": "Ziwei Wang, Yonhon Ng, Pieter van Goor, Robert Mahony", "title": "Event Camera Calibration of Per-pixel Biased Contrast Threshold", "comments": "11 pages, 7 figures, the paper has been accepted for publication at\n  the Australian Conference on Robotics and Automation, 2019", "journal-ref": "Australian Conference on Robotics and Automation, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras output asynchronous events to represent intensity changes with\na high temporal resolution, even under extreme lighting conditions. Currently,\nmost of the existing works use a single contrast threshold to estimate the\nintensity change of all pixels. However, complex circuit bias and manufacturing\nimperfections cause biased pixels and mismatch contrast threshold among pixels,\nwhich may lead to undesirable outputs. In this paper, we propose a new event\ncamera model and two calibration approaches which cover event-only cameras and\nhybrid image-event cameras. When intensity images are simultaneously provided\nalong with events, we also propose an efficient online method to calibrate\nevent cameras that adapts to time-varying event rates. We demonstrate the\nadvantages of our proposed methods compared to the state-of-the-art on several\ndifferent event camera datasets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 03:16:13 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Wang", "Ziwei", ""], ["Ng", "Yonhon", ""], ["van Goor", "Pieter", ""], ["Mahony", "Robert", ""]]}, {"id": "2012.09384", "submitter": "Zhonghan Niu", "authors": "Zhonghan Niu, Zhaoxi Chen, Linyi Li, Yubin Yang, Bo Li, Jinfeng Yi", "title": "On the Limitations of Denoising Strategies as Adversarial Defenses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As adversarial attacks against machine learning models have raised increasing\nconcerns, many denoising-based defense approaches have been proposed. In this\npaper, we summarize and analyze the defense strategies in the form of symmetric\ntransformation via data denoising and reconstruction (denoted as $F+$ inverse\n$F$, $F-IF$ Framework). In particular, we categorize these denoising strategies\nfrom three aspects (i.e. denoising in the spatial domain, frequency domain, and\nlatent space, respectively). Typically, defense is performed on the entire\nadversarial example, both image and perturbation are modified, making it\ndifficult to tell how it defends against the perturbations. To evaluate the\nrobustness of these denoising strategies intuitively, we directly apply them to\ndefend against adversarial noise itself (assuming we have obtained all of it),\nwhich saving us from sacrificing benign accuracy. Surprisingly, our\nexperimental results show that even if most of the perturbations in each\ndimension is eliminated, it is still difficult to obtain satisfactory\nrobustness. Based on the above findings and analyses, we propose the adaptive\ncompression strategy for different frequency bands in the feature domain to\nimprove the robustness. Our experiment results show that the adaptive\ncompression strategies enable the model to better suppress adversarial\nperturbations, and improve robustness compared with existing denoising\nstrategies.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 03:54:30 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Niu", "Zhonghan", ""], ["Chen", "Zhaoxi", ""], ["Li", "Linyi", ""], ["Yang", "Yubin", ""], ["Li", "Bo", ""], ["Yi", "Jinfeng", ""]]}, {"id": "2012.09386", "submitter": "Lavanya Umapathy", "authors": "Lavanya Umapathy, Mahesh Bharath Keerthivasan, Natalie M. Zahr, Ali\n  Bilgin, Manojkumar Saranathan", "title": "A Contrast Synthesized Thalamic Nuclei Segmentation Scheme using\n  Convolutional Neural Networks", "comments": "24 pages, 7 figures, submitted to Neuroinformatics December 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Thalamic nuclei have been implicated in several neurological diseases.\nWMn-MPRAGE images have been shown to provide better intra-thalamic nuclear\ncontrast compared to conventional MPRAGE images but the additional acquisition\nresults in increased examination times. In this work, we investigated 3D\nConvolutional Neural Network (CNN) based techniques for thalamic nuclei\nparcellation from conventional MPRAGE images. Two 3D CNNs were developed and\ncompared for thalamic nuclei parcellation using MPRAGE images: a) a native\ncontrast segmentation (NCS) and b) a synthesized contrast segmentation (SCS)\nusing WMn-MPRAGE images synthesized from MPRAGE images. We trained the two\nsegmentation frameworks using MPRAGE images (n=35) and thalamic nuclei labels\ngenerated on WMn-MPRAGE images using a multi-atlas based parcellation\ntechnique. The segmentation accuracy and clinical utility were evaluated on a\ncohort comprising of healthy subjects and patients with alcohol use disorder\n(AUD) (n=45). The SCS network yielded higher Dice scores in the Medial\ngeniculate nucleus (P=.003) and Centromedian nucleus (P=.01) with lower volume\ndifferences for Ventral anterior (P=.001) and Ventral posterior lateral (P=.01)\nnuclei when compared to the NCS network. A Bland-Altman analysis revealed\ntighter limits of agreement with lower coefficient of variation between true\nvolumes and those predicted by the SCS network. The SCS network demonstrated a\nsignificant atrophy in Ventral lateral posterior nucleus in AUD patients\ncompared to healthy age-matched controls (P=0.01), agreeing with previous\nstudies on thalamic atrophy in alcoholism, whereas the NCS network showed\nspurious atrophy of the Ventral posterior lateral nucleus. CNN-based contrast\nsynthesis prior to segmentation can provide fast and accurate thalamic nuclei\nsegmentation from conventional MPRAGE images.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 04:05:11 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Umapathy", "Lavanya", ""], ["Keerthivasan", "Mahesh Bharath", ""], ["Zahr", "Natalie M.", ""], ["Bilgin", "Ali", ""], ["Saranathan", "Manojkumar", ""]]}, {"id": "2012.09393", "submitter": "Tianxiao Zhang", "authors": "Tianxiao Zhang, Xiaohan Zhang, Yiju Yang, Zongbo Wang, Guanghui Wang", "title": "Efficient Golf Ball Detection and Tracking Based on Convolutional Neural\n  Networks and Kalman Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the problem of online golf ball detection and tracking\nfrom image sequences. An efficient real-time approach is proposed by exploiting\nconvolutional neural networks (CNN) based object detection and a Kalman filter\nbased prediction. Five classical deep learning-based object detection networks\nare implemented and evaluated for ball detection, including YOLO v3 and its\ntiny version, YOLO v4, Faster R-CNN, SSD, and RefineDet. The detection is\nperformed on small image patches instead of the entire image to increase the\nperformance of small ball detection. At the tracking stage, a discrete Kalman\nfilter is employed to predict the location of the ball and a small image patch\nis cropped based on the prediction. Then, the object detector is utilized to\nrefine the location of the ball and update the parameters of Kalman filter. In\norder to train the detection models and test the tracking algorithm, a\ncollection of golf ball dataset is created and annotated. Extensive comparative\nexperiments are performed to demonstrate the effectiveness and superior\ntracking performance of the proposed scheme.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 04:55:27 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 18:06:10 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Zhang", "Tianxiao", ""], ["Zhang", "Xiaohan", ""], ["Yang", "Yiju", ""], ["Wang", "Zongbo", ""], ["Wang", "Guanghui", ""]]}, {"id": "2012.09398", "submitter": "Chenxin Xu", "authors": "Chenxin Xu, Siheng Chen, Maosen Li, Ya Zhang", "title": "Invariant Teacher and Equivariant Student for Unsupervised 3D Human Pose\n  Estimation", "comments": "Accepted in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method based on teacher-student learning framework for 3D\nhuman pose estimation without any 3D annotation or side information. To solve\nthis unsupervised-learning problem, the teacher network adopts\npose-dictionary-based modeling for regularization to estimate a physically\nplausible 3D pose. To handle the decomposition ambiguity in the teacher\nnetwork, we propose a cycle-consistent architecture promoting a 3D\nrotation-invariant property to train the teacher network. To further improve\nthe estimation accuracy, the student network adopts a novel graph convolution\nnetwork for flexibility to directly estimate the 3D coordinates. Another\ncycle-consistent architecture promoting 3D rotation-equivariant property is\nadopted to exploit geometry consistency, together with knowledge distillation\nfrom the teacher network to improve the pose estimation performance. We conduct\nextensive experiments on Human3.6M and MPI-INF-3DHP. Our method reduces the 3D\njoint prediction error by 11.4% compared to state-of-the-art unsupervised\nmethods and also outperforms many weakly-supervised methods that use side\ninformation on Human3.6M. Code will be available at\nhttps://github.com/sjtuxcx/ITES.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 05:32:44 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Xu", "Chenxin", ""], ["Chen", "Siheng", ""], ["Li", "Maosen", ""], ["Zhang", "Ya", ""]]}, {"id": "2012.09401", "submitter": "Soo Ye Kim", "authors": "Soo Ye Kim, Kfir Aberman, Nori Kanazawa, Rahul Garg, Neal Wadhwa,\n  Huiwen Chang, Nikhil Karnad, Munchurl Kim, Orly Liba", "title": "Zoom-to-Inpaint: Image Inpainting with High-Frequency Details", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Although deep learning has enabled a huge leap forward in image inpainting,\ncurrent methods are often unable to synthesize realistic high-frequency\ndetails. In this paper, we propose applying super-resolution to coarsely\nreconstructed outputs, refining them at high resolution, and then downscaling\nthe output to the original resolution. By introducing high-resolution images to\nthe refinement network, our framework is able to reconstruct finer details that\nare usually smoothed out due to spectral bias - the tendency of neural networks\nto reconstruct low frequencies better than high frequencies. To assist training\nthe refinement network on large upscaled holes, we propose a progressive\nlearning technique in which the size of the missing regions increases as\ntraining progresses. Our zoom-in, refine and zoom-out strategy, combined with\nhigh-resolution supervision and progressive learning, constitutes a\nframework-agnostic approach for enhancing high-frequency details that can be\napplied to any CNN-based inpainting method. We provide qualitative and\nquantitative evaluations along with an ablation analysis to show the\neffectiveness of our approach. This seemingly simple, yet powerful approach,\noutperforms state-of-the-art inpainting methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 05:39:37 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 01:40:49 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Kim", "Soo Ye", ""], ["Aberman", "Kfir", ""], ["Kanazawa", "Nori", ""], ["Garg", "Rahul", ""], ["Wadhwa", "Neal", ""], ["Chang", "Huiwen", ""], ["Karnad", "Nikhil", ""], ["Kim", "Munchurl", ""], ["Liba", "Orly", ""]]}, {"id": "2012.09402", "submitter": "Sai Praneeth Sunkesula", "authors": "Sai Praneeth Reddy Sunkesula, Rishabh Dabral, Ganesh Ramakrishnan", "title": "LIGHTEN: Learning Interactions with Graph and Hierarchical TEmporal\n  Networks for HOI in videos", "comments": "9 pages, 6 figures, ACM Multimedia Conference 2020", "journal-ref": "MM20 Proceedings of the 28th ACM International Conference on\n  Multimedia, October 2020, Pages 691 to 699", "doi": "10.1145/3394171.3413778", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Analyzing the interactions between humans and objects from a video includes\nidentification of the relationships between humans and the objects present in\nthe video. It can be thought of as a specialized version of Visual Relationship\nDetection, wherein one of the objects must be a human. While traditional\nmethods formulate the problem as inference on a sequence of video segments, we\npresent a hierarchical approach, LIGHTEN, to learn visual features to\neffectively capture spatio-temporal cues at multiple granularities in a video.\nUnlike current approaches, LIGHTEN avoids using ground truth data like depth\nmaps or 3D human pose, thus increasing generalization across non-RGBD datasets\nas well. Furthermore, we achieve the same using only the visual features,\ninstead of the commonly used hand-crafted spatial features. We achieve\nstate-of-the-art results in human-object interaction detection (88.9% and\n92.6%) and anticipation tasks of CAD-120 and competitive results on image based\nHOI detection in V-COCO dataset, setting a new benchmark for visual features\nbased approaches. Code for LIGHTEN is available at\nhttps://github.com/praneeth11009/LIGHTEN-Learning-Interactions-with-Graphs-and-Hierarchical-TEmporal-Networks-for-HOI\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 05:44:07 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Sunkesula", "Sai Praneeth Reddy", ""], ["Dabral", "Rishabh", ""], ["Ramakrishnan", "Ganesh", ""]]}, {"id": "2012.09407", "submitter": "Taiga Kashima", "authors": "Taiga Kashima, Yoshihiro Yamada, Shunta Saito", "title": "Joint Search of Data Augmentation Policies and Network Architectures", "comments": "AAAI 2021 Workshop: Learning Network Architecture during Training", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The common pipeline of training deep neural networks consists of several\nbuilding blocks such as data augmentation and network architecture selection.\nAutoML is a research field that aims at automatically designing those parts,\nbut most methods explore each part independently because it is more challenging\nto simultaneously search all the parts. In this paper, we propose a joint\noptimization method for data augmentation policies and network architectures to\nbring more automation to the design of training pipeline. The core idea of our\napproach is to make the whole part differentiable. The proposed method combines\ndifferentiable methods for augmentation policy search and network architecture\nsearch to jointly optimize them in the end-to-end manner. The experimental\nresults show our method achieves competitive or superior performance to the\nindependently searched results.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 06:09:44 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 11:59:50 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Kashima", "Taiga", ""], ["Yamada", "Yoshihiro", ""], ["Saito", "Shunta", ""]]}, {"id": "2012.09409", "submitter": "David Deng", "authors": "David Deng and Avideh Zakhor", "title": "Temporal LiDAR Frame Prediction for Autonomous Driving", "comments": "In 3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anticipating the future in a dynamic scene is critical for many fields such\nas autonomous driving and robotics. In this paper we propose a class of novel\nneural network architectures to predict future LiDAR frames given previous\nones. Since the ground truth in this application is simply the next frame in\nthe sequence, we can train our models in a self-supervised fashion. Our\nproposed architectures are based on FlowNet3D and Dynamic Graph CNN. We use\nChamfer Distance (CD) and Earth Mover's Distance (EMD) as loss functions and\nevaluation metrics. We train and evaluate our models using the newly released\nnuScenes dataset, and characterize their performance and complexity with\nseveral baselines. Compared to directly using FlowNet3D, our proposed\narchitectures achieve CD and EMD nearly an order of magnitude lower. In\naddition, we show that our predictions generate reasonable scene flow\napproximations without using any labelled supervision.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 06:19:59 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Deng", "David", ""], ["Zakhor", "Avideh", ""]]}, {"id": "2012.09413", "submitter": "Guodong Xu", "authors": "Guodong Xu, Ziwei Liu, Chen Change Loy", "title": "Computation-Efficient Knowledge Distillation via Uncertainty-Aware Mixup", "comments": "The code is available at: https://github.com/xuguodong03/UNIXKD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation, which involves extracting the \"dark knowledge\" from a\nteacher network to guide the learning of a student network, has emerged as an\nessential technique for model compression and transfer learning. Unlike\nprevious works that focus on the accuracy of student network, here we study a\nlittle-explored but important question, i.e., knowledge distillation\nefficiency. Our goal is to achieve a performance comparable to conventional\nknowledge distillation with a lower computation cost during training. We show\nthat the UNcertainty-aware mIXup (UNIX) can serve as a clean yet effective\nsolution. The uncertainty sampling strategy is used to evaluate the\ninformativeness of each training sample. Adaptive mixup is applied to uncertain\nsamples to compact knowledge. We further show that the redundancy of\nconventional knowledge distillation lies in the excessive learning of easy\nsamples. By combining uncertainty and mixup, our approach reduces the\nredundancy and makes better use of each query to the teacher network. We\nvalidate our approach on CIFAR100 and ImageNet. Notably, with only 79%\ncomputation cost, we outperform conventional knowledge distillation on CIFAR100\nand achieve a comparable result on ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 06:52:16 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Xu", "Guodong", ""], ["Liu", "Ziwei", ""], ["Loy", "Chen Change", ""]]}, {"id": "2012.09418", "submitter": "Jianren Wang", "authors": "Xia Chen, Jianren Wang, David Held, Martial Hebert", "title": "PanoNet3D: Combining Semantic and Geometric Understanding for LiDARPoint\n  Cloud Detection", "comments": "3DV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual data in autonomous driving perception, such as camera image and LiDAR\npoint cloud, can be interpreted as a mixture of two aspects: semantic feature\nand geometric structure. Semantics come from the appearance and context of\nobjects to the sensor, while geometric structure is the actual 3D shape of\npoint clouds. Most detectors on LiDAR point clouds focus only on analyzing the\ngeometric structure of objects in real 3D space. Unlike previous works, we\npropose to learn both semantic feature and geometric structure via a unified\nmulti-view framework. Our method exploits the nature of LiDAR scans -- 2D range\nimages, and applies well-studied 2D convolutions to extract semantic features.\nBy fusing semantic and geometric features, our method outperforms\nstate-of-the-art approaches in all categories by a large margin. The\nmethodology of combining semantic and geometric features provides a unique\nperspective of looking at the problems in real-world 3D point cloud detection.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 06:58:34 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Chen", "Xia", ""], ["Wang", "Jianren", ""], ["Held", "David", ""], ["Hebert", "Martial", ""]]}, {"id": "2012.09434", "submitter": "Xiaolong Liu", "authors": "Xiaolong Liu (1), Yao Hu (2), Song Bai (2,3), Fei Ding (2), Xiang Bai\n  (1), Philip H.S. Torr (3) ((1) Huazhong University of Science and Technology,\n  (2) Alibaba Group, (3) University of Oxford)", "title": "Multi-shot Temporal Event Localization: a Benchmark", "comments": "CVPR 2021. Project page at https://songbai.site/muses/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current developments in temporal event or action localization usually target\nactions captured by a single camera. However, extensive events or actions in\nthe wild may be captured as a sequence of shots by multiple cameras at\ndifferent positions. In this paper, we propose a new and challenging task\ncalled multi-shot temporal event localization, and accordingly, collect a large\nscale dataset called MUlti-Shot EventS (MUSES). MUSES has 31,477 event\ninstances for a total of 716 video hours. The core nature of MUSES is the\nfrequent shot cuts, for an average of 19 shots per instance and 176 shots per\nvideo, which induces large intrainstance variations. Our comprehensive\nevaluations show that the state-of-the-art method in temporal action\nlocalization only achieves an mAP of 13.1% at IoU=0.5. As a minor contribution,\nwe present a simple baseline approach for handling the intra-instance\nvariations, which reports an mAP of 18.9% on MUSES and 56.9% on THUMOS14 at\nIoU=0.5. To facilitate research in this direction, we release the dataset and\nthe project code at https://songbai.site/muses/ .\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 08:10:28 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 11:16:12 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Liu", "Xiaolong", ""], ["Hu", "Yao", ""], ["Bai", "Song", ""], ["Ding", "Fei", ""], ["Bai", "Xiang", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "2012.09439", "submitter": "Kangcheng Liu", "authors": "Kangcheng Liu, Zhi Gao, Feng Lin, and Ben M. Chen", "title": "FG-Net: Fast Large-Scale LiDAR Point Clouds Understanding Network\n  Leveraging Correlated Feature Mining and Geometric-Aware Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This work presents FG-Net, a general deep learning framework for large-scale\npoint clouds understanding without voxelizations, which achieves accurate and\nreal-time performance with a single NVIDIA GTX 1080 GPU. First, a novel noise\nand outlier filtering method is designed to facilitate subsequent high-level\ntasks. For effective understanding purpose, we propose a deep convolutional\nneural network leveraging correlated feature mining and deformable convolution\nbased geometric-aware modelling, in which the local feature relationships and\ngeometric patterns can be fully exploited. For the efficiency issue, we put\nforward an inverse density sampling operation and a feature pyramid based\nresidual learning strategy to save the computational cost and memory\nconsumption respectively. Extensive experiments on real-world challenging\ndatasets demonstrated that our approaches outperform state-of-the-art\napproaches in terms of accuracy and efficiency. Moreover, weakly supervised\ntransfer learning is also conducted to demonstrate the generalization capacity\nof our method.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 08:20:09 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 10:19:12 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Liu", "Kangcheng", ""], ["Gao", "Zhi", ""], ["Lin", "Feng", ""], ["Chen", "Ben M.", ""]]}, {"id": "2012.09444", "submitter": "Ying Bi", "authors": "Ying Bi, Bing Xue, and Mengjie Zhang", "title": "Learning and Sharing: A Multitask Genetic Programming Approach to Image\n  Feature Learning", "comments": "Submitted to IEEE Transactions on Evolutionary Computation", "journal-ref": "IEEE Transactions on Evolutionary Computation, 2021", "doi": "10.1109/TEVC.2021.3097043", "report-no": "2012.09444", "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using evolutionary computation algorithms to solve multiple tasks with\nknowledge sharing is a promising approach. Image feature learning can be\nconsidered as a multitask problem because different tasks may have a similar\nfeature space. Genetic programming (GP) has been successfully applied to image\nfeature learning for classification. However, most of the existing GP methods\nsolve one task, independently, using sufficient training data. No multitask GP\nmethod has been developed for image feature learning. Therefore, this paper\ndevelops a multitask GP approach to image feature learning for classification\nwith limited training data. Owing to the flexible representation of GP, a new\nknowledge sharing mechanism based on a new individual representation is\ndeveloped to allow GP to automatically learn what to share across two tasks and\nto improve its learning performance. The shared knowledge is encoded as a\ncommon tree, which can represent the common/general features of two tasks. With\nthe new individual representation, each task is solved using the features\nextracted from a common tree and a task-specific tree representing\ntask-specific features. To learn the best common and task-specific trees, a new\nevolutionary process and new fitness functions are developed. The performance\nof the proposed approach is examined on six multitask problems of 12 image\nclassification datasets with limited training data and compared with three GP\nand 14 non-GP-based competitive methods. Experimental results show that the new\napproach outperforms these compared methods in almost all the comparisons.\nFurther analysis reveals that the new approach learns simple yet effective\ncommon trees with high effectiveness and transferability.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 08:34:22 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 20:51:55 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Bi", "Ying", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""]]}, {"id": "2012.09472", "submitter": "Kelvin Shak", "authors": "Kelvin Shak, Mundher Al-Shabi, Andrea Liew, Boon Leong Lan, Wai Yee\n  Chan, Kwan Hoong Ng, Maxine Tan", "title": "A new semi-supervised self-training method for lung cancer prediction", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Background and Objective: Early detection of lung cancer is crucial as it has\nhigh mortality rate with patients commonly present with the disease at stage 3\nand above. There are only relatively few methods that simultaneously detect and\nclassify nodules from computed tomography (CT) scans. Furthermore, very few\nstudies have used semi-supervised learning for lung cancer prediction. This\nstudy presents a complete end-to-end scheme to detect and classify lung nodules\nusing the state-of-the-art Self-training with Noisy Student method on a\ncomprehensive CT lung screening dataset of around 4,000 CT scans.\n  Methods: We used three datasets, namely LUNA16, LIDC and NLST, for this\nstudy. We first utilise a three-dimensional deep convolutional neural network\nmodel to detect lung nodules in the detection stage. The classification model\nknown as Maxout Local-Global Network uses non-local networks to detect global\nfeatures including shape features, residual blocks to detect local features\nincluding nodule texture, and a Maxout layer to detect nodule variations. We\ntrained the first Self-training with Noisy Student model to predict lung cancer\non the unlabelled NLST datasets. Then, we performed Mixup regularization to\nenhance our scheme and provide robustness to erroneous labels.\n  Results and Conclusions: Our new Mixup Maxout Local-Global network achieves\nan AUC of 0.87 on 2,005 completely independent testing scans from the NLST\ndataset. Our new scheme significantly outperformed the next highest performing\nmethod at the 5% significance level using DeLong's test (p = 0.0001). This\nstudy presents a new complete end-to-end scheme to predict lung cancer using\nSelf-training with Noisy Student combined with Mixup regularization. On a\ncompletely independent dataset of 2,005 scans, we achieved state-of-the-art\nperformance even with more images as compared to other methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 09:53:51 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Shak", "Kelvin", ""], ["Al-Shabi", "Mundher", ""], ["Liew", "Andrea", ""], ["Lan", "Boon Leong", ""], ["Chan", "Wai Yee", ""], ["Ng", "Kwan Hoong", ""], ["Tan", "Maxine", ""]]}, {"id": "2012.09491", "submitter": "Quan Quan", "authors": "Quan Quan, Qiyuan Wang, Liu Li, Yuanqi Du, S. Kevin Zhou", "title": "CT Film Recovery via Disentangling Geometric Deformation and\n  Illumination Variation: Simulated Datasets and Deep Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While medical images such as computed tomography (CT) are stored in DICOM\nformat in hospital PACS, it is still quite routine in many countries to print a\nfilm as a transferable medium for the purposes of self-storage and secondary\nconsultation. Also, with the ubiquitousness of mobile phone cameras, it is\nquite common to take pictures of the CT films, which unfortunately suffer from\ngeometric deformation and illumination variation. In this work, we study the\nproblem of recovering a CT film, which marks the first attempt in the\nliterature, to the best of our knowledge. We start with building a large-scale\nhead CT film database CTFilm20K, consisting of approximately 20,000 pictures,\nusing the widely used computer graphics software Blender. We also record all\naccompanying information related to the geometric deformation (such as 3D\ncoordinate, depth, normal, and UV maps) and illumination variation (such as\nalbedo map). Then we propose a deep framework to disentangle geometric\ndeformation and illumination variation using the multiple maps extracted from\nthe CT films to collaboratively guide the recovery process. Extensive\nexperiments on simulated and real images demonstrate the superiority of our\napproach over the previous approaches. We plan to open source the simulated\nimages and deep models for promoting the research on CT film recovery\n(https://anonymous.4open.science/r/e6b1f6e3-9b36-423f-a225-55b7d0b55523/).\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 10:32:19 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Quan", "Quan", ""], ["Wang", "Qiyuan", ""], ["Li", "Liu", ""], ["Du", "Yuanqi", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2012.09496", "submitter": "Moran Li", "authors": "Moran Li, Yuan Gao, Nong Sang", "title": "Exploiting Learnable Joint Groups for Hand Pose Estimation", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to estimate 3D hand pose by recovering the 3D\ncoordinates of joints in a group-wise manner, where less-related joints are\nautomatically categorized into different groups and exhibit different features.\nThis is different from the previous methods where all the joints are considered\nholistically and share the same feature. The benefits of our method are\nillustrated by the principle of multi-task learning (MTL), i.e., by separating\nless-related joints into different groups (as different tasks), our method\nlearns different features for each of them, therefore efficiently avoids the\nnegative transfer (among less related tasks/groups of joints). The key of our\nmethod is a novel binary selector that automatically selects related joints\ninto the same group. We implement such a selector with binary values\nstochastically sampled from a Concrete distribution, which is constructed using\nGumbel softmax on trainable parameters. This enables us to preserve the\ndifferentiable property of the whole network. We further exploit features from\nthose less-related groups by carrying out an additional feature fusing scheme\namong them, to learn more discriminative features. This is realized by\nimplementing multiple 1x1 convolutions on the concatenated features, where each\njoint group contains a unique 1x1 convolution for feature fusion. The detailed\nablation analysis and the extensive experiments on several benchmark datasets\ndemonstrate the promising performance of the proposed method over the\nstate-of-the-art (SOTA) methods. Besides, our method achieves top-1 among all\nthe methods that do not exploit the dense 3D shape labels on the most recently\nreleased FreiHAND competition at the submission date. The source code and\nmodels are available at https://github.com/ moranli-aca/LearnableGroups-Hand.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 10:47:28 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Li", "Moran", ""], ["Gao", "Yuan", ""], ["Sang", "Nong", ""]]}, {"id": "2012.09501", "submitter": "Qingsong Yao", "authors": "Qingsong Yao, Zecheng He, Yi Lin, Kai Ma, Yefeng Zheng and S. Kevin\n  Zhou", "title": "A Hierarchical Feature Constraint to Camouflage Medical Adversarial\n  Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) for medical images are extremely vulnerable to\nadversarial examples (AEs), which poses security concerns on clinical decision\nmaking. Luckily, medical AEs are also easy to detect in hierarchical feature\nspace per our study herein. To better understand this phenomenon, we thoroughly\ninvestigate the intrinsic characteristic of medical AEs in feature space,\nproviding both empirical evidence and theoretical explanations for the\nquestion: why are medical adversarial attacks easy to detect? We first perform\na stress test to reveal the vulnerability of deep representations of medical\nimages, in contrast to natural images. We then theoretically prove that typical\nadversarial attacks to binary disease diagnosis network manipulate the\nprediction by continuously optimizing the vulnerable representations in a fixed\ndirection, resulting in outlier features that make medical AEs easy to detect.\nHowever, this vulnerability can also be exploited to hide the AEs in the\nfeature space. We propose a novel hierarchical feature constraint (HFC) as an\nadd-on to existing adversarial attacks, which encourages the hiding of the\nadversarial representation within the normal feature distribution. We evaluate\nthe proposed method on two public medical image datasets, namely {Fundoscopy}\nand {Chest X-Ray}. Experimental results demonstrate the superiority of our\nadversarial attack method as it bypasses an array of state-of-the-art\nadversarial detectors more easily than competing attack methods, supporting\nthat the great vulnerability of medical features allows an attacker more room\nto manipulate the adversarial representations.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 11:00:02 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 03:05:11 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Yao", "Qingsong", ""], ["He", "Zecheng", ""], ["Lin", "Yi", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2012.09503", "submitter": "David Nilsson", "authors": "David Nilsson, Aleksis Pirinen, Erik G\\\"artner, Cristian Sminchisescu", "title": "Embodied Visual Active Learning for Semantic Segmentation", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study the task of embodied visual active learning, where an agent is set\nto explore a 3d environment with the goal to acquire visual scene understanding\nby actively selecting views for which to request annotation. While accurate on\nsome benchmarks, today's deep visual recognition pipelines tend to not\ngeneralize well in certain real-world scenarios, or for unusual viewpoints.\nRobotic perception, in turn, requires the capability to refine the recognition\ncapabilities for the conditions where the mobile system operates, including\ncluttered indoor environments or poor illumination. This motivates the proposed\ntask, where an agent is placed in a novel environment with the objective of\nimproving its visual recognition capability. To study embodied visual active\nlearning, we develop a battery of agents - both learnt and pre-specified - and\nwith different levels of knowledge of the environment. The agents are equipped\nwith a semantic segmentation network and seek to acquire informative views,\nmove and explore in order to propagate annotations in the neighbourhood of\nthose views, then refine the underlying segmentation network by online\nretraining. The trainable method uses deep reinforcement learning with a reward\nfunction that balances two competing objectives: task performance, represented\nas visual recognition accuracy, which requires exploring the environment, and\nthe necessary amount of annotated data requested during active exploration. We\nextensively evaluate the proposed models using the photorealistic Matterport3D\nsimulator and show that a fully learnt method outperforms comparable\npre-specified counterparts, even when requesting fewer annotations.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 11:02:34 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Nilsson", "David", ""], ["Pirinen", "Aleksis", ""], ["G\u00e4rtner", "Erik", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "2012.09542", "submitter": "Novanto Yudistira", "authors": "Novanto Yudistira, Muthu Subash Kavitha, Takio Kurita", "title": "Weakly-Supervised Action Localization and Action Recognition using\n  Global-Local Attention of 3D CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D Convolutional Neural Network (3D CNN) captures spatial and temporal\ninformation on 3D data such as video sequences. However, due to the convolution\nand pooling mechanism, the information loss seems unavoidable. To improve the\nvisual explanations and classification in 3D CNN, we propose two approaches; i)\naggregate layer-wise global to local (global-local) discrete gradients using\ntrained 3DResNext network, and ii) implement attention gating network to\nimprove the accuracy of the action recognition. The proposed approach intends\nto show the usefulness of every layer termed as global-local attention in 3D\nCNN via visual attribution, weakly-supervised action localization, and action\nrecognition. Firstly, the 3DResNext is trained and applied for action\nclassification using backpropagation concerning the maximum predicted class.\nThe gradients and activations of every layer are then up-sampled. Later,\naggregation is used to produce more nuanced attention, which points out the\nmost critical part of the predicted class's input videos. We use contour\nthresholding of final attention for final localization. We evaluate spatial and\ntemporal action localization in trimmed videos using fine-grained visual\nexplanation via 3DCam. Experimental results show that the proposed approach\nproduces informative visual explanations and discriminative attention.\nFurthermore, the action recognition via attention gating on each layer produces\nbetter classification results than the baseline model.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 12:29:16 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Yudistira", "Novanto", ""], ["Kavitha", "Muthu Subash", ""], ["Kurita", "Takio", ""]]}, {"id": "2012.09550", "submitter": "Yaojun Wu", "authors": "Yaojun Wu, Xin Li, Zhizheng Zhang, Xin Jin, Zhibo Chen", "title": "Learned Block-based Hybrid Image Compression", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on learned image compression perform encoding and decoding\nprocesses in a full-resolution manner, resulting in two problems when deployed\nfor practical applications. First, parallel acceleration of the autoregressive\nentropy model cannot be achieved due to serial decoding. Second,\nfull-resolution inference often causes the out-of-memory(OOM) problem with\nlimited GPU resources, especially for high-resolution images. Block partition\nis a good design choice to handle the above issues, but it brings about new\nchallenges in reducing the redundancy between blocks and eliminating block\neffects. To tackle the above challenges, this paper provides a learned\nblock-based hybrid image compression (LBHIC) framework. Specifically, we\nintroduce explicit intra prediction into a learned image compression framework\nto utilize the relation among adjacent blocks. Superior to context modeling by\nlinear weighting of neighbor pixels in traditional codecs, we propose a\ncontextual prediction module (CPM) to better capture long-range correlations by\nutilizing the strip pooling to extract the most relevant information in\nneighboring latent space, thus achieving effective information prediction.\nMoreover, to alleviate blocking artifacts, we further propose a boundary-aware\npostprocessing module (BPM) with the edge importance taken into account.\nExtensive experiments demonstrate that the proposed LBHIC codec outperforms the\nVVC, with a bit-rate conservation of 4.1%, and reduces the decoding time by\napproximately 86.7% compared with that of state-of-the-art learned image\ncompression methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 12:47:39 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 14:24:06 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2021 11:34:29 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wu", "Yaojun", ""], ["Li", "Xin", ""], ["Zhang", "Zhizheng", ""], ["Jin", "Xin", ""], ["Chen", "Zhibo", ""]]}, {"id": "2012.09571", "submitter": "Eric Lopez-Lopez", "authors": "Eric Lopez-Lopez, Carlos V. Regueiro, Xose M. Pardo", "title": "Incremental Learning from Low-labelled Stream Data in Open-Set Video\n  Face Recognition", "comments": "17 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep Learning approaches have brought solutions, with impressive performance,\nto general classification problems where wealthy of annotated data are provided\nfor training. In contrast, less progress has been made in continual learning of\na set of non-stationary classes, mainly when applied to unsupervised problems\nwith streaming data.\n  Here, we propose a novel incremental learning approach which combines a deep\nfeatures encoder with an Open-Set Dynamic Ensembles of SVM, to tackle the\nproblem of identifying individuals of interest (IoI) from streaming face data.\nFrom a simple weak classifier trained on a few video-frames, our method can use\nunsupervised operational data to enhance recognition. Our approach adapts to\nnew patterns avoiding catastrophic forgetting and partially heals itself from\nmiss-adaptation. Besides, to better comply with real world conditions, the\nsystem was designed to operate in an open-set setting. Results show a benefit\nof up to 15% F1-score increase respect to non-adaptive state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 13:28:13 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Lopez-Lopez", "Eric", ""], ["Regueiro", "Carlos V.", ""], ["Pardo", "Xose M.", ""]]}, {"id": "2012.09573", "submitter": "L\\'eo Maczyta", "authors": "L. Maczyta, P. Bouthemy and O. Le Meur", "title": "Trajectory saliency detection using consistency-oriented latent codes\n  from a recurrent auto-encoder", "comments": null, "journal-ref": null, "doi": "10.1109/TCSVT.2021.3078804", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are concerned with the detection of progressive dynamic\nsaliency from video sequences. More precisely, we are interested in saliency\nrelated to motion and likely to appear progressively over time. It can be\nrelevant to trigger alarms, to dedicate additional processing or to detect\nspecific events. Trajectories represent the best way to support progressive\ndynamic saliency detection. Accordingly, we will talk about trajectory\nsaliency. A trajectory will be qualified as salient if it deviates from normal\ntrajectories that share a common motion pattern related to a given context.\nFirst, we need a compact while discriminative representation of trajectories.\nWe adopt a (nearly) unsupervised learning-based approach. The latent code\nestimated by a recurrent auto-encoder provides the desired representation. In\naddition, we enforce consistency for normal (similar) trajectories through the\nauto-encoder loss function. The distance of the trajectory code to a prototype\ncode accounting for normality is the means to detect salient trajectories. We\nvalidate our trajectory saliency detection method on synthetic and real\ntrajectory datasets, and highlight the contributions of its different\ncomponents. We show that our method outperforms existing methods on several\nscenarios drawn from the publicly available dataset of pedestrian trajectories\nacquired in a railway station (Alahi 2014).\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 13:29:11 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 12:28:40 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Maczyta", "L.", ""], ["Bouthemy", "P.", ""], ["Meur", "O. Le", ""]]}, {"id": "2012.09607", "submitter": "Sadeep Jayasumana", "authors": "Sadeep Jayasumana, Srikumar Ramalingam, Sanjiv Kumar", "title": "Kernelized Classification in Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a kernelized classification layer for deep networks. Although\nconventional deep networks introduce an abundance of nonlinearity for\nrepresentation (feature) learning, they almost universally use a linear\nclassifier on the learned feature vectors. We advocate a nonlinear\nclassification layer by using the kernel trick on the softmax cross-entropy\nloss function during training and the scorer function during testing. However,\nthe choice of the kernel remains a challenge. To tackle this, we theoretically\nshow the possibility of optimizing over all possible positive definite kernels\napplicable to our problem setting. This theory is then used to device a new\nkernelized classification layer that learns the optimal kernel function for a\ngiven problem automatically within the deep network itself. We show the\nusefulness of the proposed nonlinear classification layer on several datasets\nand tasks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 21:43:19 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 21:41:28 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Jayasumana", "Sadeep", ""], ["Ramalingam", "Srikumar", ""], ["Kumar", "Sanjiv", ""]]}, {"id": "2012.09654", "submitter": "Jennifer Hobbs", "authors": "Saba Dadsetan, Gisele Rose, Naira Hovakimyan, Jennifer Hobbs", "title": "Detection and Prediction of Nutrient Deficiency Stress using\n  Longitudinal Aerial Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early, precise detection of nutrient deficiency stress (NDS) has key economic\nas well as environmental impact; precision application of chemicals in place of\nblanket application reduces operational costs for the growers while reducing\nthe amount of chemicals which may enter the environment unnecessarily.\nFurthermore, earlier treatment reduces the amount of loss and therefore boosts\ncrop production during a given season. With this in mind, we collect sequences\nof high-resolution aerial imagery and construct semantic segmentation models to\ndetect and predict NDS across the field. Our work sits at the intersection of\nagriculture, remote sensing, and modern computer vision and deep learning.\nFirst, we establish a baseline for full-field detection of NDS and quantify the\nimpact of pretraining, backbone architecture, input representation, and\nsampling strategy. We then quantify the amount of information available at\ndifferent points in the season by building a single-timestamp model based on a\nUNet. Next, we construct our proposed spatiotemporal architecture, which\ncombines a UNet with a convolutional LSTM layer, to accurately detect regions\nof the field showing NDS; this approach has an impressive IOU score of 0.53.\nFinally, we show that this architecture can be trained to predict regions of\nthe field which are expected to show NDS in a later flight -- potentially more\nthan three weeks in the future -- maintaining an IOU score of 0.47-0.51\ndepending on how far in advance the prediction is made. We will also release a\ndataset which we believe will benefit the computer vision, remote sensing, as\nwell as agriculture fields. This work contributes to the recent developments in\ndeep learning for remote sensing and agriculture, while addressing a key social\nchallenge with implications for economics and sustainability.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 15:06:15 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Dadsetan", "Saba", ""], ["Rose", "Gisele", ""], ["Hovakimyan", "Naira", ""], ["Hobbs", "Jennifer", ""]]}, {"id": "2012.09662", "submitter": "Fabio Massimo Zennaro", "authors": "Alexander Egiazarov, Fabio Massimo Zennaro, Vasileios Mavroeidis", "title": "Firearm Detection via Convolutional Neural Networks: Comparing a\n  Semantic Segmentation Model Against End-to-End Solutions", "comments": "10 pages, 5 figures, presented at CyberHunt workshop at IEEE Big Data\n  Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Threat detection of weapons and aggressive behavior from live video can be\nused for rapid detection and prevention of potentially deadly incidents such as\nterrorism, general criminal offences, or even domestic violence. One way for\nachieving this is through the use of artificial intelligence and, in\nparticular, machine learning for image analysis. In this paper we conduct a\ncomparison between a traditional monolithic end-to-end deep learning model and\na previously proposed model based on an ensemble of simpler neural networks\ndetecting fire-weapons via semantic segmentation. We evaluated both models from\ndifferent points of view, including accuracy, computational and data\ncomplexity, flexibility and reliability. Our results show that a semantic\nsegmentation model provides considerable amount of flexibility and resilience\nin the low data environment compared to classical deep model models, although\nits configuration and tuning presents a challenge in achieving the same levels\nof accuracy as an end-to-end model.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 15:19:29 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Egiazarov", "Alexander", ""], ["Zennaro", "Fabio Massimo", ""], ["Mavroeidis", "Vasileios", ""]]}, {"id": "2012.09666", "submitter": "H Jacinto", "authors": "Luka Daoud, Muhammad Kamran Latif, H S. Jacinto, Nader Rafla", "title": "A fully pipelined FPGA accelerator for scale invariant feature transform\n  keypoint descriptor matching,", "comments": null, "journal-ref": "Microprocessors and Microsystems 72 (2020): 102919", "doi": "10.1016/j.micpro.2019.102919", "report-no": null, "categories": "cs.CV cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The scale invariant feature transform (SIFT) algorithm is considered a\nclassical feature extraction algorithm within the field of computer vision.\nSIFT keypoint descriptor matching is a computationally intensive process due to\nthe amount of data consumed. In this work, we designed a novel fully pipelined\nhardware accelerator architecture for SIFT keypoint descriptor matching. The\naccelerator core was implemented and tested on a field programmable gate array\n(FPGA). The proposed hardware architecture is able to properly handle the\nmemory bandwidth necessary for a fully-pipelined implementation and hits the\nroofline performance model, achieving the potential maximum throughput. The\nfully pipelined matching architecture was designed based on the consine angle\ndistance method. Our architecture was optimized for 16-bit fixed-point\noperations and implemented on hardware using a Xilinx Zynq-based FPGA\ndevelopment board. Our proposed architecture shows a noticeable reduction of\narea resources compared with its counterparts in literature, while maintaining\nhigh throughput by alleviating memory bandwidth restrictions. The results show\na reduction in consumed device resources of up to 91 percent in LUTs and 79\npercent of BRAMs. Our hardware implementation is 15.7 times faster than the\ncomparable software approach.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 15:29:41 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Daoud", "Luka", ""], ["Latif", "Muhammad Kamran", ""], ["Jacinto", "H S.", ""], ["Rafla", "Nader", ""]]}, {"id": "2012.09667", "submitter": "Sadique Adnan Siddiqui", "authors": "Sadique Adnan Siddiqui, Axel Vierling and Karsten Berns", "title": "Multi-Modal Depth Estimation Using Convolutional Neural Networks", "comments": "submitted to IEEE International Symposium on Safety, Security, and\n  Rescue Robotics (SSRR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of dense depth predictions from sparse\ndistance sensor data and a single camera image on challenging weather\nconditions. This work explores the significance of different sensor modalities\nsuch as camera, Radar, and Lidar for estimating depth by applying Deep Learning\napproaches. Although Lidar has higher depth-sensing abilities than Radar and\nhas been integrated with camera images in lots of previous works, depth\nestimation using CNN's on the fusion of robust Radar distance data and camera\nimages has not been explored much. In this work, a deep regression network is\nproposed utilizing a transfer learning approach consisting of an encoder where\na high performing pre-trained model has been used to initialize it for\nextracting dense features and a decoder for upsampling and predicting desired\ndepth. The results are demonstrated on Nuscenes, KITTI, and a Synthetic dataset\nwhich was created using the CARLA simulator. Also, top-view zoom-camera images\ncaptured from the crane on a construction site are evaluated to estimate the\ndistance of the crane boom carrying heavy loads from the ground to show the\nusability in safety-critical applications.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 15:31:49 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Siddiqui", "Sadique Adnan", ""], ["Vierling", "Axel", ""], ["Berns", "Karsten", ""]]}, {"id": "2012.09673", "submitter": "Ricard Durall Lopez", "authors": "Ricard Durall, Avraam Chatzimichailidis, Peter Labus and Janis Keuper", "title": "Combating Mode Collapse in GAN training: An Empirical Analysis using\n  Hessian Eigenvalues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative adversarial networks (GANs) provide state-of-the-art results in\nimage generation. However, despite being so powerful, they still remain very\nchallenging to train. This is in particular caused by their highly non-convex\noptimization space leading to a number of instabilities. Among them, mode\ncollapse stands out as one of the most daunting ones. This undesirable event\noccurs when the model can only fit a few modes of the data distribution, while\nignoring the majority of them. In this work, we combat mode collapse using\nsecond-order gradient information. To do so, we analyse the loss surface\nthrough its Hessian eigenvalues, and show that mode collapse is related to the\nconvergence towards sharp minima. In particular, we observe how the eigenvalues\nof the $G$ are directly correlated with the occurrence of mode collapse.\nFinally, motivated by these findings, we design a new optimization algorithm\ncalled nudged-Adam (NuGAN) that uses spectral information to overcome mode\ncollapse, leading to empirically more stable convergence properties.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 15:40:27 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Durall", "Ricard", ""], ["Chatzimichailidis", "Avraam", ""], ["Labus", "Peter", ""], ["Keuper", "Janis", ""]]}, {"id": "2012.09688", "submitter": "Meng-Hao Guo", "authors": "Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R.\n  Martin and Shi-Min Hu", "title": "PCT: Point cloud transformer", "comments": "11 pages, 5 figures", "journal-ref": "Computational Visual Media, 2021, Vol. 7, No. 2, Pages: 187 - 199", "doi": "10.1007/s41095-021-0229-5", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The irregular domain and lack of ordering make it challenging to design deep\nneural networks for point cloud processing. This paper presents a novel\nframework named Point Cloud Transformer(PCT) for point cloud learning. PCT is\nbased on Transformer, which achieves huge success in natural language\nprocessing and displays great potential in image processing. It is inherently\npermutation invariant for processing a sequence of points, making it\nwell-suited for point cloud learning. To better capture local context within\nthe point cloud, we enhance input embedding with the support of farthest point\nsampling and nearest neighbor search. Extensive experiments demonstrate that\nthe PCT achieves the state-of-the-art performance on shape classification, part\nsegmentation and normal estimation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 15:55:17 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 11:12:08 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 04:36:37 GMT"}, {"version": "v4", "created": "Mon, 7 Jun 2021 02:31:47 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Guo", "Meng-Hao", ""], ["Cai", "Jun-Xiong", ""], ["Liu", "Zheng-Ning", ""], ["Mu", "Tai-Jiang", ""], ["Martin", "Ralph R.", ""], ["Hu", "Shi-Min", ""]]}, {"id": "2012.09700", "submitter": "Xuanhong Chen", "authors": "Xuanhong Chen, Kairui Feng, Naiyuan Liu, Yifan Lu, Zhengyan Tong,\n  Bingbing Ni, Ziang Liu, Ning Lin", "title": "RainNet: A Large-Scale Dataset for Spatial Precipitation Downscaling", "comments": "Submit to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spatial Precipitation Downscaling is one of the most important problems in\nthe geo-science community. However, it still remains an unaddressed issue. Deep\nlearning is a promising potential solution for downscaling. In order to\nfacilitate the research on precipitation downscaling for deep learning, we\npresent the first REAL (non-simulated) Large-Scale Spatial Precipitation\nDownscaling Dataset, RainNet, which contains 62,424 pairs of low-resolution and\nhigh-resolution precipitation maps for 17 years. Contrary to simulated data,\nthis real dataset covers various types of real meteorological phenomena (e.g.,\nHurricane, Squall, etc.), and shows the physical characters - Temporal\nMisalignment, Temporal Sparse and Fluid Properties - that challenge the\ndownscaling algorithms. In order to fully explore potential downscaling\nsolutions, we propose an implicit physical estimation framework to learn the\nabove characteristics. Eight metrics specifically considering the physical\nproperty of the data set are raised, while fourteen models are evaluated on the\nproposed dataset. Finally, we analyze the effectiveness and feasibility of\nthese models on precipitation downscaling task. The Dataset and Code will be\navailable at https://neuralchen.github.io/RainNet/.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 16:12:17 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 03:22:57 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Chen", "Xuanhong", ""], ["Feng", "Kairui", ""], ["Liu", "Naiyuan", ""], ["Lu", "Yifan", ""], ["Tong", "Zhengyan", ""], ["Ni", "Bingbing", ""], ["Liu", "Ziang", ""], ["Lin", "Ning", ""]]}, {"id": "2012.09708", "submitter": "Harshit Rampal", "authors": "Harshit Rampal, Aman Mohanty", "title": "Efficient CNN-LSTM based Image Captioning using Neural Network\n  Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Neural Networks are eminent in achieving state of the art performance\non tasks under Computer Vision, Natural Language Processing and related\nverticals. However, they are notorious for their voracious memory and compute\nappetite which further obstructs their deployment on resource limited edge\ndevices. In order to achieve edge deployment, researchers have developed\npruning and quantization algorithms to compress such networks without\ncompromising their efficacy. Such compression algorithms are broadly\nexperimented on standalone CNN and RNN architectures while in this work, we\npresent an unconventional end to end compression pipeline of a CNN-LSTM based\nImage Captioning model. The model is trained using VGG16 or ResNet50 as an\nencoder and an LSTM decoder on the flickr8k dataset. We then examine the\neffects of different compression architectures on the model and design a\ncompression architecture that achieves a 73.1% reduction in model size, 71.3%\nreduction in inference time and a 7.7% increase in BLEU score as compared to\nits uncompressed counterpart.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 16:25:09 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Rampal", "Harshit", ""], ["Mohanty", "Aman", ""]]}, {"id": "2012.09719", "submitter": "Michael Kagan", "authors": "Michael Kagan", "title": "Image-Based Jet Analysis", "comments": "To appear in Artificial Intelligence for High Energy Physics, World\n  Scientific Publishing", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.CV cs.LG hep-ex hep-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image-based jet analysis is built upon the jet image representation of jets\nthat enables a direct connection between high energy physics and the fields of\ncomputer vision and deep learning. Through this connection, a wide array of new\njet analysis techniques have emerged. In this text, we survey jet image based\nclassification models, built primarily on the use of convolutional neural\nnetworks, examine the methods to understand what these models have learned and\nwhat is their sensitivity to uncertainties, and review the recent successes in\nmoving these models from phenomenological studies to real world application on\nexperiments at the LHC. Beyond jet classification, several other applications\nof jet image based techniques, including energy estimation, pileup noise\nreduction, data generation, and anomaly detection, are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 16:42:29 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 14:34:34 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Kagan", "Michael", ""]]}, {"id": "2012.09732", "submitter": "Daniel Yarnell", "authors": "Daniel Yarnell, Xian Wang", "title": "Robust Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated captioning of photos is a mission that incorporates the\ndifficulties of photo analysis and text generation. One essential feature of\ncaptioning is the concept of attention: how to determine what to specify and in\nwhich sequence. In this study, we leverage the Object Relation using\nadversarial robust cut algorithm, that grows upon this method by specifically\nembedding knowledge about the spatial association between input data through\ngraph representation. Our experimental study represent the promising\nperformance of our proposed method for image captioning.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 00:33:17 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Yarnell", "Daniel", ""], ["Wang", "Xian", ""]]}, {"id": "2012.09742", "submitter": "Xinxin Zhu", "authors": "Xinxin Zhu and Weining Wang and Longteng Guo and Jing Liu", "title": "AutoCaption: Image Captioning with Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image captioning transforms complex visual information into abstract natural\nlanguage for representation, which can help computers understanding the world\nquickly. However, due to the complexity of the real environment, it needs to\nidentify key objects and realize their connections, and further generate\nnatural language. The whole process involves a visual understanding module and\na language generation module, which brings more challenges to the design of\ndeep neural networks than other tasks. Neural Architecture Search (NAS) has\nshown its important role in a variety of image recognition tasks. Besides, RNN\nplays an essential role in the image captioning task. We introduce a\nAutoCaption method to better design the decoder module of the image captioning\nwhere we use the NAS to design the decoder module called AutoRNN automatically.\nWe use the reinforcement learning method based on shared parameters for\nautomatic design the AutoRNN efficiently. The search space of the AutoCaption\nincludes connections between the layers and the operations in layers both, and\nit can make AutoRNN express more architectures. In particular, RNN is\nequivalent to a subset of our search space. Experiments on the MSCOCO datasets\nshow that our AutoCaption model can achieve better performance than traditional\nhand-design methods. Our AutoCaption obtains the best published CIDEr\nperformance of 135.8% on COCO Karpathy test split. When further using ensemble\ntechnology, CIDEr is boosted up to 139.5%.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 18:15:27 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Zhu", "Xinxin", ""], ["Wang", "Weining", ""], ["Guo", "Longteng", ""], ["Liu", "Jing", ""]]}, {"id": "2012.09743", "submitter": "Romain Cosentino Mr", "authors": "Romain Cosentino, Randall Balestriero, Yanis Bahroun, Anirvan\n  Sengupta, Richard Baraniuk, Behnaam Aazhang", "title": "Interpretable Image Clustering via Diffeomorphism-Aware K-Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We design an interpretable clustering algorithm aware of the nonlinear\nstructure of image manifolds. Our approach leverages the interpretability of\n$K$-means applied in the image space while addressing its clustering\nperformance issues. Specifically, we develop a measure of similarity between\nimages and centroids that encompasses a general class of deformations:\ndiffeomorphisms, rendering the clustering invariant to them. Our work leverages\nthe Thin-Plate Spline interpolation technique to efficiently learn\ndiffeomorphisms best characterizing the image manifolds. Extensive numerical\nsimulations show that our approach competes with state-of-the-art methods on\nvarious datasets.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 16:11:39 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Cosentino", "Romain", ""], ["Balestriero", "Randall", ""], ["Bahroun", "Yanis", ""], ["Sengupta", "Anirvan", ""], ["Baraniuk", "Richard", ""], ["Aazhang", "Behnaam", ""]]}, {"id": "2012.09755", "submitter": "Michael Girard", "authors": "Satish K. Panda, Haris Cheong, Tin A. Tun, Sripad K. Devella,\n  Ramaswami Krishnadas, Martin L. Buist, Shamira Perera, Ching-Yu Cheng, Tin\n  Aung, Alexandre H. Thi\\'ery, and Micha\\\"el J. A. Girard", "title": "Describing the Structural Phenotype of the Glaucomatous Optic Nerve Head\n  Using Artificial Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optic nerve head (ONH) typically experiences complex neural- and\nconnective-tissue structural changes with the development and progression of\nglaucoma, and monitoring these changes could be critical for improved diagnosis\nand prognosis in the glaucoma clinic. The gold-standard technique to assess\nstructural changes of the ONH clinically is optical coherence tomography (OCT).\nHowever, OCT is limited to the measurement of a few hand-engineered parameters,\nsuch as the thickness of the retinal nerve fiber layer (RNFL), and has not yet\nbeen qualified as a stand-alone device for glaucoma diagnosis and prognosis\napplications. We argue this is because the vast amount of information available\nin a 3D OCT scan of the ONH has not been fully exploited. In this study we\npropose a deep learning approach that can: \\textbf{(1)} fully exploit\ninformation from an OCT scan of the ONH; \\textbf{(2)} describe the structural\nphenotype of the glaucomatous ONH; and that can \\textbf{(3)} be used as a\nrobust glaucoma diagnosis tool. Specifically, the structural features\nidentified by our algorithm were found to be related to clinical observations\nof glaucoma. The diagnostic accuracy from these structural features was $92.0\n\\pm 2.3 \\%$ with a sensitivity of $90.0 \\pm 2.4 \\% $ (at $95 \\%$ specificity).\nBy changing their magnitudes in steps, we were able to reveal how the\nmorphology of the ONH changes as one transitions from a `non-glaucoma' to a\n`glaucoma' condition. We believe our work may have strong clinical implication\nfor our understanding of glaucoma pathogenesis, and could be improved in the\nfuture to also predict future loss of vision.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 17:15:30 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Panda", "Satish K.", ""], ["Cheong", "Haris", ""], ["Tun", "Tin A.", ""], ["Devella", "Sripad K.", ""], ["Krishnadas", "Ramaswami", ""], ["Buist", "Martin L.", ""], ["Perera", "Shamira", ""], ["Cheng", "Ching-Yu", ""], ["Aung", "Tin", ""], ["Thi\u00e9ry", "Alexandre H.", ""], ["Girard", "Micha\u00ebl J. A.", ""]]}, {"id": "2012.09760", "submitter": "Kevin Lin", "authors": "Kevin Lin, Lijuan Wang, Zicheng Liu", "title": "End-to-End Human Pose and Mesh Reconstruction with Transformers", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method, called MEsh TRansfOrmer (METRO), to reconstruct 3D\nhuman pose and mesh vertices from a single image. Our method uses a transformer\nencoder to jointly model vertex-vertex and vertex-joint interactions, and\noutputs 3D joint coordinates and mesh vertices simultaneously. Compared to\nexisting techniques that regress pose and shape parameters, METRO does not rely\non any parametric mesh models like SMPL, thus it can be easily extended to\nother objects such as hands. We further relax the mesh topology and allow the\ntransformer self-attention mechanism to freely attend between any two vertices,\nmaking it possible to learn non-local relationships among mesh vertices and\njoints. With the proposed masked vertex modeling, our method is more robust and\neffective in handling challenging situations like partial occlusions. METRO\ngenerates new state-of-the-art results for human mesh reconstruction on the\npublic Human3.6M and 3DPW datasets. Moreover, we demonstrate the\ngeneralizability of METRO to 3D hand reconstruction in the wild, outperforming\nexisting state-of-the-art methods on FreiHAND dataset. Code and pre-trained\nmodels are available at https://github.com/microsoft/MeshTransformer.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 17:17:29 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 01:20:42 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 15:56:07 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Lin", "Kevin", ""], ["Wang", "Lijuan", ""], ["Liu", "Zicheng", ""]]}, {"id": "2012.09771", "submitter": "Aleksei Shpilman", "authors": "Vladislav Belyaev, Aleksandra Malysheva, Aleksei Shpilman", "title": "End-to-end Deep Object Tracking with Circular Loss Function for Rotated\n  Bounding Box", "comments": null, "journal-ref": null, "doi": "10.1109/REDUNDANCY48165.2019.9003330", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task object tracking is vital in numerous applications such as autonomous\ndriving, intelligent surveillance, robotics, etc. This task entails the\nassigning of a bounding box to an object in a video stream, given only the\nbounding box for that object on the first frame. In 2015, a new type of video\nobject tracking (VOT) dataset was created that introduced rotated bounding\nboxes as an extension of axis-aligned ones. In this work, we introduce a novel\nend-to-end deep learning method based on the Transformer Multi-Head Attention\narchitecture. We also present a new type of loss function, which takes into\naccount the bounding box overlap and orientation.\n  Our Deep Object Tracking model with Circular Loss Function (DOTCL) shows an\nconsiderable improvement in terms of robustness over current state-of-the-art\nend-to-end deep learning models. It also outperforms state-of-the-art object\ntracking methods on VOT2018 dataset in terms of expected average overlap (EAO)\nmetric.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 17:29:29 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Belyaev", "Vladislav", ""], ["Malysheva", "Aleksandra", ""], ["Shpilman", "Aleksei", ""]]}, {"id": "2012.09790", "submitter": "Yilun Du", "authors": "Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B. Tenenbaum, Jiajun Wu", "title": "Neural Radiance Flow for 4D View Synthesis and Video Processing", "comments": "Website: https://yilundu.github.io/nerflow/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method, Neural Radiance Flow (NeRFlow),to learn a 4D\nspatial-temporal representation of a dynamic scene from a set of RGB images.\nKey to our approach is the use of a neural implicit representation that learns\nto capture the 3D occupancy, radiance, and dynamics of the scene. By enforcing\nconsistency across different modalities, our representation enables multi-view\nrendering in diverse dynamic scenes, including water pouring, robotic\ninteraction, and real images, outperforming state-of-the-art methods for\nspatial-temporal view synthesis. Our approach works even when inputs images are\ncaptured with only one camera. We further demonstrate that the learned\nrepresentation can serve as an implicit scene prior, enabling video processing\ntasks such as image super-resolution and de-noising without any additional\nsupervision.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 17:54:32 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Du", "Yilun", ""], ["Zhang", "Yinan", ""], ["Yu", "Hong-Xing", ""], ["Tenenbaum", "Joshua B.", ""], ["Wu", "Jiajun", ""]]}, {"id": "2012.09793", "submitter": "Xinpeng Wang", "authors": "Xinpeng Wang, Chandan Yeshwanth, Matthias Nie{\\ss}ner", "title": "SceneFormer: Indoor Scene Generation with Transformers", "comments": "Video:https://youtu.be/gSClrMpmtn4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the task of indoor scene generation by generating a sequence of\nobjects, along with their locations and orientations conditioned on a room\nlayout. Large-scale indoor scene datasets allow us to extract patterns from\nuser-designed indoor scenes, and generate new scenes based on these patterns.\nExisting methods rely on the 2D or 3D appearance of these scenes in addition to\nobject positions, and make assumptions about the possible relations between\nobjects. In contrast, we do not use any appearance information, and implicitly\nlearn object relations using the self-attention mechanism of transformers. We\nshow that our model design leads to faster scene generation with similar or\nimproved levels of realism compared to previous methods. Our method is also\nflexible, as it can be conditioned not only on the room layout but also on text\ndescriptions of the room, using only the cross-attention mechanism of\ntransformers. Our user study shows that our generated scenes are preferred to\nthe state-of-the-art FastSynth scenes 53.9% and 56.7% of the time for bedroom\nand living room scenes, respectively. At the same time, we generate a scene in\n1.48 seconds on average, 20% faster than FastSynth.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 17:57:27 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 08:34:36 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Wang", "Xinpeng", ""], ["Yeshwanth", "Chandan", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2012.09810", "submitter": "Alexander Watson Mr", "authors": "Alexander Watson", "title": "Deep Learning Techniques for Super-Resolution in Video Games", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational cost of video game graphics is increasing and hardware for\nprocessing graphics is struggling to keep up. This means that computer\nscientists need to develop creative new ways to improve the performance of\ngraphical processing hardware. Deep learning techniques for video\nsuper-resolution can enable video games to have high quality graphics whilst\noffsetting much of the computational cost. These emerging technologies allow\nconsumers to have improved performance and enjoyment from video games and have\nthe potential to become standard within the game development industry.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:22:05 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Watson", "Alexander", ""]]}, {"id": "2012.09811", "submitter": "Xiaolong Wang", "authors": "Qiang Zhang, Tete Xiao, Alexei A. Efros, Lerrel Pinto, Xiaolong Wang", "title": "Learning Cross-Domain Correspondence for Control with Dynamics\n  Cycle-Consistency", "comments": "Project page: https://sjtuzq.github.io/cycle_dynamics.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the heart of many robotics problems is the challenge of learning\ncorrespondences across domains. For instance, imitation learning requires\nobtaining correspondence between humans and robots; sim-to-real requires\ncorrespondence between physics simulators and the real world; transfer learning\nrequires correspondences between different robotics environments. This paper\naims to learn correspondence across domains differing in representation (vision\nvs. internal state), physics parameters (mass and friction), and morphology\n(number of limbs). Importantly, correspondences are learned using unpaired and\nrandomly collected data from the two domains. We propose \\textit{dynamics\ncycles} that align dynamic robot behavior across two domains using a\ncycle-consistency constraint. Once this correspondence is found, we can\ndirectly transfer the policy trained on one domain to the other, without\nneeding any additional fine-tuning on the second domain. We perform experiments\nacross a variety of problem domains, both in simulation and on real robot. Our\nframework is able to align uncalibrated monocular video of a real robot arm to\ndynamic state-action trajectories of a simulated arm without paired data. Video\ndemonstrations of our results are available at:\nhttps://sjtuzq.github.io/cycle_dynamics.html .\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:22:25 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Zhang", "Qiang", ""], ["Xiao", "Tete", ""], ["Efros", "Alexei A.", ""], ["Pinto", "Lerrel", ""], ["Wang", "Xiaolong", ""]]}, {"id": "2012.09831", "submitter": "Luca Bertinetto", "authors": "Steinar Laenen and Luca Bertinetto", "title": "On Episodes, Prototypical Networks, and Few-shot Learning", "comments": "19 pages. A preliminary version of this work appeared as an oral\n  presentation at NeurIPS 2020 meta-learning workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Episodic learning is a popular practice among researchers and practitioners\ninterested in few-shot learning. It consists of organising training in a series\nof learning problems, each relying on small \"support\" and \"query\" sets to mimic\nthe few-shot circumstances encountered during evaluation. In this paper, we\ninvestigate the usefulness of episodic learning in Prototypical Networks and\nMatching Networks, two of the most popular algorithms making use of this\npractice. Surprisingly, in our experiments we found that, for Prototypical and\nMatching Networks, it is detrimental to use the episodic learning strategy of\nseparating training samples between support and query set, as it is a\ndata-inefficient way to exploit training batches. These \"non-episodic\"\nvariants, which are closely related to the classic Neighbourhood Component\nAnalysis, reliably improve over their episodic counterparts in multiple\ndatasets, achieving an accuracy that (in the case of Prototypical Networks) is\ncompetitive with the state-of-the-art, despite being extremely simple.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:52:47 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Laenen", "Steinar", ""], ["Bertinetto", "Luca", ""]]}, {"id": "2012.09838", "submitter": "Hila Chefer", "authors": "Hila Chefer, Shir Gur, Lior Wolf", "title": "Transformer Interpretability Beyond Attention Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-attention techniques, and specifically Transformers, are dominating the\nfield of text processing and are becoming increasingly popular in computer\nvision classification tasks. In order to visualize the parts of the image that\nled to a certain classification, existing methods either rely on the obtained\nattention maps or employ heuristic propagation along the attention graph. In\nthis work, we propose a novel way to compute relevancy for Transformer\nnetworks. The method assigns local relevance based on the Deep Taylor\nDecomposition principle and then propagates these relevancy scores through the\nlayers. This propagation involves attention layers and skip connections, which\nchallenge existing methods. Our solution is based on a specific formulation\nthat is shown to maintain the total relevancy across layers. We benchmark our\nmethod on very recent visual Transformer networks, as well as on a text\nclassification problem, and demonstrate a clear advantage over the existing\nexplainability methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:56:33 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 11:19:28 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Chefer", "Hila", ""], ["Gur", "Shir", ""], ["Wolf", "Lior", ""]]}, {"id": "2012.09841", "submitter": "Patrick Esser", "authors": "Patrick Esser and Robin Rombach and Bj\\\"orn Ommer", "title": "Taming Transformers for High-Resolution Image Synthesis", "comments": "Changelog can be found in the supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designed to learn long-range interactions on sequential data, transformers\ncontinue to show state-of-the-art results on a wide variety of tasks. In\ncontrast to CNNs, they contain no inductive bias that prioritizes local\ninteractions. This makes them expressive, but also computationally infeasible\nfor long sequences, such as high-resolution images. We demonstrate how\ncombining the effectiveness of the inductive bias of CNNs with the expressivity\nof transformers enables them to model and thereby synthesize high-resolution\nimages. We show how to (i) use CNNs to learn a context-rich vocabulary of image\nconstituents, and in turn (ii) utilize transformers to efficiently model their\ncomposition within high-resolution images. Our approach is readily applied to\nconditional synthesis tasks, where both non-spatial information, such as object\nclasses, and spatial information, such as segmentations, can control the\ngenerated image. In particular, we present the first results on\nsemantically-guided synthesis of megapixel images with transformers and obtain\nthe state of the art among autoregressive models on class-conditional ImageNet.\nCode and pretrained models can be found at\nhttps://github.com/CompVis/taming-transformers .\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:57:28 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 18:56:10 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 16:07:21 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Esser", "Patrick", ""], ["Rombach", "Robin", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "2012.09842", "submitter": "Georgi Tinchev", "authors": "Georgi Tinchev, Shuda Li, Kai Han, David Mitchell, Rigas Kouskouridas", "title": "$\\mathbb{X}$Resolution Correspondence Networks", "comments": "Preprint. Code is available at https://xyz-r-d.github.io/xrcnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim at establishing accurate dense correspondences between\na pair of images with overlapping field of view under challenging illumination\nvariation, viewpoint changes, and style differences. Through an extensive\nablation study of the state-of-the-art correspondence networks, we surprisingly\ndiscovered that the widely adopted 4D correlation tensor and its related\nlearning and processing modules could be de-parameterised and removed from\ntraining with merely a minor impact over the final matching accuracy. Disabling\nthese computational expensive modules dramatically speeds up the training\nprocedure and allows to use 4 times bigger batch size, which in turn\ncompensates for the accuracy drop. Together with a multi-GPU inference stage,\nour method facilitates the systematic investigation of the relationship between\nmatching accuracy and up-sampling resolution of the native testing images from\n1280 to 4K. This leads to discovery of the existence of an optimal resolution\n$\\mathbb{X}$ that produces accurate matching performance surpassing the\nstate-of-the-art methods particularly over the lower error band on public\nbenchmarks for the proposed network.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:57:58 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 17:27:37 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Tinchev", "Georgi", ""], ["Li", "Shuda", ""], ["Han", "Kai", ""], ["Mitchell", "David", ""], ["Kouskouridas", "Rigas", ""]]}, {"id": "2012.09843", "submitter": "Georgios Pavlakos", "authors": "Georgios Pavlakos, Jitendra Malik, Angjoo Kanazawa", "title": "Human Mesh Recovery from Multiple Shots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos from edited media like movies are a useful, yet under-explored source\nof information. The rich variety of appearance and interactions between humans\ndepicted over a large temporal context in these films could be a valuable\nsource of data. However, the richness of data comes at the expense of\nfundamental challenges such as abrupt shot changes and close up shots of actors\nwith heavy truncation, which limits the applicability of existing human 3D\nunderstanding methods. In this paper, we address these limitations with an\ninsight that while shot changes of the same scene incur a discontinuity between\nframes, the 3D structure of the scene still changes smoothly. This allows us to\nhandle frames before and after the shot change as multi-view signal that\nprovide strong cues to recover the 3D state of the actors. We propose a\nmulti-shot optimization framework, which leads to improved 3D reconstruction\nand mining of long sequences with pseudo ground truth 3D human mesh. We show\nthat the resulting data is beneficial in the training of various human mesh\nrecovery models: for single image, we achieve improved robustness; for video we\npropose a pure transformer-based temporal encoder, which can naturally handle\nmissing observations due to shot changes in the input frames. We demonstrate\nthe importance of the insight and proposed models through extensive\nexperiments. The tools we develop open the door to processing and analyzing in\n3D content from a large library of edited media, which could be helpful for\nmany downstream applications. Project page:\nhttps://geopavlakos.github.io/multishot\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:58:02 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Pavlakos", "Georgios", ""], ["Malik", "Jitendra", ""], ["Kanazawa", "Angjoo", ""]]}, {"id": "2012.09854", "submitter": "Ronghang Hu", "authors": "Ronghang Hu, Nikhila Ravi, Alex Berg, Deepak Pathak", "title": "Worldsheet: Wrapping the World in a 3D Sheet for View Synthesis from a\n  Single Image", "comments": "v2 diff: Added occlusion handling via layered Wordsheets. Webpage at\n  https://worldsheet.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Worldsheet, a method for novel view synthesis using just a single\nRGB image as input. The main insight is that simply shrink-wrapping a planar\nmesh sheet onto the input image, consistent with the learned intermediate\ndepth, captures underlying geometry sufficient to generate photorealistic\nunseen views with large viewpoint changes. To operationalize this, we propose a\nnovel differentiable texture sampler that allows our wrapped mesh sheet to be\ntextured and rendered differentiably into an image from a target viewpoint. Our\napproach is category-agnostic, end-to-end trainable without using any 3D\nsupervision, and requires a single image at test time. We also explore a simple\nextension by stacking multiple layers of Worldsheets to better handle\nocclusions. Worldsheet consistently outperforms prior state-of-the-art methods\non single-image view synthesis across several datasets. Furthermore, this\nsimple idea captures novel views surprisingly well on a wide range of\nhigh-resolution in-the-wild images, converting them into navigable 3D pop-ups.\nVideo results and code at https://worldsheet.github.io.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:59:52 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 03:46:44 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hu", "Ronghang", ""], ["Ravi", "Nikhila", ""], ["Berg", "Alex", ""], ["Pathak", "Deepak", ""]]}, {"id": "2012.09855", "submitter": "Andrew Liu", "authors": "Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah\n  Snavely, Angjoo Kanazawa", "title": "Infinite Nature: Perpetual View Generation of Natural Scenes from a\n  Single Image", "comments": "Project page at https://infinite-nature.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We introduce the problem of perpetual view generation -- long-range\ngeneration of novel views corresponding to an arbitrarily long camera\ntrajectory given a single image. This is a challenging problem that goes far\nbeyond the capabilities of current view synthesis methods, which work for a\nlimited range of viewpoints and quickly degenerate when presented with a large\ncamera motion. Methods designed for video generation also have limited ability\nto produce long video sequences and are often agnostic to scene geometry. We\ntake a hybrid approach that integrates both geometry and image synthesis in an\niterative render, refine, and repeat framework, allowing for long-range\ngeneration that cover large distances after hundreds of frames. Our approach\ncan be trained from a set of monocular video sequences without any manual\nannotation. We propose a dataset of aerial footage of natural coastal scenes,\nand compare our method with recent view synthesis and conditional video\ngeneration baselines, showing that it can generate plausible scenes for much\nlonger time horizons over large camera trajectories compared to existing\nmethods. Please visit our project page at https://infinite-nature.github.io/.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:59:57 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 05:49:19 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Liu", "Andrew", ""], ["Tucker", "Richard", ""], ["Jampani", "Varun", ""], ["Makadia", "Ameesh", ""], ["Snavely", "Noah", ""], ["Kanazawa", "Angjoo", ""]]}, {"id": "2012.09856", "submitter": "Zhe Cao", "authors": "Zhe Cao, Ilija Radosavovic, Angjoo Kanazawa, Jitendra Malik", "title": "Reconstructing Hand-Object Interactions in the Wild", "comments": "Project page: https://people.eecs.berkeley.edu/~zhecao/rhoi/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we explore reconstructing hand-object interactions in the wild.\nThe core challenge of this problem is the lack of appropriate 3D labeled data.\nTo overcome this issue, we propose an optimization-based procedure which does\nnot require direct 3D supervision. The general strategy we adopt is to exploit\nall available related data (2D bounding boxes, 2D hand keypoints, 2D instance\nmasks, 3D object models, 3D in-the-lab MoCap) to provide constraints for the 3D\nreconstruction. Rather than optimizing the hand and object individually, we\noptimize them jointly which allows us to impose additional constraints based on\nhand-object contact, collision, and occlusion. Our method produces compelling\nreconstructions on the challenging in-the-wild data from the EPIC Kitchens and\nthe 100 Days of Hands datasets, across a range of object categories.\nQuantitatively, we demonstrate that our approach compares favorably to existing\napproaches in the lab settings where ground truth 3D annotations are available.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:59:58 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Cao", "Zhe", ""], ["Radosavovic", "Ilija", ""], ["Kanazawa", "Angjoo", ""], ["Malik", "Jitendra", ""]]}, {"id": "2012.09859", "submitter": "Chengyuan Li", "authors": "Chengyuan Li, Jun Liu, Hailong Hong, Wenju Mao, Chenjie Wang, Chudi\n  Hu, Xin Su, Bin Luo", "title": "Object Detection based on OcSaFPN in Aerial Images with Noise", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking the deep learning-based algorithms into account has become a crucial\nway to boost object detection performance in aerial images. While various\nneural network representations have been developed, previous works are still\ninefficient to investigate the noise-resilient performance, especially on\naerial images with noise taken by the cameras with telephoto lenses, and most\nof the research is concentrated in the field of denoising. Of course, denoising\nusually requires an additional computational burden to obtain higher quality\nimages, while noise-resilient is more of a description of the robustness of the\nnetwork itself to different noises, which is an attribute of the algorithm\nitself. For this reason, the work will be started by analyzing the\nnoise-resilient performance of the neural network, and then propose two\nhypotheses to build a noise-resilient structure. Based on these hypotheses, we\ncompare the noise-resilient ability of the Oct-ResNet with frequency division\nprocessing and the commonly used ResNet. In addition, previous feature pyramid\nnetworks used for aerial object detection tasks are not specifically designed\nfor the frequency division feature maps of the Oct-ResNet, and they usually\nlack attention to bridging the semantic gap between diverse feature maps from\ndifferent depths. On the basis of this, a novel octave convolution-based\nsemantic attention feature pyramid network (OcSaFPN) is proposed to get higher\naccuracy in object detection with noise. The proposed algorithm tested on three\ndatasets demonstrates that the proposed OcSaFPN achieves a state-of-the-art\ndetection performance with Gaussian noise or multiplicative noise. In addition,\nmore experiments have proved that the OcSaFPN structure can be easily added to\nexisting algorithms, and the noise-resilient ability can be effectively\nimproved.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 01:28:51 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Li", "Chengyuan", ""], ["Liu", "Jun", ""], ["Hong", "Hailong", ""], ["Mao", "Wenju", ""], ["Wang", "Chenjie", ""], ["Hu", "Chudi", ""], ["Su", "Xin", ""], ["Luo", "Bin", ""]]}, {"id": "2012.09890", "submitter": "Amirhossein Dadashzadeh", "authors": "Amirhossein Dadashzadeh, Alan Whone, Michal Rolinski, Majid Mirmehdi", "title": "Exploring Motion Boundaries in an End-to-End Network for Vision-based\n  Parkinson's Severity Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating neurological disorders such as Parkinson's disease (PD) is a\nchallenging task that requires the assessment of several motor and non-motor\nfunctions. In this paper, we present an end-to-end deep learning framework to\nmeasure PD severity in two important components, hand movement and gait, of the\nUnified Parkinson's Disease Rating Scale (UPDRS). Our method leverages on an\nInflated 3D CNN trained by a temporal segment framework to learn spatial and\nlong temporal structure in video data. We also deploy a temporal attention\nmechanism to boost the performance of our model. Further, motion boundaries are\nexplored as an extra input modality to assist in obfuscating the effects of\ncamera motion for better movement assessment. We ablate the effects of\ndifferent data modalities on the accuracy of the proposed network and compare\nwith other popular architectures. We evaluate our proposed method on a dataset\nof 25 PD patients, obtaining 72.3% and 77.1% top-1 accuracy on hand movement\nand gait tasks respectively.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 19:20:17 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 17:07:52 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Dadashzadeh", "Amirhossein", ""], ["Whone", "Alan", ""], ["Rolinski", "Michal", ""], ["Mirmehdi", "Majid", ""]]}, {"id": "2012.09904", "submitter": "Hesham Mostafa", "authors": "Souvik Kundu, Hesham Mostafa, Sharath Nittur Sridhar, Sairam\n  Sundaresan", "title": "Attention-based Image Upsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional layers are an integral part of many deep neural network\nsolutions in computer vision. Recent work shows that replacing the standard\nconvolution operation with mechanisms based on self-attention leads to improved\nperformance on image classification and object detection tasks. In this work,\nwe show how attention mechanisms can be used to replace another canonical\noperation: strided transposed convolution. We term our novel attention-based\noperation attention-based upsampling since it increases/upsamples the spatial\ndimensions of the feature maps. Through experiments on single image\nsuper-resolution and joint-image upsampling tasks, we show that attention-based\nupsampling consistently outperforms traditional upsampling methods based on\nstrided transposed convolution or based on adaptive filters while using fewer\nparameters. We show that the inherent flexibility of the attention mechanism,\nwhich allows it to use separate sources for calculating the attention\ncoefficients and the attention targets, makes attention-based upsampling a\nnatural choice when fusing information from multiple image modalities.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 19:58:10 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Kundu", "Souvik", ""], ["Mostafa", "Hesham", ""], ["Sridhar", "Sharath Nittur", ""], ["Sundaresan", "Sairam", ""]]}, {"id": "2012.09945", "submitter": "Giovanni Ometto", "authors": "Giovanni Ometto, Giovanni Montesano, Usha Chakravarthy, Frank Kee,\n  Ruth E. Hogg and David P. Crabb", "title": "Fast 3-dimensional estimation of the Foveal Avascular Zone from OCTA", "comments": "6 pages, 3 figures, submitted to IEEE I2MTC 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area of the foveal avascular zone (FAZ) from en face images of optical\ncoherence tomography angiography (OCTA) is one of the most common measurement\nbased on this technology. However, its use in clinic is limited by the high\nvariation of the FAZ area across normal subjects, while the calculation of the\nvolumetric measurement of the FAZ is limited by the high noise that\ncharacterizes OCTA scans. We designed an algorithm that exploits the higher\nsignal-to-noise ratio of en face images to efficiently identify the capillary\nnetwork of the inner retina in 3-dimensions (3D), under the assumption that the\ncapillaries in separate plexuses do not overlap. The network is then processed\nwith morphological operations to identify the 3D FAZ within the bounding\nsegmentations of the inner retina. The FAZ volume and area in different\nplexuses were calculated for a dataset of 430 eyes. Then, the measurements were\nanalyzed using linear mixed effect models to identify differences between three\ngroups of eyes: healthy, diabetic without diabetic retinopathy (DR) and\ndiabetic with DR. Results showed significant differences in the FAZ volume\nbetween the different groups but not in the area measurements. These results\nsuggest that volumetric FAZ could be a better diagnostic detector than the\nplanar FAZ. The efficient methodology that we introduced could allow the fast\ncalculation of the FAZ volume in clinics, as well as providing the 3D\nsegmentation of the capillary network of the inner retina.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 21:51:13 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Ometto", "Giovanni", ""], ["Montesano", "Giovanni", ""], ["Chakravarthy", "Usha", ""], ["Kee", "Frank", ""], ["Hogg", "Ruth E.", ""], ["Crabb", "David P.", ""]]}, {"id": "2012.09950", "submitter": "Rajesh Kumar", "authors": "Rajesh Kumar and Can Isik and Vir V Phoha", "title": "Treadmill Assisted Gait Spoofing (TAGS): An Emerging Threat to wearable\n  Sensor-based Gait Authentication", "comments": "17 pages", "journal-ref": "ACM Journal of Digital Threats: Research and Practice, June 2021", "doi": "10.1145/3442151", "report-no": null, "categories": "cs.CR cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we examine the impact of Treadmill Assisted Gait Spoofing\n(TAGS) on Wearable Sensor-based Gait Authentication (WSGait). We consider more\nrealistic implementation and deployment scenarios than the previous study,\nwhich focused only on the accelerometer sensor and a fixed set of features.\nSpecifically, we consider the situations in which the implementation of WSGait\ncould be using one or more sensors embedded into modern smartphones. Besides,\nit could be using different sets of features or different classification\nalgorithms, or both. Despite the use of a variety of sensors, feature sets\n(ranked by mutual information), and six different classification algorithms,\nTAGS was able to increase the average False Accept Rate (FAR) from 4% to 26%.\nSuch a considerable increase in the average FAR, especially under the stringent\nimplementation and deployment scenarios considered in this study, calls for a\nfurther investigation into the design of evaluations of WSGait before its\ndeployment for public use.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 21:58:41 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Kumar", "Rajesh", ""], ["Isik", "Can", ""], ["Phoha", "Vir V", ""]]}, {"id": "2012.09955", "submitter": "Ziyan Wang", "authors": "Ziyan Wang, Timur Bagautdinov, Stephen Lombardi, Tomas Simon, Jason\n  Saragih, Jessica Hodgins, Michael Zollh\\\"ofer", "title": "Learning Compositional Radiance Fields of Dynamic Human Heads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photorealistic rendering of dynamic humans is an important ability for\ntelepresence systems, virtual shopping, synthetic data generation, and more.\nRecently, neural rendering methods, which combine techniques from computer\ngraphics and machine learning, have created high-fidelity models of humans and\nobjects. Some of these methods do not produce results with high-enough fidelity\nfor driveable human models (Neural Volumes) whereas others have extremely long\nrendering times (NeRF). We propose a novel compositional 3D representation that\ncombines the best of previous methods to produce both higher-resolution and\nfaster results. Our representation bridges the gap between discrete and\ncontinuous volumetric representations by combining a coarse 3D-structure-aware\ngrid of animation codes with a continuous learned scene function that maps\nevery position and its corresponding local animation code to its view-dependent\nemitted radiance and local volume density. Differentiable volume rendering is\nemployed to compute photo-realistic novel views of the human head and upper\nbody as well as to train our novel representation end-to-end using only 2D\nsupervision. In addition, we show that the learned dynamic radiance field can\nbe used to synthesize novel unseen expressions based on a global animation\ncode. Our approach achieves state-of-the-art results for synthesizing novel\nviews of dynamic human heads and the upper body.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 22:19:27 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Wang", "Ziyan", ""], ["Bagautdinov", "Timur", ""], ["Lombardi", "Stephen", ""], ["Simon", "Tomas", ""], ["Saragih", "Jason", ""], ["Hodgins", "Jessica", ""], ["Zollh\u00f6fer", "Michael", ""]]}, {"id": "2012.09958", "submitter": "Josh Beal", "authors": "Josh Beal, Eric Kim, Eric Tzeng, Dong Huk Park, Andrew Zhai, Dmitry\n  Kislyuk", "title": "Toward Transformer-Based Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have become the dominant model in natural language processing,\nowing to their ability to pretrain on massive amounts of data, then transfer to\nsmaller, more specific tasks via fine-tuning. The Vision Transformer was the\nfirst major attempt to apply a pure transformer model directly to images as\ninput, demonstrating that as compared to convolutional networks,\ntransformer-based architectures can achieve competitive results on benchmark\nclassification tasks. However, the computational complexity of the attention\noperator means that we are limited to low-resolution inputs. For more complex\ntasks such as detection or segmentation, maintaining a high input resolution is\ncrucial to ensure that models can properly identify and reflect fine details in\ntheir output. This naturally raises the question of whether or not\ntransformer-based architectures such as the Vision Transformer are capable of\nperforming tasks other than classification. In this paper, we determine that\nVision Transformers can be used as a backbone by a common detection task head\nto produce competitive COCO results. The model that we propose, ViT-FRCNN,\ndemonstrates several known properties associated with transformers, including\nlarge pretraining capacity and fast fine-tuning performance. We also\ninvestigate improvements over a standard detection backbone, including superior\nperformance on out-of-domain images, better performance on large objects, and a\nlessened reliance on non-maximum suppression. We view ViT-FRCNN as an important\nstepping stone toward a pure-transformer solution of complex vision tasks such\nas object detection.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 22:33:14 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Beal", "Josh", ""], ["Kim", "Eric", ""], ["Tzeng", "Eric", ""], ["Park", "Dong Huk", ""], ["Zhai", "Andrew", ""], ["Kislyuk", "Dmitry", ""]]}, {"id": "2012.09962", "submitter": "Tianhong Li", "authors": "Tianhong Li, Lijie Fan, Yuan Yuan, Hao He, Yonglong Tian, Dina Katabi", "title": "Information-Preserving Contrastive Learning for Self-Supervised\n  Representations", "comments": "The first two authors contributed equally to this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contrastive learning is very effective at learning useful representations\nwithout supervision. Yet contrastive learning has its limitations. It may learn\na shortcut that is irrelevant to the downstream task, and discard relevant\ninformation. Past work has addressed this limitation via custom data\naugmentations that eliminate the shortcut. This solution however does not work\nfor data modalities that are not interpretable by humans, e.g., radio signals.\nFor such modalities, it is hard for a human to guess which shortcuts may exist\nin the signal, or how they can be eliminated. Even for interpretable data,\nsometimes eliminating the shortcut may be undesirable. The shortcut may be\nirrelevant to one downstream task but important to another. In this case, it is\ndesirable to learn a representation that captures both the shortcut information\nand the information relevant to the other downstream task. This paper presents\ninformation-preserving contrastive learning (IPCL), a new framework for\nunsupervised representation learning that preserves relevant information even\nin the presence of shortcuts. We empirically show that the representations\nlearned by IPCL outperforms contrastive learning in supporting different\nmodalities and multiple diverse downstream tasks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 22:46:17 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 16:53:53 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Li", "Tianhong", ""], ["Fan", "Lijie", ""], ["Yuan", "Yuan", ""], ["He", "Hao", ""], ["Tian", "Yonglong", ""], ["Katabi", "Dina", ""]]}, {"id": "2012.09963", "submitter": "Artem Sevastopolsky", "authors": "Artem Sevastopolsky, Savva Ignatiev, Gonzalo Ferrer, Evgeny Burnaev,\n  Victor Lempitsky", "title": "Relightable 3D Head Portraits from a Smartphone Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a system for creating a relightable 3D portrait of a human head\nis presented. Our neural pipeline operates on a sequence of frames captured by\na smartphone camera with the flash blinking (flash-no flash sequence). A coarse\npoint cloud reconstructed via structure-from-motion software and multi-view\ndenoising is then used as a geometric proxy. Afterwards, a deep rendering\nnetwork is trained to regress dense albedo, normals, and environmental lighting\nmaps for arbitrary new viewpoints. Effectively, the proxy geometry and the\nrendering network constitute a relightable 3D portrait model, that can be\nsynthesized from an arbitrary viewpoint and under arbitrary lighting, e.g.\ndirectional light, point light, or an environment map. The model is fitted to\nthe sequence of frames with human face-specific priors that enforce the\nplausibility of albedo-lighting decomposition and operates at the interactive\nframe rate. We evaluate the performance of the method under varying lighting\nconditions and at the extrapolated viewpoints and compare with existing\nrelighting methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 22:49:02 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Sevastopolsky", "Artem", ""], ["Ignatiev", "Savva", ""], ["Ferrer", "Gonzalo", ""], ["Burnaev", "Evgeny", ""], ["Lempitsky", "Victor", ""]]}, {"id": "2012.09988", "submitter": "Adel Ahmadyan", "authors": "Adel Ahmadyan, Liangkai Zhang, Jianing Wei, Artsiom Ablavatski,\n  Matthias Grundmann", "title": "Objectron: A Large Scale Dataset of Object-Centric Videos in the Wild\n  with Pose Annotations", "comments": "Github repo see https://github.com/google-research-datasets/Objectron", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D object detection has recently become popular due to many applications in\nrobotics, augmented reality, autonomy, and image retrieval. We introduce the\nObjectron dataset to advance the state of the art in 3D object detection and\nfoster new research and applications, such as 3D object tracking, view\nsynthesis, and improved 3D shape representation. The dataset contains\nobject-centric short videos with pose annotations for nine categories and\nincludes 4 million annotated images in 14,819 annotated videos. We also propose\na new evaluation metric, 3D Intersection over Union, for 3D object detection.\nWe demonstrate the usefulness of our dataset in 3D object detection tasks by\nproviding baseline models trained on this dataset. Our dataset and evaluation\nsource code are available online at http://www.objectron.dev\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 00:34:18 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Ahmadyan", "Adel", ""], ["Zhang", "Liangkai", ""], ["Wei", "Jianing", ""], ["Ablavatski", "Artsiom", ""], ["Grundmann", "Matthias", ""]]}, {"id": "2012.10013", "submitter": "Xingjian Zhen", "authors": "Xingjian Zhen, Rudrasis Chakraborty, Liu Yang, Vikas Singh", "title": "Flow-based Generative Models for Learning Manifold to Manifold Mappings", "comments": "This paper has been accepted by AAAI 2021. A video introduction is on\n  YouTube: https://youtu.be/0r96U0vXsCM The official GitHub repo is:\n  https://github.com/zhenxingjian/Dual_Manifold_GLOW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many measurements or observations in computer vision and machine learning\nmanifest as non-Euclidean data. While recent proposals (like spherical CNN)\nhave extended a number of deep neural network architectures to manifold-valued\ndata, and this has often provided strong improvements in performance, the\nliterature on generative models for manifold data is quite sparse. Partly due\nto this gap, there are also no modality transfer/translation models for\nmanifold-valued data whereas numerous such methods based on generative models\nare available for natural images. This paper addresses this gap, motivated by a\nneed in brain imaging -- in doing so, we expand the operating range of certain\ngenerative models (as well as generative models for modality transfer) from\nnatural images to images with manifold-valued measurements. Our main result is\nthe design of a two-stream version of GLOW (flow-based invertible generative\nmodels) that can synthesize information of a field of one type of\nmanifold-valued measurements given another. On the theoretical side, we\nintroduce three kinds of invertible layers for manifold-valued data, which are\nnot only analogous to their functionality in flow-based generative models\n(e.g., GLOW) but also preserve the key benefits (determinants of the Jacobian\nare easy to calculate). For experiments, on a large dataset from the Human\nConnectome Project (HCP), we show promising results where we can reliably and\naccurately reconstruct brain images of a field of orientation distribution\nfunctions (ODF) from diffusion tensor images (DTI), where the latter has a\n$5\\times$ faster acquisition time but at the expense of worse angular\nresolution.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 02:19:18 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 17:28:12 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zhen", "Xingjian", ""], ["Chakraborty", "Rudrasis", ""], ["Yang", "Liu", ""], ["Singh", "Vikas", ""]]}, {"id": "2012.10017", "submitter": "Zhengeng Yang", "authors": "Zhengeng Yang, Hongshan Yu, Yong He, Zhi-Hong Mao, Ajmal Mian", "title": "Self-supervised Learning with Fully Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning based methods have achieved great success in many\ncomputer vision tasks, their performance relies on a large number of densely\nannotated samples that are typically difficult to obtain. In this paper, we\nfocus on the problem of learning representation from unlabeled data for\nsemantic segmentation. Inspired by two patch-based methods, we develop a novel\nself-supervised learning framework by formulating the Jigsaw Puzzle problem as\na patch-wise classification process and solving it with a fully convolutional\nnetwork. By learning to solve a Jigsaw Puzzle problem with 25 patches and\ntransferring the learned features to semantic segmentation task on Cityscapes\ndataset, we achieve a 5.8 percentage point improvement over the baseline model\nthat initialized from random values. Moreover, experiments show that our\nself-supervised learning method can be applied to different datasets and\nmodels. In particular, we achieved competitive performance with the\nstate-of-the-art methods on the PASCAL VOC2012 dataset using significant fewer\ntraining images.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 02:31:28 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Yang", "Zhengeng", ""], ["Yu", "Hongshan", ""], ["He", "Yong", ""], ["Mao", "Zhi-Hong", ""], ["Mian", "Ajmal", ""]]}, {"id": "2012.10042", "submitter": "Xu Zelin", "authors": "Zelin Xu, Ke Chen, Changxing Ding, Yaowei Wang, Kui Jia", "title": "3D Object Classification on Partial Point Clouds: A Practical\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a 3D counterpart of object classification in images, object point cloud\nclassification is fundamental to 3D scene understanding, and has drawn great\nresearch attention since the release of benchmarking datasets, such as the\nModelNet and the ShapeNet. These benchmarks assume point clouds covering\ncomplete surfaces of object instances, for which plenty of high-performing\nmethods have been developed. However, their settings deviate from those often\nmet in practice, where, due to (self-)occlusion, a point cloud covering partial\nsurface of an object is captured from an arbitrary view. We show in this paper\nthat performance of existing point cloud classification methods drops\ndrastically under the considered practical single-view, partial setting; the\nphenomenon is consistent with the observation that semantic category of a\npartial object surface is less ambiguous only when its distribution on the\nwhole surface is clearly specified. To this end, we argue for a single-view,\npartial setting where supervised learning of object pose estimation should be\naccompanied with classification. Technically, we propose a baseline method of\nPose-Accompanied Point cloud classification Network (PAPNet); built upon\nSE(3)-equivariant convolutions, the PAPNet learns intermediate pose\ntransformations for equivariant features defined on vector fields, which makes\nthe subsequent classification easier (ideally) in the category-level, canonical\npose. We adapt existing ModelNet40 and ScanNet datasets on point set\nclassification to the introduced single-view, partial setting to verify our\nhypothesis. Thorough experiments confirm the necessity of object pose\nestimation; our PAPNet also outperforms existing methods greatly on the new\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 04:00:56 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 02:59:10 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 03:13:56 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Xu", "Zelin", ""], ["Chen", "Ke", ""], ["Ding", "Changxing", ""], ["Wang", "Yaowei", ""], ["Jia", "Kui", ""]]}, {"id": "2012.10043", "submitter": "Peter Schaldenbrand", "authors": "Peter Schaldenbrand and Jean Oh", "title": "Content Masked Loss: Human-Like Brush Stroke Planning in a Reinforcement\n  Learning Painting Agent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The objective of most Reinforcement Learning painting agents is to minimize\nthe loss between a target image and the paint canvas. Human painter artistry\nemphasizes important features of the target image rather than simply\nreproducing it (DiPaola 2007). Using adversarial or L2 losses in the RL\npainting models, although its final output is generally a work of finesse,\nproduces a stroke sequence that is vastly different from that which a human\nwould produce since the model does not have knowledge about the abstract\nfeatures in the target image. In order to increase the human-like planning of\nthe model without the use of expensive human data, we introduce a new loss\nfunction for use with the model's reward function: Content Masked Loss. In the\ncontext of robot painting, Content Masked Loss employs an object detection\nmodel to extract features which are used to assign higher weight to regions of\nthe canvas that a human would find important for recognizing content. The\nresults, based on 332 human evaluators, show that the digital paintings\nproduced by our Content Masked model show detectable subject matter earlier in\nthe stroke sequence than existing methods without compromising on the quality\nof the final painting. Our code is available at\nhttps://github.com/pschaldenbrand/ContentMaskedLoss.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 04:02:13 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 16:28:47 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Schaldenbrand", "Peter", ""], ["Oh", "Jean", ""]]}, {"id": "2012.10066", "submitter": "Fan Lu", "authors": "Fan Lu and Guang Chen and Sanqing Qu and Zhijun Li and Yinlong Liu and\n  Alois Knoll", "title": "PointINet: Point Cloud Frame Interpolation Network", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  LiDAR point cloud streams are usually sparse in time dimension, which is\nlimited by hardware performance. Generally, the frame rates of mechanical LiDAR\nsensors are 10 to 20 Hz, which is much lower than other commonly used sensors\nlike cameras. To overcome the temporal limitations of LiDAR sensors, a novel\ntask named Point Cloud Frame Interpolation is studied in this paper. Given two\nconsecutive point cloud frames, Point Cloud Frame Interpolation aims to\ngenerate intermediate frame(s) between them. To achieve that, we propose a\nnovel framework, namely Point Cloud Frame Interpolation Network (PointINet).\nBased on the proposed method, the low frame rate point cloud streams can be\nupsampled to higher frame rates. We start by estimating bi-directional 3D scene\nflow between the two point clouds and then warp them to the given time step\nbased on the 3D scene flow. To fuse the two warped frames and generate\nintermediate point cloud(s), we propose a novel learning-based points fusion\nmodule, which simultaneously takes two warped point clouds into consideration.\nWe design both quantitative and qualitative experiments to evaluate the\nperformance of the point cloud frame interpolation method and extensive\nexperiments on two large scale outdoor LiDAR datasets demonstrate the\neffectiveness of the proposed PointINet. Our code is available at\nhttps://github.com/ispc-lab/PointINet.git.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 06:15:01 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Lu", "Fan", ""], ["Chen", "Guang", ""], ["Qu", "Sanqing", ""], ["Li", "Zhijun", ""], ["Liu", "Yinlong", ""], ["Knoll", "Alois", ""]]}, {"id": "2012.10071", "submitter": "Limin Wang", "authors": "Limin Wang, Zhan Tong, Bin Ji, Gangshan Wu", "title": "TDN: Temporal Difference Networks for Efficient Action Recognition", "comments": "CVPR2021 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Temporal modeling still remains challenging for action recognition in videos.\nTo mitigate this issue, this paper presents a new video architecture, termed as\nTemporal Difference Network (TDN), with a focus on capturing multi-scale\ntemporal information for efficient action recognition. The core of our TDN is\nto devise an efficient temporal module (TDM) by explicitly leveraging a\ntemporal difference operator, and systematically assess its effect on\nshort-term and long-term motion modeling. To fully capture temporal information\nover the entire video, our TDN is established with a two-level difference\nmodeling paradigm. Specifically, for local motion modeling, temporal difference\nover consecutive frames is used to supply 2D CNNs with finer motion pattern,\nwhile for global motion modeling, temporal difference across segments is\nincorporated to capture long-range structure for motion feature excitation. TDN\nprovides a simple and principled temporal modeling framework and could be\ninstantiated with the existing CNNs at a small extra computational cost. Our\nTDN presents a new state of the art on the Something-Something V1 & V2 datasets\nand is on par with the best performance on the Kinetics-400 dataset. In\naddition, we conduct in-depth ablation studies and plot the visualization\nresults of our TDN, hopefully providing insightful analysis on temporal\ndifference modeling. We release the code at https://github.com/MCG-NJU/TDN.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 06:31:08 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 01:51:10 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wang", "Limin", ""], ["Tong", "Zhan", ""], ["Ji", "Bin", ""], ["Wu", "Gangshan", ""]]}, {"id": "2012.10078", "submitter": "Kai Wang", "authors": "Kai Wang, Yuxin Gu, Xiaojiang Peng, Panpan Zhang, Baigui Sun, Hao Li", "title": "AU-Guided Unsupervised Domain Adaptive Facial Expression Recognition", "comments": "This is a very simple CD-FER framework", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The domain diversities including inconsistent annotation and varied image\ncollection conditions inevitably exist among different facial expression\nrecognition (FER) datasets, which pose an evident challenge for adapting the\nFER model trained on one dataset to another one. Recent works mainly focus on\ndomain-invariant deep feature learning with adversarial learning mechanism,\nignoring the sibling facial action unit (AU) detection task which has obtained\ngreat progress. Considering AUs objectively determine facial expressions, this\npaper proposes an AU-guided unsupervised Domain Adaptive FER (AdaFER) framework\nto relieve the annotation bias between different FER datasets. In AdaFER, we\nfirst leverage an advanced model for AU detection on both source and target\ndomain. Then, we compare the AU results to perform AU-guided annotating, i.e.,\ntarget faces that own the same AUs with source faces would inherit the labels\nfrom source domain. Meanwhile, to achieve domain-invariant compact features, we\nutilize an AU-guided triplet training which randomly collects\nanchor-positive-negative triplets on both domains with AUs. We conduct\nextensive experiments on several popular benchmarks and show that AdaFER\nachieves state-of-the-art results on all these benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 07:17:30 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 10:04:40 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Wang", "Kai", ""], ["Gu", "Yuxin", ""], ["Peng", "Xiaojiang", ""], ["Zhang", "Panpan", ""], ["Sun", "Baigui", ""], ["Li", "Hao", ""]]}, {"id": "2012.10079", "submitter": "Deniz Gurevin", "authors": "Deniz Gurevin, Shanglin Zhou, Lynn Pepin, Bingbing Li, Mikhail Bragin,\n  Caiwen Ding, Fei Miao", "title": "Enabling Retrain-free Deep Neural Network Pruning using Surrogate\n  Lagrangian Relaxation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network pruning is a widely used technique to reduce computation cost and\nmodel size for deep neural networks. However, the typical three-stage pipeline,\ni.e., training, pruning and retraining (fine-tuning) significantly increases\nthe overall training trails. In this paper, we develop a systematic\nweight-pruning optimization approach based on Surrogate Lagrangian relaxation\n(SLR), which is tailored to overcome difficulties caused by the discrete nature\nof the weight-pruning problem while ensuring fast convergence. We further\naccelerate the convergence of the SLR by using quadratic penalties. Model\nparameters obtained by SLR during the training phase are much closer to their\noptimal values as compared to those obtained by other state-of-the-art methods.\nWe evaluate the proposed method on image classification tasks, i.e., ResNet-18\nand ResNet-50 using ImageNet, and ResNet-18, ResNet-50 and VGG-16 using\nCIFAR-10, as well as object detection tasks, i.e., YOLOv3 and YOLOv3-tiny using\nCOCO 2014 and Ultra-Fast-Lane-Detection using TuSimple lane detection dataset.\nExperimental results demonstrate that our SLR-based weight-pruning optimization\napproach achieves higher compression rate than state-of-the-arts under the same\naccuracy requirement. It also achieves a high model accuracy even at the\nhard-pruning stage without retraining (reduces the traditional three-stage\npruning to two-stage). Given a limited budget of retraining epochs, our\napproach quickly recovers the model accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 07:17:30 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 04:50:49 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Gurevin", "Deniz", ""], ["Zhou", "Shanglin", ""], ["Pepin", "Lynn", ""], ["Li", "Bingbing", ""], ["Bragin", "Mikhail", ""], ["Ding", "Caiwen", ""], ["Miao", "Fei", ""]]}, {"id": "2012.10083", "submitter": "Yusuke Monno", "authors": "Hironori Hidaka, Yusuke Monno, Masatoshi Okutomi", "title": "Spectral Reflectance Estimation Using Projector with Unknown Spectral\n  Power Distribution", "comments": "Presented at CIC2020. Projector's SPD data is available at\n  http://www.ok.sc.e.titech.ac.jp/res/PCSSfM/pro-cam_reflectance.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lighting-based multispectral imaging system using an RGB camera and a\nprojector is one of the most practical and low-cost systems to acquire\nmultispectral observations for estimating the scene's spectral reflectance\ninformation. However, existing projector-based systems assume that the spectral\npower distribution (SPD) of each projector primary is known, which requires\nadditional equipment such as a spectrometer to measure the SPD. In this paper,\nwe present a method for jointly estimating the spectral reflectance and the SPD\nof each projector primary. In addition to adopting a common spectral\nreflectance basis model, we model the projector's SPD by a low-dimensional\nmodel using basis functions obtained by a newly collected projector's SPD\ndatabase. Then, the spectral reflectances and the projector's SPDs are\nalternatively estimated based on the basis models. We experimentally show the\nperformance of our joint estimation using a different number of projected\nilluminations and investigate the potential of the spectral reflectance\nestimation using a projector with unknown SPD.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 07:23:47 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Hidaka", "Hironori", ""], ["Monno", "Yusuke", ""], ["Okutomi", "Masatoshi", ""]]}, {"id": "2012.10102", "submitter": "Xiaozhong Ji", "authors": "Xiaozhong Ji, Guangpin Tao, Yun Cao, Ying Tai, Tong Lu, Chengjie Wang,\n  Jilin Li, Feiyue Huang", "title": "Frequency Consistent Adaptation for Real World Super Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep-learning based Super-Resolution (SR) methods have achieved\nremarkable performance on images with known degradation. However, these methods\nalways fail in real-world scene, since the Low-Resolution (LR) images after the\nideal degradation (e.g., bicubic down-sampling) deviate from real source\ndomain. The domain gap between the LR images and the real-world images can be\nobserved clearly on frequency density, which inspires us to explictly narrow\nthe undesired gap caused by incorrect degradation. From this point of view, we\ndesign a novel Frequency Consistent Adaptation (FCA) that ensures the frequency\ndomain consistency when applying existing SR methods to the real scene. We\nestimate degradation kernels from unsupervised images and generate the\ncorresponding LR images. To provide useful gradient information for kernel\nestimation, we propose Frequency Density Comparator (FDC) by distinguishing the\nfrequency density of images on different scales. Based on the domain-consistent\nLR-HR pairs, we train easy-implemented Convolutional Neural Network (CNN) SR\nmodels. Extensive experiments show that the proposed FCA improves the\nperformance of the SR model under real-world setting achieving state-of-the-art\nresults with high fidelity and plausible perception, thus providing a novel\neffective framework for real-world SR application.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 08:25:39 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Ji", "Xiaozhong", ""], ["Tao", "Guangpin", ""], ["Cao", "Yun", ""], ["Tai", "Ying", ""], ["Lu", "Tong", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""]]}, {"id": "2012.10122", "submitter": "Yuxing Huang", "authors": "Yuxing Huang, Shaodi You, Ying Fu and Qiu Shen", "title": "Weakly-supervised Semantic Segmentation in Cityscape via Hyperspectral\n  Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-resolution hyperspectral images (HSIs) contain the response of each\npixel in different spectral bands, which can be used to effectively distinguish\nvarious objects in complex scenes. While HSI cameras have become low cost,\nalgorithms based on it have not been well exploited. In this paper, we focus on\na novel topic, weakly-supervised semantic segmentation in cityscape via HSIs.\nIt is based on the idea that high-resolution HSIs in city scenes contain rich\nspectral information, which can be easily associated to semantics without\nmanual labeling. Therefore, it enables low cost, highly reliable semantic\nsegmentation in complex scenes. Specifically, in this paper, we theoretically\nanalyze the HSIs and introduce a weakly-supervised HSI semantic segmentation\nframework, which utilizes spectral information to improve the coarse labels to\na finer degree. The experimental results show that our method can obtain highly\ncompetitive labels and even have higher edge fineness than artificial fine\nlabels in some classes. At the same time, the results also show that the\nrefined labels can effectively improve the effect of semantic segmentation. The\ncombination of HSIs and semantic segmentation proves that HSIs have great\npotential in high-level visual tasks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 09:29:17 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 09:15:23 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Huang", "Yuxing", ""], ["You", "Shaodi", ""], ["Fu", "Ying", ""], ["Shen", "Qiu", ""]]}, {"id": "2012.10133", "submitter": "Xingxing Zuo", "authors": "Xingxing Zuo, Nathaniel Merrill, Wei Li, Yong Liu, Marc Pollefeys,\n  Guoquan Huang", "title": "CodeVIO: Visual-Inertial Odometry with Learned Optimizable Dense Depth", "comments": "6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a lightweight, tightly-coupled deep depth network\nand visual-inertial odometry (VIO) system, which can provide accurate state\nestimates and dense depth maps of the immediate surroundings. Leveraging the\nproposed lightweight Conditional Variational Autoencoder (CVAE) for depth\ninference and encoding, we provide the network with previously marginalized\nsparse features from VIO to increase the accuracy of initial depth prediction\nand generalization capability. The compact encoded depth maps are then updated\njointly with navigation states in a sliding window estimator in order to\nprovide the dense local scene geometry. We additionally propose a novel method\nto obtain the CVAE's Jacobian which is shown to be more than an order of\nmagnitude faster than previous works, and we additionally leverage\nFirst-Estimate Jacobian (FEJ) to avoid recalculation. As opposed to previous\nworks relying on completely dense residuals, we propose to only provide sparse\nmeasurements to update the depth code and show through careful experimentation\nthat our choice of sparse measurements and FEJs can still significantly improve\nthe estimated depth maps. Our full system also exhibits state-of-the-art pose\nestimation accuracy, and we show that it can run in real-time with\nsingle-thread execution while utilizing GPU acceleration only for the network\nand code Jacobian.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 09:42:54 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 16:18:18 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Zuo", "Xingxing", ""], ["Merrill", "Nathaniel", ""], ["Li", "Wei", ""], ["Liu", "Yong", ""], ["Pollefeys", "Marc", ""], ["Huang", "Guoquan", ""]]}, {"id": "2012.10150", "submitter": "Thang Vu", "authors": "Thang Vu, Haeyong Kang, Chang D. Yoo", "title": "SCNet: Training Inference Sample Consistency for Instance Segmentation", "comments": "To appear in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cascaded architectures have brought significant performance improvement in\nobject detection and instance segmentation. However, there are lingering issues\nregarding the disparity in the Intersection-over-Union (IoU) distribution of\nthe samples between training and inference. This disparity can potentially\nexacerbate detection accuracy. This paper proposes an architecture referred to\nas Sample Consistency Network (SCNet) to ensure that the IoU distribution of\nthe samples at training time is close to that at inference time. Furthermore,\nSCNet incorporates feature relay and utilizes global contextual information to\nfurther reinforce the reciprocal relationships among classifying, detecting,\nand segmenting sub-tasks. Extensive experiments on the standard COCO dataset\nreveal the effectiveness of the proposed method over multiple evaluation\nmetrics, including box AP, mask AP, and inference speed. In particular, while\nrunning 38\\% faster, the proposed SCNet improves the AP of the box and mask\npredictions by respectively 1.3 and 2.3 points compared to the strong Cascade\nMask R-CNN baseline. Code is available at\n\\url{https://github.com/thangvubk/SCNet}.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 10:26:54 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Vu", "Thang", ""], ["Kang", "Haeyong", ""], ["Yoo", "Chang D.", ""]]}, {"id": "2012.10160", "submitter": "Jos\\'e Morano", "authors": "Jos\\'e Morano, \\'Alvaro S. Hervella, Noelia Barreira, Jorge Novo,\n  Jos\\'e Rouco", "title": "Multimodal Transfer Learning-based Approaches for Retinal Vascular\n  Segmentation", "comments": null, "journal-ref": "Proceedings of the 24th European Conference on Artificial\n  Intelligence (ECAI 2020)", "doi": "10.3233/FAIA200303", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ophthalmology, the study of the retinal microcirculation is a key issue in\nthe analysis of many ocular and systemic diseases, like hypertension or\ndiabetes. This motivates the research on improving the retinal vasculature\nsegmentation. Nowadays, Fully Convolutional Neural Networks (FCNs) usually\nrepresent the most successful approach to image segmentation. However, the\nsuccess of these models is conditioned by an adequate selection and adaptation\nof the architectures and techniques used, as well as the availability of a\nlarge amount of annotated data. These two issues become specially relevant when\napplying FCNs to medical image segmentation as, first, the existent models are\nusually adjusted from broad domain applications over photographic images, and\nsecond, the amount of annotated data is usually scarcer. In this work, we\npresent multimodal transfer learning-based approaches for retinal vascular\nsegmentation, performing a comparative study of recent FCN architectures. In\nparticular, to overcome the annotated data scarcity, we propose the novel\napplication of self-supervised network pretraining that takes advantage of\nexistent unlabelled multimodal data. The results demonstrate that the\nself-supervised pretrained networks obtain significantly better vascular masks\nwith less training in the target task, independently of the network\narchitecture, and that some FCN architecture advances motivated for broad\ndomain applications do not translate into significant improvements over the\nvasculature segmentation task.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 10:38:35 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Morano", "Jos\u00e9", ""], ["Hervella", "\u00c1lvaro S.", ""], ["Barreira", "Noelia", ""], ["Novo", "Jorge", ""], ["Rouco", "Jos\u00e9", ""]]}, {"id": "2012.10162", "submitter": "Jianbo Liu", "authors": "Jianbo Liu, Sijie Ren, Yuanjie Zheng, Xiaogang Wang, Hongsheng Li", "title": "A Holistically-Guided Decoder for Deep Representation Learning with\n  Applications to Semantic Segmentation and Object Detection", "comments": "arXiv admin note: substantial text overlap with arXiv:2008.10487", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both high-level and high-resolution feature representations are of great\nimportance in various visual understanding tasks. To acquire high-resolution\nfeature maps with high-level semantic information, one common strategy is to\nadopt dilated convolutions in the backbone networks to extract high-resolution\nfeature maps, such as the dilatedFCN-based methods for semantic segmentation.\nHowever, due to many convolution operations are conducted on the\nhigh-resolution feature maps, such methods have large computational complexity\nand memory consumption. In this paper, we propose one novel holistically-guided\ndecoder which is introduced to obtain the high-resolution semantic-rich feature\nmaps via the multi-scale features from the encoder. The decoding is achieved\nvia novel holistic codeword generation and codeword assembly operations, which\ntake advantages of both the high-level and low-level features from the encoder\nfeatures. With the proposed holistically-guided decoder, we implement the\nEfficientFCN architecture for semantic segmentation and HGD-FPN for object\ndetection and instance segmentation. The EfficientFCN achieves comparable or\neven better performance than state-of-the-art methods with only 1/3 of their\ncomputational costs for semantic segmentation on PASCAL Context, PASCAL VOC,\nADE20K datasets. Meanwhile, the proposed HGD-FPN achieves $>2\\%$ higher mean\nAverage Precision (mAP) when integrated into several object detection\nframeworks with ResNet-50 encoding backbones.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 10:51:49 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Liu", "Jianbo", ""], ["Ren", "Sijie", ""], ["Zheng", "Yuanjie", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "2012.10189", "submitter": "Mingjie Wang", "authors": "Mingjie Wang, Hao Cai, Xianfeng Han, Jun Zhou, Minglun Gong", "title": "STNet: Scale Tree Network with Multi-level Auxiliator for Crowd Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting remains a challenging task because the presence of drastic\nscale variation, density inconsistency, and complex background can seriously\ndegrade the counting accuracy. To battle the ingrained issue of accuracy\ndegradation, we propose a novel and powerful network called Scale Tree Network\n(STNet) for accurate crowd counting. STNet consists of two key components: a\nScale-Tree Diversity Enhancer and a Semi-supervised Multi-level Auxiliator.\nSpecifically, the Diversity Enhancer is designed to enrich scale diversity,\nwhich alleviates limitations of existing methods caused by insufficient level\nof scales. A novel tree structure is adopted to hierarchically parse\ncoarse-to-fine crowd regions. Furthermore, a simple yet effective Multi-level\nAuxiliator is presented to aid in exploiting generalisable shared\ncharacteristics at multiple levels, allowing more accurate pixel-wise\nbackground cognition. The overall STNet is trained in an end-to-end manner,\nwithout the needs for manually tuning loss weights between the main and the\nauxiliary tasks. Extensive experiments on four challenging crowd datasets\ndemonstrate the superiority of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 12:18:45 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Wang", "Mingjie", ""], ["Cai", "Hao", ""], ["Han", "Xianfeng", ""], ["Zhou", "Jun", ""], ["Gong", "Minglun", ""]]}, {"id": "2012.10192", "submitter": "Michael Ying Yang", "authors": "Yaping Lin, George Vosselman, Yanpeng Cao, Michael Ying Yang", "title": "LGENet: Local and Global Encoder Network for Semantic Segmentation of\n  Airborne Laser Scanning Point Clouds", "comments": "Submitted to ISPRS Journal of Photogrammetry and Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretation of Airborne Laser Scanning (ALS) point clouds is a critical\nprocedure for producing various geo-information products like 3D city models,\ndigital terrain models and land use maps. In this paper, we present a local and\nglobal encoder network (LGENet) for semantic segmentation of ALS point clouds.\nAdapting the KPConv network, we first extract features by both 2D and 3D point\nconvolutions to allow the network to learn more representative local geometry.\nThen global encoders are used in the network to exploit contextual information\nat the object and point level. We design a segment-based Edge Conditioned\nConvolution to encode the global context between segments. We apply a\nspatial-channel attention module at the end of the network, which not only\ncaptures the global interdependencies between points but also models\ninteractions between channels. We evaluate our method on two ALS datasets\nnamely, the ISPRS benchmark dataset and DCF2019 dataset. For the ISPRS\nbenchmark dataset, our model achieves state-of-the-art results with an overall\naccuracy of 0.845 and an average F1 score of 0.737. With regards to the DFC2019\ndataset, our proposed network achieves an overall accuracy of 0.984 and an\naverage F1 score of 0.834.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 12:26:53 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Lin", "Yaping", ""], ["Vosselman", "George", ""], ["Cao", "Yanpeng", ""], ["Yang", "Michael Ying", ""]]}, {"id": "2012.10210", "submitter": "Tom Winterbottom", "authors": "Thomas Winterbottom, Sarah Xiao, Alistair McLean, Noura Al Moubayed", "title": "On Modality Bias in the TVQA Dataset", "comments": "10 pages, 4 Figures, 2 Tables, +Supp Mats, BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  TVQA is a large scale video question answering (video-QA) dataset based on\npopular TV shows. The questions were specifically designed to require \"both\nvision and language understanding to answer\". In this work, we demonstrate an\ninherent bias in the dataset towards the textual subtitle modality. We infer\nsaid bias both directly and indirectly, notably finding that models trained\nwith subtitles learn, on-average, to suppress video feature contribution. Our\nresults demonstrate that models trained on only the visual information can\nanswer ~45% of the questions, while using only the subtitles achieves ~68%. We\nfind that a bilinear pooling based joint representation of modalities damages\nmodel performance by 9% implying a reliance on modality specific information.\nWe also show that TVQA fails to benefit from the RUBi modality bias reduction\ntechnique popularised in VQA. By simply improving text processing using BERT\nembeddings with the simple model first proposed for TVQA, we achieve\nstate-of-the-art results (72.13%) compared to the highly complex STAGE model\n(70.50%). We recommend a multimodal evaluation framework that can highlight\nbiases in models and isolate visual and textual reliant subsets of data. Using\nthis framework we propose subsets of TVQA that respond exclusively to either or\nboth modalities in order to facilitate multimodal modelling as TVQA originally\nintended.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 13:06:23 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Winterbottom", "Thomas", ""], ["Xiao", "Sarah", ""], ["McLean", "Alistair", ""], ["Moubayed", "Noura Al", ""]]}, {"id": "2012.10217", "submitter": "An Tao", "authors": "An Tao, Yueqi Duan, Yi Wei, Jiwen Lu, Jie Zhou", "title": "SegGroup: Seg-Level Supervision for 3D Instance and Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing point cloud instance and semantic segmentation methods rely\nheavily on strong supervision signals, which require point-level labels for\nevery point in the scene. However, such strong supervision suffers from large\nannotation costs, arousing the need to study efficient annotating. In this\npaper, we discover that the locations of instances matter for 3D scene\nsegmentation. By fully taking the advantages of locations, we design a weakly\nsupervised point cloud segmentation algorithm that only requires clicking on\none point per instance to indicate its location for annotation. With\nover-segmentation for pre-processing, we extend these location annotations into\nsegments as seg-level labels. We further design a segment grouping network\n(SegGroup) to generate pseudo point-level labels under seg-level labels by\nhierarchically grouping the unlabeled segments into the relevant nearby labeled\nsegments, so that existing point-level supervised segmentation models can\ndirectly consume these pseudo labels for training. Experimental results show\nthat our seg-level supervised method (SegGroup) achieves comparable results\nwith the fully annotated point-level supervised methods. Moreover, it also\noutperforms the recent weakly supervised methods given a fixed annotation\nbudget.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 13:23:34 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 12:42:25 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Tao", "An", ""], ["Duan", "Yueqi", ""], ["Wei", "Yi", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "2012.10283", "submitter": "Feiyan Hu", "authors": "Feiyan Hu, Eva Mohedano, Noel O'Connor and Kevin McGuinness", "title": "Temporal Bilinear Encoding Network of Audio-Visual Features at Low\n  Sampling Rates", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning based video classification architectures are typically\ntrained end-to-end on large volumes of data and require extensive computational\nresources. This paper aims to exploit audio-visual information in video\nclassification with a 1 frame per second sampling rate. We propose Temporal\nBilinear Encoding Networks (TBEN) for encoding both audio and visual long range\ntemporal information using bilinear pooling and demonstrate bilinear pooling is\nbetter than average pooling on the temporal dimension for videos with low\nsampling rate. We also embed the label hierarchy in TBEN to further improve the\nrobustness of the classifier. Experiments on the FGA240 fine-grained\nclassification dataset using TBEN achieve a new state-of-the-art\n(hit@1=47.95%). We also exploit the possibility of incorporating TBEN with\nmultiple decoupled modalities like visual semantic and motion features:\nexperiments on UCF101 sampled at 1 FPS achieve close to state-of-the-art\naccuracy (hit@1=91.03%) while requiring significantly less computational\nresources than competing approaches for both training and prediction.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 14:59:34 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Hu", "Feiyan", ""], ["Mohedano", "Eva", ""], ["O'Connor", "Noel", ""], ["McGuinness", "Kevin", ""]]}, {"id": "2012.10285", "submitter": "Tom Winterbottom", "authors": "Thomas Winterbottom, Sarah Xiao, Alistair McLean, Noura Al Moubayed", "title": "Trying Bilinear Pooling in Video-QA", "comments": "16 Pages, 8 Figures, 4 Tables, +Supp Mats", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Bilinear pooling (BLP) refers to a family of operations recently developed\nfor fusing features from different modalities predominantly developed for VQA\nmodels. A bilinear (outer-product) expansion is thought to encourage models to\nlearn interactions between two feature spaces and has experimentally\noutperformed `simpler' vector operations (concatenation and\nelement-wise-addition/multiplication) on VQA benchmarks. Successive BLP\ntechniques have yielded higher performance with lower computational expense and\nare often implemented alongside attention mechanisms. However, despite\nsignificant progress in VQA, BLP methods have not been widely applied to more\nrecently explored video question answering (video-QA) tasks. In this paper, we\nbegin to bridge this research gap by applying BLP techniques to various\nvideo-QA benchmarks, namely: TVQA, TGIF-QA, Ego-VQA and MSVD-QA. We share our\nresults on the TVQA baseline model, and the recently proposed\nheterogeneous-memory-enchanced multimodal attention (HME) model. Our\nexperiments include both simply replacing feature concatenation in the existing\nmodels with BLP, and a modified version of the TVQA baseline to accommodate BLP\nwe name the `dual-stream' model. We find that our relatively simple integration\nof BLP does not increase, and mostly harms, performance on these video-QA\nbenchmarks. Using recently proposed theoretical multimodal fusion taxonomies,\nwe offer insight into why BLP-driven performance gain for video-QA benchmarks\nmay be more difficult to achieve than in earlier VQA models. We suggest a few\nadditional `best-practices' to consider when applying BLP to video-QA. We\nstress that video-QA models should carefully consider where the complex\nrepresentational potential from BLP is actually needed to avoid computational\nexpense on `redundant' fusion.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 15:01:50 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Winterbottom", "Thomas", ""], ["Xiao", "Sarah", ""], ["McLean", "Alistair", ""], ["Moubayed", "Noura Al", ""]]}, {"id": "2012.10294", "submitter": "Martin Dyrba", "authors": "Martin Dyrba, Moritz Hanzig, Slawek Altenstein, Sebastian Bader,\n  Tommaso Ballarini, Frederic Brosseron, Katharina Buerger, Daniel Cantr\\'e,\n  Peter Dechent, Laura Dobisch, Emrah D\\\"uzel, Michael Ewers, Klaus Fliessbach,\n  Wenzel Glanz, John D. Haynes, Michael T. Heneka, Daniel Janowitz, Deniz Baris\n  Keles, Ingo Kilimann, Christoph Laske, Franziska Maier, Coraline D. Metzger,\n  Matthias H. Munk, Robert Perneczky, Oliver Peters, Lukas Preis, Josef\n  Priller, Boris Rauchmann, Nina Roy, Klaus Scheffler, Anja Schneider, Bj\\\"orn\n  H. Schott, Annika Spottke, Eike J. Spruth, Marc-Andr\\'e Weber, Birgit\n  Ertl-Wagner, Michael Wagner, Jens Wiltfang, Frank Jessen, Stefan J. Teipel", "title": "Improving 3D convolutional neural network comprehensibility via\n  interactive visualization of relevance maps: Evaluation in Alzheimer's\n  disease", "comments": "24 pages, 9 figures/tables, supplementary material, source code\n  available on GitHub", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Although convolutional neural networks (CNN) achieve high\ndiagnostic accuracy for detecting Alzheimer's disease (AD) dementia based on\nmagnetic resonance imaging (MRI) scans, they are not yet applied in clinical\nroutine. One important reason for this is a lack of model comprehensibility.\nRecently developed visualization methods for deriving CNN relevance maps may\nhelp to fill this gap. We investigated whether models with higher accuracy also\nrely more on discriminative brain regions predefined by prior knowledge.\n  Methods: We trained a CNN for the detection of AD in N=663 T1-weighted MRI\nscans of patients with dementia and amnestic mild cognitive impairment (MCI)\nand verified the accuracy of the models via cross-validation and in three\nindependent samples including N=1655 cases. We evaluated the association of\nrelevance scores and hippocampus volume to validate the clinical utility of\nthis approach. To improve model comprehensibility, we implemented an\ninteractive visualization of 3D CNN relevance maps.\n  Results: Across three independent datasets, group separation showed high\naccuracy for AD dementia vs. controls (AUC$\\geq$0.92) and moderate accuracy for\nMCI vs. controls (AUC$\\approx$0.75). Relevance maps indicated that hippocampal\natrophy was considered as the most informative factor for AD detection, with\nadditional contributions from atrophy in other cortical and subcortical\nregions. Relevance scores within the hippocampus were highly correlated with\nhippocampal volumes (Pearson's r$\\approx$-0.86, p<0.001).\n  Conclusion: The relevance maps highlighted atrophy in regions that we had\nhypothesized a priori. This strengthens the comprehensibility of the CNN\nmodels, which were trained in a purely data-driven manner based on the scans\nand diagnosis labels.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 15:16:50 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 17:57:38 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 16:52:24 GMT"}, {"version": "v4", "created": "Mon, 17 May 2021 13:41:03 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Dyrba", "Martin", ""], ["Hanzig", "Moritz", ""], ["Altenstein", "Slawek", ""], ["Bader", "Sebastian", ""], ["Ballarini", "Tommaso", ""], ["Brosseron", "Frederic", ""], ["Buerger", "Katharina", ""], ["Cantr\u00e9", "Daniel", ""], ["Dechent", "Peter", ""], ["Dobisch", "Laura", ""], ["D\u00fczel", "Emrah", ""], ["Ewers", "Michael", ""], ["Fliessbach", "Klaus", ""], ["Glanz", "Wenzel", ""], ["Haynes", "John D.", ""], ["Heneka", "Michael T.", ""], ["Janowitz", "Daniel", ""], ["Keles", "Deniz Baris", ""], ["Kilimann", "Ingo", ""], ["Laske", "Christoph", ""], ["Maier", "Franziska", ""], ["Metzger", "Coraline D.", ""], ["Munk", "Matthias H.", ""], ["Perneczky", "Robert", ""], ["Peters", "Oliver", ""], ["Preis", "Lukas", ""], ["Priller", "Josef", ""], ["Rauchmann", "Boris", ""], ["Roy", "Nina", ""], ["Scheffler", "Klaus", ""], ["Schneider", "Anja", ""], ["Schott", "Bj\u00f6rn H.", ""], ["Spottke", "Annika", ""], ["Spruth", "Eike J.", ""], ["Weber", "Marc-Andr\u00e9", ""], ["Ertl-Wagner", "Birgit", ""], ["Wagner", "Michael", ""], ["Wiltfang", "Jens", ""], ["Jessen", "Frank", ""], ["Teipel", "Stefan J.", ""]]}, {"id": "2012.10296", "submitter": "Lam Huynh", "authors": "Lam Huynh, Phong Nguyen, Jiri Matas, Esa Rahtu, Janne Heikkila", "title": "Boosting Monocular Depth Estimation with Lightweight 3D Point Fusion", "comments": "15 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of fusing monocular depth estimation\nwith a conventional multi-view stereo or SLAM to exploit the best of both\nworlds, that is, the accurate dense depth of the first one and lightweightness\nof the second one. More specifically, we use a conventional pipeline to produce\na sparse 3D point cloud that is fed to a monocular depth estimation network to\nenhance its performance. In this way, we can achieve accuracy similar to\nmulti-view stereo with a considerably smaller number of weights. We also show\nthat even as few as 32 points is sufficient to outperform the best monocular\ndepth estimation methods, and around 200 points to gain full advantage of the\nadditional information. Moreover, we demonstrate the efficacy of our approach\nby integrating it with a SLAM system built-in on mobile devices.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 15:19:46 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Huynh", "Lam", ""], ["Nguyen", "Phong", ""], ["Matas", "Jiri", ""], ["Rahtu", "Esa", ""], ["Heikkila", "Janne", ""]]}, {"id": "2012.10355", "submitter": "Gabriele Lagani", "authors": "Gabriele Lagani, Raffaele Mazziotti, Fabrizio Falchi, Claudio Gennaro,\n  Guido Marco Cicchini, Tommaso Pizzorusso, Federico Cremisi, Giuseppe Amato", "title": "Assessing Pattern Recognition Performance of Neuronal Cultures through\n  Accurate Simulation", "comments": "4 pages, 2 figures. Submitted to NER 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Previous work has shown that it is possible to train neuronal cultures on\nMulti-Electrode Arrays (MEAs), to recognize very simple patterns. However, this\nwork was mainly focused to demonstrate that it is possible to induce plasticity\nin cultures, rather than performing a rigorous assessment of their pattern\nrecognition performance. In this paper, we address this gap by developing a\nmethodology that allows us to assess the performance of neuronal cultures on a\nlearning task. Specifically, we propose a digital model of the real cultured\nneuronal networks; we identify biologically plausible simulation parameters\nthat allow us to reliably reproduce the behavior of real cultures; we use the\nsimulated culture to perform handwritten digit recognition and rigorously\nevaluate its performance; we also show that it is possible to find improved\nsimulation parameters for the specific task, which can guide the creation of\nreal cultures.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 16:59:11 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 11:16:54 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Lagani", "Gabriele", ""], ["Mazziotti", "Raffaele", ""], ["Falchi", "Fabrizio", ""], ["Gennaro", "Claudio", ""], ["Cicchini", "Guido Marco", ""], ["Pizzorusso", "Tommaso", ""], ["Cremisi", "Federico", ""], ["Amato", "Giuseppe", ""]]}, {"id": "2012.10366", "submitter": "Mihai Fieraru", "authors": "Mihai Fieraru, Mihai Zanfir, Elisabeta Oneata, Alin-Ionut Popa, Vlad\n  Olaru, Cristian Sminchisescu", "title": "Learning Complex 3D Human Self-Contact", "comments": "To be published in the Proceedings of the 35th AAAI Conference on\n  Artificial Intelligence (AAAI-2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular estimation of three dimensional human self-contact is fundamental\nfor detailed scene analysis including body language understanding and behaviour\nmodeling. Existing 3d reconstruction methods do not focus on body regions in\nself-contact and consequently recover configurations that are either far from\neach other or self-intersecting, when they should just touch. This leads to\nperceptually incorrect estimates and limits impact in those very fine-grained\nanalysis domains where detailed 3d models are expected to play an important\nrole. To address such challenges we detect self-contact and design 3d losses to\nexplicitly enforce it. Specifically, we develop a model for Self-Contact\nPrediction (SCP), that estimates the body surface signature of self-contact,\nleveraging the localization of self-contact in the image, during both training\nand inference. We collect two large datasets to support learning and\nevaluation: (1) HumanSC3D, an accurate 3d motion capture repository containing\n$1,032$ sequences with $5,058$ contact events and $1,246,487$ ground truth 3d\nposes synchronized with images collected from multiple views, and (2)\nFlickrSC3D, a repository of $3,969$ images, containing $25,297$\nsurface-to-surface correspondences with annotated image spatial support. We\nalso illustrate how more expressive 3d reconstructions can be recovered under\nself-contact signature constraints and present monocular detection of\nface-touch as one of the multiple applications made possible by more accurate\nself-contact models.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 17:09:34 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Fieraru", "Mihai", ""], ["Zanfir", "Mihai", ""], ["Oneata", "Elisabeta", ""], ["Popa", "Alin-Ionut", ""], ["Olaru", "Vlad", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "2012.10412", "submitter": "Yanan Zhang", "authors": "Yanan Zhang, Di Huang, Yunhong Wang", "title": "PC-RGNN: Point Cloud Completion and Graph Neural Network for 3D Object\n  Detection", "comments": "To appear at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR-based 3D object detection is an important task for autonomous driving\nand current approaches suffer from sparse and partial point clouds of distant\nand occluded objects. In this paper, we propose a novel two-stage approach,\nnamely PC-RGNN, dealing with such challenges by two specific solutions. On the\none hand, we introduce a point cloud completion module to recover high-quality\nproposals of dense points and entire views with original structures preserved.\nOn the other hand, a graph neural network module is designed, which\ncomprehensively captures relations among points through a local-global\nattention mechanism as well as multi-scale graph based context aggregation,\nsubstantially strengthening encoded features. Extensive experiments on the\nKITTI benchmark show that the proposed approach outperforms the previous\nstate-of-the-art baselines by remarkable margins, highlighting its\neffectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 18:06:43 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 14:36:00 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2020 03:12:40 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Zhang", "Yanan", ""], ["Huang", "Di", ""], ["Wang", "Yunhong", ""]]}, {"id": "2012.10424", "submitter": "John Zarka", "authors": "John Zarka, Florentin Guth, St\\'ephane Mallat", "title": "Separation and Concentration in Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical experiments demonstrate that deep neural network classifiers\nprogressively separate class distributions around their mean, achieving linear\nseparability on the training set, and increasing the Fisher discriminant ratio.\nWe explain this mechanism with two types of operators. We prove that a\nrectifier without biases applied to sign-invariant tight frames can separate\nclass means and increase Fisher ratios. On the opposite, a soft-thresholding on\ntight frames can reduce within-class variabilities while preserving class\nmeans. Variance reduction bounds are proved for Gaussian mixture models. For\nimage classification, we show that separation of class means can be achieved\nwith rectified wavelet tight frames that are not learned. It defines a\nscattering transform. Learning $1 \\times 1$ convolutional tight frames along\nscattering channels and applying a soft-thresholding reduces within-class\nvariabilities. The resulting scattering network reaches the classification\naccuracy of ResNet-18 on CIFAR-10 and ImageNet, with fewer layers and no\nlearned biases.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 18:27:37 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 15:06:51 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Zarka", "John", ""], ["Guth", "Florentin", ""], ["Mallat", "St\u00e9phane", ""]]}, {"id": "2012.10467", "submitter": "Sayna Ebrahimi", "authors": "Sayna Ebrahimi, William Gan, Dian Chen, Giscard Biamby, Kamyar Salahi,\n  Michael Laielli, Shizhan Zhu, Trevor Darrell", "title": "Minimax Active Learning", "comments": "Project page is available at\n  https://people.eecs.berkeley.edu/~sayna/mal.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Active learning aims to develop label-efficient algorithms by querying the\nmost representative samples to be labeled by a human annotator. Current active\nlearning techniques either rely on model uncertainty to select the most\nuncertain samples or use clustering or reconstruction to choose the most\ndiverse set of unlabeled examples. While uncertainty-based strategies are\nsusceptible to outliers, solely relying on sample diversity does not capture\nthe information available on the main task. In this work, we develop a\nsemi-supervised minimax entropy-based active learning algorithm that leverages\nboth uncertainty and diversity in an adversarial manner. Our model consists of\nan entropy minimizing feature encoding network followed by an entropy\nmaximizing classification layer. This minimax formulation reduces the\ndistribution gap between the labeled/unlabeled data, while a discriminator is\nsimultaneously trained to distinguish the labeled/unlabeled data. The highest\nentropy samples from the classifier that the discriminator predicts as\nunlabeled are selected for labeling. We evaluate our method on various image\nclassification and semantic segmentation benchmark datasets and show superior\nperformance over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 19:03:40 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 15:31:03 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Ebrahimi", "Sayna", ""], ["Gan", "William", ""], ["Chen", "Dian", ""], ["Biamby", "Giscard", ""], ["Salahi", "Kamyar", ""], ["Laielli", "Michael", ""], ["Zhu", "Shizhan", ""], ["Darrell", "Trevor", ""]]}, {"id": "2012.10472", "submitter": "Aidan Draper", "authors": "Aidan Draper and Laura L. Taylor", "title": "A Survey on the Visual Perceptions of Gaussian Noise Filtering on\n  Photography", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statisticians, as well as machine learning and computer vision experts, have\nbeen studying image reconstitution through denoising different domains of\nphotography, such as textual documentation, tomographic, astronomical, and\nlow-light photography. In this paper, we apply common inferential kernel\nfilters in the R and python languages, as well as Adobe Lightroom's denoise\nfilter, and compare their effectiveness in removing noise from JPEG images. We\nran standard benchmark tests to evaluate each method's effectiveness for\nremoving noise. In doing so, we also surveyed students at Elon University about\ntheir opinion of a single filtered photo from a collection of photos processed\nby the various filter methods. Many scientists believe that noise filters cause\nblurring and image quality loss so we analyzed whether or not people felt as\nthough denoising causes any quality loss as compared to their noiseless images.\nIndividuals assigned scores indicating the image quality of a denoised photo\ncompared to its noiseless counterpart on a 1 to 10 scale. Survey scores are\ncompared across filters to evaluate whether there were significant differences\nin image quality scores received. Benchmark scores were compared to the visual\nperception scores. Then, an analysis of covariance test was run to identify\nwhether or not survey training scores explained any unplanned variation in\nvisual scores assigned by students across the filter methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 19:18:47 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Draper", "Aidan", ""], ["Taylor", "Laura L.", ""]]}, {"id": "2012.10495", "submitter": "Gaurav Kuppa", "authors": "Gaurav Kuppa, Andrew Jong, Vera Liu, Ziwei Liu, and Teng-Sheng Moh", "title": "ShineOn: Illuminating Design Choices for Practical Video-based Virtual\n  Clothing Try-on", "comments": "In Proceedings of Generation of Human Behavior Workshop at IEEE WACV\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual try-on has garnered interest as a neural rendering benchmark task to\nevaluate complex object transfer and scene composition. Recent works in virtual\nclothing try-on feature a plethora of possible architectural and data\nrepresentation choices. However, they present little clarity on quantifying the\nisolated visual effect of each choice, nor do they specify the hyperparameter\ndetails that are key to experimental reproduction. Our work, ShineOn,\napproaches the try-on task from a bottom-up approach and aims to shine light on\nthe visual and quantitative effects of each experiment. We build a series of\nscientific experiments to isolate effective design choices in video synthesis\nfor virtual clothing try-on. Specifically, we investigate the effect of\ndifferent pose annotations, self-attention layer placement, and activation\nfunctions on the quantitative and qualitative performance of video virtual\ntry-on. We find that DensePose annotations not only enhance face details but\nalso decrease memory usage and training time. Next, we find that attention\nlayers improve face and neck quality. Finally, we show that GELU and ReLU\nactivation functions are the most effective in our experiments despite the\nappeal of newer activations such as Swish and Sine. We will release a\nwell-organized code base, hyperparameters, and model checkpoints to support the\nreproducibility of our results. We expect our extensive experiments and code to\ngreatly inform future design choices in video virtual try-on. Our code may be\naccessed at https://github.com/andrewjong/ShineOn-Virtual-Tryon.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 20:13:09 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 00:14:31 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Kuppa", "Gaurav", ""], ["Jong", "Andrew", ""], ["Liu", "Vera", ""], ["Liu", "Ziwei", ""], ["Moh", "Teng-Sheng", ""]]}, {"id": "2012.10516", "submitter": "Mehrdad Shafiei Dizaji", "authors": "Mehrdad Shafiei Dizaji, Devin Harris", "title": "Computer Vision based Tomography of Structures Using 3D Digital Image\n  Correlation", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Internal properties of a sample can be observed by medical imaging tools,\nsuch as ultrasound devices, magnetic resonance imaging (MRI) and optical\ncoherence tomography (OCT) which are based on relying on changes in material\ndensity or chemical composition [1-21]. As a preliminary investigation, the\nfeasibility to detect interior defects inferred from the discrepancy in\nelasticity modulus distribution of a three-dimensional heterogeneous sample\nusing only surface full-field measurements and finite element model updating as\nan inverse optimization algorithm without any assumption about local\nhomogeneities and also the elasticity modulus distribution is investigated.\nRecently, the authors took advantages of the digital image correlation\ntechnique as a full field measurement in constitutive property identification\nof a full-scale steel component [22-27]. To the extension of previous works, in\nthis brief technical note, the new idea intended at recovering unseen\nvolumetric defect distributions within the interior of three-dimensional\nheterogeneous space of the structural component using 3D-Digital Image\nCorrelation for structural identification [28-57]. As a proof of concept, the\nresults of this paper illustrate the potential to identify invisible internal\ndefect by the proposed computer vision technique establishes the potential for\nnew opportunities to characterize internal heterogeneous materials for their\nmechanical property distribution and condition state.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 21:17:00 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Dizaji", "Mehrdad Shafiei", ""], ["Harris", "Devin", ""]]}, {"id": "2012.10518", "submitter": "Francis Williams", "authors": "Francis Williams, Or Litany, Avneesh Sud, Kevin Swersky, Andrea\n  Tagliasacchi", "title": "Human 3D keypoints via spatial uncertainty modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a technique for 3D human keypoint estimation that directly\nmodels the notion of spatial uncertainty of a keypoint. Our technique employs a\nprincipled approach to modelling spatial uncertainty inspired from techniques\nin robust statistics. Furthermore, our pipeline requires no 3D ground truth\nlabels, relying instead on (possibly noisy) 2D image-level keypoints. Our\nmethod achieves near state-of-the-art performance on Human3.6m while being\nefficient to evaluate and straightforward to\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 21:26:27 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Williams", "Francis", ""], ["Litany", "Or", ""], ["Sud", "Avneesh", ""], ["Swersky", "Kevin", ""], ["Tagliasacchi", "Andrea", ""]]}, {"id": "2012.10530", "submitter": "Scott Workman", "authors": "Scott Workman, Nathan Jacobs", "title": "Dynamic Traffic Modeling From Overhead Imagery", "comments": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to use overhead imagery to understand patterns in traffic flow,\nfor instance answering questions such as how fast could you traverse Times\nSquare at 3am on a Sunday. A traditional approach for solving this problem\nwould be to model the speed of each road segment as a function of time.\nHowever, this strategy is limited in that a significant amount of data must\nfirst be collected before a model can be used and it fails to generalize to new\nareas. Instead, we propose an automatic approach for generating dynamic maps of\ntraffic speeds using convolutional neural networks. Our method operates on\noverhead imagery, is conditioned on location and time, and outputs a local\nmotion model that captures likely directions of travel and corresponding travel\nspeeds. To train our model, we take advantage of historical traffic data\ncollected from New York City. Experimental results demonstrate that our method\ncan be applied to generate accurate city-scale traffic models.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 21:48:03 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Workman", "Scott", ""], ["Jacobs", "Nathan", ""]]}, {"id": "2012.10533", "submitter": "Matthew Sinclair Dr", "authors": "Matthew Sinclair and Andreas Schuh and Karl Hahn and Kersten Petersen\n  and Ying Bai and James Batten and Michiel Schaap and Ben Glocker", "title": "Atlas-ISTN: Joint Segmentation, Registration and Atlas Construction with\n  Image-and-Spatial Transformer Networks", "comments": "33 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models for semantic segmentation are able to learn powerful\nrepresentations for pixel-wise predictions, but are sensitive to noise at test\ntime and do not guarantee a plausible topology. Image registration models on\nthe other hand are able to warp known topologies to target images as a means of\nsegmentation, but typically require large amounts of training data, and have\nnot widely been benchmarked against pixel-wise segmentation models. We propose\nAtlas-ISTN, a framework that jointly learns segmentation and registration on 2D\nand 3D image data, and constructs a population-derived atlas in the process.\nAtlas-ISTN learns to segment multiple structures of interest and to register\nthe constructed, topologically consistent atlas labelmap to an intermediate\npixel-wise segmentation. Additionally, Atlas-ISTN allows for test time\nrefinement of the model's parameters to optimize the alignment of the atlas\nlabelmap to an intermediate pixel-wise segmentation. This process both\nmitigates for noise in the target image that can result in spurious pixel-wise\npredictions, as well as improves upon the one-pass prediction of the model.\nBenefits of the Atlas-ISTN framework are demonstrated qualitatively and\nquantitatively on 2D synthetic data and 3D cardiac computed tomography and\nbrain magnetic resonance image data, out-performing both segmentation and\nregistration baseline models. Atlas-ISTN also provides inter-subject\ncorrespondence of the structures of interest, enabling population-level shape\nand motion analysis.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 21:53:09 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Sinclair", "Matthew", ""], ["Schuh", "Andreas", ""], ["Hahn", "Karl", ""], ["Petersen", "Kersten", ""], ["Bai", "Ying", ""], ["Batten", "James", ""], ["Schaap", "Michiel", ""], ["Glocker", "Ben", ""]]}, {"id": "2012.10544", "submitter": "Micah Goldblum", "authors": "Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi\n  Schwarzschild, Dawn Song, Aleksander Madry, Bo Li, Tom Goldstein", "title": "Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks,\n  and Defenses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As machine learning systems grow in scale, so do their training data\nrequirements, forcing practitioners to automate and outsource the curation of\ntraining data in order to achieve state-of-the-art performance. The absence of\ntrustworthy human supervision over the data collection process exposes\norganizations to security vulnerabilities; training data can be manipulated to\ncontrol and degrade the downstream behaviors of learned models. The goal of\nthis work is to systematically categorize and discuss a wide range of dataset\nvulnerabilities and exploits, approaches for defending against these threats,\nand an array of open problems in this space. In addition to describing various\npoisoning and backdoor threat models and the relationships among them, we\ndevelop their unified taxonomy.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 22:38:47 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 03:03:30 GMT"}, {"version": "v3", "created": "Sun, 28 Mar 2021 19:01:07 GMT"}, {"version": "v4", "created": "Wed, 31 Mar 2021 22:21:34 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Goldblum", "Micah", ""], ["Tsipras", "Dimitris", ""], ["Xie", "Chulin", ""], ["Chen", "Xinyun", ""], ["Schwarzschild", "Avi", ""], ["Song", "Dawn", ""], ["Madry", "Aleksander", ""], ["Li", "Bo", ""], ["Goldstein", "Tom", ""]]}, {"id": "2012.10545", "submitter": "Richard Marriott", "authors": "Richard T. Marriott, Sami Romdhani and Liming Chen", "title": "A 3D GAN for Improved Large-pose Facial Recognition", "comments": "To be presented at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial recognition using deep convolutional neural networks relies on the\navailability of large datasets of face images. Many examples of identities are\nneeded, and for each identity, a large variety of images are needed in order\nfor the network to learn robustness to intra-class variation. In practice, such\ndatasets are difficult to obtain, particularly those containing adequate\nvariation of pose. Generative Adversarial Networks (GANs) provide a potential\nsolution to this problem due to their ability to generate realistic, synthetic\nimages. However, recent studies have shown that current methods of\ndisentangling pose from identity are inadequate. In this work we incorporate a\n3D morphable model into the generator of a GAN in order to learn a nonlinear\ntexture model from in-the-wild images. This allows generation of new, synthetic\nidentities, and manipulation of pose, illumination and expression without\ncompromising the identity. Our synthesised data is used to augment training of\nfacial recognition networks with performance evaluated on the challenging CFP\nand CPLFW datasets.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 22:41:15 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 10:32:18 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Marriott", "Richard T.", ""], ["Romdhani", "Sami", ""], ["Chen", "Liming", ""]]}, {"id": "2012.10548", "submitter": "Richard Marriott", "authors": "Richard T. Marriott, Sami Romdhani, St\\'ephane Gentric and Liming Chen", "title": "Robustness of Facial Recognition to GAN-based Face-morphing Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face-morphing attacks have been a cause for concern for a number of years.\nStriving to remain one step ahead of attackers, researchers have proposed many\nmethods of both creating and detecting morphed images. These detection methods,\nhowever, have generally proven to be inadequate. In this work we identify two\nnew, GAN-based methods that an attacker may already have in his arsenal. Each\nmethod is evaluated against state-of-the-art facial recognition (FR) algorithms\nand we demonstrate that improvements to the fidelity of FR algorithms do lead\nto a reduction in the success rate of attacks provided morphed images are\nconsidered when setting operational acceptance thresholds.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 23:06:20 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Marriott", "Richard T.", ""], ["Romdhani", "Sami", ""], ["Gentric", "St\u00e9phane", ""], ["Chen", "Liming", ""]]}, {"id": "2012.10553", "submitter": "Richard Marriott", "authors": "Richard T. Marriott, Safa Madiouni, Sami Romdhani, St\\'ephane Gentric\n  and Liming Chen", "title": "An Assessment of GANs for Identity-related Applications", "comments": "Presented at IJCB 2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are now capable of producing synthetic\nface images of exceptionally high visual quality. In parallel to the\ndevelopment of GANs themselves, efforts have been made to develop metrics to\nobjectively assess the characteristics of the synthetic images, mainly focusing\non visual quality and the variety of images. Little work has been done,\nhowever, to assess overfitting of GANs and their ability to generate new\nidentities. In this paper we apply a state of the art biometric network to\nvarious datasets of synthetic images and perform a thorough assessment of their\nidentity-related characteristics. We conclude that GANs can indeed be used to\ngenerate new, imagined identities meaning that applications such as\nanonymisation of image sets and augmentation of training datasets with\ndistractor images are viable applications. We also assess the ability of GANs\nto disentangle identity from other image characteristics and propose a novel\nGAN triplet loss that we show to improve this disentanglement.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 23:41:13 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Marriott", "Richard T.", ""], ["Madiouni", "Safa", ""], ["Romdhani", "Sami", ""], ["Gentric", "St\u00e9phane", ""], ["Chen", "Liming", ""]]}, {"id": "2012.10564", "submitter": "Abhishek Dubey", "authors": "Abhishek K Dubey, Michael T Young, Christopher Stanley, Dalton Lunga,\n  Jacob Hinkle", "title": "Computer-aided abnormality detection in chest radiographs in a clinical\n  setting via domain-adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning (DL) models are being deployed at medical centers to aid\nradiologists for diagnosis of lung conditions from chest radiographs. Such\nmodels are often trained on a large volume of publicly available labeled\nradiographs. These pre-trained DL models' ability to generalize in clinical\nsettings is poor because of the changes in data distributions between publicly\navailable and privately held radiographs. In chest radiographs, the\nheterogeneity in distributions arises from the diverse conditions in X-ray\nequipment and their configurations used for generating the images. In the\nmachine learning community, the challenges posed by the heterogeneity in the\ndata generation source is known as domain shift, which is a mode shift in the\ngenerative model. In this work, we introduce a domain-shift detection and\nremoval method to overcome this problem. Our experimental results show the\nproposed method's effectiveness in deploying a pre-trained DL model for\nabnormality detection in chest radiographs in a clinical setting.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 01:01:48 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Dubey", "Abhishek K", ""], ["Young", "Michael T", ""], ["Stanley", "Christopher", ""], ["Lunga", "Dalton", ""], ["Hinkle", "Jacob", ""]]}, {"id": "2012.10565", "submitter": "Edward Zhang", "authors": "Edward Zhang, Ricardo Martin-Brualla, Janne Kontkanen, Brian Curless", "title": "No Shadow Left Behind: Removing Objects and their Shadows using\n  Approximate Lighting and Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing objects from images is a challenging problem that is important for\nmany applications, including mixed reality. For believable results, the shadows\nthat the object casts should also be removed. Current inpainting-based methods\nonly remove the object itself, leaving shadows behind, or at best require\nspecifying shadow regions to inpaint. We introduce a deep learning pipeline for\nremoving a shadow along with its caster. We leverage rough scene models in\norder to remove a wide variety of shadows (hard or soft, dark or subtle, large\nor thin) from surfaces with a wide variety of textures. We train our pipeline\non synthetically rendered data, and show qualitative and quantitative results\non both synthetic and real scenes.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 01:05:40 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhang", "Edward", ""], ["Martin-Brualla", "Ricardo", ""], ["Kontkanen", "Janne", ""], ["Curless", "Brian", ""]]}, {"id": "2012.10580", "submitter": "Xinwei Sun", "authors": "Xinwei Sun, Botong Wu, Wei Chen", "title": "Identifying Invariant Texture Violation for Robust Deepfake Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deepfake detection methods have reported promising in-distribution\nresults, by accessing published large-scale dataset. However, due to the\nnon-smooth synthesis method, the fake samples in this dataset may expose\nobvious artifacts (e.g., stark visual contrast, non-smooth boundary), which\nwere heavily relied on by most of the frame-level detection methods above. As\nthese artifacts do not come up in real media forgeries, the above methods can\nsuffer from a large degradation when applied to fake images that close to\nreality. To improve the robustness for high-realism fake data, we propose the\nInvariant Texture Learning (InTeLe) framework, which only accesses the\npublished dataset with low visual quality. Our method is based on the prior\nthat the microscopic facial texture of the source face is inevitably violated\nby the texture transferred from the target person, which can hence be regarded\nas the invariant characterization shared among all fake images. To learn such\nan invariance for deepfake detection, our InTeLe introduces an auto-encoder\nframework with different decoders for pristine and fake images, which are\nfurther appended with a shallow classifier in order to separate out the obvious\nartifact-effect. Equipped with such a separation, the extracted embedding by\nencoder can capture the texture violation in fake images, followed by the\nclassifier for the final pristine/fake prediction. As a theoretical guarantee,\nwe prove the identifiability of such an invariance texture violation, i.e., to\nbe precisely inferred from observational data. The effectiveness and utility of\nour method are demonstrated by promising generalization ability from\nlow-quality images with obvious artifacts to fake images with high realism.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 03:02:15 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Sun", "Xinwei", ""], ["Wu", "Botong", ""], ["Chen", "Wei", ""]]}, {"id": "2012.10610", "submitter": "Siddha Ganju", "authors": "Anirudh Koul, Siddha Ganju, Meher Kasam, James Parr", "title": "SpaceML: Distributed Open-source Research with Citizen Scientists for\n  the Advancement of Space Technology for NASA", "comments": "Accepted to COSPAR 2021 Workshop on Cloud Computing for Space\n  Sciences. arXiv admin note: text overlap with arXiv:2011.04776. Edit\n  2021/02/16: Converted Space ML to SpaceML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.space-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditionally, academic labs conduct open-ended research with the primary\nfocus on discoveries with long-term value, rather than direct products that can\nbe deployed in the real world. On the other hand, research in the industry is\ndriven by its expected commercial return on investment, and hence focuses on a\nreal world product with short-term timelines. In both cases, opportunity is\nselective, often available to researchers with advanced educational\nbackgrounds. Research often happens behind closed doors and may be kept\nconfidential until either its publication or product release, exacerbating the\nproblem of AI reproducibility and slowing down future research by others in the\nfield. As many research organizations tend to exclusively focus on specific\nareas, opportunities for interdisciplinary research reduce. Undertaking\nlong-term bold research in unexplored fields with non-commercial yet great\npublic value is hard due to factors including the high upfront risk, budgetary\nconstraints, and a lack of availability of data and experts in niche fields.\nOnly a few companies or well-funded research labs can afford to do such\nlong-term research. With research organizations focused on an exploding array\nof fields and resources spread thin, opportunities for the maturation of\ninterdisciplinary research reduce. Apart from these exigencies, there is also a\nneed to engage citizen scientists through open-source contributors to play an\nactive part in the research dialogue. We present a short case study of SpaceML,\nan extension of the Frontier Development Lab, an AI accelerator for NASA.\nSpaceML distributes open-source research and invites volunteer citizen\nscientists to partake in development and deployment of high social value\nproducts at the intersection of space and AI.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 07:00:09 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 18:06:43 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2021 17:31:15 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Koul", "Anirudh", ""], ["Ganju", "Siddha", ""], ["Kasam", "Meher", ""], ["Parr", "James", ""]]}, {"id": "2012.10631", "submitter": "Binyi Su", "authors": "Binyi Su, Haiyong Chen, Zhong Zhou", "title": "BAF-Detector: An Efficient CNN-Based Detector for Photovoltaic Cell\n  Defect Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-scale defect detection for photovoltaic (PV) cell\nelectroluminescence (EL) images is a challenging task, due to the feature\nvanishing as network deepens. To address this problem, an attention-based\ntop-down and bottom-up architecture is developed to accomplish multi-scale\nfeature fusion. This architecture, called Bidirectional Attention Feature\nPyramid Network (BAFPN), can make all layers of the pyramid share similar\nsemantic features. In BAFPN, cosine similarity is employed to measure the\nimportance of each pixel in the fused features. Furthermore, a novel object\ndetector is proposed, called BAF-Detector, which embeds BAFPN into Region\nProposal Network (RPN) in Faster RCNN+FPN. BAFPN improves the robustness of the\nnetwork to scales, thus the proposed detector achieves a good performance in\nmulti-scale defects detection task. Finally, the experimental results on a\nlarge-scale EL dataset including 3629 images, 2129 of which are defective, show\nthat the proposed method achieves 98.70% (F-measure), 88.07% (mAP), and 73.29%\n(IoU) in terms of multi-scale defects classification and detection results in\nraw PV cell EL images.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 08:48:14 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 02:38:09 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Su", "Binyi", ""], ["Chen", "Haiyong", ""], ["Zhou", "Zhong", ""]]}, {"id": "2012.10643", "submitter": "Yingjie Liu", "authors": "Yingjie Liu", "title": "Dense Multiscale Feature Fusion Pyramid Networks for Object Detection in\n  UAV-Captured Images", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although much significant progress has been made in the research field of\nobject detection with deep learning, there still exists a challenging task for\nthe objects with small size, which is notably pronounced in UAV-captured\nimages. Addressing these issues, it is a critical need to explore the feature\nextraction methods that can extract more sufficient feature information of\nsmall objects. In this paper, we propose a novel method called Dense Multiscale\nFeature Fusion Pyramid Networks(DMFFPN), which is aimed at obtaining rich\nfeatures as much as possible, improving the information propagation and reuse.\nSpecifically, the dense connection is designed to fully utilize the\nrepresentation from the different convolutional layers. Furthermore, cascade\narchitecture is applied in the second stage to enhance the localization\ncapability. Experiments on the drone-based datasets named VisDrone-DET suggest\na competitive performance of our method.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 10:05:31 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Liu", "Yingjie", ""]]}, {"id": "2012.10657", "submitter": "Anurag Singh", "authors": "Anurag Singh, Deepak Kumar Sharma, Sudhir Kumar Sharma", "title": "Self-Supervision based Task-Specific Image Collection Summarization", "comments": "Differences between author about ranking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Successful applications of deep learning (DL) requires large amount of\nannotated data. This often restricts the benefits of employing DL to businesses\nand individuals with large budgets for data-collection and computation.\nSummarization offers a possible solution by creating much smaller\nrepresentative datasets that can allow real-time deep learning and analysis of\nbig data and thus democratize use of DL. In the proposed work, our aim is to\nexplore a novel approach to task-specific image corpus summarization using\nsemantic information and self-supervision. Our method uses a\nclassification-based Wasserstein generative adversarial network (CLSWGAN) as a\nfeature generating network. The model also leverages rotational invariance as\nself-supervision and classification on another task. All these objectives are\nadded on a features from resnet34 to make it discriminative and robust. The\nmodel then generates a summary at inference time by using K-means clustering in\nthe semantic embedding space. Thus, another main advantage of this model is\nthat it does not need to be retrained each time to obtain summaries of\ndifferent lengths which is an issue with current end-to-end models. We also\ntest our model efficacy by means of rigorous experiments both qualitatively and\nquantitatively.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 10:58:04 GMT"}, {"version": "v2", "created": "Sat, 26 Dec 2020 20:22:55 GMT"}, {"version": "v3", "created": "Tue, 29 Dec 2020 07:20:23 GMT"}, {"version": "v4", "created": "Fri, 1 Jan 2021 08:58:35 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Singh", "Anurag", ""], ["Sharma", "Deepak Kumar", ""], ["Sharma", "Sudhir Kumar", ""]]}, {"id": "2012.10660", "submitter": "Martin Servin", "authors": "Waqqas-ur-Rehman Butt and Martin Servin", "title": "The importance of silhouette optimization in 3D shape reconstruction\n  system from multiple object scenes", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a multi stage 3D shape reconstruction system of multiple\nobject scenes by considering the silhouette inconsistencies in shape-from\nsilhouette SFS method. These inconsistencies are common in multiple view images\ndue to object occlusions in different views, segmentation and shadows or\nreflection due to objects or light directions. These factors raise huge\nchallenges when attempting to construct the 3D shape by using existing\napproaches which reconstruct only that part of the volume which projects\nconsistently in all the silhouettes, leaving the rest unreconstructed. As a\nresult, final shape are not robust due to multi view objects occlusion and\nshadows. In this regard, we consider the primary factors affecting\nreconstruction by analyzing the multiple images and perform pre-processing\nsteps to optimize the silhouettes. Finally, the 3D shape is reconstructed by\nusing the volumetric approach SFS. Theory and experimental results show that,\nthe performance of the modified algorithm was efficiently improved, which can\nimprove the accuracy of the reconstructed shape and being robust to errors in\nthe silhouettes, volume and computational inexpensive.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 11:16:57 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Butt", "Waqqas-ur-Rehman", ""], ["Servin", "Martin", ""]]}, {"id": "2012.10671", "submitter": "Shreyank N Gowda", "authors": "Shreyank N Gowda, Marcus Rohrbach, Laura Sevilla-Lara", "title": "SMART Frame Selection for Action Recognition", "comments": "To be published in AAAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Action recognition is computationally expensive. In this paper, we address\nthe problem of frame selection to improve the accuracy of action recognition.\nIn particular, we show that selecting good frames helps in action recognition\nperformance even in the trimmed videos domain. Recent work has successfully\nleveraged frame selection for long, untrimmed videos, where much of the content\nis not relevant, and easy to discard. In this work, however, we focus on the\nmore standard short, trimmed action recognition problem. We argue that good\nframe selection can not only reduce the computational cost of action\nrecognition but also increase the accuracy by getting rid of frames that are\nhard to classify. In contrast to previous work, we propose a method that\ninstead of selecting frames by considering one at a time, considers them\njointly. This results in a more efficient selection, where good frames are more\neffectively distributed over the video, like snapshots that tell a story. We\ncall the proposed frame selection SMART and we test it in combination with\ndifferent backbone architectures and on multiple benchmarks (Kinetics,\nSomething-something, UCF101). We show that the SMART frame selection\nconsistently improves the accuracy compared to other frame selection strategies\nwhile reducing the computational cost by a factor of 4 to 10 times.\nAdditionally, we show that when the primary goal is recognition performance,\nour selection strategy can improve over recent state-of-the-art models and\nframe selection strategies on various benchmarks (UCF101, HMDB51, FCVID, and\nActivityNet).\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 12:24:00 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Gowda", "Shreyank N", ""], ["Rohrbach", "Marcus", ""], ["Sevilla-Lara", "Laura", ""]]}, {"id": "2012.10674", "submitter": "Menglin Wang", "authors": "Menglin Wang, Baisheng Lai, Jianqiang Huang, Xiaojin Gong, Xian-Sheng\n  Hua", "title": "Camera-aware Proxies for Unsupervised Person Re-Identification", "comments": "Accepted to AAAI 2021. Code is available at:\n  https://github.com/Terminator8758/CAP-master", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the purely unsupervised person re-identification (Re-ID)\nproblem that requires no annotations. Some previous methods adopt clustering\ntechniques to generate pseudo labels and use the produced labels to train Re-ID\nmodels progressively. These methods are relatively simple but effective.\nHowever, most clustering-based methods take each cluster as a pseudo identity\nclass, neglecting the large intra-ID variance caused mainly by the change of\ncamera views. To address this issue, we propose to split each single cluster\ninto multiple proxies and each proxy represents the instances coming from the\nsame camera. These camera-aware proxies enable us to deal with large intra-ID\nvariance and generate more reliable pseudo labels for learning. Based on the\ncamera-aware proxies, we design both intra- and inter-camera contrastive\nlearning components for our Re-ID model to effectively learn the ID\ndiscrimination ability within and across cameras. Meanwhile, a proxy-balanced\nsampling strategy is also designed, which facilitates our learning further.\nExtensive experiments on three large-scale Re-ID datasets show that our\nproposed approach outperforms most unsupervised methods by a significant\nmargin. Especially, on the challenging MSMT17 dataset, we gain $14.3\\%$ Rank-1\nand $10.2\\%$ mAP improvements when compared to the second place. Code is\navailable at: \\texttt{https://github.com/Terminator8758/CAP-master}.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 12:37:04 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 12:41:42 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Wang", "Menglin", ""], ["Lai", "Baisheng", ""], ["Huang", "Jianqiang", ""], ["Gong", "Xiaojin", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "2012.10685", "submitter": "Idan Pazi", "authors": "Idan Pazi, Dvir Ginzburg, Dan Raviv", "title": "Unsupervised Scale-Invariant Multispectral Shape Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alignment between non-rigid stretchable structures is one of the hardest\ntasks in computer vision, as the invariant properties are hard to define on one\nhand, and on the other hand no labelled data exists for real datasets. We\npresent unsupervised neural network architecture based upon the spectrum of\nscale-invariant geometry. We build ontop the functional maps architecture, but\nshow that learning local features, as done until now, is not enough once the\nisometric assumption breaks but can be solved using scale-invariant geometry.\nOur method is agnostic to local-scale deformations and shows superior\nperformance for matching shapes from different domains when compared to\nexisting spectral state-of-the-art solutions.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 13:44:45 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Pazi", "Idan", ""], ["Ginzburg", "Dvir", ""], ["Raviv", "Dan", ""]]}, {"id": "2012.10692", "submitter": "Xin Jin", "authors": "Xin Jin, Hongyu Zhang, Xiaodong Li, Haoyang Yu, Beisheng Liu, Shujiang\n  Xie, Amit Kumar Singh and Yujie Li", "title": "Confused Modulo Projection based Somewhat Homomorphic Encryption --\n  Cryptosystem, Library and Applications on Secure Smart Cities", "comments": "IEEE Internet of Things Journal (IOTJ), Published Online: 7 August\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the development of cloud computing, the storage and processing of\nmassive visual media data has gradually transferred to the cloud server. For\nexample, if the intelligent video monitoring system cannot process a large\namount of data locally, the data will be uploaded to the cloud. Therefore, how\nto process data in the cloud without exposing the original data has become an\nimportant research topic. We propose a single-server version of somewhat\nhomomorphic encryption cryptosystem based on confused modulo projection theorem\nnamed CMP-SWHE, which allows the server to complete blind data processing\nwithout \\emph{seeing} the effective information of user data. On the client\nside, the original data is encrypted by amplification, randomization, and\nsetting confusing redundancy. Operating on the encrypted data on the server\nside is equivalent to operating on the original data. As an extension, we\ndesigned and implemented a blind computing scheme of accelerated version based\non batch processing technology to improve efficiency. To make this algorithm\neasy to use, we also designed and implemented an efficient general blind\ncomputing library based on CMP-SWHE. We have applied this library to foreground\nextraction, optical flow tracking and object detection with satisfactory\nresults, which are helpful for building smart cities. We also discuss how to\nextend the algorithm to deep learning applications. Compared with other\nhomomorphic encryption cryptosystems and libraries, the results show that our\nmethod has obvious advantages in computing efficiency. Although our algorithm\nhas some tiny errors ($10^{-6}$) when the data is too large, it is very\nefficient and practical, especially suitable for blind image and video\nprocessing.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 14:20:56 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Jin", "Xin", ""], ["Zhang", "Hongyu", ""], ["Li", "Xiaodong", ""], ["Yu", "Haoyang", ""], ["Liu", "Beisheng", ""], ["Xie", "Shujiang", ""], ["Singh", "Amit Kumar", ""], ["Li", "Yujie", ""]]}, {"id": "2012.10704", "submitter": "Michael Ying Yang", "authors": "Logambal Madhuanand, Francesco Nex, Michael Ying Yang", "title": "Self-supervised monocular depth estimation from oblique UAV videos", "comments": "Submitted to ISPRS Journal of Photogrammetry and Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UAVs have become an essential photogrammetric measurement as they are\naffordable, easily accessible and versatile. Aerial images captured from UAVs\nhave applications in small and large scale texture mapping, 3D modelling,\nobject detection tasks, DTM and DSM generation etc. Photogrammetric techniques\nare routinely used for 3D reconstruction from UAV images where multiple images\nof the same scene are acquired. Developments in computer vision and deep\nlearning techniques have made Single Image Depth Estimation (SIDE) a field of\nintense research. Using SIDE techniques on UAV images can overcome the need for\nmultiple images for 3D reconstruction. This paper aims to estimate depth from a\nsingle UAV aerial image using deep learning. We follow a self-supervised\nlearning approach, Self-Supervised Monocular Depth Estimation (SMDE), which\ndoes not need ground truth depth or any extra information other than images for\nlearning to estimate depth. Monocular video frames are used for training the\ndeep learning model which learns depth and pose information jointly through two\ndifferent networks, one each for depth and pose. The predicted depth and pose\nare used to reconstruct one image from the viewpoint of another image utilising\nthe temporal information from videos. We propose a novel architecture with two\n2D CNN encoders and a 3D CNN decoder for extracting information from\nconsecutive temporal frames. A contrastive loss term is introduced for\nimproving the quality of image generation. Our experiments are carried out on\nthe public UAVid video dataset. The experimental results demonstrate that our\nmodel outperforms the state-of-the-art methods in estimating the depths.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 14:53:28 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Madhuanand", "Logambal", ""], ["Nex", "Francesco", ""], ["Yang", "Michael Ying", ""]]}, {"id": "2012.10706", "submitter": "Ziang Cao", "authors": "Changhong Fu, Ziang Cao, Yiming Li, Junjie Ye and Chen Feng", "title": "Siamese Anchor Proposal Network for High-Speed Aerial Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the domain of visual tracking, most deep learning-based trackers highlight\nthe accuracy but casting aside efficiency. Therefore, their real-world\ndeployment on mobile platforms like the unmanned aerial vehicle (UAV) is\nimpeded. In this work, a novel two-stage Siamese network-based method is\nproposed for aerial tracking, i.e., stage-1 for high-quality anchor proposal\ngeneration, stage-2 for refining the anchor proposal. Different from\nanchor-based methods with numerous pre-defined fixed-sized anchors, our\nno-prior method can 1) increase the robustness and generalization to different\nobjects with various sizes, especially to small, occluded, and fast-moving\nobjects, under complex scenarios in light of the adaptive anchor generation, 2)\nmake calculation feasible due to the substantial decrease of anchor numbers. In\naddition, compared to anchor-free methods, our framework has better performance\nowing to refinement at stage-2. Comprehensive experiments on three benchmarks\nhave proven the superior performance of our approach, with a speed of around\n200 frames/s.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 14:53:56 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 03:14:40 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 02:10:09 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Fu", "Changhong", ""], ["Cao", "Ziang", ""], ["Li", "Yiming", ""], ["Ye", "Junjie", ""], ["Feng", "Chen", ""]]}, {"id": "2012.10708", "submitter": "Martin Servin", "authors": "Waqqas-ur-Rehman Butt and Martin Servin", "title": "Static object detection and segmentation in videos based on dual\n  foregrounds difference with noise filtering", "comments": "9 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents static object detection and segmentation method in videos\nfrom cluttered scenes. Robust static object detection is still challenging task\ndue to presence of moving objects in many surveillance applications. The level\nof difficulty is extremely influenced by on how you label the object to be\nidentified as static that do not establish the original background but appeared\nin the video at different time. In this context, background subtraction\ntechnique based on the frame difference concept is applied to the\nidentification of static objects. Firstly, we estimate a frame differencing\nforeground mask image by computing the difference of each frame with respect to\na static reference frame. The Mixture of Gaussian MOG method is applied to\ndetect the moving particles and then outcome foreground mask is subtracted from\nframe differencing foreground mask. Pre-processing techniques, illumination\nequalization and de-hazing methods are applied to handle low contrast and to\nreduce the noise from scattered materials in the air e.g. water droplets and\ndust particles. Finally, a set of mathematical morphological operation and\nlargest connected-component analysis is applied to segment the object and\nsuppress the noise. The proposed method was built for rock breaker station\napplication and effectively validated with real, synthetic and two public data\nsets. The results demonstrate the proposed approach can robustly detect,\nsegmented the static objects without any prior information of tracking.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 15:01:59 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Butt", "Waqqas-ur-Rehman", ""], ["Servin", "Martin", ""]]}, {"id": "2012.10715", "submitter": "Mahdyar Ravanbakhsh", "authors": "Ahmet Kerem Aksoy, Mahdyar Ravanbakhsh, Tristan Kreuziger, Begum Demir", "title": "CCML: A Novel Collaborative Learning Model for Classification of Remote\n  Sensing Images with Noisy Multi-Labels", "comments": "Our code is publicly available online:\n  http://www.noisy-labels-in-rs.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The development of accurate methods for multi-label classification (MLC) of\nremote sensing (RS) images is one of the most important research topics in RS.\nDeep Convolutional Neural Networks (CNNs) based methods have triggered\nsubstantial performance gains in RS MLC problems, requiring a large number of\nreliable training images annotated by multiple land-cover class labels.\nCollecting such data is time-consuming and costly. To address this problem, the\npublicly available thematic products, which can include noisy labels, can be\nused for annotating RS images with zero-labeling cost. However, multi-label\nnoise (which can be associated with wrong as well as missing label annotations)\ncan distort the learning process of the MLC algorithm, resulting in inaccurate\npredictions. The detection and correction of label noise are challenging tasks,\nespecially in a multi-label scenario, where each image can be associated with\nmore than one label. To address this problem, we propose a novel Consensual\nCollaborative Multi-Label Learning (CCML) method to alleviate the adverse\neffects of multi-label noise during the training phase of the CNN model. CCML\nidentifies, ranks, and corrects noisy multi-labels in RS images based on four\nmain modules: 1) group lasso module; 2) discrepancy module; 3) flipping module;\nand 4) swap module. The task of the group lasso module is to detect the\npotentially noisy labels assigned to the multi-labeled training images, and the\ndiscrepancy module ensures that the two collaborative networks learn diverse\nfeatures, while obtaining the same predictions. The flipping module is designed\nto correct the identified noisy multi-labels, while the swap module task is\ndevoted to exchanging the ranking information between two networks. Our code is\npublicly available online: http://www.noisy-labels-in-rs.org\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 15:42:24 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 10:15:06 GMT"}, {"version": "v3", "created": "Wed, 12 May 2021 08:03:46 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Aksoy", "Ahmet Kerem", ""], ["Ravanbakhsh", "Mahdyar", ""], ["Kreuziger", "Tristan", ""], ["Demir", "Begum", ""]]}, {"id": "2012.10728", "submitter": "Xuan Qin", "authors": "Xuan Qin, Meizhu Liu, Yifan Hu, Christina Moo, Christian M. Riblet,\n  Changwei Hu, Kevin Yen and Haibin Ling", "title": "Political Posters Identification with Appearance-Text Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a method that efficiently utilizes appearance\nfeatures and text vectors to accurately classify political posters from other\nsimilar political images. The majority of this work focuses on political\nposters that are designed to serve as a promotion of a certain political event,\nand the automated identification of which can lead to the generation of\ndetailed statistics and meets the judgment needs in a variety of areas.\nStarting with a comprehensive keyword list for politicians and political\nevents, we curate for the first time an effective and practical political\nposter dataset containing 13K human-labeled political images, including 3K\npolitical posters that explicitly support a movement or a campaign. Second, we\nmake a thorough case study for this dataset and analyze common patterns and\noutliers of political posters. Finally, we propose a model that combines the\npower of both appearance and text information to classify political posters\nwith significantly high accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 16:14:51 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Qin", "Xuan", ""], ["Liu", "Meizhu", ""], ["Hu", "Yifan", ""], ["Moo", "Christina", ""], ["Riblet", "Christian M.", ""], ["Hu", "Changwei", ""], ["Yen", "Kevin", ""], ["Ling", "Haibin", ""]]}, {"id": "2012.10744", "submitter": "Yudhik Agrawal", "authors": "Neeraj Battan, Yudhik Agrawal, Veeravalli Saisooryarao, Aman Goel and\n  Avinash Sharma", "title": "GlocalNet: Class-aware Long-term Human Motion Synthesis", "comments": "Appearing in 2021 IEEE Winter Conference on Applications of Computer\n  Vision (WACV)", "journal-ref": "2021 IEEE Winter Conference on Applications of Computer Vision\n  (WACV)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Synthesis of long-term human motion skeleton sequences is essential to aid\nhuman-centric video generation with potential applications in Augmented\nReality, 3D character animations, pedestrian trajectory prediction, etc.\nLong-term human motion synthesis is a challenging task due to multiple factors\nlike, long-term temporal dependencies among poses, cyclic repetition across\nposes, bi-directional and multi-scale dependencies among poses, variable speed\nof actions, and a large as well as partially overlapping space of temporal pose\nvariations across multiple class/types of human activities. This paper aims to\naddress these challenges to synthesize a long-term (> 6000 ms) human motion\ntrajectory across a large variety of human activity classes (>50). We propose a\ntwo-stage activity generation method to achieve this goal, where the first\nstage deals with learning the long-term global pose dependencies in activity\nsequences by learning to synthesize a sparse motion trajectory while the second\nstage addresses the generation of dense motion trajectories taking the output\nof the first stage. We demonstrate the superiority of the proposed method over\nSOTA methods using various quantitative evaluation metrics on publicly\navailable datasets.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 17:50:48 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Battan", "Neeraj", ""], ["Agrawal", "Yudhik", ""], ["Saisooryarao", "Veeravalli", ""], ["Goel", "Aman", ""], ["Sharma", "Avinash", ""]]}, {"id": "2012.10758", "submitter": "Aliaksei Mikhailiuk", "authors": "Aliaksei Mikhailiuk, Maria Perez-Ortiz, Dingcheng Yue, Wilson Suen,\n  Rafal K. Mantiuk", "title": "Consolidated Dataset and Metrics for High-Dynamic-Range Image Quality", "comments": null, "journal-ref": null, "doi": "10.1109/TMM.2021.3076298", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing popularity of high-dynamic-range (HDR) image and video content\nbrings the need for metrics that could predict the severity of image\nimpairments as seen on displays of different brightness levels and dynamic\nrange. Such metrics should be trained and validated on a sufficiently large\nsubjective image quality dataset to ensure robust performance. As the existing\nHDR quality datasets are limited in size, we created a Unified Photometric\nImage Quality dataset (UPIQ) with over 4,000 images by realigning and merging\nexisting HDR and standard-dynamic-range (SDR) datasets. The realigned quality\nscores share the same unified quality scale across all datasets. Such\nrealignment was achieved by collecting additional cross-dataset quality\ncomparisons and re-scaling data with a psychometric scaling method. Images in\nthe proposed dataset are represented in absolute photometric and colorimetric\nunits, corresponding to light emitted from a display. We use the new dataset to\nretrain existing HDR metrics and show that the dataset is sufficiently large\nfor training deep architectures. We show the utility of the dataset on\nbrightness aware image compression.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 18:53:59 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 20:02:42 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Mikhailiuk", "Aliaksei", ""], ["Perez-Ortiz", "Maria", ""], ["Yue", "Dingcheng", ""], ["Suen", "Wilson", ""], ["Mantiuk", "Rafal K.", ""]]}, {"id": "2012.10769", "submitter": "Maciej Sypetkowski", "authors": "Maciej Sypetkowski, Jakub Jasiulewicz, Zbigniew Wojna", "title": "Augmentation Inside the Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present augmentation inside the network, a method that\nsimulates data augmentation techniques for computer vision problems on\nintermediate features of a convolutional neural network. We perform these\ntransformations, changing the data flow through the network, and sharing common\ncomputations when it is possible. Our method allows us to obtain smoother\nspeed-accuracy trade-off adjustment and achieves better results than using\nstandard test-time augmentation (TTA) techniques. Additionally, our approach\ncan improve model performance even further when coupled with test-time\naugmentation. We validate our method on the ImageNet-2012 and CIFAR-100\ndatasets for image classification. We propose a modification that is 30% faster\nthan the flip test-time augmentation and achieves the same results for\nCIFAR-100.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 20:07:03 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Sypetkowski", "Maciej", ""], ["Jasiulewicz", "Jakub", ""], ["Wojna", "Zbigniew", ""]]}, {"id": "2012.10782", "submitter": "Lukas Hoyer", "authors": "Lukas Hoyer, Dengxin Dai, Yuhua Chen, Adrian K\\\"oring, Suman Saha, Luc\n  Van Gool", "title": "Three Ways to Improve Semantic Segmentation with Self-Supervised Depth\n  Estimation", "comments": "CVPR21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep networks for semantic segmentation requires large amounts of\nlabeled training data, which presents a major challenge in practice, as\nlabeling segmentation masks is a highly labor-intensive process. To address\nthis issue, we present a framework for semi-supervised semantic segmentation,\nwhich is enhanced by self-supervised monocular depth estimation from unlabeled\nimage sequences. In particular, we propose three key contributions: (1) We\ntransfer knowledge from features learned during self-supervised depth\nestimation to semantic segmentation, (2) we implement a strong data\naugmentation by blending images and labels using the geometry of the scene, and\n(3) we utilize the depth feature diversity as well as the level of difficulty\nof learning depth in a student-teacher framework to select the most useful\nsamples to be annotated for semantic segmentation. We validate the proposed\nmodel on the Cityscapes dataset, where all three modules demonstrate\nsignificant performance gains, and we achieve state-of-the-art results for\nsemi-supervised semantic segmentation. The implementation is available at\nhttps://github.com/lhoyer/improving_segmentation_with_selfsupervised_depth.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 21:18:03 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 09:46:36 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Hoyer", "Lukas", ""], ["Dai", "Dengxin", ""], ["Chen", "Yuhua", ""], ["K\u00f6ring", "Adrian", ""], ["Saha", "Suman", ""], ["Van Gool", "Luc", ""]]}, {"id": "2012.10787", "submitter": "Rishab Khincha", "authors": "Rishab Khincha, Soundarya Krishnan, Tirtharaj Dash, Lovekesh Vig and\n  Ashwin Srinivasan", "title": "Constructing and Evaluating an Explainable Model for COVID-19 Diagnosis\n  from Chest X-rays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, our focus is on constructing models to assist a clinician in\nthe diagnosis of COVID-19 patients in situations where it is easier and cheaper\nto obtain X-ray data than to obtain high-quality images like those from CT\nscans. Deep neural networks have repeatedly been shown to be capable of\nconstructing highly predictive models for disease detection directly from image\ndata. However, their use in assisting clinicians has repeatedly hit a stumbling\nblock due to their black-box nature. Some of this difficulty can be alleviated\nif predictions were accompanied by explanations expressed in clinically\nrelevant terms. In this paper, deep neural networks are used to extract\ndomain-specific features(morphological features like ground-glass opacity and\ndisease indications like pneumonia) directly from the image data. Predictions\nabout these features are then used to construct a symbolic model (a decision\ntree) for the diagnosis of COVID-19 from chest X-rays, accompanied with two\nkinds of explanations: visual (saliency maps, derived from the neural stage),\nand textual (logical descriptions, derived from the symbolic stage). A\nradiologist rates the usefulness of the visual and textual explanations. Our\nresults demonstrate that neural models can be employed usefully in identifying\ndomain-specific features from low-level image data; that textual explanations\nin terms of clinically relevant features may be useful; and that visual\nexplanations will need to be clinically meaningful to be useful.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 21:33:42 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 12:30:32 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Khincha", "Rishab", ""], ["Krishnan", "Soundarya", ""], ["Dash", "Tirtharaj", ""], ["Vig", "Lovekesh", ""], ["Srinivasan", "Ashwin", ""]]}, {"id": "2012.10802", "submitter": "Rui Fan", "authors": "Rui Fan, Umar Ozgunalp, Yuan Wang, Ming Liu, Ioannis Pitas", "title": "Rethinking Road Surface 3D Reconstruction and Pothole Detection: From\n  Perspective Transformation to Disparity Map Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Potholes are one of the most common forms of road damage, which can severely\naffect driving comfort, road safety and vehicle condition. Pothole detection is\ntypically performed by either structural engineers or certified inspectors.\nThis task is, however, not only hazardous for the personnel but also extremely\ntime-consuming. This paper presents an efficient pothole detection algorithm\nbased on road disparity map estimation and segmentation. We first generalize\nthe perspective transformation by incorporating the stereo rig roll angle. The\nroad disparities are then estimated using semi-global matching. A disparity map\ntransformation algorithm is then performed to better distinguish the damaged\nroad areas. Finally, we utilize simple linear iterative clustering to group the\ntransformed disparities into a collection of superpixels. The potholes are then\ndetected by finding the superpixels, whose values are lower than an adaptively\ndetermined threshold. The proposed algorithm is implemented on an NVIDIA RTX\n2080 Ti GPU in CUDA. The experiments demonstrate the accuracy and efficiency of\nour proposed road pothole detection algorithm, where an accuracy of 99.6% and\nan F-score of 89.4% are achieved.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 22:41:23 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 02:54:59 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Fan", "Rui", ""], ["Ozgunalp", "Umar", ""], ["Wang", "Yuan", ""], ["Liu", "Ming", ""], ["Pitas", "Ioannis", ""]]}, {"id": "2012.10812", "submitter": "Rishab Parthasarathy", "authors": "Rishab Parthasarathy and Rohan Bhowmik", "title": "Quantum Optical Convolutional Neural Network: A Novel Image Recognition\n  Framework for Quantum Computing", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large machine learning models based on Convolutional Neural Networks (CNNs)\nwith rapidly increasing number of parameters, trained with massive amounts of\ndata, are being deployed in a wide array of computer vision tasks from\nself-driving cars to medical imaging. The insatiable demand for computing\nresources required to train these models is fast outpacing the advancement of\nclassical computing hardware, and new frameworks including Optical Neural\nNetworks (ONNs) and quantum computing are being explored as future\nalternatives.\n  In this work, we report a novel quantum computing based deep learning model,\nthe Quantum Optical Convolutional Neural Network (QOCNN), to alleviate the\ncomputational bottleneck in future computer vision applications. Using the\npopular MNIST dataset, we have benchmarked this new architecture against a\ntraditional CNN based on the seminal LeNet model. We have also compared the\nperformance with previously reported ONNs, namely the GridNet and ComplexNet,\nas well as a Quantum Optical Neural Network (QONN) that we built by combining\nthe ComplexNet with quantum based sinusoidal nonlinearities. In essence, our\nwork extends the prior research on QONN by adding quantum convolution and\npooling layers preceding it.\n  We have evaluated all the models by determining their accuracies, confusion\nmatrices, Receiver Operating Characteristic (ROC) curves, and Matthews\nCorrelation Coefficients. The performance of the models were similar overall,\nand the ROC curves indicated that the new QOCNN model is robust. Finally, we\nestimated the gains in computational efficiencies from executing this novel\nframework on a quantum computer. We conclude that switching to a quantum\ncomputing based approach to deep learning may result in comparable accuracies\nto classical models, while achieving unprecedented boosts in computational\nperformances and drastic reduction in power consumption.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 23:10:04 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Parthasarathy", "Rishab", ""], ["Bhowmik", "Rohan", ""]]}, {"id": "2012.10821", "submitter": "Sebastiano Vascon Mr", "authors": "Sebastiano Vascon, Sinem Aslan, Gianluca Bigaglia, Lorenzo Giudice,\n  Marcello Pelillo", "title": "Transductive Visual Verb Sense Disambiguation", "comments": "Accepted at the IEEE Workshop on Application of Computer Vision 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verb Sense Disambiguation is a well-known task in NLP, the aim is to find the\ncorrect sense of a verb in a sentence. Recently, this problem has been extended\nin a multimodal scenario, by exploiting both textual and visual features of\nambiguous verbs leading to a new problem, the Visual Verb Sense Disambiguation\n(VVSD). Here, the sense of a verb is assigned considering the content of an\nimage paired with it rather than a sentence in which the verb appears.\nAnnotating a dataset for this task is more complex than textual disambiguation,\nbecause assigning the correct sense to a pair of $<$image, verb$>$ requires\nboth non-trivial linguistic and visual skills. In this work, differently from\nthe literature, the VVSD task will be performed in a transductive\nsemi-supervised learning (SSL) setting, in which only a small amount of labeled\ninformation is required, reducing tremendously the need for annotated data. The\ndisambiguation process is based on a graph-based label propagation method which\ntakes into account mono or multimodal representations for $<$image, verb$>$\npairs. Experiments have been carried out on the recently published dataset\nVerSe, the only available dataset for this task. The achieved results\noutperform the current state-of-the-art by a large margin while using only a\nsmall fraction of labeled samples per sense. Code available:\nhttps://github.com/GiBg1aN/TVVSD.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 01:07:30 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Vascon", "Sebastiano", ""], ["Aslan", "Sinem", ""], ["Bigaglia", "Gianluca", ""], ["Giudice", "Lorenzo", ""], ["Pelillo", "Marcello", ""]]}, {"id": "2012.10840", "submitter": "Angela Wang", "authors": "Matthew T. Dearing, Xiaoyan Wang", "title": "Analyzing the Performance of Graph Neural Networks with Pipe Parallelism", "comments": "Proceedings of the conference MLSys'21 Workshop on Graph Neural\n  Networks and Systems (GNNSys'21), San Jose, CA, USA, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many interesting datasets ubiquitous in machine learning and deep learning\ncan be described via graphs. As the scale and complexity of graph-structured\ndatasets increase, such as in expansive social networks, protein folding,\nchemical interaction networks, and material phase transitions, improving the\nefficiency of the machine learning techniques applied to these is crucial. In\nthis study, we focus on Graph Neural Networks (GNN) that have found great\nsuccess in tasks such as node or edge classification and link prediction.\nHowever, standard GNN models have scaling limits due to necessary recursive\ncalculations performed through dense graph relationships that lead to memory\nand runtime bottlenecks. While new approaches for processing larger networks\nare needed to advance graph techniques, and several have been proposed, we\nstudy how GNNs could be parallelized using existing tools and frameworks that\nare known to be successful in the deep learning community. In particular, we\ninvestigate applying pipeline parallelism to GNN models with GPipe, introduced\nby Google in 2018.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 04:20:38 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 16:59:33 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Dearing", "Matthew T.", ""], ["Wang", "Xiaoyan", ""]]}, {"id": "2012.10844", "submitter": "Huaxi Huang", "authors": "Huaxi Huang, Junjie Zhang, Jian Zhang, Qiang Wu, Chang Xu", "title": "PTN: A Poisson Transfer Network for Semi-supervised Few-shot Learning", "comments": "AAAI 2021 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The predicament in semi-supervised few-shot learning (SSFSL) is to maximize\nthe value of the extra unlabeled data to boost the few-shot learner. In this\npaper, we propose a Poisson Transfer Network (PTN) to mine the unlabeled\ninformation for SSFSL from two aspects. First, the Poisson Merriman Bence Osher\n(MBO) model builds a bridge for the communications between labeled and\nunlabeled examples. This model serves as a more stable and informative\nclassifier than traditional graph-based SSFSL methods in the message-passing\nprocess of the labels. Second, the extra unlabeled samples are employed to\ntransfer the knowledge from base classes to novel classes through contrastive\nlearning. Specifically, we force the augmented positive pairs close while push\nthe negative ones distant. Our contrastive transfer scheme implicitly learns\nthe novel-class embeddings to alleviate the over-fitting problem on the few\nlabeled data. Thus, we can mitigate the degeneration of embedding generality in\nnovel classes. Extensive experiments indicate that PTN outperforms the\nstate-of-the-art few-shot and SSFSL models on miniImageNet and tieredImageNet\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 04:44:37 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 04:37:33 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 05:20:11 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Huang", "Huaxi", ""], ["Zhang", "Junjie", ""], ["Zhang", "Jian", ""], ["Wu", "Qiang", ""], ["Xu", "Chang", ""]]}, {"id": "2012.10852", "submitter": "Sindhu Hegde", "authors": "Sindhu B Hegde, K R Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri,\n  C.V. Jawahar", "title": "Visual Speech Enhancement Without A Real Visual Stream", "comments": "10 pages, 4 figures, Accepted in WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we re-think the task of speech enhancement in unconstrained\nreal-world environments. Current state-of-the-art methods use only the audio\nstream and are limited in their performance in a wide range of real-world\nnoises. Recent works using lip movements as additional cues improve the quality\nof generated speech over \"audio-only\" methods. But, these methods cannot be\nused for several applications where the visual stream is unreliable or\ncompletely absent. We propose a new paradigm for speech enhancement by\nexploiting recent breakthroughs in speech-driven lip synthesis. Using one such\nmodel as a teacher network, we train a robust student network to produce\naccurate lip movements that mask away the noise, thus acting as a \"visual noise\nfilter\". The intelligibility of the speech enhanced by our pseudo-lip approach\nis comparable (< 3% difference) to the case of using real lips. This implies\nthat we can exploit the advantages of using lip movements even in the absence\nof a real video stream. We rigorously evaluate our model using quantitative\nmetrics as well as human evaluations. Additional ablation studies and a demo\nvideo on our website containing qualitative comparisons and results clearly\nillustrate the effectiveness of our approach. We provide a demo video which\nclearly illustrates the effectiveness of our proposed approach on our website:\n\\url{http://cvit.iiit.ac.in/research/projects/cvit-projects/visual-speech-enhancement-without-a-real-visual-stream}.\nThe code and models are also released for future research:\n\\url{https://github.com/Sindhu-Hegde/pseudo-visual-speech-denoising}.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 06:02:12 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Hegde", "Sindhu B", ""], ["Prajwal", "K R", ""], ["Mukhopadhyay", "Rudrabha", ""], ["Namboodiri", "Vinay", ""], ["Jawahar", "C. V.", ""]]}, {"id": "2012.10856", "submitter": "Parikshit Sakurikar", "authors": "Parikshit Sakurikar, P. J. Narayanan", "title": "Geometric Scene Refocusing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An image captured with a wide-aperture camera exhibits a finite\ndepth-of-field, with focused and defocused pixels. A compact and robust\nrepresentation of focus and defocus helps analyze and manipulate such images.\nIn this work, we study the fine characteristics of images with a shallow\ndepth-of-field in the context of focal stacks. We present a composite measure\nfor focus that is a combination of existing measures. We identify in-focus\npixels, dual-focus pixels, pixels that exhibit bokeh and spatially-varying blur\nkernels between focal slices. We use these to build a novel representation that\nfacilitates easy manipulation of focal stacks. We present a comprehensive\nalgorithm for post-capture refocusing in a geometrically correct manner. Our\napproach can refocus the scene at high fidelity while preserving fine aspects\nof focus and defocus blur.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 06:33:55 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Sakurikar", "Parikshit", ""], ["Narayanan", "P. J.", ""]]}, {"id": "2012.10860", "submitter": "Guangming Wang", "authors": "Guangming Wang, Muyao Chen, Hanwen Liu, Yehui Yang, Zhe Liu, Hesheng\n  Wang", "title": "Anchor-Based Spatio-Temporal Attention 3D Convolutional Networks for\n  Dynamic 3D Point Cloud Sequences", "comments": "10 pages, 6 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of measurement technology, LiDAR and depth cameras\nare widely used in the perception of the 3D environment. Recent learning based\nmethods for robot perception most focus on the image or video, but deep\nlearning methods for dynamic 3D point cloud sequences are underexplored.\nTherefore, developing efficient and accurate perception method compatible with\nthese advanced instruments is pivotal to autonomous driving and service robots.\nAn Anchor-based Spatio-Temporal Attention 3D Convolution operation (ASTA3DConv)\nis proposed in this paper to process dynamic 3D point cloud sequences. The\nproposed convolution operation builds a regular receptive field around each\npoint by setting several virtual anchors around each point. The features of\nneighborhood points are firstly aggregated to each anchor based on the\nspatio-temporal attention mechanism. Then, anchor-based 3D convolution is\nadopted to aggregate these anchors' features to the core points. The proposed\nmethod makes better use of the structured information within the local region\nand learns spatio-temporal embedding features from dynamic 3D point cloud\nsequences. Anchor-based Spatio-Temporal Attention 3D Convolutional Neural\nNetworks (ASTA3DCNNs) are built for classification and segmentation tasks based\non the proposed ASTA3DConv and evaluated on action recognition and semantic\nsegmentation tasks. The experiments and ablation studies on MSRAction3D and\nSynthia datasets demonstrate the superior performance and effectiveness of our\nmethod for dynamic 3D point cloud sequences. Our method achieves the\nstate-of-the-art performance among the methods with dynamic 3D point cloud\nsequences as input on MSRAction3D and Synthia datasets.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 07:35:37 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 13:55:33 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Wang", "Guangming", ""], ["Chen", "Muyao", ""], ["Liu", "Hanwen", ""], ["Yang", "Yehui", ""], ["Liu", "Zhe", ""], ["Wang", "Hesheng", ""]]}, {"id": "2012.10870", "submitter": "Savyasachi Gupta", "authors": "Dhananjai Chand, Savyasachi Gupta, and Ilaiah Kavati", "title": "Computer Vision based Accident Detection for Autonomous Vehicles", "comments": "Contains 6 pages & 4 figures. Presented in 17th INDICON 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous Deep Learning and sensor-based models have been developed to detect\npotential accidents with an autonomous vehicle. However, a self-driving car\nneeds to be able to detect accidents between other vehicles in its path and\ntake appropriate actions such as to slow down or stop and inform the concerned\nauthorities. In this paper, we propose a novel support system for self-driving\ncars that detects vehicular accidents through a dashboard camera. The system\nleverages the Mask R-CNN framework for vehicle detection and a centroid\ntracking algorithm to track the detected vehicle. Additionally, the framework\ncalculates various parameters such as speed, acceleration, and trajectory to\ndetermine whether an accident has occurred between any of the tracked vehicles.\nThe framework has been tested on a custom dataset of dashcam footage and\nachieves a high accident detection rate while maintaining a low false alarm\nrate.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 08:51:10 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Chand", "Dhananjai", ""], ["Gupta", "Savyasachi", ""], ["Kavati", "Ilaiah", ""]]}, {"id": "2012.10873", "submitter": "Aviad Aberdam", "authors": "Aviad Aberdam, Ron Litman, Shahar Tsiper, Oron Anschel, Ron Slossberg,\n  Shai Mazor, R. Manmatha, Pietro Perona", "title": "Sequence-to-Sequence Contrastive Learning for Text Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for sequence-to-sequence contrastive learning (SeqCLR)\nof visual representations, which we apply to text recognition. To account for\nthe sequence-to-sequence structure, each feature map is divided into different\ninstances over which the contrastive loss is computed. This operation enables\nus to contrast in a sub-word level, where from each image we extract several\npositive pairs and multiple negative examples. To yield effective visual\nrepresentations for text recognition, we further suggest novel augmentation\nheuristics, different encoder architectures and custom projection heads.\nExperiments on handwritten text and on scene text show that when a text decoder\nis trained on the learned representations, our method outperforms\nnon-sequential contrastive methods. In addition, when the amount of supervision\nis reduced, SeqCLR significantly improves performance compared with supervised\ntraining, and when fine-tuned with 100% of the labels, our method achieves\nstate-of-the-art results on standard handwritten text recognition benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 09:07:41 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Aberdam", "Aviad", ""], ["Litman", "Ron", ""], ["Tsiper", "Shahar", ""], ["Anschel", "Oron", ""], ["Slossberg", "Ron", ""], ["Mazor", "Shai", ""], ["Manmatha", "R.", ""], ["Perona", "Pietro", ""]]}, {"id": "2012.10878", "submitter": "Savyasachi Gupta", "authors": "Savyasachi Gupta, Dhananjai Chand, and Ilaiah Kavati", "title": "Computer Vision based Animal Collision Avoidance Framework for\n  Autonomous Vehicles", "comments": "Contains 12 pages & 7 figures. Presented in 5th CVIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animals have been a common sighting on roads in India which leads to several\naccidents between them and vehicles every year. This makes it vital to develop\na support system for driverless vehicles that assists in preventing these forms\nof accidents. In this paper, we propose a neoteric framework for avoiding\nvehicle-to-animal collisions by developing an efficient approach for the\ndetection of animals on highways using deep learning and computer vision\ntechniques on dashcam video. Our approach leverages the Mask R-CNN model for\ndetecting and identifying various commonly found animals. Then, we perform lane\ndetection to deduce whether a detected animal is on the vehicle's lane or not\nand track its location and direction of movement using a centroid based object\ntracking algorithm. This approach ensures that the framework is effective at\ndetermining whether an animal is obstructing the path or not of an autonomous\nvehicle in addition to predicting its movement and giving feedback accordingly.\nThis system was tested under various lighting and weather conditions and was\nobserved to perform relatively well, which leads the way for prominent\ndriverless vehicle's support systems for avoiding vehicular collisions with\nanimals on Indian roads in real-time.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 09:51:23 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Gupta", "Savyasachi", ""], ["Chand", "Dhananjai", ""], ["Kavati", "Ilaiah", ""]]}, {"id": "2012.10880", "submitter": "Yan Luo", "authors": "Yan Luo, Chongyang Zhang, Muming Zhao, Hao Zhou, Jun Sun", "title": "Where, What, Whether: Multi-modal Learning Meets Pedestrian Detection", "comments": "This work is being revised. The updated version will be upload few\n  months later", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pedestrian detection benefits greatly from deep convolutional neural networks\n(CNNs). However, it is inherently hard for CNNs to handle situations in the\npresence of occlusion and scale variation. In this paper, we propose W$^3$Net,\nwhich attempts to address above challenges by decomposing the pedestrian\ndetection task into \\textbf{\\textit{W}}here, \\textbf{\\textit{W}}hat and\n\\textbf{\\textit{W}}hether problem directing against pedestrian localization,\nscale prediction and classification correspondingly. Specifically, for a\npedestrian instance, we formulate its feature by three steps. i) We generate a\nbird view map, which is naturally free from occlusion issues, and scan all\npoints on it to look for suitable locations for each pedestrian instance. ii)\nInstead of utilizing pre-fixed anchors, we model the interdependency between\ndepth and scale aiming at generating depth-guided scales at different locations\nfor better matching instances of different sizes. iii) We learn a latent vector\nshared by both visual and corpus space, by which false positives with similar\nvertical structure but lacking human partial features would be filtered out. We\nachieve state-of-the-art results on widely used datasets (Citypersons and\nCaltech). In particular. when evaluating on heavy occlusion subset, our results\nreduce MR$^{-2}$ from 49.3$\\%$ to 18.7$\\%$ on Citypersons, and from 45.18$\\%$\nto 28.33$\\%$ on Caltech.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 10:15:39 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Luo", "Yan", ""], ["Zhang", "Chongyang", ""], ["Zhao", "Muming", ""], ["Zhou", "Hao", ""], ["Sun", "Jun", ""]]}, {"id": "2012.10890", "submitter": "Guoqing Wang", "authors": "Chao Yang, Guoqing Wang, Dongsheng Li, Huawei Shen, Su Feng, Bin Jiang", "title": "PPGN: Phrase-Guided Proposal Generation Network For Referring Expression\n  Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reference expression comprehension (REC) aims to find the location that the\nphrase refer to in a given image. Proposal generation and proposal\nrepresentation are two effective techniques in many two-stage REC methods.\nHowever, most of the existing works only focus on proposal representation and\nneglect the importance of proposal generation. As a result, the low-quality\nproposals generated by these methods become the performance bottleneck in REC\ntasks. In this paper, we reconsider the problem of proposal generation, and\npropose a novel phrase-guided proposal generation network (PPGN). The main\nimplementation principle of PPGN is refining visual features with text and\ngenerate proposals through regression. Experiments show that our method is\neffective and achieve SOTA performance in benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 11:21:06 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Yang", "Chao", ""], ["Wang", "Guoqing", ""], ["Li", "Dongsheng", ""], ["Shen", "Huawei", ""], ["Feng", "Su", ""], ["Jiang", "Bin", ""]]}, {"id": "2012.10898", "submitter": "Chenxi Duan", "authors": "Chenxi Duan, Rui Li", "title": "Multi-Head Linear Attention Generative Adversarial Network for Thin\n  Cloud Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In remote sensing images, the existence of the thin cloud is an inevitable\nand ubiquitous phenomenon that crucially reduces the quality of imageries and\nlimits the scenarios of application. Therefore, thin cloud removal is an\nindispensable procedure to enhance the utilization of remote sensing images.\nGenerally, even though contaminated by thin clouds, the pixels still retain\nmore or less surface information. Hence, different from thick cloud removal,\nthin cloud removal algorithms normally concentrate on inhibiting the cloud\ninfluence rather than substituting the cloud-contaminated pixels. Meanwhile,\nconsidering the surface features obscured by the cloud are usually similar to\nadjacent areas, the dependency between each pixel of the input is useful to\nreconstruct contaminated areas. In this paper, to make full use of the\ndependencies between pixels of the image, we propose a Multi-Head Linear\nAttention Generative Adversarial Network (MLAGAN) for Thin Cloud Removal. The\nMLA-GAN is based on the encoding-decoding framework consisting of multiple\nattention-based layers and deconvolutional layers. Compared with six deep\nlearning-based thin cloud removal benchmarks, the experimental results on the\nRICE1 and RICE2 datasets demonstrate that the proposed framework MLA-GAN has\ndominant advantages in thin cloud removal.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 11:50:54 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Duan", "Chenxi", ""], ["Li", "Rui", ""]]}, {"id": "2012.10902", "submitter": "Ioan Andrei B\\^arsan", "authors": "Ioan Andrei B\\^arsan, Shenlong Wang, Andrei Pokrovsky, Raquel Urtasun", "title": "Learning to Localize Using a LiDAR Intensity Map", "comments": "12 pages, 7 figures, 5 tables; Presented at the 2nd Conference on\n  Robot Learning (CoRL), 2018", "journal-ref": "Proceedings of The 2nd Conference on Robot Learning, PMLR\n  87:605-616, 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a real-time, calibration-agnostic and effective\nlocalization system for self-driving cars. Our method learns to embed the\nonline LiDAR sweeps and intensity map into a joint deep embedding space.\nLocalization is then conducted through an efficient convolutional matching\nbetween the embeddings. Our full system can operate in real-time at 15Hz while\nachieving centimeter level accuracy across different LiDAR sensors and\nenvironments. Our experiments illustrate the performance of the proposed\napproach over a large-scale dataset consisting of over 4000km of driving.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 11:56:23 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["B\u00e2rsan", "Ioan Andrei", ""], ["Wang", "Shenlong", ""], ["Pokrovsky", "Andrei", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2012.10921", "submitter": "Mutian Xu", "authors": "Mutian Xu, Junhao Zhang, Zhipeng Zhou, Mingye Xu, Xiaojuan Qi, Yu Qiao", "title": "Learning Geometry-Disentangled Representation for Complementary\n  Understanding of 3D Object Point Cloud", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2D image processing, some attempts decompose images into high and low\nfrequency components for describing edge and smooth parts respectively.\nSimilarly, the contour and flat area of 3D objects, such as the boundary and\nseat area of a chair, describe different but also complementary geometries.\nHowever, such investigation is lost in previous deep networks that understand\npoint clouds by directly treating all points or local patches equally. To solve\nthis problem, we propose Geometry-Disentangled Attention Network (GDANet).\nGDANet introduces Geometry-Disentangle Module to dynamically disentangle point\nclouds into the contour and flat part of 3D objects, respectively denoted by\nsharp and gentle variation components. Then GDANet exploits Sharp-Gentle\nComplementary Attention Module that regards the features from sharp and gentle\nvariation components as two holistic representations, and pays different\nattentions to them while fusing them respectively with original point cloud\nfeatures. In this way, our method captures and refines the holistic and\ncomplementary 3D geometric semantics from two distinct disentangled components\nto supplement the local information. Extensive experiments on 3D object\nclassification and segmentation benchmarks demonstrate that GDANet achieves the\nstate-of-the-arts with fewer parameters. Code is released on\nhttps://github.com/mutianxu/GDANet.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 13:35:00 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 08:36:53 GMT"}, {"version": "v3", "created": "Sun, 7 Feb 2021 06:45:10 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Xu", "Mutian", ""], ["Zhang", "Junhao", ""], ["Zhou", "Zhipeng", ""], ["Xu", "Mingye", ""], ["Qi", "Xiaojuan", ""], ["Qiao", "Yu", ""]]}, {"id": "2012.10930", "submitter": "Xiao Zhang", "authors": "Xiao Zhang, Chunsheng Liu, Faliang Chang", "title": "Guidance Module Network for Video Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video captioning has been a challenging and significant task that describes\nthe content of a video clip in a single sentence. The model of video captioning\nis usually an encoder-decoder. We find that the normalization of extracted\nvideo features can improve the final performance of video captioning.\nEncoder-decoder model is usually trained using teacher-enforced strategies to\nmake the prediction probability of each word close to a 0-1 distribution and\nignore other words. In this paper, we present a novel architecture which\nintroduces a guidance module to encourage the encoder-decoder model to generate\nwords related to the past and future words in a caption. Based on the\nnormalization and guidance module, guidance module net (GMNet) is built.\nExperimental results on commonly used dataset MSVD show that proposed GMNet can\nimprove the performance of the encoder-decoder model on video captioning tasks.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 14:02:28 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhang", "Xiao", ""], ["Liu", "Chunsheng", ""], ["Chang", "Faliang", ""]]}, {"id": "2012.10932", "submitter": "Qingjie Liu", "authors": "Hao Zeng and Qingjie Liu and Mingming Zhang and Xiaoqing Han and\n  Yunhong Wang", "title": "Semi-supervised Hyperspectral Image Classification with Graph Clustering\n  Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral image classification (HIC) is an important but challenging\ntask, and a problem that limits the algorithmic development in this field is\nthat the ground truths of hyperspectral images (HSIs) are extremely hard to\nobtain. Recently a handful of HIC methods are developed based on the graph\nconvolution networks (GCNs), which effectively relieves the scarcity of labeled\ndata for deep learning based HIC methods. To further lift the classification\nperformance, in this work we propose a graph convolution network (GCN) based\nframework for HSI classification that uses two clustering operations to better\nexploit multi-hop node correlations and also effectively reduce graph size. In\nparticular, we first cluster the pixels with similar spectral features into a\nsuperpixel and build the graph based on the superpixels of the input HSI. Then\ninstead of performing convolution over this superpixel graph, we further\npartition it into several sub-graphs by pruning the edges with weak weights, so\nas to strengthen the correlations of nodes with high similarity. This second\nround of clustering also further reduces the graph size, thus reducing the\ncomputation burden of graph convolution. Experimental results on three widely\nused benchmark datasets well prove the effectiveness of our proposed framework.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 14:16:59 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zeng", "Hao", ""], ["Liu", "Qingjie", ""], ["Zhang", "Mingming", ""], ["Han", "Xiaoqing", ""], ["Wang", "Yunhong", ""]]}, {"id": "2012.10941", "submitter": "Lucas Ventura", "authors": "Lucas Ventura, Amanda Duarte, Xavier Giro-i-Nieto", "title": "Can Everybody Sign Now? Exploring Sign Language Video Generation from 2D\n  Poses", "comments": "Video here: https://youtu.be/4ve1sGzWl2g", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work have addressed the generation of human poses represented by 2D/3D\ncoordinates of human joints for sign language. We use the state of the art in\nDeep Learning for motion transfer and evaluate them on How2Sign, an American\nSign Language dataset, to generate videos of signers performing sign language\ngiven a 2D pose skeleton. We evaluate the generated videos quantitatively and\nqualitatively showing that the current models are not enough to generated\nadequate videos for Sign Language due to lack of detail in hands.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 14:43:32 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 14:44:33 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ventura", "Lucas", ""], ["Duarte", "Amanda", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "2012.10942", "submitter": "Ioan Andrei B\\^arsan", "authors": "Xinkai Wei, Ioan Andrei B\\^arsan, Shenlong Wang, Julieta Martinez,\n  Raquel Urtasun", "title": "Learning to Localize Through Compressed Binary Maps", "comments": "18 pages, 12 figures, 6 tables; Presented at CVPR 2019", "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2019, pp. 10316-10324", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main difficulties of scaling current localization systems to large\nenvironments is the on-board storage required for the maps. In this paper we\npropose to learn to compress the map representation such that it is optimal for\nthe localization task. As a consequence, higher compression rates can be\nachieved without loss of localization accuracy when compared to standard coding\nschemes that optimize for reconstruction, thus ignoring the end task. Our\nexperiments show that it is possible to learn a task-specific compression which\nreduces storage requirements by two orders of magnitude over general-purpose\ncodecs such as WebP without sacrificing performance.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 14:47:15 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Wei", "Xinkai", ""], ["B\u00e2rsan", "Ioan Andrei", ""], ["Wang", "Shenlong", ""], ["Martinez", "Julieta", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2012.10952", "submitter": "Yong Wang", "authors": "Yutong Cai, Yong Wang", "title": "MA-Unet: An improved version of Unet based on multi-scale and attention\n  mechanism for medical image segmentation", "comments": "13 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although convolutional neural networks (CNNs) are promoting the development\nof medical image semantic segmentation, the standard model still has some\nshortcomings. First, the feature mapping from the encoder and decoder\nsub-networks in the skip connection operation has a large semantic difference.\nSecond, the remote feature dependence is not effectively modeled. Third, the\nglobal context information of different scales is ignored. In this paper, we\ntry to eliminate semantic ambiguity in skip connection operations by adding\nattention gates (AGs), and use attention mechanisms to combine local features\nwith their corresponding global dependencies, explicitly model the dependencies\nbetween channels and use multi-scale predictive fusion to utilize global\ninformation at different scales. Compared with other state-of-the-art\nsegmentation networks, our model obtains better segmentation performance while\nintroducing fewer parameters.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 15:29:18 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Cai", "Yutong", ""], ["Wang", "Yong", ""]]}, {"id": "2012.10958", "submitter": "Reza Maalek", "authors": "Reza Maalek, Derek Lichti, and Shahrokh Maalek", "title": "Towards Automatic Digital Documentation and Progress Reporting of\n  Mechanical Construction Pipes using Smartphones", "comments": null, "journal-ref": null, "doi": "10.1016/j.autcon.2021.103735", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This manuscript presents a new framework towards automated digital\ndocumentation and progress reporting of mechanical pipes in building\nconstruction projects, using smartphones. New methods were proposed to optimize\nvideo frame rate to achieve a desired image overlap; define metric scale for 3D\nreconstruction; extract pipes from point clouds; and classify pipes according\nto their planned bill of quantity radii. The effectiveness of the proposed\nmethods in both laboratory (six pipes) and construction site (58 pipes)\nconditions was evaluated. It was observed that the proposed metric scale\ndefinition achieved sub-millimeter pipe radius estimation accuracy. Both\nlaboratory and field experiments revealed that increasing the defined image\noverlap improved point cloud quality, pipe classification quality, and pipe\nradius/length estimation. Overall, it was found possible to achieve pipe\nclassification F-measure, radius estimation accuracy, and length estimation\npercent error of 96.4%, 5.4mm, and 5.0%, respectively, on construction sites\nusing at least 95% image overlap.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 15:53:34 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 17:29:06 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Maalek", "Reza", ""], ["Lichti", "Derek", ""], ["Maalek", "Shahrokh", ""]]}, {"id": "2012.10961", "submitter": "Shahzad Akbar", "authors": "Syed Ale Hassan, Shahzad Akbar", "title": "Recent Developments in Detection of Central Serous Retinopathy through\n  Imaging and Artificial Intelligence Techniques A Review", "comments": "18 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Central Serous Retinopathy (CSR) is a major significant disease\nresponsible for causing blindness and vision loss among numerous people across\nthe globe. This disease is also known as the Central Serous Chorioretinopathy\n(CSC) occurs due to the accumulation of watery fluids behind the retina. The\ndetection of CSR at an early stage allows taking preventive measures to avert\nany impairment to the human eye. Traditionally, several manual detection\nmethods were developed for observing CSR, but they were proven to be\ninaccurate, unreliable, and time-consuming. Consequently, the research\ncommunity embarked on seeking automated solutions for CSR detection. With the\nadvent of modern technology in the 21st century, Artificial Intelligence (AI)\ntechniques are immensely popular in numerous research fields including the\nautomated CSR detection. This paper offers a comprehensive review of various\nadvanced technologies and researches, contributing to the automated CSR\ndetection in this scenario. Additionally, it discusses the benefits and\nlimitations of many classical imaging methods ranging from Optical Coherence\nTomography (OCT) and the Fundus imaging, to more recent approaches like AI\nbased Machine/Deep Learning techniques. Study primary objective is to analyze\nand compare many Artificial Intelligence (AI) algorithms that have efficiently\nachieved automated CSR detection using OCT imaging. Furthermore, it describes\nvarious retinal datasets and strategies proposed for CSR assessment and\naccuracy. Finally, it is concluded that the most recent Deep Learning (DL)\nclassifiers are performing accurate, fast, and reliable detection of CSR.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 16:08:56 GMT"}, {"version": "v2", "created": "Sat, 26 Dec 2020 17:15:03 GMT"}, {"version": "v3", "created": "Sat, 20 Mar 2021 05:35:56 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Hassan", "Syed Ale", ""], ["Akbar", "Shahzad", ""]]}, {"id": "2012.10974", "submitter": "Vladislav Golyanik", "authors": "Moritz Kappel and Vladislav Golyanik and Mohamed Elgharib and Jann-Ole\n  Henningson and Hans-Peter Seidel and Susana Castillo and Christian Theobalt\n  and Marcus Magnor", "title": "High-Fidelity Neural Human Motion Transfer from Monocular Video", "comments": "14 pages, 8 figures; project page:\n  https://graphics.tu-bs.de/publications/kappel2020high-fidelity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based human motion transfer creates video animations of humans\nfollowing a source motion. Current methods show remarkable results for\ntightly-clad subjects. However, the lack of temporally consistent handling of\nplausible clothing dynamics, including fine and high-frequency details,\nsignificantly limits the attainable visual quality. We address these\nlimitations for the first time in the literature and present a new framework\nwhich performs high-fidelity and temporally-consistent human motion transfer\nwith natural pose-dependent non-rigid deformations, for several types of loose\ngarments. In contrast to the previous techniques, we perform image generation\nin three subsequent stages, synthesizing human shape, structure, and\nappearance. Given a monocular RGB video of an actor, we train a stack of\nrecurrent deep neural networks that generate these intermediate representations\nfrom 2D poses and their temporal derivatives. Splitting the difficult motion\ntransfer problem into subtasks that are aware of the temporal motion context\nhelps us to synthesize results with plausible dynamics and pose-dependent\ndetail. It also allows artistic control of results by manipulation of\nindividual framework stages. In the experimental results, we significantly\noutperform the state-of-the-art in terms of video realism. Our code and data\nwill be made publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 16:54:38 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Kappel", "Moritz", ""], ["Golyanik", "Vladislav", ""], ["Elgharib", "Mohamed", ""], ["Henningson", "Jann-Ole", ""], ["Seidel", "Hans-Peter", ""], ["Castillo", "Susana", ""], ["Theobalt", "Christian", ""], ["Magnor", "Marcus", ""]]}, {"id": "2012.10992", "submitter": "Bin Yang", "authors": "Ming Liang, Bin Yang, Shenlong Wang, Raquel Urtasun", "title": "Deep Continuous Fusion for Multi-Sensor 3D Object Detection", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a novel 3D object detector that can exploit both\nLIDAR as well as cameras to perform very accurate localization. Towards this\ngoal, we design an end-to-end learnable architecture that exploits continuous\nconvolutions to fuse image and LIDAR feature maps at different levels of\nresolution. Our proposed continuous fusion layer encode both discrete-state\nimage features as well as continuous geometric information. This enables us to\ndesign a novel, reliable and efficient end-to-end learnable 3D object detector\nbased on multiple sensors. Our experimental evaluation on both KITTI as well as\na large scale 3D object detection benchmark shows significant improvements over\nthe state of the art.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 18:43:41 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Liang", "Ming", ""], ["Yang", "Bin", ""], ["Wang", "Shenlong", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2012.11002", "submitter": "Mai Bui", "authors": "Haowen Deng, Mai Bui, Nassir Navab, Leonidas Guibas, Slobodan Ilic,\n  Tolga Birdal", "title": "Deep Bingham Networks: Dealing with Uncertainty and Ambiguity in Pose\n  Estimation", "comments": "arXiv admin note: text overlap with arXiv:2004.04807", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce Deep Bingham Networks (DBN), a generic framework\nthat can naturally handle pose-related uncertainties and ambiguities arising in\nalmost all real life applications concerning 3D data. While existing works\nstrive to find a single solution to the pose estimation problem, we make peace\nwith the ambiguities causing high uncertainty around which solutions to\nidentify as the best. Instead, we report a family of poses which capture the\nnature of the solution space. DBN extends the state of the art direct pose\nregression networks by (i) a multi-hypotheses prediction head which can yield\ndifferent distribution modes; and (ii) novel loss functions that benefit from\nBingham distributions on rotations. This way, DBN can work both in unambiguous\ncases providing uncertainty information, and in ambiguous scenes where an\nuncertainty per mode is desired. On a technical front, our network regresses\ncontinuous Bingham mixture models and is applicable to both 2D data such as\nimages and to 3D data such as point clouds. We proposed new training strategies\nso as to avoid mode or posterior collapse during training and to improve\nnumerical stability. Our methods are thoroughly tested on two different\napplications exploiting two different modalities: (i) 6D camera relocalization\nfrom images; and (ii) object pose estimation from 3D point clouds,\ndemonstrating decent advantages over the state of the art. For the former we\ncontributed our own dataset composed of five indoor scenes where it is\nunavoidable to capture images corresponding to views that are hard to uniquely\nidentify. For the latter we achieve the top results especially for symmetric\nobjects of ModelNet dataset.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 19:20:26 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Deng", "Haowen", ""], ["Bui", "Mai", ""], ["Navab", "Nassir", ""], ["Guibas", "Leonidas", ""], ["Ilic", "Slobodan", ""], ["Birdal", "Tolga", ""]]}, {"id": "2012.11014", "submitter": "Kenneth Marino", "authors": "Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, Marcus\n  Rohrbach", "title": "KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain\n  Knowledge-Based VQA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most challenging question types in VQA is when answering the\nquestion requires outside knowledge not present in the image. In this work we\nstudy open-domain knowledge, the setting when the knowledge required to answer\na question is not given/annotated, neither at training nor test time. We tap\ninto two types of knowledge representations and reasoning. First, implicit\nknowledge which can be learned effectively from unsupervised language\npre-training and supervised training data with transformer-based models.\nSecond, explicit, symbolic knowledge encoded in knowledge bases. Our approach\ncombines both - exploiting the powerful implicit reasoning of transformer\nmodels for answer prediction, and integrating symbolic representations from a\nknowledge graph, while never losing their explicit semantics to an implicit\nembedding. We combine diverse sources of knowledge to cover the wide variety of\nknowledge needed to solve knowledge-based questions. We show our approach,\nKRISP (Knowledge Reasoning with Implicit and Symbolic rePresentations),\nsignificantly outperforms state-of-the-art on OK-VQA, the largest available\ndataset for open-domain knowledge-based VQA. We show with extensive ablations\nthat while our model successfully exploits implicit knowledge reasoning, the\nsymbolic answer module which explicitly connects the knowledge graph to the\nanswer vocabulary is critical to the performance of our method and generalizes\nto rare answers.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 20:13:02 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Marino", "Kenneth", ""], ["Chen", "Xinlei", ""], ["Parikh", "Devi", ""], ["Gupta", "Abhinav", ""], ["Rohrbach", "Marcus", ""]]}, {"id": "2012.11025", "submitter": "Abhishek Singh", "authors": "Abhishek Singh, Ayush Chopra, Vivek Sharma, Ethan Garza, Emily Zhang,\n  Praneeth Vepakomma, Ramesh Raskar", "title": "DISCO: Dynamic and Invariant Sensitive Channel Obfuscation for deep\n  neural networks", "comments": "Presented at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning models have shown remarkable performance in image\nclassification. While these deep learning systems are getting closer to\npractical deployment, the common assumption made about data is that it does not\ncarry any sensitive information. This assumption may not hold for many\npractical cases, especially in the domain where an individual's personal\ninformation is involved, like healthcare and facial recognition systems. We\nposit that selectively removing features in this latent space can protect the\nsensitive information and provide a better privacy-utility trade-off.\nConsequently, we propose DISCO which learns a dynamic and data driven pruning\nfilter to selectively obfuscate sensitive information in the feature space. We\npropose diverse attack schemes for sensitive inputs \\& attributes and\ndemonstrate the effectiveness of DISCO against state-of-the-art methods through\nquantitative and qualitative evaluation. Finally, we also release an evaluation\nbenchmark dataset of 1 million sensitive representations to encourage rigorous\nexploration of novel attack schemes.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 21:15:13 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 07:39:03 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Singh", "Abhishek", ""], ["Chopra", "Ayush", ""], ["Sharma", "Vivek", ""], ["Garza", "Ethan", ""], ["Zhang", "Emily", ""], ["Vepakomma", "Praneeth", ""], ["Raskar", "Ramesh", ""]]}, {"id": "2012.11049", "submitter": "Alejandro Martin", "authors": "Javier Huertas-Tato, Alejandro Mart\\'in, Juli\\'an Fierrez, David\n  Camacho", "title": "Fusing CNNs and statistical indicators to improve image classification", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Networks have dominated the field of computer vision for the\nlast ten years, exhibiting extremely powerful feature extraction capabilities\nand outstanding classification performance. The main strategy to prolong this\ntrend relies on further upscaling networks in size. However, costs increase\nrapidly while performance improvements may be marginal. We hypothesise that\nadding heterogeneous sources of information may be more cost-effective to a CNN\nthan building a bigger network. In this paper, an ensemble method is proposed\nfor accurate image classification, fusing automatically detected features\nthrough Convolutional Neural Network architectures with a set of manually\ndefined statistical indicators. Through a combination of the predictions of a\nCNN and a secondary classifier trained on statistical features, better\nclassification performance can be cheaply achieved. We test multiple learning\nalgorithms and CNN architectures on a diverse number of datasets to validate\nour proposal, making public all our code and data via GitHub. According to our\nresults, the inclusion of additional indicators and an ensemble classification\napproach helps to increase the performance in 8 of 9 datasets, with a\nremarkable increase of more than 10% precision in two of them.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 23:24:31 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 11:16:48 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Huertas-Tato", "Javier", ""], ["Mart\u00edn", "Alejandro", ""], ["Fierrez", "Juli\u00e1n", ""], ["Camacho", "David", ""]]}, {"id": "2012.11101", "submitter": "Jiemin Fang", "authors": "Jie Qin, Jiemin Fang, Qian Zhang, Wenyu Liu, Xingang Wang, Xinggang\n  Wang", "title": "ResizeMix: Mixing Data with Preserved Object Information and True Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is a powerful technique to increase the diversity of data,\nwhich can effectively improve the generalization ability of neural networks in\nimage recognition tasks. Recent data mixing based augmentation strategies have\nachieved great success. Especially, CutMix uses a simple but effective method\nto improve the classifiers by randomly cropping a patch from one image and\npasting it on another image. To further promote the performance of CutMix, a\nseries of works explore to use the saliency information of the image to guide\nthe mixing. We systematically study the importance of the saliency information\nfor mixing data, and find that the saliency information is not so necessary for\npromoting the augmentation performance. Furthermore, we find that the cutting\nbased data mixing methods carry two problems of label misallocation and object\ninformation missing, which cannot be resolved simultaneously. We propose a more\neffective but very easily implemented method, namely ResizeMix. We mix the data\nby directly resizing the source image to a small patch and paste it on another\nimage. The obtained patch preserves more substantial object information\ncompared with conventional cut-based methods. ResizeMix shows evident\nadvantages over CutMix and the saliency-guided methods on both image\nclassification and object detection tasks without additional computation cost,\nwhich even outperforms most costly search-based automatic augmentation methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 03:43:13 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Qin", "Jie", ""], ["Fang", "Jiemin", ""], ["Zhang", "Qian", ""], ["Liu", "Wenyu", ""], ["Wang", "Xingang", ""], ["Wang", "Xinggang", ""]]}, {"id": "2012.11107", "submitter": "Botong Wu", "authors": "Botong Wu, Sijie Ren, Jing Li, Xinwei Sun, Shiming Li, Yizhou Wang", "title": "Forecasting Irreversible Disease via Progression Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting Parapapillary atrophy (PPA), i.e., a symptom related to most\nirreversible eye diseases, provides an alarm for implementing an intervention\nto slow down the disease progression at early stage. A key question for this\nforecast is: how to fully utilize the historical data (e.g., retinal image) up\nto the current stage for future disease prediction? In this paper, we provide\nan answer with a novel framework, namely \\textbf{D}isease \\textbf{F}orecast via\n\\textbf{P}rogression \\textbf{L}earning (\\textbf{DFPL}), which exploits the\nirreversibility prior (i.e., cannot be reversed once diagnosed). Specifically,\nbased on this prior, we decompose two factors that contribute to the prediction\nof the future disease: i) the current disease label given the data (retinal\nimage, clinical attributes) at present and ii) the future disease label given\nthe progression of the retinal images that from the current to the future. To\nmodel these two factors, we introduce the current and progression predictors in\nDFPL, respectively. In order to account for the degree of progression of the\ndisease, we propose a temporal generative model to accurately generate the\nfuture image and compare it with the current one to get a residual image. The\ngenerative model is implemented by a recurrent neural network, in order to\nexploit the dependency of the historical data. To verify our approach, we apply\nit to a PPA in-house dataset and it yields a significant improvement\n(\\textit{e.g.}, \\textbf{4.48\\%} of accuracy; \\textbf{3.45\\%} of AUC) over\nothers. Besides, our generative model can accurately localize the\ndisease-related regions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 03:54:35 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 08:59:01 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Wu", "Botong", ""], ["Ren", "Sijie", ""], ["Li", "Jing", ""], ["Sun", "Xinwei", ""], ["Li", "Shiming", ""], ["Wang", "Yizhou", ""]]}, {"id": "2012.11113", "submitter": "Yang Yifei", "authors": "Yifei Yang, Shibing Xiang, Ruixiang Zhang", "title": "Improving unsupervised anomaly localization by applying multi-scale\n  memories to autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoder and its variants have been widely applicated in anomaly\ndetection.The previous work memory-augmented deep autoencoder proposed\nmemorizing normality to detect anomaly, however it neglects the feature\ndiscrepancy between different resolution scales, therefore we introduce\nmulti-scale memories to record scale-specific features and multi-scale\nattention fuser between the encoding and decoding module of the autoencoder for\nanomaly detection, namely MMAE.MMAE updates slots at corresponding resolution\nscale as prototype features during unsupervised learning. For anomaly\ndetection, we accomplish anomaly removal by replacing the original encoded\nimage features at each scale with most relevant prototype features,and fuse\nthese features before feeding to the decoding module to reconstruct image.\nExperimental results on various datasets testify that our MMAE successfully\nremoves anomalies at different scales and performs favorably on several\ndatasets compared to similar reconstruction-based methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 04:44:40 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Yang", "Yifei", ""], ["Xiang", "Shibing", ""], ["Zhang", "Ruixiang", ""]]}, {"id": "2012.11116", "submitter": "Fangneng Zhan", "authors": "Fangneng Zhan, Changgong Zhang, Yingchen Yu, Yuan Chang, Shijian Lu,\n  Feiying Ma, Xuansong Xie", "title": "EMLight: Lighting Estimation via Spherical Distribution Approximation", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Illumination estimation from a single image is critical in 3D rendering and\nit has been investigated extensively in the computer vision and computer\ngraphic research community. On the other hand, existing works estimate\nillumination by either regressing light parameters or generating illumination\nmaps that are often hard to optimize or tend to produce inaccurate predictions.\nWe propose Earth Mover Light (EMLight), an illumination estimation framework\nthat leverages a regression network and a neural projector for accurate\nillumination estimation. We decompose the illumination map into spherical light\ndistribution, light intensity and the ambient term, and define the illumination\nestimation as a parameter regression task for the three illumination\ncomponents. Motivated by the Earth Mover distance, we design a novel spherical\nmover's loss that guides to regress light distribution parameters accurately by\ntaking advantage of the subtleties of spherical distribution. Under the\nguidance of the predicted spherical distribution, light intensity and ambient\nterm, the neural projector synthesizes panoramic illumination maps with\nrealistic light frequency. Extensive experiments show that EMLight achieves\naccurate illumination estimation and the generated relighting in 3D object\nembedding exhibits superior plausibility and fidelity as compared with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 04:54:08 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhan", "Fangneng", ""], ["Zhang", "Changgong", ""], ["Yu", "Yingchen", ""], ["Chang", "Yuan", ""], ["Lu", "Shijian", ""], ["Ma", "Feiying", ""], ["Xie", "Xuansong", ""]]}, {"id": "2012.11134", "submitter": "Su Feng", "authors": "Chao Yang, Su Feng, Dongsheng Li, Huawei Shen, Guoqing Wang and Bin\n  Jiang", "title": "Learning content and context with language bias for Visual Question\n  Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual Question Answering (VQA) is a challenging multimodal task to answer\nquestions about an image. Many works concentrate on how to reduce language bias\nwhich makes models answer questions ignoring visual content and language\ncontext. However, reducing language bias also weakens the ability of VQA models\nto learn context prior. To address this issue, we propose a novel learning\nstrategy named CCB, which forces VQA models to answer questions relying on\nContent and Context with language Bias. Specifically, CCB establishes Content\nand Context branches on top of a base VQA model and forces them to focus on\nlocal key content and global effective context respectively. Moreover, a joint\nloss function is proposed to reduce the importance of biased samples and retain\ntheir beneficial influence on answering questions. Experiments show that CCB\noutperforms the state-of-the-art methods in terms of accuracy on VQA-CP v2.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 06:22:50 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Yang", "Chao", ""], ["Feng", "Su", ""], ["Li", "Dongsheng", ""], ["Shen", "Huawei", ""], ["Wang", "Guoqing", ""], ["Jiang", "Bin", ""]]}, {"id": "2012.11140", "submitter": "Alessandro Achille", "authors": "Alessandro Achille, Aditya Golatkar, Avinash Ravichandran, Marzia\n  Polito, Stefano Soatto", "title": "LQF: Linear Quadratic Fine-Tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers that are linear in their parameters, and trained by optimizing a\nconvex loss function, have predictable behavior with respect to changes in the\ntraining data, initial conditions, and optimization. Such desirable properties\nare absent in deep neural networks (DNNs), typically trained by non-linear\nfine-tuning of a pre-trained model. Previous attempts to linearize DNNs have\nled to interesting theoretical insights, but have not impacted the practice due\nto the substantial performance gap compared to standard non-linear\noptimization. We present the first method for linearizing a pre-trained model\nthat achieves comparable performance to non-linear fine-tuning on most of\nreal-world image classification tasks tested, thus enjoying the\ninterpretability of linear models without incurring punishing losses in\nperformance. LQF consists of simple modifications to the architecture, loss\nfunction and optimization typically used for classification: Leaky-ReLU instead\nof ReLU, mean squared loss instead of cross-entropy, and pre-conditioning using\nKronecker factorization. None of these changes in isolation is sufficient to\napproach the performance of non-linear fine-tuning. When used in combination,\nthey allow us to reach comparable performance, and even superior in the\nlow-data regime, while enjoying the simplicity, robustness and interpretability\nof linear-quadratic optimization.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 06:40:20 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Achille", "Alessandro", ""], ["Golatkar", "Aditya", ""], ["Ravichandran", "Avinash", ""], ["Polito", "Marzia", ""], ["Soatto", "Stefano", ""]]}, {"id": "2012.11150", "submitter": "Sungwon Han", "authors": "Sungwon Park, Sungwon Han, Sundong Kim, Danu Kim, Sungkyu Park,\n  Seunghoon Hong and Meeyoung Cha", "title": "Improving Unsupervised Image Clustering With Robust Learning", "comments": "Accepted at CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image clustering methods often introduce alternative objectives\nto indirectly train the model and are subject to faulty predictions and\noverconfident results. To overcome these challenges, the current research\nproposes an innovative model RUC that is inspired by robust learning. RUC's\nnovelty is at utilizing pseudo-labels of existing image clustering models as a\nnoisy dataset that may include misclassified samples. Its retraining process\ncan revise misaligned knowledge and alleviate the overconfidence problem in\npredictions. The model's flexible structure makes it possible to be used as an\nadd-on module to other clustering methods and helps them achieve better\nperformance on multiple datasets. Extensive experiments show that the proposed\nmodel can adjust the model confidence with better calibration and gain\nadditional robustness against adversarial noise.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 07:02:11 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 15:36:14 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Park", "Sungwon", ""], ["Han", "Sungwon", ""], ["Kim", "Sundong", ""], ["Kim", "Danu", ""], ["Park", "Sungkyu", ""], ["Hong", "Seunghoon", ""], ["Cha", "Meeyoung", ""]]}, {"id": "2012.11151", "submitter": "Keisuke Uemura", "authors": "Keisuke Uemura (1 and 2), Yoshito Otake (1), Masaki Takao (3), Mazen\n  Soufi (1), Akihiro Kawasaki (1), Nobuhiko Sugano (2), Yoshinobu Sato (1) ((1)\n  Division of Information Science, Graduate School of Science and Technology,\n  Nara Institute of Science and Technology, Ikoma city, Japan, (2) Department\n  of Orthopaedic Medical Engineering, Osaka University Graduate School of\n  Medicine, Suita city, Japan, (3) Department of Orthopaedics, Osaka University\n  Graduate School of Medicine, Suita city, Japan)", "title": "Automated segmentation of an intensity calibration phantom in clinical\n  CT images using a convolutional neural network", "comments": "29 pages, 7 figures. The source code and the model used for\n  segmenting the phantom are open and can be accessed via\n  https://github.com/keisuke-uemura/CT-Intensity-Calibration-Phantom-Segmentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Purpose: To apply a convolutional neural network (CNN) to develop a system\nthat segments intensity calibration phantom regions in computed tomography (CT)\nimages, and to test the system in a large cohort to evaluate its robustness.\nMethods: A total of 1040 cases (520 cases each from two institutions), in which\nan intensity calibration phantom (B-MAS200, Kyoto Kagaku, Kyoto, Japan) was\nused, were included herein. A training dataset was created by manually\nsegmenting the regions of the phantom for 40 cases (20 cases each).\nSegmentation accuracy of the CNN model was assessed with the Dice coefficient\nand the average symmetric surface distance (ASD) through the 4-fold cross\nvalidation. Further, absolute differences of radiodensity values (in Hounsfield\nunits: HU) were compared between manually segmented regions and automatically\nsegmented regions. The system was tested on the remaining 1000 cases. For each\ninstitution, linear regression was applied to calculate coefficients for the\ncorrelation between radiodensity and the densities of the phantom. Results:\nAfter training, the median Dice coefficient was 0.977, and the median ASD was\n0.116 mm. When segmented regions were compared between manual segmentation and\nautomated segmentation, the median absolute difference was 0.114 HU. For the\ntest cases, the median correlation coefficient was 0.9998 for one institution\nand was 0.9999 for the other, with a minimum value of 0.9863. Conclusions: The\nCNN model successfully segmented the calibration phantom's regions in the CT\nimages with excellent accuracy, and the automated method was found to be at\nleast equivalent to the conventional manual method. Future study should\nintegrate the system by automatically segmenting the region of interest in\nbones such that the bone mineral density can be fully automatically quantified\nfrom CT images.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 07:02:33 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Uemura", "Keisuke", "", "1 and 2"], ["Otake", "Yoshito", ""], ["Takao", "Masaki", ""], ["Soufi", "Mazen", ""], ["Kawasaki", "Akihiro", ""], ["Sugano", "Nobuhiko", ""], ["Sato", "Yoshinobu", ""]]}, {"id": "2012.11184", "submitter": "HaiChao Zhang", "authors": "Haichao Zhang, Kuangrong Hao, Lei Gao, Bing Wei, Xuesong Tang", "title": "Optimizing Deep Neural Networks through Neuroevolution with Stochastic\n  Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved remarkable success in computer\nvision; however, training DNNs for satisfactory performance remains challenging\nand suffers from sensitivity to empirical selections of an optimization\nalgorithm for training. Stochastic gradient descent (SGD) is dominant in\ntraining a DNN by adjusting neural network weights to minimize the DNNs loss\nfunction. As an alternative approach, neuroevolution is more in line with an\nevolutionary process and provides some key capabilities that are often\nunavailable in SGD, such as the heuristic black-box search strategy based on\nindividual collaboration in neuroevolution. This paper proposes a novel\napproach that combines the merits of both neuroevolution and SGD, enabling\nevolutionary search, parallel exploration, and an effective probe for optimal\nDNNs. A hierarchical cluster-based suppression algorithm is also developed to\novercome similar weight updates among individuals for improving population\ndiversity. We implement the proposed approach in four representative DNNs based\non four publicly-available datasets. Experiment results demonstrate that the\nfour DNNs optimized by the proposed approach all outperform corresponding ones\noptimized by only SGD on all datasets. The performance of DNNs optimized by the\nproposed approach also outperforms state-of-the-art deep networks. This work\nalso presents a meaningful attempt for pursuing artificial general\nintelligence.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 08:54:14 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhang", "Haichao", ""], ["Hao", "Kuangrong", ""], ["Gao", "Lei", ""], ["Wei", "Bing", ""], ["Tang", "Xuesong", ""]]}, {"id": "2012.11185", "submitter": "Shenqqi Geng", "authors": "Shengqi Geng", "title": "Infrared image pedestrian target detection based on Yolov3 and migration\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the gradual application of infrared night vision vehicle assistance\nsystem in automatic driving, the accuracy of the collected infrared images of\npedestrians is gradually improved. In this paper, the migration learning method\nis used to apply YOLOv3 model to realize pedestrian target detection in\ninfrared images. The target detection model YOLOv3 is migrated to the CVC\ninfrared pedestrian data set, and Diou loss is used to replace the loss\nfunction of the original YOLO model to test different super parameters to\nobtain the best migration learning effect. The experimental results show that\nin the pedestrian detection task of CVC data set, the average accuracy (AP) of\nYolov3 model reaches 96.35%, and that of Diou-Yolov3 model is 72.14%, but the\nlatter has a faster convergence rate of loss curve. The effect of migration\nlearning can be obtained by comparing the two models.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 08:55:48 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Geng", "Shengqi", ""]]}, {"id": "2012.11187", "submitter": "Xinyu Zhang", "authors": "Xinyu Zhang, Xinlong Wang, Jia-Wang Bian, Chunhua Shen, Mingyu You", "title": "Diverse Knowledge Distillation for End-to-End Person Search", "comments": "Accepted to AAAI, 2021. Code is available at:\n  https://git.io/DKD-PersonSearch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Person search aims to localize and identify a specific person from a gallery\nof images. Recent methods can be categorized into two groups, i.e., two-step\nand end-to-end approaches. The former views person search as two independent\ntasks and achieves dominant results using separately trained person detection\nand re-identification (Re-ID) models. The latter performs person search in an\nend-to-end fashion. Although the end-to-end approaches yield higher inference\nefficiency, they largely lag behind those two-step counterparts in terms of\naccuracy. In this paper, we argue that the gap between the two kinds of methods\nis mainly caused by the Re-ID sub-networks of end-to-end methods. To this end,\nwe propose a simple yet strong end-to-end network with diverse knowledge\ndistillation to break the bottleneck. We also design a spatial-invariant\naugmentation to assist model to be invariant to inaccurate detection results.\nExperimental results on the CUHK-SYSU and PRW datasets demonstrate the\nsuperiority of our method against existing approaches -- it achieves on par\naccuracy with state-of-the-art two-step methods while maintaining high\nefficiency due to the single joint model. Code is available at:\nhttps://git.io/DKD-PersonSearch.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 09:04:27 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhang", "Xinyu", ""], ["Wang", "Xinlong", ""], ["Bian", "Jia-Wang", ""], ["Shen", "Chunhua", ""], ["You", "Mingyu", ""]]}, {"id": "2012.11193", "submitter": "Xuanhong Chen", "authors": "Xuanhong Chen, Ziang Liu, Ting Qiu, Bingbing Ni, Naiyuan Liu, Xiwei\n  Hu, Yuhan Li", "title": "Image Translation via Fine-grained Knowledge Transfer", "comments": "Submitted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Prevailing image-translation frameworks mostly seek to process images via the\nend-to-end style, which has achieved convincing results. Nonetheless, these\nmethods lack interpretability and are not scalable on different\nimage-translation tasks (e.g., style transfer, HDR, etc.). In this paper, we\npropose an interpretable knowledge-based image-translation framework, which\nrealizes the image-translation through knowledge retrieval and transfer. In\ndetails, the framework constructs a plug-and-play and model-agnostic general\npurpose knowledge library, remembering task-specific styles, tones, texture\npatterns, etc. Furthermore, we present a fast ANN searching approach, Bandpass\nHierarchical K-Means (BHKM), to cope with the difficulty of searching in the\nenormous knowledge library. Extensive experiments well demonstrate the\neffectiveness and feasibility of our framework in different image-translation\ntasks. In particular, backtracking experiments verify the interpretability of\nour method. Our code soon will be available at\nhttps://github.com/AceSix/Knowledge_Transfer.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 09:18:48 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Chen", "Xuanhong", ""], ["Liu", "Ziang", ""], ["Qiu", "Ting", ""], ["Ni", "Bingbing", ""], ["Liu", "Naiyuan", ""], ["Hu", "Xiwei", ""], ["Li", "Yuhan", ""]]}, {"id": "2012.11207", "submitter": "Zhengyu Zhao", "authors": "Zhengyu Zhao, Zhuoran Liu, Martha Larson", "title": "On Success and Simplicity: A Second Look at Transferable Targeted\n  Attacks", "comments": "Code available at https://github.com/ZhengyuZhao/Targeted-Tansfer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Achieving transferability of targeted attacks is reputed to be remarkably\ndifficult. Currently, state-of-the-art approaches are resource-intensive\nbecause they necessitate training model(s) for each target class with\nadditional data. In our investigation, we find, however, that simple\ntransferable attacks which require neither additional data nor model training\ncan achieve surprisingly high targeted transferability. This insight has been\noverlooked until now, mainly due to the widespread practice of unreasonably\nrestricting attack optimization to a limited number of iterations. In\nparticular, we, for the first time, identify that a simple logit loss can yield\ncompetitive results with the state of the arts. Our analysis spans a variety of\ntransfer settings, especially including three new, realistic settings: an\nensemble transfer setting with little model similarity, a worse-case setting\nwith low-ranked target classes, and also a real-world attack against the Google\nCloud Vision API. Results in these new settings demonstrate that the commonly\nadopted, easy settings cannot fully reveal the actual properties of different\nattacks and may cause misleading comparisons. We also show the usefulness of\nthe simple logit loss for generating targeted universal adversarial\nperturbations in a data-free and training-free manner. Overall, the aim of our\nanalysis is to inspire a more meaningful evaluation on targeted\ntransferability.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 09:41:29 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 15:18:35 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 20:50:00 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhao", "Zhengyu", ""], ["Liu", "Zhuoran", ""], ["Larson", "Martha", ""]]}, {"id": "2012.11211", "submitter": "Yi Ding", "authors": "Yi Ding, Wei Zheng, Guozheng Wu, Ji Geng, Mingsheng Cao, Zhiguang Qin", "title": "A Multi-View Dynamic Fusion Framework: How to Improve the Multimodal\n  Brain Tumor Segmentation from Multi-Views?", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When diagnosing the brain tumor, doctors usually make a diagnosis by\nobserving multimodal brain images from the axial view, the coronal view and the\nsagittal view, respectively. And then they make a comprehensive decision to\nconfirm the brain tumor based on the information obtained from multi-views.\nInspired by this diagnosing process and in order to further utilize the 3D\ninformation hidden in the dataset, this paper proposes a multi-view dynamic\nfusion framework to improve the performance of brain tumor segmentation. The\nproposed framework consists of 1) a multi-view deep neural network\narchitecture, which represents multi learning networks for segmenting the brain\ntumor from different views and each deep neural network corresponds to\nmulti-modal brain images from one single view and 2) the dynamic decision\nfusion method, which is mainly used to fuse segmentation results from\nmulti-views as an integrate one and two different fusion methods, the voting\nmethod and the weighted averaging method, have been adopted to evaluate the\nfusing process. Moreover, the multi-view fusion loss, which consists of the\nsegmentation loss, the transition loss and the decision loss, is proposed to\nfacilitate the training process of multi-view learning networks so as to keep\nthe consistency of appearance and space, not only in the process of fusing\nsegmentation results, but also in the process of training the learning network.\n\\par By evaluating the proposed framework on BRATS 2015 and BRATS 2018, it can\nbe found that the fusion results from multi-views achieve a better performance\nthan the segmentation result from the single view and the effectiveness of\nproposed multi-view fusion loss has also been proved. Moreover, the proposed\nframework achieves a better segmentation performance and a higher efficiency\ncompared to other counterpart methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 09:45:23 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Ding", "Yi", ""], ["Zheng", "Wei", ""], ["Wu", "Guozheng", ""], ["Geng", "Ji", ""], ["Cao", "Mingsheng", ""], ["Qin", "Zhiguang", ""]]}, {"id": "2012.11212", "submitter": "Siyuan Cheng", "authors": "Siyuan Cheng, Yingqi Liu, Shiqing Ma, Xiangyu Zhang", "title": "Deep Feature Space Trojan Attack of Neural Networks by Controlled\n  Detoxification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trojan (backdoor) attack is a form of adversarial attack on deep neural\nnetworks where the attacker provides victims with a model trained/retrained on\nmalicious data. The backdoor can be activated when a normal input is stamped\nwith a certain pattern called trigger, causing misclassification. Many existing\ntrojan attacks have their triggers being input space patches/objects (e.g., a\npolygon with solid color) or simple input transformations such as Instagram\nfilters. These simple triggers are susceptible to recent backdoor detection\nalgorithms. We propose a novel deep feature space trojan attack with five\ncharacteristics: effectiveness, stealthiness, controllability, robustness and\nreliance on deep features. We conduct extensive experiments on 9 image\nclassifiers on various datasets including ImageNet to demonstrate these\nproperties and show that our attack can evade state-of-the-art defense.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 09:46:12 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 04:10:38 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Cheng", "Siyuan", ""], ["Liu", "Yingqi", ""], ["Ma", "Shiqing", ""], ["Zhang", "Xiangyu", ""]]}, {"id": "2012.11214", "submitter": "Tahira Iqbal", "authors": "Tahira Iqbal, Arslan Shaukat, Usman Akram and Zartasha Mustansar", "title": "Automatic Diagnosis of Pneumothorax from Chest Radiographs: A Systematic\n  Literature Review", "comments": "30 pages, 5 figures, 4 Tables to be published in journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Among various medical imaging tools, chest radiographs are the most important\nand widely used diagnostic tool for detection of thoracic pathologies. Research\nis being carried out in order to propose robust automatic diagnostic tool for\ndetection of pathologies from chest radiographs. Artificial Intelligence\ntechniques especially deep learning methodologies have found to be giving\npromising results in automating the field of medicine. Lot of research has been\ndone for automatic and fast detection of pneumothorax from chest radiographs\nwhile proposing several frameworks based on artificial intelligence and machine\nlearning techniques. This study summarizes the existing literature for the\nautomatic detection of pneumothorax from chest x-rays along with describing the\navailable chest radiographs datasets. The comparative analysis of the\nliterature is also provided in terms of goodness. Limitations of the existing\nliterature along with the research gaps is also given for further\ninvestigation. The paper provides a brief overview of the present work for\npneumothorax detection for helping the researchers in selection of optimal\napproach for future research.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 09:49:56 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 16:03:01 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Iqbal", "Tahira", ""], ["Shaukat", "Arslan", ""], ["Akram", "Usman", ""], ["Mustansar", "Zartasha", ""]]}, {"id": "2012.11225", "submitter": "Heewon Kim", "authors": "Heewon Kim, Sungyong Baik, Myungsub Choi, Janghoon Choi, Kyoung Mu Lee", "title": "Searching for Controllable Image Restoration Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diverse user preferences over images have recently led to a great amount of\ninterest in controlling the imagery effects for image restoration tasks.\nHowever, existing methods require separate inference through the entire network\nper each output, which hinders users from readily comparing multiple imagery\neffects due to long latency. To this end, we propose a novel framework based on\na neural architecture search technique that enables efficient generation of\nmultiple imagery effects via two stages of pruning: task-agnostic and\ntask-specific pruning. Specifically, task-specific pruning learns to adaptively\nremove the irrelevant network parameters for each task, while task-agnostic\npruning learns to find an efficient architecture by sharing the early layers of\nthe network across different tasks. Since the shared layers allow for feature\nreuse, only a single inference of the task-agnostic layers is needed to\ngenerate multiple imagery effects from the input image. Using the proposed\ntask-agnostic and task-specific pruning schemes together significantly reduces\nthe FLOPs and the actual latency of inference compared to the baseline. We\nreduce 95.7% of the FLOPs when generating 27 imagery effects, and make the GPU\nlatency 73.0% faster on 4K-resolution images.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 10:08:18 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Kim", "Heewon", ""], ["Baik", "Sungyong", ""], ["Choi", "Myungsub", ""], ["Choi", "Janghoon", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2012.11230", "submitter": "Cheeun Hong", "authors": "Cheeun Hong, Heewon Kim, Junghun Oh, Kyoung Mu Lee", "title": "DAQ: Distribution-Aware Quantization for Deep Image Super-Resolution\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantizing deep convolutional neural networks for image super-resolution\nsubstantially reduces their computational costs. However, existing works either\nsuffer from a severe performance drop in ultra-low precision of 4 or lower\nbit-widths, or require a heavy fine-tuning process to recover the performance.\nTo our knowledge, this vulnerability to low precisions relies on two\nstatistical observations of feature map values. First, distribution of feature\nmap values varies significantly per channel and per input image. Second,\nfeature maps have outliers that can dominate the quantization error. Based on\nthese observations, we propose a novel distribution-aware quantization scheme\n(DAQ) which facilitates accurate training-free quantization in ultra-low\nprecision. A simple function of DAQ determines dynamic range of feature maps\nand weights with low computational burden. Furthermore, our method enables\nmixed-precision quantization by calculating the relative sensitivity of each\nchannel, without any training process involved. Nonetheless, quantization-aware\ntraining is also applicable for auxiliary performance gain. Our new method\noutperforms recent training-free and even training-based quantization methods\nto the state-of-the-art image super-resolution networks in ultra-low precision.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 10:19:42 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Hong", "Cheeun", ""], ["Kim", "Heewon", ""], ["Oh", "Junghun", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2012.11253", "submitter": "Hichem Sahbi", "authors": "Mingyuan Jiu and Hichem Sahbi", "title": "Image Annotation based on Deep Hierarchical Context Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context modeling is one of the most fertile subfields of visual recognition\nwhich aims at designing discriminant image representations while incorporating\ntheir intrinsic and extrinsic relationships. However, the potential of context\nmodeling is currently underexplored and most of the existing solutions are\neither context-free or restricted to simple handcrafted geometric\nrelationships. We introduce in this paper DHCN: a novel Deep Hierarchical\nContext Network that leverages different sources of contexts including\ngeometric and semantic relationships. The proposed method is based on the\nminimization of an objective function mixing a fidelity term, a context\ncriterion and a regularizer. The solution of this objective function defines\nthe architecture of a bi-level hierarchical context network; the first level of\nthis network captures scene geometry while the second one corresponds to\nsemantic relationships. We solve this representation learning problem by\ntraining its underlying deep network whose parameters correspond to the most\ninfluencing bi-level contextual relationships and we evaluate its performances\non image annotation using the challenging ImageCLEF benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 11:07:42 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Jiu", "Mingyuan", ""], ["Sahbi", "Hichem", ""]]}, {"id": "2012.11260", "submitter": "Mengshi Qi", "authors": "Mengshi Qi, Edoardo Remelli, Mathieu Salzmann, Pascal Fua", "title": "Unsupervised Domain Adaptation with Temporal-Consistent Self-Training\n  for 3D Hand-Object Joint Reconstruction", "comments": "In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning-solutions for hand-object 3D pose and shape estimation are now\nvery effective when an annotated dataset is available to train them to handle\nthe scenarios and lighting conditions they will encounter at test time.\nUnfortunately, this is not always the case, and one often has to resort to\ntraining them on synthetic data, which does not guarantee that they will work\nwell in real situations. In this paper, we introduce an effective approach to\naddressing this challenge by exploiting 3D geometric constraints within a cycle\ngenerative adversarial network (CycleGAN) to perform domain adaptation.\nFurthermore, in contrast to most existing works, which fail to leverage the\nrich temporal information available in unlabeled real videos as a source of\nsupervision, we propose to enforce short- and long-term temporal consistency to\nfine-tune the domain-adapted model in a self-supervised fashion. We will\ndemonstrate that our approach outperforms state-of-the-art 3D hand-object joint\nreconstruction methods on three widely-used benchmarks and will make our code\npublicly available.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 11:27:56 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Qi", "Mengshi", ""], ["Remelli", "Edoardo", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "2012.11294", "submitter": "Zhiang Liu", "authors": "Jiang-Jiang Liu, Zhi-Ang Liu, Ming-Ming Cheng", "title": "Centralized Information Interaction for Salient Object Detection", "comments": "V2 updates the evaluation results of all methods on the ECSSD dataset\n  (Table. 3 on Page. 8). In V1 we used the old version of ground-truths of\n  ECSSD, which were updated later by its authors. In V2 we use the updated ones\n  instead. Although the numerical evaluation scores of all methods on ECSSD in\n  V1 and V2 vary slightly, the overall trending is still the same", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The U-shape structure has shown its advantage in salient object detection for\nefficiently combining multi-scale features. However, most existing U-shape\nbased methods focused on improving the bottom-up and top-down pathways while\nignoring the connections between them. This paper shows that by centralizing\nthese connections, we can achieve the cross-scale information interaction among\nthem, hence obtaining semantically stronger and positionally more precise\nfeatures. To inspire the potential of the newly proposed strategy, we further\ndesign a relative global calibration module that can simultaneously process\nmulti-scale inputs without spatial interpolation. Benefiting from the above\nstrategy and module, our proposed approach can aggregate features more\neffectively while introducing only a few additional parameters. Our approach\ncan cooperate with various existing U-shape-based salient object detection\nmethods by substituting the connections between the bottom-up and top-down\npathways. Experimental results demonstrate that our proposed approach performs\nfavorably against the previous state-of-the-arts on five widely used benchmarks\nwith less computational complexity. The source code will be publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 12:42:06 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 14:47:36 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Liu", "Jiang-Jiang", ""], ["Liu", "Zhi-Ang", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "2012.11295", "submitter": "Carmelo Sferrazza", "authors": "Carmelo Sferrazza and Raffaello D'Andrea", "title": "Sim-to-real for high-resolution optical tactile sensing: From images to\n  3D contact force distributions", "comments": "Accompanying video: https://youtu.be/SyyxzROVVyY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The images captured by vision-based tactile sensors carry information about\nhigh-resolution tactile fields, such as the distribution of the contact forces\napplied to their soft sensing surface. However, extracting the information\nencoded in the images is challenging and often addressed with learning-based\napproaches, which generally require a large amount of training data. This\narticle proposes a strategy to generate tactile images in simulation for a\nvision-based tactile sensor based on an internal camera that tracks the motion\nof spherical particles within a soft material. The deformation of the material\nis simulated in a finite element environment under a diverse set of contact\nconditions, and spherical particles are projected to a simulated image.\nFeatures extracted from the images are mapped to the 3D contact force\ndistribution, with the ground truth also obtained via finite-element\nsimulations, with an artificial neural network that is therefore entirely\ntrained on synthetic data avoiding the need for real-world data collection. The\nresulting model exhibits high accuracy when evaluated on real-world tactile\nimages, is transferable across multiple tactile sensors without further\ntraining, and is suitable for efficient real-time inference.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 12:43:33 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 11:45:11 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Sferrazza", "Carmelo", ""], ["D'Andrea", "Raffaello", ""]]}, {"id": "2012.11301", "submitter": "Linn \\\"Ostr\\\"om", "authors": "Patrik Persson, Linn \\\"Ostr\\\"om, Carl Olsson", "title": "Monocular Depth Parameterizing Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth estimation is a highly challenging problem that is often\naddressed with deep neural networks. While these are able to use recognition of\nimage features to predict reasonably looking depth maps the result often has\nlow metric accuracy. In contrast traditional stereo methods using multiple\ncameras provide highly accurate estimation when pixel matching is possible. In\nthis work we propose to combine the two approaches leveraging their respective\nstrengths. For this purpose we propose a network structure that given an image\nprovides a parameterization of a set of depth maps with feasible shapes.\nOptimizing over the parameterization then allows us to search the shapes for a\nphoto consistent solution with respect to other images. This allows us to\nenforce geometric properties that are difficult to observe in single image as\nwell as relaxes the learning problem allowing us to use relatively small\nnetworks. Our experimental evaluation shows that our method generates more\naccurate depth maps and generalizes better than competing state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 13:02:41 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Persson", "Patrik", ""], ["\u00d6str\u00f6m", "Linn", ""], ["Olsson", "Carl", ""]]}, {"id": "2012.11307", "submitter": "Ling Lo", "authors": "Hong-Xia Xie, Ling Lo, Hong-Han Shuai and Wen-Huang Cheng", "title": "An Overview of Facial Micro-Expression Analysis: Data, Methodology and\n  Challenge", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial micro-expressions indicate brief and subtle facial movements that\nappear during emotional communication. In comparison to macro-expressions,\nmicro-expressions are more challenging to be analyzed due to the short span of\ntime and the fine-grained changes. In recent years, micro-expression\nrecognition (MER) has drawn much attention because it can benefit a wide range\nof applications, e.g. police interrogation, clinical diagnosis, depression\nanalysis, and business negotiation. In this survey, we offer a fresh overview\nto discuss new research directions and challenges these days for MER tasks. For\nexample, we review MER approaches from three novel aspects: macro-to-micro\nadaptation, recognition based on key apex frames, and recognition based on\nfacial action units. Moreover, to mitigate the problem of limited and biased ME\ndata, synthetic data generation is surveyed for the diversity enrichment of\nmicro-expression data. Since micro-expression spotting can boost\nmicro-expression analysis, the state-of-the-art spotting works are also\nintroduced in this paper. At last, we discuss the challenges in MER research\nand provide potential solutions as well as possible directions for further\ninvestigation.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 13:20:17 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Xie", "Hong-Xia", ""], ["Lo", "Ling", ""], ["Shuai", "Hong-Han", ""], ["Cheng", "Wen-Huang", ""]]}, {"id": "2012.11310", "submitter": "Hugo Bertiche", "authors": "Hugo Bertiche, Meysam Madadi and Sergio Escalera", "title": "PBNS: Physically Based Neural Simulator for Unsupervised Garment Pose\n  Space Deformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a methodology to automatically obtain Pose Space Deformation (PSD)\nbasis for rigged garments through deep learning. Classical approaches rely on\nPhysically Based Simulations (PBS) to animate clothes. These are general\nsolutions that, given a sufficiently fine-grained discretization of space and\ntime, can achieve highly realistic results. However, they are computationally\nexpensive and any scene modification prompts the need of re-simulation. Linear\nBlend Skinning (LBS) with PSD offers a lightweight alternative to PBS, though,\nit needs huge volumes of data to learn proper PSD. We propose using deep\nlearning, formulated as an implicit PBS, to unsupervisedly learn realistic\ncloth Pose Space Deformations in a constrained scenario: dressed humans.\nFurthermore, we show it is possible to train these models in an amount of time\ncomparable to a PBS of a few sequences. To the best of our knowledge, we are\nthe first to propose a neural simulator for cloth. While deep-based approaches\nin the domain are becoming a trend, these are data-hungry models. Moreover,\nauthors often propose complex formulations to better learn wrinkles from PBS\ndata. Supervised learning leads to physically inconsistent predictions that\nrequire collision solving to be used. Also, dependency on PBS data limits the\nscalability of these solutions, while their formulation hinders its\napplicability and compatibility. By proposing an unsupervised methodology to\nlearn PSD for LBS models (3D animation standard), we overcome both of these\ndrawbacks. Results obtained show cloth-consistency in the animated garments and\nmeaningful pose-dependant folds and wrinkles. Our solution is extremely\nefficient, handles multiple layers of cloth, allows unsupervised outfit\nresizing and can be easily applied to any custom 3D avatar.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 13:22:10 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 10:35:47 GMT"}, {"version": "v3", "created": "Fri, 21 May 2021 18:41:35 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Bertiche", "Hugo", ""], ["Madadi", "Meysam", ""], ["Escalera", "Sergio", ""]]}, {"id": "2012.11368", "submitter": "Kaiqi Chen", "authors": "Kaiqi Chen, Jialing Liu, Jianhua Zhang, Zhenhua Wang", "title": "Accurate Object Association and Pose Updating for Semantic SLAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays in the field of semantic SLAM, how to correctly use semantic\ninformation for data association is still a problem worthy of study. The key to\nsolving this problem is to correctly associate multiple object measurements of\none object landmark, and refine the pose of object landmark. However, different\nobjects locating closely are prone to be associated as one object landmark, and\nit is difficult to pick up a best pose from multiple object measurements\nassociated with one object landmark. To tackle these problems, we propose a\nhierarchical object association strategy by means of multiple object tracking,\nthrough which closing objects will be correctly associated to different object\nlandmarks, and an approach to refine the pose of object landmark from multiple\nobject measurements. The proposed method is evaluated on a simulated sequence\nand several sequences in the Kitti dataset. Experimental results show a very\nimpressive improvement with respect to the traditional SLAM and the\nstate-of-the-art semantic SLAM method.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 14:21:09 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Chen", "Kaiqi", ""], ["Liu", "Jialing", ""], ["Zhang", "Jianhua", ""], ["Wang", "Zhenhua", ""]]}, {"id": "2012.11389", "submitter": "Siqing Zhang", "authors": "Siqing Zhang, Ruoyi Du, Dongliang Chang, Zhanyu Ma, Jun Guo", "title": "Knowledge Transfer Based Fine-grained Visual Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fine-grained visual classification (FGVC) aims to distinguish the sub-classes\nof the same category and its essential solution is to mine the subtle and\ndiscriminative regions. Convolution neural networks (CNNs), which employ the\ncross entropy loss (CE-loss) as the loss function, show poor performance since\nthe model can only learn the most discriminative part and ignore other\nmeaningful regions. Some existing works try to solve this problem by mining\nmore discriminative regions by some detection techniques or attention\nmechanisms. However, most of them will meet the background noise problem when\ntrying to find more discriminative regions. In this paper, we address it in a\nknowledge transfer learning manner. Multiple models are trained one by one, and\nall previously trained models are regarded as teacher models to supervise the\ntraining of the current one. Specifically, a orthogonal loss (OR-loss) is\nproposed to encourage the network to find diverse and meaningful regions. In\naddition, the first model is trained with only CE-Loss. Finally, all models'\noutputs with complementary knowledge are combined together for the final\nprediction result. We demonstrate the superiority of the proposed method and\nobtain state-of-the-art (SOTA) performances on three popular FGVC datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 14:41:08 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhang", "Siqing", ""], ["Du", "Ruoyi", ""], ["Chang", "Dongliang", ""], ["Ma", "Zhanyu", ""], ["Guo", "Jun", ""]]}, {"id": "2012.11409", "submitter": "Xuran Pan", "authors": "Xuran Pan, Zhuofan Xia, Shiji Song, Li Erran Li, Gao Huang", "title": "3D Object Detection with Pointformer", "comments": "Accepted by IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2021. Code is available at\n  https://github.com/Vladimir2506/Pointformer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature learning for 3D object detection from point clouds is very\nchallenging due to the irregularity of 3D point cloud data. In this paper, we\npropose Pointformer, a Transformer backbone designed for 3D point clouds to\nlearn features effectively. Specifically, a Local Transformer module is\nemployed to model interactions among points in a local region, which learns\ncontext-dependent region features at an object level. A Global Transformer is\ndesigned to learn context-aware representations at the scene level. To further\ncapture the dependencies among multi-scale representations, we propose\nLocal-Global Transformer to integrate local features with global features from\nhigher resolution. In addition, we introduce an efficient coordinate refinement\nmodule to shift down-sampled points closer to object centroids, which improves\nobject proposal generation. We use Pointformer as the backbone for\nstate-of-the-art object detection models and demonstrate significant\nimprovements over original models on both indoor and outdoor datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 15:12:54 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 08:33:35 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 02:25:36 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Pan", "Xuran", ""], ["Xia", "Zhuofan", ""], ["Song", "Shiji", ""], ["Li", "Li Erran", ""], ["Huang", "Gao", ""]]}, {"id": "2012.11413", "submitter": "Chenchen Zhao", "authors": "Chenchen Zhao and Hao Li", "title": "Exploiting Vulnerability of Pooling in Convolutional Neural Networks by\n  Strict Layer-Output Manipulation for Adversarial Attacks", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have been more and more applied in mobile\nrobotics such as intelligent vehicles. Security of CNNs in robotics\napplications is an important issue, for which potential adversarial attacks on\nCNNs are worth research. Pooling is a typical step of dimension reduction and\ninformation discarding in CNNs. Such information discarding may result in\nmis-deletion and mis-preservation of data features which largely influence the\noutput of the network. This may aggravate the vulnerability of CNNs to\nadversarial attacks. In this paper, we conduct adversarial attacks on CNNs from\nthe perspective of network structure by investigating and exploiting the\nvulnerability of pooling. First, a novel adversarial attack methodology named\nStrict Layer-Output Manipulation (SLOM) is proposed. Then an attack method\nbased on Strict Pooling Manipulation (SPM) which is an instantiation of the\nSLOM spirit is designed to effectively realize both type I and type II\nadversarial attacks on a target CNN. Performances of attacks based on SPM at\ndifferent depths are also investigated and compared. Moreover, performances of\nattack methods designed by instantiating the SLOM spirit with different\noperation layers of CNNs are compared. Experiment results reflect that pooling\ntends to be more vulnerable to adversarial attacks than other operations in\nCNNs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 15:18:41 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhao", "Chenchen", ""], ["Li", "Hao", ""]]}, {"id": "2012.11431", "submitter": "Chenchen Zhao", "authors": "Chenchen Zhao and Hao Li", "title": "Amplifying the Anterior-Posterior Difference via Data Enhancement -- A\n  More Robust Deep Monocular Orientation Estimation Solution", "comments": "7 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep-learning based monocular orientation estimation algorithms\nfaces the problem of confusion between the anterior and posterior parts of the\nobjects, caused by the feature similarity of such parts in typical objects in\ntraffic scenes such as cars and pedestrians. While difficult to solve, the\nproblem may lead to serious orientation estimation errors, and pose threats to\nthe upcoming decision making process of the ego vehicle, since the predicted\ntracks of objects may have directions opposite to ground truths. In this paper,\nwe mitigate this problem by proposing a pretraining method. The method focuses\non predicting the left/right semicircle in which the orientation of the object\nis located. The trained semicircle prediction model is then integrated into the\norientation angle estimation model which predicts a value in range $[0, \\pi]$.\nExperiment results show that the proposed semicircle prediction enhances the\naccuracy of orientation estimation, and mitigates the problem stated above.\nWith the proposed method, a backbone achieves similar state-of-the-art\norientation estimation performance to existing approaches with well-designed\nnetwork structures.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 15:36:13 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhao", "Chenchen", ""], ["Li", "Hao", ""]]}, {"id": "2012.11432", "submitter": "Samuel Ofosu Mensah", "authors": "Samuel Ofosu Mensah, Bubacarr Bah, Willie Brink", "title": "Towards the Localisation of Lesions in Diabetic Retinopathy", "comments": "8 pages, 4 figures, used svproc document class, Computing Conference\n  2021 - London", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) have successfully been used to classify\ndiabetic retinopathy (DR) fundus images in recent times. However, deeper\nrepresentations in CNNs may capture higher-level semantics at the expense of\nspatial resolution. To make predictions usable for ophthalmologists, we use a\npost-attention technique called Gradient-weighted Class Activation Mapping\n(Grad-CAM) on the penultimate layer of deep learning models to produce coarse\nlocalisation maps on DR fundus images. This is to help identify discriminative\nregions in the images, consequently providing evidence for ophthalmologists to\nmake a diagnosis and potentially save lives by early diagnosis. Specifically,\nthis study uses pre-trained weights from four state-of-the-art deep learning\nmodels to produce and compare localisation maps of DR fundus images. The models\nused include VGG16, ResNet50, InceptionV3, and InceptionResNetV2. We find that\nInceptionV3 achieves the best performance with a test classification accuracy\nof 96.07%, and localise lesions better and faster than the other models.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 15:39:17 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 09:29:26 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Mensah", "Samuel Ofosu", ""], ["Bah", "Bubacarr", ""], ["Brink", "Willie", ""]]}, {"id": "2012.11442", "submitter": "Chenchen Zhao", "authors": "Chenchen Zhao and Hao Li", "title": "Blurring Fools the Network -- Adversarial Attacks by Feature Peak\n  Suppression and Gaussian Blurring", "comments": "7 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing pixel-level adversarial attacks on neural networks may be deficient\nin real scenarios, since pixel-level changes on the data cannot be fully\ndelivered to the neural network after camera capture and multiple image\npreprocessing steps. In contrast, in this paper, we argue from another\nperspective that gaussian blurring, a common technique of image preprocessing,\ncan be aggressive itself in specific occasions, thus exposing the network to\nreal-world adversarial attacks. We first propose an adversarial attack demo\nnamed peak suppression (PS) by suppressing the values of peak elements in the\nfeatures of the data. Based on the blurring spirit of PS, we further apply\ngaussian blurring to the data, to investigate the potential influence and\nthreats of gaussian blurring to performance of the network. Experiment results\nshow that PS and well-designed gaussian blurring can form adversarial attacks\nthat completely change classification results of a well-trained target network.\nWith the strong physical significance and wide applications of gaussian\nblurring, the proposed approach will also be capable of conducting real world\nattacks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 15:47:14 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhao", "Chenchen", ""], ["Li", "Hao", ""]]}, {"id": "2012.11460", "submitter": "Viraj Prabhu", "authors": "Viraj Prabhu, Shivam Khare, Deeksha Kartik, Judy Hoffman", "title": "SENTRY: Selective Entropy Optimization via Committee Consistency for\n  Unsupervised Domain Adaptation", "comments": "Code available at https://github.com/virajprabhu/SENTRY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many existing approaches for unsupervised domain adaptation (UDA) focus on\nadapting under only data distribution shift and offer limited success under\nadditional cross-domain label distribution shift. Recent work based on\nself-training using target pseudo-labels has shown promise, but on challenging\nshifts pseudo-labels may be highly unreliable, and using them for self-training\nmay cause error accumulation and domain misalignment. We propose Selective\nEntropy Optimization via Committee Consistency (SENTRY), a UDA algorithm that\njudges the reliability of a target instance based on its predictive consistency\nunder a committee of random image transformations. Our algorithm then\nselectively minimizes predictive entropy to increase confidence on highly\nconsistent target instances, while maximizing predictive entropy to reduce\nconfidence on highly inconsistent ones. In combination with pseudo-label based\napproximate target class balancing, our approach leads to significant\nimprovements over the state-of-the-art on 27/31 domain shifts from standard UDA\nbenchmarks as well as benchmarks designed to stress-test adaptation under label\ndistribution shift.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 16:24:50 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Prabhu", "Viraj", ""], ["Khare", "Shivam", ""], ["Kartik", "Deeksha", ""], ["Hoffman", "Judy", ""]]}, {"id": "2012.11486", "submitter": "Douglas Gomes PhD", "authors": "Douglas Pinto Sampaio Gomes, Lihong Zheng", "title": "Leaf Segmentation and Counting with Deep Learning: on Model Certainty,\n  Test-Time Augmentation, Trade-Offs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Plant phenotyping tasks such as leaf segmentation and counting are\nfundamental to the study of phenotypic traits. Since it is well-suited for\nthese tasks, deep supervised learning has been prevalent in recent works\nproposing better performing models at segmenting and counting leaves. Despite\ngood efforts from research groups, one of the main challenges for proposing\nbetter methods is still the limitation of labelled data availability. The main\nefforts of the field seem to be augmenting existing limited data sets, and some\naspects of the modelling process have been under-discussed. This paper explores\nsuch topics and present experiments that led to the development of the\nbest-performing method in the Leaf Segmentation Challenge and in another\nexternal data set of Komatsuna plants. The model has competitive performance\nwhile been arguably simpler than other recently proposed ones. The experiments\nalso brought insights such as the fact that model cardinality and test-time\naugmentation may have strong applications in object segmentation of single\nclass and high occlusion, and regarding the data distribution of recently\nproposed data sets for benchmarking.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 17:00:05 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Gomes", "Douglas Pinto Sampaio", ""], ["Zheng", "Lihong", ""]]}, {"id": "2012.11489", "submitter": "Helin Dutagaci", "authors": "Kaya Turgut, Helin Dutagaci, Gilles Galopin, David Rousseau", "title": "Segmentation of structural parts of rosebush plants with 3D point-based\n  deep learning methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Segmentation of structural parts of 3D models of plants is an important step\nfor plant phenotyping, especially for monitoring architectural and\nmorphological traits. This work introduces a benchmark for assessing the\nperformance of 3D point-based deep learning methods on organ segmentation of 3D\nplant models, specifically rosebush models. Six recent deep learning\narchitectures that segment 3D point clouds into semantic parts were adapted and\ncompared. The methods were tested on the ROSE-X data set, containing fully\nannotated 3D models of real rosebush plants. The contribution of incorporating\nsynthetic 3D models generated through Lindenmayer systems into training data\nwas also investigated.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 17:01:50 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Turgut", "Kaya", ""], ["Dutagaci", "Helin", ""], ["Galopin", "Gilles", ""], ["Rousseau", "David", ""]]}, {"id": "2012.11525", "submitter": "Xuanqin Mou", "authors": "Congmin Chen, Xuanqin Mou", "title": "A Shift-insensitive Full Reference Image Quality Assessment Model Based\n  on Quadratic Sum of Gradient Magnitude and LOG signals", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image quality assessment that aims at estimating the subject quality of\nimages, builds models to evaluate the perceptual quality of the image in\ndifferent applications. Based on the fact that the human visual system (HVS) is\nhighly sensitive to structural information, the edge information extraction is\nwidely applied in different IQA metrics. According to previous studies, the\nimage gradient magnitude (GM) and the Laplacian of Gaussian (LOG) operator are\ntwo efficient structural features in IQA tasks. However, most of the IQA\nmetrics achieve good performance only when the distorted image is totally\nregistered with the reference image, but fail to perform on images with small\ntranslations. In this paper, we propose an FR-IQA model with the quadratic sum\nof the GM and the LOG signals, which obtains good performance in image quality\nestimation considering shift-insensitive property for not well-registered\nreference and distortion image pairs. Experimental results show that the\nproposed model works robustly on three large scale subjective IQA databases\nwhich contain a variety of distortion types and levels, and stays in the\nstate-of-the-art FR-IQA models no matter for single distortion type or across\nwhole database. Furthermore, we validated that the proposed metric performs\nbetter with shift-insensitive property compared with the CW-SSIM metric that is\nconsidered to be shift-insensitive IQA so far. Meanwhile, the proposed model is\nmuch simple than the CW-SSIM, which is efficient for applications.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 17:41:07 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Chen", "Congmin", ""], ["Mou", "Xuanqin", ""]]}, {"id": "2012.11528", "submitter": "Xi Zhu", "authors": "Xi Zhu, Zhendong Mao, Chunxiao Liu, Peng Zhang, Bin Wang, and Yongdong\n  Zhang", "title": "Overcoming Language Priors with Self-supervised Learning for Visual\n  Question Answering", "comments": "Accepted by IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most Visual Question Answering (VQA) models suffer from the language prior\nproblem, which is caused by inherent data biases. Specifically, VQA models tend\nto answer questions (e.g., what color is the banana?) based on the\nhigh-frequency answers (e.g., yellow) ignoring image contents. Existing\napproaches tackle this problem by creating delicate models or introducing\nadditional visual annotations to reduce question dependency while strengthening\nimage dependency. However, they are still subject to the language prior problem\nsince the data biases have not been even alleviated. In this paper, we\nintroduce a self-supervised learning framework to solve this problem.\nConcretely, we first automatically generate labeled data to balance the biased\ndata, and propose a self-supervised auxiliary task to utilize the balanced data\nto assist the base VQA model to overcome language priors. Our method can\ncompensate for the data biases by generating balanced data without introducing\nexternal annotations. Experimental results show that our method can\nsignificantly outperform the state-of-the-art, improving the overall accuracy\nfrom 49.50% to 57.59% on the most commonly used benchmark VQA-CP v2. In other\nwords, we can increase the performance of annotation-based methods by 16%\nwithout using external annotations.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 12:30:12 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhu", "Xi", ""], ["Mao", "Zhendong", ""], ["Liu", "Chunxiao", ""], ["Zhang", "Peng", ""], ["Wang", "Bin", ""], ["Zhang", "Yongdong", ""]]}, {"id": "2012.11551", "submitter": "Herv\\'e Le Borgne", "authors": "Antoine Plumerault, Herv\\'e Le Borgne, C\\'eline Hudelot", "title": "AVAE: Adversarial Variational Auto Encoder", "comments": "pre-print version of an article to appear in the proceedings of the\n  International Conference on Pattern Recognition (ICPR 2020) in January 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the wide variety of image generative models, two models stand out:\nVariational Auto Encoders (VAE) and Generative Adversarial Networks (GAN). GANs\ncan produce realistic images, but they suffer from mode collapse and do not\nprovide simple ways to get the latent representation of an image. On the other\nhand, VAEs do not have these problems, but they often generate images less\nrealistic than GANs. In this article, we explain that this lack of realism is\npartially due to a common underestimation of the natural image manifold\ndimensionality. To solve this issue we introduce a new framework that combines\nVAE and GAN in a novel and complementary way to produce an auto-encoding model\nthat keeps VAEs properties while generating images of GAN-quality. We evaluate\nour approach both qualitatively and quantitatively on five image datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 18:29:56 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Plumerault", "Antoine", ""], ["Borgne", "Herv\u00e9 Le", ""], ["Hudelot", "C\u00e9line", ""]]}, {"id": "2012.11552", "submitter": "Spyros Gidaris", "authors": "Spyros Gidaris, Andrei Bursuc, Gilles Puy, Nikos Komodakis, Matthieu\n  Cord, Patrick P\\'erez", "title": "Online Bag-of-Visual-Words Generation for Unsupervised Representation\n  Learning", "comments": "Technical report. Code at https://github.com/valeoai/obow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning image representations without human supervision is an important and\nactive research field. Several recent approaches have successfully leveraged\nthe idea of making such a representation invariant under different types of\nperturbations, especially via contrastive-based instance discrimination\ntraining. Although effective visual representations should indeed exhibit such\ninvariances, there are other important characteristics, such as encoding\ncontextual reasoning skills, for which alternative reconstruction-based\napproaches might be better suited.\n  With this in mind, we propose a teacher-student scheme to learn\nrepresentations by training a convnet to reconstruct a bag-of-visual-words\n(BoW) representation of an image, given as input a perturbed version of that\nsame image. Our strategy performs an online training of both the teacher\nnetwork (whose role is to generate the BoW targets) and the student network\n(whose role is to learn representations), along with an online update of the\nvisual-words vocabulary (used for the BoW targets). This idea effectively\nenables fully online BoW-guided unsupervised learning. Extensive experiments\ndemonstrate the interest of our BoW-based strategy which surpasses previous\nstate-of-the-art methods (including contrastive-based ones) in several\napplications. For instance, in downstream tasks such Pascal object detection,\nPascal classification and Places205 classification, our method improves over\nall prior unsupervised approaches, thus establishing new state-of-the-art\nresults that are also significantly better even than those of supervised\npre-training. We provide the implementation code at\nhttps://github.com/valeoai/obow.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 18:31:21 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Gidaris", "Spyros", ""], ["Bursuc", "Andrei", ""], ["Puy", "Gilles", ""], ["Komodakis", "Nikos", ""], ["Cord", "Matthieu", ""], ["P\u00e9rez", "Patrick", ""]]}, {"id": "2012.11575", "submitter": "Francis Engelmann", "authors": "Francis Engelmann, Konstantinos Rematas, Bastian Leibe, Vittorio\n  Ferrari", "title": "From Points to Multi-Object 3D Reconstruction", "comments": "CVPR2021 - Project Page:\n  https://francisengelmann.github.io/points2objects/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a method to detect and reconstruct multiple 3D objects from a\nsingle RGB image. The key idea is to optimize for detection, alignment and\nshape jointly over all objects in the RGB image, while focusing on realistic\nand physically plausible reconstructions. To this end, we propose a keypoint\ndetector that localizes objects as center points and directly predicts all\nobject properties, including 9-DoF bounding boxes and 3D shapes -- all in a\nsingle forward pass. The proposed method formulates 3D shape reconstruction as\na shape selection problem, i.e. it selects among exemplar shapes from a given\ndatabase. This makes it agnostic to shape representations, which enables a\nlightweight reconstruction of realistic and visually-pleasing shapes based on\nCAD-models, while the training objective is formulated around point clouds and\nvoxel representations. A collision-loss promotes non-intersecting objects,\nfurther increasing the reconstruction realism. Given the RGB image, the\npresented approach performs lightweight reconstruction in a single-stage, it is\nreal-time capable, fully differentiable and end-to-end trainable. Our\nexperiments compare multiple approaches for 9-DoF bounding box estimation,\nevaluate the novel shape-selection mechanism and compare to recent methods in\nterms of 3D bounding box estimation and 3D shape reconstruction quality.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 18:52:21 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 21:05:22 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 22:25:48 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Engelmann", "Francis", ""], ["Rematas", "Konstantinos", ""], ["Leibe", "Bastian", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "2012.11577", "submitter": "Farkhondeh Asadi Dr", "authors": "Mustafa Ghaderzadeh and Farkhondeh Asadi", "title": "Deep Learning in Detection and Diagnosis of Covid-19 using Radiology\n  Modalities: A Systematic Review", "comments": "12 pages,4 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Purpose: Early detection and diagnosis of Covid-19 and accurate separation of\npatients with non-Covid-19 cases at the lowest cost and in the early stages of\nthe disease are one of the main challenges in the epidemic of Covid-19.\nConcerning the novelty of the disease, the diagnostic methods based on\nradiological images suffer shortcomings despite their many uses in diagnostic\ncenters. Accordingly, medical and computer researchers tended to use\nmachine-learning models to analyze radiology images.\n  Methods: Present systematic review was conducted by searching three databases\nof PubMed, Scopus, and Web of Science from November 1, 2019, to July 20, 2020\nBased on a search strategy, the keywords were Covid-19, Deep learning,\nDiagnosis and Detection leading to the extraction of 168 articles that\nultimately, 37 articles were selected as the research population by applying\ninclusion and exclusion criteria. Result: This review study provides an\noverview of the current state of all models for the detection and diagnosis of\nCovid-19 through radiology modalities and their processing based on deep\nlearning. According to the finding, Deep learning Based models have an\nextraordinary capacity to achieve an accurate and efficient system for the\ndetection and diagnosis of Covid-19, which using of them in the processing of\nCT-Scan and X-Ray images, would lead to a significant increase in sensitivity\nand specificity values.\n  Conclusion: The Application of Deep Learning (DL) in the field of Covid-19\nradiologic image processing leads to the reduction of false-positive and\nnegative errors in the detection and diagnosis of this disease and provides an\noptimal opportunity to provide fast, cheap, and safe diagnostic services to\npatients.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 18:54:01 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Ghaderzadeh", "Mustafa", ""], ["Asadi", "Farkhondeh", ""]]}, {"id": "2012.11581", "submitter": "Mohamed Hassan", "authors": "Mohamed Hassan, Partha Ghosh, Joachim Tesch, Dimitrios Tzionas,\n  Michael J. Black", "title": "Populating 3D Scenes by Learning Human-Scene Interaction", "comments": null, "journal-ref": "CVPR2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans live within a 3D space and constantly interact with it to perform\ntasks. Such interactions involve physical contact between surfaces that is\nsemantically meaningful. Our goal is to learn how humans interact with scenes\nand leverage this to enable virtual characters to do the same. To that end, we\nintroduce a novel Human-Scene Interaction (HSI) model that encodes proximal\nrelationships, called POSA for \"Pose with prOximitieS and contActs\". The\nrepresentation of interaction is body-centric, which enables it to generalize\nto new scenes. Specifically, POSA augments the SMPL-X parametric human body\nmodel such that, for every mesh vertex, it encodes (a) the contact probability\nwith the scene surface and (b) the corresponding semantic scene label. We learn\nPOSA with a VAE conditioned on the SMPL-X vertices, and train on the PROX\ndataset, which contains SMPL-X meshes of people interacting with 3D scenes, and\nthe corresponding scene semantics from the PROX-E dataset. We demonstrate the\nvalue of POSA with two applications. First, we automatically place 3D scans of\npeople in scenes. We use a SMPL-X model fit to the scan as a proxy and then\nfind its most likely placement in 3D. POSA provides an effective representation\nto search for \"affordances\" in the scene that match the likely contact\nrelationships for that pose. We perform a perceptual study that shows\nsignificant improvement over the state of the art on this task. Second, we show\nthat POSA's learned representation of body-scene interaction supports monocular\nhuman pose estimation that is consistent with a 3D scene, improving on the\nstate of the art. Our model and code are available for research purposes at\nhttps://posa.is.tue.mpg.de.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 18:57:55 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 15:26:07 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Hassan", "Mohamed", ""], ["Ghosh", "Partha", ""], ["Tesch", "Joachim", ""], ["Tzionas", "Dimitrios", ""], ["Black", "Michael J.", ""]]}, {"id": "2012.11582", "submitter": "Yuval Nirkin", "authors": "Yuval Nirkin, Lior Wolf, Tal Hassner", "title": "HyperSeg: Patch-wise Hypernetwork for Real-time Semantic Segmentation", "comments": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel, real-time, semantic segmentation network in which the\nencoder both encodes and generates the parameters (weights) of the decoder.\nFurthermore, to allow maximal adaptivity, the weights at each decoder block\nvary spatially. For this purpose, we design a new type of hypernetwork,\ncomposed of a nested U-Net for drawing higher level context features, a\nmulti-headed weight generating module which generates the weights of each block\nin the decoder immediately before they are consumed, for efficient memory\nutilization, and a primary network that is composed of novel dynamic patch-wise\nconvolutions. Despite the usage of less-conventional blocks, our architecture\nobtains real-time performance. In terms of the runtime vs. accuracy trade-off,\nwe surpass state of the art (SotA) results on popular semantic segmentation\nbenchmarks: PASCAL VOC 2012 (val. set) and real-time semantic segmentation on\nCityscapes, and CamVid. The code is available: https://nirkin.com/hyperseg.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 18:58:18 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 10:40:36 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Nirkin", "Yuval", ""], ["Wolf", "Lior", ""], ["Hassner", "Tal", ""]]}, {"id": "2012.11583", "submitter": "Changan Chen", "authors": "Changan Chen, Ziad Al-Halah, Kristen Grauman", "title": "Semantic Audio-Visual Navigation", "comments": "Project page:\n  http://vision.cs.utexas.edu/projects/semantic-audio-visual-navigation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on audio-visual navigation assumes a constantly-sounding target\nand restricts the role of audio to signaling the target's position. We\nintroduce semantic audio-visual navigation, where objects in the environment\nmake sounds consistent with their semantic meaning (e.g., toilet flushing, door\ncreaking) and acoustic events are sporadic or short in duration. We propose a\ntransformer-based model to tackle this new semantic AudioGoal task,\nincorporating an inferred goal descriptor that captures both spatial and\nsemantic properties of the target. Our model's persistent multimodal memory\nenables it to reach the goal even long after the acoustic event stops. In\nsupport of the new task, we also expand the SoundSpaces audio simulations to\nprovide semantically grounded sounds for an array of objects in Matterport3D.\nOur method strongly outperforms existing audio-visual navigation methods by\nlearning to associate semantic, acoustic, and visual cues.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 18:59:04 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 01:59:26 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Chen", "Changan", ""], ["Al-Halah", "Ziad", ""], ["Grauman", "Kristen", ""]]}, {"id": "2012.11585", "submitter": "Justin Liang", "authors": "Justin Liang, Raquel Urtasun", "title": "End-to-End Deep Structured Models for Drawing Crosswalks", "comments": null, "journal-ref": "ECCV 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of detecting crosswalks from LiDAR and\ncamera imagery. Towards this goal, given multiple LiDAR sweeps and the\ncorresponding imagery, we project both inputs onto the ground surface to\nproduce a top down view of the scene. We then leverage convolutional neural\nnetworks to extract semantic cues about the location of the crosswalks. These\nare then used in combination with road centerlines from freely available maps\n(e.g., OpenStreetMaps) to solve a structured optimization problem which draws\nthe final crosswalk boundaries. Our experiments over crosswalks in a large city\narea show that 96.6% automation can be achieved.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 18:59:08 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 23:33:43 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2021 20:35:01 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Liang", "Justin", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2012.11587", "submitter": "Chuang Gan", "authors": "Jianwei Yang, Jiayuan Mao, Jiajun Wu, Devi Parikh, David D. Cox,\n  Joshua B. Tenenbaum, Chuang Gan", "title": "Object-Centric Diagnosis of Visual Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When answering questions about an image, it not only needs knowing what --\nunderstanding the fine-grained contents (e.g., objects, relationships) in the\nimage, but also telling why -- reasoning over grounding visual cues to derive\nthe answer for a question. Over the last few years, we have seen significant\nprogress on visual question answering. Though impressive as the accuracy grows,\nit still lags behind to get knowing whether these models are undertaking\ngrounding visual reasoning or just leveraging spurious correlations in the\ntraining data. Recently, a number of works have attempted to answer this\nquestion from perspectives such as grounding and robustness. However, most of\nthem are either focusing on the language side or coarsely studying the\npixel-level attention maps. In this paper, by leveraging the step-wise object\ngrounding annotations provided in the GQA dataset, we first present a\nsystematical object-centric diagnosis of visual reasoning on grounding and\nrobustness, particularly on the vision side. According to the extensive\ncomparisons across different models, we find that even models with high\naccuracy are not good at grounding objects precisely, nor robust to visual\ncontent perturbations. In contrast, symbolic and modular models have a\nrelatively better grounding and robustness, though at the cost of accuracy. To\nreconcile these different aspects, we further develop a diagnostic model,\nnamely Graph Reasoning Machine. Our model replaces purely symbolic visual\nrepresentation with probabilistic scene graph and then applies teacher-forcing\ntraining for the visual reasoning module. The designed model improves the\nperformance on all three metrics over the vanilla neural-symbolic model while\ninheriting the transparency. Further ablation studies suggest that this\nimprovement is mainly due to more accurate image understanding and proper\nintermediate reasoning supervisions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 18:59:28 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Yang", "Jianwei", ""], ["Mao", "Jiayuan", ""], ["Wu", "Jiajun", ""], ["Parikh", "Devi", ""], ["Cox", "David D.", ""], ["Tenenbaum", "Joshua B.", ""], ["Gan", "Chuang", ""]]}, {"id": "2012.11655", "submitter": "Hyojin Park", "authors": "Hyojin Park, Jayeon Yoo, Seohyeong Jeong, Ganesh Venkatesh, Nojun Kwak", "title": "Learning Dynamic Network Using a Reuse Gate Function in Semi-supervised\n  Video Object Segmentation", "comments": "CVPR2021, code: https://github.com/HYOJINPARK/Reuse_VOS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art approaches for Semi-supervised Video Object\nSegmentation (Semi-VOS) propagates information from previous frames to generate\nsegmentation mask for the current frame. This results in high-quality\nsegmentation across challenging scenarios such as changes in appearance and\nocclusion. But it also leads to unnecessary computations for stationary or\nslow-moving objects where the change across frames is minimal. In this work, we\nexploit this observation by using temporal information to quickly identify\nframes with minimal change and skip the heavyweight mask generation step. To\nrealize this efficiency, we propose a novel dynamic network that estimates\nchange across frames and decides which path -- computing a full network or\nreusing previous frame's feature -- to choose depending on the expected\nsimilarity. Experimental results show that our approach significantly improves\ninference speed without much accuracy degradation on challenging Semi-VOS\ndatasets -- DAVIS 16, DAVIS 17, and YouTube-VOS. Furthermore, our approach can\nbe applied to multiple Semi-VOS methods demonstrating its generality. The code\nis available in https://github.com/HYOJINPARK/Reuse_VOS.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 19:40:17 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 11:25:26 GMT"}, {"version": "v3", "created": "Sun, 16 May 2021 11:54:06 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Park", "Hyojin", ""], ["Yoo", "Jayeon", ""], ["Jeong", "Seohyeong", ""], ["Venkatesh", "Ganesh", ""], ["Kwak", "Nojun", ""]]}, {"id": "2012.11673", "submitter": "Aman Gupta", "authors": "Sirjan Kafle, Aman Gupta, Xue Xia, Ananth Sankar, Xi Chen, Di Wen,\n  Liang Zhang", "title": "Smoothed Gaussian Mixture Models for Video Classification and\n  Recommendation", "comments": "11 pages, 3 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster-and-aggregate techniques such as Vector of Locally Aggregated\nDescriptors (VLAD), and their end-to-end discriminatively trained equivalents\nlike NetVLAD have recently been popular for video classification and action\nrecognition tasks. These techniques operate by assigning video frames to\nclusters and then representing the video by aggregating residuals of frames\nwith respect to the mean of each cluster. Since some clusters may see very\nlittle video-specific data, these features can be noisy. In this paper, we\npropose a new cluster-and-aggregate method which we call smoothed Gaussian\nmixture model (SGMM), and its end-to-end discriminatively trained equivalent,\nwhich we call deep smoothed Gaussian mixture model (DSGMM). SGMM represents\neach video by the parameters of a Gaussian mixture model (GMM) trained for that\nvideo. Low-count clusters are addressed by smoothing the video-specific\nestimates with a universal background model (UBM) trained on a large number of\nvideos. The primary benefit of SGMM over VLAD is smoothing which makes it less\nsensitive to small number of training samples. We show, through extensive\nexperiments on the YouTube-8M classification task, that SGMM/DSGMM is\nconsistently better than VLAD/NetVLAD by a small but statistically significant\nmargin. We also show results using a dataset created at LinkedIn to predict if\na member will watch an uploaded video.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 06:52:41 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Kafle", "Sirjan", ""], ["Gupta", "Aman", ""], ["Xia", "Xue", ""], ["Sankar", "Ananth", ""], ["Chen", "Xi", ""], ["Wen", "Di", ""], ["Zhang", "Liang", ""]]}, {"id": "2012.11684", "submitter": "Radu Tudor Ionescu", "authors": "Ismat Ara Reshma, Sylvain Cussat-Blanc, Radu Tudor Ionescu, Herv\\'e\n  Luga, Josiane Mothe", "title": "Natural vs Balanced Distribution in Deep Learning on Whole Slide Images\n  for Cancer Detection", "comments": "Accepted at SAC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class distribution of data is one of the factors that regulates the\nperformance of machine learning models. However, investigations on the impact\nof different distributions available in the literature are very few, sometimes\nabsent for domain-specific tasks. In this paper, we analyze the impact of\nnatural and balanced distributions of the training set in deep learning (DL)\nmodels applied on histological images, also known as whole slide images (WSIs).\nWSIs are considered as the gold standard for cancer diagnosis. In recent years,\nresearchers have turned their attention to DL models to automate and accelerate\nthe diagnosis process. In the training of such DL models, filtering out the\nnon-regions-of-interest from the WSIs and adopting an artificial distribution\n(usually, a balanced distribution) is a common trend. In our analysis, we show\nthat keeping the WSIs data in their usual distribution (which we call natural\ndistribution) for DL training produces fewer false positives (FPs) with\ncomparable false negatives (FNs) than the artificially-obtained balanced\ndistribution. We conduct an empirical comparative study with 10 random folds\nfor each distribution, comparing the resulting average performance levels in\nterms of five different evaluation metrics. Experimental results show the\neffectiveness of the natural distribution over the balanced one across all the\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 21:18:49 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Reshma", "Ismat Ara", ""], ["Cussat-Blanc", "Sylvain", ""], ["Ionescu", "Radu Tudor", ""], ["Luga", "Herv\u00e9", ""], ["Mothe", "Josiane", ""]]}, {"id": "2012.11691", "submitter": "Youssef Mroueh", "authors": "Pierre Dognin, Igor Melnyk, Youssef Mroueh, Inkit Padhi, Mattia\n  Rigotti, Jarret Ross, Yair Schiff", "title": "Alleviating Noisy Data in Image Captioning with Cooperative Distillation", "comments": "CVPR 2020 VizWiz Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image captioning systems have made substantial progress, largely due to the\navailability of curated datasets like Microsoft COCO or Vizwiz that have\naccurate descriptions of their corresponding images. Unfortunately, scarce\navailability of such cleanly labeled data results in trained algorithms\nproducing captions that can be terse and idiosyncratically specific to details\nin the image. We propose a new technique, cooperative distillation that\ncombines clean curated datasets with the web-scale automatically extracted\ncaptions of the Google Conceptual Captions dataset (GCC), which can have poor\ndescriptions of images, but is abundant in size and therefore provides a rich\nvocabulary resulting in more expressive captions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 21:32:28 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Dognin", "Pierre", ""], ["Melnyk", "Igor", ""], ["Mroueh", "Youssef", ""], ["Padhi", "Inkit", ""], ["Rigotti", "Mattia", ""], ["Ross", "Jarret", ""], ["Schiff", "Yair", ""]]}, {"id": "2012.11696", "submitter": "Youssef Mroueh", "authors": "Pierre Dognin, Igor Melnyk, Youssef Mroueh, Inkit Padhi, Mattia\n  Rigotti, Jarret Ross, Yair Schiff, Richard A. Young, Brian Belgodere", "title": "Image Captioning as an Assistive Technology: Lessons Learned from VizWiz\n  2020 Challenge", "comments": "In submission to JAIR. Copyright may be transferred without notice,\n  after which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning has recently demonstrated impressive progress largely owing\nto the introduction of neural network algorithms trained on curated dataset\nlike MS-COCO. Often work in this field is motivated by the promise of\ndeployment of captioning systems in practical applications. However, the\nscarcity of data and contexts in many competition datasets renders the utility\nof systems trained on these datasets limited as an assistive technology in\nreal-world settings, such as helping visually impaired people navigate and\naccomplish everyday tasks. This gap motivated the introduction of the novel\nVizWiz dataset, which consists of images taken by the visually impaired and\ncaptions that have useful, task-oriented information. In an attempt to help the\nmachine learning computer vision field realize its promise of producing\ntechnologies that have positive social impact, the curators of the VizWiz\ndataset host several competitions, including one for image captioning. This\nwork details the theory and engineering from our winning submission to the 2020\ncaptioning competition. Our work provides a step towards improved assistive\nimage captioning systems.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 21:48:18 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 00:16:56 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Dognin", "Pierre", ""], ["Melnyk", "Igor", ""], ["Mroueh", "Youssef", ""], ["Padhi", "Inkit", ""], ["Rigotti", "Mattia", ""], ["Ross", "Jarret", ""], ["Schiff", "Yair", ""], ["Young", "Richard A.", ""], ["Belgodere", "Brian", ""]]}, {"id": "2012.11704", "submitter": "Bin Yang", "authors": "Bin Yang, Ming Liang, Raquel Urtasun", "title": "HDNET: Exploiting HD Maps for 3D Object Detection", "comments": "Spotlight paper at 2nd Conference on Robot Learning (CoRL 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we show that High-Definition (HD) maps provide strong priors\nthat can boost the performance and robustness of modern 3D object detectors.\nTowards this goal, we design a single stage detector that extracts geometric\nand semantic features from the HD maps. As maps might not be available\neverywhere, we also propose a map prediction module that estimates the map on\nthe fly from raw LiDAR data. We conduct extensive experiments on KITTI as well\nas a large-scale 3D detection benchmark containing 1 million frames, and show\nthat the proposed map-aware detector consistently outperforms the\nstate-of-the-art in both mapped and un-mapped scenarios. Importantly the whole\nframework runs at 20 frames per second.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 21:59:54 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Yang", "Bin", ""], ["Liang", "Ming", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2012.11717", "submitter": "Yuejiang Liu", "authors": "Yuejiang Liu, Qi Yan, Alexandre Alahi", "title": "Social NCE: Contrastive Learning of Socially-aware Motion\n  Representations", "comments": "Code is available at\n  https://github.com/vita-epfl/social-nce-crowdnav,\n  https://github.com/YuejiangLIU/social-nce-trajectron-plus-plus and\n  https://github.com/qiyan98/social-nce-stgcnn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning socially-aware motion representations is at the core of recent\nadvances in human trajectory forecasting and robot navigation in crowded\nspaces. Despite promising progress, existing neural motion models often\nstruggle to generalize in closed-loop operations (e.g., output colliding\ntrajectories), when the training set lacks examples collected from dangerous\nscenarios. In this work, we propose to address this issue via contrastive\nlearning with negative data augmentation. Concretely, we introduce a social\ncontrastive loss that encourages the encoded motion representation to preserve\nsufficient information for distinguishing a positive future event from a set of\nnegative ones. We explicitly draw these negative samples based on our domain\nknowledge of unfavorable circumstances in the multi-agent context. Experimental\nresults show that the proposed method dramatically reduces the collision rates\nof recent trajectory forecasting, behavioral cloning and reinforcement learning\nalgorithms, outperforming current state-of-the-art models on several\nbenchmarks. Our method makes few assumptions about neural architecture designs,\nand hence can be used as a generic way to promote the robustness of neural\nmotion models.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 22:25:06 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 17:54:33 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Liu", "Yuejiang", ""], ["Yan", "Qi", ""], ["Alahi", "Alexandre", ""]]}, {"id": "2012.11745", "submitter": "Zbigniew Wojna", "authors": "Tien Chu, Kamil Mykitiuk, Miron Szewczyk, Adam Wiktor, Zbigniew Wojna", "title": "Training DNNs in O(1) memory with MEM-DFA using Random Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work presents a method for reducing memory consumption to a constant\ncomplexity when training deep neural networks. The algorithm is based on the\nmore biologically plausible alternatives of the backpropagation (BP): direct\nfeedback alignment (DFA) and feedback alignment (FA), which use random matrices\nto propagate error. The proposed method, memory-efficient direct feedback\nalignment (MEM-DFA), uses higher independence of layers in DFA and allows\navoiding storing at once all activation vectors, unlike standard BP, FA, and\nDFA. Thus, our algorithm's memory usage is constant regardless of the number of\nlayers in a neural network. The method increases the computational cost only by\na constant factor of one extra forward pass.\n  The MEM-DFA, BP, FA, and DFA were evaluated along with their memory profiles\non MNIST and CIFAR-10 datasets on various neural network models. Our\nexperiments agree with our theoretical results and show a significant decrease\nin the memory cost of MEM-DFA compared to the other algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 23:27:40 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Chu", "Tien", ""], ["Mykitiuk", "Kamil", ""], ["Szewczyk", "Miron", ""], ["Wiktor", "Adam", ""], ["Wojna", "Zbigniew", ""]]}, {"id": "2012.11753", "submitter": "Qian Wang", "authors": "Qian Wang, Toby P. Breckon", "title": "Contraband Materials Detection Within Volumetric 3D Computed Tomography\n  Baggage Security Screening Imagery", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic prohibited object detection within 2D/3D X-ray Computed Tomography\n(CT) has been studied in literature to enhance the aviation security screening\nat checkpoints. Deep Convolutional Neural Networks (CNN) have demonstrated\nsuperior performance in 2D X-ray imagery. However, there exists very limited\nproof of how deep neural networks perform in materials detection within\nvolumetric 3D CT baggage screening imagery. We attempt to close this gap by\napplying Deep Neural Networks in 3D contraband substance detection based on\ntheir material signatures. Specifically, we formulate it as a 3D semantic\nsegmentation problem to identify material types for all voxels based on which\ncontraband materials can be detected. To this end, we firstly investigate 3D\nCNN based semantic segmentation algorithms such as 3D U-Net and its variants.\nIn contrast to the original dense representation form of volumetric 3D CT data,\nwe propose to convert the CT volumes into sparse point clouds which allows the\nuse of point cloud processing approaches such as PointNet++ towards more\nefficient processing. Experimental results on a publicly available dataset (NEU\nATR) demonstrate the effectiveness of both 3D U-Net and PointNet++ in materials\ndetection in 3D CT imagery for baggage security screening.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 23:48:06 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Wang", "Qian", ""], ["Breckon", "Toby P.", ""]]}, {"id": "2012.11772", "submitter": "Maximilian Fiedler", "authors": "Maximilian Fiedler and Andreas Alpers", "title": "Power-SLIC: Diagram-based superpixel generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpixel algorithms, which group pixels similar in color and other\nlow-level properties, are increasingly used for pre-processing in image\nsegmentation. Commonly important criteria for the computation of superpixels\nare boundary adherence, speed, and regularity.\n  Boundary adherence and regularity are typically contradictory goals. Most\nrecent algorithms have focused on improving boundary adherence. Motivated by\nimproving superpixel regularity, we propose a diagram-based superpixel\ngeneration method called Power-SLIC.\n  On the BSDS500 data set, Power-SLIC outperforms other state-of-the-art\nalgorithms in terms of compactness and boundary precision, and its boundary\nadherence is the most robust against varying levels of Gaussian noise. In terms\nof speed, Power-SLIC is competitive with SLIC.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 00:57:21 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Fiedler", "Maximilian", ""], ["Alpers", "Andreas", ""]]}, {"id": "2012.11779", "submitter": "Philip Edwards PhD", "authors": "P.J. \"Eddie'' Edwards, Dimitris Psychogyios, Stefanie Speidel, Lena\n  Maier-Hein and Danail Stoyanov", "title": "SERV-CT: A disparity dataset from CT for validation of endoscopic 3D\n  reconstruction", "comments": "Submitted to Medical Image Analysis. 14 Figures, 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision, reference datasets have been highly successful in\npromoting algorithmic development in stereo reconstruction. Surgical scenes\ngives rise to specific problems, including the lack of clear corner features,\nhighly specular surfaces and the presence of blood and smoke. Publicly\navailable datasets have been produced using CT and either phantom images or\nbiological tissue samples covering a relatively small region of the endoscope\nfield-of-view. We present a stereo-endoscopic reconstruction validation dataset\nbased on CT (SERV-CT). Two {\\it ex vivo} small porcine full torso cadavers were\nplaced within the view of the endoscope with both the endoscope and target\nanatomy visible in the CT scan. Orientation of the endoscope was manually\naligned to the stereoscopic view. Reference disparities and occlusions were\ncalculated for 8 stereo pairs from each sample. For the second sample an RGB\nsurface was acquired to aid alignment of smooth, featureless surfaces. Repeated\nmanual alignments showed an RMS disparity accuracy of ~2 pixels and a depth\naccuracy of ~2mm. The reference dataset includes endoscope image pairs with\ncorresponding calibration, disparities, depths and occlusions covering the\nmajority of the endoscopic image and a range of tissue types. Smooth specular\nsurfaces and images with significant variation of depth are included. We\nassessed the performance of various stereo algorithms from online available\nrepositories. There is a significant variation between algorithms, highlighting\nsome of the challenges of surgical endoscopic images. The SERV-CT dataset\nprovides an easy to use stereoscopic validation for surgical applications with\nsmooth reference disparities and depths with coverage over the majority of the\nendoscopic images. This complements existing resources well and we hope will\naid the development of surgical endoscopic anatomical reconstruction\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 01:28:30 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Edwards", "P. J. \"Eddie''", ""], ["Psychogyios", "Dimitris", ""], ["Speidel", "Stefanie", ""], ["Maier-Hein", "Lena", ""], ["Stoyanov", "Danail", ""]]}, {"id": "2012.11780", "submitter": "Jonathan Kissi-Ameyaw", "authors": "J. Kissi-Ameyaw, K. McIsaac, X. Wang, G. R. Osinski", "title": "Towards an Automatic System for Extracting Planar Orientations from\n  Software Generated Point Clouds", "comments": "11 pages, 10 figures, submitted to Computers and Geoscience", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In geology, a key activity is the characterisation of geological structures\n(surface formation topology and rock units) using Planar Orientation\nmeasurements such as Strike, Dip and Dip Direction. In general these\nmeasurements are collected manually using basic equipment; usually a\ncompass/clinometer and a backboard, recorded on a map by hand. Various\ncomputing techniques and technologies, such as Lidar, have been utilised in\norder to automate this process and update the collection paradigm for these\ntypes of measurements. Techniques such as Structure from Motion (SfM)\nreconstruct of scenes and objects by generating a point cloud from input\nimages, with detailed reconstruction possible on the decimetre scale. SfM-type\ntechniques provide advantages in areas of cost and usability in more varied\nenvironmental conditions, while sacrificing the extreme levels of data\nfidelity. Here is presented a methodology of data acquisition and a Machine\nLearning-based software system: GeoStructure, developed to automate the\nmeasurement of orientation measurements. Rather than deriving measurements\nusing a method applied to the input images, such as the Hough Transform, this\nmethod takes measurements directly from the reconstructed point cloud surfaces.\nPoint cloud noise is mitigated using a Mahalanobis distance implementation.\nSignificant structure is characterised using a k-nearest neighbour region\ngrowing algorithm, and final surface orientations are quantified using the\nplane, and normal direction cosines.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 01:35:47 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Kissi-Ameyaw", "J.", ""], ["McIsaac", "K.", ""], ["Wang", "X.", ""], ["Osinski", "G. R.", ""]]}, {"id": "2012.11803", "submitter": "Bingyao Huang", "authors": "Bingyao Huang and Ruyi Lian and Dimitris Samaras and Haibin Ling", "title": "Modeling Deep Learning Based Privacy Attacks on Physical Mail", "comments": "Source code: https://github.com/BingyaoHuang/Neural-STE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mail privacy protection aims to prevent unauthorized access to hidden content\nwithin an envelope since normal paper envelopes are not as safe as we think. In\nthis paper, for the first time, we show that with a well designed deep learning\nmodel, the hidden content may be largely recovered without opening the\nenvelope. We start by modeling deep learning-based privacy attacks on physical\nmail content as learning the mapping from the camera-captured envelope front\nface image to the hidden content, then we explicitly model the mapping as a\ncombination of perspective transformation, image dehazing and denoising using a\ndeep convolutional neural network, named Neural-STE (See-Through-Envelope). We\nshow experimentally that hidden content details, such as texture and image\nstructure, can be clearly recovered. Finally, our formulation and model allow\nus to design envelopes that can counter deep learning-based privacy attacks on\nphysical mail.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 02:54:00 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 21:02:54 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Huang", "Bingyao", ""], ["Lian", "Ruyi", ""], ["Samaras", "Dimitris", ""], ["Ling", "Haibin", ""]]}, {"id": "2012.11806", "submitter": "Yu Cheng", "authors": "Yu Cheng, Bo Wang, Bo Yang, Robby T. Tan", "title": "Graph and Temporal Convolutional Networks for 3D Multi-person Pose\n  Estimation in Monocular Videos", "comments": "Accepted to AAAI 2021. Code is available at:\n  https://github.com/3dpose/GnTCN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent progress, 3D multi-person pose estimation from monocular\nvideos is still challenging due to the commonly encountered problem of missing\ninformation caused by occlusion, partially out-of-frame target persons, and\ninaccurate person detection. To tackle this problem, we propose a novel\nframework integrating graph convolutional networks (GCNs) and temporal\nconvolutional networks (TCNs) to robustly estimate camera-centric multi-person\n3D poses that do not require camera parameters. In particular, we introduce a\nhuman-joint GCN, which, unlike the existing GCN, is based on a directed graph\nthat employs the 2D pose estimator's confidence scores to improve the pose\nestimation results. We also introduce a human-bone GCN, which models the bone\nconnections and provides more information beyond human joints. The two GCNs\nwork together to estimate the spatial frame-wise 3D poses and can make use of\nboth visible joint and bone information in the target frame to estimate the\noccluded or missing human-part information. To further refine the 3D pose\nestimation, we use our temporal convolutional networks (TCNs) to enforce the\ntemporal and human-dynamics constraints. We use a joint-TCN to estimate\nperson-centric 3D poses across frames, and propose a velocity-TCN to estimate\nthe speed of 3D joints to ensure the consistency of the 3D pose estimation in\nconsecutive frames. Finally, to estimate the 3D human poses for multiple\npersons, we propose a root-TCN that estimates camera-centric 3D poses without\nrequiring camera parameters. Quantitative and qualitative evaluations\ndemonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 03:01:19 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 05:34:06 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 06:26:48 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Cheng", "Yu", ""], ["Wang", "Bo", ""], ["Yang", "Bo", ""], ["Tan", "Robby T.", ""]]}, {"id": "2012.11807", "submitter": "Zijian Li", "authors": "Ruichu Cai, Zijian Li, Pengfei Wei, Jie Qiao, Kun Zhang, Zhifeng Hao", "title": "Learning Disentangled Semantic Representation for Domain Adaptation", "comments": null, "journal-ref": null, "doi": "10.24963/ijcai.2019/285", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation is an important but challenging task. Most of the existing\ndomain adaptation methods struggle to extract the domain-invariant\nrepresentation on the feature space with entangling domain information and\nsemantic information. Different from previous efforts on the entangled feature\nspace, we aim to extract the domain invariant semantic information in the\nlatent disentangled semantic representation (DSR) of the data. In DSR, we\nassume the data generation process is controlled by two independent sets of\nvariables, i.e., the semantic latent variables and the domain latent variables.\nUnder the above assumption, we employ a variational auto-encoder to reconstruct\nthe semantic latent variables and domain latent variables behind the data. We\nfurther devise a dual adversarial network to disentangle these two sets of\nreconstructed latent variables. The disentangled semantic latent variables are\nfinally adapted across the domains. Experimental studies testify that our model\nyields state-of-the-art performance on several domain adaptation benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 03:03:36 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Cai", "Ruichu", ""], ["Li", "Zijian", ""], ["Wei", "Pengfei", ""], ["Qiao", "Jie", ""], ["Zhang", "Kun", ""], ["Hao", "Zhifeng", ""]]}, {"id": "2012.11810", "submitter": "Haoyu He", "authors": "Haoyu He, Jing Zhang, Bhavani Thuraisingham, Dacheng Tao", "title": "Progressive One-shot Human Parsing", "comments": "Accepted in AAAI 2021. 9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior human parsing models are limited to parsing humans into classes\npre-defined in the training data, which is not flexible to generalize to unseen\nclasses, e.g., new clothing in fashion analysis. In this paper, we propose a\nnew problem named one-shot human parsing (OSHP) that requires to parse human\ninto an open set of reference classes defined by any single reference example.\nDuring training, only base classes defined in the training set are exposed,\nwhich can overlap with part of reference classes. In this paper, we devise a\nnovel Progressive One-shot Parsing network (POPNet) to address two critical\nchallenges , i.e., testing bias and small sizes. POPNet consists of two\ncollaborative metric learning modules named Attention Guidance Module and\nNearest Centroid Module, which can learn representative prototypes for base\nclasses and quickly transfer the ability to unseen classes during testing,\nthereby reducing testing bias. Moreover, POPNet adopts a progressive human\nparsing framework that can incorporate the learned knowledge of parent classes\nat the coarse granularity to help recognize the descendant classes at the fine\ngranularity, thereby handling the small sizes issue. Experiments on the ATR-OS\nbenchmark tailored for OSHP demonstrate POPNet outperforms other representative\none-shot segmentation models by large margins and establishes a strong\nbaseline. Source code can be found at\nhttps://github.com/Charleshhy/One-shot-Human-Parsing.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 03:06:11 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 04:50:06 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 11:51:23 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["He", "Haoyu", ""], ["Zhang", "Jing", ""], ["Thuraisingham", "Bhavani", ""], ["Tao", "Dacheng", ""]]}, {"id": "2012.11812", "submitter": "Shuang Zhou", "authors": "Shuang Zhou, Lingchao Guo, Zhaoming Lu, Xiangming Wen, Wei Zheng,\n  Yiming Wang", "title": "Subject-independent Human Pose Image Construction with Commodity Wi-Fi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, commodity Wi-Fi devices have been shown to be able to construct\nhuman pose images, i.e., human skeletons, as fine-grained as cameras. Existing\npapers achieve good results when constructing the images of subjects who are in\nthe prior training samples. However, the performance drops when it comes to new\nsubjects, i.e., the subjects who are not in the training samples. This paper\nfocuses on solving the subject-generalization problem in human pose image\nconstruction. To this end, we define the subject as the domain. Then we design\na Domain-Independent Neural Network (DINN) to extract subject-independent\nfeatures and convert them into fine-grained human pose images. We also propose\na novel training method to train the DINN and it has no re-training overhead\ncomparing with the domain-adversarial approach. We build a prototype system and\nexperimental results demonstrate that our system can construct fine-grained\nhuman pose images of new subjects with commodity Wi-Fi in both the visible and\nthrough-wall scenarios, which shows the effectiveness and the\nsubject-generalization ability of our model.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 03:15:56 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Zhou", "Shuang", ""], ["Guo", "Lingchao", ""], ["Lu", "Zhaoming", ""], ["Wen", "Xiangming", ""], ["Zheng", "Wei", ""], ["Wang", "Yiming", ""]]}, {"id": "2012.11834", "submitter": "Teguh Budianto", "authors": "Teguh Budianto, Tomohiro Nakai, Kazunori Imoto, Takahiro Takimoto,\n  Kosuke Haruki", "title": "Dual-encoder Bidirectional Generative Adversarial Networks for Anomaly\n  Detection", "comments": "2020 19th IEEE International Conference on Machine Learning and\n  Applications (ICMLA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have shown promise for various\nproblems including anomaly detection. When anomaly detection is performed using\nGAN models that learn only the features of normal data samples, data that are\nnot similar to normal data are detected as abnormal samples. The present\napproach is developed by employing a dual-encoder in a bidirectional GAN\narchitecture that is trained simultaneously with a generator and a\ndiscriminator network. Through the learning mechanism, the proposed method aims\nto reduce the problem of bad cycle consistency, in which a bidirectional GAN\nmight not be able to reproduce samples with a large difference between normal\nand abnormal samples. We assume that bad cycle consistency occurs when the\nmethod does not preserve enough information of the sample data. We show that\nour proposed method performs well in capturing the distribution of normal\nsamples, thereby improving anomaly detection on GAN-based models. Experiments\nare reported in which our method is applied to publicly available datasets,\nincluding application to a brain magnetic resonance imaging anomaly detection\nsystem.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 05:05:33 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Budianto", "Teguh", ""], ["Nakai", "Tomohiro", ""], ["Imoto", "Kazunori", ""], ["Takimoto", "Takahiro", ""], ["Haruki", "Kosuke", ""]]}, {"id": "2012.11840", "submitter": "Abbas Khosravi", "authors": "Hamzeh Asgharnezhad, Afshar Shamsi, Roohallah Alizadehsani, Abbas\n  Khosravi, Saeid Nahavandi, Zahra Alizadeh Sani, and Dipti Srinivasan", "title": "Objective Evaluation of Deep Uncertainty Predictions for COVID-19\n  Detection", "comments": "7 pages, 6 figures, 1 Table, 36 refrences", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been widely applied for detecting COVID-19\nin medical images. Existing studies mainly apply transfer learning and other\ndata representation strategies to generate accurate point estimates. The\ngeneralization power of these networks is always questionable due to being\ndeveloped using small datasets and failing to report their predictive\nconfidence. Quantifying uncertainties associated with DNN predictions is a\nprerequisite for their trusted deployment in medical settings. Here we apply\nand evaluate three uncertainty quantification techniques for COVID-19 detection\nusing chest X-Ray (CXR) images. The novel concept of uncertainty confusion\nmatrix is proposed and new performance metrics for the objective evaluation of\nuncertainty estimates are introduced. Through comprehensive experiments, it is\nshown that networks pertained on CXR images outperform networks pretrained on\nnatural image datasets such as ImageNet. Qualitatively and quantitatively\nevaluations also reveal that the predictive uncertainty estimates are\nstatistically higher for erroneous predictions than correct predictions.\nAccordingly, uncertainty quantification methods are capable of flagging risky\npredictions with high uncertainty estimates. We also observe that ensemble\nmethods more reliably capture uncertainties during the inference.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 05:43:42 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Asgharnezhad", "Hamzeh", ""], ["Shamsi", "Afshar", ""], ["Alizadehsani", "Roohallah", ""], ["Khosravi", "Abbas", ""], ["Nahavandi", "Saeid", ""], ["Sani", "Zahra Alizadeh", ""], ["Srinivasan", "Dipti", ""]]}, {"id": "2012.11841", "submitter": "Ye-Ming Meng", "authors": "Ye-Ming Meng, Jing Zhang, Peng Zhang, Chao Gao and Shi-Ju Ran", "title": "Residual Matrix Product State for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.str-el cs.CV quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor network (TN), which originates from quantum physics, shows broad\nprospects in classical and quantum machine learning (ML). However, there still\nexists a considerable gap of accuracy between TN and the sophisticated neural\nnetwork (NN) models for classical ML. It is still elusive how far TN ML can be\nimproved by, e.g., borrowing the techniques from NN. In this work, we propose\nthe residual matrix product state (ResMPS) by combining the ideas of matrix\nproduct state (MPS) and residual NN. ResMPS can be treated as a network where\nits layers map the \"hidden\" features to the outputs (e.g., classifications),\nand the variational parameters of the layers are the functions of the features\nof samples (e.g., pixels of images). This is essentially different from NN,\nwhere the layers map feed-forwardly the features to the output. ResMPS can\nnaturally incorporate with the non-linear activations and dropout layers, and\noutperforms the state-of-the-art TN models on the efficiency, stability, and\nexpression power. Besides, ResMPS is interpretable from the perspective of\npolynomial expansion, where the factorization and exponential machines\nnaturally emerge. Our work contributes to connecting and hybridizing neural and\ntensor networks, which is crucial to understand the working mechanisms further\nand improve both models' performances.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 05:44:20 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Meng", "Ye-Ming", ""], ["Zhang", "Jing", ""], ["Zhang", "Peng", ""], ["Gao", "Chao", ""], ["Ran", "Shi-Ju", ""]]}, {"id": "2012.11847", "submitter": "Xiaopeng Guo", "authors": "Liye Mei, Yalan Yu, Yueyun Weng, Xiaopeng Guo, Yan Liu, Du Wang, Sheng\n  Liu, Fuling Zhou, and Cheng Lei", "title": "Adversarial Multiscale Feature Learning for Overlapping Chromosome\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chromosome karyotype analysis is of great clinical importance in the\ndiagnosis and treatment of diseases, especially for genetic diseases. Since\nmanual analysis is highly time and effort consuming, computer-assisted\nautomatic chromosome karyotype analysis based on images is routinely used to\nimprove the efficiency and accuracy of the analysis. Due to the strip shape of\nthe chromosomes, they easily get overlapped with each other when imaged,\nsignificantly affecting the accuracy of the analysis afterward. Conventional\noverlapping chromosome segmentation methods are usually based on manually\ntagged features, hence, the performance of which is easily affected by the\nquality, such as resolution and brightness, of the images. To address the\nproblem, in this paper, we present an adversarial multiscale feature learning\nframework to improve the accuracy and adaptability of overlapping chromosome\nsegmentation. Specifically, we first adopt the nested U-shape network with\ndense skip connections as the generator to explore the optimal representation\nof the chromosome images by exploiting multiscale features. Then we use the\nconditional generative adversarial network (cGAN) to generate images similar to\nthe original ones, the training stability of which is enhanced by applying the\nleast-square GAN objective. Finally, we employ Lovasz-Softmax to help the model\nconverge in a continuous optimization setting. Comparing with the established\nalgorithms, the performance of our framework is proven superior by using public\ndatasets in eight evaluation criteria, showing its great potential in\noverlapping chromosome segmentation\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 06:04:22 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 05:44:18 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Mei", "Liye", ""], ["Yu", "Yalan", ""], ["Weng", "Yueyun", ""], ["Guo", "Xiaopeng", ""], ["Liu", "Yan", ""], ["Wang", "Du", ""], ["Liu", "Sheng", ""], ["Zhou", "Fuling", ""], ["Lei", "Cheng", ""]]}, {"id": "2012.11851", "submitter": "Jun Ikeda", "authors": "Jun Ikeda, Hiroyuki Seshime, Xueting Wang and Toshihiko Yamasaki", "title": "Predicting Online Video Advertising Effects with Multimodal Deep\n  Learning", "comments": "Accepted at International Conference on Pattern Recognition 2020\n  (ICPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With expansion of the video advertising market, research to predict the\neffects of video advertising is getting more attention. Although effect\nprediction of image advertising has been explored a lot, prediction for video\nadvertising is still challenging with seldom research. In this research, we\npropose a method for predicting the click through rate (CTR) of video\nadvertisements and analyzing the factors that determine the CTR. In this paper,\nwe demonstrate an optimized framework for accurately predicting the effects by\ntaking advantage of the multimodal nature of online video advertisements\nincluding video, text, and metadata features. In particular, the two types of\nmetadata, i.e., categorical and continuous, are properly separated and\nnormalized. To avoid overfitting, which is crucial in our task because the\ntraining data are not very rich, additional regularization layers are inserted.\nExperimental results show that our approach can achieve a correlation\ncoefficient as high as 0.695, which is a significant improvement from the\nbaseline (0.487).\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 06:24:01 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Ikeda", "Jun", ""], ["Seshime", "Hiroyuki", ""], ["Wang", "Xueting", ""], ["Yamasaki", "Toshihiko", ""]]}, {"id": "2012.11856", "submitter": "Xianxu Hou", "authors": "Xianxu Hou, Xiaokang Zhang, Linlin Shen, Zhihui Lai, Jun Wan", "title": "GuidedStyle: Attribute Knowledge Guided Style Manipulation for Semantic\n  Face Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although significant progress has been made in synthesizing high-quality and\nvisually realistic face images by unconditional Generative Adversarial Networks\n(GANs), there still lacks of control over the generation process in order to\nachieve semantic face editing. In addition, it remains very challenging to\nmaintain other face information untouched while editing the target attributes.\nIn this paper, we propose a novel learning framework, called GuidedStyle, to\nachieve semantic face editing on StyleGAN by guiding the image generation\nprocess with a knowledge network. Furthermore, we allow an attention mechanism\nin StyleGAN generator to adaptively select a single layer for style\nmanipulation. As a result, our method is able to perform disentangled and\ncontrollable edits along various attributes, including smiling, eyeglasses,\ngender, mustache and hair color. Both qualitative and quantitative results\ndemonstrate the superiority of our method over other competing methods for\nsemantic face editing. Moreover, we show that our model can be also applied to\ndifferent types of real and artistic face editing, demonstrating strong\ngeneralization ability.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 06:53:31 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Hou", "Xianxu", ""], ["Zhang", "Xiaokang", ""], ["Shen", "Linlin", ""], ["Lai", "Zhihui", ""], ["Wan", "Jun", ""]]}, {"id": "2012.11860", "submitter": "Aksh Garg", "authors": "Aksh Garg, Sana Salehi, Marianna La Rocca, Rachael Garner, and\n  Dominique Duncan", "title": "Efficient and Visualizable Convolutional Neural Networks for COVID-19\n  Classification Using Chest CT", "comments": "35 pages, 4 figures, 5 Tables. Paper in review by Expert Systems with\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With COVID-19 cases rising rapidly, deep learning has emerged as a promising\ndiagnosis technique. However, identifying the most accurate models to\ncharacterize COVID-19 patients is challenging because comparing results\nobtained with different types of data and acquisition processes is non-trivial.\nIn this paper we designed, evaluated, and compared the performance of 20\nconvolutional neutral networks in classifying patients as COVID-19 positive,\nhealthy, or suffering from other pulmonary lung infections based on Chest CT\nscans, serving as the first to consider the EfficientNet family for COVID-19\ndiagnosis and employ intermediate activation maps for visualizing model\nperformance. All models are trained and evaluated in Python using 4173 Chest CT\nimages from the dataset entitled \"A COVID multiclass dataset of CT scans,\" with\n2168, 758, and 1247 images of patients that are COVID-19 positive, healthy, or\nsuffering from other pulmonary infections, respectively. EfficientNet-B5 was\nidentified as the best model with an F1 score of 0.9769+/-0.0046, accuracy of\n0.9759+/-0.0048, sensitivity of 0.9788+/-0.0055, specificity of\n0.9730+/-0.0057, and precision of 0.9751 +/- 0.0051. On an alternate 2-class\ndataset, EfficientNetB5 obtained an accuracy of 0.9845+/-0.0109, F1 score of\n0.9599+/-0.0251, sensitivity of 0.9682+/-0.0099, specificity of\n0.9883+/-0.0150, and precision of 0.9526 +/- 0.0523. Intermediate activation\nmaps and Gradient-weighted Class Activation Mappings offered\nhuman-interpretable evidence of the model's perception of ground-class\nopacities and consolidations, hinting towards a promising use-case of\nartificial intelligence-assisted radiology tools. With a prediction speed of\nunder 0.1 seconds on GPUs and 0.5 seconds on CPUs, our proposed model offers a\nrapid, scalable, and accurate diagnostic for COVID-19.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 07:09:48 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 05:29:28 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Garg", "Aksh", ""], ["Salehi", "Sana", ""], ["La Rocca", "Marianna", ""], ["Garner", "Rachael", ""], ["Duncan", "Dominique", ""]]}, {"id": "2012.11866", "submitter": "Zehua Sun", "authors": "Zehua Sun, Qiuhong Ke, Hossein Rahmani, Mohammed Bennamoun, Gang Wang\n  and Jun Liu", "title": "Human Action Recognition from Various Data Modalities: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Action Recognition (HAR) aims to understand human behavior and assign a\nlabel to each action. It has a wide range of applications, and therefore has\nbeen attracting increasing attention in the field of computer vision. Human\nactions can be represented using various data modalities, such as RGB,\nskeleton, depth, infrared, point cloud, event stream, audio, acceleration,\nradar, and WiFi signal, which encode different sources of useful yet distinct\ninformation and have various advantages depending on the application scenarios.\nConsequently, lots of existing works have attempted to investigate different\ntypes of approaches for HAR using various modalities. In this paper, we present\na comprehensive survey of recent progress in deep learning methods for HAR\nbased on the type of input data modality. Specifically, we review the current\nmainstream deep learning methods for single data modalities and multiple data\nmodalities, including the fusion-based and the co-learning-based frameworks. We\nalso present comparative results on several benchmark datasets for HAR,\ntogether with insightful observations and inspiring future research directions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 07:37:43 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 05:34:43 GMT"}, {"version": "v3", "created": "Fri, 29 Jan 2021 12:13:25 GMT"}, {"version": "v4", "created": "Fri, 23 Jul 2021 15:30:59 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Sun", "Zehua", ""], ["Ke", "Qiuhong", ""], ["Rahmani", "Hossein", ""], ["Bennamoun", "Mohammed", ""], ["Wang", "Gang", ""], ["Liu", "Jun", ""]]}, {"id": "2012.11879", "submitter": "Xi Li", "authors": "Zequn Qin, Pengyi Zhang, Fei Wu and Xi Li", "title": "FcaNet: Frequency Channel Attention Networks", "comments": "ICCV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanism, especially channel attention, has gained great success\nin the computer vision field. Many works focus on how to design efficient\nchannel attention mechanisms while ignoring a fundamental problem, i.e.,\nchannel attention mechanism uses scalar to represent channel, which is\ndifficult due to massive information loss. In this work, we start from a\ndifferent view and regard the channel representation problem as a compression\nprocess using frequency analysis. Based on the frequency analysis, we\nmathematically prove that the conventional global average pooling is a special\ncase of the feature decomposition in the frequency domain. With the proof, we\nnaturally generalize the compression of the channel attention mechanism in the\nfrequency domain and propose our method with multi-spectral channel attention,\ntermed as FcaNet. FcaNet is simple but effective. We can change a few lines of\ncode in the calculation to implement our method within existing channel\nattention methods. Moreover, the proposed method achieves state-of-the-art\nresults compared with other channel attention methods on image classification,\nobject detection, and instance segmentation tasks. Our method could\nconsistently outperform the baseline SENet, with the same number of parameters\nand the same computational cost. Our code and models will are publicly\navailable at https://github.com/cfzd/FcaNet.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 08:33:27 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 11:11:59 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 02:42:04 GMT"}, {"version": "v4", "created": "Fri, 23 Jul 2021 07:03:52 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Qin", "Zequn", ""], ["Zhang", "Pengyi", ""], ["Wu", "Fei", ""], ["Li", "Xi", ""]]}, {"id": "2012.11892", "submitter": "Aydogan Ozcan", "authors": "Xilin Yang, Luzhe Huang, Yilin Luo, Yichen Wu, Hongda Wang, Yair\n  Rivenson, and Aydogan Ozcan", "title": "Deep learning-based virtual refocusing of images using an engineered\n  point-spread function", "comments": "7 Pages, 3 Figures, 1 Table", "journal-ref": "ACS Photonics (2021)", "doi": "10.1021/acsphotonics.1c00660", "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a virtual image refocusing method over an extended depth of field\n(DOF) enabled by cascaded neural networks and a double-helix point-spread\nfunction (DH-PSF). This network model, referred to as W-Net, is composed of two\ncascaded generator and discriminator network pairs. The first generator network\nlearns to virtually refocus an input image onto a user-defined plane, while the\nsecond generator learns to perform a cross-modality image transformation,\nimproving the lateral resolution of the output image. Using this W-Net model\nwith DH-PSF engineering, we extend the DOF of a fluorescence microscope by\n~20-fold. This approach can be applied to develop deep learning-enabled image\nreconstruction methods for localization microscopy techniques that utilize\nengineered PSFs to improve their imaging performance, including spatial\nresolution and volumetric imaging throughput.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 09:15:26 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Yang", "Xilin", ""], ["Huang", "Luzhe", ""], ["Luo", "Yilin", ""], ["Wu", "Yichen", ""], ["Wang", "Hongda", ""], ["Rivenson", "Yair", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "2012.11905", "submitter": "Silvan Mertes", "authors": "Silvan Mertes, Tobias Huber, Katharina Weitz, Alexander Heimerl,\n  Elisabeth Andr\\'e", "title": "This is not the Texture you are looking for! Introducing Novel\n  Counterfactual Explanations for Non-Experts using Generative Adversarial\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.HC cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the ongoing rise of machine learning, the need for methods for\nexplaining decisions made by artificial intelligence systems is becoming a more\nand more important topic. Especially for image classification tasks, many\nstate-of-the-art tools to explain such classifiers rely on visual highlighting\nof important areas of the input data. Contrary, counterfactual explanation\nsystems try to enable a counterfactual reasoning by modifying the input image\nin a way such that the classifier would have made a different prediction. By\ndoing so, the users of counterfactual explanation systems are equipped with a\ncompletely different kind of explanatory information. However, methods for\ngenerating realistic counterfactual explanations for image classifiers are\nstill rare. In this work, we present a novel approach to generate such\ncounterfactual image explanations based on adversarial image-to-image\ntranslation techniques. Additionally, we conduct a user study to evaluate our\napproach in a use case which was inspired by a healthcare scenario. Our results\nshow that our approach leads to significantly better results regarding mental\nmodels, explanation satisfaction, trust, emotions, and self-efficacy than two\nstate-of-the art systems that work with saliency maps, namely LIME and LRP.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 10:08:05 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Mertes", "Silvan", ""], ["Huber", "Tobias", ""], ["Weitz", "Katharina", ""], ["Heimerl", "Alexander", ""], ["Andr\u00e9", "Elisabeth", ""]]}, {"id": "2012.11911", "submitter": "Tahira Iqbal", "authors": "Tahira Iqbal, Arslan Shaukat, Usman Akram, Zartasha Mustansar and\n  Yung-Cheol Byun", "title": "A Hybrid VDV Model for Automatic Diagnosis of Pneumothorax using\n  Class-Imbalanced Chest X-rays Dataset", "comments": "21 pages, 4 figures, 12 Tables, TO BE PUBLISHED", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pneumothorax, a life threatening disease, needs to be diagnosed immediately\nand efficiently. The prognosis in this case is not only time consuming but also\nprone to human errors. So an automatic way of accurate diagnosis using chest\nX-rays is the utmost requirement. To-date, most of the available medical images\ndatasets have class-imbalance issue. The main theme of this study is to solve\nthis problem along with proposing an automated way of detecting pneumothorax.\nWe first compare the existing approaches to tackle the class-imbalance issue\nand find that data-level-ensemble (i.e. ensemble of subsets of dataset)\noutperforms other approaches. Thus, we propose a novel framework named as VDV\nmodel, which is a complex model-level-ensemble of data-level-ensembles and uses\nthree convolutional neural networks (CNN) including VGG16, VGG-19 and\nDenseNet-121 as fixed feature extractors. In each data-level-ensemble features\nextracted from one of the pre-defined CNN are fed to support vector machine\n(SVM) classifier, and output from each data-level-ensemble is calculated using\nvoting method. Once outputs from the three data-level-ensembles with three\ndifferent CNN architectures are obtained, then, again, voting method is used to\ncalculate the final prediction. Our proposed framework is tested on SIIM ACR\nPneumothorax dataset and Random Sample of NIH Chest X-ray dataset (RS-NIH). For\nthe first dataset, 85.17% Recall with 86.0% Area under the Receiver Operating\nCharacteristic curve (AUC) is attained. For the second dataset, 90.9% Recall\nwith 95.0% AUC is achieved with random split of data while 85.45% recall with\n77.06% AUC is obtained with patient-wise split of data. For RS-NIH, the\nobtained results are higher as compared to previous results from literature\nHowever, for first dataset, direct comparison cannot be made, since this\ndataset has not been used earlier for Pneumothorax classification.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 10:20:04 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Iqbal", "Tahira", ""], ["Shaukat", "Arslan", ""], ["Akram", "Usman", ""], ["Mustansar", "Zartasha", ""], ["Byun", "Yung-Cheol", ""]]}, {"id": "2012.11938", "submitter": "Weitong Hua", "authors": "Weitong Hua, Jiaxin Guo, Yue Wang and Rong Xiong", "title": "3D Point-to-Keypoint Voting Network for 6D Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object 6D pose estimation is an important research topic in the field of\ncomputer vision due to its wide application requirements and the challenges\nbrought by complexity and changes in the real-world. We think fully exploring\nthe characteristics of spatial relationship between points will help to improve\nthe pose estimation performance, especially in the scenes of background clutter\nand partial occlusion. But this information was usually ignored in previous\nwork using RGB image or RGB-D data. In this paper, we propose a framework for\n6D pose estimation from RGB-D data based on spatial structure characteristics\nof 3D keypoints. We adopt point-wise dense feature embedding to vote for 3D\nkeypoints, which makes full use of the structure information of the rigid body.\nAfter the direction vectors pointing to the keypoints are predicted by CNN, we\nuse RANSAC voting to calculate the coordinate of the 3D keypoints, then the\npose transformation can be easily obtained by the least square method. In\naddition, a spatial dimension sampling strategy for points is employed, which\nmakes the method achieve excellent performance on small training sets. The\nproposed method is verified on two benchmark datasets, LINEMOD and OCCLUSION\nLINEMOD. The experimental results show that our method outperforms the\nstate-of-the-art approaches, achieves ADD(-S) accuracy of 98.7\\% on LINEMOD\ndataset and 52.6\\% on OCCLUSION LINEMOD dataset in real-time.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 11:43:15 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Hua", "Weitong", ""], ["Guo", "Jiaxin", ""], ["Wang", "Yue", ""], ["Xiong", "Rong", ""]]}, {"id": "2012.11952", "submitter": "Maheshi Dissanayake", "authors": "Shanaka Ramesh Gunasekara, HNTK Kaldera, Maheshi B. Dissanayake", "title": "A Feasibility study for Deep learning based automated brain tumor\n  segmentation using Magnetic Resonance Images", "comments": "submitted to the JRTE journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning algorithms have accounted for the rapid acceleration of\nresearch in artificial intelligence in medical image analysis, interpretation,\nand segmentation with many potential applications across various sub\ndisciplines in medicine. However, only limited number of research which\ninvestigates these application scenarios, are deployed into the clinical sector\nfor the evaluation of the real requirement and the practical challenges of the\nmodel deployment. In this research, a deep convolutional neural network (CNN)\nbased classification network and Faster RCNN based localization network were\ndeveloped for brain tumor MR image classification and tumor localization. A\ntypical edge detection algorithm called Prewitt was used for tumor segmentation\ntask, based on the output of the tumor localization. Overall performance of the\nproposed tumor segmentation architecture, was analyzed using objective quality\nparameters including Accuracy, Boundary Displacement Error (BDE), Dice score\nand confidence interval. A subjective quality assessment of the model was\nconducted based on the Double Stimulus Impairment Scale (DSIS) protocol using\nthe input of medical expertise. It was observed that the confidence level of\nour segmented output was in a similar range to that of experts. Also, the\nNeurologists have rated the output of our model as highly accurate\nsegmentation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 12:11:42 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Gunasekara", "Shanaka Ramesh", ""], ["Kaldera", "HNTK", ""], ["Dissanayake", "Maheshi B.", ""]]}, {"id": "2012.12009", "submitter": "U\\u{g}ur \\c{C}o\\u{g}alan", "authors": "U\\u{g}ur \\c{C}o\\u{g}alan, Mojtaba Bemana, Karol Myszkowski, Hans-Peter\n  Seidel, Tobias Ritschel", "title": "HDR Denoising and Deblurring by Learning Spatio-temporal Distortion\n  Models", "comments": "13 pages, 9 figures, project webpage: http://deephdr.mpi-inf.mpg.de/", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to reconstruct sharp and noise-free high-dynamic range (HDR) video\nfrom a dual-exposure sensor that records different low-dynamic range (LDR)\ninformation in different pixel columns: Odd columns provide low-exposure,\nsharp, but noisy information; even columns complement this with less noisy,\nhigh-exposure, but motion-blurred data. Previous LDR work learns to deblur and\ndenoise (DISTORTED->CLEAN) supervised by pairs of CLEAN and DISTORTED images.\nRegrettably, capturing DISTORTED sensor readings is time-consuming; as well,\nthere is a lack of CLEAN HDR videos. We suggest a method to overcome those two\nlimitations. First, we learn a different function instead: CLEAN->DISTORTED,\nwhich generates samples containing correlated pixel noise, and row and column\nnoise, as well as motion blur from a low number of CLEAN sensor readings.\nSecond, as there is not enough CLEAN HDR video available, we devise a method to\nlearn from LDR video in-stead. Our approach compares favorably to several\nstrong baselines, and can boost existing methods when they are re-trained on\nour data. Combined with spatial and temporal super-resolution, it enables\napplications such as re-lighting with low noise or blur.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 13:53:26 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 10:14:56 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 13:06:49 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["\u00c7o\u011falan", "U\u011fur", ""], ["Bemana", "Mojtaba", ""], ["Myszkowski", "Karol", ""], ["Seidel", "Hans-Peter", ""], ["Ritschel", "Tobias", ""]]}, {"id": "2012.12012", "submitter": "Yi Ding", "authors": "Yi Ding, Qiqi Yang, Guozheng Wu, Jian Zhang, Zhiguang Qin", "title": "Multiple Instance Segmentation in Brachial Plexus Ultrasound Image Using\n  BPMSegNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of nerve is difficult as structures of nerves are\nchallenging to image and to detect in ultrasound images. Nevertheless, the\nnerve identification in ultrasound images is a crucial step to improve\nperformance of regional anesthesia. In this paper, a network called Brachial\nPlexus Multi-instance Segmentation Network (BPMSegNet) is proposed to identify\ndifferent tissues (nerves, arteries, veins, muscles) in ultrasound images. The\nBPMSegNet has three novel modules. The first is the spatial local contrast\nfeature, which computes contrast features at different scales. The second one\nis the self-attention gate, which reweighs the channels in feature maps by\ntheir importance. The third is the addition of a skip concatenation with\ntransposed convolution within a feature pyramid network. The proposed BPMSegNet\nis evaluated by conducting experiments on our constructed Ultrasound Brachial\nPlexus Dataset (UBPD). Quantitative experimental results show the proposed\nnetwork can segment multiple tissues from the ultrasound images with a good\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 13:57:30 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Ding", "Yi", ""], ["Yang", "Qiqi", ""], ["Wu", "Guozheng", ""], ["Zhang", "Jian", ""], ["Qin", "Zhiguang", ""]]}, {"id": "2012.12014", "submitter": "Ron Ferens", "authors": "Yoli Shavit and Ron Ferens", "title": "Do We Really Need Scene-specific Pose Encoders?", "comments": "To be presented at ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Visual pose regression models estimate the camera pose from a query image\nwith a single forward pass. Current models learn pose encoding from an image\nusing deep convolutional networks which are trained per scene. The resulting\nencoding is typically passed to a multi-layer perceptron in order to regress\nthe pose. In this work, we propose that scene-specific pose encoders are not\nrequired for pose regression and that encodings trained for visual similarity\ncan be used instead. In order to test our hypothesis, we take a shallow\narchitecture of several fully connected layers and train it with pre-computed\nencodings from a generic image retrieval model. We find that these encodings\nare not only sufficient to regress the camera pose, but that, when provided to\na branching fully connected architecture, a trained model can achieve\ncompetitive results and even surpass current \\textit{state-of-the-art} pose\nregressors in some cases. Moreover, we show that for outdoor localization, the\nproposed architecture is the only pose regressor, to date, consistently\nlocalizing in under 2 meters and 5 degrees.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 13:59:52 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Shavit", "Yoli", ""], ["Ferens", "Ron", ""]]}, {"id": "2012.12071", "submitter": "Ho Yin Chau", "authors": "Ho Yin Chau, Frank Qiu, Yubei Chen, Bruno Olshausen", "title": "Disentangling images with Lie group transformations and sparse coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete spatial patterns and their continuous transformations are two\nimportant regularities contained in natural signals. Lie groups and\nrepresentation theory are mathematical tools that have been used in previous\nworks to model continuous image transformations. On the other hand, sparse\ncoding is an important tool for learning dictionaries of patterns in natural\nsignals. In this paper, we combine these ideas in a Bayesian generative model\nthat learns to disentangle spatial patterns and their continuous\ntransformations in a completely unsupervised manner. Images are modeled as a\nsparse superposition of shape components followed by a transformation that is\nparameterized by n continuous variables. The shape components and\ntransformations are not predefined, but are instead adapted to learn the\nsymmetries in the data, with the constraint that the transformations form a\nrepresentation of an n-dimensional torus. Training the model on a dataset\nconsisting of controlled geometric transformations of specific MNIST digits\nshows that it can recover these transformations along with the digits. Training\non the full MNIST dataset shows that it can learn both the basic digit shapes\nand the natural transformations such as shearing and stretching that are\ncontained in this data.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 19:11:32 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Chau", "Ho Yin", ""], ["Qiu", "Frank", ""], ["Chen", "Yubei", ""], ["Olshausen", "Bruno", ""]]}, {"id": "2012.12084", "submitter": "Ye Chen", "authors": "Ye Chen and Yuankai Huo", "title": "Limitation of Acyclic Oriented Graphs Matching as Cell Tracking Accuracy\n  Measure when Evaluating Mitosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-object tracking (MOT) in computer vision and cell tracking in\nbiomedical image analysis are two similar research fields, whose common aim is\nto achieve instance level object detection/segmentation and associate such\nobjects across different video frames. However, one major difference between\nthese two tasks is that cell tracking also aim to detect mitosis (cell\ndivision), which is typically not considered in MOT tasks. Therefore, the\nacyclic oriented graphs matching (AOGM) has been used as de facto standard\nevaluation metrics for cell tracking, rather than directly using the evaluation\nmetrics in computer vision, such as multiple object tracking accuracy (MOTA),\nID Switches (IDS), ID F1 Score (IDF1) etc. However, based on our experiments,\nwe realized that AOGM did not always function as expected for mitosis events.\nIn this paper, we exhibit the limitations of evaluating mitosis with AOGM using\nboth simulated and real cell tracking data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 15:25:47 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Chen", "Ye", ""], ["Huo", "Yuankai", ""]]}, {"id": "2012.12086", "submitter": "Ying Yang", "authors": "Yubao Sun, Ying Yang, Qingshan Liu, Mohan Kankanhalli", "title": "Unsupervised Spatial-spectral Network Learning for Hyperspectral\n  Compressive Snapshot Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral compressive imaging takes advantage of compressive sensing\ntheory to achieve coded aperture snapshot measurement without temporal\nscanning, and the entire three-dimensional spatial-spectral data is captured by\na two-dimensional projection during a single integration period. Its core issue\nis how to reconstruct the underlying hyperspectral image using compressive\nsensing reconstruction algorithms. Due to the diversity in the spectral\nresponse characteristics and wavelength range of different spectral imaging\ndevices, previous works are often inadequate to capture complex spectral\nvariations or lack the adaptive capacity to new hyperspectral imagers. In order\nto address these issues, we propose an unsupervised spatial-spectral network to\nreconstruct hyperspectral images only from the compressive snapshot\nmeasurement. The proposed network acts as a conditional generative model\nconditioned on the snapshot measurement, and it exploits the spatial-spectral\nattention module to capture the joint spatial-spectral correlation of\nhyperspectral images. The network parameters are optimized to make sure that\nthe network output can closely match the given snapshot measurement according\nto the imaging model, thus the proposed network can adapt to different imaging\nsettings, which can inherently enhance the applicability of the network.\nExtensive experiments upon multiple datasets demonstrate that our network can\nachieve better reconstruction results than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 12:29:04 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Sun", "Yubao", ""], ["Yang", "Ying", ""], ["Liu", "Qingshan", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "2012.12089", "submitter": "Ali Dauda Baba", "authors": "Iliyas Ibrahim Iliyas, Isah Rambo Saidu, Ali Baba Dauda, Suleiman\n  Tasiu", "title": "Prediction of Chronic Kidney Disease Using Deep Neural Network", "comments": "14 paages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural Network (DNN) is becoming a focal point in Machine Learning\nresearch. Its application is penetrating into different fields and solving\nintricate and complex problems. DNN is now been applied in health image\nprocessing to detect various ailment such as cancer and diabetes. Another\ndisease that is causing threat to our health is the kidney disease. This\ndisease is becoming prevalent due to substances and elements we intake. Death\nis imminent and inevitable within few days without at least one functioning\nkidney. Ignoring the kidney malfunction can cause chronic kidney disease\nleading to death. Frequently, Chronic Kidney Disease (CKD) and its symptoms are\nmild and gradual, often go unnoticed for years only to be realized lately.\nBade, a Local Government of Yobe state in Nigeria has been a center of\nattention by medical practitioners due to the prevalence of CKD. Unfortunately,\na technical approach in culminating the disease is yet to be attained. We\nobtained a record of 400 patients with 10 attributes as our dataset from Bade\nGeneral Hospital. We used DNN model to predict the absence or presence of CKD\nin the patients. The model produced an accuracy of 98%. Furthermore, we\nidentified and highlighted the Features importance to provide the ranking of\nthe features used in the prediction of the CKD. The outcome revealed that two\nattributes; Creatinine and Bicarbonate have the highest influence on the CKD\nprediction.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 15:31:14 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Iliyas", "Iliyas Ibrahim", ""], ["Saidu", "Isah Rambo", ""], ["Dauda", "Ali Baba", ""], ["Tasiu", "Suleiman", ""]]}, {"id": "2012.12101", "submitter": "Gustau Camps-Valls", "authors": "Aleksandra Wolanin, Gustau Camps-Valls, Luis G\\'omez-Chova, Gonzalo\n  Mateo-Garc\\'ia, Christiaan van der Tol, Yongguang Zhang, Luis Guanter", "title": "Estimating Crop Primary Productivity with Sentinel-2 and Landsat 8 using\n  Machine Learning Methods Trained with Radiative Transfer Simulations", "comments": null, "journal-ref": null, "doi": "10.1016/j.rse.2019.03.002", "report-no": "Preprint of paper published in Remote Sensing of Environment Volume\n  225, May 2019, Pages 441-457", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Satellite remote sensing has been widely used in the last decades for\nagricultural applications, {both for assessing vegetation condition and for\nsubsequent yield prediction.} Existing remote sensing-based methods to estimate\ngross primary productivity (GPP), which is an important variable to indicate\ncrop photosynthetic function and stress, typically rely on empirical or\nsemi-empirical approaches, which tend to over-simplify photosynthetic\nmechanisms. In this work, we take advantage of all parallel developments in\nmechanistic photosynthesis modeling and satellite data availability for\nadvanced monitoring of crop productivity. In particular, we combine\nprocess-based modeling with the soil-canopy energy balance radiative transfer\nmodel (SCOPE) with Sentinel-2 {and Landsat 8} optical remote sensing data and\nmachine learning methods in order to estimate crop GPP. Our model successfully\nestimates GPP across a variety of C3 crop types and environmental conditions\neven though it does not use any local information from the corresponding sites.\nThis highlights its potential to map crop productivity from new satellite\nsensors at a global scale with the help of current Earth observation cloud\ncomputing platforms.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 16:23:13 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Wolanin", "Aleksandra", ""], ["Camps-Valls", "Gustau", ""], ["G\u00f3mez-Chova", "Luis", ""], ["Mateo-Garc\u00eda", "Gonzalo", ""], ["van der Tol", "Christiaan", ""], ["Zhang", "Yongguang", ""], ["Guanter", "Luis", ""]]}, {"id": "2012.12102", "submitter": "Chul Moon", "authors": "Chul Moon, Qiwei Li, Guanghua Xiao", "title": "Using Persistent Homology Topological Features to Characterize Medical\n  Images: Case Studies on Lung and Brain Cancers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Tumor shape is a key factor that affects tumor growth and metastasis. This\npaper proposes a topological feature computed by persistent homology to\ncharacterize tumor progression from digital pathology and radiology images and\nexamines its effect on the time-to-event data. The proposed topological\nfeatures are invariant to scale-preserving transformation and can summarize\nvarious tumor shape patterns. The topological features are represented in\nfunctional space and used as functional predictors in a functional Cox\nproportional hazards model. The proposed model enables interpretable inference\nabout the association between topological shape features and survival risks.\nTwo case studies are conducted using consecutive 143 lung cancer and 77 brain\ntumor patients. The results of both studies show that the topological features\npredict survival prognosis after adjusting clinical variables, and the\npredicted high-risk groups have significantly (at the level of 0.01) worse\nsurvival outcomes than the low-risk groups. Also, the topological shape\nfeatures found to be positively associated with survival hazards are irregular\nand heterogeneous shape patterns, which are known to be related to tumor\nprogression.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:16:59 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 18:48:40 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Moon", "Chul", ""], ["Li", "Qiwei", ""], ["Xiao", "Guanghua", ""]]}, {"id": "2012.12104", "submitter": "Bing Liu", "authors": "Bing Liu (1), Yu Tang (2), Yuxiong Ji (1), Yu Shen (1), and Yuchuan Du\n  (1) ((1) Key Laboratory of Road and Traffic Engineering of the Ministry of\n  Education, Tongji University, Shanghai, China, (2) Tandon School of\n  Engineering, New York University, New York, USA)", "title": "A Deep Reinforcement Learning Approach for Ramp Metering Based on\n  Traffic Video Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ramp metering that uses traffic signals to regulate vehicle flows from the\non-ramps has been widely implemented to improve vehicle mobility of the\nfreeway. Previous studies generally update signal timings in real-time based on\npredefined traffic measures collected by point detectors, such as traffic\nvolumes and occupancies. Comparing with point detectors, traffic cameras-which\nhave been increasingly deployed on road networks-could cover larger areas and\nprovide more detailed traffic information. In this work, we propose a deep\nreinforcement learning (DRL) method to explore the potential of traffic video\ndata in improving the efficiency of ramp metering. The proposed method uses\ntraffic video frames as inputs and learns the optimal control strategies\ndirectly from the high-dimensional visual inputs. A real-world case study\ndemonstrates that, in comparison with a state-of-the-practice method, the\nproposed DRL method results in 1) lower travel times in the mainline, 2)\nshorter vehicle queues at the on-ramp, and 3) higher traffic flows downstream\nof the merging area. The results suggest that the proposed method is able to\nextract useful information from the video data for better ramp metering\ncontrols.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 05:08:41 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Liu", "Bing", ""], ["Tang", "Yu", ""], ["Ji", "Yuxiong", ""], ["Shen", "Yu", ""], ["Du", "Yuchuan", ""]]}, {"id": "2012.12105", "submitter": "Adrian Perez-Suay", "authors": "Anna Mateo-Sanchis, Jordi Mu\\~noz-Mar\\'i, Adri\\'an P\\'erez-Suay,\n  Gustau Camps-Valls", "title": "Warped Gaussian Processes in Remote Sensing Parameter Estimation and\n  Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper introduces warped Gaussian processes (WGP) regression in remote\nsensing applications. WGP models output observations as a parametric nonlinear\ntransformation of a GP. The parameters of such prior model are then learned via\nstandard maximum likelihood. We show the good performance of the proposed model\nfor the estimation of oceanic chlorophyll content from multispectral data,\nvegetation parameters (chlorophyll, leaf area index, and fractional vegetation\ncover) from hyperspectral data, and in the detection of the causal direction in\na collection of 28 bivariate geoscience and remote sensing causal problems. The\nmodel consistently performs better than the standard GP and the more advanced\nheteroscedastic GP model, both in terms of accuracy and more sensible\nconfidence intervals.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 09:02:59 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Mateo-Sanchis", "Anna", ""], ["Mu\u00f1oz-Mar\u00ed", "Jordi", ""], ["P\u00e9rez-Suay", "Adri\u00e1n", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.12108", "submitter": "Barbara Benato", "authors": "Barbara C. Benato and Italos E. de Souza and Felipe L. Galv\\~ao and\n  Alexandre X. Falc\\~ao", "title": "Convolutional Neural Networks from Image Markers", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A technique named Feature Learning from Image Markers (FLIM) was recently\nproposed to estimate convolutional filters, with no backpropagation, from\nstrokes drawn by a user on very few images (e.g., 1-3) per class, and\ndemonstrated for coconut-tree image classification. This paper extends FLIM for\nfully connected layers and demonstrates it on different image classification\nproblems. The work evaluates marker selection from multiple users and the\nimpact of adding a fully connected layer. The results show that FLIM-based\nconvolutional neural networks can outperform the same architecture trained from\nscratch by backpropagation.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 22:58:23 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Benato", "Barbara C.", ""], ["de Souza", "Italos E.", ""], ["Galv\u00e3o", "Felipe L.", ""], ["Falc\u00e3o", "Alexandre X.", ""]]}, {"id": "2012.12109", "submitter": "Menghan Xia", "authors": "Menghan Xia and Yi Wang and Chu Han and Tien-Tsin Wong", "title": "Enhance Convolutional Neural Networks with Noise Incentive Block", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a generic modeling tool, Convolutional Neural Networks (CNNs) have been\nwidely employed in image generation and translation tasks. However, when fed\nwith a flat input, current CNN models may fail to generate vivid results due to\nthe spatially shared convolution kernels. We call it the flatness degradation\nof CNNs. Unfortunately, such degradation is the greatest obstacles to generate\na spatially-variant output from a flat input, which has been barely discussed\nin the previous literature. To tackle this problem, we propose a model agnostic\nsolution, i.e. Noise Incentive Block (NIB), which serves as a generic plug-in\nfor any CNN generation model. The key idea is to break the flat input condition\nwhile keeping the intactness of the original information. Specifically, the NIB\nperturbs the input data symmetrically with a noise map and reassembles them in\nthe feature domain as driven by the objective function. Extensive experiments\nshow that existing CNN models equipped with NIB survive from the flatness\ndegradation and are able to generate visually better results with richer\ndetails in some specific image generation tasks given flat inputs, e.g.\nsemantic image synthesis, data-hidden image generation, and deep neural\ndithering.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 09:01:45 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 06:24:05 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Xia", "Menghan", ""], ["Wang", "Yi", ""], ["Han", "Chu", ""], ["Wong", "Tien-Tsin", ""]]}, {"id": "2012.12111", "submitter": "Fabio Valerio Massoli", "authors": "Fabio Valerio Massoli, Fabrizio Falchi, Alperen Kantarci, \\c{S}eymanur\n  Akti, Hazim Kemal Ekenel, Giuseppe Amato", "title": "MOCCA: Multi-Layer One-Class ClassificAtion for Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Anomalies are ubiquitous in all scientific fields and can express an\nunexpected event due to incomplete knowledge about the data distribution or an\nunknown process that suddenly comes into play and distorts the observations.\nDue to such events' rarity, it is common to train deep learning models on\n\"normal\", i.e. non-anomalous, datasets only, thus letting the neural network to\nmodel the distribution beneath the input data. In this context, we propose our\ndeep learning approach to the anomaly detection problem named\nMulti-LayerOne-Class Classification (MOCCA). We explicitly leverage the\npiece-wise nature of deep neural networks by exploiting information extracted\nat different depths to detect abnormal data instances. We show how combining\nthe representations extracted from multiple layers of a model leads to higher\ndiscrimination performance than typical approaches proposed in the literature\nthat are based neural networks' final output only. We propose to train the\nmodel by minimizing the $L_2$ distance between the input representation and a\nreference point, the anomaly-free training data centroid, at each considered\nlayer. We conduct extensive experiments on publicly available datasets for\nanomaly detection, namely CIFAR10, MVTec AD, and ShanghaiTech, considering both\nthe single-image and video-based scenarios. We show that our method reaches\nsuperior performances compared to the state-of-the-art approaches available in\nthe literature. Moreover, we provide a model analysis to give insight on how\nour approach works.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 08:32:56 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 09:40:17 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Massoli", "Fabio Valerio", ""], ["Falchi", "Fabrizio", ""], ["Kantarci", "Alperen", ""], ["Akti", "\u015eeymanur", ""], ["Ekenel", "Hazim Kemal", ""], ["Amato", "Giuseppe", ""]]}, {"id": "2012.12115", "submitter": "Joao Leite", "authors": "Manuel de Sousa Ribeiro, Ludwig Krippahl, Joao Leite", "title": "Explainable Abstract Trains Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Explainable Abstract Trains Dataset is an image dataset containing\nsimplified representations of trains. It aims to provide a platform for the\napplication and research of algorithms for justification and explanation\nextraction. The dataset is accompanied by an ontology that conceptualizes and\nclassifies the depicted trains based on their visual characteristics, allowing\nfor a precise understanding of how each train was labeled. Each image in the\ndataset is annotated with multiple attributes describing the trains' features\nand with bounding boxes for the train elements.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 13:18:42 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Ribeiro", "Manuel de Sousa", ""], ["Krippahl", "Ludwig", ""], ["Leite", "Joao", ""]]}, {"id": "2012.12125", "submitter": "Aleksei Shpilman", "authors": "Aleksei Shpilman, Dmitry Boikiy, Marina Polyakova, Daniel Kudenko,\n  Anton Burakov and Elena Nadezhdina", "title": "Deep Learning of Cell Classification using Microscope Images of\n  Intracellular Microtubule Networks", "comments": null, "journal-ref": null, "doi": "10.1109/ICMLA.2017.0-186", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Microtubule networks (MTs) are a component of a cell that may indicate the\npresence of various chemical compounds and can be used to recognize properties\nsuch as treatment resistance. Therefore, the classification of MT images is of\ngreat relevance for cell diagnostics. Human experts find it particularly\ndifficult to recognize the levels of chemical compound exposure of a cell.\nImproving the accuracy with automated techniques would have a significant\nimpact on cell therapy. In this paper we present the application of Deep\nLearning to MT image classification and evaluate it on a large MT image dataset\nof animal cells with three degrees of exposure to a chemical agent. The results\ndemonstrate that the learned deep network performs on par or better at the\ncorresponding cell classification task than human experts. Specifically, we\nshow that the task of recognizing different levels of chemical agent exposure\ncan be handled significantly better by the neural network than by human\nexperts.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 09:32:18 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Shpilman", "Aleksei", ""], ["Boikiy", "Dmitry", ""], ["Polyakova", "Marina", ""], ["Kudenko", "Daniel", ""], ["Burakov", "Anton", ""], ["Nadezhdina", "Elena", ""]]}, {"id": "2012.12139", "submitter": "Md. Kishor Morol", "authors": "Al Momin Faruk, Hasan Al Faraby, Md. Muzahidul Azad, Md. Riduyan\n  Fedous, Md. Kishor Morol", "title": "Image to Bengali Caption Generation Using Deep CNN and Bidirectional\n  Gated Recurrent Unit", "comments": "Accepted at ICCIT2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is very little notable research on generating descriptions of the\nBengali language. About 243 million people speak in Bengali, and it is the 7th\nmost spoken language on the planet. The purpose of this research is to propose\na CNN and Bidirectional GRU based architecture model that generates natural\nlanguage captions in the Bengali language from an image. Bengali people can use\nthis research to break the language barrier and better understand each other's\nperspectives. It will also help many blind people with their everyday lives.\nThis paper used an encoder-decoder approach to generate captions. We used a\npre-trained Deep convolutional neural network (DCNN) called InceptonV3image\nembedding model as the encoder for analysis, classification, and annotation of\nthe dataset's images Bidirectional Gated Recurrent unit (BGRU) layer as the\ndecoder to generate captions. Argmax and Beam search is used to produce the\nhighest possible quality of the captions. A new dataset called BNATURE is used,\nwhich comprises 8000 images with five captions per image. It is used for\ntraining and testing the proposed model. We obtained BLEU-1, BLEU-2, BLEU-3,\nBLEU-4 and Meteor is 42.6, 27.95, 23, 66, 16.41, 28.7 respectively.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 16:22:02 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Faruk", "Al Momin", ""], ["Faraby", "Hasan Al", ""], ["Azad", "Md. Muzahidul", ""], ["Fedous", "Md. Riduyan", ""], ["Morol", "Md. Kishor", ""]]}, {"id": "2012.12160", "submitter": "Justin Liang", "authors": "Justin Liang, Namdar Homayounfar, Wei-Chiu Ma, Shenlong Wang, Raquel\n  Urtasun", "title": "Convolutional Recurrent Network for Road Boundary Extraction", "comments": null, "journal-ref": "CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating high definition maps that contain precise information of static\nelements of the scene is of utmost importance for enabling self driving cars to\ndrive safely. In this paper, we tackle the problem of drivable road boundary\nextraction from LiDAR and camera imagery. Towards this goal, we design a\nstructured model where a fully convolutional network obtains deep features\nencoding the location and direction of road boundaries and then, a\nconvolutional recurrent network outputs a polyline representation for each one\nof them. Importantly, our method is fully automatic and does not require a user\nin the loop. We showcase the effectiveness of our method on a large North\nAmerican city where we obtain perfect topology of road boundaries 99.3% of the\ntime at a high precision and recall.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 18:59:12 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Liang", "Justin", ""], ["Homayounfar", "Namdar", ""], ["Ma", "Wei-Chiu", ""], ["Wang", "Shenlong", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2012.12175", "submitter": "Gary Huang", "authors": "Gary B Huang and Huei-Fang Yang and Shin-ya Takemura and Pat Rivlin\n  and Stephen M Plaza", "title": "Latent Feature Representation via Unsupervised Learning for Pattern\n  Discovery in Massive Electron Microscopy Image Volumes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to facilitate exploration and analysis of new large data\nsets. In particular, we give an unsupervised deep learning approach to learning\na latent representation that captures semantic similarity in the data set. The\ncore idea is to use data augmentations that preserve semantic meaning to\ngenerate synthetic examples of elements whose feature representations should be\nclose to one another.\n  We demonstrate the utility of our method applied to nano-scale electron\nmicroscopy data, where even relatively small portions of animal brains can\nrequire terabytes of image data. Although supervised methods can be used to\npredict and identify known patterns of interest, the scale of the data makes it\ndifficult to mine and analyze patterns that are not known a priori. We show the\nability of our learned representation to enable query by example, so that if a\nscientist notices an interesting pattern in the data, they can be presented\nwith other locations with matching patterns. We also demonstrate that\nclustering of data in the learned space correlates with biologically-meaningful\ndistinctions. Finally, we introduce a visualization tool and software ecosystem\nto facilitate user-friendly interactive analysis and uncover interesting\nbiological patterns. In short, our work opens possible new avenues in\nunderstanding of and discovery in large data sets, arising in domains such as\nEM analysis.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 17:14:19 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Huang", "Gary B", ""], ["Yang", "Huei-Fang", ""], ["Takemura", "Shin-ya", ""], ["Rivlin", "Pat", ""], ["Plaza", "Stephen M", ""]]}, {"id": "2012.12180", "submitter": "Faramarz Naderi Darbaghshahi", "authors": "Faramarz Naderi Darbaghshahi, Mohammad Reza Mohammadi, Mohsen Soryani", "title": "Cloud removal in remote sensing images using generative adversarial\n  networks and SAR-to-optical image translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satellite images are often contaminated by clouds. Cloud removal has received\nmuch attention due to the wide range of satellite image applications. As the\nclouds thicken, the process of removing the clouds becomes more challenging. In\nsuch cases, using auxiliary images such as near-infrared or synthetic aperture\nradar (SAR) for reconstructing is common. In this study, we attempt to solve\nthe problem using two generative adversarial networks (GANs). The first\ntranslates SAR images into optical images, and the second removes clouds using\nthe translated images of prior GAN. Also, we propose dilated residual inception\nblocks (DRIBs) instead of vanilla U-net in the generator networks and use\nstructural similarity index measure (SSIM) in addition to the L1 Loss function.\nReducing the number of downsamplings and expanding receptive fields by dilated\nconvolutions increase the quality of output images. We used the SEN1-2 dataset\nto train and test both GANs, and we made cloudy images by adding synthetic\nclouds to optical images. The restored images are evaluated with PSNR and SSIM.\nWe compare the proposed method with state-of-the-art deep learning models and\nachieve more accurate results in both SAR-to-optical translation and cloud\nremoval parts.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 17:19:14 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Darbaghshahi", "Faramarz Naderi", ""], ["Mohammadi", "Mohammad Reza", ""], ["Soryani", "Mohsen", ""]]}, {"id": "2012.12188", "submitter": "Guang Yang A", "authors": "Yinzhe Wu, Suzan Hatipoglu, Diego Alonso-\\'Alvarez, Peter Gatehouse,\n  David Firmin, Jennifer Keegan, Guang Yang", "title": "Automated Multi-Channel Segmentation for the 4D Myocardial Velocity\n  Mapping Cardiac MR", "comments": "7 pages, 3 figures, accepted by SPIE Medical Imaging 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Four-dimensional (4D) left ventricular myocardial velocity mapping (MVM) is a\ncardiac magnetic resonance (CMR) technique that allows assessment of cardiac\nmotion in three orthogonal directions. Accurate and reproducible delineation of\nthe myocardium is crucial for accurate analysis of peak systolic and diastolic\nmyocardial velocities. In addition to the conventionally available magnitude\nCMR data, 4D MVM also acquires three velocity-encoded phase datasets which are\nused to generate velocity maps. These can be used to facilitate and improve\nmyocardial delineation. Based on the success of deep learning in medical image\nprocessing, we propose a novel automated framework that improves the standard\nU-Net based methods on these CMR multi-channel data (magnitude and phase) by\ncross-channel fusion with attention module and shape information based\npost-processing to achieve accurate delineation of both epicardium and\nendocardium contours. To evaluate the results, we employ the widely used Dice\nscores and the quantification of myocardial longitudinal peak velocities. Our\nproposed network trained with multi-channel data shows enhanced performance\ncompared to standard U-Net based networks trained with single-channel data.\nBased on the results, our method provides compelling evidence for the design\nand application for the multi-channel image analysis of the 4D MVM CMR data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 16:15:55 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Wu", "Yinzhe", ""], ["Hatipoglu", "Suzan", ""], ["Alonso-\u00c1lvarez", "Diego", ""], ["Gatehouse", "Peter", ""], ["Firmin", "David", ""], ["Keegan", "Jennifer", ""], ["Yang", "Guang", ""]]}, {"id": "2012.12195", "submitter": "Di Feng", "authors": "Di Feng, Zining Wang, Yiyang Zhou, Lars Rosenbaum, Fabian Timm, Klaus\n  Dietmayer, Masayoshi Tomizuka, Wei Zhan", "title": "Labels Are Not Perfect: Inferring Spatial Uncertainty in Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The availability of many real-world driving datasets is a key reason behind\nthe recent progress of object detection algorithms in autonomous driving.\nHowever, there exist ambiguity or even failures in object labels due to\nerror-prone annotation process or sensor observation noise. Current public\nobject detection datasets only provide deterministic object labels without\nconsidering their inherent uncertainty, as does the common training process or\nevaluation metrics for object detectors. As a result, an in-depth evaluation\namong different object detection methods remains challenging, and the training\nprocess of object detectors is sub-optimal, especially in probabilistic object\ndetection. In this work, we infer the uncertainty in bounding box labels from\nLiDAR point clouds based on a generative model, and define a new representation\nof the probabilistic bounding box through a spatial uncertainty distribution.\nComprehensive experiments show that the proposed model reflects complex\nenvironmental noises in LiDAR perception and the label quality. Furthermore, we\npropose Jaccard IoU (JIoU) as a new evaluation metric that extends IoU by\nincorporating label uncertainty. We conduct an in-depth comparison among\nseveral LiDAR-based object detectors using the JIoU metric. Finally, we\nincorporate the proposed label uncertainty in a loss function to train a\nprobabilistic object detector and to improve its detection accuracy. We verify\nour proposed methods on two public datasets (KITTI, Waymo), as well as on\nsimulation data. Code is released at https://bit.ly/2W534yo.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 09:11:44 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Feng", "Di", ""], ["Wang", "Zining", ""], ["Zhou", "Yiyang", ""], ["Rosenbaum", "Lars", ""], ["Timm", "Fabian", ""], ["Dietmayer", "Klaus", ""], ["Tomizuka", "Masayoshi", ""], ["Zhan", "Wei", ""]]}, {"id": "2012.12206", "submitter": "Yichi Zhang", "authors": "Yichi Zhang and Junhao Pan and Xinheng Liu and Hongzheng Chen and\n  Deming Chen and Zhiru Zhang", "title": "FracBNN: Accurate and FPGA-Efficient Binary Neural Networks with\n  Fractional Activations", "comments": "Published at the 29th ACM/SIGDA International Symposium on\n  Field-Programmable Gate Arrays (FPGA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary neural networks (BNNs) have 1-bit weights and activations. Such\nnetworks are well suited for FPGAs, as their dominant computations are bitwise\narithmetic and the memory requirement is also significantly reduced. However,\ncompared to start-of-the-art compact convolutional neural network (CNN) models,\nBNNs tend to produce a much lower accuracy on realistic datasets such as\nImageNet. In addition, the input layer of BNNs has gradually become a major\ncompute bottleneck, because it is conventionally excluded from binarization to\navoid a large accuracy loss. This work proposes FracBNN, which exploits\nfractional activations to substantially improve the accuracy of BNNs.\nSpecifically, our approach employs a dual-precision activation scheme to\ncompute features with up to two bits, using an additional sparse binary\nconvolution. We further binarize the input layer using a novel thermometer\nencoding. Overall, FracBNN preserves the key benefits of conventional BNNs,\nwhere all convolutional layers are computed in pure binary MAC operations\n(BMACs). We design an efficient FPGA-based accelerator for our novel BNN model\nthat supports the fractional activations. To evaluate the performance of\nFracBNN under a resource-constrained scenario, we implement the entire\noptimized network architecture on an embedded FPGA (Xilinx Ultra96v2). Our\nexperiments on ImageNet show that FracBNN achieves an accuracy comparable to\nMobileNetV2, surpassing the best-known BNN design on FPGAs with an increase of\n28.9% in top-1 accuracy and a 2.5x reduction in model size. FracBNN also\noutperforms a recently introduced BNN model with an increase of 2.4% in top-1\naccuracy while using the same model size. On the embedded FPGA device, FracBNN\ndemonstrates the ability of real-time image classification.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 17:49:30 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Zhang", "Yichi", ""], ["Pan", "Junhao", ""], ["Liu", "Xinheng", ""], ["Chen", "Hongzheng", ""], ["Chen", "Deming", ""], ["Zhang", "Zhiru", ""]]}, {"id": "2012.12215", "submitter": "Seunghwan Jung", "authors": "Seung Hwan Jung, Minyoung Chung, and Yeong-Gil Shin", "title": "Robust Kernel-based Feature Representation for 3D Point Cloud Analysis\n  via Circular Graph Convolutional Network", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature descriptors of point clouds are used in several applications, such as\nregistration and part segmentation of 3D point clouds. Learning discriminative\nrepresentations of local geometric features is unquestionably the most\nimportant task for accurate point cloud analyses. However, it is challenging to\ndevelop rotation or scale-invariant descriptors. Most previous studies have\neither ignored rotations or empirically studied optimal scale parameters, which\nhinders the applicability of the methods for real-world datasets. In this\npaper, we present a new local feature description method that is robust to\nrotation, density, and scale variations. Moreover, to improve representations\nof the local descriptors, we propose a global aggregation method. First, we\nplace kernels aligned around each point in the normal direction. To avoid the\nsign problem of the normal vector, we use a symmetric kernel point distribution\nin the tangential plane. From each kernel point, we first projected the points\nfrom the spatial space to the feature space, which is robust to multiple scales\nand rotation, based on angles and distances. Subsequently, we perform graph\nconvolutions by considering local kernel point structures and long-range global\ncontext, obtained by a global aggregation method. We experimented with our\nproposed descriptors on benchmark datasets (i.e., ModelNet40 and ShapeNetPart)\nto evaluate the performance of registration, classification, and part\nsegmentation on 3D point clouds. Our method showed superior performances when\ncompared to the state-of-the-art methods by reducing 70$\\%$ of the rotation and\ntranslation errors in the registration task. Our method also showed comparable\nperformance in the classification and part-segmentation tasks with simple and\nlow-dimensional architectures.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 18:02:57 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 02:15:02 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 08:18:54 GMT"}, {"version": "v4", "created": "Thu, 14 Jan 2021 02:37:19 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Jung", "Seung Hwan", ""], ["Chung", "Minyoung", ""], ["Shin", "Yeong-Gil", ""]]}, {"id": "2012.12229", "submitter": "Gabriele Lagani", "authors": "Gabriele Lagani, Giuseppe Amato, Fabrizio Falchi, Claudio Gennaro", "title": "Training Convolutional Neural Networks With Hebbian Principal Component\n  Analysis", "comments": "12 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent work has shown that biologically plausible Hebbian learning can be\nintegrated with backpropagation learning (backprop), when training deep\nconvolutional neural networks. In particular, it has been shown that Hebbian\nlearning can be used for training the lower or the higher layers of a neural\nnetwork. For instance, Hebbian learning is effective for re-training the higher\nlayers of a pre-trained deep neural network, achieving comparable accuracy\nw.r.t. SGD, while requiring fewer training epochs, suggesting potential\napplications for transfer learning. In this paper we build on these results and\nwe further improve Hebbian learning in these settings, by using a nonlinear\nHebbian Principal Component Analysis (HPCA) learning rule, in place of the\nHebbian Winner Takes All (HWTA) strategy used in previous work. We test this\napproach in the context of computer vision. In particular, the HPCA rule is\nused to train Convolutional Neural Networks in order to extract relevant\nfeatures from the CIFAR-10 image dataset. The HPCA variant that we explore\nfurther improves the previous results, motivating further interest towards\nbiologically plausible learning algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 18:17:46 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Lagani", "Gabriele", ""], ["Amato", "Giuseppe", ""], ["Falchi", "Fabrizio", ""], ["Gennaro", "Claudio", ""]]}, {"id": "2012.12235", "submitter": "Andrew Ilyas", "authors": "Hadi Salman, Andrew Ilyas, Logan Engstrom, Sai Vemprala, Aleksander\n  Madry, Ashish Kapoor", "title": "Unadversarial Examples: Designing Objects for Robust Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of realistic computer vision settings wherein one can\ninfluence the design of the objects being recognized. We develop a framework\nthat leverages this capability to significantly improve vision models'\nperformance and robustness. This framework exploits the sensitivity of modern\nmachine learning algorithms to input perturbations in order to design \"robust\nobjects,\" i.e., objects that are explicitly optimized to be confidently\ndetected or classified. We demonstrate the efficacy of the framework on a wide\nvariety of vision-based tasks ranging from standard benchmarks, to\n(in-simulation) robotics, to real-world experiments. Our code can be found at\nhttps://git.io/unadversarial .\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 18:26:07 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Salman", "Hadi", ""], ["Ilyas", "Andrew", ""], ["Engstrom", "Logan", ""], ["Vemprala", "Sai", ""], ["Madry", "Aleksander", ""], ["Kapoor", "Ashish", ""]]}, {"id": "2012.12247", "submitter": "Edgar Tretschk", "authors": "Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollh\\\"ofer,\n  Christoph Lassner, Christian Theobalt", "title": "Non-Rigid Neural Radiance Fields: Reconstruction and Novel View\n  Synthesis of a Dynamic Scene From Monocular Video", "comments": "Project page (incl. supplemental videos and code):\n  https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction and\nnovel view synthesis approach for general non-rigid dynamic scenes. Our\napproach takes RGB images of a dynamic scene as input, e.g., from a monocular\nvideo recording, and creates a high-quality space-time geometry and appearance\nrepresentation. In particular, we show that even a single handheld\nconsumer-grade camera is sufficient to synthesize sophisticated renderings of a\ndynamic scene from novel virtual camera views, for example a `bullet-time'\nvideo effect. Our method disentangles the dynamic scene into a canonical volume\nand its deformation. Scene deformation is implemented as ray bending, where\nstraight rays are deformed non-rigidly to represent scene motion. We also\npropose a novel rigidity regression network that enables us to better constrain\nrigid regions of the scene, which leads to more stable results. The ray bending\nand rigidity network are trained without any explicit supervision. In addition\nto novel view synthesis, our formulation enables dense correspondence\nestimation across views and time, as well as compelling video editing\napplications such as motion exaggeration. We demonstrate the effectiveness of\nour method using extensive evaluations, including ablation studies and\ncomparisons to the state of the art. We urge the reader to watch the\nsupplemental video for qualitative results. Our code will be open sourced.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 18:46:12 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 07:24:46 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2021 18:08:43 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Tretschk", "Edgar", ""], ["Tewari", "Ayush", ""], ["Golyanik", "Vladislav", ""], ["Zollh\u00f6fer", "Michael", ""], ["Lassner", "Christoph", ""], ["Theobalt", "Christian", ""]]}, {"id": "2012.12258", "submitter": "Chau Yi Li", "authors": "Chau Yi Li, Riccardo Mazzon, Andrea Cavallaro", "title": "Underwater image filtering: methods, datasets and evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Underwater images are degraded by the selective attenuation of light that\ndistorts colours and reduces contrast. The degradation extent depends on the\nwater type, the distance between an object and the camera, and the depth under\nthe water surface the object is at. Underwater image filtering aims to restore\nor to enhance the appearance of objects captured in an underwater image.\nRestoration methods compensate for the actual degradation, whereas enhancement\nmethods improve either the perceived image quality or the performance of\ncomputer vision algorithms. The growing interest in underwater image filtering\nmethods--including learning-based approaches used for both restoration and\nenhancement--and the associated challenges call for a comprehensive review of\nthe state of the art. In this paper, we review the design principles of\nfiltering methods and revisit the oceanology background that is fundamental to\nidentify the degradation causes. We discuss image formation models and the\nresults of restoration methods in various water types. Furthermore, we present\ntask-dependent enhancement methods and categorise datasets for training neural\nnetworks and for method evaluation. Finally, we discuss evaluation strategies,\nincluding subjective tests and quality assessment measures. We complement this\nsurvey with a platform ( https://puiqe.eecs.qmul.ac.uk/ ), which hosts\nstate-of-the-art underwater filtering methods and facilitates comparisons.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 18:56:39 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Li", "Chau Yi", ""], ["Mazzon", "Riccardo", ""], ["Cavallaro", "Andrea", ""]]}, {"id": "2012.12259", "submitter": "Haotian Liu", "authors": "Haotian Liu, Rafael A. Rivera Soto, Fanyi Xiao, Yong Jae Lee", "title": "YolactEdge: Real-time Instance Segmentation on the Edge", "comments": "\\c{opyright} 2021 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose YolactEdge, the first competitive instance segmentation approach\nthat runs on small edge devices at real-time speeds. Specifically, YolactEdge\nruns at up to 30.8 FPS on a Jetson AGX Xavier (and 172.7 FPS on an RTX 2080 Ti)\nwith a ResNet-101 backbone on 550x550 resolution images. To achieve this, we\nmake two improvements to the state-of-the-art image-based real-time method\nYOLACT: (1) applying TensorRT optimization while carefully trading off speed\nand accuracy, and (2) a novel feature warping module to exploit temporal\nredundancy in videos. Experiments on the YouTube VIS and MS COCO datasets\ndemonstrate that YolactEdge produces a 3-5x speed up over existing real-time\nmethods while producing competitive mask and box detection accuracy. We also\nconduct ablation studies to dissect our design choices and modules. Code and\nmodels are available at https://github.com/haotian-liu/yolact_edge.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 18:58:18 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 17:55:10 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Liu", "Haotian", ""], ["Soto", "Rafael A. Rivera", ""], ["Xiao", "Fanyi", ""], ["Lee", "Yong Jae", ""]]}, {"id": "2012.12261", "submitter": "Xuan Luo", "authors": "Xuan Luo, Xuaner Zhang, Paul Yoo, Ricardo Martin-Brualla, Jason\n  Lawrence, Steven M. Seitz", "title": "Time-Travel Rephotography", "comments": "Project Page: https://time-travel-rephotography.github.io Video:\n  https://youtu.be/eNOGqNCbcV8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many historical people are captured only in old, faded, black and white\nphotos, that have been distorted by the limitations of early cameras and the\npassage of time. This paper simulates traveling back in time with a modern\ncamera to rephotograph famous subjects. Unlike conventional image restoration\nfilters which apply independent operations like denoising, colorization, and\nsuperresolution, we leverage the StyleGAN2 framework to project old photos into\nthe space of modern high-resolution photos, achieving all of these effects in a\nunified framework. A unique challenge with this approach is capturing the\nidentity and pose of the photo's subject and not the many artifacts in\nlow-quality antique photos. Our comparisons to current state-of-the-art\nrestoration filters show significant improvements and compelling results for a\nvariety of important historical people.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 18:59:12 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Luo", "Xuan", ""], ["Zhang", "Xuaner", ""], ["Yoo", "Paul", ""], ["Martin-Brualla", "Ricardo", ""], ["Lawrence", "Jason", ""], ["Seitz", "Steven M.", ""]]}, {"id": "2012.12265", "submitter": "Chengzhi Mao", "authors": "Chengzhi Mao, Augustine Cha, Amogh Gupta, Hao Wang, Junfeng Yang, Carl\n  Vondrick", "title": "Generative Interventions for Causal Learning", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for learning robust visual representations that\ngeneralize to new viewpoints, backgrounds, and scene contexts. Discriminative\nmodels often learn naturally occurring spurious correlations, which cause them\nto fail on images outside of the training distribution. In this paper, we show\nthat we can steer generative models to manufacture interventions on features\ncaused by confounding factors. Experiments, visualizations, and theoretical\nresults show this method learns robust representations more consistent with the\nunderlying causal relationships. Our approach improves performance on multiple\ndatasets demanding out-of-distribution generalization, and we demonstrate\nstate-of-the-art performance generalizing from ImageNet to ObjectNet dataset.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 16:01:55 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 14:55:21 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Mao", "Chengzhi", ""], ["Cha", "Augustine", ""], ["Gupta", "Amogh", ""], ["Wang", "Hao", ""], ["Yang", "Junfeng", ""], ["Vondrick", "Carl", ""]]}, {"id": "2012.12302", "submitter": "Samuel Rivera", "authors": "Samuel Rivera, Joel Klipfel, Deborah Weeks", "title": "Flexible deep transfer learning by separate feature embeddings and\n  manifold alignment", "comments": null, "journal-ref": null, "doi": "10.1117/12.2557063", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition is a key enabler across industry and defense. As\ntechnology changes, algorithms must keep pace with new requirements and data.\nNew modalities and higher resolution sensors should allow for increased\nalgorithm robustness. Unfortunately, algorithms trained on existing labeled\ndatasets do not directly generalize to new data because the data distributions\ndo not match. Transfer learning (TL) or domain adaptation (DA) methods have\nestablished the groundwork for transferring knowledge from existing labeled\nsource data to new unlabeled target datasets. However, current DA approaches\nassume similar source and target feature spaces and suffer in the case of\nmassive domain shifts or changes in the feature space. Existing methods assume\nthe data are either the same modality, or can be aligned to a common feature\nspace. Therefore, most methods are not designed to support a fundamental domain\nchange such as visual to auditory data. We propose a novel deep learning\nframework that overcomes this limitation by learning separate feature\nextractions for each domain while minimizing the distance between the domains\nin a latent lower-dimensional space. The alignment is achieved by considering\nthe data manifold along with an adversarial training procedure. We demonstrate\nthe effectiveness of the approach versus traditional methods with several\nablation experiments on synthetic, measured, and satellite image datasets. We\nalso provide practical guidelines for training the network while overcoming\nvanishing gradients which inhibit learning in some adversarial training\nsettings.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 19:24:44 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Rivera", "Samuel", ""], ["Klipfel", "Joel", ""], ["Weeks", "Deborah", ""]]}, {"id": "2012.12306", "submitter": "Adrian Perez-Suay", "authors": "Adri\\'an P\\'erez-Suay, Julia Amor\\'os-L\\'opez, Luis G\\'omez-Chova,\n  Jordi Mu\\~noz-Mar\\'i, Dieter Just, Gustau Camps-Valls", "title": "Pattern Recognition Scheme for Large-Scale Cloud Detection over\n  Landmarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Landmark recognition and matching is a critical step in many Image Navigation\nand Registration (INR) models for geostationary satellite services, as well as\nto maintain the geometric quality assessment (GQA) in the instrument data\nprocessing chain of Earth observation satellites. Matching the landmark\naccurately is of paramount relevance, and the process can be strongly impacted\nby the cloud contamination of a given landmark. This paper introduces a\ncomplete pattern recognition methodology able to detect the presence of clouds\nover landmarks using Meteosat Second Generation (MSG) data. The methodology is\nbased on the ensemble combination of dedicated support vector machines (SVMs)\ndependent on the particular landmark and illumination conditions. This\ndivide-and-conquer strategy is motivated by the data complexity and follows a\nphysically-based strategy that considers variability both in seasonality and\nillumination conditions along the day to split observations. In addition, it\nallows training the classification scheme with millions of samples at an\naffordable computational costs. The image archive was composed of 200 landmark\ntest sites with near 7 million multispectral images that correspond to MSG\nacquisitions during 2010. Results are analyzed in terms of cloud detection\naccuracy and computational cost. We provide illustrative source code and a\nportion of the huge training data to the community.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 09:53:08 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["P\u00e9rez-Suay", "Adri\u00e1n", ""], ["Amor\u00f3s-L\u00f3pez", "Julia", ""], ["G\u00f3mez-Chova", "Luis", ""], ["Mu\u00f1oz-Mar\u00ed", "Jordi", ""], ["Just", "Dieter", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.12307", "submitter": "Adrian Perez-Suay", "authors": "Jos\\'e A. Padr\\'on Hidalgo, Adri\\'an P\\'erez-Suay, Fatih Nar, Gustau\n  Camps-Valls", "title": "Nonlinear Cook distance for Anomalous Change Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work we propose a method to find anomalous changes in remote sensing\nimages based on the chronochrome approach. A regressor between images is used\nto discover the most {\\em influential points} in the observed data. Typically,\nthe pixels with largest residuals are decided to be anomalous changes. In order\nto find the anomalous pixels we consider the Cook distance and propose its\nnonlinear extension using random Fourier features as an efficient nonlinear\nmeasure of impact. Good empirical performance is shown over different\nmultispectral images both visually and quantitatively evaluated with ROC\ncurves.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 11:11:31 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Hidalgo", "Jos\u00e9 A. Padr\u00f3n", ""], ["P\u00e9rez-Suay", "Adri\u00e1n", ""], ["Nar", "Fatih", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.12308", "submitter": "Adrian Perez-Suay", "authors": "Fatih Nar, Adri\\'an P\\'erez-Suay, Jos\\'e Antonio Padr\\'on, Gustau\n  Camps-Valls", "title": "Randomized RX for target detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This work tackles the target detection problem through the well-known global\nRX method. The RX method models the clutter as a multivariate Gaussian\ndistribution, and has been extended to nonlinear distributions using kernel\nmethods. While the kernel RX can cope with complex clutters, it requires a\nconsiderable amount of computational resources as the number of clutter pixels\ngets larger. Here we propose random Fourier features to approximate the\nGaussian kernel in kernel RX and consequently our development keep the accuracy\nof the nonlinearity while reducing the computational cost which is now\ncontrolled by an hyperparameter. Results over both synthetic and real-world\nimage target detection problems show space and time efficiency of the proposed\nmethod while providing high detection performance.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 19:18:49 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Nar", "Fatih", ""], ["P\u00e9rez-Suay", "Adri\u00e1n", ""], ["Padr\u00f3n", "Jos\u00e9 Antonio", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.12311", "submitter": "Prashant Rajaram", "authors": "Prashant Rajaram and Puneet Manchanda", "title": "Video Influencers: Unboxing the Mystique", "comments": "61 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Influencer marketing is being used increasingly as a tool to reach customers\nbecause of the growing popularity of social media stars who primarily reach\ntheir audience(s) via custom videos. Despite the rapid growth in influencer\nmarketing, there has been little research on the design and effectiveness of\ninfluencer videos. Using publicly available data on YouTube influencer videos,\nwe implement novel interpretable deep learning architectures, supported by\ntransfer learning, to identify significant relationships between advertising\ncontent in videos (across text, audio, and images) and video views, interaction\nrates and sentiment. By avoiding ex-ante feature engineering and instead using\nex-post interpretation, our approach avoids making a trade-off between\ninterpretability and predictive ability. We filter out relationships that are\naffected by confounding factors unassociated with an increase in attention to\nvideo elements, thus facilitating the generation of plausible causal\nrelationships between video elements and marketing outcomes which can be tested\nin the field. A key finding is that brand mentions in the first 30 seconds of a\nvideo are on average associated with a significant increase in attention to the\nbrand but a significant decrease in sentiment expressed towards the video. We\nillustrate the learnings from our approach for both influencers and brands.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 19:32:52 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Rajaram", "Prashant", ""], ["Manchanda", "Puneet", ""]]}, {"id": "2012.12314", "submitter": "Namdar Homayounfar", "authors": "Namdar Homayounfar, Wei-Chiu Ma, Shrinidhi Kowshika Lakshmikanth,\n  Raquel Urtasun", "title": "Hierarchical Recurrent Attention Networks for Structured Online Maps", "comments": "Published at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of online road network extraction from\nsparse 3D point clouds. Our method is inspired by how an annotator builds a\nlane graph, by first identifying how many lanes there are and then drawing each\none in turn. We develop a hierarchical recurrent network that attends to\ninitial regions of a lane boundary and traces them out completely by outputting\na structured polyline. We also propose a novel differentiable loss function\nthat measures the deviation of the edges of the ground truth polylines and\ntheir predictions. This is more suitable than distances on vertices, as there\nexists many ways to draw equivalent polylines. We demonstrate the effectiveness\nof our method on a 90 km stretch of highway, and show that we can recover the\nright topology 92\\% of the time.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 19:35:53 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Homayounfar", "Namdar", ""], ["Ma", "Wei-Chiu", ""], ["Lakshmikanth", "Shrinidhi Kowshika", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2012.12334", "submitter": "Yunqiang Li", "authors": "Yunqiang Li and Jan van Gemert", "title": "Deep Unsupervised Image Hashing by Maximizing Bit Entropy", "comments": "9 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised hashing is important for indexing huge image or video\ncollections without having expensive annotations available. Hashing aims to\nlearn short binary codes for compact storage and efficient semantic retrieval.\nWe propose an unsupervised deep hashing layer called Bi-half Net that maximizes\nentropy of the binary codes. Entropy is maximal when both possible values of\nthe bit are uniformly (half-half) distributed. To maximize bit entropy, we do\nnot add a term to the loss function as this is difficult to optimize and tune.\nInstead, we design a new parameter-free network layer to explicitly force\ncontinuous image features to approximate the optimal half-half bit\ndistribution. This layer is shown to minimize a penalized term of the\nWasserstein distance between the learned continuous image features and the\noptimal half-half bit distribution. Experimental results on the image datasets\nFlickr25k, Nus-wide, Cifar-10, Mscoco, Mnist and the video datasets Ucf-101 and\nHmdb-51 show that our approach leads to compact codes and compares favorably to\nthe current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 20:10:15 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Li", "Yunqiang", ""], ["van Gemert", "Jan", ""]]}, {"id": "2012.12352", "submitter": "Letitia Parcalabescu", "authors": "Letitia Parcalabescu and Albert Gatt and Anette Frank and Iacer\n  Calixto", "title": "Seeing past words: Testing the cross-modal capabilities of pretrained\n  V&L models on counting tasks", "comments": "Paper accepted for publication at MMSR 2021; 13 pages, 3 figures, 7\n  Tables", "journal-ref": "Proceedings of the 1st Workshop on Multimodal Semantic\n  Representations (MMSR), 2021, Groningen, Netherlands (Online), Association\n  for Computational Linguistics, p. 32--44", "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the reasoning ability of pretrained vision and language (V&L)\nmodels in two tasks that require multimodal integration: (1) discriminating a\ncorrect image-sentence pair from an incorrect one, and (2) counting entities in\nan image. We evaluate three pretrained V&L models on these tasks: ViLBERT,\nViLBERT 12-in-1 and LXMERT, in zero-shot and finetuned settings. Our results\nshow that models solve task (1) very well, as expected, since all models are\npretrained on task (1). However, none of the pretrained V&L models is able to\nadequately solve task (2), our counting probe, and they cannot generalise to\nout-of-distribution quantities. We propose a number of explanations for these\nfindings: LXMERT (and to some extent ViLBERT 12-in-1) show some evidence of\ncatastrophic forgetting on task (1). Concerning our results on the counting\nprobe, we find evidence that all models are impacted by dataset bias, and also\nfail to individuate entities in the visual input. While a selling point of\npretrained V&L models is their ability to solve complex tasks, our findings\nsuggest that understanding their reasoning and grounding capabilities requires\nmore targeted investigations on specific phenomena.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 21:01:44 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 16:38:24 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 11:03:08 GMT"}, {"version": "v4", "created": "Thu, 17 Jun 2021 17:51:56 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Parcalabescu", "Letitia", ""], ["Gatt", "Albert", ""], ["Frank", "Anette", ""], ["Calixto", "Iacer", ""]]}, {"id": "2012.12360", "submitter": "Hunter Blanton", "authors": "Hunter Blanton, Scott Workman, Nathan Jacobs", "title": "A Structure-Aware Method for Direct Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating camera pose from a single image is a fundamental problem in\ncomputer vision. Existing methods for solving this task fall into two distinct\ncategories, which we refer to as direct and indirect. Direct methods, such as\nPoseNet, regress pose from the image as a fixed function, for example using a\nfeed-forward convolutional network. Such methods are desirable because they are\ndeterministic and run in constant time. Indirect methods for pose regression\nare often non-deterministic, with various external dependencies such as image\nretrieval and hypothesis sampling. We propose a direct method that takes\ninspiration from structure-based approaches to incorporate explicit 3D\nconstraints into the network. Our approach maintains the desirable qualities of\nother direct methods while achieving much lower error in general.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 21:19:36 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Blanton", "Hunter", ""], ["Workman", "Scott", ""], ["Jacobs", "Nathan", ""]]}, {"id": "2012.12368", "submitter": "Theodoros Tsiligkaridis", "authors": "Theodoros Tsiligkaridis, Jay Roberts", "title": "Understanding Frank-Wolfe Adversarial Training", "comments": "Accepted to ICML 2021 Adversarial Machine Learning Workshop. Under\n  review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks are easily fooled by small perturbations known as\nadversarial attacks. Adversarial Training (AT) is a technique that\napproximately solves a robust optimization problem to minimize the worst-case\nloss and is widely regarded as the most effective defense against such attacks.\nWe develop a theoretical framework for adversarial training with FW\noptimization (FW-AT) that reveals a geometric connection between the loss\nlandscape and the distortion of $\\ell_\\infty$ FW attacks (the attack's $\\ell_2$\nnorm). Specifically, we show that high distortion of FW attacks is equivalent\nto low variation along the attack path. It is then experimentally demonstrated\non various deep neural network architectures that $\\ell_\\infty$ attacks against\nrobust models achieve near maximal $\\ell_2$ distortion. This mathematical\ntransparency differentiates FW from the more popular Projected Gradient Descent\n(PGD) optimization. To demonstrate the utility of our theoretical framework we\ndevelop FW-Adapt, a novel adversarial training algorithm which uses simple\ndistortion measure to adaptively change number of attack steps during training.\nFW-Adapt provides strong robustness at lower training times in comparison to\nPGD-AT for a variety of white-box and black-box attacks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 21:36:52 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 17:38:24 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 15:54:03 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Tsiligkaridis", "Theodoros", ""], ["Roberts", "Jay", ""]]}, {"id": "2012.12372", "submitter": "Maximilian Augustin", "authors": "Maximilian Augustin, Matthias Hein", "title": "Out-distribution aware Self-training in an Open World Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning heavily depends on large labeled datasets which limits further\nimprovements. While unlabeled data is available in large amounts, in particular\nin image recognition, it does not fulfill the closed world assumption of\nsemi-supervised learning that all unlabeled data are task-related. The goal of\nthis paper is to leverage unlabeled data in an open world setting to further\nimprove prediction performance. For this purpose, we introduce out-distribution\naware self-training, which includes a careful sample selection strategy based\non the confidence of the classifier. While normal self-training deteriorates\nprediction performance, our iterative scheme improves using up to 15 times the\namount of originally labeled data. Moreover, our classifiers are by design\nout-distribution aware and can thus distinguish task-related inputs from\nunrelated ones.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 12:25:04 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Augustin", "Maximilian", ""], ["Hein", "Matthias", ""]]}, {"id": "2012.12377", "submitter": "Namdar Homayounfar", "authors": "Namdar Homayounfar, Wei-Chiu Ma, Justin Liang, Xinyu Wu, Jack Fan,\n  Raquel Urtasun", "title": "DAGMapper: Learning to Map by Discovering Lane Topology", "comments": "Published at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental challenges to scale self-driving is being able to\ncreate accurate high definition maps (HD maps) with low cost. Current attempts\nto automate this process typically focus on simple scenarios, estimate\nindependent maps per frame or do not have the level of precision required by\nmodern self driving vehicles. In contrast, in this paper we focus on drawing\nthe lane boundaries of complex highways with many lanes that contain topology\nchanges due to forks and merges. Towards this goal, we formulate the problem as\ninference in a directed acyclic graphical model (DAG), where the nodes of the\ngraph encode geometric and topological properties of the local regions of the\nlane boundaries. Since we do not know a priori the topology of the lanes, we\nalso infer the DAG topology (i.e., nodes and edges) for each region. We\ndemonstrate the effectiveness of our approach on two major North American\nHighways in two different states and show high precision and recall as well as\n89% correct topology.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 21:58:57 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Homayounfar", "Namdar", ""], ["Ma", "Wei-Chiu", ""], ["Liang", "Justin", ""], ["Wu", "Xinyu", ""], ["Fan", "Jack", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2012.12395", "submitter": "Bin Yang", "authors": "Wenjie Luo, Bin Yang, Raquel Urtasun", "title": "Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion\n  Forecasting with a Single Convolutional Net", "comments": "CVPR 2018 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we propose a novel deep neural network that is able to jointly\nreason about 3D detection, tracking and motion forecasting given data captured\nby a 3D sensor. By jointly reasoning about these tasks, our holistic approach\nis more robust to occlusion as well as sparse data at range. Our approach\nperforms 3D convolutions across space and time over a bird's eye view\nrepresentation of the 3D world, which is very efficient in terms of both memory\nand computation. Our experiments on a new very large scale dataset captured in\nseveral north american cities, show that we can outperform the state-of-the-art\nby a large margin. Importantly, by sharing computation we can perform all tasks\nin as little as 30 ms.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 22:43:35 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Luo", "Wenjie", ""], ["Yang", "Bin", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2012.12397", "submitter": "Bin Yang", "authors": "Ming Liang, Bin Yang, Yun Chen, Rui Hu, Raquel Urtasun", "title": "Multi-Task Multi-Sensor Fusion for 3D Object Detection", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we propose to exploit multiple related tasks for accurate\nmulti-sensor 3D object detection. Towards this goal we present an end-to-end\nlearnable architecture that reasons about 2D and 3D object detection as well as\nground estimation and depth completion. Our experiments show that all these\ntasks are complementary and help the network learn better representations by\nfusing information at various levels. Importantly, our approach leads the KITTI\nbenchmark on 2D, 3D and BEV object detection, while being real time.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 22:49:15 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Liang", "Ming", ""], ["Yang", "Bin", ""], ["Chen", "Yun", ""], ["Hu", "Rui", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2012.12401", "submitter": "Naman Kohli", "authors": "Sonal Doomra, Naman Kohli, Shounak Athavale", "title": "Turn Signal Prediction: A Federated Learning Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Driving etiquette takes a different flavor for each locality as drivers not\nonly comply with rules/laws but also abide by local unspoken convention. When\nto have the turn signal (indicator) on/off is one such etiquette which does not\nhave a definitive right or wrong answer. Learning this behavior from the\nabundance of data generated from various sensor modalities integrated in the\nvehicle is a suitable candidate for deep learning. But what makes it a prime\ncandidate for Federated Learning are privacy concerns and bandwidth limitations\nfor any data aggregation. This paper presents a long short-term memory (LSTM)\nbased Turn Signal Prediction (on or off) model using vehicle control area\nnetwork (CAN) signal data. The model is trained using two approaches, one by\ncentrally aggregating the data and the other in a federated manner. Centrally\ntrained models and federated models are compared under similar hyperparameter\nsettings. This research demonstrates the efficacy of federated learning, paving\nthe way for in-vehicle learning of driving etiquette.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 22:58:22 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Doomra", "Sonal", ""], ["Kohli", "Naman", ""], ["Athavale", "Shounak", ""]]}, {"id": "2012.12402", "submitter": "Bin Yang", "authors": "Yun Chen, Bin Yang, Ming Liang, Raquel Urtasun", "title": "Learning Joint 2D-3D Representations for Depth Completion", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we tackle the problem of depth completion from RGBD data.\nTowards this goal, we design a simple yet effective neural network block that\nlearns to extract joint 2D and 3D features. Specifically, the block consists of\ntwo domain-specific sub-networks that apply 2D convolution on image pixels and\ncontinuous convolution on 3D points, with their output features fused in image\nspace. We build the depth completion network simply by stacking the proposed\nblock, which has the advantage of learning hierarchical representations that\nare fully fused between 2D and 3D spaces at multiple levels. We demonstrate the\neffectiveness of our approach on the challenging KITTI depth completion\nbenchmark and show that our approach outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 22:58:29 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Chen", "Yun", ""], ["Yang", "Bin", ""], ["Liang", "Ming", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2012.12406", "submitter": "Kevin Thomas", "authors": "Kevin A. Thomas (1), Dominik Krzemi\\'nski (2), {\\L}ukasz Kidzi\\'nski\n  (3), Rohan Paul (1), Elka B. Rubin (4), Eni Halilaj (5), Marianne S. Black\n  (4) Akshay Chaudhari (1,4), Garry E. Gold (3,4,6), Scott L. Delp (3,6,7) ((1)\n  Department of Biomedical Data Science, Stanford University, California, USA\n  (2) Cardiff University Brain Research Imaging Centre, Cardiff University,\n  United Kingdom (3) Department of Biomedical Engineering, Stanford University,\n  California, USA (4) Department of Radiology, Stanford University, California,\n  USA (5) Department of Mechanical Engineering, Carnegie Mellon University,\n  Pennsylvania, USA (6) Department of Orthopaedic Surgery, Stanford University,\n  California, USA (7) Department of Mechanical Engineering, Stanford\n  University, California, USA)", "title": "Open source software for automatic subregional assessment of knee\n  cartilage degradation using quantitative T2 relaxometry and deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM q-bio.TO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Objective: We evaluate a fully-automated femoral cartilage segmentation model\nfor measuring T2 relaxation values and longitudinal changes using multi-echo\nspin echo (MESE) MRI. We have open sourced this model and corresponding\nsegmentations. Methods: We trained a neural network to segment femoral\ncartilage from MESE MRIs. Cartilage was divided into 12 subregions along\nmedial-lateral, superficial-deep, and anterior-central-posterior boundaries.\nSubregional T2 values and four-year changes were calculated using a\nmusculoskeletal radiologist's segmentations (Reader 1) and the model's\nsegmentations. These were compared using 28 held out images. A subset of 14\nimages were also evaluated by a second expert (Reader 2) for comparison.\nResults: Model segmentations agreed with Reader 1 segmentations with a Dice\nscore of 0.85 +/- 0.03. The model's estimated T2 values for individual\nsubregions agreed with those of Reader 1 with an average Spearman correlation\nof 0.89 and average mean absolute error (MAE) of 1.34 ms. The model's estimated\nfour-year change in T2 for individual regions agreed with Reader 1 with an\naverage correlation of 0.80 and average MAE of 1.72 ms. The model agreed with\nReader 1 at least as closely as Reader 2 agreed with Reader 1 in terms of Dice\nscore (0.85 vs 0.75) and subregional T2 values. Conclusions: We present a fast,\nfully-automated model for segmentation of MESE MRIs. Assessments of cartilage\nhealth using its segmentations agree with those of an expert as closely as\nexperts agree with one another. This has the potential to accelerate\nosteoarthritis research.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 23:08:41 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Thomas", "Kevin A.", ""], ["Krzemi\u0144ski", "Dominik", ""], ["Kidzi\u0144ski", "\u0141ukasz", ""], ["Paul", "Rohan", ""], ["Rubin", "Elka B.", ""], ["Halilaj", "Eni", ""], ["Black", "Marianne S.", ""], ["Chaudhari", "Akshay", ""], ["Gold", "Garry E.", ""], ["Delp", "Scott L.", ""]]}, {"id": "2012.12410", "submitter": "Benjamin Maas", "authors": "Benjamin Maas, Erfan Zabeh, Soroush Arabshahi", "title": "QuickTumorNet: Fast Automatic Multi-Class Segmentation of Brain Tumors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-invasive techniques such as magnetic resonance imaging (MRI) are widely\nemployed in brain tumor diagnostics. However, manual segmentation of brain\ntumors from 3D MRI volumes is a time-consuming task that requires trained\nexpert radiologists. Due to the subjectivity of manual segmentation, there is\nlow inter-rater reliability which can result in diagnostic discrepancies. As\nthe success of many brain tumor treatments depends on early intervention, early\ndetection is paramount. In this context, a fully automated segmentation method\nfor brain tumor segmentation is necessary as an efficient and reliable method\nfor brain tumor detection and quantification. In this study, we propose an\nend-to-end approach for brain tumor segmentation, capitalizing on a modified\nversion of QuickNAT, a brain tissue type segmentation deep convolutional neural\nnetwork (CNN). Our method was evaluated on a data set of 233 patient's T1\nweighted images containing three tumor type classes annotated (meningioma,\nglioma, and pituitary). Our model, QuickTumorNet, demonstrated fast, reliable,\nand accurate brain tumor segmentation that can be utilized to assist clinicians\nin diagnosis and treatment.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 23:16:43 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Maas", "Benjamin", ""], ["Zabeh", "Erfan", ""], ["Arabshahi", "Soroush", ""]]}, {"id": "2012.12412", "submitter": "Ilya Ovodov", "authors": "Ilya G. Ovodov", "title": "Optical Braille Recognition Using Object Detection CNN", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes an optical Braille recognition method that uses an object\ndetection convolutional neural network to detect whole Braille characters at\nonce. The proposed algorithm is robust to the deformation of the page shown in\nthe image and perspective distortions. It makes it usable for recognition of\nBraille texts being shoot on a smartphone camera, including bowed pages and\nperspective distorted images. The proposed algorithm shows high performance and\naccuracy compared to existing methods. We also introduce a new \"Angelina\nBraille Images Dataset\" containing 240 annotated photos of Braille texts. The\nproposed algorithm and dataset are available at GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 23:22:59 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Ovodov", "Ilya G.", ""]]}, {"id": "2012.12413", "submitter": "Jelica Vasiljevic", "authors": "Jelica Vasiljevi\\'c and Friedrich Feuerhake and C\\'edric Wemmert and\n  Thomas Lampert", "title": "Towards Histopathological Stain Invariance by Unsupervised Domain\n  Augmentation using Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The application of supervised deep learning methods in digital pathology is\nlimited due to their sensitivity to domain shift. Digital Pathology is an area\nprone to high variability due to many sources, including the common practice of\nevaluating several consecutive tissue sections stained with different staining\nprotocols. Obtaining labels for each stain is very expensive and time consuming\nas it requires a high level of domain knowledge. In this article, we propose an\nunsupervised augmentation approach based on adversarial image-to-image\ntranslation, which facilitates the training of stain invariant supervised\nconvolutional neural networks. By training the network on one commonly used\nstaining modality and applying it to images that include corresponding, but\ndifferently stained, tissue structures, the presented method demonstrates\nsignificant improvements over other approaches. These benefits are illustrated\nin the problem of glomeruli segmentation in seven different staining modalities\n(PAS, Jones H&E, CD68, Sirius Red, CD34, H&E and CD3) and analysis of the\nlearned representations demonstrate their stain invariance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 23:32:17 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Vasiljevi\u0107", "Jelica", ""], ["Feuerhake", "Friedrich", ""], ["Wemmert", "C\u00e9dric", ""], ["Lampert", "Thomas", ""]]}, {"id": "2012.12418", "submitter": "Xingyi Yang", "authors": "Xingyi Yang", "title": "Stochastic Gradient Variance Reduction by Solving a Filtering Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks (DNN) are typically optimized using stochastic gradient\ndescent (SGD). However, the estimation of the gradient using stochastic samples\ntends to be noisy and unreliable, resulting in large gradient variance and bad\nconvergence. In this paper, we propose \\textbf{Filter Gradient Decent}~(FGD),\nan efficient stochastic optimization algorithm that makes the consistent\nestimation of the local gradient by solving an adaptive filtering problem with\ndifferent design of filters. Our method reduces variance in stochastic gradient\ndescent by incorporating the historical states to enhance the current\nestimation. It is able to correct noisy gradient direction as well as to\naccelerate the convergence of learning. We demonstrate the effectiveness of the\nproposed Filter Gradient Descent on numerical optimization and training neural\nnetworks, where it achieves superior and robust performance compared with\ntraditional momentum-based methods. To the best of our knowledge, we are the\nfirst to provide a practical solution that integrates filtering into gradient\nestimation by making the analogy between gradient estimation and filtering\nproblems in signal processing. (The code is provided in\nhttps://github.com/Adamdad/Filter-Gradient-Decent)\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 23:48:42 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 01:53:12 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Yang", "Xingyi", ""]]}, {"id": "2012.12425", "submitter": "Ho Hin Lee", "authors": "Ho Hin Lee, Yucheng Tang, Shunxing Bao, Richard G. Abramson, Yuankai\n  Huo, Bennett A. Landman", "title": "RAP-Net: Coarse-to-Fine Multi-Organ Segmentation with Single Random\n  Anatomical Prior", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing coarse-to-fine abdominal multi-organ segmentation facilitates to\nextract high-resolution segmentation minimizing the lost of spatial contextual\ninformation. However, current coarse-to-refine approaches require a significant\nnumber of models to perform single organ refine segmentation corresponding to\nthe extracted organ region of interest (ROI). We propose a coarse-to-fine\npipeline, which starts from the extraction of the global prior context of\nmultiple organs from 3D volumes using a low-resolution coarse network, followed\nby a fine phase that uses a single refined model to segment all abdominal\norgans instead of multiple organ corresponding models. We combine the\nanatomical prior with corresponding extracted patches to preserve the\nanatomical locations and boundary information for performing high-resolution\nsegmentation across all organs in a single model. To train and evaluate our\nmethod, a clinical research cohort consisting of 100 patient volumes with 13\norgans well-annotated is used. We tested our algorithms with 4-fold\ncross-validation and computed the Dice score for evaluating the segmentation\nperformance of the 13 organs. Our proposed method using single auto-context\noutperforms the state-of-the-art on 13 models with an average Dice score 84.58%\nversus 81.69% (p<0.0001).\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 00:22:05 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 01:43:14 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Lee", "Ho Hin", ""], ["Tang", "Yucheng", ""], ["Bao", "Shunxing", ""], ["Abramson", "Richard G.", ""], ["Huo", "Yuankai", ""], ["Landman", "Bennett A.", ""]]}, {"id": "2012.12432", "submitter": "Ho Hin Lee", "authors": "Ho Hin Lee, Yucheng Tang, Kaiwen Xu, Shunxing Bao, Agnes B. Fogo,\n  Raymond Harris, Mark P. de Caestecker, Mattias Heinrich, Jeffrey M.\n  Spraggins, Yuankai Huo, Bennett A. Landman", "title": "Multi-Contrast Computed Tomography Healthy Kidney Atlas", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of three-dimensional multi-modal tissue maps provides an\nopportunity to spur interdisciplinary innovations across temporal and spatial\nscales through information integration. While the preponderance of effort is\nallocated to the cellular level and explore the changes in cell interactions\nand organizations, contextualizing findings within organs and systems is\nessential to visualize and interpret higher resolution linkage across scales.\nThere is a substantial normal variation of kidney morphometry and appearance\nacross body size, sex, and imaging protocols in abdominal computed tomography\n(CT). A volumetric atlas framework is needed to integrate and visualize the\nvariability across scales. However, there is no abdominal and retroperitoneal\norgans atlas framework for multi-contrast CT. Hence, we proposed a\nhigh-resolution CT retroperitoneal atlas specifically optimized for the kidney\nacross non-contrast CT and early arterial, late arterial, venous and delayed\ncontrast enhanced CT. Briefly, we introduce a deep learning-based volume of\ninterest extraction method and an automated two-stage hierarchal registration\npipeline to register abdominal volumes to a high-resolution CT atlas template.\nTo generate and evaluate the atlas, multi-contrast modality CT scans of 500\nsubjects (without reported history of renal disease, age: 15-50 years, 250\nmales & 250 females) were processed. We demonstrate a stable generalizability\nof the atlas template for integrating the normal kidney variation from small to\nlarge, across contrast modalities and populations with great variability of\ndemographics. The linkage of atlas and demographics provided a better\nunderstanding of the variation of kidney anatomy across populations.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 01:03:16 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 01:44:04 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Lee", "Ho Hin", ""], ["Tang", "Yucheng", ""], ["Xu", "Kaiwen", ""], ["Bao", "Shunxing", ""], ["Fogo", "Agnes B.", ""], ["Harris", "Raymond", ""], ["de Caestecker", "Mark P.", ""], ["Heinrich", "Mattias", ""], ["Spraggins", "Jeffrey M.", ""], ["Huo", "Yuankai", ""], ["Landman", "Bennett A.", ""]]}, {"id": "2012.12437", "submitter": "Julieta Martinez", "authors": "Julieta Martinez, Sasha Doubov, Jack Fan, Ioan Andrei B\\^arsan,\n  Shenlong Wang, Gell\\'ert M\\'attyus, Raquel Urtasun", "title": "Pit30M: A Benchmark for Global Localization in the Age of Self-Driving\n  Cars", "comments": "Published at IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in understanding whether retrieval-based localization\napproaches are good enough in the context of self-driving vehicles. Towards\nthis goal, we introduce Pit30M, a new image and LiDAR dataset with over 30\nmillion frames, which is 10 to 100 times larger than those used in previous\nwork. Pit30M is captured under diverse conditions (i.e., season, weather, time\nof the day, traffic), and provides accurate localization ground truth. We also\nautomatically annotate our dataset with historical weather and astronomical\ndata, as well as with image and LiDAR semantic segmentation as a proxy measure\nfor occlusion. We benchmark multiple existing methods for image and LiDAR\nretrieval and, in the process, introduce a simple, yet effective convolutional\nnetwork-based LiDAR retrieval method that is competitive with the state of the\nart. Our work provides, for the first time, a benchmark for sub-metre\nretrieval-based localization at city scale. The dataset, additional\nexperimental results, as well as more information about the sensors,\ncalibration, and metadata, are available on the project website:\nhttps://uber.com/atg/datasets/pit30m\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 01:24:41 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Martinez", "Julieta", ""], ["Doubov", "Sasha", ""], ["Fan", "Jack", ""], ["B\u00e2rsan", "Ioan Andrei", ""], ["Wang", "Shenlong", ""], ["M\u00e1ttyus", "Gell\u00e9rt", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2012.12440", "submitter": "Shilong Shen", "authors": "Shilong Shen", "title": "Correspondence Learning for Controllable Person Image Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a generative model for controllable person image synthesis,as\nshown in Figure , which can be applied to pose-guided person image synthesis,\n$i.e.$, converting the pose of a source person image to the target pose while\npreserving the texture of that source person image, and clothing-guided person\nimage synthesis, $i.e.$, changing the clothing texture of a source person image\nto the desired clothing texture. By explicitly establishing the dense\ncorrespondence between the target pose and the source image, we can effectively\naddress the misalignment introduced by pose tranfer and generate high-quality\nimages. Specifically, we first generate the target semantic map under the\nguidence of the target pose, which can provide more accurate pose\nrepresentation and structural constraints during the generation process. Then,\ndecomposed attribute encoder is used to extract the component features, which\nnot only helps to establish a more accurate dense correspondence, but also\nrealizes the clothing-guided person generation. After that, we will establish a\ndense correspondence between the target pose and the source image within the\nsharded domain. The source image feature is warped according to the dense\ncorrespondence to flexibly account for deformations. Finally, the network\nrenders image based on the warped source image feature and the target pose.\nExperimental results show that our method is superior to state-of-the-art\nmethods in pose-guided person generation and its effectiveness in\nclothing-guided person generation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 01:35:00 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Shen", "Shilong", ""]]}, {"id": "2012.12445", "submitter": "Bo Wu", "authors": "Bo Wu, Bo Lang", "title": "MG-SAGC: A multiscale graph and its self-adaptive graph convolution\n  network for 3D point clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enhance the ability of neural networks to extract local point cloud\nfeatures and improve their quality, in this paper, we propose a multiscale\ngraph generation method and a self-adaptive graph convolution method. First, we\npropose a multiscale graph generation method for point clouds. This approach\ntransforms point clouds into a structured multiscale graph form that supports\nmultiscale analysis of point clouds in the scale space and can obtain the\ndimensional features of point cloud data at different scales, thus making it\neasier to obtain the best point cloud features. Because traditional\nconvolutional neural networks are not applicable to graph data with irregular\nvertex neighborhoods, this paper presents an sef-adaptive graph convolution\nkernel that uses the Chebyshev polynomial to fit an irregular convolution\nfilter based on the theory of optimal approximation. In this paper, we adopt\nmax pooling to synthesize the features of different scale maps and generate the\npoint cloud features. In experiments conducted on three widely used public\ndatasets, the proposed method significantly outperforms other state-of-the-art\nmodels, demonstrating its effectiveness and generalizability.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 01:58:41 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Wu", "Bo", ""], ["Lang", "Bo", ""]]}, {"id": "2012.12447", "submitter": "Jie Li", "authors": "Jie Li, Binglin Li, Min Gao", "title": "Skeleton-based Approaches based on Machine Vision: A Survey", "comments": "10 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, skeleton-based approaches have achieved rapid progress on the basis\nof great success in skeleton representation. Plenty of researches focus on\nsolving specific problems according to skeleton features. Some skeleton-based\napproaches have been mentioned in several overviews on object detection as a\nnon-essential part. Nevertheless, there has not been any thorough analysis of\nskeleton-based approaches attentively. Instead of describing these techniques\nin terms of theoretical constructs, we devote to summarizing skeleton-based\napproaches with regard to application fields and given tasks as comprehensively\nas possible. This paper is conducive to further understanding of skeleton-based\napplication and dealing with particular issues.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 02:03:37 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Li", "Jie", ""], ["Li", "Binglin", ""], ["Gao", "Min", ""]]}, {"id": "2012.12453", "submitter": "Chi-Sheng Shih", "authors": "W.-Y. Hong, C.-L. Kao, Y.-H. Kuo, J.-R. Wang, W.-L. Chang and C.-S.\n  Shih", "title": "CholecSeg8k: A Semantic Segmentation Dataset for Laparoscopic\n  Cholecystectomy Based on Cholec80", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computer-assisted surgery has been developed to enhance surgery correctness\nand safety. However, researchers and engineers suffer from limited annotated\ndata to develop and train better algorithms. Consequently, the development of\nfundamental algorithms such as Simultaneous Localization and Mapping (SLAM) is\nlimited. This article elaborates on the efforts of preparing the dataset for\nsemantic segmentation, which is the foundation of many computer-assisted\nsurgery mechanisms. Based on the Cholec80 dataset [3], we extracted 8,080\nlaparoscopic cholecystectomy image frames from 17 video clips in Cholec80 and\nannotated the images. The dataset is named CholecSeg8K and its total size is\n3GB. Each of these images is annotated at pixel-level for thirteen classes,\nwhich are commonly founded in laparoscopic cholecystectomy surgery. CholecSeg8k\nis released under the license CC BY- NC-SA 4.0.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 02:23:15 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Hong", "W. -Y.", ""], ["Kao", "C. -L.", ""], ["Kuo", "Y. -H.", ""], ["Wang", "J. -R.", ""], ["Chang", "W. -L.", ""], ["Shih", "C. -S.", ""]]}, {"id": "2012.12473", "submitter": "Parisa Ghane", "authors": "Parisa Ghane, Narges Zarnaghi Naghsh, Ulisses Braga-Neto", "title": "Comparison of Classification Algorithms Towards Subject-Specific and\n  Subject-Independent BCI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motor imagery brain computer interface designs are considered difficult due\nto limitations in subject-specific data collection and calibration, as well as\ndemanding system adaptation requirements. Recently, subject-independent (SI)\ndesigns received attention because of their possible applicability to multiple\nusers without prior calibration and rigorous system adaptation. SI designs are\nchallenging and have shown low accuracy in the literature. Two major factors in\nsystem performance are the classification algorithm and the quality of\navailable data. This paper presents a comparative study of classification\nperformance for both SS and SI paradigms. Our results show that classification\nalgorithms for SS models display large variance in performance. Therefore,\ndistinct classification algorithms per subject may be required. SI models\ndisplay lower variance in performance but should only be used if a relatively\nlarge sample size is available. For SI models, LDA and CART had the highest\naccuracy for small and moderate sample size, respectively, whereas we\nhypothesize that SVM would be superior to the other classifiers if large\ntraining sample-size was available. Additionally, one should choose the design\napproach considering the users. While the SS design sound more promising for a\nspecific subject, an SI approach can be more convenient for mentally or\nphysically challenged users.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 04:05:20 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 15:05:54 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Ghane", "Parisa", ""], ["Naghsh", "Narges Zarnaghi", ""], ["Braga-Neto", "Ulisses", ""]]}, {"id": "2012.12477", "submitter": "Mohamed Abdelsalam", "authors": "Mohamed Abdelsalam, Mojtaba Faramarzi, Shagun Sodhani, Sarath Chandar", "title": "IIRC: Incremental Implicitly-Refined Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We introduce the \"Incremental Implicitly-Refined Classi-fication (IIRC)\"\nsetup, an extension to the class incremental learning setup where the incoming\nbatches of classes have two granularity levels. i.e., each sample could have a\nhigh-level (coarse) label like \"bear\" and a low-level (fine) label like \"polar\nbear\". Only one label is provided at a time, and the model has to figure out\nthe other label if it has already learnfed it. This setup is more aligned with\nreal-life scenarios, where a learner usually interacts with the same family of\nentities multiple times, discovers more granularity about them, while still\ntrying not to forget previous knowledge. Moreover, this setup enables\nevaluating models for some important lifelong learning challenges that cannot\nbe easily addressed under the existing setups. These challenges can be\nmotivated by the example \"if a model was trained on the class bear in one task\nand on polar bear in another task, will it forget the concept of bear, will it\nrightfully infer that a polar bear is still a bear? and will it wrongfully\nassociate the label of polar bear to other breeds of bear?\". We develop a\nstandardized benchmark that enables evaluating models on the IIRC setup. We\nevaluate several state-of-the-art lifelong learning algorithms and highlight\ntheir strengths and limitations. For example, distillation-based methods\nperform relatively well but are prone to incorrectly predicting too many labels\nper image. We hope that the proposed setup, along with the benchmark, would\nprovide a meaningful problem setting to the practitioners\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 04:21:01 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 21:58:52 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Abdelsalam", "Mohamed", ""], ["Faramarzi", "Mojtaba", ""], ["Sodhani", "Shagun", ""], ["Chandar", "Sarath", ""]]}, {"id": "2012.12481", "submitter": "Huayu Li", "authors": "Huayu Li, Haiyu Wu, Xiwen Chen, Hanning Zhang, and Abolfazl Razi", "title": "Towards Boosting the Channel Attention in Real Image Denoising :\n  Sub-band Pyramid Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional layers in Artificial Neural Networks (ANN) treat the channel\nfeatures equally without feature selection flexibility. While using ANNs for\nimage denoising in real-world applications with unknown noise distributions,\nparticularly structured noise with learnable patterns, modeling informative\nfeatures can substantially boost the performance. Channel attention methods in\nreal image denoising tasks exploit dependencies between the feature channels,\nhence being a frequency component filtering mechanism. Existing channel\nattention modules typically use global statics as descriptors to learn the\ninter-channel correlations. This method deems inefficient at learning\nrepresentative coefficients for re-scaling the channels in frequency level.\nThis paper proposes a novel Sub-band Pyramid Attention (SPA) based on wavelet\nsub-band pyramid to recalibrate the frequency components of the extracted\nfeatures in a more fine-grained fashion. We equip the SPA blocks on a network\ndesigned for real image denoising. Experimental results show that the proposed\nmethod achieves a remarkable improvement than the benchmark naive channel\nattention block. Furthermore, our results show how the pyramid level affects\nthe performance of the SPA blocks and exhibits favorable generalization\ncapability for the SPA blocks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 04:28:33 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Li", "Huayu", ""], ["Wu", "Haiyu", ""], ["Chen", "Xiwen", ""], ["Zhang", "Hanning", ""], ["Razi", "Abolfazl", ""]]}, {"id": "2012.12482", "submitter": "Shahira Abousamra", "authors": "Shahira Abousamra and Minh Hoai and Dimitris Samaras and Chao Chen", "title": "Localization in the Crowd with Topological Constraints", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of crowd localization, i.e., the prediction of dots\ncorresponding to people in a crowded scene. Due to various challenges, a\nlocalization method is prone to spatial semantic errors, i.e., predicting\nmultiple dots within a same person or collapsing multiple dots in a cluttered\nregion. We propose a topological approach targeting these semantic errors. We\nintroduce a topological constraint that teaches the model to reason about the\nspatial arrangement of dots. To enforce this constraint, we define a\npersistence loss based on the theory of persistent homology. The loss compares\nthe topographic landscape of the likelihood map and the topology of the ground\ntruth. Topological reasoning improves the quality of the localization algorithm\nespecially near cluttered regions. On multiple public benchmarks, our method\noutperforms previous localization methods. Additionally, we demonstrate the\npotential of our method in improving the performance in the crowd counting\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 04:33:48 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Abousamra", "Shahira", ""], ["Hoai", "Minh", ""], ["Samaras", "Dimitris", ""], ["Chen", "Chao", ""]]}, {"id": "2012.12496", "submitter": "Zichang He", "authors": "Zichang He, Bo Zhao, Zheng Zhang", "title": "Active Sampling for Accelerated MRI with Low-Rank Tensors", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Magnetic resonance imaging (MRI) is a powerful imaging modality that\nrevolutionizes medicine and biology. The imaging speed of high-dimensional MRI\nis often limited, which constrains its practical utility. Recently, low-rank\ntensor models have been exploited to enable fast MR imaging with sparse\nsampling. Most existing methods use some pre-defined sampling design, and\nactive sensing has not been explored for low-rank tensor imaging. In this\npaper, we introduce an active low-rank tensor model for fast MR imaging. We\npropose an active sampling method based on a Query-by-Committee model, making\nuse of the benefits of low-rank tensor structure. Numerical experiments on a\n3-D MRI data set demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 05:35:04 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 20:55:29 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["He", "Zichang", ""], ["Zhao", "Bo", ""], ["Zhang", "Zheng", ""]]}, {"id": "2012.12502", "submitter": "Pengtao Xie", "authors": "Xuefeng Du, Pengtao Xie", "title": "Small-Group Learning, with Application to Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In human learning, an effective learning methodology is small-group learning:\na small group of students work together towards the same learning objective,\nwhere they express their understanding of a topic to their peers, compare their\nideas, and help each other to trouble-shoot problems. In this paper, we aim to\ninvestigate whether this human learning method can be borrowed to train better\nmachine learning models, by developing a novel ML framework -- small-group\nlearning (SGL). In our framework, a group of learners (ML models) with\ndifferent model architectures collaboratively help each other to learn by\nleveraging their complementary advantages. Specifically, each learner uses its\nintermediately trained model to generate a pseudo-labeled dataset and re-trains\nits model using pseudo-labeled datasets generated by other learners. SGL is\nformulated as a multi-level optimization framework consisting of three learning\nstages: each learner trains a model independently and uses this model to\nperform pseudo-labeling; each learner trains another model using datasets\npseudo-labeled by other learners; learners improve their architectures by\nminimizing validation losses. An efficient algorithm is developed to solve the\nmulti-level optimization problem. We apply SGL for neural architecture search.\nResults on CIFAR-100, CIFAR-10, and ImageNet demonstrate the effectiveness of\nour method.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 05:56:47 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 03:38:50 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Du", "Xuefeng", ""], ["Xie", "Pengtao", ""]]}, {"id": "2012.12507", "submitter": "Se Young Chun", "authors": "Dongwon Park, Dong Un Kang, Se Young Chun", "title": "Blur More To Deblur Better: Multi-Blur2Deblur For Efficient Video\n  Deblurring", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key components for video deblurring is how to exploit neighboring\nframes. Recent state-of-the-art methods either used aligned adjacent frames to\nthe center frame or propagated the information on past frames to the current\nframe recurrently. Here we propose multi-blur-to-deblur (MB2D), a novel concept\nto exploit neighboring frames for efficient video deblurring. Firstly, inspired\nby unsharp masking, we argue that using more blurred images with long exposures\nas additional inputs significantly improves performance. Secondly, we propose\nmulti-blurring recurrent neural network (MBRNN) that can synthesize more\nblurred images from neighboring frames, yielding substantially improved\nperformance with existing video deblurring methods. Lastly, we propose\nmulti-scale deblurring with connecting recurrent feature map from MBRNN (MSDR)\nto achieve state-of-the-art performance on the popular GoPro and Su datasets in\nfast and memory efficient ways.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 06:17:31 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Park", "Dongwon", ""], ["Kang", "Dong Un", ""], ["Chun", "Se Young", ""]]}, {"id": "2012.12509", "submitter": "Sheng Huang", "authors": "Fengtao Zhou and Sheng Huang and Yun Xing", "title": "Deep Semantic Dictionary Learning for Multi-label Image Classification", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Compared with single-label image classification, multi-label image\nclassification is more practical and challenging. Some recent studies attempted\nto leverage the semantic information of categories for improving multi-label\nimage classification performance. However, these semantic-based methods only\ntake semantic information as type of complements for visual representation\nwithout further exploitation. In this paper, we present an innovative path\ntowards the solution of the multi-label image classification which considers it\nas a dictionary learning task. A novel end-to-end model named Deep Semantic\nDictionary Learning (DSDL) is designed. In DSDL, an auto-encoder is applied to\ngenerate the semantic dictionary from class-level semantics and then such\ndictionary is utilized for representing the visual features extracted by\nConvolutional Neural Network (CNN) with label embeddings. The DSDL provides a\nsimple but elegant way to exploit and reconcile the label, semantic and visual\nspaces simultaneously via conducting the dictionary learning among them.\nMoreover, inspired by iterative optimization of traditional dictionary\nlearning, we further devise a novel training strategy named Alternately\nParameters Update Strategy (APUS) for optimizing DSDL, which alternately\noptimizes the representation coefficients and the semantic dictionary in\nforward and backward propagation. Extensive experimental results on three\npopular benchmarks demonstrate that our method achieves promising performances\nin comparison with the state-of-the-arts. Our codes and models have been\nreleased at {https://github.com/ZFT-CQU/DSDL}.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 06:22:47 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 12:22:09 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Zhou", "Fengtao", ""], ["Huang", "Sheng", ""], ["Xing", "Yun", ""]]}, {"id": "2012.12510", "submitter": "Xiao Ma", "authors": "Daisheng Jin, Xiao Ma, Chongzhi Zhang, Yizhuo Zhou, Jiashu Tao,\n  Mingyuan Zhang, Haiyu Zhao, Shuai Yi, Zhoujun Li, Xianglong Liu, Hongsheng Li", "title": "Towards Overcoming False Positives in Visual Relationship Detection", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we investigate the cause of the high false positive rate in\nVisual Relationship Detection (VRD). We observe that during training, the\nrelationship proposal distribution is highly imbalanced: most of the negative\nrelationship proposals are easy to identify, e.g., the inaccurate object\ndetection, which leads to the under-fitting of low-frequency difficult\nproposals. This paper presents Spatially-Aware Balanced negative pRoposal\nsAmpling (SABRA), a robust VRD framework that alleviates the influence of false\npositives. To effectively optimize the model under imbalanced distribution,\nSABRA adopts Balanced Negative Proposal Sampling (BNPS) strategy for mini-batch\nsampling. BNPS divides proposals into 5 well defined sub-classes and generates\na balanced training distribution according to the inverse frequency. BNPS gives\nan easier optimization landscape and significantly reduces the number of false\npositives. To further resolve the low-frequency challenging false positive\nproposals with high spatial ambiguity, we improve the spatial modeling ability\nof SABRA on two aspects: a simple and efficient multi-head heterogeneous graph\nattention network (MH-GAT) that models the global spatial interactions of\nobjects, and a spatial mask decoder that learns the local spatial\nconfiguration. SABRA outperforms SOTA methods by a large margin on two\nhuman-object interaction (HOI) datasets and one general VRD dataset.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 06:28:00 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 12:06:11 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Jin", "Daisheng", ""], ["Ma", "Xiao", ""], ["Zhang", "Chongzhi", ""], ["Zhou", "Yizhuo", ""], ["Tao", "Jiashu", ""], ["Zhang", "Mingyuan", ""], ["Zhao", "Haiyu", ""], ["Yi", "Shuai", ""], ["Li", "Zhoujun", ""], ["Liu", "Xianglong", ""], ["Li", "Hongsheng", ""]]}, {"id": "2012.12515", "submitter": "Mohammed Elmogy Dr.", "authors": "Eman AbdelMaksoud, Sherif Barakat, and Mohammed Elmogy", "title": "Diabetic Retinopathy Grading System Based on Transfer Learning", "comments": "6 pages, 5 figures, 3 tables, none", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Much effort is being made by the researchers in order to detect and diagnose\ndiabetic retinopathy (DR) accurately automatically. The disease is very\ndangerous as it can cause blindness suddenly if it is not continuously\nscreened. Therefore, many computers aided diagnosis (CAD) systems have been\ndeveloped to diagnose the various DR grades. Recently, many CAD systems based\non deep learning (DL) methods have been adopted to get deep learning merits in\ndiagnosing the pathological abnormalities of DR disease. In this paper, we\npresent a full based-DL CAD system depending on multi-label classification. In\nthe proposed DL CAD system, we present a customized efficientNet model in order\nto diagnose the early and advanced grades of the DR disease. Learning transfer\nis very useful in training small datasets. We utilized IDRiD dataset. It is a\nmulti-label dataset. The experiments manifest that the proposed DL CAD system\nis robust, reliable, and deigns promising results in detecting and grading DR.\nThe proposed system achieved accuracy (ACC) equals 86%, and the Dice similarity\ncoefficient (DSC) equals 78.45.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 07:02:36 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["AbdelMaksoud", "Eman", ""], ["Barakat", "Sherif", ""], ["Elmogy", "Mohammed", ""]]}, {"id": "2012.12516", "submitter": "Uday Singh Saini", "authors": "Uday Singh Saini, Evangelos E. Papalexakis", "title": "Analyzing Representations inside Convolutional Neural Networks", "comments": "Work in Progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How can we discover and succinctly summarize the concepts that a neural\nnetwork has learned? Such a task is of great importance in applications of\nnetworks in areas of inference that involve classification, like medical\ndiagnosis based on fMRI/x-ray etc. In this work, we propose a framework to\ncategorize the concepts a network learns based on the way it clusters a set of\ninput examples, clusters neurons based on the examples they activate for, and\ninput features all in the same latent space. This framework is unsupervised and\ncan work without any labels for input features, it only needs access to\ninternal activations of the network for each input example, thereby making it\nwidely applicable. We extensively evaluate the proposed method and demonstrate\nthat it produces human-understandable and coherent concepts that a ResNet-18\nhas learned on the CIFAR-100 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 07:10:17 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Saini", "Uday Singh", ""], ["Papalexakis", "Evangelos E.", ""]]}, {"id": "2012.12519", "submitter": "Zhijun Hu", "authors": "Zhijun Hu, Yong Xu, Jie Wen, Lilei Sun, Raja S P", "title": "Vehicle Re-identification Based on Dual Distance Center Loss", "comments": "30 pages 13figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, deep learning has been widely used in the field of vehicle\nre-identification. When training a deep model, softmax loss is usually used as\na supervision tool. However, the softmax loss performs well for closed-set\ntasks, but not very well for open-set tasks. In this paper, we sum up five\nshortcomings of center loss and solved all of them by proposing a dual distance\ncenter loss (DDCL). Especially we solve the shortcoming that center loss must\ncombine with the softmax loss to supervise training the model, which provides\nus with a new perspective to examine the center loss. In addition, we verify\nthe inconsistency between the proposed DDCL and softmax loss in the feature\nspace, which makes the center loss no longer be limited by the softmax loss in\nthe feature space after removing the softmax loss. To be specifically, we add\nthe Pearson distance on the basis of the Euclidean distance to the same center,\nwhich makes all features of the same class be confined to the intersection of a\nhypersphere and a hypercube in the feature space. The proposed Pearson distance\nstrengthens the intra-class compactness of the center loss and enhances the\ngeneralization ability of center loss. Moreover, by designing a Euclidean\ndistance threshold between all center pairs, which not only strengthens the\ninter-class separability of center loss, but also makes the center loss (or\nDDCL) works well without the combination of softmax loss. We apply DDCL in the\nfield of vehicle re-identification named VeRi-776 dataset and VehicleID\ndataset. And in order to verify its good generalization ability, we also verify\nit in two datasets commonly used in the field of person re-identification named\nMSMT17 dataset and Market1501 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 07:14:53 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Hu", "Zhijun", ""], ["Xu", "Yong", ""], ["Wen", "Jie", ""], ["Sun", "Lilei", ""], ["P", "Raja S", ""]]}, {"id": "2012.12528", "submitter": "Asaf Shabtai", "authors": "Alon Zolfi and Moshe Kravchik and Yuval Elovici and Asaf Shabtai", "title": "The Translucent Patch: A Physical and Universal Attack on Object\n  Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical adversarial attacks against object detectors have seen increasing\nsuccess in recent years. However, these attacks require direct access to the\nobject of interest in order to apply a physical patch. Furthermore, to hide\nmultiple objects, an adversarial patch must be applied to each object. In this\npaper, we propose a contactless translucent physical patch containing a\ncarefully constructed pattern, which is placed on the camera's lens, to fool\nstate-of-the-art object detectors. The primary goal of our patch is to hide all\ninstances of a selected target class. In addition, the optimization method used\nto construct the patch aims to ensure that the detection of other (untargeted)\nclasses remains unharmed. Therefore, in our experiments, which are conducted on\nstate-of-the-art object detection models used in autonomous driving, we study\nthe effect of the patch on the detection of both the selected target class and\nthe other classes. We show that our patch was able to prevent the detection of\n42.27% of all stop sign instances while maintaining high (nearly 80%) detection\nof the other classes.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 07:47:13 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Zolfi", "Alon", ""], ["Kravchik", "Moshe", ""], ["Elovici", "Yuval", ""], ["Shabtai", "Asaf", ""]]}, {"id": "2012.12535", "submitter": "Hongtao Kang", "authors": "Hongtao Kang, Die Luo, Weihua Feng, Junbo Hu, Shaoqun Zeng, Tingwei\n  Quan, and Xiuli Liu", "title": "StainNet: a fast and robust stain normalization network", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stain normalization often refers to transferring the color distribution of\nthe source image to that of the target image and has been widely used in\nbiomedical image analysis. The conventional stain normalization is regarded as\nconstructing a pixel-by-pixel color mapping model, which only depends on one\nreference image, and can not accurately achieve the style transformation\nbetween image datasets. In principle, this style transformation can be well\nsolved by the deep learning-based methods due to its complicated network\nstructure, whereas, its complicated structure results in the low computational\nefficiency and artifacts in the style transformation, which has restricted the\npractical application. Here, we use distillation learning to reduce the\ncomplexity of deep learning methods and a fast and robust network called\nStainNet to learn the color mapping between the source image and target image.\nStainNet can learn the color mapping relationship from a whole dataset and\nadjust the color value in a pixel-to-pixel manner. The pixel-to-pixel manner\nrestricts the network size and avoids artifacts in the style transformation.\nThe results on the cytopathology and histopathology datasets show that StainNet\ncan achieve comparable performance to the deep learning-based methods.\nComputation results demonstrate StainNet is more than 40 times faster than\nStainGAN and can normalize a 100,000x100,000 whole slide image in 40 seconds.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 08:16:27 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 13:33:28 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 07:38:31 GMT"}, {"version": "v4", "created": "Wed, 13 Jan 2021 05:49:19 GMT"}, {"version": "v5", "created": "Sat, 16 Jan 2021 05:26:38 GMT"}, {"version": "v6", "created": "Sat, 24 Jul 2021 01:41:38 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Kang", "Hongtao", ""], ["Luo", "Die", ""], ["Feng", "Weihua", ""], ["Hu", "Junbo", ""], ["Zeng", "Shaoqun", ""], ["Quan", "Tingwei", ""], ["Liu", "Xiuli", ""]]}, {"id": "2012.12545", "submitter": "Suhyeon Lee", "authors": "Suhyeon Lee, Junhyuk Hyun, Hongje Seong, Euntai Kim", "title": "Unsupervised Domain Adaptation for Semantic Segmentation by Content\n  Transfer", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the unsupervised domain adaptation (UDA) for\nsemantic segmentation, which aims to segment the unlabeled real data using\nlabeled synthetic data. The main problem of UDA for semantic segmentation\nrelies on reducing the domain gap between the real image and synthetic image.\nTo solve this problem, we focused on separating information in an image into\ncontent and style. Here, only the content has cues for semantic segmentation,\nand the style makes the domain gap. Thus, precise separation of content and\nstyle in an image leads to effect as supervision of real data even when\nlearning with synthetic data. To make the best of this effect, we propose a\nzero-style loss. Even though we perfectly extract content for semantic\nsegmentation in the real domain, another main challenge, the class imbalance\nproblem, still exists in UDA for semantic segmentation. We address this problem\nby transferring the contents of tail classes from synthetic to real domain.\nExperimental results show that the proposed method achieves the\nstate-of-the-art performance in semantic segmentation on the major two UDA\nsettings.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 09:01:00 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Lee", "Suhyeon", ""], ["Hyun", "Junhyuk", ""], ["Seong", "Hongje", ""], ["Kim", "Euntai", ""]]}, {"id": "2012.12554", "submitter": "Alina Kuznetsova", "authors": "A. Kuznetsova, A. Talati, Y. Luo, K. Simmons and V. Ferrari", "title": "Efficient video annotation with visual interpolation and frame selection\n  guidance", "comments": "accepted to WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a unified framework for generic video annotation with bounding\nboxes. Video annotation is a longstanding problem, as it is a tedious and\ntime-consuming process. We tackle two important challenges of video annotation:\n(1) automatic temporal interpolation and extrapolation of bounding boxes\nprovided by a human annotator on a subset of all frames, and (2) automatic\nselection of frames to annotate manually. Our contribution is two-fold: first,\nwe propose a model that has both interpolating and extrapolating capabilities;\nsecond, we propose a guiding mechanism that sequentially generates suggestions\nfor what frame to annotate next, based on the annotations made previously. We\nextensively evaluate our approach on several challenging datasets in simulation\nand demonstrate a reduction in terms of the number of manual bounding boxes\ndrawn by 60% over linear interpolation and by 35% over an off-the-shelf\ntracker. Moreover, we also show 10% annotation time improvement over a\nstate-of-the-art method for video annotation with bounding boxes [25]. Finally,\nwe run human annotation experiments and provide extensive analysis of the\nresults, showing that our approach reduces actual measured annotation time by\n50% compared to commonly used linear interpolation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 09:31:40 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Kuznetsova", "A.", ""], ["Talati", "A.", ""], ["Luo", "Y.", ""], ["Simmons", "K.", ""], ["Ferrari", "V.", ""]]}, {"id": "2012.12556", "submitter": "Kai Han", "authors": "Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua\n  Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang,\n  Dacheng Tao", "title": "A Survey on Visual Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer, first applied to the field of natural language processing, is a\ntype of deep neural network mainly based on the self-attention mechanism.\nThanks to its strong representation capabilities, researchers are looking at\nways to apply transformer to computer vision tasks. In a variety of visual\nbenchmarks, transformer-based models perform similar to or better than other\ntypes of networks such as convolutional and recurrent networks. Given its high\nperformance and no need for human-defined inductive bias, transformer is\nreceiving more and more attention from the computer vision community. In this\npaper, we review these visual transformer models by categorizing them in\ndifferent tasks and analyzing their advantages and disadvantages. The main\ncategories we explore include the backbone network, high/mid-level vision,\nlow-level vision, and video processing. We also take a brief look at the\nself-attention mechanism in computer vision, as it is the base component in\ntransformer. Furthermore, we include efficient transformer methods for pushing\ntransformer into real device-based applications. Toward the end of this paper,\nwe discuss the challenges and provide several further research directions for\nvisual transformers.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 09:37:54 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 07:09:36 GMT"}, {"version": "v3", "created": "Sat, 30 Jan 2021 09:33:55 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Han", "Kai", ""], ["Wang", "Yunhe", ""], ["Chen", "Hanting", ""], ["Chen", "Xinghao", ""], ["Guo", "Jianyuan", ""], ["Liu", "Zhenhua", ""], ["Tang", "Yehui", ""], ["Xiao", "An", ""], ["Xu", "Chunjing", ""], ["Xu", "Yixing", ""], ["Yang", "Zhaohui", ""], ["Zhang", "Yiman", ""], ["Tao", "Dacheng", ""]]}, {"id": "2012.12558", "submitter": "Jin Liu", "authors": "Jin Liu, Jianqin Yin", "title": "Multi-grained Trajectory Graph Convolutional Networks for\n  Habit-unrelated Human Motion Prediction", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion prediction is an essential part for human-robot collaboration.\nUnlike most of the existing methods mainly focusing on improving the\neffectiveness of spatiotemporal modeling for accurate prediction, we take\neffectiveness and efficiency into consideration, aiming at the prediction\nquality, computational efficiency and the lightweight of the model. A\nmulti-grained trajectory graph convolutional networks based and lightweight\nframework is proposed for habit-unrelated human motion prediction.\nSpecifically, we represent human motion as multi-grained trajectories,\nincluding joint trajectory and sub-joint trajectory. Based on the advanced\nrepresentation, multi-grained trajectory graph convolutional networks are\nproposed to explore the spatiotemporal dependencies at the multiple\ngranularities. Moreover, considering the right-handedness habit of the vast\nmajority of people, a new motion generation method is proposed to generate the\nmotion with left-handedness, to better model the motion with less bias to the\nhuman habit. Experimental results on challenging datasets, including Human3.6M\nand CMU Mocap, show that the proposed model outperforms state-of-the-art with\nless than 0.12 times parameters, which demonstrates the effectiveness and\nefficiency of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 09:41:50 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Liu", "Jin", ""], ["Yin", "Jianqin", ""]]}, {"id": "2012.12561", "submitter": "Shouju Wang", "authors": "Jiulou Zhang, Yuxia Tang, Shouju Wang", "title": "GANDA: A deep generative adversarial network predicts the spatial\n  distribution of nanoparticles in tumor pixelly", "comments": "28 pages, 17 figures, 3 tables", "journal-ref": null, "doi": "10.1016/j.jconrel.2021.06.039", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Intratumoral nanoparticles (NPs) distribution is critical for the success of\nnanomedicine in imaging and treatment, but computational models to describe the\nNPs distribution remain unavailable due to the complex tumor-nano interactions.\nHere, we develop a Generative Adversarial Network for Distribution Analysis\n(GANDA) to describe and conditionally generates the intratumoral quantum dots\n(QDs) distribution after i.v. injection. This deep generative model is trained\nautomatically by 27 775 patches of tumor vessels and cell nuclei decomposed\nfrom whole-slide images of 4T1 breast cancer sections. The GANDA model can\nconditionally generate images of intratumoral QDs distribution under the\nconstraint of given tumor vessels and cell nuclei channels with the same\nspatial resolution (pixels-to-pixels), minimal loss (mean squared error, MSE =\n1.871) and excellent reliability (intraclass correlation, ICC = 0.94).\nQuantitative analysis of QDs extravasation distance (ICC = 0.95) and subarea\ndistribution (ICC = 0.99) is allowed on the generated images without knowing\nthe real QDs distribution. We believe this deep generative model may provide\nopportunities to investigate how influencing factors affect NPs distribution in\nindividual tumors and guide nanomedicine optimization for molecular imaging and\npersonalized treatment.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 09:47:07 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 09:13:22 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhang", "Jiulou", ""], ["Tang", "Yuxia", ""], ["Wang", "Shouju", ""]]}, {"id": "2012.12570", "submitter": "Guodong Zeng", "authors": "Guodong Zeng, Till D. Lerch, Florian Schmaranzer, Guoyan Zheng,\n  Juergen Burger, Kate Gerber, Moritz Tannast, Klaus Siebenrock, Nicolas Gerber", "title": "ICMSC: Intra- and Cross-modality Semantic Consistency for Unsupervised\n  Domain Adaptation on Hip Joint Bone Segmentation", "comments": "12 pages, 3 figures, submitted to IPMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unsupervised domain adaptation (UDA) for cross-modality medical image\nsegmentation has shown great progress by domain-invariant feature learning or\nimage appearance translation. Adapted feature learning usually cannot detect\ndomain shifts at the pixel level and is not able to achieve good results in\ndense semantic segmentation tasks. Image appearance translation, e.g. CycleGAN,\ntranslates images into different styles with good appearance, despite its\npopulation, its semantic consistency is hardly to maintain and results in poor\ncross-modality segmentation. In this paper, we propose intra- and\ncross-modality semantic consistency (ICMSC) for UDA and our key insight is that\nthe segmentation of synthesised images in different styles should be\nconsistent. Specifically, our model consists of an image translation module and\na domain-specific segmentation module. The image translation module is a\nstandard CycleGAN, while the segmentation module contains two domain-specific\nsegmentation networks. The intra-modality semantic consistency (IMSC) forces\nthe reconstructed image after a cycle to be segmented in the same way as the\noriginal input image, while the cross-modality semantic consistency (CMSC)\nencourages the synthesized images after translation to be segmented exactly the\nsame as before translation. Comprehensive experimental results on\ncross-modality hip joint bone segmentation show the effectiveness of our\nproposed method, which achieves an average DICE of 81.61% on the acetabulum and\n88.16% on the proximal femur, outperforming other state-of-the-art methods. It\nis worth to note that without UDA, a model trained on CT for hip joint bone\nsegmentation is non-transferable to MRI and has almost zero-DICE segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 09:58:38 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Zeng", "Guodong", ""], ["Lerch", "Till D.", ""], ["Schmaranzer", "Florian", ""], ["Zheng", "Guoyan", ""], ["Burger", "Juergen", ""], ["Gerber", "Kate", ""], ["Tannast", "Moritz", ""], ["Siebenrock", "Klaus", ""], ["Gerber", "Nicolas", ""]]}, {"id": "2012.12619", "submitter": "Zuoyu Yan", "authors": "Zuoyu Yan, Xiaode Zhang, Liangcai Gao, Ke Yuan and Zhi Tang", "title": "ConvMath: A Convolutional Sequence Network for Mathematical Expression\n  Recognition", "comments": "Accepted in ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent advances in optical character recognition (OCR),\nmathematical expressions still face a great challenge to recognize due to their\ntwo-dimensional graphical layout. In this paper, we propose a convolutional\nsequence modeling network, ConvMath, which converts the mathematical expression\ndescription in an image into a LaTeX sequence in an end-to-end way. The network\ncombines an image encoder for feature extraction and a convolutional decoder\nfor sequence generation. Compared with other Long Short Term Memory(LSTM) based\nencoder-decoder models, ConvMath is entirely based on convolution, thus it is\neasy to perform parallel computation. Besides, the network adopts multi-layer\nattention mechanism in the decoder, which allows the model to align output\nsymbols with source feature vectors automatically, and alleviates the problem\nof lacking coverage while training the model. The performance of ConvMath is\nevaluated on an open dataset named IM2LATEX-100K, including 103556 samples. The\nexperimental results demonstrate that the proposed network achieves\nstate-of-the-art accuracy and much better efficiency than previous methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 12:08:18 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Yan", "Zuoyu", ""], ["Zhang", "Xiaode", ""], ["Gao", "Liangcai", ""], ["Yuan", "Ke", ""], ["Tang", "Zhi", ""]]}, {"id": "2012.12626", "submitter": "Haoliang Sun", "authors": "Haoliang Sun, Xiantong Zhen, Chris Bailey, Parham Rasoulinejad, Yilong\n  Yin, Shuo Li", "title": "Direct Estimation of Spinal Cobb Angles by Structured Multi-Output\n  Regression", "comments": "Proceedings of International Conference on Information Processing in\n  Medical Imaging (IPMI 2017)", "journal-ref": null, "doi": "10.1007/978-3-319-59050-9", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Cobb angle that quantitatively evaluates the spinal curvature plays an\nimportant role in the scoliosis diagnosis and treatment. Conventional\nmeasurement of these angles suffers from huge variability and low reliability\ndue to intensive manual intervention. However, since there exist high ambiguity\nand variability around boundaries of vertebrae, it is challenging to obtain\nCobb angles automatically. In this paper, we formulate the estimation of the\nCobb angles from spinal X-rays as a multi-output regression task. We propose\nstructured support vector regression (S^2VR) to jointly estimate Cobb angles\nand landmarks of the spine in X-rays in one single framework. The proposed\nS^2VR can faithfully handle the nonlinear relationship between input images and\nquantitative outputs, while explicitly capturing the intrinsic correlation of\noutputs. We introduce the manifold regularization to exploit the geometry of\nthe output space. We propose learning the kernel in S2VR by kernel target\nalignment to enhance its discriminative ability. The proposed method is\nevaluated on the spinal X-rays dataset of 439 scoliosis subjects, which\nachieves the inspiring correlation coefficient of 92.76% with ground truth\nobtained manually by human experts and outperforms two baseline methods. Our\nmethod achieves the direct estimation of Cobb angles with high accuracy, which\nindicates its great potential in clinical use.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 12:33:46 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Sun", "Haoliang", ""], ["Zhen", "Xiantong", ""], ["Bailey", "Chris", ""], ["Rasoulinejad", "Parham", ""], ["Yin", "Yilong", ""], ["Li", "Shuo", ""]]}, {"id": "2012.12643", "submitter": "Ron Slossberg", "authors": "Ron Slossberg, Oron Anschel, Amir Markovitz, Ron Litman, Aviad\n  Aberdam, Shahar Tsiper, Shai Mazor, Jon Wu and R. Manmatha", "title": "On Calibration of Scene-Text Recognition Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work, we study the problem of word-level confidence calibration for\nscene-text recognition (STR). Although the topic of confidence calibration has\nbeen an active research area for the last several decades, the case of\nstructured and sequence prediction calibration has been scarcely explored. We\nanalyze several recent STR methods and show that they are consistently\noverconfident. We then focus on the calibration of STR models on the word\nrather than the character level. In particular, we demonstrate that for\nattention based decoders, calibration of individual character predictions\nincreases word-level calibration error compared to an uncalibrated model. In\naddition, we apply existing calibration methodologies as well as new\nsequence-based extensions to numerous STR models, demonstrating reduced\ncalibration error by up to a factor of nearly 7. Finally, we show consistently\nimproved accuracy results by applying our proposed sequence calibration method\nas a preprocessing step to beam-search.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 13:25:25 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Slossberg", "Ron", ""], ["Anschel", "Oron", ""], ["Markovitz", "Amir", ""], ["Litman", "Ron", ""], ["Aberdam", "Aviad", ""], ["Tsiper", "Shahar", ""], ["Mazor", "Shai", ""], ["Wu", "Jon", ""], ["Manmatha", "R.", ""]]}, {"id": "2012.12645", "submitter": "Haoyang Zhang", "authors": "Haoyang Zhang, Ying Wang, Feras Dayoub and Niko S\\\"underhauf", "title": "SWA Object Detection", "comments": "9 pages; polished", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Do you want to improve 1.0 AP for your object detector without any inference\ncost and any change to your detector? Let us tell you such a recipe. It is\nsurprisingly simple: train your detector for an extra 12 epochs using cyclical\nlearning rates and then average these 12 checkpoints as your final detection\nmodel}. This potent recipe is inspired by Stochastic Weights Averaging (SWA),\nwhich is proposed in arXiv:1803.05407 for improving generalization in deep\nneural networks. We found it also very effective in object detection. In this\ntechnique report, we systematically investigate the effects of applying SWA to\nobject detection as well as instance segmentation. Through extensive\nexperiments, we discover the aforementioned workable policy of performing SWA\nin object detection, and we consistently achieve $\\sim$1.0 AP improvement over\nvarious popular detectors on the challenging COCO benchmark, including Mask\nRCNN, Faster RCNN, RetinaNet, FCOS, YOLOv3 and VFNet. We hope this work will\nmake more researchers in object detection know this technique and help them\ntrain better object detectors. Code is available at:\nhttps://github.com/hyz-xmaster/swa_object_detection .\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 13:26:10 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 14:51:55 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 04:27:39 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Zhang", "Haoyang", ""], ["Wang", "Ying", ""], ["Dayoub", "Feras", ""], ["S\u00fcnderhauf", "Niko", ""]]}, {"id": "2012.12652", "submitter": "Paul Desbordes", "authors": "Paul Desbordes and Diksha and Benoit Macq", "title": "Prognostic Power of Texture Based Morphological Operations in a\n  Radiomics Study for Lung Cancer", "comments": "9 pages, 3 tables, 3 figures, 31 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The importance of radiomics features for predicting patient outcome is now\nwell-established. Early study of prognostic features can lead to a more\nefficient treatment personalisation. For this reason new radiomics features\nobtained through mathematical morphology-based operations are proposed. Their\nstudy is conducted on an open database of patients suffering from Nonsmall\nCells Lung Carcinoma (NSCLC). The tumor features are extracted from the CT\nimages and analyzed via PCA and a Kaplan-Meier survival analysis in order to\nselect the most relevant ones. Among the 1,589 studied features, 32 are found\nrelevant to predict patient survival: 27 classical radiomics features and five\nMM features (including both granularity and morphological covariance features).\nThese features will contribute towards the prognostic models, and eventually to\nclinical decision making and the course of treatment for patients.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 13:38:19 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Desbordes", "Paul", ""], ["Diksha", "", ""], ["Macq", "Benoit", ""]]}, {"id": "2012.12712", "submitter": "Candelaria Mosquera", "authors": "Candelaria Mosquera (1 and 2), Facundo Nahuel Diaz (3), Fernando\n  Binder (1), Jose Martin Rabellino (3), Sonia Elizabeth Benitez (1), Alejandro\n  Daniel Beres\\~nak (3), Alberto Seehaus (3), Gabriel Ducrey (3), Jorge Alberto\n  Ocantos (3) and Daniel Roberto Luna (1) ((1) Health Informatics Department\n  Hospital Italiano de Buenos Aires,(2) Universidad Tecnologica Nacional,(3)\n  Radiology Department Hospital Italiano de Buenos Aires)", "title": "Chest x-ray automated triage: a semiologic approach designed for\n  clinical implementation, exploiting different types of labels through a\n  combination of four Deep Learning architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BACKGROUND AND OBJECTIVES: The multiple chest x-ray datasets released in the\nlast years have ground-truth labels intended for different computer vision\ntasks, suggesting that performance in automated chest-xray interpretation might\nimprove by using a method that can exploit diverse types of annotations. This\nwork presents a Deep Learning method based on the late fusion of different\nconvolutional architectures, that allows training with heterogeneous data with\na simple implementation, and evaluates its performance on independent test\ndata. We focused on obtaining a clinically useful tool that could be\nsuccessfully integrated into a hospital workflow. MATERIALS AND METHODS: Based\non expert opinion, we selected four target chest x-ray findings, namely lung\nopacities, fractures, pneumothorax and pleural effusion. For each finding we\ndefined the most adequate type of ground-truth label, and built four training\ndatasets combining images from public chest x-ray datasets and our\ninstitutional archive. We trained four different Deep Learning architectures\nand combined their outputs with a late fusion strategy, obtaining a unified\ntool. Performance was measured on two test datasets: an external\nopenly-available dataset, and a retrospective institutional dataset, to\nestimate performance on local population. RESULTS: The external and local test\nsets had 4376 and 1064 images, respectively, for which the model showed an area\nunder the Receiver Operating Characteristics curve of 0.75 (95%CI: 0.74-0.76)\nand 0.87 (95%CI: 0.86-0.89) in the detection of abnormal chest x-rays. For the\nlocal population, a sensitivity of 86% (95%CI: 84-90), and a specificity of 88%\n(95%CI: 86-90) were obtained, with no significant differences between\ndemographic subgroups. We present examples of heatmaps to show the accomplished\nlevel of interpretability, examining true and false positives.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 14:38:35 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Mosquera", "Candelaria", "", "1 and 2"], ["Diaz", "Facundo Nahuel", ""], ["Binder", "Fernando", ""], ["Rabellino", "Jose Martin", ""], ["Benitez", "Sonia Elizabeth", ""], ["Beres\u00f1ak", "Alejandro Daniel", ""], ["Seehaus", "Alberto", ""], ["Ducrey", "Gabriel", ""], ["Ocantos", "Jorge Alberto", ""], ["Luna", "Daniel Roberto", ""]]}, {"id": "2012.12741", "submitter": "Wenwei Zhang", "authors": "Wenwei Zhang, Zhe Wang, Chen Change Loy", "title": "Exploring Data Augmentation for Multi-Modality 3D Object Detection", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is counter-intuitive that multi-modality methods based on point cloud and\nimages perform only marginally better or sometimes worse than approaches that\nsolely use point cloud. This paper investigates the reason behind this\nphenomenon. Due to the fact that multi-modality data augmentation must maintain\nconsistency between point cloud and images, recent methods in this field\ntypically use relatively insufficient data augmentation. This shortage makes\ntheir performance under expectation. Therefore, we contribute a pipeline, named\ntransformation flow, to bridge the gap between single and multi-modality data\naugmentation with transformation reversing and replaying. In addition,\nconsidering occlusions, a point in different modalities may be occupied by\ndifferent objects, making augmentations such as cut and paste non-trivial for\nmulti-modality detection. We further present Multi-mOdality Cut and pAste\n(MoCa), which simultaneously considers occlusion and physical plausibility to\nmaintain the multi-modality consistency. Without using ensemble of detectors,\nour multi-modality detector achieves new state-of-the-art performance on\nnuScenes dataset and competitive performance on KITTI 3D benchmark. Our method\nalso wins the best PKL award in the 3rd nuScenes detection challenge. Code and\nmodels will be released at https://github.com/open-mmlab/mmdetection3d.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 15:23:16 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 16:23:20 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Zhang", "Wenwei", ""], ["Wang", "Zhe", ""], ["Loy", "Chen Change", ""]]}, {"id": "2012.12754", "submitter": "Carlos Busso", "authors": "Sumit Jha, Carlos Busso", "title": "Estimation of Driver's Gaze Region from Head Position and Orientation\n  using Probabilistic Confidence Regions", "comments": "13 Pages, 12 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A smart vehicle should be able to understand human behavior and predict their\nactions to avoid hazardous situations. Specific traits in human behavior can be\nautomatically predicted, which can help the vehicle make decisions, increasing\nsafety. One of the most important aspects pertaining to the driving task is the\ndriver's visual attention. Predicting the driver's visual attention can help a\nvehicle understand the awareness state of the driver, providing important\ncontextual information. While estimating the exact gaze direction is difficult\nin the car environment, a coarse estimation of the visual attention can be\nobtained by tracking the position and orientation of the head. Since the\nrelation between head pose and gaze direction is not one-to-one, this paper\nproposes a formulation based on probabilistic models to create salient regions\ndescribing the visual attention of the driver. The area of the predicted region\nis small when the model has high confidence on the prediction, which is\ndirectly learned from the data. We use Gaussian process regression (GPR) to\nimplement the framework, comparing the performance with different regression\nformulations such as linear regression and neural network based methods. We\nevaluate these frameworks by studying the tradeoff between spatial resolution\nand accuracy of the probability map using naturalistic recordings collected\nwith the UTDrive platform. We observe that the GPR method produces the best\nresult creating accurate predictions with localized salient regions. For\nexample, the 95% confidence region is defined by an area that covers 3.77%\nregion of a sphere surrounding the driver.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 15:48:43 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Jha", "Sumit", ""], ["Busso", "Carlos", ""]]}, {"id": "2012.12758", "submitter": "Caterina De Bacco", "authors": "Diego Baptista and Caterina De Bacco", "title": "Principled network extraction from images", "comments": "8 figures", "journal-ref": null, "doi": "10.1098/rsos.210025", "report-no": null, "categories": "cs.CV nlin.AO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images of natural systems may represent patterns of network-like structure,\nwhich could reveal important information about the topological properties of\nthe underlying subject. However, the image itself does not automatically\nprovide a formal definition of a network in terms of sets of nodes and edges.\nInstead, this information should be suitably extracted from the raw image data.\nMotivated by this, we present a principled model to extract network topologies\nfrom images that is scalable and efficient. We map this goal into solving a\nrouting optimization problem where the solution is a network that minimizes an\nenergy function which can be interpreted in terms of an operational and\ninfrastructural cost. Our method relies on recent results from optimal\ntransport theory and is a principled alternative to standard image-processing\ntechniques that are based on heuristics. We test our model on real images of\nthe retinal vascular system, slime mold and river networks and compare with\nroutines combining image-processing techniques. Results are tested in terms of\na similarity measure related to the amount of information preserved in the\nextraction. We find that our model finds networks from retina vascular network\nimages that are more similar to hand-labeled ones, while also giving high\nperformance in extracting networks from images of rivers and slime mold for\nwhich there is no ground truth available. While there is no unique method that\nfits all the images the best, our approach performs consistently across\ndatasets, its algorithmic implementation is efficient and can be fully\nautomatized to be run on several datasets with little supervision.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 15:56:09 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Baptista", "Diego", ""], ["De Bacco", "Caterina", ""]]}, {"id": "2012.12784", "submitter": "Ahmed Zgaren", "authors": "Ahmed Zgaren, Wassim Bouachir, Riadh Ksantini", "title": "Coarse-to-Fine Object Tracking Using Deep Features and Correlation\n  Filters", "comments": null, "journal-ref": "Advances in Visual Computing 2020. Lecture Notes in Computer\n  Science, vol 12509. Springer", "doi": "10.1007/978-3-030-64556-4_40", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last years, deep learning trackers achieved stimulating results\nwhile bringing interesting ideas to solve the tracking problem. This progress\nis mainly due to the use of learned deep features obtained by training deep\nconvolutional neural networks (CNNs) on large image databases. But since CNNs\nwere originally developed for image classification, appearance modeling\nprovided by their deep layers might be not enough discriminative for the\ntracking task. In fact,such features represent high-level information, that is\nmore related to object category than to a specific instance of the object.\nMotivated by this observation, and by the fact that discriminative correlation\nfilters(DCFs) may provide a complimentary low-level information, we presenta\nnovel tracking algorithm taking advantage of both approaches. We formulate the\ntracking task as a two-stage procedure. First, we exploit the generalization\nability of deep features to coarsely estimate target translation, while\nensuring invariance to appearance change. Then, we capitalize on the\ndiscriminative power of correlation filters to precisely localize the tracked\nobject. Furthermore, we designed an update control mechanism to learn\nappearance change while avoiding model drift. We evaluated the proposed tracker\non object tracking benchmarks. Experimental results show the robustness of our\nalgorithm, which performs favorably against CNN and DCF-based trackers. Code is\navailable at: https://github.com/AhmedZgaren/Coarse-to-fine-Tracker\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 16:43:21 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Zgaren", "Ahmed", ""], ["Bouachir", "Wassim", ""], ["Ksantini", "Riadh", ""]]}, {"id": "2012.12809", "submitter": "Christopher Grimm", "authors": "Christopher Grimm, Tai Fei, Ernst Warsitz, Ridha Farhoud, Tobias\n  Breddermann, Reinhold Haeb-Umbach", "title": "Warping of Radar Data into Camera Image for Cross-Modal Supervision in\n  Automotive Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we present a novel framework to project automotive radar\nrange-Doppler (RD) spectrum into camera image. The utilized warping operation\nis designed to be fully differentiable, which allows error backpropagation\nthrough the operation. This enables the training of neural networks (NN)\noperating exclusively on RD spectrum by utilizing labels provided from camera\nvision models. As the warping operation relies on accurate scene flow,\nadditionally, we present a novel scene flow estimation algorithm fed from\ncamera, lidar and radar, enabling us to improve the accuracy of the warping\noperation. We demonstrate the framework in multiple applications like\ndirection-of-arrival (DoA) estimation, target detection, semantic segmentation\nand estimation of radar power from camera data. Extensive evaluations have been\ncarried out for the DoA application and suggest superior quality for NN based\nestimators compared to classical estimators. The novel scene flow estimation\napproach is benchmarked against state-of-the-art scene flow algorithms and\noutperforms them by roughly a third.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 17:12:59 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Grimm", "Christopher", ""], ["Fei", "Tai", ""], ["Warsitz", "Ernst", ""], ["Farhoud", "Ridha", ""], ["Breddermann", "Tobias", ""], ["Haeb-Umbach", "Reinhold", ""]]}, {"id": "2012.12820", "submitter": "Andreanne Lemay", "authors": "Andreanne Lemay, Charley Gros, Zhizheng Zhuo, Jie Zhang, Yunyun Duan,\n  Julien Cohen-Adad, Yaou Liu", "title": "Multiclass Spinal Cord Tumor Segmentation on MRI with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spinal cord tumors lead to neurological morbidity and mortality. Being able\nto obtain morphometric quantification (size, location, growth rate) of the\ntumor, edema, and cavity can result in improved monitoring and treatment\nplanning. Such quantification requires the segmentation of these structures\ninto three separate classes. However, manual segmentation of 3-dimensional\nstructures is time-consuming and tedious, motivating the development of\nautomated methods. Here, we tailor a model adapted to the spinal cord tumor\nsegmentation task. Data were obtained from 343 patients using\ngadolinium-enhanced T1-weighted and T2-weighted MRI scans with cervical,\nthoracic, and/or lumbar coverage. The dataset includes the three most common\nintramedullary spinal cord tumor types: astrocytomas, ependymomas, and\nhemangioblastomas. The proposed approach is a cascaded architecture with\nU-Net-based models that segments tumors in a two-stage process: locate and\nlabel. The model first finds the spinal cord and generates bounding box\ncoordinates. The images are cropped according to this output, leading to a\nreduced field of view, which mitigates class imbalance. The tumor is then\nsegmented. The segmentation of the tumor, cavity, and edema (as a single class)\nreached 76.7 $\\pm$ 1.5% of Dice score and the segmentation of tumors alone\nreached 61.8 $\\pm$ 4.0% Dice score. The true positive detection rate was above\n87% for tumor, edema, and cavity. To the best of our knowledge, this is the\nfirst fully automatic deep learning model for spinal cord tumor segmentation.\nThe multiclass segmentation pipeline is available in the Spinal Cord Toolbox\n(https://spinalcordtoolbox.com/). It can be run with custom data on a regular\ncomputer within seconds.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 17:31:08 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 17:01:08 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 05:25:53 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 13:32:45 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Lemay", "Andreanne", ""], ["Gros", "Charley", ""], ["Zhuo", "Zhizheng", ""], ["Zhang", "Jie", ""], ["Duan", "Yunyun", ""], ["Cohen-Adad", "Julien", ""], ["Liu", "Yaou", ""]]}, {"id": "2012.12821", "submitter": "Liming Jiang", "authors": "Liming Jiang, Bo Dai, Wayne Wu, Chen Change Loy", "title": "Focal Frequency Loss for Image Reconstruction and Synthesis", "comments": "GitHub: https://github.com/EndlessSora/focal-frequency-loss", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image reconstruction and synthesis have witnessed remarkable progress thanks\nto the development of generative models. Nonetheless, gaps could still exist\nbetween the real and generated images, especially in the frequency domain. In\nthis study, we show that narrowing gaps in the frequency domain can ameliorate\nimage reconstruction and synthesis quality further. We propose a novel focal\nfrequency loss, which allows a model to adaptively focus on frequency\ncomponents that are hard to synthesize by down-weighting the easy ones. This\nobjective function is complementary to existing spatial losses, offering great\nimpedance against the loss of important frequency information due to the\ninherent bias of neural networks. We demonstrate the versatility and\neffectiveness of focal frequency loss to improve popular models, such as VAE,\npix2pix, and SPADE, in both perceptual quality and quantitative performance. We\nfurther show its potential on StyleGAN2.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 17:32:04 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 09:20:30 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Jiang", "Liming", ""], ["Dai", "Bo", ""], ["Wu", "Wayne", ""], ["Loy", "Chen Change", ""]]}, {"id": "2012.12854", "submitter": "Youdong Mao", "authors": "Zhaolong Wu, Shuwen Zhang, Wei Li Wang, Yinping Ma, Yuanchen Dong and\n  Youdong Mao", "title": "Deep manifold learning reveals hidden dynamics of proteasome\n  autoregulation", "comments": "81 pages, 16 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cond-mat.stat-mech cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2.5-MDa 26S proteasome maintains proteostasis and regulates myriad\ncellular processes. How polyubiquitylated substrate interactions regulate\nproteasome activity is not understood. Here we introduce a deep manifold\nlearning framework, named AlphaCryo4D, which enables atomic-level cryogenic\nelectron microscopy (cryo-EM) reconstructions of nonequilibrium conformational\ncontinuum and reconstitutes hidden dynamics of proteasome autoregulation in the\nact of substrate degradation. AlphaCryo4D integrates 3D deep residual learning\nwith manifold embedding of free-energy landscapes, which directs 3D clustering\nvia an energy-based particle-voting algorithm. In blind assessments using\nsimulated heterogeneous cryo-EM datasets, AlphaCryo4D achieved 3D\nclassification accuracy three times that of conventional method and\nreconstructed continuous conformational changes of a 130-kDa protein at\nsub-3-angstrom resolution. By using AlphaCryo4D to analyze a single\nexperimental cryo-EM dataset, we identified 64 conformers of the\nsubstrate-bound human 26S proteasome, revealing conformational entanglement of\ntwo regulatory particles in the doubly capped holoenzymes and their energetic\ndifferences with singly capped ones. Novel ubiquitin-binding sites are\ndiscovered on the RPN2, RPN10 and Alpha5 subunits to remodel polyubiquitin\nchains for deubiquitylation and recycle. Importantly, AlphaCryo4D choreographs\nsingle-nucleotide-exchange dynamics of proteasomal AAA-ATPase motor during\ntranslocation initiation, which upregulates proteolytic activity by\nallosterically promoting nucleophilic attack. Our systemic analysis illuminates\na grand hierarchical allostery for proteasome autoregulation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 18:23:53 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 06:34:31 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wu", "Zhaolong", ""], ["Zhang", "Shuwen", ""], ["Wang", "Wei Li", ""], ["Ma", "Yinping", ""], ["Dong", "Yuanchen", ""], ["Mao", "Youdong", ""]]}, {"id": "2012.12877", "submitter": "Hugo Touvron", "authors": "Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa,\n  Alexandre Sablayrolles, Herv\\'e J\\'egou", "title": "Training data-efficient image transformers & distillation through\n  attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, neural networks purely based on attention were shown to address\nimage understanding tasks such as image classification. However, these visual\ntransformers are pre-trained with hundreds of millions of images using an\nexpensive infrastructure, thereby limiting their adoption.\n  In this work, we produce a competitive convolution-free transformer by\ntraining on Imagenet only. We train them on a single computer in less than 3\ndays. Our reference vision transformer (86M parameters) achieves top-1 accuracy\nof 83.1% (single-crop evaluation) on ImageNet with no external data.\n  More importantly, we introduce a teacher-student strategy specific to\ntransformers. It relies on a distillation token ensuring that the student\nlearns from the teacher through attention. We show the interest of this\ntoken-based distillation, especially when using a convnet as a teacher. This\nleads us to report results competitive with convnets for both Imagenet (where\nwe obtain up to 85.2% accuracy) and when transferring to other tasks. We share\nour code and models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 18:42:10 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 15:52:50 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Touvron", "Hugo", ""], ["Cord", "Matthieu", ""], ["Douze", "Matthijs", ""], ["Massa", "Francisco", ""], ["Sablayrolles", "Alexandre", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "2012.12880", "submitter": "Jiawei Yang", "authors": "Jiawei Yang, Yuan Liang, Yao Zhang, Weinan Song, Kun Wang, Lei He", "title": "Exploring Instance-Level Uncertainty for Medical Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of deep learning to predict with uncertainty is recognized as key\nfor its adoption in clinical routines. Moreover, performance gain has been\nenabled by modelling uncertainty according to empirical evidence. While\nprevious work has widely discussed the uncertainty estimation in segmentation\nand classification tasks, its application on bounding-box-based detection has\nbeen limited, mainly due to the challenge of bounding box aligning. In this\nwork, we explore to augment a 2.5D detection CNN with two different\nbounding-box-level (or instance-level) uncertainty estimates, i.e., predictive\nvariance and Monte Carlo (MC) sample variance. Experiments are conducted for\nlung nodule detection on LUNA16 dataset, a task where significant semantic\nambiguities can exist between nodules and non-nodules. Results show that our\nmethod improves the evaluating score from 84.57% to 88.86% by utilizing a\ncombination of both types of variances. Moreover, we show the generated\nuncertainty enables superior operating points compared to using the probability\nthreshold only, and can further boost the performance to 89.52%. Example nodule\ndetections are visualized to further illustrate the advantages of our method.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 18:46:37 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 09:52:28 GMT"}, {"version": "v3", "created": "Sun, 7 Feb 2021 16:06:55 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Yang", "Jiawei", ""], ["Liang", "Yuan", ""], ["Zhang", "Yao", ""], ["Song", "Weinan", ""], ["Wang", "Kun", ""], ["He", "Lei", ""]]}, {"id": "2012.12884", "submitter": "Chung-Yi Weng", "authors": "Chung-Yi Weng, Brian Curless, Ira Kemelmacher-Shlizerman", "title": "Vid2Actor: Free-viewpoint Animatable Person Synthesis from Video in the\n  Wild", "comments": "Project Page: https://grail.cs.washington.edu/projects/vid2actor/\n  Supplementary Video: https://youtu.be/Zec8Us0v23o", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Given an \"in-the-wild\" video of a person, we reconstruct an animatable model\nof the person in the video. The output model can be rendered in any body pose\nto any camera view, via the learned controls, without explicit 3D mesh\nreconstruction. At the core of our method is a volumetric 3D human\nrepresentation reconstructed with a deep network trained on input video,\nenabling novel pose/view synthesis. Our method is an advance over GAN-based\nimage-to-image translation since it allows image synthesis for any pose and\ncamera via the internal 3D representation, while at the same time it does not\nrequire a pre-rigged model or ground truth meshes for training, as in\nmesh-based learning. Experiments validate the design choices and yield results\non synthetic data and on real videos of diverse people performing unconstrained\nactivities (e.g. dancing or playing tennis). Finally, we demonstrate motion\nre-targeting and bullet-time rendering with the learned models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 18:50:42 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Weng", "Chung-Yi", ""], ["Curless", "Brian", ""], ["Kemelmacher-Shlizerman", "Ira", ""]]}, {"id": "2012.12890", "submitter": "Amit Raj", "authors": "Amit Raj, Julian Tanke, James Hays, Minh Vo, Carsten Stoll, Christoph\n  Lassner", "title": "ANR: Articulated Neural Rendering for Virtual Avatars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The combination of traditional rendering with neural networks in Deferred\nNeural Rendering (DNR) provides a compelling balance between computational\ncomplexity and realism of the resulting images. Using skinned meshes for\nrendering articulating objects is a natural extension for the DNR framework and\nwould open it up to a plethora of applications. However, in this case the\nneural shading step must account for deformations that are possibly not\ncaptured in the mesh, as well as alignment inaccuracies and dynamics -- which\ncan confound the DNR pipeline. We present Articulated Neural Rendering (ANR), a\nnovel framework based on DNR which explicitly addresses its limitations for\nvirtual human avatars. We show the superiority of ANR not only with respect to\nDNR but also with methods specialized for avatar creation and animation. In two\nuser studies, we observe a clear preference for our avatar model and we\ndemonstrate state-of-the-art performance on quantitative evaluation metrics.\nPerceptually, we observe better temporal stability, level of detail and\nplausibility.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 18:56:11 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Raj", "Amit", ""], ["Tanke", "Julian", ""], ["Hays", "James", ""], ["Vo", "Minh", ""], ["Stoll", "Carsten", ""], ["Lassner", "Christoph", ""]]}, {"id": "2012.12896", "submitter": "Jingling Li", "authors": "Jingling Li, Mozhi Zhang, Keyulu Xu, John P. Dickerson, Jimmy Ba", "title": "Noisy Labels Can Induce Good Representations", "comments": "27 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current success of deep learning depends on large-scale labeled datasets.\nIn practice, high-quality annotations are expensive to collect, but noisy\nannotations are more affordable. Previous works report mixed empirical results\nwhen training with noisy labels: neural networks can easily memorize random\nlabels, but they can also generalize from noisy labels. To explain this puzzle,\nwe study how architecture affects learning with noisy labels. We observe that\nif an architecture \"suits\" the task, training with noisy labels can induce\nuseful hidden representations, even when the model generalizes poorly; i.e.,\nthe last few layers of the model are more negatively affected by noisy labels.\nThis finding leads to a simple method to improve models trained on noisy\nlabels: replacing the final dense layers with a linear model, whose weights are\nlearned from a small set of clean data. We empirically validate our findings\nacross three architectures (Convolutional Neural Networks, Graph Neural\nNetworks, and Multi-Layer Perceptrons) and two domains (graph algorithmic tasks\nand image classification). Furthermore, we achieve state-of-the-art results on\nimage classification benchmarks by combining our method with existing\napproaches on noisy label training.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 18:58:05 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Li", "Jingling", ""], ["Zhang", "Mozhi", ""], ["Xu", "Keyulu", ""], ["Dickerson", "John P.", ""], ["Ba", "Jimmy", ""]]}, {"id": "2012.12899", "submitter": "Pengtao Xie", "authors": "Ramtin Hosseini, Pengtao Xie", "title": "Learning by Self-Explanation, with Application to Neural Architecture\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning by self-explanation is an effective learning technique in human\nlearning, where students explain a learned topic to themselves for deepening\ntheir understanding of this topic. It is interesting to investigate whether\nthis explanation-driven learning methodology broadly used by humans is helpful\nfor improving machine learning as well. Based on this inspiration, we propose a\nnovel machine learning method called learning by self-explanation (LeaSE). In\nour approach, an explainer model improves its learning ability by trying to\nclearly explain to an audience model regarding how a prediction outcome is\nmade. LeaSE is formulated as a four-level optimization problem involving a\nsequence of four learning stages which are conducted end-to-end in a unified\nframework: 1) explainer learns; 2) explainer explains; 3) audience learns; 4)\nexplainer re-learns based on the performance of the audience. We develop an\nefficient algorithm to solve the LeaSE problem. We apply LeaSE for neural\narchitecture search on CIFAR-100, CIFAR-10, and ImageNet. Experimental results\nstrongly demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 07:39:54 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 01:05:48 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Hosseini", "Ramtin", ""], ["Xie", "Pengtao", ""]]}, {"id": "2012.12975", "submitter": "Riza Velioglu", "authors": "Riza Velioglu, Jewgeni Rose", "title": "Detecting Hate Speech in Memes Using Multimodal Deep Learning\n  Approaches: Prize-winning solution to Hateful Memes Challenge", "comments": "Presented at NeurIPS (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memes on the Internet are often harmless and sometimes amusing. However, by\nusing certain types of images, text, or combinations of both, the seemingly\nharmless meme becomes a multimodal type of hate speech -- a hateful meme. The\nHateful Memes Challenge is a first-of-its-kind competition which focuses on\ndetecting hate speech in multimodal memes and it proposes a new data set\ncontaining 10,000+ new examples of multimodal content. We utilize VisualBERT --\nwhich meant to be the BERT of vision and language -- that was trained\nmultimodally on images and captions and apply Ensemble Learning. Our approach\nachieves 0.811 AUROC with an accuracy of 0.765 on the challenge test set and\nplaced third out of 3,173 participants in the Hateful Memes Challenge.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 21:09:52 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Velioglu", "Riza", ""], ["Rose", "Jewgeni", ""]]}, {"id": "2012.12987", "submitter": "Raphael Abreu", "authors": "Rafael F. C. Oliveira, Fabio Barreto, Raphael Abreu", "title": "Convolutional Neural Network for Elderly Wandering Prediction in Indoor\n  Scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work proposes a way to detect the wandering activity of Alzheimer's\npatients from path data collected from non-intrusive indoor sensors around the\nhouse. Due to the lack of adequate data, we've manually generated a dataset of\n220 paths using our own developed application. Wandering patterns in the\nliterature are normally identified by visual features (such as loops or random\nmovement), thus our dataset was transformed into images and augmented.\nConvolutional layers were used on the neural network model since they tend to\nhave good results finding patterns, especially on images. The Convolutional\nNeural Network model was trained with the generated data and achieved an f1\nscore (relation between precision and recall) of 75%, recall of 60%, and\nprecision of 100% on our 10 sample validation slice\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 21:27:37 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Oliveira", "Rafael F. C.", ""], ["Barreto", "Fabio", ""], ["Abreu", "Raphael", ""]]}, {"id": "2012.12991", "submitter": "Sedat Ozer", "authors": "Berat Mert Albaba, Sedat Ozer", "title": "SyNet: An Ensemble Network for Object Detection in UAV Images", "comments": "accepted for publication at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in camera equipped drone applications and their widespread\nuse increased the demand on vision based object detection algorithms for aerial\nimages. Object detection process is inherently a challenging task as a generic\ncomputer vision problem, however, since the use of object detection algorithms\non UAVs (or on drones) is relatively a new area, it remains as a more\nchallenging problem to detect objects in aerial images. There are several\nreasons for that including: (i) the lack of large drone datasets including\nlarge object variance, (ii) the large orientation and scale variance in drone\nimages when compared to the ground images, and (iii) the difference in texture\nand shape features between the ground and the aerial images. Deep learning\nbased object detection algorithms can be classified under two main categories:\n(a) single-stage detectors and (b) multi-stage detectors. Both single-stage and\nmulti-stage solutions have their advantages and disadvantages over each other.\nHowever, a technique to combine the good sides of each of those solutions could\nyield even a stronger solution than each of those solutions individually. In\nthis paper, we propose an ensemble network, SyNet, that combines a multi-stage\nmethod with a single-stage one with the motivation of decreasing the high false\nnegative rate of multi-stage detectors and increasing the quality of the\nsingle-stage detector proposals. As building blocks, CenterNet and Cascade\nR-CNN with pretrained feature extractors are utilized along with an ensembling\nstrategy. We report the state of the art results obtained by our proposed\nsolution on two different datasets: namely MS-COCO and visDrone with \\%52.1\n$mAP_{IoU = 0.75}$ is obtained on MS-COCO $val2017$ dataset and \\%26.2\n$mAP_{IoU = 0.75}$ is obtained on VisDrone $test-set$.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 21:38:32 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Albaba", "Berat Mert", ""], ["Ozer", "Sedat", ""]]}, {"id": "2012.12996", "submitter": "G\\\"ulcan Can", "authors": "G\\\"ulcan Can, Dario Mantegazza, Gabriele Abbate, S\\'ebastien Chappuis,\n  Alessandro Giusti", "title": "Semantic Segmentation on Swiss3DCities: A Benchmark Study on Aerial\n  Photogrammetric 3D Pointcloud Dataset", "comments": "This paper is submitted to an Elsevier journal for possible\n  publication. Copyrights may be transferred without notice", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce a new outdoor urban 3D pointcloud dataset, covering a total area\nof 2.7 $km^2$, sampled from three Swiss cities with different characteristics.\nThe dataset is manually annotated for semantic segmentation with per-point\nlabels, and is built using photogrammetry from images acquired by multirotors\nequipped with high-resolution cameras. In contrast to datasets acquired with\nground LiDAR sensors, the resulting point clouds are uniformly dense and\ncomplete, and are useful to disparate applications, including autonomous\ndriving, gaming and smart city planning. As a benchmark, we report quantitative\nresults of PointNet++, an established point-based deep 3D semantic segmentation\nmodel; on this model, we additionally study the impact of using different\ncities for model generalization.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 21:48:47 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Can", "G\u00fclcan", ""], ["Mantegazza", "Dario", ""], ["Abbate", "Gabriele", ""], ["Chappuis", "S\u00e9bastien", ""], ["Giusti", "Alessandro", ""]]}, {"id": "2012.13014", "submitter": "Nelson Alves", "authors": "Nelson Alves, Marco Ruiz, Marco Reis, Tiago Cajahyba, Davi Oliveira,\n  Ana Barreto, Eduardo F. Simas Filho, Wagner L. A. de Oliveira, Leizer\n  Schnitman, Roberto L. S. Monteiro", "title": "Low-latency Perception in Off-Road Dynamical Low Visibility Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This work proposes a perception system for autonomous vehicles and advanced\ndriver assistance specialized on unpaved roads and off-road environments. In\nthis research, the authors have investigated the behavior of Deep Learning\nalgorithms applied to semantic segmentation of off-road environments and\nunpaved roads under differents adverse conditions of visibility. Almost 12,000\nimages of different unpaved and off-road environments were collected and\nlabeled. It was assembled an off-road proving ground exclusively for its\ndevelopment. The proposed dataset also contains many adverse situations such as\nrain, dust, and low light. To develop the system, we have used convolutional\nneural networks trained to segment obstacles and areas where the car can pass\nthrough. We developed a Configurable Modular Segmentation Network (CMSNet)\nframework to help create different architectures arrangements and test them on\nthe proposed dataset. Besides, we also have ported some CMSNet configurations\nby removing and fusing many layers using TensorRT, C++, and CUDA to achieve\nembedded real-time inference and allow field tests. The main contributions of\nthis work are: a new dataset for unpaved roads and off-roads environments\ncontaining many adverse conditions such as night, rain, and dust; a CMSNet\nframework; an investigation regarding the feasibility of applying deep learning\nto detect region where the vehicle can pass through when there is no clear\nboundary of the track; a study of how our proposed segmentation algorithms\nbehave in different severity levels of visibility impairment; and an evaluation\nof field tests carried out with semantic segmentation architectures ported for\nreal-time inference.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 22:54:43 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Alves", "Nelson", ""], ["Ruiz", "Marco", ""], ["Reis", "Marco", ""], ["Cajahyba", "Tiago", ""], ["Oliveira", "Davi", ""], ["Barreto", "Ana", ""], ["Filho", "Eduardo F. Simas", ""], ["de Oliveira", "Wagner L. A.", ""], ["Schnitman", "Leizer", ""], ["Monteiro", "Roberto L. S.", ""]]}, {"id": "2012.13018", "submitter": "Hieu Le", "authors": "Hieu Le and Dimitris Samaras", "title": "Physics-based Shadow Image Decomposition for Shadow Removal", "comments": "arXiv admin note: substantial text overlap with arXiv:1908.08628,\n  arXiv:2008.00267", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel deep learning method for shadow removal. Inspired by\nphysical models of shadow formation, we use a linear illumination\ntransformation to model the shadow effects in the image that allows the shadow\nimage to be expressed as a combination of the shadow-free image, the shadow\nparameters, and a matte layer. We use two deep networks, namely SP-Net and\nM-Net, to predict the shadow parameters and the shadow matte respectively. This\nsystem allows us to remove the shadow effects from images. We then employ an\ninpainting network, I-Net, to further refine the results. We train and test our\nframework on the most challenging shadow removal dataset (ISTD). Our method\nimproves the state-of-the-art in terms of root mean square error (RMSE) for the\nshadow area by 20\\%. Furthermore, this decomposition allows us to formulate a\npatch-based weakly-supervised shadow removal method. This model can be trained\nwithout any shadow-free images (that are cumbersome to acquire) and achieves\ncompetitive shadow removal results compared to state-of-the-art methods that\nare trained with fully paired shadow and shadow-free images. Last, we introduce\nSBU-Timelapse, a video shadow removal dataset for evaluating shadow removal\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 23:06:38 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Le", "Hieu", ""], ["Samaras", "Dimitris", ""]]}, {"id": "2012.13024", "submitter": "Mihee Lee", "authors": "Mihee Lee, Vladimir Pavlovic", "title": "Private-Shared Disentangled Multimodal VAE for Learning of Hybrid Latent\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-modal generative models represent an important family of deep models,\nwhose goal is to facilitate representation learning on data with multiple views\nor modalities. However, current deep multi-modal models focus on the inference\nof shared representations, while neglecting the important private aspects of\ndata within individual modalities. In this paper, we introduce a disentangled\nmulti-modal variational autoencoder (DMVAE) that utilizes disentangled VAE\nstrategy to separate the private and shared latent spaces of multiple\nmodalities. We specifically consider the instance where the latent factor may\nbe of both continuous and discrete nature, leading to the family of general\nhybrid DMVAE models. We demonstrate the utility of DMVAE on a semi-supervised\nlearning task, where one of the modalities contains partial data labels, both\nrelevant and irrelevant to the other modality. Our experiments on several\nbenchmarks indicate the importance of the private-shared disentanglement as\nwell as the hybrid latent representation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 23:33:23 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Lee", "Mihee", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "2012.13028", "submitter": "Mohammad J. Hashemi", "authors": "Mohammad J. Hashemi, Eric Keller", "title": "General Domain Adaptation Through Proportional Progressive Pseudo\n  Labeling", "comments": "Published at 2020 IEEE International Conference on Big Data (Big\n  Data)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation helps transfer the knowledge gained from a labeled source\ndomain to an unlabeled target domain. During the past few years, different\ndomain adaptation techniques have been published. One common flaw of these\napproaches is that while they might work well on one input type, such as\nimages, their performance drops when applied to others, such as text or\ntime-series. In this paper, we introduce Proportional Progressive Pseudo\nLabeling (PPPL), a simple, yet effective technique that can be implemented in a\nfew lines of code to build a more general domain adaptation technique that can\nbe applied on several different input types. At the beginning of the training\nphase, PPPL progressively reduces target domain classification error, by\ntraining the model directly with pseudo-labeled target domain samples, while\nexcluding samples with more likely wrong pseudo-labels from the training set\nand also postponing training on such samples. Experiments on 6 different\ndatasets that include tasks such as anomaly detection, text sentiment analysis\nand image classification demonstrate that PPPL can beat other baselines and\ngeneralize better.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 23:57:00 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Hashemi", "Mohammad J.", ""], ["Keller", "Eric", ""]]}, {"id": "2012.13033", "submitter": "Dario Fuoli", "authors": "Dario Fuoli, Zhiwu Huang, Danda Pani Paudel, Luc Van Gool, Radu\n  Timofte", "title": "An Efficient Recurrent Adversarial Framework for Unsupervised Real-Time\n  Video Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video enhancement is a challenging problem, more than that of stills, mainly\ndue to high computational cost, larger data volumes and the difficulty of\nachieving consistency in the spatio-temporal domain. In practice, these\nchallenges are often coupled with the lack of example pairs, which inhibits the\napplication of supervised learning strategies. To address these challenges, we\npropose an efficient adversarial video enhancement framework that learns\ndirectly from unpaired video examples. In particular, our framework introduces\nnew recurrent cells that consist of interleaved local and global modules for\nimplicit integration of spatial and temporal information. The proposed design\nallows our recurrent cells to efficiently propagate spatio-temporal information\nacross frames and reduces the need for high complexity networks. Our setting\nenables learning from unpaired videos in a cyclic adversarial manner, where the\nproposed recurrent units are employed in all architectures. Efficient training\nis accomplished by introducing one single discriminator that learns the joint\ndistribution of source and target domain simultaneously. The enhancement\nresults demonstrate clear superiority of the proposed video enhancer over the\nstate-of-the-art methods, in all terms of visual quality, quantitative metrics,\nand inference speed. Notably, our video enhancer is capable of enhancing over\n35 frames per second of FullHD video (1080x1920).\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 00:03:29 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Fuoli", "Dario", ""], ["Huang", "Zhiwu", ""], ["Paudel", "Danda Pani", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2012.13044", "submitter": "Qingfang He", "authors": "Qingfang He, Guang Cheng and Zhiying Lin", "title": "Union-net: A deep neural network model adapted to small data sets", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real applications, generally small data sets can be obtained. At present,\nmost of the practical applications of machine learning use classic models based\non big data to solve the problem of small data sets. However, the deep neural\nnetwork model has complex structure, huge model parameters, and training\nrequires more advanced equipment, which brings certain difficulties to the\napplication. Therefore, this paper proposes the concept of union convolution,\ndesigning a light deep network model union-net with a shallow network structure\nand adapting to small data sets. This model combines convolutional network\nunits with different combinations of the same input to form a union module.\nEach union module is equivalent to a convolutional layer. The serial input and\noutput between the 3 modules constitute a \"3-layer\" neural network. The output\nof each union module is fused and added as the input of the last convolutional\nlayer to form a complex network with a 4-layer network structure. It solves the\nproblem that the deep network model network is too deep and the transmission\npath is too long, which causes the loss of the underlying information\ntransmission. Because the model has fewer model parameters and fewer channels,\nit can better adapt to small data sets. It solves the problem that the deep\nnetwork model is prone to overfitting in training small data sets. Use the\npublic data sets cifar10 and 17flowers to conduct multi-classification\nexperiments. Experiments show that the Union-net model can perform well in\nclassification of large data sets and small data sets. It has high practical\nvalue in daily application scenarios. The model code is published at\nhttps://github.com/yeaso/union-net\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 00:44:06 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["He", "Qingfang", ""], ["Cheng", "Guang", ""], ["Lin", "Zhiying", ""]]}, {"id": "2012.13052", "submitter": "Zhendong Chu", "authors": "Zhendong Chu, Jing Ma, Hongning Wang", "title": "Learning from Crowds by Modeling Common Confusions", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing provides a practical way to obtain large amounts of labeled\ndata at a low cost. However, the annotation quality of annotators varies\nconsiderably, which imposes new challenges in learning a high-quality model\nfrom the crowdsourced annotations. In this work, we provide a new perspective\nto decompose annotation noise into common noise and individual noise and\ndifferentiate the source of confusion based on instance difficulty and\nannotator expertise on a per-instance-annotator basis. We realize this new\ncrowdsourcing model by an end-to-end learning solution with two types of noise\nadaptation layers: one is shared across annotators to capture their commonly\nshared confusions, and the other one is pertaining to each annotator to realize\nindividual confusion. To recognize the source of noise in each annotation, we\nuse an auxiliary network to choose the two noise adaptation layers with respect\nto both instances and annotators. Extensive experiments on both synthesized and\nreal-world benchmarks demonstrate the effectiveness of our proposed common\nnoise adaptation solution.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 01:13:23 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 05:32:29 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chu", "Zhendong", ""], ["Ma", "Jing", ""], ["Wang", "Hongning", ""]]}, {"id": "2012.13059", "submitter": "Lavanya Umapathy", "authors": "Lavanya Umapathy, Gloria Guzman Perez-Carillo, Blair Winegar,\n  Srinivasan Vedantham, Maria Altbach, and Ali Bilgin", "title": "White matter hyperintensities volume and cognition: Assessment of a deep\n  learning based lesion detection and quantification algorithm on the\n  Alzheimers Disease Neuroimaging Initiative", "comments": "3 pages, 4 figures, Submitted to The Annual Conference of\n  International Society of Magnetic Resonance in Medicine (2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relationship between cognition and white matter hyperintensities (WMH)\nvolumes often depends on the accuracy of the lesion segmentation algorithm\nused. As such, accurate detection and quantification of WMH is of great\ninterest. Here, we use a deep learning-based WMH segmentation algorithm,\nStackGen-Net, to detect and quantify WMH on 3D FLAIR volumes from ADNI. We used\na subset of subjects (n=20) and obtained manual WMH segmentations by an\nexperienced neuro-radiologist to demonstrate the accuracy of our algorithm. On\na larger cohort of subjects (n=290), we observed that larger WMH volumes\ncorrelated with worse performance on executive function (P=.004), memory\n(P=.01), and language (P=.005).\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 01:32:33 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Umapathy", "Lavanya", ""], ["Perez-Carillo", "Gloria Guzman", ""], ["Winegar", "Blair", ""], ["Vedantham", "Srinivasan", ""], ["Altbach", "Maria", ""], ["Bilgin", "Ali", ""]]}, {"id": "2012.13073", "submitter": "Shiyuan Huang", "authors": "Shiyuan Huang, Jiawei Ma, Guangxing Han, Shih-Fu Chang", "title": "Task-Adaptive Negative Class Envision for Few-Shot Open-Set Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works seek to endow recognition systems with the ability to handle the\nopen world. Few shot learning aims for fast learning of new classes from\nlimited examples, while open-set recognition considers unknown negative class\nfrom the open world. In this paper, we study the problem of few-shot open-set\nrecognition (FSOR), which learns a recognition system robust to queries from\nnew sources with few examples and from unknown open sources. To achieve that,\nwe mimic human capability of envisioning new concepts from prior knowledge, and\npropose a novel task-adaptive negative class envision method (TANE) to model\nthe open world. Essentially we use an external memory to estimate a negative\nclass representation. Moreover, we introduce a novel conjugate episode training\nstrategy that strengthens the learning process. Extensive experiments on four\npublic benchmarks show that our approach significantly improves the\nstate-of-the-art performance on few-shot open-set recognition. Besides, we\nextend our method to generalized few-shot open-set recognition (GFSOR), where\nwe also achieve performance gains on MiniImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 02:30:18 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Huang", "Shiyuan", ""], ["Ma", "Jiawei", ""], ["Han", "Guangxing", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2012.13078", "submitter": "Deepak Gupta", "authors": "Deepak K. Gupta, Devanshu Arya and Efstratios Gavves", "title": "Rotation Equivariant Siamese Networks for Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rotation is among the long prevailing, yet still unresolved, hard challenges\nencountered in visual object tracking. The existing deep learning-based\ntracking algorithms use regular CNNs that are inherently translation\nequivariant, but not designed to tackle rotations. In this paper, we first\ndemonstrate that in the presence of rotation instances in videos, the\nperformance of existing trackers is severely affected. To circumvent the\nadverse effect of rotations, we present rotation-equivariant Siamese networks\n(RE-SiamNets), built through the use of group-equivariant convolutional layers\ncomprising steerable filters. SiamNets allow estimating the change in\norientation of the object in an unsupervised manner, thereby facilitating its\nuse in relative 2D pose estimation as well. We further show that this change in\norientation can be used to impose an additional motion constraint in Siamese\ntracking through imposing restriction on the change in orientation between two\nconsecutive frames. For benchmarking, we present Rotation Tracking Benchmark\n(RTB), a dataset comprising a set of videos with rotation instances. Through\nexperiments on two popular Siamese architectures, we show that RE-SiamNets\nhandle the problem of rotation very well and out-perform their regular\ncounterparts. Further, RE-SiamNets can accurately estimate the relative change\nin pose of the target in an unsupervised fashion, namely the in-plane rotation\nthe target has sustained with respect to the reference frame.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 03:06:47 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Gupta", "Deepak K.", ""], ["Arya", "Devanshu", ""], ["Gavves", "Efstratios", ""]]}, {"id": "2012.13089", "submitter": "Li Yi", "authors": "Yunze Liu, Li Yi, Shanghang Zhang, Qingnan Fan, Thomas Funkhouser, Hao\n  Dong", "title": "P4Contrast: Contrastive Learning with Pairs of Point-Pixel Pairs for\n  RGB-D Scene Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised representation learning is a critical problem in computer\nvision, as it provides a way to pretrain feature extractors on large unlabeled\ndatasets that can be used as an initialization for more efficient and effective\ntraining on downstream tasks. A promising approach is to use contrastive\nlearning to learn a latent space where features are close for similar data\nsamples and far apart for dissimilar ones. This approach has demonstrated\ntremendous success for pretraining both image and point cloud feature\nextractors, but it has been barely investigated for multi-modal RGB-D scans,\nespecially with the goal of facilitating high-level scene understanding. To\nsolve this problem, we propose contrasting \"pairs of point-pixel pairs\", where\npositives include pairs of RGB-D points in correspondence, and negatives\ninclude pairs where one of the two modalities has been disturbed and/or the two\nRGB-D points are not in correspondence. This provides extra flexibility in\nmaking hard negatives and helps networks to learn features from both\nmodalities, not just the more discriminating one of the two. Experiments show\nthat this proposed approach yields better performance on three large-scale\nRGB-D scene understanding benchmarks (ScanNet, SUN RGB-D, and 3RScan) than\nprevious pretraining approaches.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 04:00:52 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Liu", "Yunze", ""], ["Yi", "Li", ""], ["Zhang", "Shanghang", ""], ["Fan", "Qingnan", ""], ["Funkhouser", "Thomas", ""], ["Dong", "Hao", ""]]}, {"id": "2012.13093", "submitter": "Yu-Huan Wu", "authors": "Yu-Huan Wu, Yun Liu, Le Zhang, Ming-Ming Cheng", "title": "EDN: Salient Object Detection via Extremely-Downsampled Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress on salient object detection (SOD) mainly benefits from\nmulti-scale learning, where the high-level and low-level features work\ncollaboratively in locating salient objects and discovering fine details,\nrespectively. However, most efforts are devoted to low-level feature learning\nby fusing multi-scale features or enhancing boundary representations. In this\npaper, we show another direction that improving high-level feature learning is\nessential for SOD as well. To verify this, we introduce an\nExtremely-Downsampled Network (EDN), which employs an extreme downsampling\ntechnique to effectively learn a global view of the whole image, leading to\naccurate salient object localization. A novel Scale-Correlated Pyramid\nConvolution (SCPC) is also designed to build an elegant decoder for recovering\nobject details from the above extreme downsampling. Extensive experiments\ndemonstrate that EDN achieves \\sArt performance with real-time speed. Hence,\nthis work is expected to spark some new thinking in SOD. The code will be\nreleased.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 04:23:48 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Wu", "Yu-Huan", ""], ["Liu", "Yun", ""], ["Zhang", "Le", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "2012.13095", "submitter": "Yu-Huan Wu", "authors": "Yu-Huan Wu, Yun Liu, Jun Xu, Jia-Wang Bian, Yuchao Gu, Ming-Ming Cheng", "title": "MobileSal: Extremely Efficient RGB-D Salient Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high computational cost of neural networks has prevented recent successes\nin RGB-D salient object detection (SOD) from benefiting real-world\napplications. Hence, this paper introduces a novel network, \\methodname, which\nfocuses on efficient RGB-D SOD by using mobile networks for deep feature\nextraction. The problem is that mobile networks are less powerful in feature\nrepresentation than cumbersome networks. To this end, we observe that the depth\ninformation of color images can strengthen the feature representation related\nto SOD if leveraged properly. Therefore, we propose an implicit depth\nrestoration (IDR) technique to strengthen the feature representation capability\nof mobile networks for RGB-D SOD. IDR is only adopted in the training phase and\nis omitted during testing, so it is computationally free. Besides, we propose\ncompact pyramid refinement (CPR) for efficient multi-level feature aggregation\nso that we can derive salient objects with clear boundaries. With IDR and CPR\nincorporated, \\methodname~performs favorably against \\sArt methods on seven\nchallenging RGB-D SOD datasets with much faster speed (450fps) and fewer\nparameters (6.5M). The code will be released.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 04:36:42 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Wu", "Yu-Huan", ""], ["Liu", "Yun", ""], ["Xu", "Jun", ""], ["Bian", "Jia-Wang", ""], ["Gu", "Yuchao", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "2012.13103", "submitter": "Tao Zhang", "authors": "Mengting Xu, Tao Zhang, Zhongnian Li, Daoqiang Zhang", "title": "Improving the Certified Robustness of Neural Networks via Consistency\n  Regularization", "comments": "4 pages, appeare in AAAI21-RSEML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A range of defense methods have been proposed to improve the robustness of\nneural networks on adversarial examples, among which provable defense methods\nhave been demonstrated to be effective to train neural networks that are\ncertifiably robust to the attacker. However, most of these provable defense\nmethods treat all examples equally during training process, which ignore the\ninconsistent constraint of certified robustness between correctly classified\n(natural) and misclassified examples. In this paper, we explore this\ninconsistency caused by misclassified examples and add a novel consistency\nregularization term to make better use of the misclassified examples.\nSpecifically, we identified that the certified robustness of network can be\nsignificantly improved if the constraint of certified robustness on\nmisclassified examples and correctly classified examples is consistent.\nMotivated by this discovery, we design a new defense regularization term called\nMisclassification Aware Adversarial Regularization (MAAR), which constrains the\noutput probability distributions of all examples in the certified region of the\nmisclassified example. Experimental results show that our proposed MAAR\nachieves the best certified robustness and comparable accuracy on CIFAR-10 and\nMNIST datasets in comparison with several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 05:00:50 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 03:08:33 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Xu", "Mengting", ""], ["Zhang", "Tao", ""], ["Li", "Zhongnian", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "2012.13113", "submitter": "Yonggan Fu", "authors": "Yonggan Fu, Haoran You, Yang Zhao, Yue Wang, Chaojian Li, Kailash\n  Gopalakrishnan, Zhangyang Wang, Yingyan Lin", "title": "FracTrain: Fractionally Squeezing Bit Savings Both Temporally and\n  Spatially for Efficient DNN Training", "comments": "Accepted at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthroughs in deep neural networks (DNNs) have fueled a tremendous\ndemand for intelligent edge devices featuring on-site learning, while the\npractical realization of such systems remains a challenge due to the limited\nresources available at the edge and the required massive training costs for\nstate-of-the-art (SOTA) DNNs. As reducing precision is one of the most\neffective knobs for boosting training time/energy efficiency, there has been a\ngrowing interest in low-precision DNN training. In this paper, we explore from\nan orthogonal direction: how to fractionally squeeze out more training cost\nsavings from the most redundant bit level, progressively along the training\ntrajectory and dynamically per input. Specifically, we propose FracTrain that\nintegrates (i) progressive fractional quantization which gradually increases\nthe precision of activations, weights, and gradients that will not reach the\nprecision of SOTA static quantized DNN training until the final training stage,\nand (ii) dynamic fractional quantization which assigns precisions to both the\nactivations and gradients of each layer in an input-adaptive manner, for only\n\"fractionally\" updating layer parameters. Extensive simulations and ablation\nstudies (six models, four datasets, and three training settings including\nstandard, adaptation, and fine-tuning) validate the effectiveness of FracTrain\nin reducing computational cost and hardware-quantified energy/latency of DNN\ntraining while achieving a comparable or better (-0.12%~+1.87%) accuracy. For\nexample, when training ResNet-74 on CIFAR-10, FracTrain achieves 77.6% and\n53.5% computational cost and training latency savings, respectively, compared\nwith the best SOTA baseline, while achieving a comparable (-0.07%) accuracy.\nOur codes are available at: https://github.com/RICE-EIC/FracTrain.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 05:24:10 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Fu", "Yonggan", ""], ["You", "Haoran", ""], ["Zhao", "Yang", ""], ["Wang", "Yue", ""], ["Li", "Chaojian", ""], ["Gopalakrishnan", "Kailash", ""], ["Wang", "Zhangyang", ""], ["Lin", "Yingyan", ""]]}, {"id": "2012.13118", "submitter": "Pengdi Huang", "authors": "Pengdi Huang, Liqiang Lin, Fuyou Xue, Kai Xu, Danny Cohen-Or, Hui\n  Huang", "title": "Hausdorff Point Convolution with Geometric Priors", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Without a shape-aware response, it is hard to characterize the 3D geometry of\na point cloud efficiently with a compact set of kernels. In this paper, we\nadvocate the use of Hausdorff distance as a shape-aware distance measure for\ncalculating point convolutional responses. The technique we present, coined\nHausdorff Point Convolution (HPC), is shape-aware. We show that HPC constitutes\na powerful point feature learning with a rather compact set of only four types\nof geometric priors as kernels. We further develop a HPC-based deep neural\nnetwork (HPC-DNN). Task-specific learning can be achieved by tuning the network\nweights for combining the shortest distances between input and kernel point\nsets. We also realize hierarchical feature learning by designing a multi-kernel\nHPC for multi-scale feature encoding. Extensive experiments demonstrate that\nHPC-DNN outperforms strong point convolution baselines (e.g., KPConv),\nachieving 2.8% mIoU performance boost on S3DIS and 1.5% on SemanticKITTI for\nsemantic segmentation task.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 05:41:52 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Huang", "Pengdi", ""], ["Lin", "Liqiang", ""], ["Xue", "Fuyou", ""], ["Xu", "Kai", ""], ["Cohen-Or", "Danny", ""], ["Huang", "Hui", ""]]}, {"id": "2012.13122", "submitter": "Naeha Sharif", "authors": "Naeha Sharif, Mohammed Bennamoun, Wei Liu, Syed Afaq Ali Shah", "title": "SubICap: Towards Subword-informed Image Captioning", "comments": "8 pages", "journal-ref": "Workshop on Applications of Computer Vision (WACV), 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Image Captioning (IC) systems model words as atomic units in\ncaptions and are unable to exploit the structural information in the words.\nThis makes representation of rare words very difficult and out-of-vocabulary\nwords impossible. Moreover, to avoid computational complexity, existing IC\nmodels operate over a modest sized vocabulary of frequent words, such that the\nidentity of rare words is lost. In this work we address this common limitation\nof IC systems in dealing with rare words in the corpora. We decompose words\ninto smaller constituent units 'subwords' and represent captions as a sequence\nof subwords instead of words. This helps represent all words in the corpora\nusing a significantly lower subword vocabulary, leading to better parameter\nlearning. Using subword language modeling, our captioning system improves\nvarious metric scores, with a training vocabulary size approximately 90% less\nthan the baseline and various state-of-the-art word-level models. Our\nquantitative and qualitative results and analysis signify the efficacy of our\nproposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 06:10:36 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Sharif", "Naeha", ""], ["Bennamoun", "Mohammed", ""], ["Liu", "Wei", ""], ["Shah", "Syed Afaq Ali", ""]]}, {"id": "2012.13135", "submitter": "Qingjie Liu", "authors": "Ran Qin and Qingjie Liu and Guangshuai Gao and Di Huang and Yunhong\n  Wang", "title": "MRDet: A Multi-Head Network for Accurate Oriented Object Detection in\n  Aerial Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects in aerial images usually have arbitrary orientations and are densely\nlocated over the ground, making them extremely challenge to be detected. Many\nrecently developed methods attempt to solve these issues by estimating an extra\norientation parameter and placing dense anchors, which will result in high\nmodel complexity and computational costs. In this paper, we propose an\narbitrary-oriented region proposal network (AO-RPN) to generate oriented\nproposals transformed from horizontal anchors. The AO-RPN is very efficient\nwith only a few amounts of parameters increase than the original RPN.\nFurthermore, to obtain accurate bounding boxes, we decouple the detection task\ninto multiple subtasks and propose a multi-head network to accomplish them.\nEach head is specially designed to learn the features optimal for the\ncorresponding task, which allows our network to detect objects accurately. We\nname it MRDet short for Multi-head Rotated object Detector for convenience. We\ntest the proposed MRDet on two challenging benchmarks, i.e., DOTA and HRSC2016,\nand compare it with several state-of-the-art methods. Our method achieves very\npromising results which clearly demonstrate its effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 06:36:48 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Qin", "Ran", ""], ["Liu", "Qingjie", ""], ["Gao", "Guangshuai", ""], ["Huang", "Di", ""], ["Wang", "Yunhong", ""]]}, {"id": "2012.13137", "submitter": "Naeha Sharif", "authors": "Naeha Sharif, Lyndon White, Mohammed Bennamoun, Wei Liu, Syed Afaq Ali\n  Shah", "title": "WEmbSim: A Simple yet Effective Metric for Image Captioning", "comments": "7 pages", "journal-ref": "International Conference on Digital Image Computing: Techniques\n  and Applications (DICTA), 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area of automatic image caption evaluation is still undergoing intensive\nresearch to address the needs of generating captions which can meet adequacy\nand fluency requirements. Based on our past attempts at developing highly\nsophisticated learning-based metrics, we have discovered that a simple cosine\nsimilarity measure using the Mean of Word Embeddings(MOWE) of captions can\nactually achieve a surprisingly high performance on unsupervised caption\nevaluation. This inspires our proposed work on an effective metric WEmbSim,\nwhich beats complex measures such as SPICE, CIDEr and WMD at system-level\ncorrelation with human judgments. Moreover, it also achieves the best accuracy\nat matching human consensus scores for caption pairs, against commonly used\nunsupervised methods. Therefore, we believe that WEmbSim sets a new baseline\nfor any complex metric to be justified.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 06:39:43 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Sharif", "Naeha", ""], ["White", "Lyndon", ""], ["Bennamoun", "Mohammed", ""], ["Liu", "Wei", ""], ["Shah", "Syed Afaq Ali", ""]]}, {"id": "2012.13138", "submitter": "Sobhan Hemati", "authors": "Sobhan Hemati, Mohammad Hadi Mehdizavareh, Shojaeddin Chenouri, Hamid\n  R Tizhoosh", "title": "A non-alternating graph hashing algorithm for large scale image search", "comments": "The paper is under consideration at Computer Vision and Image\n  Understanding journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the era of big data, methods for improving memory and computational\nefficiency have become crucial for successful deployment of technologies.\nHashing is one of the most effective approaches to deal with computational\nlimitations that come with big data. One natural way for formulating this\nproblem is spectral hashing that directly incorporates affinity to learn binary\ncodes. However, due to binary constraints, the optimization becomes\nintractable. To mitigate this challenge, different relaxation approaches have\nbeen proposed to reduce the computational load of obtaining binary codes and\nstill attain a good solution. The problem with all existing relaxation methods\nis resorting to one or more additional auxiliary variables to attain high\nquality binary codes while relaxing the problem. The existence of auxiliary\nvariables leads to coordinate descent approach which increases the\ncomputational complexity. We argue that introducing these variables is\nunnecessary. To this end, we propose a novel relaxed formulation for spectral\nhashing that adds no additional variables to the problem. Furthermore, instead\nof solving the problem in original space where number of variables is equal to\nthe data points, we solve the problem in a much smaller space and retrieve the\nbinary codes from this solution. This trick reduces both the memory and\ncomputational complexity at the same time. We apply two optimization\ntechniques, namely projected gradient and optimization on manifold, to obtain\nthe solution. Using comprehensive experiments on four public datasets, we show\nthat the proposed efficient spectral hashing (ESH) algorithm achieves highly\ncompetitive retrieval performance compared with state of the art at low\ncomplexity.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 06:41:54 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 14:46:05 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Hemati", "Sobhan", ""], ["Mehdizavareh", "Mohammad Hadi", ""], ["Chenouri", "Shojaeddin", ""], ["Tizhoosh", "Hamid R", ""]]}, {"id": "2012.13148", "submitter": "Ling Zhou", "authors": "Ling Zhou, Qirong Mao, Ming Dong", "title": "Objective Class-based Micro-Expression Recognition through Simultaneous\n  Action Unit Detection and Feature Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro-Expression Recognition (MER) is a challenging task as the subtle\nchanges occur over different action regions of a face. Changes in facial action\nregions are formed as Action Units (AUs), and AUs in micro-expressions can be\nseen as the actors in cooperative group activities. In this paper, we propose a\nnovel deep neural network model for objective class-based MER, which\nsimultaneously detects AUs and aggregates AU-level features into\nmicro-expression-level representation through Graph Convolutional Networks\n(GCN). Specifically, we propose two new strategies in our AU detection module\nfor more effective AU feature learning: the attention mechanism and the\nbalanced detection loss function. With those two strategies, features are\nlearned for all the AUs in a unified model, eliminating the error-prune\nlandmark detection process and tedious separate training for each AU. Moreover,\nour model incorporates a tailored objective class-based AU knowledge-graph,\nwhich facilitates the GCN to aggregate the AU-level features into a\nmicro-expression-level feature representation. Extensive experiments on two\ntasks in MEGC 2018 show that our approach significantly outperforms the current\nstate-of-the-arts in MER. Additionally, we also report our single model-based\nmicro-expression AU detection results.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 07:31:15 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 04:00:45 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Zhou", "Ling", ""], ["Mao", "Qirong", ""], ["Dong", "Ming", ""]]}, {"id": "2012.13154", "submitter": "Min Yang", "authors": "Cong Xu, Min Yang", "title": "Adversarial Momentum-Contrastive Pre-Training", "comments": "7 pages;4 figures; preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are vulnerable to semantic invariant corruptions and\nimperceptible artificial perturbations. Although data augmentation can improve\nthe robustness against the former, it offers no guarantees against the latter.\nAdversarial training, on the other hand, is quite the opposite. Recent studies\nhave shown that adversarial self-supervised pre-training is helpful to extract\nthe invariant representations under both data augmentations and adversarial\nperturbations. Based on the MoCo's idea, this paper proposes a novel\nadversarial momentum-contrastive (AMOC) pre-training approach, which designs\ntwo dynamic memory banks to maintain the historical clean and adversarial\nrepresentations respectively, so as to exploit the discriminative\nrepresentations that are consistent in a long period. Compared with the\nexisting self-supervised pre-training approaches, AMOC can use a smaller batch\nsize and fewer training epochs but learn more robust features. Empirical\nresults show that the developed approach further improves the current\nstate-of-the-art adversarial robustness. Our code is available at\n\\url{https://github.com/MTandHJ/amoc}.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 07:49:10 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 10:18:35 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Xu", "Cong", ""], ["Yang", "Min", ""]]}, {"id": "2012.13161", "submitter": "Mahesh Chandra Mukkamala", "authors": "Mahesh Chandra Mukkamala, Jalal Fadili, Peter Ochs", "title": "Global Convergence of Model Function Based Bregman Proximal Minimization\n  Algorithms", "comments": "44 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lipschitz continuity of the gradient mapping of a continuously differentiable\nfunction plays a crucial role in designing various optimization algorithms.\nHowever, many functions arising in practical applications such as low rank\nmatrix factorization or deep neural network problems do not have a Lipschitz\ncontinuous gradient. This led to the development of a generalized notion known\nas the $L$-smad property, which is based on generalized proximity measures\ncalled Bregman distances. However, the $L$-smad property cannot handle\nnonsmooth functions, for example, simple nonsmooth functions like $\\abs{x^4-1}$\nand also many practical composite problems are out of scope. We fix this issue\nby proposing the MAP property, which generalizes the $L$-smad property and is\nalso valid for a large class of nonconvex nonsmooth composite problems. Based\non the proposed MAP property, we propose a globally convergent algorithm called\nModel BPG, that unifies several existing algorithms. The convergence analysis\nis based on a new Lyapunov function. We also numerically illustrate the\nsuperior performance of Model BPG on standard phase retrieval problems, robust\nphase retrieval problems, and Poisson linear inverse problems, when compared to\na state of the art optimization method that is valid for generic nonconvex\nnonsmooth optimization problems.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 08:09:22 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Mukkamala", "Mahesh Chandra", ""], ["Fadili", "Jalal", ""], ["Ochs", "Peter", ""]]}, {"id": "2012.13177", "submitter": "Yangyang Qu", "authors": "Yangyang Qu, Kai Chen, Chao Liu, Yongsheng Ou", "title": "UMLE: Unsupervised Multi-discriminator Network for Low Light Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low-light image enhancement, such as recovering color and texture details\nfrom low-light images, is a complex and vital task. For automated driving,\nlow-light scenarios will have serious implications for vision-based\napplications. To address this problem, we propose a real-time unsupervised\ngenerative adversarial network (GAN) containing multiple discriminators, i.e. a\nmulti-scale discriminator, a texture discriminator, and a color discriminator.\nThese distinct discriminators allow the evaluation of images from different\nperspectives. Further, considering that different channel features contain\ndifferent information and the illumination is uneven in the image, we propose a\nfeature fusion attention module. This module combines channel attention with\npixel attention mechanisms to extract image features. Additionally, to reduce\ntraining time, we adopt a shared encoder for the generator and the\ndiscriminator. This makes the structure of the model more compact and the\ntraining more stable. Experiments indicate that our method is superior to the\nstate-of-the-art methods in qualitative and quantitative evaluations, and\nsignificant improvements are achieved for both autopilot positioning and\ndetection results.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 09:48:56 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 02:36:11 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Qu", "Yangyang", ""], ["Chen", "Kai", ""], ["Liu", "Chao", ""], ["Ou", "Yongsheng", ""]]}, {"id": "2012.13180", "submitter": "Khoa Nguyen Van", "authors": "Van-Khoa Nguyen, Adrian Popescu, Jerome Deshayes-Chossart", "title": "Unveiling Real-Life Effects of Online Photo Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networks give free access to their services in exchange for the right\nto exploit their users' data. Data sharing is done in an initial context which\nis chosen by the users. However, data are used by social networks and third\nparties in different contexts which are often not transparent. We propose a new\napproach which unveils potential effects of data sharing in impactful real-life\nsituations. Focus is put on visual content because of its strong influence in\nshaping online user profiles. The approach relies on three components: (1) a\nset of concepts with associated situation impact ratings obtained by\ncrowdsourcing, (2) a corresponding set of object detectors used to analyze\nusers' photos and (3) a ground truth dataset made of 500 visual user profiles\nwhich are manually rated for each situation. These components are combined in\nLERVUP, a method which learns to rate visual user profiles in each situation.\nLERVUP exploits a new image descriptor which aggregates concept ratings and\nobject detections at user level. It also uses an attention mechanism to boost\nthe detections of highly-rated concepts to prevent them from being overwhelmed\nby low-rated ones. Performance is evaluated per situation by measuring the\ncorrelation between the automatic ranking of profile ratings and a manual\nground truth. Results indicate that LERVUP is effective since a strong\ncorrelation of the two rankings is obtained. This finding indicates that\nproviding meaningful automatic situation-related feedback about the effects of\ndata sharing is feasible.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 09:52:27 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Nguyen", "Van-Khoa", ""], ["Popescu", "Adrian", ""], ["Deshayes-Chossart", "Jerome", ""]]}, {"id": "2012.13188", "submitter": "Yalda Foroutan", "authors": "Yalda Foroutan, Ahmad Kalhor, Saeid Mohammadi Nejati, Samad Sheikhaei", "title": "Control of computer pointer using hand gesture recognition in motion\n  pictures", "comments": "8 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A user interface is designed to control the computer cursor by hand detection\nand classification of its gesture. A hand dataset with 6720 image samples is\ncollected, including four classes: fist, palm, pointing to the left, and\npointing to the right. The images are captured from 15 persons in simple\nbackgrounds and different perspectives and light conditions. A CNN network is\ntrained on this dataset to predict a label for each captured image and measure\nthe similarity of them. Finally, commands are defined to click, right-click and\nmove the cursor. The algorithm has 91.88% accuracy and can be used in different\nbackgrounds.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 10:24:51 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Foroutan", "Yalda", ""], ["Kalhor", "Ahmad", ""], ["Nejati", "Saeid Mohammadi", ""], ["Sheikhaei", "Samad", ""]]}, {"id": "2012.13191", "submitter": "Jianfeng Huang", "authors": "Yimin Lin, Jianfeng Huang, Shiguo Lian", "title": "Appearance-Invariant 6-DoF Visual Localization using Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel visual localization network when outside environment has\nchanged such as different illumination, weather and season. The visual\nlocalization network is composed of a feature extraction network and pose\nregression network. The feature extraction network is made up of an encoder\nnetwork based on the Generative Adversarial Network CycleGAN, which can capture\nintrinsic appearance-invariant feature maps from unpaired samples of different\nweathers and seasons. With such an invariant feature, we use a 6-DoF pose\nregression network to tackle long-term visual localization in the presence of\noutdoor illumination, weather and season changes. A variety of challenging\ndatasets for place recognition and localization are used to prove our visual\nlocalization network, and the results show that our method outperforms\nstate-of-the-art methods in the scenarios with various environment changes.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 10:43:43 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Lin", "Yimin", ""], ["Huang", "Jianfeng", ""], ["Lian", "Shiguo", ""]]}, {"id": "2012.13210", "submitter": "Riccardo Zanella", "authors": "Daniele De Gregorio, Riccardo Zanella, Gianluca Palli, Luigi Di\n  Stefano", "title": "Effective Deployment of CNNs for 3DoF Pose Estimation and Grasping in\n  Industrial Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate how to effectively deploy deep learning in\npractical industrial settings, such as robotic grasping applications. When a\ndeep-learning based solution is proposed, usually lacks of any simple method to\ngenerate the training data. In the industrial field, where automation is the\nmain goal, not bridging this gap is one of the main reasons why deep learning\nis not as widespread as it is in the academic world. For this reason, in this\nwork we developed a system composed by a 3-DoF Pose Estimator based on\nConvolutional Neural Networks (CNNs) and an effective procedure to gather\nmassive amounts of training images in the field with minimal human\nintervention. By automating the labeling stage, we also obtain very robust\nsystems suitable for production-level usage. An open source implementation of\nour solution is provided, alongside with the dataset used for the experimental\nevaluation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 11:59:44 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["De Gregorio", "Daniele", ""], ["Zanella", "Riccardo", ""], ["Palli", "Gianluca", ""], ["Di Stefano", "Luigi", ""]]}, {"id": "2012.13212", "submitter": "Haokui Zhang", "authors": "Haokui Zhang, Ying Li, Chengrong Gong, Hao Chen, Zongwen Bai, Chunhua\n  Shen", "title": "Memory-Efficient Hierarchical Neural Architecture Search for Image\n  Restoration", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, much attention has been spent on neural architecture search (NAS)\napproaches, which often outperform manually designed architectures on highlevel\nvision tasks. Inspired by this, we attempt to leverage NAS technique to\nautomatically design efficient network architectures for low-level image\nrestoration tasks. In this paper, we propose a memory-efficient hierarchical\nNAS HiNAS (HiNAS) and apply to two such tasks: image denoising and image\nsuper-resolution. HiNAS adopts gradient based search strategies and builds an\nflexible hierarchical search space, including inner search space and outer\nsearch space, which in charge of designing cell architectures and deciding cell\nwidths, respectively. For inner search space, we propose layerwise architecture\nsharing strategy (LWAS), resulting in more flexible architectures and better\nperformance. For outer search space, we propose cell sharing strategy to save\nmemory, and considerably accelerate the search speed. The proposed HiNAS is\nboth memory and computation efficient. With a single GTX1080Ti GPU, it takes\nonly about 1 hour for searching for denoising network on BSD 500 and 3.5 hours\nfor searching for the super-resolution structure on DIV2K. Experimental results\nshow that the architectures found by HiNAS have fewer parameters and enjoy a\nfaster inference speed, while achieving highly competitive performance compared\nwith state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 12:06:17 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 03:28:13 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhang", "Haokui", ""], ["Li", "Ying", ""], ["Gong", "Chengrong", ""], ["Chen", "Hao", ""], ["Bai", "Zongwen", ""], ["Shen", "Chunhua", ""]]}, {"id": "2012.13217", "submitter": "Ioan Marius Bilasco PhD", "authors": "Delphine Poux, Benjamin Allaert, Nacim Ihaddadene, Ioan Marius\n  Bilasco, Chaabane Djeraba and Mohammed Bennamoun", "title": "Dynamic Facial Expression Recognition under Partial Occlusion with\n  Optical Flow Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video facial expression recognition is useful for many applications and\nreceived much interest lately. Although some solutions give really good results\nin a controlled environment (no occlusion), recognition in the presence of\npartial facial occlusion remains a challenging task. To handle occlusions,\nsolutions based on the reconstruction of the occluded part of the face have\nbeen proposed. These solutions are mainly based on the texture or the geometry\nof the face. However, the similarity of the face movement between different\npersons doing the same expression seems to be a real asset for the\nreconstruction. In this paper we exploit this asset and propose a new solution\nbased on an auto-encoder with skip connections to reconstruct the occluded part\nof the face in the optical flow domain. To the best of our knowledge, this is\nthe first proposition to directly reconstruct the movement for facial\nexpression recognition. We validated our approach in the controlled dataset CK+\non which different occlusions were generated. Our experiments show that the\nproposed method reduce significantly the gap, in terms of recognition accuracy,\nbetween occluded and non-occluded situations. We also compare our approach with\nexisting state-of-the-art solutions. In order to lay the basis of a\nreproducible and fair comparison in the future, we also propose a new\nexperimental protocol that includes occlusion generation and reconstruction\nevaluation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 12:28:47 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Poux", "Delphine", ""], ["Allaert", "Benjamin", ""], ["Ihaddadene", "Nacim", ""], ["Bilasco", "Ioan Marius", ""], ["Djeraba", "Chaabane", ""], ["Bennamoun", "Mohammed", ""]]}, {"id": "2012.13235", "submitter": "Vlad Sandulescu", "authors": "Vlad Sandulescu", "title": "Detecting Hateful Memes Using a Multimodal Deep Ensemble", "comments": "6 pages, NeurIPS 2020, The Hateful Memes Challenge Workshop at\n  NeurIPS 2020", "journal-ref": "The Hateful Memes Challenge Workshop at NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While significant progress has been made using machine learning algorithms to\ndetect hate speech, important technical challenges still remain to be solved in\norder to bring their performance closer to human accuracy. We investigate\nseveral of the most recent visual-linguistic Transformer architectures and\npropose improvements to increase their performance for this task. The proposed\nmodel outperforms the baselines by a large margin and ranks 5$^{th}$ on the\nleaderboard out of 3,100+ participants.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 13:01:44 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Sandulescu", "Vlad", ""]]}, {"id": "2012.13253", "submitter": "Tal Daniel", "authors": "Tal Daniel and Aviv Tamar", "title": "Soft-IntroVAE: Analyzing and Improving the Introspective Variational\n  Autoencoder", "comments": "CVPR 2021, Extended version. Code and additional information is\n  available on the project website -\n  https://taldatech.github.io/soft-intro-vae-web", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced introspective variational autoencoder (IntroVAE)\nexhibits outstanding image generations, and allows for amortized inference\nusing an image encoder. The main idea in IntroVAE is to train a VAE\nadversarially, using the VAE encoder to discriminate between generated and real\ndata samples. However, the original IntroVAE loss function relied on a\nparticular hinge-loss formulation that is very hard to stabilize in practice,\nand its theoretical convergence analysis ignored important terms in the loss.\nIn this work, we take a step towards better understanding of the IntroVAE\nmodel, its practical implementation, and its applications. We propose the\nSoft-IntroVAE, a modified IntroVAE that replaces the hinge-loss terms with a\nsmooth exponential loss on generated samples. This change significantly\nimproves training stability, and also enables theoretical analysis of the\ncomplete algorithm. Interestingly, we show that the IntroVAE converges to a\ndistribution that minimizes a sum of KL distance from the data distribution and\nan entropy term. We discuss the implications of this result, and demonstrate\nthat it induces competitive image generation and reconstruction. Finally, we\ndescribe two applications of Soft-IntroVAE to unsupervised image translation\nand out-of-distribution detection, and demonstrate compelling results. Code and\nadditional information is available on the project website --\nhttps://taldatech.github.io/soft-intro-vae-web\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 13:53:29 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 07:12:51 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Daniel", "Tal", ""], ["Tamar", "Aviv", ""]]}, {"id": "2012.13257", "submitter": "Ivan Skorokhodov", "authors": "Ivan Skorokhodov", "title": "Interpolating Points on a Non-Uniform Grid using a Mixture of Gaussians", "comments": "5 figures, 2 equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose an approach to perform non-uniform image\ninterpolation based on a Gaussian Mixture Model. Traditional image\ninterpolation methods, like nearest neighbor, bilinear, Hamming, Lanczos, etc.\nassume that the coordinates you want to interpolate from, are positioned on a\nuniform grid. However, it is not always the case in practice and we develop an\ninterpolation method that is able to generate an image from arbitrarily\npositioned pixel values. We do this by representing each known pixel as a 2D\nnormal distribution and considering each output image pixel as a sample from\nthe mixture of all the known ones. Apart from the ability to reconstruct an\nimage from arbitrarily positioned set of pixels, this also allows us to\ndifferentiate through the interpolation procedure, which might be helpful for\ndownstream applications. Our optimized CUDA kernel and the source code to\nreproduce the benchmarks is located at\nhttps://github.com/universome/non-uniform-interpolation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 13:59:39 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Skorokhodov", "Ivan", ""]]}, {"id": "2012.13259", "submitter": "Venkat Margapuri", "authors": "Venkat Margapuri and Mitchell Neilsen", "title": "Seed Phenotyping on Neural Networks using Domain Randomization and\n  Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Seed phenotyping is the idea of analyzing the morphometric characteristics of\na seed to predict the behavior of the seed in terms of development, tolerance\nand yield in various environmental conditions. The focus of the work is the\napplication and feasibility analysis of the state-of-the-art object detection\nand localization neural networks, Mask R-CNN and YOLO (You Only Look Once), for\nseed phenotyping using Tensorflow. One of the major bottlenecks of such an\nendeavor is the need for large amounts of training data. While the capture of a\nmultitude of seed images is taunting, the images are also required to be\nannotated to indicate the boundaries of the seeds on the image and converted to\ndata formats that the neural networks are able to consume. Although tools to\nmanually perform the task of annotation are available for free, the amount of\ntime required is enormous. In order to tackle such a scenario, the idea of\ndomain randomization i.e. the technique of applying models trained on images\ncontaining simulated objects to real-world objects, is considered. In addition,\ntransfer learning i.e. the idea of applying the knowledge obtained while\nsolving a problem to a different problem, is used. The networks are trained on\npre-trained weights from the popular ImageNet and COCO data sets. As part of\nthe work, experiments with different parameters are conducted on five different\nseed types namely, canola, rough rice, sorghum, soy, and wheat.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 14:04:28 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Margapuri", "Venkat", ""], ["Neilsen", "Mitchell", ""]]}, {"id": "2012.13318", "submitter": "Dinesh Kumar Vishwakarma Dr", "authors": "Ankit Yadav, Dinesh Kumar Vishwakarma", "title": "Person Re-Identification using Deep Learning Networks: A Systematic\n  Review", "comments": "34 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Person re-identification has received a lot of attention from the research\ncommunity in recent times. Due to its vital role in security based\napplications, person re-identification lies at the heart of research relevant\nto tracking robberies, preventing terrorist attacks and other security critical\nevents. While the last decade has seen tremendous growth in re-id approaches,\nvery little review literature exists to comprehend and summarize this progress.\nThis review deals with the latest state-of-the-art deep learning based\napproaches for person re-identification. While the few existing re-id review\nworks have analysed re-id techniques from a singular aspect, this review\nevaluates numerous re-id techniques from multiple deep learning aspects such as\ndeep architecture types, common Re-Id challenges (variation in pose, lightning,\nview, scale, partial or complete occlusion, background clutter), multi-modal\nRe-Id, cross-domain Re-Id challenges, metric learning approaches and video\nRe-Id contributions. This review also includes several re-id benchmarks\ncollected over the years, describing their characteristics, specifications and\ntop re-id results obtained on them. The inclusion of the latest deep re-id\nworks makes this a significant contribution to the re-id literature. Lastly,\nthe conclusion and future directions are included.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 16:36:59 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Yadav", "Ankit", ""], ["Vishwakarma", "Dinesh Kumar", ""]]}, {"id": "2012.13321", "submitter": "Hrithwik Shalu", "authors": "Joseph Stember, Hrithwik Shalu", "title": "Unsupervised deep clustering and reinforcement learning can accurately\n  segment MRI brain tumors with very small training sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Lesion segmentation in medical imaging is key to evaluating\ntreatment response. We have recently shown that reinforcement learning can be\napplied to radiological images for lesion localization. Furthermore, we\ndemonstrated that reinforcement learning addresses important limitations of\nsupervised deep learning; namely, it can eliminate the requirement for large\namounts of annotated training data and can provide valuable intuition lacking\nin supervised approaches. However, we did not address the fundamental task of\nlesion/structure-of-interest segmentation. Here we introduce a method combining\nunsupervised deep learning clustering with reinforcement learning to segment\nbrain lesions on MRI.\n  Materials and Methods: We initially clustered images using unsupervised deep\nlearning clustering to generate candidate lesion masks for each MRI image. The\nuser then selected the best mask for each of 10 training images. We then\ntrained a reinforcement learning algorithm to select the masks. We tested the\ncorresponding trained deep Q network on a separate testing set of 10 images.\nFor comparison, we also trained and tested a U-net supervised deep learning\nnetwork on the same set of training/testing images.\n  Results: Whereas the supervised approach quickly overfit the training data\nand predictably performed poorly on the testing set (16% average Dice score),\nthe unsupervised deep clustering and reinforcement learning achieved an average\nDice score of 83%.\n  Conclusion: We have demonstrated a proof-of-principle application of\nunsupervised deep clustering and reinforcement learning to segment brain\ntumors. The approach represents human-allied AI that requires minimal input\nfrom the radiologist without the need for hand-traced annotation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 16:46:45 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 05:32:15 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Stember", "Joseph", ""], ["Shalu", "Hrithwik", ""]]}, {"id": "2012.13322", "submitter": "Yangyang Qu", "authors": "Yangyang Qu, Chao liu, Yongsheng Ou", "title": "LEUGAN:Low-Light Image Enhancement by Unsupervised Generative\n  Attentional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Restoring images from low-light data is a challenging problem. Most existing\ndeep-network based algorithms are designed to be trained with pairwise images.\nDue to the lack of real-world datasets, they usually perform poorly when\ngeneralized in practice in terms of loss of image edge and color information.\nIn this paper, we propose an unsupervised generation network with\nattention-guidance to handle the low-light image enhancement task.\nSpecifically, our network contains two parts: an edge auxiliary module that\nrestores sharper edges and an attention guidance module that recovers more\nrealistic colors. Moreover, we propose a novel loss function to make the edges\nof the generated images more visible. Experiments validate that our proposed\nalgorithm performs favorably against state-of-the-art methods, especially for\nreal-world images in terms of image clarity and noise control.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 16:49:19 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Qu", "Yangyang", ""], ["liu", "Chao", ""], ["Ou", "Yongsheng", ""]]}, {"id": "2012.13340", "submitter": "Juan Eugenio Iglesias", "authors": "Juan Eugenio Iglesias, Benjamin Billot, Yael Balbastre, Azadeh Tabari,\n  John Conklin, Daniel C. Alexander, Polina Golland, Brian L. Edlow, Bruce\n  Fischl", "title": "Joint super-resolution and synthesis of 1 mm isotropic MP-RAGE volumes\n  from clinical MRI exams with scans of different orientation, resolution and\n  contrast", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Most existing algorithms for automatic 3D morphometry of human brain MRI\nscans are designed for data with near-isotropic voxels at approximately 1 mm\nresolution, and frequently have contrast constraints as well - typically\nrequiring T1 scans (e.g., MP-RAGE). This limitation prevents the analysis of\nmillions of MRI scans acquired with large inter-slice spacing (\"thick slice\")\nin clinical settings every year. The inability to quantitatively analyze these\nscans hinders the adoption of quantitative neuroimaging in healthcare, and\nprecludes research studies that could attain huge sample sizes and hence\ngreatly improve our understanding of the human brain. Recent advances in CNNs\nare producing outstanding results in super-resolution and contrast synthesis of\nMRI. However, these approaches are very sensitive to the contrast, resolution\nand orientation of the input images, and thus do not generalize to diverse\nclinical acquisition protocols - even within sites. Here we present SynthSR, a\nmethod to train a CNN that receives one or more thick-slice scans with\ndifferent contrast, resolution and orientation, and produces an isotropic scan\nof canonical contrast (typically a 1 mm MP-RAGE). The presented method does not\nrequire any preprocessing, e.g., skull stripping or bias field correction.\nCrucially, SynthSR trains on synthetic input images generated from 3D\nsegmentations, and can thus be used to train CNNs for any combination of\ncontrasts, resolutions and orientations without high-resolution training data.\nWe test the images generated with SynthSR in an array of common downstream\nanalyses, and show that they can be reliably used for subcortical segmentation\nand volumetry, image registration (e.g., for tensor-based morphometry), and, if\nsome image quality requirements are met, even cortical thickness morphometry.\nThe source code is publicly available at github.com/BBillot/SynthSR.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 17:29:53 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Iglesias", "Juan Eugenio", ""], ["Billot", "Benjamin", ""], ["Balbastre", "Yael", ""], ["Tabari", "Azadeh", ""], ["Conklin", "John", ""], ["Alexander", "Daniel C.", ""], ["Golland", "Polina", ""], ["Edlow", "Brian L.", ""], ["Fischl", "Bruce", ""]]}, {"id": "2012.13341", "submitter": "Yuchi Zhang", "authors": "Yuchi Zhang, Willis Peng, Bastian Wandt and Helge Rhodin", "title": "AudioViewer: Learning to Visualize Sound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensory substitution can help persons with perceptual deficits. In this work,\nwe attempt to visualize audio with video. Our long-term goal is to create sound\nperception for hearing impaired people, for instance, to facilitate feedback\nfor training deaf speech. Different from existing models that translate between\nspeech and text or text and images, we target an immediate and low-level\ntranslation that applies to generic environment sounds and human speech without\ndelay. No canonical mapping is known for this artificial translation task. Our\ndesign is to translate from audio to video by compressing both into a common\nlatent space with shared structure. Our core contribution is the development\nand evaluation of learned mappings that respect human perception limits and\nmaximize user comfort by enforcing priors and combining strategies from\nunpaired image translation and disentanglement. We demonstrate qualitatively\nand quantitatively that our AudioViewer model maintains important audio\nfeatures in the generated video and that generated videos of faces and numbers\nare well suited for visualizing high-dimensional audio features since they can\neasily be parsed by humans to match and distinguish between sounds, words, and\nspeakers.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 21:52:45 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 21:35:09 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 19:51:23 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Zhang", "Yuchi", ""], ["Peng", "Willis", ""], ["Wandt", "Bastian", ""], ["Rhodin", "Helge", ""]]}, {"id": "2012.13346", "submitter": "Sophia Bethany Coban", "authors": "Sophia Bethany Coban and Vladyslav Andriiashen and Poulami Somanya\n  Ganguly and Maureen van Eijnatten and Kees Joost Batenburg", "title": "Parallel-beam X-ray CT datasets of apples with internal defects and\n  label balancing for machine learning", "comments": "Data Descriptor, to be submitted, 21 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math-ph math.MP math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present three parallel-beam tomographic datasets of 94 apples with\ninternal defects along with defect label files. The datasets are prepared for\ndevelopment and testing of data-driven, learning-based image reconstruction,\nsegmentation and post-processing methods. The three versions are a noiseless\nsimulation; simulation with added Gaussian noise, and with scattering noise.\nThe datasets are based on real 3D X-ray CT data and their subsequent volume\nreconstructions. The ground truth images, based on the volume reconstructions,\nare also available through this project. Apples contain various defects, which\nnaturally introduce a label bias. We tackle this by formulating the bias as an\noptimization problem. In addition, we demonstrate solving this problem with two\nmethods: a simple heuristic algorithm and through mixed integer quadratic\nprogramming. This ensures the datasets can be split into test, training or\nvalidation subsets with the label bias eliminated. Therefore the datasets can\nbe used for image reconstruction, segmentation, automatic defect detection, and\ntesting the effects of (as well as applying new methodologies for removing)\nlabel bias in machine learning.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 17:35:33 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Coban", "Sophia Bethany", ""], ["Andriiashen", "Vladyslav", ""], ["Ganguly", "Poulami Somanya", ""], ["van Eijnatten", "Maureen", ""], ["Batenburg", "Kees Joost", ""]]}, {"id": "2012.13364", "submitter": "Sulaiman Vesal", "authors": "Sulaiman Vesal, Mingxuan Gu, Andreas Maier, Nishant Ravikumar", "title": "Spatio-temporal Multi-task Learning for Cardiac MRI Left Ventricle\n  Quantification", "comments": "Accepted at IEEE Journal of Biomedical and Health Informatics\n  (IEEE-JBHI)", "journal-ref": null, "doi": "10.1109/JBHI.2020.3046449", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantitative assessment of cardiac left ventricle (LV) morphology is\nessential to assess cardiac function and improve the diagnosis of different\ncardiovascular diseases. In current clinical practice, LV quantification\ndepends on the measurement of myocardial shape indices, which is usually\nachieved by manual contouring of the endo- and epicardial. However, this\nprocess subjected to inter and intra-observer variability, and it is a\ntime-consuming and tedious task. In this paper, we propose a spatio-temporal\nmulti-task learning approach to obtain a complete set of measurements\nquantifying cardiac LV morphology, regional-wall thickness (RWT), and\nadditionally detecting the cardiac phase cycle (systole and diastole) for a\ngiven 3D Cine-magnetic resonance (MR) image sequence. We first segment cardiac\nLVs using an encoder-decoder network and then introduce a multitask framework\nto regress 11 LV indices and classify the cardiac phase, as parallel tasks\nduring model optimization. The proposed deep learning model is based on the 3D\nspatio-temporal convolutions, which extract spatial and temporal features from\nMR images. We demonstrate the efficacy of the proposed method using cine-MR\nsequences of 145 subjects and comparing the performance with other\nstate-of-the-art quantification methods. The proposed method obtained high\nprediction accuracy, with an average mean absolute error (MAE) of 129 $mm^2$,\n1.23 $mm$, 1.76 $mm$, Pearson correlation coefficient (PCC) of 96.4%, 87.2%,\nand 97.5% for LV and myocardium (Myo) cavity regions, 6 RWTs, 3 LV dimensions,\nand an error rate of 9.0\\% for phase classification. The experimental results\nhighlight the robustness of the proposed method, despite varying degrees of\ncardiac morphology, image appearance, and low contrast in the cardiac MR\nsequences.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 17:48:35 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Vesal", "Sulaiman", ""], ["Gu", "Mingxuan", ""], ["Maier", "Andreas", ""], ["Ravikumar", "Nishant", ""]]}, {"id": "2012.13375", "submitter": "Jiarui Xu", "authors": "Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, Han Hu", "title": "Global Context Networks", "comments": "To appear in TPAMI. Full version of GCNet: Non-local Networks Meet\n  Squeeze-Excitation Networks and Beyond (arXiv:1904.11492)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Non-Local Network (NLNet) presents a pioneering approach for capturing\nlong-range dependencies within an image, via aggregating query-specific global\ncontext to each query position. However, through a rigorous empirical analysis,\nwe have found that the global contexts modeled by the non-local network are\nalmost the same for different query positions. In this paper, we take advantage\nof this finding to create a simplified network based on a query-independent\nformulation, which maintains the accuracy of NLNet but with significantly less\ncomputation. We further replace the one-layer transformation function of the\nnon-local block by a two-layer bottleneck, which further reduces the parameter\nnumber considerably. The resulting network element, called the global context\n(GC) block, effectively models global context in a lightweight manner, allowing\nit to be applied at multiple layers of a backbone network to form a global\ncontext network (GCNet). Experiments show that GCNet generally outperforms\nNLNet on major benchmarks for various recognition tasks. The code and network\nconfigurations are available at https://github.com/xvjiarui/GCNet.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 18:02:57 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Cao", "Yue", ""], ["Xu", "Jiarui", ""], ["Lin", "Stephen", ""], ["Wei", "Fangyun", ""], ["Hu", "Han", ""]]}, {"id": "2012.13392", "submitter": "Chen Chen", "authors": "Ce Zheng and Wenhan Wu and Taojiannan Yang and Sijie Zhu and Chen Chen\n  and Ruixu Liu and Ju Shen and Nasser Kehtarnavaz and Mubarak Shah", "title": "Deep Learning-Based Human Pose Estimation: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation aims to locate the human body parts and build human\nbody representation (e.g., body skeleton) from input data such as images and\nvideos. It has drawn increasing attention during the past decade and has been\nutilized in a wide range of applications including human-computer interaction,\nmotion analysis, augmented reality, and virtual reality. Although the recently\ndeveloped deep learning-based solutions have achieved high performance in human\npose estimation, there still remain challenges due to insufficient training\ndata, depth ambiguities, and occlusion. The goal of this survey paper is to\nprovide a comprehensive review of recent deep learning-based solutions for both\n2D and 3D pose estimation via a systematic analysis and comparison of these\nsolutions based on their input data and inference procedures. More than 240\nresearch papers since 2014 are covered in this survey. Furthermore, 2D and 3D\nhuman pose estimation datasets and evaluation metrics are included.\nQuantitative performance comparisons of the reviewed methods on popular\ndatasets are summarized and discussed. Finally, the challenges involved,\napplications, and future research directions are concluded. We also provide a\nregularly updated project page: \\url{https://github.com/zczcwh/DL-HPE}\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 18:49:06 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 21:50:33 GMT"}, {"version": "v3", "created": "Sat, 2 Jan 2021 19:24:54 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zheng", "Ce", ""], ["Wu", "Wenhan", ""], ["Yang", "Taojiannan", ""], ["Zhu", "Sijie", ""], ["Chen", "Chen", ""], ["Liu", "Ruixu", ""], ["Shen", "Ju", ""], ["Kehtarnavaz", "Nasser", ""], ["Shah", "Mubarak", ""]]}, {"id": "2012.13431", "submitter": "Aditya Golatkar", "authors": "Aditya Golatkar, Alessandro Achille, Avinash Ravichandran, Marzia\n  Polito, Stefano Soatto", "title": "Mixed-Privacy Forgetting in Deep Networks", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that the influence of a subset of the training samples can be removed\n-- or \"forgotten\" -- from the weights of a network trained on large-scale image\nclassification tasks, and we provide strong computable bounds on the amount of\nremaining information after forgetting. Inspired by real-world applications of\nforgetting techniques, we introduce a novel notion of forgetting in\nmixed-privacy setting, where we know that a \"core\" subset of the training\nsamples does not need to be forgotten. While this variation of the problem is\nconceptually simple, we show that working in this setting significantly\nimproves the accuracy and guarantees of forgetting methods applied to vision\nclassification tasks. Moreover, our method allows efficient removal of all\ninformation contained in non-core data by simply setting to zero a subset of\nthe weights with minimal loss in performance. We achieve these results by\nreplacing a standard deep network with a suitable linear approximation. With\nopportune changes to the network architecture and training procedure, we show\nthat such linear approximation achieves comparable performance to the original\nnetwork and that the forgetting problem becomes quadratic and can be solved\nefficiently even for large models. Unlike previous forgetting methods on deep\nnetworks, ours can achieve close to the state-of-the-art accuracy on large\nscale vision tasks. In particular, we show that our method allows forgetting\nwithout having to trade off the model accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 19:34:56 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 14:59:25 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Golatkar", "Aditya", ""], ["Achille", "Alessandro", ""], ["Ravichandran", "Avinash", ""], ["Polito", "Marzia", ""], ["Soatto", "Stefano", ""]]}, {"id": "2012.13447", "submitter": "Tingfeng Xia", "authors": "Qinchen Wang and Sixuan Wu and Tingfeng Xia", "title": "Real-Time Facial Expression Emoji Masking with Convolutional Neural\n  Networks and Homography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural network based algorithms has shown success in many applications. In\nimage processing, Convolutional Neural Networks (CNN) can be trained to\ncategorize facial expressions of images of human faces. In this work, we create\na system that masks a student's face with a emoji of the respective emotion.\nOur system consists of three building blocks: face detection using Histogram of\nGradients (HoG) and Support Vector Machine (SVM), facial expression\ncategorization using CNN trained on FER2013 dataset, and finally masking the\nrespective emoji back onto the student's face via homography estimation. (Demo:\nhttps://youtu.be/GCjtXw1y8Pw) Our results show that this pipeline is\ndeploy-able in real-time, and is usable in educational settings.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 21:25:48 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Wang", "Qinchen", ""], ["Wu", "Sixuan", ""], ["Xia", "Tingfeng", ""]]}, {"id": "2012.13466", "submitter": "Yusheng Xu", "authors": "Rong Huang, Yusheng Xu, Uwe Stilla", "title": "GraNet: Global Relation-aware Attentional Network for ALS Point Cloud\n  Classification", "comments": "Manuscript submitted to ISPRS Journal of Photogrammetry and Remote\n  Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel neural network focusing on semantic labeling\nof ALS point clouds, which investigates the importance of long-range spatial\nand channel-wise relations and is termed as global relation-aware attentional\nnetwork (GraNet). GraNet first learns local geometric description and local\ndependencies using a local spatial discrepancy attention convolution module\n(LoSDA). In LoSDA, the orientation information, spatial distribution, and\nelevation differences are fully considered by stacking several local spatial\ngeometric learning modules and the local dependencies are embedded by using an\nattention pooling module. Then, a global relation-aware attention module (GRA),\nconsisting of a spatial relation-aware attention module (SRA) and a channel\nrelation aware attention module (CRA), are investigated to further learn the\nglobal spatial and channel-wise relationship between any spatial positions and\nfeature vectors. The aforementioned two important modules are embedded in the\nmulti-scale network architecture to further consider scale changes in large\nurban areas. We conducted comprehensive experiments on two ALS point cloud\ndatasets to evaluate the performance of our proposed framework. The results\nshow that our method can achieve higher classification accuracy compared with\nother commonly used advanced classification methods. The overall accuracy (OA)\nof our method on the ISPRS benchmark dataset can be improved to 84.5% to\nclassify nine semantic classes, with an average F1 measure (AvgF1) of 73.5%. In\ndetail, we have following F1 values for each object class: powerlines: 66.3%,\nlow vegetation: 82.8%, impervious surface: 91.8%, car: 80.7%, fence: 51.2%,\nroof: 94.6%, facades: 62.1%, shrub: 49.9%, trees: 82.1%. Besides, experiments\nwere conducted using a new ALS point cloud dataset covering highly dense urban\nareas.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 23:54:45 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Huang", "Rong", ""], ["Xu", "Yusheng", ""], ["Stilla", "Uwe", ""]]}, {"id": "2012.13478", "submitter": "Ershad Banijamali Mr.", "authors": "Ershad Banijamali, Mohsen Rohani, Elmira Amirloo, Jun Luo, Pascal\n  Poupart", "title": "Prediction by Anticipation: An Action-Conditional Prediction Method\n  based on Interaction Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In autonomous driving (AD), accurately predicting changes in the environment\ncan effectively improve safety and comfort. Due to complex interactions among\ntraffic participants, however, it is very hard to achieve accurate prediction\nfor a long horizon. To address this challenge, we propose prediction by\nanticipation, which views interaction in terms of a latent probabilistic\ngenerative process wherein some vehicles move partly in response to the\nanticipated motion of other vehicles. Under this view, consecutive data frames\ncan be factorized into sequential samples from an action-conditional\ndistribution that effectively generalizes to a wider range of actions and\ndriving situations. Our proposed prediction model, variational Bayesian in\nnature, is trained to maximize the evidence lower bound (ELBO) of the\nlog-likelihood of this conditional distribution. Evaluations of our approach\nwith prominent AD datasets NGSIM I-80 and Argoverse show significant\nimprovement over current state-of-the-art in both accuracy and generalization.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 01:39:26 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Banijamali", "Ershad", ""], ["Rohani", "Mohsen", ""], ["Amirloo", "Elmira", ""], ["Luo", "Jun", ""], ["Poupart", "Pascal", ""]]}, {"id": "2012.13493", "submitter": "Chunyuan Li", "authors": "Chunyuan Li, Xiujun Li, Lei Zhang, Baolin Peng, Mingyuan Zhou,\n  Jianfeng Gao", "title": "Self-supervised Pre-training with Hard Examples Improves Visual\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised pre-training (SSP) employs random image transformations to\ngenerate training data for visual representation learning. In this paper, we\nfirst present a modeling framework that unifies existing SSP methods as\nlearning to predict pseudo-labels. Then, we propose new data augmentation\nmethods of generating training examples whose pseudo-labels are harder to\npredict than those generated via random image transformations. Specifically, we\nuse adversarial training and CutMix to create hard examples (HEXA) to be used\nas augmented views for MoCo-v2 and DeepCluster-v2, leading to two variants\nHEXA_{MoCo} and HEXA_{DCluster}, respectively. In our experiments, we pre-train\nmodels on ImageNet and evaluate them on multiple public benchmarks. Our\nevaluation shows that the two new algorithm variants outperform their original\ncounterparts, and achieve new state-of-the-art on a wide range of tasks where\nlimited task supervision is available for fine-tuning. These results verify\nthat hard examples are instrumental in improving the generalization of the\npre-trained models.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 02:44:22 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 01:21:04 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Li", "Chunyuan", ""], ["Li", "Xiujun", ""], ["Zhang", "Lei", ""], ["Peng", "Baolin", ""], ["Zhou", "Mingyuan", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2012.13498", "submitter": "Jianyang Gu", "authors": "Jianyang Gu, Hao Luo, Weihua Chen, Yiqi Jiang, Yuqi Zhang, Shuting He,\n  Fan Wang, Hao Li, Wei Jiang", "title": "1st Place Solution to VisDA-2020: Bias Elimination for Domain Adaptive\n  Pedestrian Re-identification", "comments": "1st place solution to VisDA-2020 Challenge (ECCVW 2020). The source\n  code and trained models can be obtained at\n  https://github.com/vimar-gu/Bias-Eliminate-DA-ReID", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our proposed methods for domain adaptive pedestrian\nre-identification (Re-ID) task in Visual Domain Adaptation Challenge\n(VisDA-2020). Considering the large gap between the source domain and target\ndomain, we focused on solving two biases that influenced the performance on\ndomain adaptive pedestrian Re-ID and proposed a two-stage training procedure.\nAt the first stage, a baseline model is trained with images transferred from\nsource domain to target domain and from single camera to multiple camera\nstyles. Then we introduced a domain adaptation framework to train the model on\nsource data and target data simultaneously. Different pseudo label generation\nstrategies are adopted to continuously improve the discriminative ability of\nthe model. Finally, with multiple models ensembled and additional post\nprocessing approaches adopted, our methods achieve 76.56% mAP and 84.25% rank-1\non the test set. Codes are available at\nhttps://github.com/vimar-gu/Bias-Eliminate-DA-ReID\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 03:02:46 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Gu", "Jianyang", ""], ["Luo", "Hao", ""], ["Chen", "Weihua", ""], ["Jiang", "Yiqi", ""], ["Zhang", "Yuqi", ""], ["He", "Shuting", ""], ["Wang", "Fan", ""], ["Li", "Hao", ""], ["Jiang", "Wei", ""]]}, {"id": "2012.13501", "submitter": "Lavanya Umapathy", "authors": "Lavanya Umapathy, Wyatt Unger, Faryal Shareef, Hina Arif, Diego\n  Martin, Maria Altbach, and Ali Bilgin", "title": "A Cascaded Residual UNET for Fully Automated Segmentation of Prostate\n  and Peripheral Zone in T2-weighted 3D Fast Spin Echo Images", "comments": "3 pages, 5 figures, 2 tables, Presented at The Annual Conference of\n  International Society for Magnetic Resonance in Medicine 2019\n  (http://archive.ismrm.org/2019/0833.html)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multi-parametric MR images have been shown to be effective in the\nnon-invasive diagnosis of prostate cancer. Automated segmentation of the\nprostate eliminates the need for manual annotation by a radiologist which is\ntime consuming. This improves efficiency in the extraction of imaging features\nfor the characterization of prostate tissues. In this work, we propose a fully\nautomated cascaded deep learning architecture with residual blocks, Cascaded\nMRes-UNET, for segmentation of the prostate gland and the peripheral zone in\none pass through the network. The network yields high Dice scores\n($0.91\\pm.02$), precision ($0.91\\pm.04$), and recall scores ($0.92\\pm.03$) in\nprostate segmentation compared to manual annotations by an experienced\nradiologist. The average difference in total prostate volume estimation is less\nthan 5%.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 03:16:52 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Umapathy", "Lavanya", ""], ["Unger", "Wyatt", ""], ["Shareef", "Faryal", ""], ["Arif", "Hina", ""], ["Martin", "Diego", ""], ["Altbach", "Maria", ""], ["Bilgin", "Ali", ""]]}, {"id": "2012.13522", "submitter": "Jianwen Xie", "authors": "Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu,\n  Ying Nian Wu", "title": "Generative VoxelNet: Learning Energy-Based Models for 3D Shape Synthesis\n  and Analysis", "comments": "16 pages. IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) 2020. arXiv admin note: substantial text overlap with\n  arXiv:1804.00586", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D data that contains rich geometry information of objects and scenes is\nvaluable for understanding 3D physical world. With the recent emergence of\nlarge-scale 3D datasets, it becomes increasingly crucial to have a powerful 3D\ngenerative model for 3D shape synthesis and analysis. This paper proposes a\ndeep 3D energy-based model to represent volumetric shapes. The maximum\nlikelihood training of the model follows an \"analysis by synthesis\" scheme. The\nbenefits of the proposed model are six-fold: first, unlike GANs and VAEs, the\nmodel training does not rely on any auxiliary models; second, the model can\nsynthesize realistic 3D shapes by Markov chain Monte Carlo (MCMC); third, the\nconditional model can be applied to 3D object recovery and super resolution;\nfourth, the model can serve as a building block in a multi-grid modeling and\nsampling framework for high resolution 3D shape synthesis; fifth, the model can\nbe used to train a 3D generator via MCMC teaching; sixth, the unsupervisedly\ntrained model provides a powerful feature extractor for 3D data, which is\nuseful for 3D object classification. Experiments demonstrate that the proposed\nmodel can generate high-quality 3D shape patterns and can be useful for a wide\nvariety of 3D shape analysis.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 06:09:36 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Xie", "Jianwen", ""], ["Zheng", "Zilong", ""], ["Gao", "Ruiqi", ""], ["Wang", "Wenguan", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "2012.13538", "submitter": "Yibing Zhan", "authors": "Jun Yu, Hao Zhou, Yibing Zhan, Dacheng Tao", "title": "Comprehensive Graph-conditional Similarity Preserving Network for\n  Unsupervised Cross-modal Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unsupervised cross-modal hashing (UCMH) has become a hot topic recently.\nCurrent UCMH focuses on exploring data similarities. However, current UCMH\nmethods calculate the similarity between two data, mainly relying on the two\ndata's cross-modal features. These methods suffer from inaccurate similarity\nproblems that result in a suboptimal retrieval Hamming space, because the\ncross-modal features between the data are not sufficient to describe the\ncomplex data relationships, such as situations where two data have different\nfeature representations but share the inherent concepts. In this paper, we\ndevise a deep graph-neighbor coherence preserving network (DGCPN).\nSpecifically, DGCPN stems from graph models and explores graph-neighbor\ncoherence by consolidating the information between data and their neighbors.\nDGCPN regulates comprehensive similarity preserving losses by exploiting three\ntypes of data similarities (i.e., the graph-neighbor coherence, the coexistent\nsimilarity, and the intra- and inter-modality consistency) and designs a\nhalf-real and half-binary optimization strategy to reduce the quantization\nerrors during hashing. Essentially, DGCPN addresses the inaccurate similarity\nproblem by exploring and exploiting the data's intrinsic relationships in a\ngraph. We conduct extensive experiments on three public UCMH datasets. The\nexperimental results demonstrate the superiority of DGCPN, e.g., by improving\nthe mean average precision from 0.722 to 0.751 on MIRFlickr-25K using 64-bit\nhashing codes to retrieve texts from images. We will release the source code\npackage and the trained model on https://github.com/Atmegal/DGCPN.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 07:40:59 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Yu", "Jun", ""], ["Zhou", "Hao", ""], ["Zhan", "Yibing", ""], ["Tao", "Dacheng", ""]]}, {"id": "2012.13563", "submitter": "Tiancai Wang", "authors": "Tiancai Wang, Xiangyu Zhang, Jian Sun", "title": "Implicit Feature Pyramid Network for Object Detection", "comments": "Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an implicit feature pyramid network (i-FPN) for\nobject detection. Existing FPNs stack several cross-scale blocks to obtain\nlarge receptive field. We propose to use an implicit function, recently\nintroduced in deep equilibrium model (DEQ), to model the transformation of FPN.\nWe develop a residual-like iteration to updates the hidden states efficiently.\nExperimental results on MS COCO dataset show that i-FPN can significantly boost\ndetection performance compared to baseline detectors with ResNet-50-FPN: +3.4,\n+3.2, +3.5, +4.2, +3.2 mAP on RetinaNet, Faster-RCNN, FCOS, ATSS and\nAutoAssign, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 11:30:27 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Wang", "Tiancai", ""], ["Zhang", "Xiangyu", ""], ["Sun", "Jian", ""]]}, {"id": "2012.13576", "submitter": "Minh Le", "authors": "Minh Le, Subhradeep Kayal", "title": "Revisiting Edge Detection in Convolutional Neural Networks", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The ability to detect edges is a fundamental attribute necessary to truly\ncapture visual concepts. In this paper, we prove that edges cannot be\nrepresented properly in the first convolutional layer of a neural network, and\nfurther show that they are poorly captured in popular neural network\narchitectures such as VGG-16 and ResNet. The neural networks are found to rely\non color information, which might vary in unexpected ways outside of the\ndatasets used for their evaluation. To improve their robustness, we propose\nedge-detection units and show that they reduce performance loss and generate\nqualitatively different representations. By comparing various models, we show\nthat the robustness of edge detection is an important factor contributing to\nthe robustness of models against color noise.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 13:53:04 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Le", "Minh", ""], ["Kayal", "Subhradeep", ""]]}, {"id": "2012.13580", "submitter": "Gerhard Kurz", "authors": "Gerhard Kurz, Florian Faion, Florian Pfaff, Antonio Zea, Uwe D.\n  Hanebeck", "title": "Three-dimensional Simultaneous Shape and Pose Estimation for Extended\n  Objects Using Spherical Harmonics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new recursive method for simultaneous estimation of both the\npose and the shape of a three-dimensional extended object. The key idea of the\npresented method is to represent the shape of the object using spherical\nharmonics, similar to the way Fourier series can be used in the two-dimensional\ncase. This allows us to derive a measurement equation that can be used within\nthe framework of nonlinear filters such as the UKF. We provide both simulative\nand experimental evaluations of the novel techniques.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 14:11:17 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Kurz", "Gerhard", ""], ["Faion", "Florian", ""], ["Pfaff", "Florian", ""], ["Zea", "Antonio", ""], ["Hanebeck", "Uwe D.", ""]]}, {"id": "2012.13581", "submitter": "Ajoy Mondal Dr.", "authors": "Ajoy Mondal", "title": "Camouflaged Object Detection and Tracking: A Survey", "comments": null, "journal-ref": "International Journal of Image and Graphics, 2020", "doi": "10.1142/S021946782050028X", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Moving object detection and tracking have various applications, including\nsurveillance, anomaly detection, vehicle navigation, etc. The literature on\nobject detection and tracking is rich enough, and several essential survey\npapers exist. However, the research on camouflage object detection and tracking\nlimited due to the complexity of the problem. Existing work on this problem has\nbeen done based on either biological characteristics of the camouflaged objects\nor computer vision techniques. In this article, we review the existing\ncamouflaged object detection and tracking techniques using computer vision\nalgorithms from the theoretical point of view. This article also addresses\nseveral issues of interest as well as future research direction on this area.\nWe hope this review will help the reader to learn the recent advances in\ncamouflaged object detection and tracking.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 14:15:45 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Mondal", "Ajoy", ""]]}, {"id": "2012.13582", "submitter": "Maheshi Dissanayake", "authors": "Chirath Dasanayakaa and Maheshi Buddhinee Dissanayake", "title": "Deep Learning Methods for Screening Pulmonary Tuberculosis Using Chest\n  X-rays", "comments": "Computer Methods in Biomechanics and Biomedical Engineering: Imaging\n  & Visualization (2020)", "journal-ref": "Computer Methods in Biomechanics and Biomedical Engineering:\n  Imaging & Visualization (2020)", "doi": "10.1080/21681163.2020.1808532", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Tuberculosis (TB) is a contagious bacterial airborne disease, and is one of\nthe top 10 causes of death worldwide. According to the World Health\nOrganization (WHO), around 1.8 billion people are infected with TB and 1.6\nmillion deaths were reported in 2018. More importantly,95% of cases and deaths\nwere from developing countries. Yet, TB is a completely curable disease through\nearly diagnosis. To achieve this goal one of the key requirements is efficient\nutilization of existing diagnostic technologies, among which chest X-ray is the\nfirst line of diagnostic tool used for screening for active TB. The presented\ndeep learning pipeline consists of three different state of the art deep\nlearning architectures, to generate, segment and classify lung X-rays. Apart\nfrom this image preprocessing, image augmentation, genetic algorithm based\nhyper parameter tuning and model ensembling were used to to improve the\ndiagnostic process. We were able to achieve classification accuracy of 97.1%\n(Youden's index-0.941,sensitivity of 97.9% and specificity of 96.2%) which is a\nconsiderable improvement compared to the existing work in the literature. In\nour work, we present an highly accurate, automated TB screening system using\nchest X-rays, which would be helpful especially for low income countries with\nlow access to qualified medical professionals.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 14:21:35 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Dasanayakaa", "Chirath", ""], ["Dissanayake", "Maheshi Buddhinee", ""]]}, {"id": "2012.13587", "submitter": "Jie Liu", "authors": "Jie Liu, Chuming Li, Feng Liang, Chen Lin, Ming Sun, Junjie Yan, Wanli\n  Ouyang, Dong Xu", "title": "Inception Convolution with Efficient Dilation Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a variant of standard convolution, a dilated convolution can control\neffective receptive fields and handle large scale variance of objects without\nintroducing additional computational costs. To fully explore the potential of\ndilated convolution, we proposed a new type of dilated convolution (referred to\nas inception convolution), where the convolution operations have independent\ndilation patterns among different axes, channels and layers. To develop a\npractical method for learning complex inception convolution based on the data,\na simple but effective search algorithm, referred to as efficient dilation\noptimization (EDO), is developed. Based on statistical optimization, the EDO\nmethod operates in a low-cost manner and is extremely fast when it is applied\non large scale datasets. Empirical results validate that our method achieves\nconsistent performance gains for image recognition, object detection, instance\nsegmentation, human detection, and human pose estimation. For instance, by\nsimply replacing the 3x3 standard convolution in the ResNet-50 backbone with\ninception convolution, we significantly improve the AP of Faster R-CNN from\n36.4% to 39.2% on MS COCO.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 14:58:35 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 12:54:26 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Liu", "Jie", ""], ["Li", "Chuming", ""], ["Liang", "Feng", ""], ["Lin", "Chen", ""], ["Sun", "Ming", ""], ["Yan", "Junjie", ""], ["Ouyang", "Wanli", ""], ["Xu", "Dong", ""]]}, {"id": "2012.13605", "submitter": "Wajid Arshad Abbasi", "authors": "Wajid Arshad Abbasi, Syed Ali Abbas, Saiqa Andleeb", "title": "COVIDX: Computer-aided diagnosis of Covid-19 and its severity prediction\n  with raw digital chest X-ray images", "comments": "19 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Coronavirus disease (COVID-19) is a contagious infection caused by severe\nacute respiratory syndrome coronavirus-2 (SARS-COV-2) and it has infected and\nkilled millions of people across the globe. In the absence of specific drugs or\nvaccines for the treatment of COVID-19 and the limitation of prevailing\ndiagnostic techniques, there is a requirement for some alternate automatic\nscreening systems that can be used by the physicians to quickly identify and\nisolate the infected patients. A chest X-ray (CXR) image can be used as an\nalternative modality to detect and diagnose the COVID-19. In this study, we\npresent an automatic COVID-19 diagnostic and severity prediction (COVIDX)\nsystem that uses deep feature maps from CXR images to diagnose COVID-19 and its\nseverity prediction. The proposed system uses a three-phase classification\napproach (healthy vs unhealthy, COVID-19 vs Pneumonia, and COVID-19 severity)\nusing different shallow supervised classification algorithms. We evaluated\nCOVIDX not only through 10-fold cross2 validation and by using an external\nvalidation dataset but also in real settings by involving an experienced\nradiologist. In all the evaluation settings, COVIDX outperforms all the\nexisting stateof-the-art methods designed for this purpose. We made COVIDX\neasily accessible through a cloud-based webserver and python code available at\nhttps://sites.google.com/view/wajidarshad/software and\nhttps://github.com/wajidarshad/covidx, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 17:03:06 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Abbasi", "Wajid Arshad", ""], ["Abbas", "Syed Ali", ""], ["Andleeb", "Saiqa", ""]]}, {"id": "2012.13620", "submitter": "Sagar Gubbi", "authors": "Sagar Gubbi Venkatesh and Raviteja Upadrashta and Shishir Kolathaya\n  and Bharadwaj Amrutur", "title": "Teaching Robots Novel Objects by Pointing at Them", "comments": null, "journal-ref": null, "doi": "10.1109/RO-MAN47096.2020.9223596", "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Robots that must operate in novel environments and collaborate with humans\nmust be capable of acquiring new knowledge from human experts during operation.\nWe propose teaching a robot novel objects it has not encountered before by\npointing a hand at the new object of interest. An end-to-end neural network is\nused to attend to the novel object of interest indicated by the pointing hand\nand then to localize the object in new scenes. In order to attend to the novel\nobject indicated by the pointing hand, we propose a spatial attention\nmodulation mechanism that learns to focus on the highlighted object while\nignoring the other objects in the scene. We show that a robot arm can\nmanipulate novel objects that are highlighted by pointing a hand at them. We\nalso evaluate the performance of the proposed architecture on a synthetic\ndataset constructed using emojis and on a real-world dataset of common objects.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 20:01:25 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Venkatesh", "Sagar Gubbi", ""], ["Upadrashta", "Raviteja", ""], ["Kolathaya", "Shishir", ""], ["Amrutur", "Bharadwaj", ""]]}, {"id": "2012.13623", "submitter": "Alex Fedorov", "authors": "Alex Fedorov, Tristan Sylvain, Eloy Geenjaar, Margaux Luck, Lei Wu,\n  Thomas P. DeRamus, Alex Kirilin, Dmitry Bleklov, Vince D. Calhoun, Sergey M.\n  Plis", "title": "Self-Supervised Multimodal Domino: in Search of Biomarkers for\n  Alzheimer's Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensory input from multiple sources is crucial for robust and coherent human\nperception. Different sources contribute complementary explanatory factors.\nSimilarly, research studies often collect multimodal imaging data, each of\nwhich can provide shared and unique information. This observation motivated the\ndesign of powerful multimodal self-supervised representation-learning\nalgorithms. In this paper, we unify recent work on multimodal self-supervised\nlearning under a single framework. Observing that most self-supervised methods\noptimize similarity metrics between a set of model components, we propose a\ntaxonomy of all reasonable ways to organize this process. We first evaluate\nmodels on toy multimodal MNIST datasets and then apply them to a multimodal\nneuroimaging dataset with Alzheimer's disease patients. We find that (1)\nmultimodal contrastive learning has significant benefits over its unimodal\ncounterpart, (2) the specific composition of multiple contrastive objectives is\ncritical to performance on a downstream task, (3) maximization of the\nsimilarity between representations has a regularizing effect on a neural\nnetwork, which can sometimes lead to reduced downstream performance but still\nreveal multimodal relations. Results show that the proposed approach\noutperforms previous self-supervised encoder-decoder methods based on canonical\ncorrelation analysis (CCA) or the mixture-of-experts multimodal variational\nautoEncoder (MMVAE) on various datasets with a linear evaluation protocol.\nImportantly, we find a promising solution to uncover connections between\nmodalities through a jointly shared subspace that can help advance work in our\nsearch for neuroimaging biomarkers.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 20:28:13 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 02:02:36 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 19:20:25 GMT"}, {"version": "v4", "created": "Wed, 16 Jun 2021 22:01:20 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Fedorov", "Alex", ""], ["Sylvain", "Tristan", ""], ["Geenjaar", "Eloy", ""], ["Luck", "Margaux", ""], ["Wu", "Lei", ""], ["DeRamus", "Thomas P.", ""], ["Kirilin", "Alex", ""], ["Bleklov", "Dmitry", ""], ["Calhoun", "Vince D.", ""], ["Plis", "Sergey M.", ""]]}, {"id": "2012.13628", "submitter": "Ahmadreza Jeddi", "authors": "Ahmadreza Jeddi, Mohammad Javad Shafiee, Alexander Wong", "title": "A Simple Fine-tuning Is All You Need: Towards Robust Deep Learning Via\n  Adversarial Fine-tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Adversarial Training (AT) with Projected Gradient Descent (PGD) is an\neffective approach for improving the robustness of the deep neural networks.\nHowever, PGD AT has been shown to suffer from two main limitations: i) high\ncomputational cost, and ii) extreme overfitting during training that leads to\nreduction in model generalization. While the effect of factors such as model\ncapacity and scale of training data on adversarial robustness have been\nextensively studied, little attention has been paid to the effect of a very\nimportant parameter in every network optimization on adversarial robustness:\nthe learning rate. In particular, we hypothesize that effective learning rate\nscheduling during adversarial training can significantly reduce the overfitting\nissue, to a degree where one does not even need to adversarially train a model\nfrom scratch but can instead simply adversarially fine-tune a pre-trained\nmodel. Motivated by this hypothesis, we propose a simple yet very effective\nadversarial fine-tuning approach based on a $\\textit{slow start, fast decay}$\nlearning rate scheduling strategy which not only significantly decreases\ncomputational cost required, but also greatly improves the accuracy and\nrobustness of a deep neural network. Experimental results show that the\nproposed adversarial fine-tuning approach outperforms the state-of-the-art\nmethods on CIFAR-10, CIFAR-100 and ImageNet datasets in both test accuracy and\nthe robustness, while reducing the computational cost by 8-10$\\times$.\nFurthermore, a very important benefit of the proposed adversarial fine-tuning\napproach is that it enables the ability to improve the robustness of any\npre-trained deep neural network without needing to train the model from\nscratch, which to the best of the authors' knowledge has not been previously\ndemonstrated in research literature.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 20:50:15 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Jeddi", "Ahmadreza", ""], ["Shafiee", "Mohammad Javad", ""], ["Wong", "Alexander", ""]]}, {"id": "2012.13633", "submitter": "Krzysztof Lis", "authors": "Krzysztof Lis, Sina Honari, Pascal Fua, Mathieu Salzmann", "title": "Detecting Road Obstacles by Erasing Them", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicles can encounter a myriad of obstacles on the road, and it is\nimpossible to record them all beforehand to train a detector. Instead, we\nselect image patches and inpaint them with the surrounding road texture, which\ntends to remove obstacles from those patches. We then uses a network trained to\nrecognize discrepancies between the original patch and the inpainted one, which\nsignals an erased obstacle.\n  We also contribute a new dataset for monocular road obstacle detection, and\nshow that our approach outperforms the state-of-the-art methods on both our new\ndataset and the standard Fishyscapes Lost \\& Found benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 21:56:36 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 15:54:10 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Lis", "Krzysztof", ""], ["Honari", "Sina", ""], ["Fua", "Pascal", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "2012.13662", "submitter": "Fan Lyu", "authors": "Fan Lyu, Fuyuan Hu, Victor S. Sheng, Zhengtian Wu, Qiming Fu and\n  Baochuan Fu", "title": "Coarse to Fine: Multi-label Image Classification with Global/Local\n  Attention", "comments": "Accepted by IEEE International Smart Cities Conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In our daily life, the scenes around us are always with multiple labels\nespecially in a smart city, i.e., recognizing the information of city operation\nto response and control. Great efforts have been made by using Deep Neural\nNetworks to recognize multi-label images. Since multi-label image\nclassification is very complicated, people seek to use the attention mechanism\nto guide the classification process. However, conventional attention-based\nmethods always analyzed images directly and aggressively. It is difficult for\nthem to well understand complicated scenes. In this paper, we propose a\nglobal/local attention method that can recognize an image from coarse to fine\nby mimicking how human-beings observe images. Specifically, our global/local\nattention method first concentrates on the whole image, and then focuses on\nlocal specific objects in the image. We also propose a joint max-margin\nobjective function, which enforces that the minimum score of positive labels\nshould be larger than the maximum score of negative labels horizontally and\nvertically. This function can further improve our multi-label image\nclassification method. We evaluate the effectiveness of our method on two\npopular multi-label image datasets (i.e., Pascal VOC and MS-COCO). Our\nexperimental results show that our method outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 02:36:04 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Lyu", "Fan", ""], ["Hu", "Fuyuan", ""], ["Sheng", "Victor S.", ""], ["Wu", "Zhengtian", ""], ["Fu", "Qiming", ""], ["Fu", "Baochuan", ""]]}, {"id": "2012.13666", "submitter": "Arman Haghanifar", "authors": "Arman Haghanifar, Mahdiyar Molahasani Majdabadi, Seok-Bum Ko", "title": "PaXNet: Dental Caries Detection in Panoramic X-ray using Ensemble\n  Transfer Learning and Capsule Classifier", "comments": "14 pages, 10 figures, 7 tables, 46 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Dental caries is one of the most chronic diseases involving the majority of\nthe population during their lifetime. Caries lesions are typically diagnosed by\nradiologists relying only on their visual inspection to detect via dental\nx-rays. In many cases, dental caries is hard to identify using x-rays and can\nbe misinterpreted as shadows due to different reasons such as low image\nquality. Hence, developing a decision support system for caries detection has\nbeen a topic of interest in recent years. Here, we propose an automatic\ndiagnosis system to detect dental caries in Panoramic images for the first\ntime, to the best of authors' knowledge. The proposed model benefits from\nvarious pretrained deep learning models through transfer learning to extract\nrelevant features from x-rays and uses a capsule network to draw prediction\nresults. On a dataset of 470 Panoramic images used for features extraction,\nincluding 240 labeled images for classification, our model achieved an accuracy\nscore of 86.05\\% on the test set. The obtained score demonstrates acceptable\ndetection performance and an increase in caries detection speed, as long as the\nchallenges of using Panoramic x-rays of real patients are taken into account.\nAmong images with caries lesions in the test set, our model acquired recall\nscores of 69.44\\% and 90.52\\% for mild and severe ones, confirming the fact\nthat severe caries spots are more straightforward to detect and efficient mild\ncaries detection needs a more robust and larger dataset. Considering the\nnovelty of current research study as using Panoramic images, this work is a\nstep towards developing a fully automated efficient decision support system to\nassist domain experts.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 03:00:35 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Haghanifar", "Arman", ""], ["Majdabadi", "Mahdiyar Molahasani", ""], ["Ko", "Seok-Bum", ""]]}, {"id": "2012.13668", "submitter": "Thanh Dat Ngo", "authors": "Dat Ngo, Lam Pham, Anh Nguyen, Ben Phan, Khoa Tran, Truong Nguyen", "title": "Deep Learning Framework Applied for Predicting Anomaly of Respiratory\n  Sounds", "comments": "5 pages, 2 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a robust deep learning framework used for classifying\nanomaly of respiratory cycles. Initially, our framework starts with front-end\nfeature extraction step. This step aims to transform the respiratory input\nsound into a two-dimensional spectrogram where both spectral and temporal\nfeatures are well presented. Next, an ensemble of C- DNN and Autoencoder\nnetworks is then applied to classify into four categories of respiratory\nanomaly cycles. In this work, we conducted experiments over 2017 Internal\nConference on Biomedical Health Informatics (ICBHI) benchmark dataset. As a\nresult, we achieve competitive performances with ICBHI average score of 0.49,\nICBHI harmonic score of 0.42.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 03:09:36 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Ngo", "Dat", ""], ["Pham", "Lam", ""], ["Nguyen", "Anh", ""], ["Phan", "Ben", ""], ["Tran", "Khoa", ""], ["Nguyen", "Truong", ""]]}, {"id": "2012.13676", "submitter": "Yibo Hu", "authors": "Yibo Hu, Yuzhe Ou, Xujiang Zhao, Jin-Hee Cho, Feng Chen", "title": "Multidimensional Uncertainty-Aware Evidential Neural Networks", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional deep neural networks (NNs) have significantly contributed to the\nstate-of-the-art performance in the task of classification under various\napplication domains. However, NNs have not considered inherent uncertainty in\ndata associated with the class probabilities where misclassification under\nuncertainty may easily introduce high risk in decision making in real-world\ncontexts (e.g., misclassification of objects in roads leads to serious\naccidents). Unlike Bayesian NN that indirectly infer uncertainty through weight\nuncertainties, evidential NNs (ENNs) have been recently proposed to explicitly\nmodel the uncertainty of class probabilities and use them for classification\ntasks. An ENN offers the formulation of the predictions of NNs as subjective\nopinions and learns the function by collecting an amount of evidence that can\nform the subjective opinions by a deterministic NN from data. However, the ENN\nis trained as a black box without explicitly considering inherent uncertainty\nin data with their different root causes, such as vacuity (i.e., uncertainty\ndue to a lack of evidence) or dissonance (i.e., uncertainty due to conflicting\nevidence). By considering the multidimensional uncertainty, we proposed a novel\nuncertainty-aware evidential NN called WGAN-ENN (WENN) for solving an\nout-of-distribution (OOD) detection problem. We took a hybrid approach that\ncombines Wasserstein Generative Adversarial Network (WGAN) with ENNs to jointly\ntrain a model with prior knowledge of a certain class, which has high vacuity\nfor OOD samples. Via extensive empirical experiments based on both synthetic\nand real-world datasets, we demonstrated that the estimation of uncertainty by\nWENN can significantly help distinguish OOD samples from boundary samples. WENN\noutperformed in OOD detection when compared with other competitive\ncounterparts.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 04:28:56 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 21:05:22 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Hu", "Yibo", ""], ["Ou", "Yuzhe", ""], ["Zhao", "Xujiang", ""], ["Cho", "Jin-Hee", ""], ["Chen", "Feng", ""]]}, {"id": "2012.13681", "submitter": "Zhenghao Peng", "authors": "Quanyi Li, Zhenghao Peng, Qihang Zhang, Chunxiao Liu, Bolei Zhou", "title": "Improving the Generalization of End-to-End Driving through Procedural\n  Generation", "comments": "Website: https://decisionforce.github.io/pgdrive", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years there is a growing interest in the learning-based\nself driving system. To ensure safety, such systems are first developed and\nvalidated in simulators before being deployed in the real world. However, most\nof the existing driving simulators only contain a fixed set of scenes and a\nlimited number of configurable settings. That might easily cause the\noverfitting issue for the learning-based driving systems as well as the lack of\ntheir generalization ability to unseen scenarios. To better evaluate and\nimprove the generalization of end-to-end driving, we introduce an open-ended\nand highly configurable driving simulator called PGDrive, following a key\nfeature of procedural generation. Diverse road networks are first generated by\nthe proposed generation algorithm via sampling from elementary road blocks.\nThen they are turned into interactive training environments where traffic flows\nof nearby vehicles with realistic kinematics are rendered. We validate that\ntraining with the increasing number of procedurally generated scenes\nsignificantly improves the generalization of the agent across scenarios of\ndifferent traffic densities and road networks. Many applications such as\nmulti-agent traffic simulation and safe driving benchmark can be further built\nupon the simulator. To facilitate the joint research effort of end-to-end\ndriving, we release the simulator and pretrained models at\nhttps://decisionforce.github.io/pgdrive\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 06:23:14 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 06:30:47 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Li", "Quanyi", ""], ["Peng", "Zhenghao", ""], ["Zhang", "Qihang", ""], ["Liu", "Chunxiao", ""], ["Zhou", "Bolei", ""]]}, {"id": "2012.13689", "submitter": "Yongxing Dai", "authors": "Yongxing Dai, Jun Liu, Yan Bai, Zekun Tong, Ling-Yu Duan", "title": "Dual-Refinement: Joint Label and Feature Refinement for Unsupervised\n  Domain Adaptive Person Re-Identification", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptive (UDA) person re-identification (re-ID) is a\nchallenging task due to the missing of labels for the target domain data. To\nhandle this problem, some recent works adopt clustering algorithms to off-line\ngenerate pseudo labels, which can then be used as the supervision signal for\non-line feature learning in the target domain. However, the off-line generated\nlabels often contain lots of noise that significantly hinders the\ndiscriminability of the on-line learned features, and thus limits the final UDA\nre-ID performance. To this end, we propose a novel approach, called\nDual-Refinement, that jointly refines pseudo labels at the off-line clustering\nphase and features at the on-line training phase, to alternatively boost the\nlabel purity and feature discriminability in the target domain for more\nreliable re-ID. Specifically, at the off-line phase, a new hierarchical\nclustering scheme is proposed, which selects representative prototypes for\nevery coarse cluster. Thus, labels can be effectively refined by using the\ninherent hierarchical information of person images. Besides, at the on-line\nphase, we propose an instant memory spread-out (IM-spread-out) regularization,\nthat takes advantage of the proposed instant memory bank to store sample\nfeatures of the entire dataset and enable spread-out feature learning over the\nentire training data instantly. Our Dual-Refinement method reduces the\ninfluence of noisy labels and refines the learned features within the\nalternative training process. Experiments demonstrate that our method\noutperforms the state-of-the-art methods by a large margin.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 07:35:35 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 09:07:35 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Dai", "Yongxing", ""], ["Liu", "Jun", ""], ["Bai", "Yan", ""], ["Tong", "Zekun", ""], ["Duan", "Ling-Yu", ""]]}, {"id": "2012.13690", "submitter": "Sagar Gubbi", "authors": "Sagar Gubbi Venkatesh and Bharadwaj Amrutur", "title": "One-Shot Object Localization Using Learnt Visual Cues via Siamese\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1109/IROS40897.2019.8967881", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A robot that can operate in novel and unstructured environments must be\ncapable of recognizing new, previously unseen, objects. In this work, a visual\ncue is used to specify a novel object of interest which must be localized in\nnew environments. An end-to-end neural network equipped with a Siamese network\nis used to learn the cue, infer the object of interest, and then to localize it\nin new environments. We show that a simulated robot can pick-and-place novel\nobjects pointed to by a laser pointer. We also evaluate the performance of the\nproposed approach on a dataset derived from the Omniglot handwritten character\ndataset and on a small dataset of toys.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 07:40:00 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Venkatesh", "Sagar Gubbi", ""], ["Amrutur", "Bharadwaj", ""]]}, {"id": "2012.13692", "submitter": "Jiayu Bao", "authors": "Jiayu Bao", "title": "Sparse Adversarial Attack to Object Detection", "comments": "5 pages, CIKM Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples have gained tons of attention in recent years. Many\nadversarial attacks have been proposed to attack image classifiers, but few\nwork shift attention to object detectors. In this paper, we propose Sparse\nAdversarial Attack (SAA) which enables adversaries to perform effective evasion\nattack on detectors with bounded \\emph{l$_{0}$} norm perturbation. We select\nthe fragile position of the image and designed evasion loss function for the\ntask. Experiment results on YOLOv4 and FasterRCNN reveal the effectiveness of\nour method. In addition, our SAA shows great transferability across different\ndetectors in the black-box attack setting. Codes are available at\n\\emph{https://github.com/THUrssq/Tianchi04}.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 07:52:28 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Bao", "Jiayu", ""]]}, {"id": "2012.13697", "submitter": "Yue Zhao", "authors": "Lingming Zhang, Yue Zhao, Deyu Meng, Zhiming Cui, Chenqiang Gao, Xinbo\n  Gao, Chunfeng Lian, Dinggang Shen", "title": "TSGCNet: Discriminative Geometric Feature Learning with Two-Stream\n  GraphConvolutional Network for 3D Dental Model Segmentation", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to segment teeth precisely from digitized 3D dental models is an\nessential task in computer-aided orthodontic surgical planning. To date, deep\nlearning based methods have been popularly used to handle this task.\nState-of-the-art methods directly concatenate the raw attributes of 3D inputs,\nnamely coordinates and normal vectors of mesh cells, to train a single-stream\nnetwork for fully-automated tooth segmentation. This, however, has the drawback\nof ignoring the different geometric meanings provided by those raw attributes.\nThis issue might possibly confuse the network in learning discriminative\ngeometric features and result in many isolated false predictions on the dental\nmodel. Against this issue, we propose a two-stream graph convolutional network\n(TSGCNet) to learn multi-view geometric information from different geometric\nattributes. Our TSGCNet adopts two graph-learning streams, designed in an\ninput-aware fashion, to extract more discriminative high-level geometric\nrepresentations from coordinates and normal vectors, respectively. These\nfeature representations learned from the designed two different streams are\nfurther fused to integrate the multi-view complementary information for the\ncell-wise dense prediction task. We evaluate our proposed TSGCNet on a\nreal-patient dataset of dental models acquired by 3D intraoral scanners, and\nexperimental results demonstrate that our method significantly outperforms\nstate-of-the-art methods for 3D shape segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 08:02:56 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhang", "Lingming", ""], ["Zhao", "Yue", ""], ["Meng", "Deyu", ""], ["Cui", "Zhiming", ""], ["Gao", "Chenqiang", ""], ["Gao", "Xinbo", ""], ["Lian", "Chunfeng", ""], ["Shen", "Dinggang", ""]]}, {"id": "2012.13700", "submitter": "Elisabeth Hoppe", "authors": "Elisabeth Hoppe, Jens Wetzl, Philipp Roser, Lina Felsner, Alexander\n  Preuhs, Andreas Maier", "title": "2-D Respiration Navigation Framework for 3-D Continuous Cardiac Magnetic\n  Resonance Imaging", "comments": "Accepted for Bildverarbeitung f\\\"ur die Medizin, 07.-09.03.2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous protocols for cardiac magnetic resonance imaging enable sampling\nof the cardiac anatomy simultaneously resolved into cardiac phases. To avoid\nrespiration artifacts, associated motion during the scan has to be compensated\nfor during reconstruction. In this paper, we propose a sampling adaption to\nacquire 2-D respiration information during a continuous scan. Further, we\ndevelop a pipeline to extract the different respiration states from the\nacquired signals, which are used to reconstruct data from one respiration\nphase. Our results show the benefit of the proposed workflow on the image\nquality compared to no respiration compensation, as well as a previous 1-D\nrespiration navigation approach.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 08:29:57 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Hoppe", "Elisabeth", ""], ["Wetzl", "Jens", ""], ["Roser", "Philipp", ""], ["Felsner", "Lina", ""], ["Preuhs", "Alexander", ""], ["Maier", "Andreas", ""]]}, {"id": "2012.13716", "submitter": "Raja Kumar", "authors": "Tej pratap GVSL, Raja Kumar", "title": "Hybrid and Non-Uniform quantization methods using retro synthesis data\n  for efficient inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing quantization aware training methods attempt to compensate for the\nquantization loss by leveraging on training data, like most of the\npost-training quantization methods, and are also time consuming. Both these\nmethods are not effective for privacy constraint applications as they are\ntightly coupled with training data. In contrast, this paper proposes a\ndata-independent post-training quantization scheme that eliminates the need for\ntraining data. This is achieved by generating a faux dataset, hereafter\nreferred to as Retro-Synthesis Data, from the FP32 model layer statistics and\nfurther using it for quantization. This approach outperformed state-of-the-art\nmethods including, but not limited to, ZeroQ and DFQ on models with and without\nBatch-Normalization layers for 8, 6, and 4 bit precisions on ImageNet and\nCIFAR-10 datasets. We also introduced two futuristic variants of post-training\nquantization methods namely Hybrid Quantization and Non-Uniform Quantization\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 10:48:31 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["GVSL", "Tej pratap", ""], ["Kumar", "Raja", ""]]}, {"id": "2012.13720", "submitter": "Binjie Ding", "authors": "Yanlong Cao, Binjie Ding, Zewei He, Jiangxin Yang, Jingxi Chen,\n  Yanpeng Cao and Xin Li", "title": "Learning Inter- and Intra-frame Representations for Non-Lambertian\n  Photometric Stereo", "comments": "9 pages,8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we build a two-stage Convolutional Neural Network (CNN)\narchitecture to construct inter- and intra-frame representations based on an\narbitrary number of images captured under different light directions,\nperforming accurate normal estimation of non-Lambertian objects. We\nexperimentally investigate numerous network design alternatives for identifying\nthe optimal scheme to deploy inter-frame and intra-frame feature extraction\nmodules for the photometric stereo problem. Moreover, we propose to utilize the\neasily obtained object mask for eliminating adverse interference from invalid\nbackground regions in intra-frame spatial convolutions, thus effectively\nimprove the accuracy of normal estimation for surfaces made of dark materials\nor with cast shadows. Experimental results demonstrate that proposed masked\ntwo-stage photometric stereo CNN model (MT-PS-CNN) performs favorably against\nstate-of-the-art photometric stereo techniques in terms of both accuracy and\nefficiency. In addition, the proposed method is capable of predicting accurate\nand rich surface normal details for non-Lambertian objects of complex geometry\nand performs stably given inputs captured in both sparse and dense lighting\ndistributions.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 11:22:56 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 14:08:57 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Cao", "Yanlong", ""], ["Ding", "Binjie", ""], ["He", "Zewei", ""], ["Yang", "Jiangxin", ""], ["Chen", "Jingxi", ""], ["Cao", "Yanpeng", ""], ["Li", "Xin", ""]]}, {"id": "2012.13721", "submitter": "Helin Dutagaci", "authors": "Mouad Zine-El-Abidine, Helin Dutagaci, Gilles Galopin, David Rousseau", "title": "Assigning Apples to Individual Trees in Dense Orchards using 3D Color\n  Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a 3D color point cloud processing pipeline to count apples on\nindividual apple trees in trellis structured orchards. Fruit counting at the\ntree level requires separating trees, which is challenging in dense orchards.\nWe employ point clouds acquired from the leaf-off orchard in winter period,\nwhere the branch structure is visible, to delineate tree crowns. We localize\napples in point clouds acquired in harvest period. Alignment of the two point\nclouds enables mapping apple locations to the delineated winter cloud and\nassigning each apple to its bearing tree. Our apple assignment method achieves\nan accuracy rate higher than 95%. In addition to presenting a first proof of\nfeasibility, we also provide suggestions for further improvement on our apple\nassignment pipeline.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 11:28:27 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zine-El-Abidine", "Mouad", ""], ["Dutagaci", "Helin", ""], ["Galopin", "Gilles", ""], ["Rousseau", "David", ""]]}, {"id": "2012.13726", "submitter": "Jurandy Almeida", "authors": "Samuel Felipe dos Santos and Jurandy Almeida", "title": "Faster and Accurate Compressed Video Action Recognition Straight from\n  the Frequency Domain", "comments": null, "journal-ref": "in 2020 33rd SIBGRAPI Conference on Graphics, Patterns and Images\n  (SIBGRAPI), 2020, pp. 62-68", "doi": "10.1109/SIBGRAPI51738.2020.00017", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition has become one of the most active field of research\nin computer vision due to its wide range of applications, like surveillance,\nmedical, industrial environments, smart homes, among others. Recently, deep\nlearning has been successfully used to learn powerful and interpretable\nfeatures for recognizing human actions in videos. Most of the existing deep\nlearning approaches have been designed for processing video information as RGB\nimage sequences. For this reason, a preliminary decoding process is required,\nsince video data are often stored in a compressed format. However, a high\ncomputational load and memory usage is demanded for decoding a video. To\novercome this problem, we propose a deep neural network capable of learning\nstraight from compressed video. Our approach was evaluated on two public\nbenchmarks, the UCF-101 and HMDB-51 datasets, demonstrating comparable\nrecognition performance to the state-of-the-art methods, with the advantage of\nrunning up to 2 times faster in terms of inference speed.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 12:43:53 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Santos", "Samuel Felipe dos", ""], ["Almeida", "Jurandy", ""]]}, {"id": "2012.13736", "submitter": "Pourya Shamsolmoali", "authors": "Pourya Shamsolmoali, Masoumeh Zareapoor, Eric Granger, Huiyu Zhou,\n  Ruili Wang, M. Emre Celebi and Jie Yang", "title": "Image Synthesis with Adversarial Networks: a Comprehensive Survey and\n  Case Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Generative Adversarial Networks (GANs) have been extremely successful in\nvarious application domains such as computer vision, medicine, and natural\nlanguage processing. Moreover, transforming an object or person to a desired\nshape become a well-studied research in the GANs. GANs are powerful models for\nlearning complex distributions to synthesize semantically meaningful samples.\nHowever, there is a lack of comprehensive review in this field, especially lack\nof a collection of GANs loss-variant, evaluation metrics, remedies for diverse\nimage generation, and stable training. Given the current fast GANs development,\nin this survey, we provide a comprehensive review of adversarial models for\nimage synthesis. We summarize the synthetic image generation methods, and\ndiscuss the categories including image-to-image translation, fusion image\ngeneration, label-to-image mapping, and text-to-image translation. We organize\nthe literature based on their base models, developed ideas related to\narchitectures, constraints, loss functions, evaluation metrics, and training\ndatasets. We present milestones of adversarial models, review an extensive\nselection of previous works in various categories, and present insights on the\ndevelopment route from the model-based to data-driven methods. Further, we\nhighlight a range of potential future research directions. One of the unique\nfeatures of this review is that all software implementations of these GAN\nmethods and datasets have been collected and made available in one place at\nhttps://github.com/pshams55/GAN-Case-Study.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 13:30:42 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Shamsolmoali", "Pourya", ""], ["Zareapoor", "Masoumeh", ""], ["Granger", "Eric", ""], ["Zhou", "Huiyu", ""], ["Wang", "Ruili", ""], ["Celebi", "M. Emre", ""], ["Yang", "Jie", ""]]}, {"id": "2012.13751", "submitter": "Aditya Bharti", "authors": "Aditya Bharti, N.B. Vineeth, C.V. Jawahar", "title": "Few Shot Learning With No Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Few-shot learners aim to recognize new categories given only a small number\nof training samples. The core challenge is to avoid overfitting to the limited\ndata while ensuring good generalization to novel classes. Existing literature\nmakes use of vast amounts of annotated data by simply shifting the label\nrequirement from novel classes to base classes. Since data annotation is\ntime-consuming and costly, reducing the label requirement even further is an\nimportant goal. To that end, our paper presents a more challenging few-shot\nsetting where no label access is allowed during training or testing. By\nleveraging self-supervision for learning image representations and image\nsimilarity for classification at test time, we achieve competitive baselines\nwhile using \\textbf{zero} labels, which is at least fewer labels than\nstate-of-the-art. We hope that this work is a step towards developing few-shot\nlearning methods which do not depend on annotated data at all. Our code will be\npublicly released.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 14:40:12 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Bharti", "Aditya", ""], ["Vineeth", "N. B.", ""], ["Jawahar", "C. V.", ""]]}, {"id": "2012.13755", "submitter": "Hsu-Kuang Chiu", "authors": "Hsu-kuang Chiu, Jie Li, Rares Ambrus, Jeannette Bohg", "title": "Probabilistic 3D Multi-Modal, Multi-Object Tracking for Autonomous\n  Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-object tracking is an important ability for an autonomous vehicle to\nsafely navigate a traffic scene. Current state-of-the-art follows the\ntracking-by-detection paradigm where existing tracks are associated with\ndetected objects through some distance metric. The key challenges to increase\ntracking accuracy lie in data association and track life cycle management. We\npropose a probabilistic, multi-modal, multi-object tracking system consisting\nof different trainable modules to provide robust and data-driven tracking\nresults. First, we learn how to fuse features from 2D images and 3D LiDAR point\nclouds to capture the appearance and geometric information of an object.\nSecond, we propose to learn a metric that combines the Mahalanobis and feature\ndistances when comparing a track and a new detection in data association. And\nthird, we propose to learn when to initialize a track from an unmatched object\ndetection. Through extensive quantitative and qualitative results, we show that\nour method outperforms current state-of-the-art on the NuScenes Tracking\ndataset.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 15:00:54 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Chiu", "Hsu-kuang", ""], ["Li", "Jie", ""], ["Ambrus", "Rares", ""], ["Bohg", "Jeannette", ""]]}, {"id": "2012.13762", "submitter": "Tuan N.A. Hoang", "authors": "Tuan Hoang and Thanh-Toan Do and Tam V. Nguyen and Ngai-Man Cheung", "title": "Direct Quantization for Training Highly Accurate Low Bit-width Deep\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes two novel techniques to train deep convolutional neural\nnetworks with low bit-width weights and activations. First, to obtain low\nbit-width weights, most existing methods obtain the quantized weights by\nperforming quantization on the full-precision network weights. However, this\napproach would result in some mismatch: the gradient descent updates\nfull-precision weights, but it does not update the quantized weights. To\naddress this issue, we propose a novel method that enables {direct} updating of\nquantized weights {with learnable quantization levels} to minimize the cost\nfunction using gradient descent. Second, to obtain low bit-width activations,\nexisting works consider all channels equally. However, the activation\nquantizers could be biased toward a few channels with high-variance. To address\nthis issue, we propose a method to take into account the quantization errors of\nindividual channels. With this approach, we can learn activation quantizers\nthat minimize the quantization errors in the majority of channels. Experimental\nresults demonstrate that our proposed method achieves state-of-the-art\nperformance on the image classification task, using AlexNet, ResNet and\nMobileNetV2 architectures on CIFAR-100 and ImageNet datasets.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 15:21:18 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Hoang", "Tuan", ""], ["Do", "Thanh-Toan", ""], ["Nguyen", "Tam V.", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "2012.13763", "submitter": "Hopyong Gil", "authors": "Hopyong Gil, Sangwoo Park, Yusang Park, Wongoo Han, Juyean Hong,\n  Juneyoung Jung", "title": "Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free\n  Object Detection", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing object detectors suffer from class imbalance problems that\nhinder balanced performance. In particular, anchor free object detectors have\nto solve the background imbalance problem due to detection in a per-pixel\nprediction fashion as well as foreground imbalance problem simultaneously. In\nthis work, we propose Balance-oriented focal loss that can induce balanced\nlearning by considering both background and foreground balance comprehensively.\nThis work aims to address imbalance problem in the situation of using a general\nunbalanced data of non-extreme distribution not including few shot and the\nfocal loss for anchor free object detector. We use a batch-wise alpha-balanced\nvariant of the focal loss to deal with this imbalance problem elaborately. It\nis a simple and practical solution using only re-weighting for general\nunbalanced data. It does require neither additional learning cost nor\nstructural change during inference and grouping classes is also unnecessary.\nThrough extensive experiments, we show the performance improvement for each\ncomponent and analyze the effect of linear scheduling when using re-weighting\nfor the loss. By improving the focal loss in terms of balancing foreground\nclasses, our method achieves AP gains of +1.2 in MS-COCO for the anchor free\nreal-time detector.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 15:24:03 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Gil", "Hopyong", ""], ["Park", "Sangwoo", ""], ["Park", "Yusang", ""], ["Han", "Wongoo", ""], ["Hong", "Juyean", ""], ["Jung", "Juneyoung", ""]]}, {"id": "2012.13774", "submitter": "Milos Stojmenovic", "authors": "Jovisa Zunic, Milos Stojmenovic", "title": "An Affine moment invariant for multi-component shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce an image based algorithmic tool for analyzing multi-component\nshapes here. Due to the generic concept of multi-component shapes, our method\ncan be applied to the analysis of a wide spectrum of applications where real\nobjects are analyzed based on their shapes - i.e. on their corresponded black\nand white images. The method allocates a number to a shape, herein called a\nmulti-component shapes measure. This number/measure is invariant with respect\nto affine transformations and is established based on the theoretical frame\ndeveloped in this paper. In addition, the method is easy to implement and is\nrobust (e.g. with respect to noise). We provide two small but illustrative\nexamples related to aerial image analysis and galaxy image analysis. Also, we\nprovide some synthetic examples for a better understanding of the measure\nbehavior.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 16:26:25 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zunic", "Jovisa", ""], ["Stojmenovic", "Milos", ""]]}, {"id": "2012.13778", "submitter": "Sarah Gingichashvili", "authors": "Sarah Gingichashvili and Dani Lischinski", "title": "Evaluation and Comparison of Edge-Preserving Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Edge-preserving filters play an essential role in some of the most basic\ntasks of computational photography, such as abstraction, tonemapping, detail\nenhancement and texture removal, to name a few. The abundance and diversity of\nsmoothing operators, accompanied by a lack of methodology to evaluate output\nquality and/or perform an unbiased comparison between them, could lead to\nmisunderstanding and potential misuse of such methods. This paper introduces a\nsystematic methodology for evaluating and comparing such operators and\ndemonstrates it on a diverse set of published edge-preserving filters.\nAdditionally, we present a common baseline along which a comparison of\ndifferent operators can be achieved and use it to determine equivalent\nparameter mappings between methods. Finally, we suggest some guidelines for\nobjective comparison and evaluation of edge-preserving filters.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 16:35:36 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Gingichashvili", "Sarah", ""], ["Lischinski", "Dani", ""]]}, {"id": "2012.13801", "submitter": "Pu Zhao", "authors": "Pu Zhao, Wei Niu, Geng Yuan, Yuxuan Cai, Hsin-Hsuan Sung, Sijia Liu,\n  Xipeng Shen, Bin Ren, Yanzhi Wang, Xue Lin", "title": "Achieving Real-Time LiDAR 3D Object Detection on a Mobile Device", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  3D object detection is an important task, especially in the autonomous\ndriving application domain. However, it is challenging to support the real-time\nperformance with the limited computation and memory resources on edge-computing\ndevices in self-driving cars. To achieve this, we propose a compiler-aware\nunified framework incorporating network enhancement and pruning search with the\nreinforcement learning techniques, to enable real-time inference of 3D object\ndetection on the resource-limited edge-computing devices. Specifically, a\ngenerator Recurrent Neural Network (RNN) is employed to provide the unified\nscheme for both network enhancement and pruning search automatically, without\nhuman expertise and assistance. And the evaluated performance of the unified\nschemes can be fed back to train the generator RNN. The experimental results\ndemonstrate that the proposed framework firstly achieves real-time 3D object\ndetection on mobile devices (Samsung Galaxy S20 phone) with competitive\ndetection performance.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 19:41:15 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 00:52:04 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhao", "Pu", ""], ["Niu", "Wei", ""], ["Yuan", "Geng", ""], ["Cai", "Yuxuan", ""], ["Sung", "Hsin-Hsuan", ""], ["Liu", "Sijia", ""], ["Shen", "Xipeng", ""], ["Ren", "Bin", ""], ["Wang", "Yanzhi", ""], ["Lin", "Xue", ""]]}, {"id": "2012.13823", "submitter": "Raphael Memmesheimer", "authors": "Raphael Memmesheimer, Simon H\\\"aring, Nick Theisen, Dietrich Paulus", "title": "Skeleton-DML: Deep Metric Learning for Skeleton-Based One-Shot Action\n  Recognition", "comments": "8 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One-shot action recognition allows the recognition of human-performed actions\nwith only a single training example. This can influence human-robot-interaction\npositively by enabling the robot to react to previously unseen behaviour. We\nformulate the one-shot action recognition problem as a deep metric learning\nproblem and propose a novel image-based skeleton representation that performs\nwell in a metric learning setting. Therefore, we train a model that projects\nthe image representations into an embedding space. In embedding space the\nsimilar actions have a low euclidean distance while dissimilar actions have a\nhigher distance. The one-shot action recognition problem becomes a\nnearest-neighbor search in a set of activity reference samples. We evaluate the\nperformance of our proposed representation against a variety of other\nskeleton-based image representations. In addition, we present an ablation study\nthat shows the influence of different embedding vector sizes, losses and\naugmentation. Our approach lifts the state-of-the-art by 3.3% for the one-shot\naction recognition protocol on the NTU RGB+D 120 dataset under a comparable\ntraining setup. With additional augmentation our result improved over 7.7%.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 22:31:11 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 14:33:17 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Memmesheimer", "Raphael", ""], ["H\u00e4ring", "Simon", ""], ["Theisen", "Nick", ""], ["Paulus", "Dietrich", ""]]}, {"id": "2012.13831", "submitter": "Yassine Ouali", "authors": "Yassine Ouali, C\\'eline Hudelot, Myriam Tami", "title": "Spatial Contrastive Learning for Few-Shot Classification", "comments": "ECML/PKDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore contrastive learning for few-shot classification,\nin which we propose to use it as an additional auxiliary training objective\nacting as a data-dependent regularizer to promote more general and transferable\nfeatures. In particular, we present a novel attention-based spatial contrastive\nobjective to learn locally discriminative and class-agnostic features. As a\nresult, our approach overcomes some of the limitations of the cross-entropy\nloss, such as its excessive discrimination towards seen classes, which reduces\nthe transferability of features to unseen classes. With extensive experiments,\nwe show that the proposed method outperforms state-of-the-art approaches,\nconfirming the importance of learning good and transferable embeddings for\nfew-shot learning.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 23:39:41 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 06:27:18 GMT"}, {"version": "v3", "created": "Sun, 20 Jun 2021 15:37:31 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ouali", "Yassine", ""], ["Hudelot", "C\u00e9line", ""], ["Tami", "Myriam", ""]]}, {"id": "2012.13846", "submitter": "Pan He", "authors": "Keke Zhai, Pan He, Tania Banerjee, Anand Rangarajan, and Sanjay Ranka", "title": "SparsePipe: Parallel Deep Learning for 3D Point Clouds", "comments": "Accepted in 2020 IEEE 27th International Conference on High\n  Performance Computing, Data, and Analytics (HiPC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose SparsePipe, an efficient and asynchronous parallelism approach for\nhandling 3D point clouds with multi-GPU training. SparsePipe is built to\nsupport 3D sparse data such as point clouds. It achieves this by adopting\ngeneralized convolutions with sparse tensor representation to build expressive\nhigh-dimensional convolutional neural networks. Compared to dense solutions,\nthe new models can efficiently process irregular point clouds without densely\nsliding over the entire space, significantly reducing the memory requirements\nand allowing higher resolutions of the underlying 3D volumes for better\nperformance.\n  SparsePipe exploits intra-batch parallelism that partitions input data into\nmultiple processors and further improves the training throughput with\ninter-batch pipelining to overlap communication and computing. Besides, it\nsuitably partitions the model when the GPUs are heterogeneous such that the\ncomputing is load-balanced with reduced communication overhead.\n  Using experimental results on an eight-GPU platform, we show that SparsePipe\ncan parallelize effectively and obtain better performance on current point\ncloud benchmarks for both training and inference, compared to its dense\nsolutions.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 01:47:09 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhai", "Keke", ""], ["He", "Pan", ""], ["Banerjee", "Tania", ""], ["Rangarajan", "Anand", ""], ["Ranka", "Sanjay", ""]]}, {"id": "2012.13853", "submitter": "Shoudong Han", "authors": "Hongliang Zhang, Shoudong Han, Xiaofeng Pan, Jun Zhao", "title": "ANL: Anti-Noise Learning for Cross-Domain Person Re-Identification", "comments": "12 pages, 6 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the lack of labels and the domain diversities, it is a challenge to\nstudy person re-identification in the cross-domain setting. An admirable method\nis to optimize the target model by assigning pseudo-labels for unlabeled\nsamples through clustering. Usually, attributed to the domain gaps, the\npre-trained source domain model cannot extract appropriate target domain\nfeatures, which will dramatically affect the clustering performance and the\naccuracy of pseudo-labels. Extensive label noise will lead to sub-optimal\nsolutions doubtlessly. To solve these problems, we propose an Anti-Noise\nLearning (ANL) approach, which contains two modules. The Feature Distribution\nAlignment (FDA) module is designed to gather the id-related samples and\ndisperse id-unrelated samples, through the camera-wise contrastive learning and\nadversarial adaptation. Creating a friendly cross-feature foundation for\nclustering that is to reduce clustering noise. Besides, the Reliable Sample\nSelection (RSS) module utilizes an Auxiliary Model to correct noisy labels and\nselect reliable samples for the Main Model. In order to effectively utilize the\noutlier information generated by the clustering algorithm and RSS module, we\ntrain these samples at the instance-level. The experiments demonstrate that our\nproposed ANL framework can effectively reduce the domain conflicts and\nalleviate the influence of noisy samples, as well as superior performance\ncompared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 02:38:45 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhang", "Hongliang", ""], ["Han", "Shoudong", ""], ["Pan", "Xiaofeng", ""], ["Zhao", "Jun", ""]]}, {"id": "2012.13871", "submitter": "Jun Ma", "authors": "Jun Ma", "title": "Histogram Matching Augmentation for Domain Adaptation with Application\n  to Multi-Centre, Multi-Vendor and Multi-Disease Cardiac Image Segmentation", "comments": "3rd place in MICCAI 2020 M&Ms challenge", "journal-ref": null, "doi": null, "report-no": "10", "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) have achieved high accuracy for cardiac\nstructure segmentation if training cases and testing cases are from the same\ndistribution. However, the performance would be degraded if the testing cases\nare from a distinct domain (e.g., new MRI scanners, clinical centers). In this\npaper, we propose a histogram matching (HM) data augmentation method to\neliminate the domain gap. Specifically, our method generates new training cases\nby using HM to transfer the intensity distribution of testing cases to existing\ntraining cases. The proposed method is quite simple and can be used in a\nplug-and-play way in many segmentation tasks. The method is evaluated on MICCAI\n2020 M\\&Ms challenge, and achieves average Dice scores of 0.9051, 0.8405, and\n0.8749, and Hausdorff Distances of 9.996, 12.49, and 12.68 for the left\nventricular, myocardium, and right ventricular, respectively. Our results rank\nthe third place in MICCAI 2020 M\\&Ms challenge. The code and trained models are\npublicly available at \\url{https://github.com/JunMa11/HM_DataAug}.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 06:14:35 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Ma", "Jun", ""]]}, {"id": "2012.13894", "submitter": "Chang-Hwan Son", "authors": "Chang-Hwan Son", "title": "Layer Decomposition Learning Based on Gaussian Convolution Model and\n  Residual Deblurring for Inverse Halftoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Layer decomposition to separate an input image into base and detail layers\nhas been steadily used for image restoration. Existing residual networks based\non an additive model require residual layers with a small output range for fast\nconvergence and visual quality improvement. However, in inverse halftoning,\nhomogenous dot patterns hinder a small output range from the residual layers.\nTherefore, a new layer decomposition network based on the Gaussian convolution\nmodel (GCM) and structure-aware deblurring strategy is presented to achieve\nresidual learning for both the base and detail layers. For the base layer, a\nnew GCM-based residual subnetwork is presented. The GCM utilizes a statistical\ndistribution, in which the image difference between a blurred continuous-tone\nimage and a blurred halftoned image with a Gaussian filter can result in a\nnarrow output range. Subsequently, the GCM-based residual subnetwork uses a\nGaussian-filtered halftoned image as input and outputs the image difference as\nresidual, thereby generating the base layer, i.e., the Gaussian-blurred\ncontinuous-tone image. For the detail layer, a new structure-aware residual\ndeblurring subnetwork (SARDS) is presented. To remove the Gaussian blurring of\nthe base layer, the SARDS uses the predicted base layer as input and outputs\nthe deblurred version. To more effectively restore image structures such as\nlines and texts, a new image structure map predictor is incorporated into the\ndeblurring network to induce structure-adaptive learning. This paper provides a\nmethod to realize the residual learning of both the base and detail layers\nbased on the GCM and SARDS. In addition, it is verified that the proposed\nmethod surpasses state-of-the-art methods based on U-Net, direct deblurring\nnetworks, and progressively residual networks.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 09:15:00 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 14:04:35 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Son", "Chang-Hwan", ""]]}, {"id": "2012.13901", "submitter": "Shuo Wang", "authors": "Xudong Lv, Boya Wang, Dong Ye, Shuo Wang", "title": "LCCNet: LiDAR and Camera Self-Calibration using Cost Volume Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a novel online self-calibration approach for Light\nDetection and Ranging (LiDAR) and camera sensors. Compared to the previous\nCNN-based methods that concatenate the feature maps of the RGB image and\ndecalibrated depth image, we exploit the cost volume inspired by the PWC-Net\nfor feature matching. Besides the smooth L1-Loss of the predicted extrinsic\ncalibration parameters, an additional point cloud loss is applied. Instead of\nregress the extrinsic parameters between LiDAR and camera directly, we predict\nthe decalibrated deviation from initial calibration to the ground truth. During\ninference, the calibration error decreases further with the usage of iterative\nrefinement and the temporal filtering approach. The evaluation results on the\nKITTI dataset illustrate that our approach outperforms CNN-based\nstate-of-the-art methods in terms of a mean absolute calibration error of\n0.297cm in translation and 0.017{\\deg} in rotation with miscalibration\nmagnitudes of up to 1.5m and 20{\\deg}.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 09:41:45 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 07:44:46 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Lv", "Xudong", ""], ["Wang", "Boya", ""], ["Ye", "Dong", ""], ["Wang", "Shuo", ""]]}, {"id": "2012.13912", "submitter": "Debin Meng", "authors": "Hengshun Zhou, Debin Meng, Yuanyuan Zhang, Xiaojiang Peng, Jun Du, Kai\n  Wang, Yu Qiao", "title": "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion\n  Recognition", "comments": "Accepted by ACM ICMI'19 (2019 International Conference on Multimodal\n  Interaction)", "journal-ref": null, "doi": "10.1145/3340555.3355713", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The audio-video based emotion recognition aims to classify a given video into\nbasic emotions. In this paper, we describe our approaches in EmotiW 2019, which\nmainly explores emotion features and feature fusion strategies for audio and\nvisual modality. For emotion features, we explore audio feature with both\nspeech-spectrogram and Log Mel-spectrogram and evaluate several facial features\nwith different CNN models and different emotion pretrained strategies. For\nfusion strategies, we explore intra-modal and cross-modal fusion methods, such\nas designing attention mechanisms to highlights important emotion feature,\nexploring feature concatenation and factorized bilinear pooling (FBP) for\ncross-modal feature fusion. With careful evaluation, we obtain 65.5% on the\nAFEW validation set and 62.48% on the test set and rank third in the challenge.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 10:50:24 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhou", "Hengshun", ""], ["Meng", "Debin", ""], ["Zhang", "Yuanyuan", ""], ["Peng", "Xiaojiang", ""], ["Du", "Jun", ""], ["Wang", "Kai", ""], ["Qiao", "Yu", ""]]}, {"id": "2012.13920", "submitter": "Xin Hu", "authors": "Xin Hu, Yanfei Zhong, Chang Luo, Xinyu Wang", "title": "WHU-Hi: UAV-borne hyperspectral with high spatial resolution (H2)\n  benchmark datasets for hyperspectral image classification", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Classification is an important aspect of hyperspectral images processing and\napplication. At present, the researchers mostly use the classic airborne\nhyperspectral imagery as the benchmark dataset. However, existing datasets\nsuffer from three bottlenecks: (1) low spatial resolution; (2) low labeled\npixels proportion; (3) low degree of subclasses distinction. In this paper, a\nnew benchmark dataset named the Wuhan UAV-borne hyperspectral image (WHU-Hi)\ndataset was built for hyperspectral image classification. The WHU-Hi dataset\nwith a high spectral resolution (nm level) and a very high spatial resolution\n(cm level), which we refer to here as H2 imager. Besides, the WHU-Hi dataset\nhas a higher pixel labeling ratio and finer subclasses. Some start-of-art\nhyperspectral image classification methods benchmarked the WHU-Hi dataset, and\nthe experimental results show that WHU-Hi is a challenging dataset. We hope\nWHU-Hi dataset can become a strong benchmark to accelerate future research.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 11:28:37 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 10:42:58 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Hu", "Xin", ""], ["Zhong", "Yanfei", ""], ["Luo", "Chang", ""], ["Wang", "Xinyu", ""]]}, {"id": "2012.13936", "submitter": "Baoliang Chen", "authors": "Baoliang Chen, Lingyu Zhu, Guo Li, Hongfei Fan, and Shiqi Wang", "title": "Learning Generalized Spatial-Temporal Deep Feature Representation for\n  No-Reference Video Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a no-reference video quality assessment method,\naiming to achieve high-generalization capability in cross-content, -resolution\nand -frame rate quality prediction. In particular, we evaluate the quality of a\nvideo by learning effective feature representations in spatial-temporal domain.\nIn the spatial domain, to tackle the resolution and content variations, we\nimpose the Gaussian distribution constraints on the quality features. The\nunified distribution can significantly reduce the domain gap between different\nvideo samples, resulting in a more generalized quality feature representation.\nAlong the temporal dimension, inspired by the mechanism of visual perception,\nwe propose a pyramid temporal aggregation module by involving the short-term\nand long-term memory to aggregate the frame-level quality. Experiments show\nthat our method outperforms the state-of-the-art methods on cross-dataset\nsettings, and achieves comparable performance on intra-dataset configurations,\ndemonstrating the high-generalization capability of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 13:11:53 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 19:45:46 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Chen", "Baoliang", ""], ["Zhu", "Lingyu", ""], ["Li", "Guo", ""], ["Fan", "Hongfei", ""], ["Wang", "Shiqi", ""]]}, {"id": "2012.13955", "submitter": "Mostafa Ibrahim", "authors": "Mostafa Ibrahim, Kevin Bryson", "title": "Generalized Categorisation of Digital Pathology Whole Image Slides using\n  Unsupervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This project aims to break down large pathology images into small tiles and\nthen cluster those tiles into distinct groups without the knowledge of true\nlabels, our analysis shows how difficult certain aspects of clustering tumorous\nand non-tumorous cells can be and also shows that comparing the results of\ndifferent unsupervised approaches is not a trivial task. The project also\nprovides a software package to be used by the digital pathology community, that\nuses some of the approaches developed to perform unsupervised unsupervised tile\nclassification, which could then be easily manually labelled.\n  The project uses a mixture of techniques ranging from classical clustering\nalgorithms such as K-Means and Gaussian Mixture Models to more complicated\nfeature extraction techniques such as deep Autoencoders and Multi-loss\nlearning. Throughout the project, we attempt to set a benchmark for evaluation\nusing a few measures such as completeness scores and cluster plots.\n  Throughout our results we show that Convolutional Autoencoders manages to\nslightly outperform the rest of the approaches due to its powerful internal\nrepresentation learning abilities. Moreover, we show that Gaussian Mixture\nmodels produce better results than K-Means on average due to its flexibility in\ncapturing different clusters. We also show the huge difference in the\ndifficulties of classifying different types of pathology textures.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 14:38:22 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Ibrahim", "Mostafa", ""], ["Bryson", "Kevin", ""]]}, {"id": "2012.13973", "submitter": "Hoang Son Le Mr", "authors": "Hoang Son Le, Rini Akmeliawati, Gustavo Carneiro", "title": "Domain Generalisation with Domain Augmented Supervised Contrastive\n  Learning (Student Abstract)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain generalisation (DG) methods address the problem of domain shift, when\nthere is a mismatch between the distributions of training and target domains.\nData augmentation approaches have emerged as a promising alternative for DG.\nHowever, data augmentation alone is not sufficient to achieve lower\ngeneralisation errors. This project proposes a new method that combines data\naugmentation and domain distance minimisation to address the problems\nassociated with data augmentation and provide a guarantee on the learning\nperformance, under an existing framework. Empirically, our method outperforms\nbaseline results on DG benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 16:50:40 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Le", "Hoang Son", ""], ["Akmeliawati", "Rini", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "2012.13975", "submitter": "Piotr Koniusz", "authors": "Piotr Koniusz and Hongguang Zhang", "title": "Power Normalizations in Fine-grained Image, Few-shot Image and Graph\n  Classification", "comments": "Accepted by TPAMI, July 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power Normalizations (PN) are useful non-linear operators which tackle\nfeature imbalances in classification problems. We study PNs in the deep\nlearning setup via a novel PN layer pooling feature maps. Our layer combines\nthe feature vectors and their respective spatial locations in the feature maps\nproduced by the last convolutional layer of CNN into a positive definite matrix\nwith second-order statistics to which PN operators are applied, forming\nso-called Second-order Pooling (SOP). As the main goal of this paper is to\nstudy Power Normalizations, we investigate the role and meaning of MaxExp and\nGamma, two popular PN functions. To this end, we provide probabilistic\ninterpretations of such element-wise operators and discover surrogates with\nwell-behaved derivatives for end-to-end training. Furthermore, we look at the\nspectral applicability of MaxExp and Gamma by studying Spectral Power\nNormalizations (SPN). We show that SPN on the autocorrelation/covariance matrix\nand the Heat Diffusion Process (HDP) on a graph Laplacian matrix are closely\nrelated, thus sharing their properties. Such a finding leads us to the\nculmination of our work, a fast spectral MaxExp which is a variant of HDP for\ncovariances/autocorrelation matrices. We evaluate our ideas on fine-grained\nrecognition, scene recognition, and material classification, as well as in\nfew-shot learning and graph classification.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 17:06:06 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Koniusz", "Piotr", ""], ["Zhang", "Hongguang", ""]]}, {"id": "2012.14036", "submitter": "Alireza Shamsoshoara", "authors": "Alireza Shamsoshoara, Fatemeh Afghah, Abolfazl Razi, Liming Zheng,\n  Peter Z Ful\\'e, Erik Blasch", "title": "Aerial Imagery Pile burn detection using Deep Learning: the FLAME\n  dataset", "comments": "27 Pages, 7 Figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wildfires are one of the costliest and deadliest natural disasters in the US,\ncausing damage to millions of hectares of forest resources and threatening the\nlives of people and animals. Of particular importance are risks to firefighters\nand operational forces, which highlights the need for leveraging technology to\nminimize danger to people and property. FLAME (Fire Luminosity Airborne-based\nMachine learning Evaluation) offers a dataset of aerial images of fires along\nwith methods for fire detection and segmentation which can help firefighters\nand researchers to develop optimal fire management strategies. This paper\nprovides a fire image dataset collected by drones during a prescribed burning\npiled detritus in an Arizona pine forest. The dataset includes video recordings\nand thermal heatmaps captured by infrared cameras. The captured videos and\nimages are annotated and labeled frame-wise to help researchers easily apply\ntheir fire detection and modeling algorithms. The paper also highlights\nsolutions to two machine learning problems: (1) Binary classification of video\nframes based on the presence [and absence] of fire flames. An Artificial Neural\nNetwork (ANN) method is developed that achieved a 76% classification accuracy.\n(2) Fire detection using segmentation methods to precisely determine fire\nborders. A deep learning method is designed based on the U-Net up-sampling and\ndown-sampling approach to extract a fire mask from the video frames. Our FLAME\nmethod approached a precision of 92% and a recall of 84%. Future research will\nexpand the technique for free burning broadcast fire using thermal images.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 00:00:41 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Shamsoshoara", "Alireza", ""], ["Afghah", "Fatemeh", ""], ["Razi", "Abolfazl", ""], ["Zheng", "Liming", ""], ["Ful\u00e9", "Peter Z", ""], ["Blasch", "Erik", ""]]}, {"id": "2012.14057", "submitter": "Wang Xinglu", "authors": "Xinglu Wang", "title": "Person Re-identification with Adversarial Triplet Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is an important task and has widespread applications\nin video surveillance for public security. In the past few years, deep learning\nnetwork with triplet loss has become popular for this problem. However, the\ntriplet loss usually suffers from poor local optimal and relies heavily on the\nstrategy of hard example mining. In this paper, we propose to address this\nproblem with a new deep metric learning method called Adversarial Triplet\nEmbedding (ATE), in which we simultaneously generate adversarial triplets and\ndiscriminative feature embedding in an unified framework. In particular,\nadversarial triplets are generated by introducing adversarial perturbations\ninto the training process. This adversarial game is converted into a minimax\nproblem so as to have an optimal solution from the theoretical view. Extensive\nexperiments on several benchmark datasets demonstrate the effectiveness of the\napproach against the state-of-the-art literature.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 02:01:27 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Wang", "Xinglu", ""]]}, {"id": "2012.14061", "submitter": "Wang Xinglu", "authors": "Xinglu Wang", "title": "Adversarial Multi-scale Feature Learning for Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-identification (Person ReID) is an important topic in intelligent\nsurveillance and computer vision. It aims to accurately measure visual\nsimilarities between person images for determining whether two images\ncorrespond to the same person. The key to accurately measure visual\nsimilarities is learning discriminative features, which not only captures clues\nfrom different spatial scales, but also jointly inferences on multiple scales,\nwith the ability to determine reliability and ID-relativity of each clue. To\nachieve these goals, we propose to improve Person ReID system performance from\ntwo perspective: \\textbf{1).} Multi-scale feature learning (MSFL), which\nconsists of Cross-scale information propagation (CSIP) and Multi-scale feature\nfusion (MSFF), to dynamically fuse features cross different scales.\\textbf{2).}\nMulti-scale gradient regularizor (MSGR), to emphasize ID-related factors and\nignore irrelevant factors in an adversarial manner. Combining MSFL and MSGR,\nour method achieves the state-of-the-art performance on four commonly used\nperson-ReID datasets with neglectable test-time computation overhead.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 02:18:00 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Wang", "Xinglu", ""]]}, {"id": "2012.14066", "submitter": "Yiming Wang", "authors": "Yiming Wang, Lingchao Guo, Zhaoming Lu, Xiangming Wen, Shuang Zhou,\n  and Wanyu Meng", "title": "From Point to Space: 3D Moving Human Pose Estimation Using Commodity\n  WiFi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present Wi-Mose, the first 3D moving human pose estimation\nsystem using commodity WiFi. Previous WiFi-based works have achieved 2D and 3D\npose estimation. These solutions either capture poses from one perspective or\nconstruct poses of people who are at a fixed point, preventing their wide\nadoption in daily scenarios. To reconstruct 3D poses of people who move\nthroughout the space rather than a fixed point, we fuse the amplitude and phase\ninto Channel State Information (CSI) images which can provide both pose and\nposition information. Besides, we design a neural network to extract features\nthat are only associated with poses from CSI images and then convert the\nfeatures into key-point coordinates. Experimental results show that Wi-Mose can\nlocalize key-point with 29.7mm and 37.8mm Procrustes analysis Mean Per Joint\nPosition Error (P-MPJPE) in the Line of Sight (LoS) and Non-Line of Sight\n(NLoS) scenarios, respectively, achieving higher performance than the\nstate-of-the-art method. The results indicate that Wi-Mose can capture\nhigh-precision 3D human poses throughout the space.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 02:27:26 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Wang", "Yiming", ""], ["Guo", "Lingchao", ""], ["Lu", "Zhaoming", ""], ["Wen", "Xiangming", ""], ["Zhou", "Shuang", ""], ["Meng", "Wanyu", ""]]}, {"id": "2012.14070", "submitter": "Tao Zhang", "authors": "Tao Zhang and Yang Cong and Gan Sun and Jiahua Dong and Yuyang Liu and\n  Zhengming Ding", "title": "Generative Partial Visual-Tactile Fused Object Clustering", "comments": "9 pages; 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual-tactile fused sensing for object clustering has achieved significant\nprogresses recently, since the involvement of tactile modality can effectively\nimprove clustering performance. However, the missing data (i.e., partial data)\nissues always happen due to occlusion and noises during the data collecting\nprocess. This issue is not well solved by most existing partial multi-view\nclustering methods for the heterogeneous modality challenge. Naively employing\nthese methods would inevitably induce a negative effect and further hurt the\nperformance. To solve the mentioned challenges, we propose a Generative Partial\nVisual-Tactile Fused (i.e., GPVTF) framework for object clustering. More\nspecifically, we first do partial visual and tactile features extraction from\nthe partial visual and tactile data, respectively, and encode the extracted\nfeatures in modality-specific feature subspaces. A conditional cross-modal\nclustering generative adversarial network is then developed to synthesize one\nmodality conditioning on the other modality, which can compensate missing\nsamples and align the visual and tactile modalities naturally by adversarial\nlearning. To the end, two pseudo-label based KL-divergence losses are employed\nto update the corresponding modality-specific encoders. Extensive comparative\nexperiments on three public visual-tactile datasets prove the effectiveness of\nour method.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 02:37:03 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 08:15:48 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Zhang", "Tao", ""], ["Cong", "Yang", ""], ["Sun", "Gan", ""], ["Dong", "Jiahua", ""], ["Liu", "Yuyang", ""], ["Ding", "Zhengming", ""]]}, {"id": "2012.14092", "submitter": "James Bird", "authors": "James Bird, Kellan Colburn, Linda Petzold, Philip Lubin", "title": "Model Optimization for Deep Space Exploration via Simulators and Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning, and eventually true artificial intelligence techniques, are\nextremely important advancements in astrophysics and astronomy. We explore the\napplication of deep learning using neural networks in order to automate the\ndetection of astronomical bodies for future exploration missions, such as\nmissions to search for signatures or suitability of life. The ability to\nacquire images, analyze them, and send back those that are important, as\ndetermined by the deep learning algorithm, is critical in bandwidth-limited\napplications. Our previous foundational work solidified the concept of using\nsimulator images and deep learning in order to detect planets. Optimization of\nthis process is of vital importance, as even a small loss in accuracy might be\nthe difference between capturing and completely missing a possibly-habitable\nnearby planet. Through computer vision, deep learning, and simulators, we\nintroduce methods that optimize the detection of exoplanets. We show that\nmaximum achieved accuracy can hit above 98% for multiple model architectures,\neven with a relatively small training set.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 04:36:09 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Bird", "James", ""], ["Colburn", "Kellan", ""], ["Petzold", "Linda", ""], ["Lubin", "Philip", ""]]}, {"id": "2012.14097", "submitter": "Ali Raza Shahid", "authors": "Ali Raza Shahid, Sheheryar Khan, Hong Yan", "title": "Human Expression Recognition using Facial Shape Based Fourier\n  Descriptors Fusion", "comments": null, "journal-ref": "Proceedings Volume 11433, Twelfth International Conference on\n  Machine Vision (ICMV 2019); 114330P (2020)", "doi": "10.1117/12.2557450", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Dynamic facial expression recognition has many useful applications in social\nnetworks, multimedia content analysis, security systems and others. This\nchallenging process must be done under recurrent problems of image illumination\nand low resolution which changes at partial occlusions. This paper aims to\nproduce a new facial expression recognition method based on the changes in the\nfacial muscles. The geometric features are used to specify the facial regions\ni.e., mouth, eyes, and nose. The generic Fourier shape descriptor in\nconjunction with elliptic Fourier shape descriptor is used as an attribute to\nrepresent different emotions under frequency spectrum features. Afterwards a\nmulti-class support vector machine is applied for classification of seven human\nexpression. The statistical analysis showed our approach obtained overall\ncompetent recognition using 5-fold cross validation with high accuracy on\nwell-known facial expression dataset.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 05:01:44 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Shahid", "Ali Raza", ""], ["Khan", "Sheheryar", ""], ["Yan", "Hong", ""]]}, {"id": "2012.14106", "submitter": "Arash Mohammadi", "authors": "Arash Mohammadi, Yingxu Wang, Nastaran Enshaei, Parnian Afshar,\n  Farnoosh Naderkhani, Anastasia Oikonomou, Moezedin Javad Rafiee, Helder C. R.\n  Oliveira, Svetlana Yanushkevich, and Konstantinos N. Plataniotis", "title": "Diagnosis/Prognosis of COVID-19 Images: Challenges, Opportunities, and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The novel Coronavirus disease, COVID-19, has rapidly and abruptly changed the\nworld as we knew in 2020. It becomes the most unprecedent challenge to analytic\nepidemiology in general and signal processing theories in specific. Given its\nhigh contingency nature and adverse effects across the world, it is important\nto develop efficient processing/learning models to overcome this pandemic and\nbe prepared for potential future ones. In this regard, medical imaging plays an\nimportant role for the management of COVID-19. Human-centered interpretation of\nmedical images is, however, tedious and can be subjective. This has resulted in\na surge of interest to develop Radiomics models for analysis and interpretation\nof medical images. Signal Processing (SP) and Deep Learning (DL) models can\nassist in development of robust Radiomics solutions for diagnosis/prognosis,\nseverity assessment, treatment response, and monitoring of COVID-19 patients.\nIn this article, we aim to present an overview of the current state,\nchallenges, and opportunities of developing SP/DL-empowered models for\ndiagnosis (screening/monitoring) and prognosis (outcome prediction and severity\nassessment) of COVID-19 infection. More specifically, the article starts by\nelaborating the latest development on the theoretical framework of analytic\nepidemiology and hypersignal processing for COVID-19. Afterwards, imaging\nmodalities and Radiological characteristics of COVID-19 are discussed.\nSL/DL-based Radiomic models specific to the analysis of COVID-19 infection are\nthen described covering the following four domains: Segmentation of COVID-19\nlesions; Predictive models for outcome prediction; Severity assessment, and;\nDiagnosis/classification models. Finally, open problems and opportunities are\npresented in detail.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 05:38:44 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Mohammadi", "Arash", ""], ["Wang", "Yingxu", ""], ["Enshaei", "Nastaran", ""], ["Afshar", "Parnian", ""], ["Naderkhani", "Farnoosh", ""], ["Oikonomou", "Anastasia", ""], ["Rafiee", "Moezedin Javad", ""], ["Oliveira", "Helder C. R.", ""], ["Yanushkevich", "Svetlana", ""], ["Plataniotis", "Konstantinos N.", ""]]}, {"id": "2012.14115", "submitter": "Bowen Zhao", "authors": "Bowen Zhao, Chen Chen, Wanpeng Xiao, Xi Xiao, Qi Ju, Shutao Xia", "title": "Towards A Category-extended Object Detector without Relabeling or\n  Conflicts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detectors are typically learned based on fully-annotated training data\nwith fixed pre-defined categories. However, not all possible categories of\ninterest can be known beforehand, classes are often required to be increased\nprogressively in many realistic applications. In such scenario, only the\noriginal training set annotated with the old classes and some new training data\nlabeled with the new classes are available. Based on the limited datasets\nwithout extra manual labor, a unified detector that can handle all categories\nis strongly needed. Plain joint training leads to heavy biases and poor\nperformance due to the incomplete annotations. To avoid such situation, we\npropose a practical framework in this paper. A conflict-free loss is designed\nto avoid label ambiguity, leading to an acceptable detector in one training\nround. To further improve performance, we propose a retraining phase in which\nMonte Carlo Dropout is employed to calculate the localization confidence,\ncombined with the classification confidence, to mine more accurate bounding\nboxes, and an overlap-weighted method is employed for making better use of\npseudo annotations during retraining to achieve more powerful detectors.\nExtensive experiments conducted on multiple datasets demonstrate the\neffectiveness of our framework for category-extended object detectors.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 06:44:53 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 11:18:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhao", "Bowen", ""], ["Chen", "Chen", ""], ["Xiao", "Wanpeng", ""], ["Xiao", "Xi", ""], ["Ju", "Qi", ""], ["Xia", "Shutao", ""]]}, {"id": "2012.14117", "submitter": "Mundher Al-Shabi", "authors": "Mundher Al-Shabi, Kelvin Shak, Maxine Tan", "title": "3D Axial-Attention for Lung Nodule Classification", "comments": null, "journal-ref": "International Journal of Computer Assisted Radiology and Surgery,\n  1-6 (2021)", "doi": "10.1007/s11548-021-02415-z", "report-no": null, "categories": "eess.IV cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Purpose: In recent years, Non-Local based methods have been successfully\napplied to lung nodule classification. However, these methods offer 2D\nattention or limited 3D attention to low-resolution feature maps. Moreover,\nthey still depend on a convenient local filter such as convolution as full 3D\nattention is expensive to compute and requires a big dataset, which might not\nbe available.\n  Methods: We propose to use 3D Axial-Attention, which requires a fraction of\nthe computing power of a regular Non-Local network (i.e., self-attention).\nUnlike a regular Non-Local network, the 3D Axial-Attention network applies the\nattention operation to each axis separately. Additionally, we solve the\ninvariant position problem of the Non-Local network by proposing to add 3D\npositional encoding to shared embeddings.\n  Results: We validated the proposed method on 442 benign nodules and 406\nmalignant nodules, extracted from the public LIDC-IDRI dataset by following a\nrigorous experimental setup using only nodules annotated by at least three\nradiologists. Our results show that the 3D Axial-Attention model achieves\nstate-of-the-art performance on all evaluation metrics, including AUC and\nAccuracy.\n  Conclusions: The proposed model provides full 3D attention, whereby every\nelement (i.e., pixel) in the 3D volume space attends to every other element in\nthe nodule effectively. Thus, the 3D Axial-Attention network can be used in all\nlayers without the need for local filters. The experimental results show the\nimportance of full 3D attention for classifying lung nodules.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 06:49:09 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 06:52:30 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 10:43:26 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Al-Shabi", "Mundher", ""], ["Shak", "Kelvin", ""], ["Tan", "Maxine", ""]]}, {"id": "2012.14123", "submitter": "Li-Wei Chen", "authors": "Li-Wei Chen, Wei-Chen Chiu, Chin-Tien Wu", "title": "Spectral Analysis for Semantic Segmentation with Applications on Feature\n  Truncation and Weak Annotation", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The current neural networks for semantic segmentation usually predict the\npixel-wise semantics on the down-sampled grid of images to alleviate the\ncomputational cost for dense maps. However, the accuracy of resultant\nsegmentation maps may also be down graded particularly in the regions near\nobject boundaries. In this paper, we advance to have a deeper investigation on\nthe sampling efficiency of the down-sampled grid. By applying the spectral\nanalysis that analyze on the network back propagation process in frequency\ndomain, we discover that cross-entropy is mainly contributed by the\nlow-frequency components of segmentation maps, as well as that of the feature\nin CNNs. The network performance maintains as long as the resolution of the\ndown sampled grid meets the cut-off frequency. Such finding leads us to propose\na simple yet effective feature truncation method that limits the feature size\nin CNNs and removes the associated high-frequency components. This method can\nnot only reduce the computational cost but also maintain the performance of\nsemantic segmentation networks. Moreover, one can seamlessly integrate this\nmethod with the typical network pruning approaches for further model reduction.\nOn the other hand, we propose to employee a block-wise weak annotation for\nsemantic segmentation that captures the low-frequency information of the\nsegmentation map and is easy to collect. Using the proposed analysis scheme,\none can easily estimate the efficacy of the block-wise annotation and the\nfeature truncation method.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 07:18:25 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Chen", "Li-Wei", ""], ["Chiu", "Wei-Chen", ""], ["Wu", "Chin-Tien", ""]]}, {"id": "2012.14128", "submitter": "Yichi Zhang", "authors": "Yichi Zhang", "title": "Cascaded Convolutional Neural Network for Automatic Myocardial\n  Infarction Segmentation from Delayed-Enhancement Cardiac MRI", "comments": "MICCAI 2020 STACOM Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of myocardial contours and relevant areas like\ninfraction and no-reflow is an important step for the quantitative evaluation\nof myocardial infarction. In this work, we propose a cascaded convolutional\nneural network for automatic myocardial infarction segmentation from\ndelayed-enhancement cardiac MRI. We first use a 2D U-Net to focus on the\nintra-slice information to perform a preliminary segmentation. After that, we\nuse a 3D U-Net to utilize the volumetric spatial information for a subtle\nsegmentation. Our method is evaluated on the MICCAI 2020 EMIDEC challenge\ndataset and achieves average Dice score of 0.8786, 0.7124 and 0.7851 for\nmyocardium, infarction and no-reflow respectively, outperforms all the other\nteams of the segmentation contest.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 07:41:10 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhang", "Yichi", ""]]}, {"id": "2012.14130", "submitter": "Qiegen Liu", "authors": "Kai Hong, Jin Li, Wanyun Li, Cailian Yang, Minghui Zhang, Yuhao Wang\n  and Qiegen Liu", "title": "Joint Intensity-Gradient Guided Generative Modeling for Colorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an iterative generative model for solving the automatic\ncolorization problem. Although previous researches have shown the capability to\ngenerate plausible color, the edge color overflow and the requirement of the\nreference images still exist. The starting point of the unsupervised learning\nin this study is the observation that the gradient map possesses latent\ninformation of the image. Therefore, the inference process of the generative\nmodeling is conducted in joint intensity-gradient domain. Specifically, a set\nof intensity-gradient formed high-dimensional tensors, as the network input,\nare used to train a powerful noise conditional score network at the training\nphase. Furthermore, the joint intensity-gradient constraint in data-fidelity\nterm is proposed to limit the degree of freedom within generative model at the\niterative colorization stage, and it is conducive to edge-preserving. Extensive\nexperiments demonstrated that the system outperformed state-of-the-art methods\nwhether in quantitative comparisons or user study.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 07:52:55 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Hong", "Kai", ""], ["Li", "Jin", ""], ["Li", "Wanyun", ""], ["Yang", "Cailian", ""], ["Zhang", "Minghui", ""], ["Wang", "Yuhao", ""], ["Liu", "Qiegen", ""]]}, {"id": "2012.14131", "submitter": "Islem Rekik", "authors": "Mustafa Burak Gurbuz and Islem Rekik", "title": "Deep Graph Normalizer: A Geometric Deep Learning Approach for Estimating\n  Connectional Brain Templates", "comments": "11 pages, 2 figures", "journal-ref": "International Conference on Medical Image Computing and\n  Computer-Assisted Intervention 2020", "doi": "10.1007/978-3-030-59728-3_16", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A connectional brain template (CBT) is a normalized graph-based\nrepresentation of a population of brain networks also regarded as an average\nconnectome. CBTs are powerful tools for creating representative maps of brain\nconnectivity in typical and atypical populations. Particularly, estimating a\nwell-centered and representative CBT for populations of multi-view brain\nnetworks (MVBN) is more challenging since these networks sit on complex\nmanifolds and there is no easy way to fuse different heterogeneous network\nviews. This problem remains unexplored with the exception of a few recent works\nrooted in the assumption that the relationship between connectomes are mostly\nlinear. However, such an assumption fails to capture complex patterns and\nnon-linear variation across individuals. Besides, existing methods are simply\ncomposed of sequential MVBN processing blocks without any feedback mechanism,\nleading to error accumulation. To address these issues, we propose Deep Graph\nNormalizer (DGN), the first geometric deep learning (GDL) architecture for\nnormalizing a population of MVBNs by integrating them into a single\nconnectional brain template. Our end-to-end DGN learns how to fuse multi-view\nbrain networks while capturing non-linear patterns across subjects and\npreserving brain graph topological properties by capitalizing on graph\nconvolutional neural networks. We also introduce a randomized weighted loss\nfunction which also acts as a regularizer to minimize the distance between the\npopulation of MVBNs and the estimated CBT, thereby enforcing its centeredness.\nWe demonstrate that DGN significantly outperforms existing state-of-the-art\nmethods on estimating CBTs on both small-scale and large-scale connectomic\ndatasets in terms of both representativeness and discriminability (i.e.,\nidentifying distinctive connectivities fingerprinting each brain network\npopulation).\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 08:01:49 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Gurbuz", "Mustafa Burak", ""], ["Rekik", "Islem", ""]]}, {"id": "2012.14140", "submitter": "Peyman Tahghighi", "authors": "Peyman Tahghighi, Reza A.Zoroofi, Sare Safi, Alireza Ramezani", "title": "Analysis of Macula on Color Fundus Images Using Heightmap Reconstruction\n  Through Deep Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:2009.01601", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For medical diagnosis based on retinal images, a clear understanding of 3D\nstructure is often required but due to the 2D nature of images captured, we\ncannot infer that information. However, by utilizing 3D reconstruction methods,\nwe can recover the height information of the macula area on a fundus image\nwhich can be helpful for diagnosis and screening of macular disorders. Recent\napproaches have used shading information for heightmap prediction but their\noutput was not accurate since they ignored the dependency between nearby pixels\nand only utilized shading information. Additionally, other methods were\ndependent on the availability of more than one image of the retina which is not\navailable in practice. In this paper, motivated by the success of Conditional\nGenerative Adversarial Networks(cGANs) and deeply supervised networks, we\npropose a novel architecture for the generator which enhances the details and\nthe quality of output by progressive refinement and the use of deep supervision\nto reconstruct the height information of macula on a color fundus image.\nComparisons on our own dataset illustrate that the proposed method outperforms\nall of the state-of-the-art methods in image translation and medical image\ntranslation on this particular task. Additionally, perceptual studies also\nindicate that the proposed method can provide additional information for\nophthalmologists for diagnosis.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 08:21:55 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Tahghighi", "Peyman", ""], ["Zoroofi", "Reza A.", ""], ["Safi", "Sare", ""], ["Ramezani", "Alireza", ""]]}, {"id": "2012.14142", "submitter": "Heng Liu", "authors": "Heng Liu, Jianyong Liu, Tao Tao, Shudong Hou and Jungong Han", "title": "Perception Consistency Ultrasound Image Super-resolution via\n  Self-supervised CycleGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the limitations of sensors, the transmission medium and the intrinsic\nproperties of ultrasound, the quality of ultrasound imaging is always not\nideal, especially its low spatial resolution. To remedy this situation, deep\nlearning networks have been recently developed for ultrasound image\nsuper-resolution (SR) because of the powerful approximation capability.\nHowever, most current supervised SR methods are not suitable for ultrasound\nmedical images because the medical image samples are always rare, and usually,\nthere are no low-resolution (LR) and high-resolution (HR) training pairs in\nreality. In this work, based on self-supervision and cycle generative\nadversarial network (CycleGAN), we propose a new perception consistency\nultrasound image super-resolution (SR) method, which only requires the LR\nultrasound data and can ensure the re-degenerated image of the generated SR one\nto be consistent with the original LR image, and vice versa. We first generate\nthe HR fathers and the LR sons of the test ultrasound LR image through image\nenhancement, and then make full use of the cycle loss of LR-SR-LR and HR-LR-SR\nand the adversarial characteristics of the discriminator to promote the\ngenerator to produce better perceptually consistent SR results. The evaluation\nof PSNR/IFC/SSIM, inference efficiency and visual effects under the benchmark\nCCA-US and CCA-US datasets illustrate our proposed approach is effective and\nsuperior to other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 08:24:04 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Liu", "Heng", ""], ["Liu", "Jianyong", ""], ["Tao", "Tao", ""], ["Hou", "Shudong", ""], ["Han", "Jungong", ""]]}, {"id": "2012.14163", "submitter": "M\\'elodie Boillet", "authors": "M\\'elodie Boillet, Christopher Kermorvant, Thierry Paquet", "title": "Multiple Document Datasets Pre-training Improves Text Line Detection\n  With Deep Neural Networks", "comments": null, "journal-ref": "2020 25th International Conference on Pattern Recognition (ICPR)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a fully convolutional network for the document\nlayout analysis task. While state-of-the-art methods are using models\npre-trained on natural scene images, our method Doc-UFCN relies on a U-shaped\nmodel trained from scratch for detecting objects from historical documents. We\nconsider the line segmentation task and more generally the layout analysis\nproblem as a pixel-wise classification task then our model outputs a\npixel-labeling of the input images. We show that Doc-UFCN outperforms\nstate-of-the-art methods on various datasets and also demonstrate that the\npre-trained parts on natural scene images are not required to reach good\nresults. In addition, we show that pre-training on multiple document datasets\ncan improve the performances. We evaluate the models using various metrics to\nhave a fair and complete comparison between the methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 09:48:33 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 11:36:02 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Boillet", "M\u00e9lodie", ""], ["Kermorvant", "Christopher", ""], ["Paquet", "Thierry", ""]]}, {"id": "2012.14173", "submitter": "David Morales", "authors": "David Morales, Estefania Talavera, Beatriz Remeseiro", "title": "Playing to distraction: towards a robust training of CNN classifiers\n  through visual explanation techniques", "comments": "20 pages,3 figures, 4 tables", "journal-ref": "Neural Comput & Applic (2021)", "doi": "10.1007/s00521-021-06282-2", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of deep learning is evolving in different directions, with still\nthe need for more efficient training strategies. In this work, we present a\nnovel and robust training scheme that integrates visual explanation techniques\nin the learning process. Unlike the attention mechanisms that focus on the\nrelevant parts of images, we aim to improve the robustness of the model by\nmaking it pay attention to other regions as well. Broadly speaking, the idea is\nto distract the classifier in the learning process to force it to focus not\nonly on relevant regions but also on those that, a priori, are not so\ninformative for the discrimination of the class. We tested the proposed\napproach by embedding it into the learning process of a convolutional neural\nnetwork for the analysis and classification of two well-known datasets, namely\nStanford cars and FGVC-Aircraft. Furthermore, we evaluated our model on a\nreal-case scenario for the classification of egocentric images, allowing us to\nobtain relevant information about peoples' lifestyles. In particular, we work\non the challenging EgoFoodPlaces dataset, achieving state-of-the-art results\nwith a lower level of complexity. The obtained results indicate the suitability\nof our proposed training scheme for image classification, improving the\nrobustness of the final model.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 10:24:32 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 15:28:14 GMT"}, {"version": "v3", "created": "Thu, 29 Jul 2021 08:49:37 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Morales", "David", ""], ["Talavera", "Estefania", ""], ["Remeseiro", "Beatriz", ""]]}, {"id": "2012.14176", "submitter": "Gabriela Csurka", "authors": "Gabriela Csurka", "title": "Deep Visual Domain Adaptation", "comments": "This work is a preprint of SYNASC'20 post-proceedings to be published\n  by IEEE CPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation (DA) aims at improving the performance of a model on target\ndomains by transferring the knowledge contained in different but related source\ndomains. With recent advances in deep learning models which are extremely data\nhungry, the interest for visual DA has significantly increased in the last\ndecade and the number of related work in the field exploded. The aim of this\npaper, therefore, is to give a comprehensive overview of deep domain adaptation\nmethods for computer vision applications. First, we detail and compared\ndifferent possible ways of exploiting deep architectures for domain adaptation.\nThen, we propose an overview of recent trends in deep visual DA. Finally, we\nmention a few improvement strategies, orthogonal to these methods, that can be\napplied to these models. While we mainly focus on image classification, we give\npointers to papers that extend these ideas for other applications such as\nsemantic segmentation, object detection, person re-identifications, and others.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 10:40:09 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Csurka", "Gabriela", ""]]}, {"id": "2012.14180", "submitter": "Filippo Brandolini", "authors": "Filippo Brandolini, Guillem Domingo Ribas, Andrea Zerboni, Sam Turner", "title": "A Google Earth Engine-enabled Python approach to improve identification\n  of anthropogenic palaeo-landscape features", "comments": "33 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The necessity of sustainable development for landscapes has emerged as an\nimportant theme in recent decades. Current methods take a holistic approach to\nlandscape heritage and promote an interdisciplinary dialogue to facilitate\ncomplementary landscape management strategies. With the socio-economic values\nof the natural and cultural landscape heritage increasingly recognised\nworldwide, remote sensing tools are being used more and more to facilitate the\nrecording and management of landscape heritage. Satellite remote sensing\ntechnologies have enabled significant improvements in landscape research. The\nadvent of the cloud-based platform of Google Earth Engine has allowed the rapid\nexploration and processing of satellite imagery such as the Landsat and\nCopernicus Sentinel datasets. In this paper, the use of Sentinel-2 satellite\ndata in the identification of palaeo-riverscape features has been assessed in\nthe Po Plain, selected because it is characterized by human exploitation since\nthe Mid-Holocene. A multi-temporal approach has been adopted to investigate the\npotential of satellite imagery to detect buried hydrological and anthropogenic\nfeatures along with Spectral Index and Spectral Decomposition analysis. This\nresearch represents one of the first applications of the GEE Python API in\nlandscape studies. The complete FOSS-cloud protocol proposed here consists of a\nPython code script developed in Google Colab which could be simply adapted and\nreplicated in different areas of the world\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 10:51:45 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Brandolini", "Filippo", ""], ["Ribas", "Guillem Domingo", ""], ["Zerboni", "Andrea", ""], ["Turner", "Sam", ""]]}, {"id": "2012.14184", "submitter": "Emre Baspinar", "authors": "Emre Baspinar and Luca Calatroni and Valentina Franceschi and Dario\n  Prandi", "title": "A cortical-inspired sub-Riemannian model for Poggendorff-type visual\n  illusions", "comments": null, "journal-ref": null, "doi": "10.3390/jimaging7030041", "report-no": null, "categories": "cs.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Wilson-Cowan-type models for the mathematical description of\norientation-dependent Poggendorff-like illusions. Our modelling improves two\npreviously proposed cortical-inspired approaches embedding the sub-Riemannian\nheat kernel into the neuronal interaction term, in agreement with the\nintrinsically anisotropic functional architecture of V1 based on both local and\nlateral connections. For the numerical realisation of both models, we consider\nstandard gradient descent algorithms combined with Fourier-based approaches for\nthe efficient computation of the sub-Laplacian evolution. Our numerical results\nshow that the use of the sub-Riemannian kernel allows to reproduce numerically\nvisual misperceptions and inpainting-type biases in a stronger way in\ncomparison with the previous approaches.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 11:00:28 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 06:19:44 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Baspinar", "Emre", ""], ["Calatroni", "Luca", ""], ["Franceschi", "Valentina", ""], ["Prandi", "Dario", ""]]}, {"id": "2012.14185", "submitter": "Alex Hernandez Garcia", "authors": "Alex Hernandez-Garcia", "title": "Data augmentation and image understanding", "comments": "Digital version of the PhD thesis by Alex Hernandez-Garcia, defended\n  on the 27th of November of 2020 at the Institute of Cognitive Science of the\n  University of Osnabrueck, Germany. PhD advisor: Prof. Peter Koenig.\n  Contributors are acknowledged at the beginning of each chapter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Interdisciplinary research is often at the core of scientific progress. This\ndissertation explores some advantageous synergies between machine learning,\ncognitive science and neuroscience. In particular, this thesis focuses on\nvision and images. The human visual system has been widely studied from both\nbehavioural and neuroscientific points of view, as vision is the dominant sense\nof most people. In turn, machine vision has also been an active area of\nresearch, currently dominated by the use of artificial neural networks. This\nwork focuses on learning representations that are more aligned with visual\nperception and the biological vision. For that purpose, I have studied tools\nand aspects from cognitive science and computational neuroscience, and\nattempted to incorporate them into machine learning models of vision.\n  A central subject of this dissertation is data augmentation, a commonly used\ntechnique for training artificial neural networks to augment the size of data\nsets through transformations of the images. Although often overlooked, data\naugmentation implements transformations that are perceptually plausible, since\nthey correspond to the transformations we see in our visual world -- changes in\nviewpoint or illumination, for instance. Furthermore, neuroscientists have\nfound that the brain invariantly represents objects under these\ntransformations. Throughout this dissertation, I use these insights to analyse\ndata augmentation as a particularly useful inductive bias, a more effective\nregularisation method for artificial neural networks, and as the framework to\nanalyse and improve the invariance of vision models to perceptually plausible\ntransformations. Overall, this work aims to shed more light on the properties\nof data augmentation and demonstrate the potential of interdisciplinary\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 11:00:52 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Hernandez-Garcia", "Alex", ""]]}, {"id": "2012.14186", "submitter": "Hichem Sahbi", "authors": "Hichem Sahbi", "title": "Action Recognition with Kernel-based Graph Convolutional Networks", "comments": "arXiv admin note: text overlap with arXiv:1912.05864", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning graph convolutional networks (GCNs) is an emerging field which aims\nat generalizing deep learning to arbitrary non-regular domains. Most of the\nexisting GCNs follow a neighborhood aggregation scheme, where the\nrepresentation of a node is recursively obtained by aggregating its neighboring\nnode representations using averaging or sorting operations. However, these\noperations are either ill-posed or weak to be discriminant or increase the\nnumber of training parameters and thereby the computational complexity and the\nrisk of overfitting. In this paper, we introduce a novel GCN framework that\nachieves spatial graph convolution in a reproducing kernel Hilbert space\n(RKHS). The latter makes it possible to design, via implicit kernel\nrepresentations, convolutional graph filters in a high dimensional and more\ndiscriminating space without increasing the number of training parameters. The\nparticularity of our GCN model also resides in its ability to achieve\nconvolutions without explicitly realigning nodes in the receptive fields of the\nlearned graph filters with those of the input graphs, thereby making\nconvolutions permutation agnostic and well defined. Experiments conducted on\nthe challenging task of skeleton-based action recognition show the superiority\nof the proposed method against different baselines as well as the related work.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 11:02:51 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Sahbi", "Hichem", ""]]}, {"id": "2012.14204", "submitter": "Maryam Dialameh", "authors": "Maryam Dialameh and Ali Hamzeh and Hossein Rahmani and Amir Reza\n  Radmard and Safoura Dialameh", "title": "Screening COVID-19 Based on CT/CXR Images & Building a Publicly\n  Available CT-scan Dataset of COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid outbreak of COVID-19 threatens humans life all around the world.\nDue to insufficient diagnostic infrastructures, developing an accurate,\nefficient, inexpensive, and quick diagnostic tool is of great importance. As\nchest radiography, such as chest X-ray (CXR) and CT computed tomography (CT),\nis a possible way for screening COVID-19, developing an automatic image\nclassification tool is immensely helpful for detecting the patients with\nCOVID-19. To date, researchers have proposed several different screening\nmethods; however, none of them could achieve a reliable and highly sensitive\nperformance yet. The main drawbacks of current methods are the lack of having\nenough training data, low generalization performance, and a high rate of\nfalse-positive detection. To tackle such limitations, this study firstly builds\na large-size publicly available CT-scan dataset, consisting of more than 13k\nCT-images of more than 1000 individuals, in which 8k images are taken from 500\npatients infected with COVID-19. Secondly, we propose a deep learning model for\nscreening COVID-19 using our proposed CT dataset and report the baseline\nresults. Finally, we extend the proposed CT model for screening COVID-19 from\nCXR images using a transfer learning approach. The experimental results show\nthat the proposed CT and CXR methods achieve the AUC scores of 0.886 and 0.984\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 11:52:33 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 14:55:31 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Dialameh", "Maryam", ""], ["Hamzeh", "Ali", ""], ["Rahmani", "Hossein", ""], ["Radmard", "Amir Reza", ""], ["Dialameh", "Safoura", ""]]}, {"id": "2012.14207", "submitter": "Jun Ma", "authors": "Jun Ma, Xiaoping Yang", "title": "Combining CNN and Hybrid Active Contours for Head and Neck Tumor\n  Segmentation in CT and PET images", "comments": "Second place in MICCAI 2020 HECKTOR Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Automatic segmentation of head and neck tumors plays an important role in\nradiomics analysis. In this short paper, we propose an automatic segmentation\nmethod for head and neck tumors from PET and CT images based on the combination\nof convolutional neural networks (CNNs) and hybrid active contours.\nSpecifically, we first introduce a multi-channel 3D U-Net to segment the tumor\nwith the concatenated PET and CT images. Then, we estimate the segmentation\nuncertainty by model ensembles and define a segmentation quality score to\nselect the cases with high uncertainties. Finally, we develop a hybrid active\ncontour model to refine the high uncertainty cases. Our method ranked second\nplace in the MICCAI 2020 HECKTOR challenge with average Dice Similarity\nCoefficient, precision, and recall of 0.752, 0.838, and 0.717, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 12:12:14 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Ma", "Jun", ""], ["Yang", "Xiaoping", ""]]}, {"id": "2012.14214", "submitter": "Sen Yang", "authors": "Sen Yang and Zhibin Quan and Mu Nie and Wankou Yang", "title": "TransPose: Keypoint Localization via Transformer", "comments": "Accepted by ICCV 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While CNN-based models have made remarkable progress on human pose\nestimation, what spatial dependencies they capture to localize keypoints\nremains unclear. In this work, we propose a model called \\textbf{TransPose},\nwhich introduces Transformer for human pose estimation. The attention layers\nbuilt in Transformer enable our model to capture long-range relationships\nefficiently and also can reveal what dependencies the predicted keypoints rely\non. To predict keypoint heatmaps, the last attention layer specially acts as an\naggregator, which collects contributions from image clues and forms maximum\npositions of keypoints. Such a heatmap-based localization approach via\nTransformer conforms to the principle of Activation Maximization\n\\cite{erhan2009visualizing}. And the revealed dependencies are image-specific\nand fine-grained, which also can provide evidence of how the model handles\nspecial cases, e.g., occlusion. The experiments show that TransPose achieves\n75.8 AP and 75.0 AP on COCO validation and test-dev sets with 256 $\\times$ 192\ninput resolution, while being more lightweight and faster than mainstream CNN\narchitectures. The TransPose model also transfers very well on MPII benchmark,\nyielding 93.9\\% accuracy on test set when fine-tuned with small training costs.\nCode and pre-trained models are publicly available at\n\\url{https://github.com/yangsenius/TransPose}.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 12:33:52 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 07:15:16 GMT"}, {"version": "v3", "created": "Sat, 24 Jul 2021 09:27:05 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Yang", "Sen", ""], ["Quan", "Zhibin", ""], ["Nie", "Mu", ""], ["Yang", "Wankou", ""]]}, {"id": "2012.14230", "submitter": "Bo Li", "authors": "Bo Li, Wiro J. Niessen, Stefan Klein, Marius de Groot, M. Arfan Ikram,\n  Meike W. Vernooij, Esther E. Bron", "title": "Longitudinal diffusion MRI analysis using Segis-Net: a single-step\n  deep-learning framework for simultaneous segmentation and registration", "comments": "To appear in NeuroImage", "journal-ref": null, "doi": "10.1016/j.neuroimage.2021.118004", "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This work presents a single-step deep-learning framework for longitudinal\nimage analysis, coined Segis-Net. To optimally exploit information available in\nlongitudinal data, this method concurrently learns a multi-class segmentation\nand nonlinear registration. Segmentation and registration are modeled using a\nconvolutional neural network and optimized simultaneously for their mutual\nbenefit. An objective function that optimizes spatial correspondence for the\nsegmented structures across time-points is proposed. We applied Segis-Net to\nthe analysis of white matter tracts from N=8045 longitudinal brain MRI datasets\nof 3249 elderly individuals. Segis-Net approach showed a significant increase\nin registration accuracy, spatio-temporal segmentation consistency, and\nreproducibility comparing with two multistage pipelines. This also led to a\nsignificant reduction in the sample-size that would be required to achieve the\nsame statistical power in analyzing tract-specific measures. Thus, we expect\nthat Segis-Net can serve as a new reliable tool to support longitudinal imaging\nstudies to investigate macro- and microstructural brain changes over time.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 13:48:21 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 11:06:25 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Li", "Bo", ""], ["Niessen", "Wiro J.", ""], ["Klein", "Stefan", ""], ["de Groot", "Marius", ""], ["Ikram", "M. Arfan", ""], ["Vernooij", "Meike W.", ""], ["Bron", "Esther E.", ""]]}, {"id": "2012.14240", "submitter": "Marko Mihajlovic", "authors": "Marko Mihajlovic, Silvan Weder, Marc Pollefeys, Martin R. Oswald", "title": "DeepSurfels: Learning Online Appearance Fusion", "comments": "In Proceedings IEEE Conference on Computer Vision and Pattern\n  Recognition. CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeepSurfels, a novel hybrid scene representation for geometry and\nappearance information. DeepSurfels combines explicit and neural building\nblocks to jointly encode geometry and appearance information. In contrast to\nestablished representations, DeepSurfels better represents high-frequency\ntextures, is well-suited for online updates of appearance information, and can\nbe easily combined with machine learning methods. We further present an\nend-to-end trainable online appearance fusion pipeline that fuses information\nfrom RGB images into the proposed scene representation and is trained using\nself-supervision imposed by the reprojection error with respect to the input\nimages. Our method compares favorably to classical texture mapping approaches\nas well as recent learning-based techniques. Moreover, we demonstrate lower\nruntime, im-proved generalization capabilities, and better scalability to\nlarger scenes compared to existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 14:13:33 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 16:37:04 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Mihajlovic", "Marko", ""], ["Weder", "Silvan", ""], ["Pollefeys", "Marc", ""], ["Oswald", "Martin R.", ""]]}, {"id": "2012.14249", "submitter": "Priya Kansal Dr.", "authors": "Sabari Nathan, Priya Kansal", "title": "Lesion Net -- Skin Lesion Segmentation Using Coordinate Convolution and\n  Deep Residual Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin lesions segmentation is an important step in the process of automated\ndiagnosis of the skin melanoma. However, the accuracy of segmenting melanomas\nskin lesions is quite a challenging task due to less data for training,\nirregular shapes, unclear boundaries, and different skin colors. Our proposed\napproach helps in improving the accuracy of skin lesion segmentation. Firstly,\nwe have introduced the coordinate convolutional layer before passing the input\nimage into the encoder. This layer helps the network to decide on the features\nrelated to translation invariance which further improves the generalization\ncapacity of the model. Secondly, we have leveraged the properties of deep\nresidual units along with the convolutional layers. At last, instead of using\nonly cross-entropy or Dice-loss, we have combined the two-loss functions to\noptimize the training metrics which helps in converging the loss more quickly\nand smoothly. After training and validating the proposed model on ISIC 2018\n(60% as train set + 20% as validation set), we tested the robustness of our\ntrained model on various other datasets like ISIC 2018 (20% as test-set) ISIC\n2017, 2016 and PH2 dataset. The results show that the proposed model either\noutperform or at par with the existing skin lesion segmentation methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 14:43:04 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Nathan", "Sabari", ""], ["Kansal", "Priya", ""]]}, {"id": "2012.14253", "submitter": "Eva Agapaki", "authors": "Eva Agapaki, Ioannis Brilakis", "title": "Instance Segmentation of Industrial Point Cloud Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The challenge that this paper addresses is how to efficiently minimize the\ncost and manual labour for automatically generating object oriented geometric\nDigital Twins (gDTs) of industrial facilities, so that the benefits provide\neven more value compared to the initial investment to generate these models.\nOur previous work achieved the current state-of-the-art class segmentation\nperformance (75% average accuracy per point and average AUC 90% in the CLOI\ndataset classes) as presented in (Agapaki and Brilakis 2020) and directly\nproduces labelled point clusters of the most important to model objects (CLOI\nclasses) from laser scanned industrial data. CLOI stands for C-shapes,\nL-shapes, O-shapes, I-shapes and their combinations. However, the problem of\nautomated segmentation of individual instances that can then be used to fit\ngeometric shapes remains unsolved. We argue that the use of instance\nsegmentation algorithms has the theoretical potential to provide the output\nneeded for the generation of gDTs. We solve instance segmentation in this paper\nthrough (a) using a CLOI-Instance graph connectivity algorithm that segments\nthe point clusters of an object class into instances and (b) boundary\nsegmentation of points that improves step (a). Our method was tested on the\nCLOI benchmark dataset (Agapaki et al. 2019) and segmented instances with\n76.25% average precision and 70% average recall per point among all classes.\nThis proved that it is the first to automatically segment industrial point\ncloud shapes with no prior knowledge other than the class point label and is\nthe bedrock for efficient gDT generation in cluttered industrial point clouds.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 02:34:59 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Agapaki", "Eva", ""], ["Brilakis", "Ioannis", ""]]}, {"id": "2012.14255", "submitter": "Xiaoyu Chen", "authors": "Xiaoyu Chen, Chi Zhang, Guosheng Lin, Jing Han", "title": "Compositional Prototype Network with Multi-view Comparision for Few-Shot\n  Point Cloud Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud segmentation is a fundamental visual understanding task in 3D\nvision. A fully supervised point cloud segmentation network often requires a\nlarge amount of data with point-wise annotations, which is expensive to obtain.\nIn this work, we present the Compositional Prototype Network that can undertake\npoint cloud segmentation with only a few labeled training data. Inspired by the\nfew-shot learning literature in images, our network directly transfers label\ninformation from the limited training data to unlabeled test data for\nprediction. The network decomposes the representations of complex point cloud\ndata into a set of local regional representations and utilizes them to\ncalculate the compositional prototypes of a visual concept. Our network\nincludes a key Multi-View Comparison Component that exploits the redundant\nviews of the support set. To evaluate the proposed method, we create a new\nsegmentation benchmark dataset, ScanNet-$6^i$, which is built upon ScanNet\ndataset. Extensive experiments show that our method outperforms baselines with\na significant advantage. Moreover, when we use our network to handle the\nlong-tail problem in a fully supervised point cloud segmentation dataset, it\ncan also effectively boost the performance of the few-shot classes.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 15:01:34 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Chen", "Xiaoyu", ""], ["Zhang", "Chi", ""], ["Lin", "Guosheng", ""], ["Han", "Jing", ""]]}, {"id": "2012.14259", "submitter": "Sorina Georgiana Smeureanu", "authors": "Cristina Palmero, Javier Selva, Sorina Smeureanu, Julio C. S. Jacques\n  Junior, Albert Clap\\'es, Alexa Mosegu\\'i, Zejian Zhang, David Gallardo,\n  Georgina Guilera, David Leiva, Sergio Escalera", "title": "Context-Aware Personality Inference in Dyadic Scenarios: Introducing the\n  UDIVA Dataset", "comments": "Accepted to the 11th International Workshop on Human Behavior\n  Understanding workshop at Winter Conference on Applications of Computer\n  Vision 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces UDIVA, a new non-acted dataset of face-to-face dyadic\ninteractions, where interlocutors perform competitive and collaborative tasks\nwith different behavior elicitation and cognitive workload. The dataset\nconsists of 90.5 hours of dyadic interactions among 147 participants\ndistributed in 188 sessions, recorded using multiple audiovisual and\nphysiological sensors. Currently, it includes sociodemographic, self- and\npeer-reported personality, internal state, and relationship profiling from\nparticipants. As an initial analysis on UDIVA, we propose a transformer-based\nmethod for self-reported personality inference in dyadic scenarios, which uses\naudiovisual data and different sources of context from both interlocutors to\nregress a target person's personality traits. Preliminary results from an\nincremental study show consistent improvements when using all available context\ninformation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 15:08:02 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Palmero", "Cristina", ""], ["Selva", "Javier", ""], ["Smeureanu", "Sorina", ""], ["Junior", "Julio C. S. Jacques", ""], ["Clap\u00e9s", "Albert", ""], ["Mosegu\u00ed", "Alexa", ""], ["Zhang", "Zejian", ""], ["Gallardo", "David", ""], ["Guilera", "Georgina", ""], ["Leiva", "David", ""], ["Escalera", "Sergio", ""]]}, {"id": "2012.14283", "submitter": "Sarah Schwettmann", "authors": "Sarah Schwettmann, Hendrik Strobelt, Mauro Martino", "title": "Latent Compass: Creation by Navigation", "comments": "3 pages, 2 figures, accepted at the 4th Workshop on Machine Learning\n  for Creativity and Design at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In Marius von Senden's Space and Sight, a newly sighted blind patient\ndescribes the experience of a corner as lemon-like, because corners \"prick\"\nsight like lemons prick the tongue. Prickliness, here, is a dimension in the\nfeature space of sensory experience, an effect of the perceived on the\nperceiver that arises where the two interact. In the account of the newly\nsighted, an effect familiar from one interaction translates to a novel context.\nPerception serves as the vehicle for generalization, in that an effect shared\nacross different experiences produces a concrete abstraction grounded in those\nexperiences. Cezanne and the post-impressionists, fluent in the language of\nexperience translation, realized that the way to paint a concrete form that\nbest reflected reality was to paint not what they saw, but what it was like to\nsee. We envision a future of creation using AI where what it is like to see is\nreplicable, transferrable, manipulable - part of the artist's palette that is\nboth grounded in a particular context, and generalizable beyond it.\n  An active line of research maps human-interpretable features onto directions\nin GAN latent space. Supervised and self-supervised approaches that search for\nanticipated directions or use off-the-shelf classifiers to drive image\nmanipulation in embedding space are limited in the variety of features they can\nuncover. Unsupervised approaches that discover useful new directions show that\nthe space of perceptually meaningful directions is nowhere close to being fully\nmapped. As this space is broad and full of creative potential, we want tools\nfor direction discovery that capture the richness and generalizability of human\nperception. Our approach puts creators in the discovery loop during real-time\ntool use, in order to identify directions that are perceptually meaningful to\nthem, and generate interpretable image translations along those directions.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 04:18:23 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Schwettmann", "Sarah", ""], ["Strobelt", "Hendrik", ""], ["Martino", "Mauro", ""]]}, {"id": "2012.14288", "submitter": "Pengtao Xie", "authors": "Xingchen Zhao, Xuehai He, Pengtao Xie", "title": "Learning by Ignoring, with Application to Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning by ignoring, which identifies less important things and excludes\nthem from the learning process, is broadly practiced in human learning and has\nshown ubiquitous effectiveness. There has been psychological studies showing\nthat learning to ignore certain things is a powerful tool for helping people\nfocus. In this paper, we explore whether this useful human learning methodology\ncan be borrowed to improve machine learning. We propose a novel machine\nlearning framework referred to as learning by ignoring (LBI). Our framework\nautomatically identifies pretraining data examples that have large domain shift\nfrom the target distribution by learning an ignoring variable for each example\nand excludes them from the pretraining process. We formulate LBI as a\nthree-level optimization framework where three learning stages are involved:\npretraining by minimizing the losses weighed by ignoring variables; finetuning;\nupdating the ignoring variables by minimizing the validation loss. A\ngradient-based algorithm is developed to efficiently solve the three-level\noptimization problem in LBI. Experiments on various datasets demonstrate the\neffectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 15:33:41 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 04:56:23 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Zhao", "Xingchen", ""], ["He", "Xuehai", ""], ["Xie", "Pengtao", ""]]}, {"id": "2012.14292", "submitter": "Manash Pratim Das", "authors": "Manash Pratim Das, Larry Matthies and Shreyansh Daftry", "title": "Online Photometric Calibration of Automatic Gain Thermal Infrared\n  Cameras", "comments": "8 pages, 6 figures, Pre-Print. This work has been submitted to the\n  IEEE for possible publication", "journal-ref": null, "doi": "10.1109/LRA.2021.3061401", "report-no": null, "categories": "eess.IV cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thermal infrared cameras are increasingly being used in various applications\nsuch as robot vision, industrial inspection and medical imaging, thanks to\ntheir improved resolution and portability. However, the performance of\ntraditional computer vision techniques developed for electro-optical imagery\ndoes not directly translate to the thermal domain due to two major reasons:\nthese algorithms require photometric assumptions to hold, and methods for\nphotometric calibration of RGB cameras cannot be applied to thermal-infrared\ncameras due to difference in data acquisition and sensor phenomenology. In this\npaper, we take a step in this direction, and introduce a novel algorithm for\nonline photometric calibration of thermal-infrared cameras. Our proposed method\ndoes not require any specific driver/hardware support and hence can be applied\nto any commercial off-the-shelf thermal IR camera. We present this in the\ncontext of visual odometry and SLAM algorithms, and demonstrate the efficacy of\nour proposed system through extensive experiments for both standard benchmark\ndatasets, and real-world field tests with a thermal-infrared camera in natural\noutdoor environments.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 17:51:54 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 16:09:21 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Das", "Manash Pratim", ""], ["Matthies", "Larry", ""], ["Daftry", "Shreyansh", ""]]}, {"id": "2012.14305", "submitter": "Bharat Bohara", "authors": "Bharat Bohara", "title": "Adaptive Threshold for Online Object Recognition and Re-identification\n  Tasks", "comments": "9 pages, 14 figures, submitted to Image and Vision Computing, Journal\n  - Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Choosing a decision threshold is one of the challenging job in any\nclassification tasks. How much the model is accurate, if the deciding boundary\nis not picked up carefully, its entire performance would go in vain. On the\nother hand, for imbalance classification where one of the classes is dominant\nover another, relying on the conventional method of choosing threshold would\nresult in poor performance. Even if the threshold or decision boundary is\nproperly chosen based on machine learning strategies like SVM and decision\ntree, it will fail at some point for dynamically varying databases and in case\nof identity-features that are more or less similar, like in face recognition\nand person re-identification models. Hence, with the need for adaptability of\nthe decision threshold selection for imbalanced classification and incremental\ndatabase size, an online optimization-based statistical feature learning\nadaptive technique is developed and tested on the LFW datasets and\nself-prepared athletes datasets. This method of adopting adaptive threshold\nresulted in 12-45% improvement in the model accuracy compared to the fixed\nthreshold {0.3,0.5,0.7} that are usually taken via the hit-and-trial method in\nany classification and identification tasks. Source code for the complete\nalgorithm is available at: https://github.com/Varat7v2/adaptive-threshold\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 15:40:53 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 10:00:28 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Bohara", "Bharat", ""]]}, {"id": "2012.14314", "submitter": "Nian Xue", "authors": "Zhen Li, Sunzeng Cai, Xiaoyi Wang, Zhe Liu and Nian Xue", "title": "GAKP: GRU Association and Kalman Prediction for Multiple Object Tracking", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Object Tracking (MOT) has been a useful yet challenging task in many\nreal-world applications such as video surveillance, intelligent retail, and\nsmart city. The challenge is how to model long-term temporal dependencies in an\nefficient manner. Some recent works employ Recurrent Neural Networks (RNN) to\nobtain good performance, which, however, requires a large amount of training\ndata. In this paper, we proposed a novel tracking method that integrates the\nauto-tuning Kalman method for prediction and the Gated Recurrent Unit (GRU) and\nachieves a near-optimum with a small amount of training data. Experimental\nresults show that our new algorithm can achieve competitive performance on the\nchallenging MOT benchmark, and faster and more robust than the state-of-the-art\nRNN-based online MOT algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 15:52:24 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Li", "Zhen", ""], ["Cai", "Sunzeng", ""], ["Wang", "Xiaoyi", ""], ["Liu", "Zhe", ""], ["Xue", "Nian", ""]]}, {"id": "2012.14331", "submitter": "Abhranil Das", "authors": "Abhranil Das and Wilson S Geisler", "title": "A method to integrate and classify normal distributions", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Univariate and multivariate normal probability distributions are widely used\nwhen modeling decisions under uncertainty. Computing the performance of such\nmodels requires integrating these distributions over specific domains, which\ncan vary widely across models. Besides some special cases where these integrals\nare easy to calculate, there exist no general analytical expressions, standard\nnumerical methods or software for these integrals. Here we present mathematical\nresults and open-source software that provide (i) the probability in any domain\nof a normal in any dimensions with any parameters, (ii) the probability\ndensity, cumulative distribution, and inverse cumulative distribution of any\nfunction of a normal vector, (iii) the classification errors among any number\nof normal distributions, the Bayes-optimal discriminability index and relation\nto the operating characteristic, (iv) dimension reduction and visualizations\nfor such problems, and (v) tests for how reliably these methods may be used on\ngiven data. We demonstrate these tools with vision research applications of\ndetecting occluding objects in natural scenes, and detecting camouflage.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 05:45:41 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 20:23:39 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 23:00:37 GMT"}, {"version": "v4", "created": "Wed, 7 Apr 2021 18:49:56 GMT"}, {"version": "v5", "created": "Thu, 22 Apr 2021 20:40:22 GMT"}, {"version": "v6", "created": "Mon, 26 Apr 2021 23:11:55 GMT"}, {"version": "v7", "created": "Tue, 27 Jul 2021 23:02:36 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Das", "Abhranil", ""], ["Geisler", "Wilson S", ""]]}, {"id": "2012.14336", "submitter": "Bo Feng", "authors": "Bo Feng and Geoffrey C. Fox", "title": "Spatiotemporal Pattern Mining for Nowcasting Extreme Earthquakes in\n  Southern California", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geoscience and seismology have utilized the most advanced technologies and\nequipment to monitor seismic events globally from the past few decades. With\nthe enormous amount of data, modern GPU-powered deep learning presents a\npromising approach to analyze data and discover patterns. In recent years,\nthere are plenty of successful deep learning models for picking seismic waves.\nHowever, forecasting extreme earthquakes, which can cause disasters, is still\nan underdeveloped topic in history. Relevant research in spatiotemporal\ndynamics mining and forecasting has revealed some successful predictions, a\ncrucial topic in many scientific research fields. Most studies of them have\nmany successful applications of using deep neural networks. In Geology and\nEarth science studies, earthquake prediction is one of the world's most\nchallenging problems, about which cutting-edge deep learning technologies may\nhelp discover some valuable patterns. In this project, we propose a deep\nlearning modeling approach, namely \\tseqpre, to mine spatiotemporal patterns\nfrom data to nowcast extreme earthquakes by discovering visual dynamics in\nregional coarse-grained spatial grids over time. In this modeling approach, we\nuse synthetic deep learning neural networks with domain knowledge in geoscience\nand seismology to exploit earthquake patterns for prediction using\nconvolutional long short-term memory neural networks. Our experiments show a\nstrong correlation between location prediction and magnitude prediction for\nearthquakes in Southern California. Ablation studies and visualization validate\nthe effectiveness of the proposed modeling method.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 20:13:59 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 16:43:43 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Feng", "Bo", ""], ["Fox", "Geoffrey C.", ""]]}, {"id": "2012.14345", "submitter": "Elisa Maiettini", "authors": "Elisa Maiettini and Raffaello Camoriano and Giulia Pasquale and Vadim\n  Tikhanoff and Lorenzo Rosasco and Lorenzo Natale", "title": "Data-efficient Weakly-supervised Learning for On-line Object Detection\n  under Domain Shift in Robotics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several object detection methods have recently been proposed in the\nliterature, the vast majority based on Deep Convolutional Neural Networks\n(DCNNs). Such architectures have been shown to achieve remarkable performance,\nat the cost of computationally expensive batch training and extensive labeling.\nThese methods have important limitations for robotics: Learning solely on\noff-line data may introduce biases (the so-called domain shift), and prevents\nadaptation to novel tasks. In this work, we investigate how weakly-supervised\nlearning can cope with these problems. We compare several techniques for\nweakly-supervised learning in detection pipelines to reduce model (re)training\ncosts without compromising accuracy. In particular, we show that diversity\nsampling for constructing active learning queries and strong positives\nselection for self-supervised learning enable significant annotation savings\nand improve domain shift adaptation. By integrating our strategies into a\nhybrid DCNN/FALKON on-line detection pipeline [1], our method is able to be\ntrained and updated efficiently with few labels, overcoming limitations of\nprevious work. We experimentally validate and benchmark our method on\nchallenging robotic object detection tasks under domain shift.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 16:36:11 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Maiettini", "Elisa", ""], ["Camoriano", "Raffaello", ""], ["Pasquale", "Giulia", ""], ["Tikhanoff", "Vadim", ""], ["Rosasco", "Lorenzo", ""], ["Natale", "Lorenzo", ""]]}, {"id": "2012.14359", "submitter": "Mehul Bhatt", "authors": "Jakob Suchan and Mehul Bhatt and Srikrishna Varadarajan", "title": "Commonsense Visual Sensemaking for Autonomous Driving: On Generalised\n  Neurosymbolic Online Abduction Integrating Vision and Semantics", "comments": "This is a preprint / review version of an accepted contribution to be\n  published as part of the Artificial Intelligence Journal (AIJ).? The article\n  is an extended version of an IJCAI 2019 publication [74, arXiv:1906.00107]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the need and potential of systematically integrated vision and\nsemantics solutions for visual sensemaking in the backdrop of autonomous\ndriving. A general neurosymbolic method for online visual sensemaking using\nanswer set programming (ASP) is systematically formalised and fully\nimplemented. The method integrates state of the art in visual computing, and is\ndeveloped as a modular framework that is generally usable within hybrid\narchitectures for realtime perception and control. We evaluate and demonstrate\nwith community established benchmarks KITTIMOD, MOT-2017, and MOT-2020. As\nuse-case, we focus on the significance of human-centred visual sensemaking --\ne.g., involving semantic representation and explainability, question-answering,\ncommonsense interpolation -- in safety-critical autonomous driving situations.\nThe developed neurosymbolic framework is domain-independent, with the case of\nautonomous driving designed to serve as an exemplar for online visual\nsensemaking in diverse cognitive interaction settings in the backdrop of select\nhuman-centred AI technology design considerations.\n  Keywords: Cognitive Vision, Deep Semantics, Declarative Spatial Reasoning,\nKnowledge Representation and Reasoning, Commonsense Reasoning, Visual\nAbduction, Answer Set Programming, Autonomous Driving, Human-Centred Computing\nand Design, Standardisation in Driving Technology, Spatial Cognition and AI.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 16:55:19 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Suchan", "Jakob", ""], ["Bhatt", "Mehul", ""], ["Varadarajan", "Srikrishna", ""]]}, {"id": "2012.14360", "submitter": "Hang Chen", "authors": "Hang Chen, Jun Du, Yu Hu, Li-Rong Dai, Chin-Hui Lee, Bao-Cai Yin", "title": "Lip-reading with Hierarchical Pyramidal Convolution and Self-Attention", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel deep learning architecture to improving\nword-level lip-reading. On the one hand, we first introduce the multi-scale\nprocessing into the spatial feature extraction for lip-reading. Specially, we\nproposed hierarchical pyramidal convolution (HPConv) to replace the standard\nconvolution in original module, leading to improvements over the model's\nability to discover fine-grained lip movements. On the other hand, we merge\ninformation in all time steps of the sequence by utilizing self-attention, to\nmake the model pay more attention to the relevant frames. These two advantages\nare combined together to further enhance the model's classification power.\nExperiments on the Lip Reading in the Wild (LRW) dataset show that our proposed\nmodel has achieved 86.83% accuracy, yielding 1.53% absolute improvement over\nthe current state-of-the-art. We also conducted extensive experiments to better\nunderstand the behavior of the proposed model.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 16:55:51 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Chen", "Hang", ""], ["Du", "Jun", ""], ["Hu", "Yu", ""], ["Dai", "Li-Rong", ""], ["Lee", "Chin-Hui", ""], ["Yin", "Bao-Cai", ""]]}, {"id": "2012.14371", "submitter": "Piotr Koniusz", "authors": "Piotr Koniusz and Lei Wang and Anoop Cherian", "title": "Tensor Representations for Action Recognition", "comments": "Published with TPAMI, 2020. arXiv admin note: substantial text\n  overlap with arXiv:1604.00239, as this is a journal extension of\n  arXiv:1604.00239", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human actions in video sequences are characterized by the complex interplay\nbetween spatial features and their temporal dynamics. In this paper, we propose\nnovel tensor representations for compactly capturing such higher-order\nrelationships between visual features for the task of action recognition. We\npropose two tensor-based feature representations, viz. (i) sequence\ncompatibility kernel (SCK) and (ii) dynamics compatibility kernel (DCK); the\nformer building on the spatio-temporal correlations between features, while the\nlatter explicitly modeling the action dynamics of a sequence. We also explore\ngeneralization of SCK, coined SCK(+), that operates on subsequences to capture\nthe local-global interplay of correlations, which can incorporate multi-modal\ninputs e.g., skeleton 3D body-joints and per-frame classifier scores obtained\nfrom deep learning models trained on videos. We introduce linearization of\nthese kernels that lead to compact and fast descriptors. We provide experiments\non (i) 3D skeleton action sequences, (ii) fine-grained video sequences, and\n(iii) standard non-fine-grained videos. As our final representations are\ntensors that capture higher-order relationships of features, they relate to\nco-occurrences for robust fine-grained recognition. We use higher-order tensors\nand so-called Eigenvalue Power Normalization (EPN) which have been long\nspeculated to perform spectral detection of higher-order occurrences, thus\ndetecting fine-grained relationships of features rather than merely count\nfeatures in action sequences. We prove that a tensor of order r, built from Z*\ndimensional features, coupled with EPN indeed detects if at least one\nhigher-order occurrence is `projected' into one of its binom(Z*,r) subspaces of\ndim. r represented by the tensor, thus forming a Tensor Power Normalization\nmetric endowed with binom(Z*,r) such `detectors'.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 17:27:18 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 21:44:32 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Koniusz", "Piotr", ""], ["Wang", "Lei", ""], ["Cherian", "Anoop", ""]]}, {"id": "2012.14395", "submitter": "Anirban Sarkar", "authors": "Anindya Sarkar, Anirban Sarkar, Vineeth N Balasubramanian", "title": "Enhanced Regularizers for Attributional Robustness", "comments": "15 pages, 18 figures, Accepted at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are the default choice of learning models for computer\nvision tasks. Extensive work has been carried out in recent years on explaining\ndeep models for vision tasks such as classification. However, recent work has\nshown that it is possible for these models to produce substantially different\nattribution maps even when two very similar images are given to the network,\nraising serious questions about trustworthiness. To address this issue, we\npropose a robust attribution training strategy to improve attributional\nrobustness of deep neural networks. Our method carefully analyzes the\nrequirements for attributional robustness and introduces two new regularizers\nthat preserve a model's attribution map during attacks. Our method surpasses\nstate-of-the-art attributional robustness methods by a margin of approximately\n3% to 9% in terms of attribution robustness measures on several datasets\nincluding MNIST, FMNIST, Flower and GTSRB.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 18:18:39 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Sarkar", "Anindya", ""], ["Sarkar", "Anirban", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "2012.14402", "submitter": "Alban Flachot", "authors": "Alban Flachot, Arash Akbarinia, Heiko H. Sch\\\"utt, Roland W. Fleming,\n  Felix A. Wichmann, Karl R. Gegenfurtner", "title": "Deep Neural Models for color discrimination and color constancy", "comments": "19 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Color constancy is our ability to perceive constant colors across varying\nilluminations. Here, we trained deep neural networks to be color constant and\nevaluated their performance with varying cues. Inputs to the networks consisted\nof the cone excitations in 3D-rendered images of 2115 different 3D-shapes, with\nspectral reflectances of 1600 different Munsell chips, illuminated under 278\ndifferent natural illuminations. The models were trained to classify the\nreflectance of the objects. One network, Deep65, was trained under a fixed\ndaylight D65 illumination, while DeepCC was trained under varying\nilluminations. Testing was done with 4 new illuminations with equally spaced\nCIEL*a*b* chromaticities, 2 along the daylight locus and 2 orthogonal to it. We\nfound a high degree of color constancy for DeepCC, and constancy was higher\nalong the daylight locus. When gradually removing cues from the scene,\nconstancy decreased. High levels of color constancy were achieved with\ndifferent DNN architectures. Both ResNets and classical ConvNets of varying\ndegrees of complexity performed well. However, DeepCC, a convolutional network,\nrepresented colors along the 3 color dimensions of human color vision, while\nResNets showed a more complex representation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 18:31:29 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Flachot", "Alban", ""], ["Akbarinia", "Arash", ""], ["Sch\u00fctt", "Heiko H.", ""], ["Fleming", "Roland W.", ""], ["Wichmann", "Felix A.", ""], ["Gegenfurtner", "Karl R.", ""]]}, {"id": "2012.14426", "submitter": "Jurandy Almeida", "authors": "Samuel Felipe dos Santos and Jurandy Almeida", "title": "Deep Learning Towards Edge Computing: Neural Networks Straight from\n  Compressed Data", "comments": "arXiv admin note: substantial text overlap with arXiv:2012.13726", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the popularization and grow in computational power of mobile phones,\nas well as advances in artificial intelligence, many intelligent applications\nhave been developed, meaningfully enriching people's life. For this reason,\nthere is a growing interest in the area of edge intelligence, that aims to push\nthe computation of data to the edges of the network, in order to make those\napplications more efficient and secure. Many intelligent applications rely on\ndeep learning models, like convolutional neural networks (CNNs). Over the past\ndecade, they have achieved state-of-the-art performance in many computer vision\ntasks. To increase the performance of these methods, the trend has been to use\nincreasingly deeper architectures and with more parameters, leading to a high\ncomputational cost. Indeed, this is one of the main problems faced by deep\narchitectures, limiting their applicability in domains with limited\ncomputational resources, like edge devices. To alleviate the computational\ncomplexity, we propose a deep neural network capable of learning straight from\nthe relevant information pertaining to visual content readily available in the\ncompressed representation used for image and video storage and transmission.\nThe novelty of our approach is that it was designed to operate directly on\nfrequency domain data, learning with DCT coefficients rather than RGB pixels.\nThis enables to save high computational load in full decoding the data stream\nand therefore greatly speed up the processing time, which has become a big\nbottleneck of deep learning. We evaluated our network on two challenging tasks:\n(1) image classification on the ImageNet dataset and (2) video classification\non the UCF-101 and HMDB-51 datasets. Our results demonstrate comparable\neffectiveness to the state-of-the-art methods in terms of accuracy, with the\nadvantage of being more computationally efficient.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 15:00:10 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Santos", "Samuel Felipe dos", ""], ["Almeida", "Jurandy", ""]]}, {"id": "2012.14456", "submitter": "Shiv Ram Dubey", "authors": "Jayendra Kantipudi, Shiv Ram Dubey, Soumendu Chakraborty", "title": "Color Channel Perturbation Attacks for Fooling Convolutional Neural\n  Networks and A Defense Against Such Attacks", "comments": "Accepted in IEEE Transactions on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Convolutional Neural Networks (CNNs) have emerged as a very powerful data\ndependent hierarchical feature extraction method. It is widely used in several\ncomputer vision problems. The CNNs learn the important visual features from\ntraining samples automatically. It is observed that the network overfits the\ntraining samples very easily. Several regularization methods have been proposed\nto avoid the overfitting. In spite of this, the network is sensitive to the\ncolor distribution within the images which is ignored by the existing\napproaches. In this paper, we discover the color robustness problem of CNN by\nproposing a Color Channel Perturbation (CCP) attack to fool the CNNs. In CCP\nattack new images are generated with new channels created by combining the\noriginal channels with the stochastic weights. Experiments were carried out\nover widely used CIFAR10, Caltech256 and TinyImageNet datasets in the image\nclassification framework. The VGG, ResNet and DenseNet models are used to test\nthe impact of the proposed attack. It is observed that the performance of the\nCNNs degrades drastically under the proposed CCP attack. Result show the effect\nof the proposed simple CCP attack over the robustness of the CNN trained model.\nThe results are also compared with existing CNN fooling approaches to evaluate\nthe accuracy drop. We also propose a primary defense mechanism to this problem\nby augmenting the training dataset with the proposed CCP attack. The\nstate-of-the-art performance using the proposed solution in terms of the CNN\nrobustness under CCP attack is observed in the experiments. The code is made\npublicly available at\n\\url{https://github.com/jayendrakantipudi/Color-Channel-Perturbation-Attack}.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 11:35:29 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Kantipudi", "Jayendra", ""], ["Dubey", "Shiv Ram", ""], ["Chakraborty", "Soumendu", ""]]}, {"id": "2012.14459", "submitter": "Vasiliki Tassopoulou", "authors": "Vasiliki Tassopoulou, George Retsinas, Petros Maragos", "title": "Enhancing Handwritten Text Recognition with N-gram sequence\n  decomposition and Multitask Learning", "comments": "ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current state-of-the-art approaches in the field of Handwritten Text\nRecognition are predominately single task with unigram, character level target\nunits. In our work, we utilize a Multi-task Learning scheme, training the model\nto perform decompositions of the target sequence with target units of different\ngranularity, from fine to coarse. We consider this method as a way to utilize\nn-gram information, implicitly, in the training process, while the final\nrecognition is performed using only the unigram output. % in order to highlight\nthe difference of the internal Unigram decoding of such a multi-task approach\nhighlights the capability of the learned internal representations, imposed by\nthe different n-grams at the training step. We select n-grams as our target\nunits and we experiment from unigrams to fourgrams, namely subword level\ngranularities. These multiple decompositions are learned from the network with\ntask-specific CTC losses. Concerning network architectures, we propose two\nalternatives, namely the Hierarchical and the Block Multi-task. Overall, our\nproposed model, even though evaluated only on the unigram task, outperforms its\ncounterpart single-task by absolute 2.52\\% WER and 1.02\\% CER, in the greedy\ndecoding, without any computational overhead during inference, hinting towards\nsuccessfully imposing an implicit language model.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 19:35:40 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Tassopoulou", "Vasiliki", ""], ["Retsinas", "George", ""], ["Maragos", "Petros", ""]]}, {"id": "2012.14465", "submitter": "David K A Mordecai", "authors": "Ryan C. Saxe, Samantha Kappagoda, David K.A. Mordecai", "title": "Classification of Pathological and Normal Gait: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Gait recognition is a term commonly referred to as an identification problem\nwithin the Computer Science field. There are a variety of methods and models\ncapable of identifying an individual based on their pattern of ambulatory\nlocomotion. By surveying the current literature on gait recognition, this paper\nseeks to identify appropriate metrics, devices, and algorithms for collecting\nand analyzing data regarding patterns and modes of ambulatory movement across\nindividuals. Furthermore, this survey seeks to motivate interest in a broader\nscope of longitudinal analysis regarding the perturbations in gait across\nstates (i.e. physiological, emotive, and/or cognitive states). More broadly,\ninferences to normal versus pathological gait patterns can be attributed, based\non both longitudinal and non-longitudinal forms of classification. This may\nindicate promising research directions and experimental designs, such as\ncreating algorithmic metrics for the quantification of fatigue, or models for\nforecasting episodic disorders. Furthermore, in conjunction with other\nmeasurements of physiological and environmental conditions, pathological gait\nclassification might be applicable to inference for syndromic surveillance of\ninfectious disease states or cognitive impairment.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 19:56:42 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Saxe", "Ryan C.", ""], ["Kappagoda", "Samantha", ""], ["Mordecai", "David K. A.", ""]]}, {"id": "2012.14495", "submitter": "Vishwanath Saragadam Raja Venkata", "authors": "Vishwanath Saragadam, Michael DeZeeuw, Richard Baraniuk, Ashok\n  Veeraraghavan, and Aswin Sankaranarayanan", "title": "SASSI -- Super-Pixelated Adaptive Spatio-Spectral Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel video-rate hyperspectral imager with high spatial, and\ntemporal resolutions. Our key hypothesis is that spectral profiles of pixels in\na super-pixel of an oversegmented image tend to be very similar. Hence, a\nscene-adaptive spatial sampling of an hyperspectral scene, guided by its\nsuper-pixel segmented image, is capable of obtaining high-quality\nreconstructions. To achieve this, we acquire an RGB image of the scene, compute\nits super-pixels, from which we generate a spatial mask of locations where we\nmeasure high-resolution spectrum. The hyperspectral image is subsequently\nestimated by fusing the RGB image and the spectral measurements using a\nlearnable guided filtering approach. Due to low computational complexity of the\nsuperpixel estimation step, our setup can capture hyperspectral images of the\nscenes with little overhead over traditional snapshot hyperspectral cameras,\nbut with significantly higher spatial and spectral resolutions. We validate the\nproposed technique with extensive simulations as well as a lab prototype that\nmeasures hyperspectral video at a spatial resolution of $600 \\times 900$\npixels, at a spectral resolution of 10 nm over visible wavebands, and achieving\na frame rate at $18$fps.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 21:34:18 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Saragadam", "Vishwanath", ""], ["DeZeeuw", "Michael", ""], ["Baraniuk", "Richard", ""], ["Veeraraghavan", "Ashok", ""], ["Sankaranarayanan", "Aswin", ""]]}, {"id": "2012.14517", "submitter": "Jorge F. Lazo", "authors": "Jorge F. Lazo, Sara Moccia, Emanuele Frontoni and Elena De Momi", "title": "Comparison of different CNNs for breast tumor classification from\n  ultrasound images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Breast cancer is one of the deadliest cancer worldwide. Timely detection\ncould reduce mortality rates. In the clinical routine, classifying benign and\nmalignant tumors from ultrasound (US) imaging is a crucial but challenging\ntask. An automated method, which can deal with the variability of data is\ntherefore needed.\n  In this paper, we compared different Convolutional Neural Networks (CNNs) and\ntransfer learning methods for the task of automated breast tumor\nclassification. The architectures investigated in this study were VGG-16 and\nInception V3. Two different training strategies were investigated: the first\none was using pretrained models as feature extractors and the second one was to\nfine-tune the pre-trained models. A total of 947 images were used, 587\ncorresponded to US images of benign tumors and 360 with malignant tumors. 678\nimages were used for the training and validation process, while 269 images were\nused for testing the models.\n  Accuracy and Area Under the receiver operating characteristic Curve (AUC)\nwere used as performance metrics. The best performance was obtained by fine\ntuning VGG-16, with an accuracy of 0.919 and an AUC of 0.934. The obtained\nresults open the opportunity to further investigation with a view of improving\ncancer detection.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 22:54:08 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Lazo", "Jorge F.", ""], ["Moccia", "Sara", ""], ["Frontoni", "Emanuele", ""], ["De Momi", "Elena", ""]]}, {"id": "2012.14544", "submitter": "Pramod Vadiraja", "authors": "Viny Saajan Victor, Pramod Vadiraja, Jan-Tobias Sohns, Heike Leitte", "title": "Visual Probing and Correction of Object Recognition Models with\n  Interactive user feedback", "comments": "2 Pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of state-of-the-art machine learning and deep learning\ntechnologies, several industries are moving towards the field. Applications of\nsuch technologies are highly diverse ranging from natural language processing\nto computer vision. Object recognition is one such area in the computer vision\ndomain. Although proven to perform with high accuracy, there are still areas\nwhere such models can be improved. This is in-fact highly important in\nreal-world use cases like autonomous driving or cancer detection, that are\nhighly sensitive and expect such technologies to have almost no uncertainties.\nIn this paper, we attempt to visualise the uncertainties in object recognition\nmodels and propose a correction process via user feedback. We further\ndemonstrate our approach on the data provided by the VAST 2020 Mini-Challenge\n2.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 00:36:12 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Victor", "Viny Saajan", ""], ["Vadiraja", "Pramod", ""], ["Sohns", "Jan-Tobias", ""], ["Leitte", "Heike", ""]]}, {"id": "2012.14556", "submitter": "Jun Ma", "authors": "Jun Ma", "title": "Cascaded Framework for Automatic Evaluation of Myocardial Infarction\n  from Delayed-Enhancement Cardiac MRI", "comments": "MICCAI 2020 EMIDEC Challenge 2nd in segmentation task and 1st in\n  classification task", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Automatic evaluation of myocardium and pathology plays an important role in\nthe quantitative analysis of patients suffering from myocardial infarction. In\nthis paper, we present a cascaded convolutional neural network framework for\nmyocardial infarction segmentation and classification in delayed-enhancement\ncardiac MRI. Specifically, we first use a 2D U-Net to segment the whole heart,\nincluding the left ventricle and the myocardium. Then, we crop the whole heart\nas a region of interest (ROI). Finally, a new 2D U-Net is used to segment the\ninfraction and no-reflow areas in the whole heart ROI. The segmentation method\ncan be applied to the classification task where the segmentation results with\nthe infraction or no-reflow areas are classified as pathological cases. Our\nmethod took second place in the MICCAI 2020 EMIDEC segmentation task with Dice\nscores of 86.28%, 62.24%, and 77.76% for myocardium, infraction, and no-reflow\nareas, respectively, and first place in the classification task with an\naccuracy of 92%.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 01:35:02 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Ma", "Jun", ""]]}, {"id": "2012.14564", "submitter": "Yutian Chen", "authors": "Yutian Chen, Xiaowei Xu, Dewen Zeng, Yiyu Shi, Haiyun Yuan, Jian\n  Zhuang, Yuhao Dong, Qianjun Jia, Meiping Huang", "title": "Myocardial Segmentation of Cardiac MRI Sequences with Temporal\n  Consistency for Coronary Artery Disease Diagnosis", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronary artery disease (CAD) is the most common cause of death globally, and\nits diagnosis is usually based on manual myocardial segmentation of Magnetic\nResonance Imaging (MRI) sequences. As the manual segmentation is tedious,\ntime-consuming and with low applicability, automatic myocardial segmentation\nusing machine learning techniques has been widely explored recently. However,\nalmost all the existing methods treat the input MRI sequences independently,\nwhich fails to capture the temporal information between sequences, e.g., the\nshape and location information of the myocardium in sequences along time. In\nthis paper, we propose a myocardial segmentation framework for sequence of\ncardiac MRI (CMR) scanning images of left ventricular cavity, right ventricular\ncavity, and myocardium. Specifically, we propose to combine conventional\nnetworks and recurrent networks to incorporate temporal information between\nsequences to ensure temporal consistent. We evaluated our framework on the\nAutomated Cardiac Diagnosis Challenge (ACDC) dataset. Experiment results\ndemonstrate that our framework can improve the segmentation accuracy by up to\n2% in Dice coefficient.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 01:54:09 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Chen", "Yutian", ""], ["Xu", "Xiaowei", ""], ["Zeng", "Dewen", ""], ["Shi", "Yiyu", ""], ["Yuan", "Haiyun", ""], ["Zhuang", "Jian", ""], ["Dong", "Yuhao", ""], ["Jia", "Qianjun", ""], ["Huang", "Meiping", ""]]}, {"id": "2012.14567", "submitter": "Munan Ning", "authors": "Munan Ning, Cheng Bian, Chenglang Yuan, Kai Ma, Yefeng Zheng", "title": "Ensembled ResUnet for Anatomical Brain Barriers Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accuracy segmentation of brain structures could be helpful for glioma and\nradiotherapy planning. However, due to the visual and anatomical differences\nbetween different modalities, the accurate segmentation of brain structures\nbecomes challenging. To address this problem, we first construct a residual\nblock based U-shape network with a deep encoder and shallow decoder, which can\ntrade off the framework performance and efficiency. Then, we introduce the\nTversky loss to address the issue of the class imbalance between different\nforeground and the background classes. Finally, a model ensemble strategy is\nutilized to remove outliers and further boost performance.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 02:14:30 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 08:37:53 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ning", "Munan", ""], ["Bian", "Cheng", ""], ["Yuan", "Chenglang", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2012.14569", "submitter": "Shuchang Lyu", "authors": "Qi Zhao, Shuchang Lyu, Yuewen Li, Yujing Ma, Lijiang Chen", "title": "MGML: Multi-Granularity Multi-Level Feature Ensemble Network for Remote\n  Sensing Scene Classification", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing (RS) scene classification is a challenging task to predict\nscene categories of RS images. RS images have two main characters: large\nintra-class variance caused by large resolution variance and confusing\ninformation from large geographic covering area. To ease the negative influence\nfrom the above two characters. We propose a Multi-granularity Multi-Level\nFeature Ensemble Network (MGML-FENet) to efficiently tackle RS scene\nclassification task in this paper. Specifically, we propose Multi-granularity\nMulti-Level Feature Fusion Branch (MGML-FFB) to extract multi-granularity\nfeatures in different levels of network by channel-separate feature generator\n(CS-FG). To avoid the interference from confusing information, we propose\nMulti-granularity Multi-Level Feature Ensemble Module (MGML-FEM) which can\nprovide diverse predictions by full-channel feature generator (FC-FG). Compared\nto previous methods, our proposed networks have ability to use structure\ninformation and abundant fine-grained features. Furthermore, through ensemble\nlearning method, our proposed MGML-FENets can obtain more convincing final\npredictions. Extensive classification experiments on multiple RS datasets (AID,\nNWPU-RESISC45, UC-Merced and VGoogle) demonstrate that our proposed networks\nachieve better performance than previous state-of-the-art (SOTA) networks. The\nvisualization analysis also shows the good interpretability of MGML-FENet.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 02:18:11 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Zhao", "Qi", ""], ["Lyu", "Shuchang", ""], ["Li", "Yuewen", ""], ["Ma", "Yujing", ""], ["Chen", "Lijiang", ""]]}, {"id": "2012.14584", "submitter": "Guotai Wang", "authors": "Lu Wang, Dong Guo, Guotai Wang and Shaoting Zhang", "title": "Annotation-Efficient Learning for Medical Image Segmentation based on\n  Noisy Pseudo Labels and Adversarial Learning", "comments": "13 pages, 15 figures", "journal-ref": null, "doi": "10.1109/TMI.2020.3047807", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite that deep learning has achieved state-of-the-art performance for\nmedical image segmentation, its success relies on a large set of manually\nannotated images for training that are expensive to acquire. In this paper, we\npropose an annotation-efficient learning framework for segmentation tasks that\navoids annotations of training images, where we use an improved\nCycle-Consistent Generative Adversarial Network (GAN) to learn from a set of\nunpaired medical images and auxiliary masks obtained either from a shape model\nor public datasets. We first use the GAN to generate pseudo labels for our\ntraining images under the implicit high-level shape constraint represented by a\nVariational Auto-encoder (VAE)-based discriminator with the help of the\nauxiliary masks, and build a Discriminator-guided Generator Channel Calibration\n(DGCC) module which employs our discriminator's feedback to calibrate the\ngenerator for better pseudo labels. To learn from the pseudo labels that are\nnoisy, we further introduce a noise-robust iterative learning method using\nnoise-weighted Dice loss. We validated our framework with two situations:\nobjects with a simple shape model like optic disc in fundus images and fetal\nhead in ultrasound images, and complex structures like lung in X-Ray images and\nliver in CT images. Experimental results demonstrated that 1) Our VAE-based\ndiscriminator and DGCC module help to obtain high-quality pseudo labels. 2) Our\nproposed noise-robust learning method can effectively overcome the effect of\nnoisy pseudo labels. 3) The segmentation performance of our method without\nusing annotations of training images is close or even comparable to that of\nlearning from human annotations.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 03:22:41 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Wang", "Lu", ""], ["Guo", "Dong", ""], ["Wang", "Guotai", ""], ["Zhang", "Shaoting", ""]]}, {"id": "2012.14587", "submitter": "Tianshui Chen", "authors": "Tao Pu, Tianshui Chen, Yuan Xie, Hefeng Wu, and Liang Lin", "title": "AU-Expression Knowledge Constrained Representation Learning for Facial\n  Expression Recognition", "comments": "Accepted at ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing human emotion/expressions automatically is quite an expected\nability for intelligent robotics, as it can promote better communication and\ncooperation with humans. Current deep-learning-based algorithms may achieve\nimpressive performance in some lab-controlled environments, but they always\nfail to recognize the expressions accurately for the uncontrolled in-the-wild\nsituation. Fortunately, facial action units (AU) describe subtle facial\nbehaviors, and they can help distinguish uncertain and ambiguous expressions.\nIn this work, we explore the correlations among the action units and facial\nexpressions, and devise an AU-Expression Knowledge Constrained Representation\nLearning (AUE-CRL) framework to learn the AU representations without AU\nannotations and adaptively use representations to facilitate facial expression\nrecognition. Specifically, it leverages AU-expression correlations to guide the\nlearning of the AU classifiers, and thus it can obtain AU representations\nwithout incurring any AU annotations. Then, it introduces a knowledge-guided\nattention mechanism that mines useful AU representations under the constraint\nof AU-expression correlations. In this way, the framework can capture local\ndiscriminative and complementary features to enhance facial representation for\nfacial expression recognition. We conduct experiments on the challenging\nuncontrolled datasets to demonstrate the superiority of the proposed framework\nover current state-of-the-art methods. Codes and trained models are available\nat https://github.com/HCPLab-SYSU/AUE-CRL.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 03:42:04 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 07:00:09 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Pu", "Tao", ""], ["Chen", "Tianshui", ""], ["Xie", "Yuan", ""], ["Wu", "Hefeng", ""], ["Lin", "Liang", ""]]}, {"id": "2012.14597", "submitter": "Shuyuan Lin", "authors": "Shuyuan Lin, Xing Wang, Guobao Xiao, Yan Yan, Hanzi Wang", "title": "Hierarchical Representation via Message Propagation for Robust Model\n  Fitting", "comments": null, "journal-ref": null, "doi": "10.1109/TIE.2020.3018074", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel hierarchical representation via message\npropagation (HRMP) method for robust model fitting, which simultaneously takes\nadvantages of both the consensus analysis and the preference analysis to\nestimate the parameters of multiple model instances from data corrupted by\noutliers, for robust model fitting. Instead of analyzing the information of\neach data point or each model hypothesis independently, we formulate the\nconsensus information and the preference information as a hierarchical\nrepresentation to alleviate the sensitivity to gross outliers. Specifically, we\nfirstly construct a hierarchical representation, which consists of a model\nhypothesis layer and a data point layer. The model hypothesis layer is used to\nremove insignificant model hypotheses and the data point layer is used to\nremove gross outliers. Then, based on the hierarchical representation, we\npropose an effective hierarchical message propagation (HMP) algorithm and an\nimproved affinity propagation (IAP) algorithm to prune insignificant vertices\nand cluster the remaining data points, respectively. The proposed HRMP can not\nonly accurately estimate the number and parameters of multiple model instances,\nbut also handle multi-structural data contaminated with a large number of\noutliers. Experimental results on both synthetic data and real images show that\nthe proposed HRMP significantly outperforms several state-of-the-art model\nfitting methods in terms of fitting accuracy and speed.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 04:14:19 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Lin", "Shuyuan", ""], ["Wang", "Xing", ""], ["Xiao", "Guobao", ""], ["Yan", "Yan", ""], ["Wang", "Hanzi", ""]]}, {"id": "2012.14618", "submitter": "Yajun Xu", "authors": "Yajun Xu, Shogo Arai, Diyi Liu, Fangzhou Lin, Kazuhiro Kosuge", "title": "FPCC: Fast Point Cloud Clustering for Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Instance segmentation is an important pre-processing task in numerous\nreal-world applications, such as robotics, autonomous vehicles, and\nhuman-computer interaction. However, there has been little research on 3D point\ncloud instance segmentation of bin-picking scenes in which multiple objects of\nthe same class are stacked together. Compared with the rapid development of\ndeep learning for two-dimensional (2D) image tasks, deep learning-based 3D\npoint cloud segmentation still has a lot of room for development. In such a\nsituation, distinguishing a large number of occluded objects of the same class\nis a highly challenging problem. In a usual bin-picking scene, an object model\nis known and the number of object type is one. Thus, the semantic information\ncan be ignored; instead, the focus is put on the segmentation of instances.\nBased on this task requirement, we propose a network (FPCC-Net) that infers\nfeature centers of each instance and then clusters the remaining points to the\nclosest feature center in feature embedding space. FPCC-Net includes two\nsubnets, one for inferring the feature centers for clustering and the other for\ndescribing features of each point. The proposed method is compared with\nexisting 3D point cloud and 2D segmentation methods in some bin-picking scenes.\nIt is shown that FPCC-Net improves average precision (AP) by about 40\\% than\nSGPN and can process about 60,000 points in about 0.8 [s].\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 05:58:35 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 03:10:10 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 10:01:32 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Xu", "Yajun", ""], ["Arai", "Shogo", ""], ["Liu", "Diyi", ""], ["Lin", "Fangzhou", ""], ["Kosuge", "Kazuhiro", ""]]}, {"id": "2012.14619", "submitter": "Mo Zhang", "authors": "Mo Zhang, Quanzheng Li", "title": "MS-GWNN:multi-scale graph wavelet neural network for breast cancer\n  diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Breast cancer is one of the most common cancers in women worldwide, and early\ndetection can significantly reduce the mortality rate of breast cancer. It is\ncrucial to take multi-scale information of tissue structure into account in the\ndetection of breast cancer. And thus, it is the key to design an accurate\ncomputer-aided detection (CAD) system to capture multi-scale contextual\nfeatures in a cancerous tissue. In this work, we present a novel graph\nconvolutional neural network for histopathological image classification of\nbreast cancer. The new method, named multi-scale graph wavelet neural network\n(MS-GWNN), leverages the localization property of spectral graph wavelet to\nperform multi-scale analysis. By aggregating features at different scales,\nMS-GWNN can encode the multi-scale contextual interactions in the whole\npathological slide. Experimental results on two public datasets demonstrate the\nsuperiority of the proposed method. Moreover, through ablation studies, we find\nthat multi-scale analysis has a significant impact on the accuracy of cancer\ndiagnosis.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 06:04:27 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Zhang", "Mo", ""], ["Li", "Quanzheng", ""]]}, {"id": "2012.14625", "submitter": "Todd Goodall", "authors": "Todd Goodall and Alan C. Bovik", "title": "The VIP Gallery for Video Processing Education", "comments": "6 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Digital video pervades daily life. Mobile video, digital TV, and digital\ncinema are now ubiquitous, and as such, the field of Digital Video Processing\n(DVP) has experienced tremendous growth. Digital video systems also permeate\nscientific and engineering disciplines including but not limited to astronomy,\ncommunications, surveillance, entertainment, video coding, computer vision, and\nvision research. As a consequence, educational tools for DVP must cater to a\nlarge and diverse base of students. Towards enhancing DVP education we have\ncreated a carefully constructed gallery of educational tools that is designed\nto complement a comprehensive corpus of online lectures by providing examples\nof DVP on real-world content, along with a user-friendly interface that\norganizes numerous key DVP topics ranging from analog video, to human visual\nprocessing, to modern video codecs, etc. This demonstration gallery is\ncurrently being used effectively in the graduate class ``Digital Video'' at the\nUniversity of Texas at Austin. Students receive enhanced access to concepts\nthrough both learning theory from highly visual lectures and watching concrete\nexamples from the gallery, which captures the beauty of the underlying\nprinciples of modern video processing. To better understand the educational\nvalue of these tools, we conducted a pair of questionaire-based surveys to\nassess student background, expectations, and outcomes. The survey results\nsupport the teaching efficacy of this new didactic video toolset.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 06:40:41 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Goodall", "Todd", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2012.14629", "submitter": "Daniel Stanley Tan", "authors": "Daniel Stanley Tan, Yi-Chun Chen, Trista Pei-Chun Chen, Wei-Chao Chen", "title": "TrustMAE: A Noise-Resilient Defect Classification Framework using\n  Memory-Augmented Auto-Encoders with Trust Regions", "comments": "Accepted at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a framework called TrustMAE to address the problem\nof product defect classification. Instead of relying on defective images that\nare difficult to collect and laborious to label, our framework can accept\ndatasets with unlabeled images. Moreover, unlike most anomaly detection\nmethods, our approach is robust against noises, or defective images, in the\ntraining dataset. Our framework uses a memory-augmented auto-encoder with a\nsparse memory addressing scheme to avoid over-generalizing the auto-encoder,\nand a novel trust-region memory updating scheme to keep the noises away from\nthe memory slots. The result is a framework that can reconstruct defect-free\nimages and identify the defective regions using a perceptual distance network.\nWhen compared against various state-of-the-art baselines, our approach performs\ncompetitively under noise-free MVTec datasets. More importantly, it remains\neffective at a noise level up to 40% while significantly outperforming other\nbaselines.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 06:57:36 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Tan", "Daniel Stanley", ""], ["Chen", "Yi-Chun", ""], ["Chen", "Trista Pei-Chun", ""], ["Chen", "Wei-Chao", ""]]}, {"id": "2012.14639", "submitter": "Shivang Agarwal", "authors": "Shivang Agarwal, Ajita Rattani, C. Ravindranath Chowdary", "title": "AILearn: An Adaptive Incremental Learning Model for Spoof Fingerprint\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental learning enables the learner to accommodate new knowledge without\nretraining the existing model. It is a challenging task which requires learning\nfrom new data as well as preserving the knowledge extracted from the previously\naccessed data. This challenge is known as the stability-plasticity dilemma. We\npropose AILearn, a generic model for incremental learning which overcomes the\nstability-plasticity dilemma by carefully integrating the ensemble of base\nclassifiers trained on new data with the current ensemble without retraining\nthe model from scratch using entire data. We demonstrate the efficacy of the\nproposed AILearn model on spoof fingerprint detection application. One of the\nsignificant challenges associated with spoof fingerprint detection is the\nperformance drop on spoofs generated using new fabrication materials. AILearn\nis an adaptive incremental learning model which adapts to the features of the\n``live'' and ``spoof'' fingerprint images and efficiently recognizes the new\nspoof fingerprints as well as the known spoof fingerprints when the new data is\navailable. To the best of our knowledge, AILearn is the first attempt in\nincremental learning algorithms that adapts to the properties of data for\ngenerating a diverse ensemble of base classifiers. From the experiments\nconducted on standard high-dimensional datasets LivDet 2011, LivDet 2013 and\nLivDet 2015, we show that the performance gain on new fake materials is\nsignificantly high. On an average, we achieve $49.57\\%$ improvement in accuracy\nbetween the consecutive learning phases.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 07:26:37 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Agarwal", "Shivang", ""], ["Rattani", "Ajita", ""], ["Chowdary", "C. Ravindranath", ""]]}, {"id": "2012.14661", "submitter": "Abhishek S", "authors": "Abhishek and Shekhar Verma", "title": "Parzen Window Approximation on Riemannian Manifold", "comments": "23 pages, 52 figures and the executable code is available at\n  https://github.com/gitr00ki3/vpw", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In graph motivated learning, label propagation largely depends on data\naffinity represented as edges between connected data points. The affinity\nassignment implicitly assumes even distribution of data on the manifold. This\nassumption may not hold and may lead to inaccurate metric assignment due to\ndrift towards high-density regions. The drift affected heat kernel based\naffinity with a globally fixed Parzen window either discards genuine neighbors\nor forces distant data points to become a member of the neighborhood. This\nyields a biased affinity matrix. In this paper, the bias due to uneven data\nsampling on the Riemannian manifold is catered to by a variable Parzen window\ndetermined as a function of neighborhood size, ambient dimension, flatness\nrange, etc. Additionally, affinity adjustment is used which offsets the effect\nof uneven sampling responsible for the bias. An affinity metric which takes\ninto consideration the irregular sampling effect to yield accurate label\npropagation is proposed. Extensive experiments on synthetic and real-world data\nsets confirm that the proposed method increases the classification accuracy\nsignificantly and outperforms existing Parzen window estimators in graph\nLaplacian manifold regularization methods.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 08:52:31 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Abhishek", "", ""], ["Verma", "Shekhar", ""]]}, {"id": "2012.14672", "submitter": "Xiu-Shen Wei", "authors": "Xiu-Shen Wei, Yu-Yan Xu, Yazhou Yao, Jia Wei, Si Xi, Wenyuan Xu,\n  Weidong Zhang, Xiaoxin Lv, Dengpan Fu, Qing Li, Baoying Chen, Haojie Guo,\n  Taolue Xue, Haipeng Jing, Zhiheng Wang, Tianming Zhang, Mingwen Zhang", "title": "Tips and Tricks for Webly-Supervised Fine-Grained Recognition: Learning\n  from the WebFG 2020 Challenge", "comments": "This is a technical report of the WebFG 2020 challenge\n  (https://sites.google.com/view/webfg2020) associated with ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  WebFG 2020 is an international challenge hosted by Nanjing University of\nScience and Technology, University of Edinburgh, Nanjing University, The\nUniversity of Adelaide, Waseda University, etc. This challenge mainly pays\nattention to the webly-supervised fine-grained recognition problem. In the\nliterature, existing deep learning methods highly rely on large-scale and\nhigh-quality labeled training data, which poses a limitation to their\npracticability and scalability in real world applications. In particular, for\nfine-grained recognition, a visual task that requires professional knowledge\nfor labeling, the cost of acquiring labeled training data is quite high. It\ncauses extreme difficulties to obtain a large amount of high-quality training\ndata. Therefore, utilizing free web data to train fine-grained recognition\nmodels has attracted increasing attentions from researchers in the fine-grained\ncommunity. This challenge expects participants to develop webly-supervised\nfine-grained recognition methods, which leverages web images in training\nfine-grained recognition models to ease the extreme dependence of deep learning\nmethods on large-scale manually labeled datasets and to enhance their\npracticability and scalability. In this technical report, we have pulled\ntogether the top WebFG 2020 solutions of total 54 competing teams, and discuss\nwhat methods worked best across the set of winning teams, and what surprisingly\ndid not help.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 09:14:26 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Wei", "Xiu-Shen", ""], ["Xu", "Yu-Yan", ""], ["Yao", "Yazhou", ""], ["Wei", "Jia", ""], ["Xi", "Si", ""], ["Xu", "Wenyuan", ""], ["Zhang", "Weidong", ""], ["Lv", "Xiaoxin", ""], ["Fu", "Dengpan", ""], ["Li", "Qing", ""], ["Chen", "Baoying", ""], ["Guo", "Haojie", ""], ["Xue", "Taolue", ""], ["Jing", "Haipeng", ""], ["Wang", "Zhiheng", ""], ["Zhang", "Tianming", ""], ["Zhang", "Mingwen", ""]]}, {"id": "2012.14678", "submitter": "Shuang Xu", "authors": "Shuang Xu and Lizhen Ji and Zhe Wang and Pengfei Li and Kai Sun and\n  Chunxia Zhang and Jiangshe Zhang", "title": "Towards Reducing Severe Defocus Spread Effects for Multi-Focus Image\n  Fusion via an Optimization Based Strategy", "comments": null, "journal-ref": "IEEE Transactions on Computational Imaging, vol. 6, pp. 1561-1570,\n  2020", "doi": "10.1109/TCI.2020.3039564", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-focus image fusion (MFF) is a popular technique to generate an\nall-in-focus image, where all objects in the scene are sharp. However, existing\nmethods pay little attention to defocus spread effects of the real-world\nmulti-focus images. Consequently, most of the methods perform badly in the\nareas near focus map boundaries. According to the idea that each local region\nin the fused image should be similar to the sharpest one among source images,\nthis paper presents an optimization-based approach to reduce defocus spread\neffects. Firstly, a new MFF assessmentmetric is presented by combining the\nprinciple of structure similarity and detected focus maps. Then, MFF problem is\ncast into maximizing this metric. The optimization is solved by gradient\nascent. Experiments conducted on the real-world dataset verify superiority of\nthe proposed model. The codes are available at\nhttps://github.com/xsxjtu/MFF-SSIM.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 09:26:41 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Xu", "Shuang", ""], ["Ji", "Lizhen", ""], ["Wang", "Zhe", ""], ["Li", "Pengfei", ""], ["Sun", "Kai", ""], ["Zhang", "Chunxia", ""], ["Zhang", "Jiangshe", ""]]}, {"id": "2012.14690", "submitter": "Heyi Li", "authors": "Heyi Li, Dongdong Chen, William H. Nailon, Mike E. Davies, and David\n  Laurenson", "title": "COIN: Contrastive Identifier Network for Breast Mass Diagnosis in\n  Mammography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-aided breast cancer diagnosis in mammography is a challenging\nproblem, stemming from mammographical data scarcity and data entanglement. In\nparticular, data scarcity is attributed to the privacy and expensive\nannotation. And data entanglement is due to the high similarity between benign\nand malignant masses, of which manifolds reside in lower dimensional space with\nvery small margin. To address these two challenges, we propose a deep learning\nframework, named Contrastive Identifier Network (\\textsc{COIN}), which\nintegrates adversarial augmentation and manifold-based contrastive learning.\nFirstly, we employ adversarial learning to create both on- and off-distribution\nmass contained ROIs. After that, we propose a novel contrastive loss with a\nbuilt Signed graph. Finally, the neural network is optimized in a contrastive\nlearning manner, with the purpose of improving the deep model's\ndiscriminativity on the extended dataset. In particular, by employing COIN,\ndata samples from the same category are pulled close whereas those with\ndifferent labels are pushed further in the deep latent space. Moreover, COIN\noutperforms the state-of-the-art related algorithms for solving breast cancer\ndiagnosis problem by a considerable margin, achieving 93.4\\% accuracy and\n95.0\\% AUC score. The code will release on ***.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 10:02:02 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Li", "Heyi", ""], ["Chen", "Dongdong", ""], ["Nailon", "William H.", ""], ["Davies", "Mike E.", ""], ["Laurenson", "David", ""]]}, {"id": "2012.14700", "submitter": "Sangwoong Yoon", "authors": "Sangwoong Yoon, Woo Young Kang, Sungwook Jeon, SeongEun Lee, Changjin\n  Han, Jonghun Park, Eun-Sol Kim", "title": "Image-to-Image Retrieval by Learning Similarity between Scene Graphs", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As a scene graph compactly summarizes the high-level content of an image in a\nstructured and symbolic manner, the similarity between scene graphs of two\nimages reflects the relevance of their contents. Based on this idea, we propose\na novel approach for image-to-image retrieval using scene graph similarity\nmeasured by graph neural networks. In our approach, graph neural networks are\ntrained to predict the proxy image relevance measure, computed from\nhuman-annotated captions using a pre-trained sentence similarity model. We\ncollect and publish the dataset for image relevance measured by human\nannotators to evaluate retrieval algorithms. The collected dataset shows that\nour method agrees well with the human perception of image similarity than other\ncompetitive baselines.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 10:45:20 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Yoon", "Sangwoong", ""], ["Kang", "Woo Young", ""], ["Jeon", "Sungwook", ""], ["Lee", "SeongEun", ""], ["Han", "Changjin", ""], ["Park", "Jonghun", ""], ["Kim", "Eun-Sol", ""]]}, {"id": "2012.14704", "submitter": "Kailiang Lu", "authors": "Kailiang Lu", "title": "Advances in deep learning methods for pavement surface crack detection\n  and identification with visible light visual images", "comments": "15 pages, 14 figures, 10 tables", "journal-ref": "Computer Engineering and Science, 2022", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Compared to NDT and health monitoring method for cracks in engineering\nstructures, surface crack detection or identification based on visible light\nimages is non-contact, with the advantages of fast speed, low cost and high\nprecision. Firstly, typical pavement (concrete also) crack public data sets\nwere collected, and the characteristics of sample images as well as the random\nvariable factors, including environmental, noise and interference etc., were\nsummarized. Subsequently, the advantages and disadvantages of three main crack\nidentification methods (i.e., hand-crafted feature engineering, machine\nlearning, deep learning) were compared. Finally, from the aspects of model\narchitecture, testing performance and predicting effectiveness, the development\nand progress of typical deep learning models, including self-built CNN,\ntransfer learning(TL) and encoder-decoder(ED), which can be easily deployed on\nembedded platform, were reviewed. The benchmark test shows that: 1) It has been\nable to realize real-time pixel-level crack identification on embedded\nplatform: the entire crack detection average time cost of an image sample is\nless than 100ms, either using the ED method (i.e., FPCNet) or the TL method\nbased on InceptionV3. It can be reduced to less than 10ms with TL method based\non MobileNet (a lightweight backbone base network). 2) In terms of accuracy, it\ncan reach over 99.8% on CCIC which is easily identified by human eyes. On\nSDNET2018, some samples of which are difficult to be identified, FPCNet can\nreach 97.5%, while TL method is close to 96.1%.\n  To the best of our knowledge, this paper for the first time comprehensively\nsummarizes the pavement crack public data sets, and the performance and\neffectiveness of surface crack detection and identification deep learning\nmethods for embedded platform, are reviewed and evaluated.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 11:10:12 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Lu", "Kailiang", ""]]}, {"id": "2012.14739", "submitter": "Yu Rong", "authors": "Yu Rong, Ziwei Liu, Chen Change Loy", "title": "Chasing the Tail in Monocular 3D Human Reconstruction with Prototype\n  Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved great progress in single-image 3D human\nreconstruction. However, existing methods still fall short in predicting rare\nposes. The reason is that most of the current models perform regression based\non a single human prototype, which is similar to common poses while far from\nthe rare poses. In this work, we 1) identify and analyze this learning obstacle\nand 2) propose a prototype memory-augmented network, PM-Net, that effectively\nimproves performances of predicting rare poses. The core of our framework is a\nmemory module that learns and stores a set of 3D human prototypes capturing\nlocal distributions for either common poses or rare poses. With this\nformulation, the regression starts from a better initialization, which is\nrelatively easier to converge. Extensive experiments on several widely employed\ndatasets demonstrate the proposed framework's effectiveness compared to other\nstate-of-the-art methods. Notably, our approach significantly improves the\nmodels' performances on rare poses while generating comparable results on other\nsamples.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 12:57:22 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Rong", "Yu", ""], ["Liu", "Ziwei", ""], ["Loy", "Chen Change", ""]]}, {"id": "2012.14758", "submitter": "Veeru Talreja", "authors": "Veeru Talreja, Matthew Valenti, Nasser Nasrabadi", "title": "Deep Hashing for Secure Multimodal Biometrics", "comments": null, "journal-ref": "IEEE Transactions on Information Forensics and\n  Security,vol.16,pp.1306-1321,2021", "doi": "10.1109/TIFS.2020.3033189", "report-no": null, "categories": "cs.CV cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When compared to unimodal systems, multimodal biometric systems have several\nadvantages, including lower error rate, higher accuracy, and larger population\ncoverage. However, multimodal systems have an increased demand for integrity\nand privacy because they must store multiple biometric traits associated with\neach user. In this paper, we present a deep learning framework for\nfeature-level fusion that generates a secure multimodal template from each\nuser's face and iris biometrics. We integrate a deep hashing (binarization)\ntechnique into the fusion architecture to generate a robust binary multimodal\nshared latent representation. Further, we employ a hybrid secure architecture\nby combining cancelable biometrics with secure sketch techniques and integrate\nit with a deep hashing framework, which makes it computationally prohibitive to\nforge a combination of multiple biometrics that pass the authentication. The\nefficacy of the proposed approach is shown using a multimodal database of face\nand iris and it is observed that the matching performance is improved due to\nthe fusion of multiple biometrics. Furthermore, the proposed approach also\nprovides cancelability and unlinkability of the templates along with improved\nprivacy of the biometric data. Additionally, we also test the proposed hashing\nfunction for an image retrieval application using a benchmark dataset. The main\ngoal of this paper is to develop a method for integrating multimodal fusion,\ndeep hashing, and biometric security, with an emphasis on structural data from\nmodalities like face and iris. The proposed approach is in no way a general\nbiometric security framework that can be applied to all biometric modalities,\nas further research is needed to extend the proposed framework to other\nunconstrained biometric modalities.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 14:15:05 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Talreja", "Veeru", ""], ["Valenti", "Matthew", ""], ["Nasrabadi", "Nasser", ""]]}, {"id": "2012.14766", "submitter": "Stefan Schubert", "authors": "Stefan Schubert, Peer Neubert, Peter Protzel", "title": "Graph-based non-linear least squares optimization for visual place\n  recognition in changing environments", "comments": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L) 2021. This is the accepted version", "journal-ref": null, "doi": "10.1109/LRA.2021.3052446", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual place recognition is an important subproblem of mobile robot\nlocalization. Since it is a special case of image retrieval, the basic source\nof information is the pairwise similarity of image descriptors. However, the\nembedding of the image retrieval problem in this robotic task provides\nadditional structure that can be exploited, e.g. spatio-temporal consistency.\nSeveral algorithms exist to exploit this structure, e.g., sequence processing\napproaches or descriptor standardization approaches for changing environments.\nIn this paper, we propose a graph-based framework to systematically exploit\ndifferent types of additional structure and information. The graphical model is\nused to formulate a non-linear least squares problem that can be optimized with\nstandard tools. Beyond sequences and standardization, we propose the usage of\nintra-set similarities within the database and/or the query image set as\nadditional source of information. If available, our approach also allows to\nseamlessly integrate additional knowledge about poses of database images. We\nevaluate the system on a variety of standard place recognition datasets and\ndemonstrate performance improvements for a large number of different\nconfigurations including different sources of information, different types of\nconstraints, and online or offline place recognition setups.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 14:25:34 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Schubert", "Stefan", ""], ["Neubert", "Peer", ""], ["Protzel", "Peter", ""]]}, {"id": "2012.14785", "submitter": "Yao Zhang", "authors": "Yao Zhang, Jiawei Yang, Feng Hou, Yang Liu, Yixin Wang, Jiang Tian,\n  Cheng Zhong, Yang Zhang, and Zhiqiang He", "title": "Semi-supervised Cardiac Image Segmentation via Label Propagation and\n  Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Accurate segmentation of cardiac structures can assist doctors to diagnose\ndiseases, and to improve treatment planning, which is highly demanded in the\nclinical practice. However, the shortage of annotation and the variance of the\ndata among different vendors and medical centers restrict the performance of\nadvanced deep learning methods. In this work, we present a fully automatic\nmethod to segment cardiac structures including the left (LV) and right\nventricle (RV) blood pools, as well as for the left ventricular myocardium\n(MYO) in MRI volumes. Specifically, we design a semi-supervised learning method\nto leverage unlabelled MRI sequence timeframes by label propagation. Then we\nexploit style transfer to reduce the variance among different centers and\nvendors for more robust cardiac image segmentation. We evaluate our method in\nthe M&Ms challenge 7 , ranking 2nd place among 14 competitive teams.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 14:57:03 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Zhang", "Yao", ""], ["Yang", "Jiawei", ""], ["Hou", "Feng", ""], ["Liu", "Yang", ""], ["Wang", "Yixin", ""], ["Tian", "Jiang", ""], ["Zhong", "Cheng", ""], ["Zhang", "Yang", ""], ["He", "Zhiqiang", ""]]}, {"id": "2012.14833", "submitter": "Kostas Alexis PhD", "authors": "Frank Mascarich, Kostas Alexis", "title": "Visual-Thermal Camera Dataset Release and Multi-Modal Alignment without\n  Calibration Information", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This report accompanies a dataset release on visual and thermal camera data\nand details a procedure followed to align such multi-modal camera frames in\norder to provide pixel-level correspondence between the two without using\nintrinsic or extrinsic calibration information. To achieve this goal we benefit\nfrom progress in the domain of multi-modal image alignment and specifically\nemploy the Mattes Mutual Information Metric to guide the registration process.\nIn the released dataset we release both the raw visual and thermal camera data,\nas well as the aligned frames, alongside calibration parameters with the goal\nto better facilitate the investigation on common local/global features across\nsuch multi-modal image streams.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 16:20:28 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Mascarich", "Frank", ""], ["Alexis", "Kostas", ""]]}, {"id": "2012.14840", "submitter": "Vinayak Elangovan", "authors": "Pengchang Chen and Vinayak Elangovan", "title": "Object sorting using faster R-CNN", "comments": "10 pages, 10 figures, 5 tables", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA), Vol.11, No.5/6,November 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In a factory production line, different industry parts need to be quickly\ndifferentiated and sorted for further process. Parts can be of different colors\nand shapes. It is tedious for humans to differentiate and sort these objects in\nappropriate categories. Automating this process would save more time and cost.\nIn the automation process, choosing an appropriate model to detect and classify\ndifferent objects based on specific features is more challenging. In this\npaper, three different neural network models are compared to the object sorting\nsystem. They are namely CNN, Fast R-CNN, and Faster R-CNN. These models are\ntested, and their performance is analyzed. Moreover, for the object sorting\nsystem, an Arduino-controlled 5 DoF (degree of freedom) robot arm is programmed\nto grab and drop symmetrical objects to the targeted zone. Objects are\ncategorized into classes based on color, defective and non-defective objects.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 16:41:56 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Chen", "Pengchang", ""], ["Elangovan", "Vinayak", ""]]}, {"id": "2012.14885", "submitter": "Tawfiq Salem", "authors": "Tawfiq Salem, Scott Workman, Nathan Jacobs", "title": "Learning a Dynamic Map of Visual Appearance", "comments": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The appearance of the world varies dramatically not only from place to place\nbut also from hour to hour and month to month. Every day billions of images\ncapture this complex relationship, many of which are associated with precise\ntime and location metadata. We propose to use these images to construct a\nglobal-scale, dynamic map of visual appearance attributes. Such a map enables\nfine-grained understanding of the expected appearance at any geographic\nlocation and time. Our approach integrates dense overhead imagery with location\nand time metadata into a general framework capable of mapping a wide variety of\nvisual attributes. A key feature of our approach is that it requires no manual\ndata annotation. We demonstrate how this approach can support various\napplications, including image-driven mapping, image geolocalization, and\nmetadata verification.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 18:23:56 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Salem", "Tawfiq", ""], ["Workman", "Scott", ""], ["Jacobs", "Nathan", ""]]}, {"id": "2012.14891", "submitter": "Abhishek Das", "authors": "Abhishek Das, Japsimar Singh Wahi, Siyao Li", "title": "Detecting Hate Speech in Multi-modal Memes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the past few years, there has been a surge of interest in multi-modal\nproblems, from image captioning to visual question answering and beyond. In\nthis paper, we focus on hate speech detection in multi-modal memes wherein\nmemes pose an interesting multi-modal fusion problem. We aim to solve the\nFacebook Meme Challenge \\cite{kiela2020hateful} which aims to solve a binary\nclassification problem of predicting whether a meme is hateful or not. A\ncrucial characteristic of the challenge is that it includes \"benign\nconfounders\" to counter the possibility of models exploiting unimodal priors.\nThe challenge states that the state-of-the-art models perform poorly compared\nto humans. During the analysis of the dataset, we realized that majority of the\ndata points which are originally hateful are turned into benign just be\ndescribing the image of the meme. Also, majority of the multi-modal baselines\ngive more preference to the hate speech (language modality). To tackle these\nproblems, we explore the visual modality using object detection and image\ncaptioning models to fetch the \"actual caption\" and then combine it with the\nmulti-modal representation to perform binary classification. This approach\ntackles the benign text confounders present in the dataset to improve the\nperformance. Another approach we experiment with is to improve the prediction\nwith sentiment analysis. Instead of only using multi-modal representations\nobtained from pre-trained neural networks, we also include the unimodal\nsentiment to enrich the features. We perform a detailed analysis of the above\ntwo approaches, providing compelling reasons in favor of the methodologies\nused.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 18:30:00 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Das", "Abhishek", ""], ["Wahi", "Japsimar Singh", ""], ["Li", "Siyao", ""]]}, {"id": "2012.14929", "submitter": "Hani Itani", "authors": "Hani Itani, Silvio Giancola, Ali Thabet, Bernard Ghanem", "title": "SALA: Soft Assignment Local Aggregation for Parameter Efficient 3D\n  Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we focus on designing a point local aggregation function that\nyields parameter efficient networks for 3D point cloud semantic segmentation.\nWe explore the idea of using learnable neighbor-to-grid soft assignment in\ngrid-based aggregation functions. Previous methods in literature operate on a\npredefined geometric grid such as local volume partitions or irregular kernel\npoints. A more general alternative is to allow the network to learn an\nassignment function that best suits the end task. Since it is learnable, this\nmapping is allowed to be different per layer instead of being applied uniformly\nthroughout the depth of the network. By endowing the network with the\nflexibility to learn its own neighbor-to-grid assignment, we arrive at\nparameter efficient models that achieve state-of-the-art (SOTA) performance on\nS3DIS with at least 10$\\times$ less parameters than the current reigning\nmethod. We also demonstrate competitive performance on ScanNet and PartNet\ncompared with much larger SOTA models.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 20:16:37 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 13:40:36 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Itani", "Hani", ""], ["Giancola", "Silvio", ""], ["Thabet", "Ali", ""], ["Ghanem", "Bernard", ""]]}, {"id": "2012.14950", "submitter": "Hengduo Li", "authors": "Hengduo Li, Zuxuan Wu, Abhinav Shrivastava, Larry S. Davis", "title": "2D or not 2D? Adaptive 3D Convolution Selection for Efficient Video\n  Recognition", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D convolutional networks are prevalent for video recognition. While\nachieving excellent recognition performance on standard benchmarks, they\noperate on a sequence of frames with 3D convolutions and thus are\ncomputationally demanding. Exploiting large variations among different videos,\nwe introduce Ada3D, a conditional computation framework that learns\ninstance-specific 3D usage policies to determine frames and convolution layers\nto be used in a 3D network. These policies are derived with a two-head\nlightweight selection network conditioned on each input video clip. Then, only\nframes and convolutions that are selected by the selection network are used in\nthe 3D model to generate predictions. The selection network is optimized with\npolicy gradient methods to maximize a reward that encourages making correct\npredictions with limited computation. We conduct experiments on three video\nrecognition benchmarks and demonstrate that our method achieves similar\naccuracies to state-of-the-art 3D models while requiring 20%-50% less\ncomputation across different datasets. We also show that learned policies are\ntransferable and Ada3D is compatible to different backbones and modern clip\nselection approaches. Our qualitative analysis indicates that our method\nallocates fewer 3D convolutions and frames for \"static\" inputs, yet uses more\nfor motion-intensive clips.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 21:40:38 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 02:49:12 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Li", "Hengduo", ""], ["Wu", "Zuxuan", ""], ["Shrivastava", "Abhinav", ""], ["Davis", "Larry S.", ""]]}, {"id": "2012.15000", "submitter": "Micha\\\"el Defferrard", "authors": "Micha\\\"el Defferrard, Martino Milani, Fr\\'ed\\'erick Gusset,\n  Nathana\\\"el Perraudin", "title": "DeepSphere: a graph-based spherical CNN", "comments": "published at ICLR'20, https://openreview.net/forum?id=B1e3OlStPB", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Designing a convolution for a spherical neural network requires a delicate\ntradeoff between efficiency and rotation equivariance. DeepSphere, a method\nbased on a graph representation of the sampled sphere, strikes a controllable\nbalance between these two desiderata. This contribution is twofold. First, we\nstudy both theoretically and empirically how equivariance is affected by the\nunderlying graph with respect to the number of vertices and neighbors. Second,\nwe evaluate DeepSphere on relevant problems. Experiments show state-of-the-art\nperformance and demonstrates the efficiency and flexibility of this\nformulation. Perhaps surprisingly, comparison with previous work suggests that\nanisotropic filters might be an unnecessary price to pay. Our code is available\nat https://github.com/deepsphere\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 01:35:27 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Defferrard", "Micha\u00ebl", ""], ["Milani", "Martino", ""], ["Gusset", "Fr\u00e9d\u00e9rick", ""], ["Perraudin", "Nathana\u00ebl", ""]]}, {"id": "2012.15020", "submitter": "Zhangkai Ni", "authors": "Zhangkai Ni, Wenhan Yang, Shiqi Wang, Lin Ma, and Sam Kwong", "title": "Towards Unsupervised Deep Image Enhancement with Generative Adversarial\n  Network", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.3023615", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving the aesthetic quality of images is challenging and eager for the\npublic. To address this problem, most existing algorithms are based on\nsupervised learning methods to learn an automatic photo enhancer for paired\ndata, which consists of low-quality photos and corresponding expert-retouched\nversions. However, the style and characteristics of photos retouched by experts\nmay not meet the needs or preferences of general users. In this paper, we\npresent an unsupervised image enhancement generative adversarial network\n(UEGAN), which learns the corresponding image-to-image mapping from a set of\nimages with desired characteristics in an unsupervised manner, rather than\nlearning on a large number of paired images. The proposed model is based on\nsingle deep GAN which embeds the modulation and attention mechanisms to capture\nricher global and local features. Based on the proposed model, we introduce two\nlosses to deal with the unsupervised image enhancement: (1) fidelity loss,\nwhich is defined as a L2 regularization in the feature domain of a pre-trained\nVGG network to ensure the content between the enhanced image and the input\nimage is the same, and (2) quality loss that is formulated as a relativistic\nhinge adversarial loss to endow the input image the desired characteristics.\nBoth quantitative and qualitative results show that the proposed model\neffectively improves the aesthetic quality of images. Our code is available at:\nhttps://github.com/eezkni/UEGAN.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 03:22:46 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Ni", "Zhangkai", ""], ["Yang", "Wenhan", ""], ["Wang", "Shiqi", ""], ["Ma", "Lin", ""], ["Kwong", "Sam", ""]]}, {"id": "2012.15025", "submitter": "Zhiyuan Chen Dr", "authors": "Wei Xiang Lim, Zhiyuan Chen, Amr Ahmed, Tissa Chandesa and Iman Liao", "title": "A Review of Machine Learning Techniques for Applied Eye Fundus and\n  Tongue Digital Image Processing with Diabetes Management System", "comments": "This paper is published in The International Conference on Digital\n  Image and Signal Processing (DISP 2019)At: Oxford, United Kingdom", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diabetes is a global epidemic and it is increasing at an alarming rate. The\nInternational Diabetes Federation (IDF) projected that the total number of\npeople with diabetes globally may increase by 48%, from 425 million (year 2017)\nto 629 million (year 2045). Moreover, diabetes had caused millions of deaths\nand the number is increasing drastically. Therefore, this paper addresses the\nbackground of diabetes and its complications. In addition, this paper\ninvestigates innovative applications and past researches in the areas of\ndiabetes management system with applied eye fundus and tongue digital images.\nDifferent types of existing applied eye fundus and tongue digital image\nprocessing with diabetes management systems in the market and state-of-the-art\nmachine learning techniques from previous literature have been reviewed. The\nimplication of this paper is to have an overview in diabetic research and what\nnew machine learning techniques can be proposed in solving this global\nepidemic.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 03:49:37 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Lim", "Wei Xiang", ""], ["Chen", "Zhiyuan", ""], ["Ahmed", "Amr", ""], ["Chandesa", "Tissa", ""], ["Liao", "Iman", ""]]}, {"id": "2012.15028", "submitter": "Shen Cheng", "authors": "Shen Cheng, Yuzhi Wang, Haibin Huang, Donghao Liu, Haoqiang Fan,\n  Shuaicheng Liu", "title": "NBNet: Noise Basis Learning for Image Denoising with Subspace Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we introduce NBNet, a novel framework for image denoising.\nUnlike previous works, we propose to tackle this challenging problem from a new\nperspective: noise reduction by image-adaptive projection. Specifically, we\npropose to train a network that can separate signal and noise by learning a set\nof reconstruction basis in the feature space. Subsequently, image denosing can\nbe achieved by selecting corresponding basis of the signal subspace and\nprojecting the input into such space. Our key insight is that projection can\nnaturally maintain the local structure of input signal, especially for areas\nwith low light or weak textures. Towards this end, we propose SSA, a non-local\nsubspace attention module designed explicitly to learn the basis generation as\nwell as the subspace projection. We further incorporate SSA with NBNet, a UNet\nstructured network designed for end-to-end image denosing. We conduct\nevaluations on benchmarks, including SIDD and DND, and NBNet achieves\nstate-of-the-art performance on PSNR and SSIM with significantly less\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 04:03:04 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 18:14:15 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Cheng", "Shen", ""], ["Wang", "Yuzhi", ""], ["Huang", "Haibin", ""], ["Liu", "Donghao", ""], ["Fan", "Haoqiang", ""], ["Liu", "Shuaicheng", ""]]}, {"id": "2012.15041", "submitter": "Jaouhar Fattahi", "authors": "Jaouhar Fattahi and Mohamed Mejri", "title": "Damaged Fingerprint Recognition by Convolutional Long Short-Term Memory\n  Networks for Forensic Purposes", "comments": "This paper was accepted, on December 5, 2020, for publication and\n  oral presentation at the 2021 IEEE 5th International Conference on\n  Cryptography, Security and Privacy (CSP 2021) to be held in Zhuhai, China\n  during January 8-10, 2021 and hosted by Beijing Normal University (Zhuhai)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint recognition is often a game-changing step in establishing\nevidence against criminals. However, we are increasingly finding that criminals\ndeliberately alter their fingerprints in a variety of ways to make it difficult\nfor technicians and automatic sensors to recognize their fingerprints, making\nit tedious for investigators to establish strong evidence against them in a\nforensic procedure. In this sense, deep learning comes out as a prime candidate\nto assist in the recognition of damaged fingerprints. In particular,\nconvolution algorithms. In this paper, we focus on the recognition of damaged\nfingerprints by Convolutional Long Short-Term Memory networks. We present the\narchitecture of our model and demonstrate its performance which exceeds 95%\naccuracy, 99% precision, and approaches 95% recall and 99% AUC.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 04:51:58 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Fattahi", "Jaouhar", ""], ["Mejri", "Mohamed", ""]]}, {"id": "2012.15049", "submitter": "Pratap Narra", "authors": "Rajeev Kumar Singh, Rohan Gorantla, Sai Giridhar Allada, Narra Pratap", "title": "SkiNet: A Deep Learning Solution for Skin Lesion Diagnosis with\n  Uncertainty Estimation and Explainability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin cancer is considered to be the most common human malignancy. Around 5\nmillion new cases of skin cancer are recorded in the United States annually.\nEarly identification and evaluation of skin lesions is of great clinical\nsignificance, but the disproportionate dermatologist-patient ratio poses\nsignificant problem in most developing nations. Therefore a deep learning based\narchitecture, known as SkiNet, is proposed with an objective to provide faster\nscreening solution and assistance to newly trained physicians in the clinical\ndiagnosis process. The main motive behind Skinet's design and development is to\nprovide a white box solution, addressing a critical problem of trust and\ninterpretability which is crucial for the wider adoption of Computer-aided\ndiagnosis systems by the medical practitioners. SkiNet is a two-stage pipeline\nwherein the lesion segmentation is followed by the lesion classification. In\nour SkiNet methodology, Monte Carlo dropout and test time augmentation\ntechniques have been employed to estimate epistemic and aleatoric uncertainty,\nwhile saliency-based methods are explored to provide post-hoc explanations of\nthe deep learning models. The publicly available dataset, ISIC-2018, is used to\nperform experimentation and ablation studies. The results establish the\nrobustness of the model on the traditional benchmarks while addressing the\nblack-box nature of such models to alleviate the skepticism of medical\npractitioners by incorporating transparency and confidence to the model's\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 05:39:57 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Singh", "Rajeev Kumar", ""], ["Gorantla", "Rohan", ""], ["Allada", "Sai Giridhar", ""], ["Pratap", "Narra", ""]]}, {"id": "2012.15052", "submitter": "Zhangkai Ni", "authors": "Zhangkai Ni, Wenhan Yang, Shiqi Wang, Lin Ma, and Sam Kwong", "title": "Unpaired Image Enhancement with Quality-Attention Generative Adversarial\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we aim to learn an unpaired image enhancement model, which can\nenrich low-quality images with the characteristics of high-quality images\nprovided by users. We propose a quality attention generative adversarial\nnetwork (QAGAN) trained on unpaired data based on the bidirectional Generative\nAdversarial Network (GAN) embedded with a quality attention module (QAM). The\nkey novelty of the proposed QAGAN lies in the injected QAM for the generator\nsuch that it learns domain-relevant quality attention directly from the two\ndomains. More specifically, the proposed QAM allows the generator to\neffectively select semantic-related characteristics from the spatial-wise and\nadaptively incorporate style-related attributes from the channel-wise,\nrespectively. Therefore, in our proposed QAGAN, not only discriminators but\nalso the generator can directly access both domains which significantly\nfacilitates the generator to learn the mapping function. Extensive experimental\nresults show that, compared with the state-of-the-art methods based on unpaired\nlearning, our proposed method achieves better performance in both objective and\nsubjective evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 05:57:20 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Ni", "Zhangkai", ""], ["Yang", "Wenhan", ""], ["Wang", "Shiqi", ""], ["Ma", "Lin", ""], ["Kwong", "Sam", ""]]}, {"id": "2012.15054", "submitter": "Tasfia Shermin", "authors": "Tasfia Shermin, Shyh Wei Teng, Ferdous Sohel, Manzur Murshed, Guojun\n  Lu", "title": "Bidirectional Mapping Coupled GAN for Generalized Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bidirectional mapping-based generalized zero-shot learning (GZSL) methods\nrely on the quality of synthesized features to recognize seen and unseen data.\nTherefore, learning a joint distribution of seen-unseen domains and preserving\ndomain distinction is crucial for these methods. However, existing methods only\nlearn the underlying distribution of seen data, although unseen class semantics\nare available in the GZSL problem setting. Most methods neglect retaining\ndomain distinction and use the learned distribution to recognize seen and\nunseen data. Consequently, they do not perform well. In this work, we utilize\nthe available unseen class semantics alongside seen class semantics and learn\njoint distribution through a strong visual-semantic coupling. We propose a\nbidirectional mapping coupled generative adversarial network (BMCoGAN) by\nextending the coupled generative adversarial network into a dual-domain\nlearning bidirectional mapping model. We further integrate a Wasserstein\ngenerative adversarial optimization to supervise the joint distribution\nlearning. We design a loss optimization for retaining domain distinctive\ninformation in the synthesized features and reducing bias towards seen classes,\nwhich pushes synthesized seen features towards real seen features and pulls\nsynthesized unseen features away from real seen features. We evaluate BMCoGAN\non benchmark datasets and demonstrate its superior performance against\ncontemporary methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 06:11:29 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 08:25:09 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Shermin", "Tasfia", ""], ["Teng", "Shyh Wei", ""], ["Sohel", "Ferdous", ""], ["Murshed", "Manzur", ""], ["Lu", "Guojun", ""]]}, {"id": "2012.15072", "submitter": "Peixuan Li", "authors": "Peixuan Li, Shun Su, Huaici Zhao", "title": "RTS3D: Real-time Stereo 3D Detection from 4D Feature-Consistency\n  Embedding Space for Autonomous Driving", "comments": "9 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the recent image-based 3D object detection methods using\nPseudo-LiDAR representation have shown great capabilities, a notable gap in\nefficiency and accuracy still exist compared with LiDAR-based methods. Besides,\nover-reliance on the stand-alone depth estimator, requiring a large number of\npixel-wise annotations in the training stage and more computation in the\ninferencing stage, limits the scaling application in the real world.\n  In this paper, we propose an efficient and accurate 3D object detection\nmethod from stereo images, named RTS3D. Different from the 3D occupancy space\nin the Pseudo-LiDAR similar methods, we design a novel 4D feature-consistent\nembedding (FCE) space as the intermediate representation of the 3D scene\nwithout depth supervision. The FCE space encodes the object's structural and\nsemantic information by exploring the multi-scale feature consistency warped\nfrom stereo pair. Furthermore, a semantic-guided RBF (Radial Basis Function)\nand a structure-aware attention module are devised to reduce the influence of\nFCE space noise without instance mask supervision. Experiments on the KITTI\nbenchmark show that RTS3D is the first true real-time system (FPS$>$24) for\nstereo image 3D detection meanwhile achieves $10\\%$ improvement in average\nprecision comparing with the previous state-of-the-art method. The code will be\navailable at https://github.com/Banconxuan/RTS3D\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 07:56:37 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Li", "Peixuan", ""], ["Su", "Shun", ""], ["Zhao", "Huaici", ""]]}, {"id": "2012.15086", "submitter": "Zhuosheng Zhang", "authors": "Zhuosheng Zhang, Haojie Yu, Hai Zhao, Rui Wang, Masao Utiyama", "title": "Accurate Word Representations with Universal Visual Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word representation is a fundamental component in neural language\nunderstanding models. Recently, pre-trained language models (PrLMs) offer a new\nperformant method of contextualized word representations by leveraging the\nsequence-level context for modeling. Although the PrLMs generally give more\naccurate contextualized word representations than non-contextualized models do,\nthey are still subject to a sequence of text contexts without diverse hints for\nword representation from multimodality. This paper thus proposes a visual\nrepresentation method to explicitly enhance conventional word embedding with\nmultiple-aspect senses from visual guidance. In detail, we build a small-scale\nword-image dictionary from a multimodal seed dataset where each word\ncorresponds to diverse related images. The texts and paired images are encoded\nin parallel, followed by an attention layer to integrate the multimodal\nrepresentations. We show that the method substantially improves the accuracy of\ndisambiguation. Experiments on 12 natural language understanding and machine\ntranslation tasks further verify the effectiveness and the generalization\ncapability of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 09:11:50 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Zhang", "Zhuosheng", ""], ["Yu", "Haojie", ""], ["Zhao", "Hai", ""], ["Wang", "Rui", ""], ["Utiyama", "Masao", ""]]}, {"id": "2012.15104", "submitter": "Wei He", "authors": "Wei He, Naoto Yokoya, and Xin Yuan", "title": "Fast Hyperspectral Image Recovery via Non-iterative Fusion of\n  Dual-Camera Compressive Hyperspectral Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coded aperture snapshot spectral imaging (CASSI) is a promising technique to\ncapture the three-dimensional hyperspectral image (HSI) using a single coded\ntwo-dimensional (2D) measurement, in which algorithms are used to perform the\ninverse problem. Due to the ill-posed nature, various regularizers have been\nexploited to reconstruct the 3D data from the 2D measurement. Unfortunately,\nthe accuracy and computational complexity are unsatisfied. One feasible\nsolution is to utilize additional information such as the RGB measurement in\nCASSI. Considering the combined CASSI and RGB measurement, in this paper, we\npropose a new fusion model for the HSI reconstruction. We investigate the\nspectral low-rank property of HSI composed of a spectral basis and spatial\ncoefficients. Specifically, the RGB measurement is utilized to estimate the\ncoefficients, meanwhile the CASSI measurement is adopted to provide the\northogonal spectral basis. We further propose a patch processing strategy to\nenhance the spectral low-rank property of HSI. The proposed model neither\nrequires non-local processing or iteration, nor the spectral sensing matrix of\nthe RGB detector. Extensive experiments on both simulated and real HSI dataset\ndemonstrate that our proposed method outperforms previous state-of-the-art not\nonly in quality but also speeds up the reconstruction more than 5000 times.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 10:29:32 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["He", "Wei", ""], ["Yokoya", "Naoto", ""], ["Yuan", "Xin", ""]]}, {"id": "2012.15124", "submitter": "Pengyu Zhang", "authors": "Yongri Piao and Zhengkun Rong and Shuang Xu and Miao Zhang and Huchuan\n  Lu", "title": "DUT-LFSaliency: Versatile Dataset and Light Field-to-RGB Saliency\n  Detection", "comments": "15 pages, 12 figures. If you have any questions, please contact\n  yrpiao@dlut.edu.cn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Light field data exhibit favorable characteristics conducive to saliency\ndetection. The success of learning-based light field saliency detection is\nheavily dependent on how a comprehensive dataset can be constructed for higher\ngeneralizability of models, how high dimensional light field data can be\neffectively exploited, and how a flexible model can be designed to achieve\nversatility for desktop computers and mobile devices. To answer these\nquestions, first we introduce a large-scale dataset to enable versatile\napplications for RGB, RGB-D and light field saliency detection, containing 102\nclasses and 4204 samples. Second, we present an asymmetrical two-stream model\nconsisting of the Focal stream and RGB stream. The Focal stream is designed to\nachieve higher performance on desktop computers and transfer focusness\nknowledge to the RGB stream, relying on two tailor-made modules. The RGB stream\nguarantees the flexibility and memory/computation efficiency on mobile devices\nthrough three distillation schemes. Experiments demonstrate that our Focal\nstream achieves state-of-the-arts performance. The RGB stream achieves Top-2\nF-measure on DUTLF-V2, which tremendously minimizes the model size by 83% and\nboosts FPS by 5 times, compared with the best performing method. Furthermore,\nour proposed distillation schemes are applicable to RGB saliency models,\nachieving impressive performance gains while ensuring flexibility.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 11:53:27 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Piao", "Yongri", ""], ["Rong", "Zhengkun", ""], ["Xu", "Shuang", ""], ["Zhang", "Miao", ""], ["Lu", "Huchuan", ""]]}, {"id": "2012.15136", "submitter": "Jun Ma", "authors": "Jun Ma, Ziwei Nie", "title": "Exploring Large Context for Cerebral Aneurysm Segmentation", "comments": "2nd place in MICCAI 2020 CADA challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Automated segmentation of aneurysms from 3D CT is important for the\ndiagnosis, monitoring, and treatment planning of the cerebral aneurysm disease.\nThis short paper briefly presents the main technique details of the aneurysm\nsegmentation method in the MICCAI 2020 CADA challenge. The main contribution is\nthat we configure the 3D U-Net with a large patch size, which can obtain the\nlarge context. Our method ranked second on the MICCAI 2020 CADA testing dataset\nwith an average Jaccard of 0.7593. Our code and trained models are publicly\navailable at \\url{https://github.com/JunMa11/CADA2020}.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 12:51:43 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Ma", "Jun", ""], ["Nie", "Ziwei", ""]]}, {"id": "2012.15159", "submitter": "Yuewwen Li", "authors": "Yuewen Li, Wenquan Feng, Shuchang Lyu, Qi Zhao, Xuliang Li", "title": "MM-FSOD: Meta and metric integrated few-shot object detection", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the object detection task, CNN (Convolutional neural networks) models\nalways need a large amount of annotated examples in the training process. To\nreduce the dependency of expensive annotations, few-shot object detection has\nbecome an increasing research focus. In this paper, we present an effective\nobject detection framework (MM-FSOD) that integrates metric learning and\nmeta-learning to tackle the few-shot object detection task. Our model is a\nclass-agnostic detection model that can accurately recognize new categories,\nwhich are not appearing in training samples. Specifically, to fast learn the\nfeatures of new categories without a fine-tuning process, we propose a\nmeta-representation module (MR module) to learn intra-class mean prototypes. MR\nmodule is trained with a meta-learning method to obtain the ability to\nreconstruct high-level features. To further conduct similarity of features\nbetween support prototype with query RoIs features, we propose a Pearson metric\nmodule (PR module) which serves as a classifier. Compared to the previous\ncommonly used metric method, cosine distance metric. PR module enables the\nmodel to align features into discriminative embedding space. We conduct\nextensive experiments on benchmark datasets FSOD, MS COCO, and PASCAL VOC to\ndemonstrate the feasibility and efficiency of our model. Comparing with the\nprevious method, MM-FSOD achieves state-of-the-art (SOTA) results.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 14:02:52 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Li", "Yuewen", ""], ["Feng", "Wenquan", ""], ["Lyu", "Shuchang", ""], ["Zhao", "Qi", ""], ["Li", "Xuliang", ""]]}, {"id": "2012.15175", "submitter": "Zhengxiong Luo", "authors": "Zhengxiong Luo, Zhicheng Wang, Yan Huang, Tieniu Tan, Erjin Zhou", "title": "Rethinking the Heatmap Regression for Bottom-up Human Pose Estimation", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Heatmap regression has become the most prevalent choice for nowadays human\npose estimation methods. The ground-truth heatmaps are usually constructed via\ncovering all skeletal keypoints by 2D gaussian kernels. The standard deviations\nof these kernels are fixed. However, for bottom-up methods, which need to\nhandle a large variance of human scales and labeling ambiguities, the current\npractice seems unreasonable. To better cope with these problems, we propose the\nscale-adaptive heatmap regression (SAHR) method, which can adaptively adjust\nthe standard deviation for each keypoint. In this way, SAHR is more tolerant of\nvarious human scales and labeling ambiguities. However, SAHR may aggravate the\nimbalance between fore-background samples, which potentially hurts the\nimprovement of SAHR. Thus, we further introduce the weight-adaptive heatmap\nregression (WAHR) to help balance the fore-background samples. Extensive\nexperiments show that SAHR together with WAHR largely improves the accuracy of\nbottom-up human pose estimation. As a result, we finally outperform the\nstate-of-the-art model by +1.5AP and achieve 72.0AP on COCO test-dev2017, which\nis com-arable with the performances of most top-down methods. Source codes are\navailable at https://github.com/greatlog/SWAHR-HumanPose.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 14:39:41 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 06:52:40 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 13:42:49 GMT"}, {"version": "v4", "created": "Thu, 25 Mar 2021 05:38:37 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Luo", "Zhengxiong", ""], ["Wang", "Zhicheng", ""], ["Huang", "Yan", ""], ["Tan", "Tieniu", ""], ["Zhou", "Erjin", ""]]}, {"id": "2012.15183", "submitter": "Krishna Kanth Nakka", "authors": "Krishna Kanth Nakka and Mathieu Salzmann", "title": "Temporally-Transferable Perturbations: Efficient, One-Shot Adversarial\n  Attacks for Online Visual Object Trackers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the trackers based on Siamese networks have emerged as\nhighly effective and efficient for visual object tracking (VOT). While these\nmethods were shown to be vulnerable to adversarial attacks, as most deep\nnetworks for visual recognition tasks, the existing attacks for VOT trackers\nall require perturbing the search region of every input frame to be effective,\nwhich comes at a non-negligible cost, considering that VOT is a real-time task.\nIn this paper, we propose a framework to generate a single temporally\ntransferable adversarial perturbation from the object template image only. This\nperturbation can then be added to every search image, which comes at virtually\nno cost, and still, successfully fool the tracker. Our experiments evidence\nthat our approach outperforms the state-of-the-art attacks on the standard VOT\nbenchmarks in the untargeted scenario. Furthermore, we show that our formalism\nnaturally extends to targeted attacks that force the tracker to follow any\ngiven trajectory by precomputing diverse directional perturbations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 15:05:53 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Nakka", "Krishna Kanth", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "2012.15244", "submitter": "Debesh Jha", "authors": "Debesh Jha, Steven A. Hicks, Krister Emanuelsen, H{\\aa}vard Johansen,\n  Dag Johansen, Thomas de Lange, Michael A. Riegler, P{\\aa}l Halvorsen", "title": "Medico Multimedia Task at MediaEval 2020: Automatic Polyp Segmentation", "comments": "MediaEval 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Colorectal cancer is the third most common cause of cancer worldwide.\nAccording to Global cancer statistics 2018, the incidence of colorectal cancer\nis increasing in both developing and developed countries. Early detection of\ncolon anomalies such as polyps is important for cancer prevention, and\nautomatic polyp segmentation can play a crucial role for this. Regardless of\nthe recent advancement in early detection and treatment options, the estimated\npolyp miss rate is still around 20\\%. Support via an automated computer-aided\ndiagnosis system could be one of the potential solutions for the overlooked\npolyps. Such detection systems can help low-cost design solutions and save\ndoctors time, which they could for example use to perform more patient\nexaminations. In this paper, we introduce the 2020 Medico challenge, provide\nsome information on related work and the dataset, describe the task and\nevaluation metrics, and discuss the necessity of organizing the Medico\nchallenge.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 17:47:38 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Jha", "Debesh", ""], ["Hicks", "Steven A.", ""], ["Emanuelsen", "Krister", ""], ["Johansen", "H\u00e5vard", ""], ["Johansen", "Dag", ""], ["de Lange", "Thomas", ""], ["Riegler", "Michael A.", ""], ["Halvorsen", "P\u00e5l", ""]]}, {"id": "2012.15245", "submitter": "Debesh Jha", "authors": "Nikhil Kumar Tomar, Debesh Jha, Sharib Ali, H{\\aa}vard D. Johansen,\n  Dag Johansen, Michael A. Riegler, and P{\\aa}l Halvorsen", "title": "DDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Colonoscopy is the gold standard for examination and detection of colorectal\npolyps. Localization and delineation of polyps can play a vital role in\ntreatment (e.g., surgical planning) and prognostic decision making. Polyp\nsegmentation can provide detailed boundary information for clinical analysis.\nConvolutional neural networks have improved the performance in colonoscopy.\nHowever, polyps usually possess various challenges, such as intra-and\ninter-class variation and noise. While manual labeling for polyp assessment\nrequires time from experts and is prone to human error (e.g., missed lesions),\nan automated, accurate, and fast segmentation can improve the quality of\ndelineated lesion boundaries and reduce missed rate. The Endotect challenge\nprovides an opportunity to benchmark computer vision methods by training on the\npublicly available Hyperkvasir and testing on a separate unseen dataset. In\nthis paper, we propose a novel architecture called ``DDANet'' based on a dual\ndecoder attention network. Our experiments demonstrate that the model trained\non the Kvasir-SEG dataset and tested on an unseen dataset achieves a dice\ncoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of\n0.8577, demonstrating the generalization ability of our model.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 17:52:35 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Tomar", "Nikhil Kumar", ""], ["Jha", "Debesh", ""], ["Ali", "Sharib", ""], ["Johansen", "H\u00e5vard D.", ""], ["Johansen", "Dag", ""], ["Riegler", "Michael A.", ""], ["Halvorsen", "P\u00e5l", ""]]}, {"id": "2012.15247", "submitter": "Debesh Jha", "authors": "Saruar Alam, Nikhil Kumar Tomar, Aarati Thakur, Debesh Jha, Ashish\n  Rauniyar", "title": "Automatic Polyp Segmentation using U-Net-ResNet50", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Polyps are the predecessors to colorectal cancer which is considered as one\nof the leading causes of cancer-related deaths worldwide. Colonoscopy is the\nstandard procedure for the identification, localization, and removal of\ncolorectal polyps. Due to variability in shape, size, and surrounding tissue\nsimilarity, colorectal polyps are often missed by the clinicians during\ncolonoscopy. With the use of an automatic, accurate, and fast polyp\nsegmentation method during the colonoscopy, many colorectal polyps can be\neasily detected and removed. The ``Medico automatic polyp segmentation\nchallenge'' provides an opportunity to study polyp segmentation and build an\nefficient and accurate segmentation algorithm. We use the U-Net with\npre-trained ResNet50 as the encoder for the polyp segmentation. The model is\ntrained on Kvasir-SEG dataset provided for the challenge and tested on the\norganizer's dataset and achieves a dice coefficient of 0.8154, Jaccard of\n0.7396, recall of 0.8533, precision of 0.8532, accuracy of 0.9506, and F2 score\nof 0.8272, demonstrating the generalization ability of our model.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 17:59:18 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Alam", "Saruar", ""], ["Tomar", "Nikhil Kumar", ""], ["Thakur", "Aarati", ""], ["Jha", "Debesh", ""], ["Rauniyar", "Ashish", ""]]}, {"id": "2012.15279", "submitter": "Shri Prakash Dwivedi", "authors": "Shri Prakash Dwivedi", "title": "Some Algorithms on Exact, Approximate and Error-Tolerant Graph Matching", "comments": "Ph.D. Thesis, Indian Institute of Technology (BHU), Varanasi, July\n  2019. (Adviser: Dr. R.S. Singh)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The graph is one of the most widely used mathematical structures in\nengineering and science because of its representational power and inherent\nability to demonstrate the relationship between objects. The objective of this\nwork is to introduce the novel graph matching techniques using the\nrepresentational power of the graph and apply it to structural pattern\nrecognition applications. We present an extensive survey of various exact and\ninexact graph matching techniques. Graph matching using the concept of\nhomeomorphism is presented. A category of graph matching algorithms is\npresented, which reduces the graph size by removing the less important nodes\nusing some measure of relevance. We present an approach to error-tolerant graph\nmatching using node contraction where the given graph is transformed into\nanother graph by contracting smaller degree nodes. We use this scheme to extend\nthe notion of graph edit distance, which can be used as a trade-off between\nexecution time and accuracy. We describe an approach to graph matching by\nutilizing the various node centrality information, which reduces the graph size\nby removing a fraction of nodes from both graphs based on a given centrality\nmeasure. The graph matching problem is inherently linked to the geometry and\ntopology of graphs. We introduce a novel approach to measure graph similarity\nusing geometric graphs. We define the vertex distance between two geometric\ngraphs using the position of their vertices and show it to be a metric over the\nset of all graphs with vertices only. We define edge distance between two\ngraphs based on the angular orientation, length and position of the edges. Then\nwe combine the notion of vertex distance and edge distance to define the graph\ndistance between two geometric graphs and show it to be a metric. Finally, we\nuse the proposed graph similarity framework to perform exact and error-tolerant\ngraph matching.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 18:51:06 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Dwivedi", "Shri Prakash", ""]]}, {"id": "2012.15294", "submitter": "Veronica Vilaplana", "authors": "Laura Mora Ballestar and Veronica Vilaplana", "title": "MRI brain tumor segmentation and uncertainty estimation using 3D-UNet\n  architectures", "comments": "Extended paper. BrainLes Workshop, MICCAI 2020. arXiv admin note:\n  substantial text overlap with arXiv:2009.12188", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automation of brain tumor segmentation in 3D magnetic resonance images (MRIs)\nis key to assess the diagnostic and treatment of the disease. In recent years,\nconvolutional neural networks (CNNs) have shown improved results in the task.\nHowever, high memory consumption is still a problem in 3D-CNNs. Moreover, most\nmethods do not include uncertainty information, which is especially critical in\nmedical diagnosis. This work studies 3D encoder-decoder architectures trained\nwith patch-based techniques to reduce memory consumption and decrease the\neffect of unbalanced data. The different trained models are then used to create\nan ensemble that leverages the properties of each model, thus increasing the\nperformance. We also introduce voxel-wise uncertainty information, both\nepistemic and aleatoric using test-time dropout (TTD) and data-augmentation\n(TTA) respectively. In addition, a hybrid approach is proposed that helps\nincrease the accuracy of the segmentation. The model and uncertainty estimation\nmeasurements proposed in this work have been used in the BraTS'20 Challenge for\ntask 1 and 3 regarding tumor segmentation and uncertainty estimation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 19:28:53 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Ballestar", "Laura Mora", ""], ["Vilaplana", "Veronica", ""]]}, {"id": "2012.15318", "submitter": "Yong Xia", "authors": "Haozhe Jia, Weidong Cai, Heng Huang, Yong Xia", "title": "H2NF-Net for Brain Tumor Segmentation using Multimodal MR Imaging: 2nd\n  Place Solution to BraTS Challenge 2020 Segmentation Task", "comments": "Accepted by MICCAI BrainLesion Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a Hybrid High-resolution and Non-local Feature\nNetwork (H2NF-Net) to segment brain tumor in multimodal MR images. Our H2NF-Net\nuses the single and cascaded HNF-Nets to segment different brain tumor\nsub-regions and combines the predictions together as the final segmentation. We\ntrained and evaluated our model on the Multimodal Brain Tumor Segmentation\nChallenge (BraTS) 2020 dataset. The results on the test set show that the\ncombination of the single and cascaded models achieved average Dice scores of\n0.78751, 0.91290, and 0.85461, as well as Hausdorff distances ($95\\%$) of\n26.57525, 4.18426, and 4.97162 for the enhancing tumor, whole tumor, and tumor\ncore, respectively. Our method won the second place in the BraTS 2020 challenge\nsegmentation task out of nearly 80 participants.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 20:44:55 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Jia", "Haozhe", ""], ["Cai", "Weidong", ""], ["Huang", "Heng", ""], ["Xia", "Yong", ""]]}, {"id": "2012.15343", "submitter": "Loic Peter", "authors": "Loic Peter, Marcel Tella-Amo, Dzhoshkun Ismail Shakir, Jan Deprest,\n  Sebastien Ourselin, Juan Eugenio Iglesias, Tom Vercauteren", "title": "Active Annotation of Informative Overlapping Frames in Video Mosaicking\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video mosaicking requires the registration of overlapping frames located at\ndistant timepoints in the sequence to ensure global consistency of the\nreconstructed scene. However, fully automated registration of such long-range\npairs is (i) challenging when the registration of images itself is difficult;\nand (ii) computationally expensive for long sequences due to the large number\nof candidate pairs for registration. In this paper, we introduce an efficient\nframework for the active annotation of long-range pairwise correspondences in a\nsequence. Our framework suggests pairs of images that are sought to be\ninformative to an oracle agent (e.g., a human user, or a reliable matching\nalgorithm) who provides visual correspondences on each suggested pair.\nInformative pairs are retrieved according to an iterative strategy based on a\nprincipled annotation reward coupled with two complementary and online\nadaptable models of frame overlap. In addition to the efficient construction of\na mosaic, our framework provides, as a by-product, ground truth landmark\ncorrespondences which can be used for evaluation or learning purposes. We\nevaluate our approach in both automated and interactive scenarios via\nexperiments on synthetic sequences, on a publicly available dataset for aerial\nimaging and on a clinical dataset for placenta mosaicking during fetal surgery.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 22:19:19 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Peter", "Loic", ""], ["Tella-Amo", "Marcel", ""], ["Shakir", "Dzhoshkun Ismail", ""], ["Deprest", "Jan", ""], ["Ourselin", "Sebastien", ""], ["Iglesias", "Juan Eugenio", ""], ["Vercauteren", "Tom", ""]]}, {"id": "2012.15359", "submitter": "Yirui Wang", "authors": "Yirui Wang, Kang Zheng, Chi-Tung Chang, Xiao-Yun Zhou, Zhilin Zheng,\n  Lingyun Huang, Jing Xiao, Le Lu, Chien-Hung Liao, Shun Miao", "title": "Knowledge Distillation with Adaptive Asymmetric Label Sharpening for\n  Semi-supervised Fracture Detection in Chest X-rays", "comments": "Accepted to IPMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exploiting available medical records to train high performance computer-aided\ndiagnosis (CAD) models via the semi-supervised learning (SSL) setting is\nemerging to tackle the prohibitively high labor costs involved in large-scale\nmedical image annotations. Despite the extensive attentions received on SSL,\nprevious methods failed to 1) account for the low disease prevalence in medical\nrecords and 2) utilize the image-level diagnosis indicated from the medical\nrecords. Both issues are unique to SSL for CAD models. In this work, we propose\na new knowledge distillation method that effectively exploits large-scale\nimage-level labels extracted from the medical records, augmented with limited\nexpert annotated region-level labels, to train a rib and clavicle fracture CAD\nmodel for chest X-ray (CXR). Our method leverages the teacher-student model\nparadigm and features a novel adaptive asymmetric label sharpening (AALS)\nalgorithm to address the label imbalance problem that specially exists in\nmedical domain. Our approach is extensively evaluated on all CXR (N = 65,845)\nfrom the trauma registry of anonymous hospital over a period of 9 years\n(2008-2016), on the most common rib and clavicle fractures. The experiment\nresults demonstrate that our method achieves the state-of-the-art fracture\ndetection performance, i.e., an area under receiver operating characteristic\ncurve (AUROC) of 0.9318 and a free-response receiver operating characteristic\n(FROC) score of 0.8914 on the rib fractures, significantly outperforming\nprevious approaches by an AUROC gap of 1.63% and an FROC improvement by 3.74%.\nConsistent performance gains are also observed for clavicle fracture detection.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 23:12:40 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 00:48:04 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Wang", "Yirui", ""], ["Zheng", "Kang", ""], ["Chang", "Chi-Tung", ""], ["Zhou", "Xiao-Yun", ""], ["Zheng", "Zhilin", ""], ["Huang", "Lingyun", ""], ["Xiao", "Jing", ""], ["Lu", "Le", ""], ["Liao", "Chien-Hung", ""], ["Miao", "Shun", ""]]}, {"id": "2012.15370", "submitter": "Baris Gecer", "authors": "Baris Gecer, Jiankang Deng, Stefanos Zafeiriou", "title": "OSTeC: One-Shot Texture Completion", "comments": "Project page: https://github.com/barisgecer/OSTeC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The last few years have witnessed the great success of non-linear generative\nmodels in synthesizing high-quality photorealistic face images. Many recent 3D\nfacial texture reconstruction and pose manipulation from a single image\napproaches still rely on large and clean face datasets to train image-to-image\nGenerative Adversarial Networks (GANs). Yet the collection of such a large\nscale high-resolution 3D texture dataset is still very costly and difficult to\nmaintain age/ethnicity balance. Moreover, regression-based approaches suffer\nfrom generalization to the in-the-wild conditions and are unable to fine-tune\nto a target-image. In this work, we propose an unsupervised approach for\none-shot 3D facial texture completion that does not require large-scale texture\ndatasets, but rather harnesses the knowledge stored in 2D face generators. The\nproposed approach rotates an input image in 3D and fill-in the unseen regions\nby reconstructing the rotated image in a 2D face generator, based on the\nvisible parts. Finally, we stitch the most visible textures at different angles\nin the UV image-plane. Further, we frontalize the target image by projecting\nthe completed texture into the generator. The qualitative and quantitative\nexperiments demonstrate that the completed UV textures and frontalized images\nare of high quality, resembles the original identity, can be used to train a\ntexture GAN model for 3DMM fitting and improve pose-invariant face recognition.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 23:53:26 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Gecer", "Baris", ""], ["Deng", "Jiankang", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2012.15373", "submitter": "Stephen Tian", "authors": "Stephen Tian, Suraj Nair, Frederik Ebert, Sudeep Dasari, Benjamin\n  Eysenbach, Chelsea Finn, Sergey Levine", "title": "Model-Based Visual Planning with Self-Supervised Functional Distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A generalist robot must be able to complete a variety of tasks in its\nenvironment. One appealing way to specify each task is in terms of a goal\nobservation. However, learning goal-reaching policies with reinforcement\nlearning remains a challenging problem, particularly when hand-engineered\nreward functions are not available. Learned dynamics models are a promising\napproach for learning about the environment without rewards or task-directed\ndata, but planning to reach goals with such a model requires a notion of\nfunctional similarity between observations and goal states. We present a\nself-supervised method for model-based visual goal reaching, which uses both a\nvisual dynamics model as well as a dynamical distance function learned using\nmodel-free reinforcement learning. Our approach learns entirely using offline,\nunlabeled data, making it practical to scale to large and diverse datasets. In\nour experiments, we find that our method can successfully learn models that\nperform a variety of tasks at test-time, moving objects amid distractors with a\nsimulated robotic arm and even learning to open and close a drawer using a\nreal-world robot. In comparisons, we find that this approach substantially\noutperforms both model-free and model-based prior methods. Videos and\nvisualizations are available here: http://sites.google.com/berkeley.edu/mbold.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 23:59:09 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Tian", "Stephen", ""], ["Nair", "Suraj", ""], ["Ebert", "Frederik", ""], ["Dasari", "Sudeep", ""], ["Eysenbach", "Benjamin", ""], ["Finn", "Chelsea", ""], ["Levine", "Sergey", ""]]}, {"id": "2012.15376", "submitter": "Sascha Saralajew", "authors": "Lars Ohnemus and Lukas Ewecker and Ebubekir Asan and Stefan Roos and\n  Simon Isele and Jakob Ketterer and Leopold M\\\"uller and Sascha Saralajew", "title": "Provident Vehicle Detection at Night: The PVDN Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For advanced driver assistance systems, it is crucial to have information\nabout oncoming vehicles as early as possible. At night, this task is especially\ndifficult due to poor lighting conditions. For that, during nighttime, every\nvehicle uses headlamps to improve sight and therefore ensure safe driving. As\nhumans, we intuitively assume oncoming vehicles before the vehicles are\nactually physically visible by detecting light reflections caused by their\nheadlamps. In this paper, we present a novel dataset containing 59746 annotated\ngrayscale images out of 346 different scenes in a rural environment at night.\nIn these images, all oncoming vehicles, their corresponding light objects\n(e.g., headlamps), and their respective light reflections (e.g., light\nreflections on guardrails) are labeled. This is accompanied by an in-depth\nanalysis of the dataset characteristics. With that, we are providing the first\nopen-source dataset with comprehensive ground truth data to enable research\ninto new methods of detecting oncoming vehicles based on the light reflections\nthey cause, long before they are directly visible. We consider this as an\nessential step to further close the performance gap between current advanced\ndriver assistance systems and human behavior.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 00:06:26 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 22:00:48 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Ohnemus", "Lars", ""], ["Ewecker", "Lukas", ""], ["Asan", "Ebubekir", ""], ["Roos", "Stefan", ""], ["Isele", "Simon", ""], ["Ketterer", "Jakob", ""], ["M\u00fcller", "Leopold", ""], ["Saralajew", "Sascha", ""]]}, {"id": "2012.15378", "submitter": "Emad Barsoum", "authors": "Emad Barsoum, John Kender, Zicheng Liu", "title": "3D Human motion anticipation and classification", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.09561", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion prediction and understanding is a challenging problem. Due to\nthe complex dynamic of human motion and the non-deterministic aspect of future\nprediction. We propose a novel sequence-to-sequence model for human motion\nprediction and feature learning, trained with a modified version of generative\nadversarial network, with a custom loss function that takes inspiration from\nhuman motion animation and can control the variation between multiple predicted\nmotion from the same input poses.\n  Our model learns to predict multiple future sequences of human poses from the\nsame input sequence. We show that the discriminator learns general presentation\nof human motion by using the learned feature in action recognition task.\nFurthermore, to quantify the quality of the non-deterministic predictions, we\nsimultaneously train a motion-quality-assessment network that learns the\nprobability that a given sequence of poses is a real human motion or not.\n  We test our model on two of the largest human pose datasets: NTURGB-D and\nHuman3.6M. We train on both single and multiple action types. Its predictive\npower for motion estimation is demonstrated by generating multiple plausible\nfutures from the same input and show the effect of each of the loss functions.\nFurthermore, we show that it takes less than half the number of epochs to train\nan activity recognition network by using the feature learned from the\ndiscriminator.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 00:19:39 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Barsoum", "Emad", ""], ["Kender", "John", ""], ["Liu", "Zicheng", ""]]}, {"id": "2012.15386", "submitter": "Yuhang Wu", "authors": "Yuhang Wu, Sunpreet S. Arora, Yanhong Wu, Hao Yang", "title": "Beating Attackers At Their Own Games: Adversarial Example Detection\n  Using Adversarial Gradient Directions", "comments": "Accepted at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial examples are input examples that are specifically crafted to\ndeceive machine learning classifiers. State-of-the-art adversarial example\ndetection methods characterize an input example as adversarial either by\nquantifying the magnitude of feature variations under multiple perturbations or\nby measuring its distance from estimated benign example distribution. Instead\nof using such metrics, the proposed method is based on the observation that the\ndirections of adversarial gradients when crafting (new) adversarial examples\nplay a key role in characterizing the adversarial space. Compared to detection\nmethods that use multiple perturbations, the proposed method is efficient as it\nonly applies a single random perturbation on the input example. Experiments\nconducted on two different databases, CIFAR-10 and ImageNet, show that the\nproposed detection method achieves, respectively, 97.9% and 98.6% AUC-ROC (on\naverage) on five different adversarial attacks, and outperforms multiple\nstate-of-the-art detection methods. Results demonstrate the effectiveness of\nusing adversarial gradient directions for adversarial example detection.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 01:12:24 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Wu", "Yuhang", ""], ["Arora", "Sunpreet S.", ""], ["Wu", "Yanhong", ""], ["Yang", "Hao", ""]]}, {"id": "2012.15397", "submitter": "Hajar Emami Gohari", "authors": "Hajar Emami, Qiong Liu, Ming Dong", "title": "FREA-Unet: Frequency-aware U-net for Modality Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Positron emission tomography (PET) imaging has been widely used in\ndiagnosis of number of diseases, it has costly acquisition process which\ninvolves radiation exposure to patients. However, magnetic resonance imaging\n(MRI) is a safer imaging modality that does not involve patient's exposure to\nradiation. Therefore, a need exists for an efficient and automated PET image\ngeneration from MRI data. In this paper, we propose a new frequency-aware\nattention U-net for generating synthetic PET images. Specifically, we\nincorporate attention mechanism into different U-net layers responsible for\nestimating low/high frequency scales of the image. Our frequency-aware\nattention Unet computes the attention scores for feature maps in low/high\nfrequency layers and use it to help the model focus more on the most important\nregions, leading to more realistic output images. Experimental results on 30\nsubjects from Alzheimers Disease Neuroimaging Initiative (ADNI) dataset\ndemonstrate good performance of the proposed model in PET image synthesis that\nachieved superior performance, both qualitative and quantitative, over current\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 01:58:44 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Emami", "Hajar", ""], ["Liu", "Qiong", ""], ["Dong", "Ming", ""]]}, {"id": "2012.15413", "submitter": "Chiranjibi Sitaula", "authors": "Chiranjibi Sitaula and Sunil Aryal", "title": "New Bag of Deep Visual Words based features to classify chest x-ray\n  images for COVID-19 diagnosis", "comments": "Submitted to Health Information Science and Systems (Springer) for\n  review", "journal-ref": "Health Information Science and Systems (Springer), 2021", "doi": "10.1007/s13755-021-00152-w", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because the infection by Severe Acute Respiratory Syndrome Coronavirus 2\n(COVID-19) causes the pneumonia-like effect in the lungs, the examination of\nchest x-rays can help to diagnose the diseases. For automatic analysis of\nimages, they are represented in machines by a set of semantic features. Deep\nLearning (DL) models are widely used to extract features from images. General\ndeep features may not be appropriate to represent chest x-rays as they have a\nfew semantic regions. Though the Bag of Visual Words (BoVW) based features are\nshown to be more appropriate for x-ray type of images, existing BoVW features\nmay not capture enough information to differentiate COVID-19 infection from\nother pneumonia-related infections. In this paper, we propose a new BoVW method\nover deep features, called Bag of Deep Visual Words (BoDVW), by removing the\nfeature map normalization step and adding deep features normalization step on\nthe raw feature maps. This helps to preserve the semantics of each feature map\nthat may have important clues to differentiate COVID-19 from pneumonia. We\nevaluate the effectiveness of our proposed BoDVW features in chest x-rays\nclassification using Support Vector Machine (SVM) to diagnose COVID-19. Our\nresults on a publicly available COVID-19 x-ray dataset reveal that our features\nproduce stable and prominent classification accuracy, particularly\ndifferentiating COVID-19 infection from other pneumonia, in shorter computation\ntime compared to the state-of-the-art methods. Thus, our method could be a very\nuseful tool for quick diagnosis of COVID-19 patients on a large scale.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 02:57:02 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 05:03:12 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Sitaula", "Chiranjibi", ""], ["Aryal", "Sunil", ""]]}, {"id": "2012.15432", "submitter": "Hui Feng Prof.", "authors": "Hui Feng and Jundong Guo and Sam Shuzhi Ge", "title": "SharpGAN: Receptive Field Block Net for Dynamic Scene Deblurring", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When sailing at sea, the smart ship will inevitably produce swaying motion\ndue to the action of wind, wave and current, which makes the image collected by\nthe visual sensor appear motion blur. This will have an adverse effect on the\nobject detection algorithm based on the vision sensor, thereby affect the\nnavigation safety of the smart ship. In order to remove the motion blur in the\nimages during the navigation of the smart ship, we propose SharpGAN, a new\nimage deblurring method based on the generative adversarial network. First of\nall, the Receptive Field Block Net (RFBNet) is introduced to the deblurring\nnetwork to strengthen the network's ability to extract the features of blurred\nimage. Secondly, we propose a feature loss that combines different levels of\nimage features to guide the network to perform higher-quality deblurring and\nimprove the feature similarity between the restored images and the sharp image.\nFinally, we propose to use the lightweight RFB-s module to improve the\nreal-time performance of deblurring network. Compared with the existing\ndeblurring methods on large-scale real sea image datasets and large-scale\ndeblurring datasets, the proposed method not only has better deblurring\nperformance in visual perception and quantitative criteria, but also has higher\ndeblurring efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 03:57:12 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Feng", "Hui", ""], ["Guo", "Jundong", ""], ["Ge", "Sam Shuzhi", ""]]}, {"id": "2012.15439", "submitter": "Can Peng", "authors": "Can Peng, Kun Zhao, Sam Maksoud, Meng Li, Brian C. Lovell", "title": "SID: Incremental Learning for Anchor-Free Object Detection via Selective\n  and Inter-Related Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental learning requires a model to continually learn new tasks from\nstreaming data. However, traditional fine-tuning of a well-trained deep neural\nnetwork on a new task will dramatically degrade performance on the old task --\na problem known as catastrophic forgetting. In this paper, we address this\nissue in the context of anchor-free object detection, which is a new trend in\ncomputer vision as it is simple, fast, and flexible. Simply adapting current\nincremental learning strategies fails on these anchor-free detectors due to\nlack of consideration of their specific model structures. To deal with the\nchallenges of incremental learning on anchor-free object detectors, we propose\na novel incremental learning paradigm called Selective and Inter-related\nDistillation (SID). In addition, a novel evaluation metric is proposed to\nbetter assess the performance of detectors under incremental learning\nconditions. By selective distilling at the proper locations and further\ntransferring additional instance relation knowledge, our method demonstrates\nsignificant advantages on the benchmark datasets PASCAL VOC and COCO.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 04:12:06 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Peng", "Can", ""], ["Zhao", "Kun", ""], ["Maksoud", "Sam", ""], ["Li", "Meng", ""], ["Lovell", "Brian C.", ""]]}, {"id": "2012.15442", "submitter": "Qian Zhang", "authors": "Yixuan Sun, Chengyao Li, Qian Zhang, Aimin Zhou and Guixu Zhang", "title": "Survey of the Detection and Classification of Pulmonary Lesions via CT\n  and X-Ray", "comments": "19 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the prevalence of several pulmonary diseases, especially the\ncoronavirus disease 2019 (COVID-19) pandemic, has attracted worldwide\nattention. These diseases can be effectively diagnosed and treated with the\nhelp of lung imaging. With the development of deep learning technology and the\nemergence of many public medical image datasets, the diagnosis of lung diseases\nvia medical imaging has been further improved. This article reviews pulmonary\nCT and X-ray image detection and classification in the last decade. It also\nprovides an overview of the detection of lung nodules, pneumonia, and other\ncommon lung lesions based on the imaging characteristics of various lesions.\nFurthermore, this review introduces 26 commonly used public medical image\ndatasets, summarizes the latest technology, and discusses current challenges\nand future research directions.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 04:29:50 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Sun", "Yixuan", ""], ["Li", "Chengyao", ""], ["Zhang", "Qian", ""], ["Zhou", "Aimin", ""], ["Zhang", "Guixu", ""]]}, {"id": "2012.15454", "submitter": "Wei-Ning Hsu", "authors": "Wei-Ning Hsu, David Harwath, Christopher Song, James Glass", "title": "Text-Free Image-to-Speech Synthesis Using Learned Segmental Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present the first model for directly synthesizing fluent,\nnatural-sounding spoken audio captions for images that does not require natural\nlanguage text as an intermediate representation or source of supervision.\nInstead, we connect the image captioning module and the speech synthesis module\nwith a set of discrete, sub-word speech units that are discovered with a\nself-supervised visual grounding task. We conduct experiments on the Flickr8k\nspoken caption dataset in addition to a novel corpus of spoken audio captions\ncollected for the popular MSCOCO dataset, demonstrating that our generated\ncaptions also capture diverse visual semantics of the images they describe. We\ninvestigate several different intermediate speech representations, and\nempirically find that the representation must satisfy several important\nproperties to serve as drop-in replacements for text.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 05:28:38 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Hsu", "Wei-Ning", ""], ["Harwath", "David", ""], ["Song", "Christopher", ""], ["Glass", "James", ""]]}, {"id": "2012.15460", "submitter": "Peize Sun", "authors": "Peize Sun, Jinkun Cao, Yi Jiang, Rufeng Zhang, Enze Xie, Zehuan Yuan,\n  Changhu Wang, Ping Luo", "title": "TransTrack: Multiple Object Tracking with Transformer", "comments": "update MOT17 and MOT20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose TransTrack, a simple but efficient scheme to solve\nthe multiple object tracking problems. TransTrack leverages the transformer\narchitecture, which is an attention-based query-key mechanism. It applies\nobject features from the previous frame as a query of the current frame and\nintroduces a set of learned object queries to enable detecting new-coming\nobjects. It builds up a novel joint-detection-and-tracking paradigm by\naccomplishing object detection and object association in a single shot,\nsimplifying complicated multi-step settings in tracking-by-detection methods.\nOn MOT17 and MOT20 benchmark, TransTrack achieves 74.5\\% and 64.5\\% MOTA,\nrespectively, competitive to the state-of-the-art methods. We expect TransTrack\nto provide a novel perspective for multiple object tracking. The code is\navailable at: \\url{https://github.com/PeizeSun/TransTrack}.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 06:03:00 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 15:58:37 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Sun", "Peize", ""], ["Cao", "Jinkun", ""], ["Jiang", "Yi", ""], ["Zhang", "Rufeng", ""], ["Xie", "Enze", ""], ["Yuan", "Zehuan", ""], ["Wang", "Changhu", ""], ["Luo", "Ping", ""]]}, {"id": "2012.15463", "submitter": "Mohammad Akbari", "authors": "Mohammad Akbari, Jie Liang, Jingning Han, Chengjie Tu", "title": "Learned Multi-Resolution Variable-Rate Image Compression with\n  Octave-based Residual Blocks", "comments": "10 pages, 9 figures, 1 table; accepted to IEEE Transactions on\n  Multimedia 2020. arXiv admin note: substantial text overlap with\n  arXiv:1912.05688", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep learning-based image compression has shown the potential to\noutperform traditional codecs. However, most existing methods train multiple\nnetworks for multiple bit rates, which increase the implementation complexity.\nIn this paper, we propose a new variable-rate image compression framework,\nwhich employs generalized octave convolutions (GoConv) and generalized octave\ntransposed-convolutions (GoTConv) with built-in generalized divisive\nnormalization (GDN) and inverse GDN (IGDN) layers. Novel GoConv- and\nGoTConv-based residual blocks are also developed in the encoder and decoder\nnetworks. Our scheme also uses a stochastic rounding-based scalar quantization.\nTo further improve the performance, we encode the residual between the input\nand the reconstructed image from the decoder network as an enhancement layer.\nTo enable a single model to operate with different bit rates and to learn\nmulti-rate image features, a new objective function is introduced. Experimental\nresults show that the proposed framework trained with variable-rate objective\nfunction outperforms the standard codecs such as H.265/HEVC-based BPG and\nstate-of-the-art learning-based variable-rate methods.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 06:26:56 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Akbari", "Mohammad", ""], ["Liang", "Jie", ""], ["Han", "Jingning", ""], ["Tu", "Chengjie", ""]]}, {"id": "2012.15470", "submitter": "Senthil Purushwalkam", "authors": "Senthil Purushwalkam, Sebastian Vicenc Amengual Gari, Vamsi Krishna\n  Ithapu, Carl Schissler, Philip Robinson, Abhinav Gupta, Kristen Grauman", "title": "Audio-Visual Floorplan Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given only a few glimpses of an environment, how much can we infer about its\nentire floorplan? Existing methods can map only what is visible or immediately\napparent from context, and thus require substantial movements through a space\nto fully map it. We explore how both audio and visual sensing together can\nprovide rapid floorplan reconstruction from limited viewpoints. Audio not only\nhelps sense geometry outside the camera's field of view, but it also reveals\nthe existence of distant freespace (e.g., a dog barking in another room) and\nsuggests the presence of rooms not visible to the camera (e.g., a dishwasher\nhumming in what must be the kitchen to the left). We introduce AV-Map, a novel\nmulti-modal encoder-decoder framework that reasons jointly about audio and\nvision to reconstruct a floorplan from a short input video sequence. We train\nour model to predict both the interior structure of the environment and the\nassociated rooms' semantic labels. Our results on 85 large real-world\nenvironments show the impact: with just a few glimpses spanning 26% of an area,\nwe can estimate the whole area with 66% accuracy -- substantially better than\nthe state of the art approach for extrapolating visual maps.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 07:00:34 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Purushwalkam", "Senthil", ""], ["Gari", "Sebastian Vicenc Amengual", ""], ["Ithapu", "Vamsi Krishna", ""], ["Schissler", "Carl", ""], ["Robinson", "Philip", ""], ["Gupta", "Abhinav", ""], ["Grauman", "Kristen", ""]]}, {"id": "2012.15497", "submitter": "Kun Wei", "authors": "Kun Wei, Cheng Deng, Xu Yang, and Maosen Li", "title": "Incremental Embedding Learning via Zero-Shot Translation", "comments": "9 page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern deep learning methods have achieved great success in machine learning\nand computer vision fields by learning a set of pre-defined datasets. Howerver,\nthese methods perform unsatisfactorily when applied into real-world situations.\nThe reason of this phenomenon is that learning new tasks leads the trained\nmodel quickly forget the knowledge of old tasks, which is referred to as\ncatastrophic forgetting. Current state-of-the-art incremental learning methods\ntackle catastrophic forgetting problem in traditional classification networks\nand ignore the problem existing in embedding networks, which are the basic\nnetworks for image retrieval, face recognition, zero-shot learning, etc.\nDifferent from traditional incremental classification networks, the semantic\ngap between the embedding spaces of two adjacent tasks is the main challenge\nfor embedding networks under incremental learning setting. Thus, we propose a\nnovel class-incremental method for embedding network, named as zero-shot\ntranslation class-incremental method (ZSTCI), which leverages zero-shot\ntranslation to estimate and compensate the semantic gap without any exemplars.\nThen, we try to learn a unified representation for two adjacent tasks in\nsequential learning process, which captures the relationships of previous\nclasses and current classes precisely. In addition, ZSTCI can easily be\ncombined with existing regularization-based incremental learning methods to\nfurther improve performance of embedding networks. We conduct extensive\nexperiments on CUB-200-2011 and CIFAR100, and the experiment results prove the\neffectiveness of our method. The code of our method has been released.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 08:21:37 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Wei", "Kun", ""], ["Deng", "Cheng", ""], ["Yang", "Xu", ""], ["Li", "Maosen", ""]]}, {"id": "2012.15503", "submitter": "Qilong Zhang", "authors": "Lianli Gao, Qilong Zhang, Jingkuan Song and Heng Tao Shen", "title": "Patch-wise++ Perturbation for Adversarial Targeted Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although great progress has been made on adversarial attacks for deep neural\nnetworks (DNNs), their transferability is still unsatisfactory, especially for\ntargeted attacks. There are two problems behind that have been long overlooked:\n1) the conventional setting of $T$ iterations with the step size of\n$\\epsilon/T$ to comply with the $\\epsilon$-constraint. In this case, most of\nthe pixels are allowed to add very small noise, much less than $\\epsilon$; and\n2) usually manipulating pixel-wise noise. However, features of a pixel\nextracted by DNNs are influenced by its surrounding regions, and different DNNs\ngenerally focus on different discriminative regions in recognition. To tackle\nthese issues, our previous work proposes a patch-wise iterative method (PIM)\naimed at crafting adversarial examples with high transferability. Specifically,\nwe introduce an amplification factor to the step size in each iteration, and\none pixel's overall gradient overflowing the $\\epsilon$-constraint is properly\nassigned to its surrounding regions by a project kernel. But targeted attacks\naim to push the adversarial examples into the territory of a specific class,\nand the amplification factor may lead to underfitting. Thus, we introduce the\ntemperature and propose a patch-wise++ iterative method (PIM++) to further\nimprove transferability without significantly sacrificing the performance of\nthe white-box attack. Our method can be generally integrated to any\ngradient-based attack methods. Compared with the current state-of-the-art\nattack methods, we significantly improve the success rate by 33.1\\% for defense\nmodels and 31.4\\% for normally trained models on average.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 08:40:42 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 07:34:21 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 12:52:44 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Gao", "Lianli", ""], ["Zhang", "Qilong", ""], ["Song", "Jingkuan", ""], ["Shen", "Heng Tao", ""]]}, {"id": "2012.15531", "submitter": "Zhi-Qin Zhan", "authors": "Zhi-Qin Zhan, Huazhu Fu, Yan-Yao Yang, Jingjing Chen, Jie Liu, and\n  Yu-Gang Jiang", "title": "Colonoscopy Polyp Detection: Domain Adaptation From Medical Report\n  Images to Real-time Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic colorectal polyp detection in colonoscopy video is a fundamental\ntask, which has received a lot of attention. Manually annotating polyp region\nin a large scale video dataset is time-consuming and expensive, which limits\nthe development of deep learning techniques. A compromise is to train the\ntarget model by using labeled images and infer on colonoscopy videos. However,\nthere are several issues between the image-based training and video-based\ninference, including domain differences, lack of positive samples, and temporal\nsmoothness. To address these issues, we propose an Image-video-joint polyp\ndetection network (Ivy-Net) to address the domain gap between colonoscopy\nimages from historical medical reports and real-time videos. In our Ivy-Net, a\nmodified mixup is utilized to generate training data by combining the positive\nimages and negative video frames at the pixel level, which could learn the\ndomain adaptive representations and augment the positive samples.\nSimultaneously, a temporal coherence regularization (TCR) is proposed to\nintroduce the smooth constraint on feature-level in adjacent frames and improve\npolyp detection by unlabeled colonoscopy videos. For evaluation, a new large\ncolonoscopy polyp dataset is collected, which contains 3056 images from\nhistorical medical reports of 889 positive patients and 7.5-hour videos of 69\npatients (28 positive). The experiments on the collected dataset demonstrate\nthat our Ivy-Net achieves the state-of-the-art result on colonoscopy video.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 10:33:09 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Zhan", "Zhi-Qin", ""], ["Fu", "Huazhu", ""], ["Yang", "Yan-Yao", ""], ["Chen", "Jingjing", ""], ["Liu", "Jie", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "2012.15564", "submitter": "Yichi Zhang", "authors": "Yichi Zhang, Qingcheng Liao, Lin Yuan, He Zhu, Jiezhen Xing, Jicong\n  Zhang", "title": "Exploiting Shared Knowledge from Non-COVID Lesions for\n  Annotation-Efficient COVID-19 CT Lung Infection Segmentation", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The novel Coronavirus disease (COVID-19) is a highly contagious virus and has\nspread all over the world, posing an extremely serious threat to all countries.\nAutomatic lung infection segmentation from computed tomography (CT) plays an\nimportant role in the quantitative analysis of COVID-19. However, the major\nchallenge lies in the inadequacy of annotated COVID-19 datasets. Currently,\nthere are several public non-COVID lung lesion segmentation datasets, providing\nthe potential for generalizing useful information to the related COVID-19\nsegmentation task. In this paper, we propose a novel relation-driven\ncollaborative learning model to exploit shared knowledge from non-COVID lesions\nfor annotation-efficient COVID-19 CT lung infection segmentation. The model\nconsists of a general encoder to capture general lung lesion features based on\nmultiple non-COVID lesions, and a target encoder to focus on task-specific\nfeatures based on COVID-19 infections. Features extracted from the two parallel\nencoders are concatenated for the subsequent decoder part. We develop a\ncollaborative learning scheme to regularize feature-level relation consistency\nof given input and encourage the model to learn more general and discriminative\nrepresentation of COVID-19 infections. Extensive experiments demonstrate that\ntrained with limited COVID-19 data, exploiting shared knowledge from non-COVID\nlesions can further improve state-of-the-art performance with up to 3.0% in\ndice similarity coefficient and 4.2% in normalized surface dice. Our proposed\nmethod promotes new insights into annotation-efficient deep learning for\nCOVID-19 infection segmentation and illustrates strong potential for real-world\napplications in the global fight against COVID-19 in the absence of sufficient\nhigh-quality annotations.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 11:40:29 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 01:36:52 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Zhang", "Yichi", ""], ["Liao", "Qingcheng", ""], ["Yuan", "Lin", ""], ["Zhu", "He", ""], ["Xing", "Jiezhen", ""], ["Zhang", "Jicong", ""]]}, {"id": "2012.15565", "submitter": "Sriram Krishna", "authors": "Sriram Krishna, Siddarth Vinay, Srinivas K S", "title": "Searching a Raw Video Database using Natural Language Queries", "comments": "6 pages, 12 figures, to appear in the proceedings of the First\n  International Conference on Advances in Electrical, Computing, Communications\n  and Sustainable Technologies (ICAECT 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of videos being produced and consequently stored in databases for\nvideo streaming platforms has been increasing exponentially over time. This\nvast database should be easily index-able to find the requisite clip or video\nto match the given search specification, preferably in the form of a textual\nquery. This work aims to provide an end-to-end pipeline to search a video\ndatabase with a voice query from the end user. The pipeline makes use of\nRecurrent Neural Networks in combination with Convolutional Neural Networks to\ngenerate captions of the video clips present in the database.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 11:43:04 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Krishna", "Sriram", ""], ["Vinay", "Siddarth", ""], ["S", "Srinivas K", ""]]}, {"id": "2012.15575", "submitter": "Ziwen Xu", "authors": "Ziwen Xu, beiji Zou, Qing Liu", "title": "A Deep Retinal Image Quality Assessment Network with Salient Structure\n  Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Retinal image quality assessment is an essential prerequisite for diagnosis\nof retinal diseases. Its goal is to identify retinal images in which anatomic\nstructures and lesions attracting ophthalmologists' attention most are\nexhibited clearly and definitely while reject poor quality fundus images.\nMotivated by this, we mimic the way that ophthalmologists assess the quality of\nretinal images and propose a method termed SalStructuIQA. First, two salient\nstructures for automated retinal quality assessment. One is the large-size\nsalient structures including optic disc region and exudates in large-size. The\nother is the tiny-size salient structures which mainly include vessels. Then we\nincorporate the proposed two salient structure priors with deep convolutional\nneural network (CNN) to shift the focus of CNN to salient structures.\nAccordingly, we develop two CNN architectures: Dual-branch SalStructIQA and\nSingle-branch SalStructIQA. Dual-branch SalStructIQA contains two CNN branches\nand one is guided by large-size salient structures while the other is guided by\ntiny-size salient structures. Single-branch SalStructIQA contains one CNN\nbranch, which is guided by the concatenation of salient structures in both\nlarge-size and tiny-size. Experimental results on Eye-Quality dataset show that\nour proposed Dual-branch SalStructIQA outperforms the state-of-the-art methods\nfor retinal image quality assessment and Single-branch SalStructIQA is much\nlight-weight comparing with state-of-the-art deep retinal image quality\nassessment methods and still achieves competitive performances.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 12:22:11 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Xu", "Ziwen", ""], ["Zou", "beiji", ""], ["Liu", "Qing", ""]]}, {"id": "2012.15635", "submitter": "Alan Smeaton", "authors": "Lorin Sweeney, Graham Healy, Alan F. Smeaton", "title": "Leveraging Audio Gestalt to Predict Media Memorability", "comments": "3 pages, 1 Figure, 2 Tables", "journal-ref": "MediaEval Multimedia Benchmark Workshop Working Notes, 14-15\n  December 2020", "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Memorability determines what evanesces into emptiness, and what worms its way\ninto the deepest furrows of our minds. It is the key to curating more\nmeaningful media content as we wade through daily digital torrents. The\nPredicting Media Memorability task in MediaEval 2020 aims to address the\nquestion of media memorability by setting the task of automatically predicting\nvideo memorability. Our approach is a multimodal deep learning-based late\nfusion that combines visual, semantic, and auditory features. We used audio\ngestalt to estimate the influence of the audio modality on overall video\nmemorability, and accordingly inform which combination of features would best\npredict a given video's memorability scores.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 14:50:42 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Sweeney", "Lorin", ""], ["Healy", "Graham", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "2012.15638", "submitter": "Yiming Zeng", "authors": "Yiming Zeng, Yue Qian, Zhiyu Zhu, Junhui Hou, Hui Yuan, Ying He", "title": "CorrNet3D: Unsupervised End-to-end Learning of Dense Correspondence for\n  3D Point Clouds", "comments": "This paper was accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the intuition that one can transform two aligned point clouds to\neach other more easily and meaningfully than a misaligned pair, we propose\nCorrNet3D -- the first unsupervised and end-to-end deep learning-based\nframework -- to drive the learning of dense correspondence between 3D shapes by\nmeans of deformation-like reconstruction to overcome the need for annotated\ndata. Specifically, CorrNet3D consists of a deep feature embedding module and\ntwo novel modules called correspondence indicator and symmetric deformer.\nFeeding a pair of raw point clouds, our model first learns the pointwise\nfeatures and passes them into the indicator to generate a learnable\ncorrespondence matrix used to permute the input pair. The symmetric deformer,\nwith an additional regularized loss, transforms the two permuted point clouds\nto each other to drive the unsupervised learning of the correspondence. The\nextensive experiments on both synthetic and real-world datasets of rigid and\nnon-rigid 3D shapes show our CorrNet3D outperforms state-of-the-art methods to\na large extent, including those taking meshes as input. CorrNet3D is a flexible\nframework in that it can be easily adapted to supervised learning if annotated\ndata are available. The source code and pre-trained model will be available at\nhttps://github.com/ZENGYIMING-EAMON/CorrNet3D.git.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 14:55:51 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 07:50:28 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zeng", "Yiming", ""], ["Qian", "Yue", ""], ["Zhu", "Zhiyu", ""], ["Hou", "Junhui", ""], ["Yuan", "Hui", ""], ["He", "Ying", ""]]}, {"id": "2012.15641", "submitter": "Alan Smeaton", "authors": "Phuc H. Le-Khac and Ayush K. Rai and Graham Healy and Alan F. Smeaton\n  and Noel E. O'Connor", "title": "Investigating Memorability of Dynamic Media", "comments": "3 pages, 1 figure. 1 table", "journal-ref": "MediaEval Multimedia Benchmark Workshop Working Notes, 14-15\n  December 2020", "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Predicting Media Memorability task in MediaEval'20 has some challenging\naspects compared to previous years. In this paper we identify the high-dynamic\ncontent in videos and dataset of limited size as the core challenges for the\ntask, we propose directions to overcome some of these challenges and we present\nour initial result in these directions.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 15:01:44 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Le-Khac", "Phuc H.", ""], ["Rai", "Ayush K.", ""], ["Healy", "Graham", ""], ["Smeaton", "Alan F.", ""], ["O'Connor", "Noel E.", ""]]}, {"id": "2012.15650", "submitter": "Alan Smeaton", "authors": "Alba Garc\\'ia Seco De Herrera and Rukiye Savran Kiziltepe and Jon\n  Chamberlain and Mihai Gabriel Constantin and Claire-H\\'el\\`ene Demarty and\n  Faiyaz Doctor and Bogdan Ionescu and Alan F. Smeaton", "title": "Overview of MediaEval 2020 Predicting Media Memorability Task: What\n  Makes a Video Memorable?", "comments": "3 pages, 1 Figure", "journal-ref": "MediaEval Multimedia Benchmark Workshop Working Notes, 14-15\n  December 2020", "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the MediaEval 2020 \\textit{Predicting Media\nMemorability} task. After first being proposed at MediaEval 2018, the\nPredicting Media Memorability task is in its 3rd edition this year, as the\nprediction of short-term and long-term video memorability (VM) remains a\nchallenging task. In 2020, the format remained the same as in previous\neditions. This year the videos are a subset of the TRECVid 2019 Video-to-Text\ndataset, containing more action rich video content as compared with the 2019\ntask. In this paper a description of some aspects of this task is provided,\nincluding its main characteristics, a description of the collection, the ground\ntruth dataset, evaluation metrics and the requirements for participants' run\nsubmissions.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 15:12:52 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["De Herrera", "Alba Garc\u00eda Seco", ""], ["Kiziltepe", "Rukiye Savran", ""], ["Chamberlain", "Jon", ""], ["Constantin", "Mihai Gabriel", ""], ["Demarty", "Claire-H\u00e9l\u00e8ne", ""], ["Doctor", "Faiyaz", ""], ["Ionescu", "Bogdan", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "2012.15680", "submitter": "Ay\\c{c}a Takmaz", "authors": "Ay\\c{c}a Takmaz, Danda Pani Paudel, Thomas Probst, Ajad Chhatkuli,\n  Martin R. Oswald, Luc Van Gool", "title": "Unsupervised Monocular Depth Reconstruction of Non-Rigid Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth reconstruction of complex and dynamic scenes is a highly\nchallenging problem. While for rigid scenes learning-based methods have been\noffering promising results even in unsupervised cases, there exists little to\nno literature addressing the same for dynamic and deformable scenes. In this\nwork, we present an unsupervised monocular framework for dense depth estimation\nof dynamic scenes, which jointly reconstructs rigid and non-rigid parts without\nexplicitly modelling the camera motion. Using dense correspondences, we derive\na training objective that aims to opportunistically preserve pairwise distances\nbetween reconstructed 3D points. In this process, the dense depth map is\nlearned implicitly using the as-rigid-as-possible hypothesis. Our method\nprovides promising results, demonstrating its capability of reconstructing 3D\nfrom challenging videos of non-rigid scenes. Furthermore, the proposed method\nalso provides unsupervised motion segmentation results as an auxiliary output.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 16:02:03 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Takmaz", "Ay\u00e7a", ""], ["Paudel", "Danda Pani", ""], ["Probst", "Thomas", ""], ["Chhatkuli", "Ajad", ""], ["Oswald", "Martin R.", ""], ["Van Gool", "Luc", ""]]}, {"id": "2012.15685", "submitter": "Haoyue Bai", "authors": "Haoyue Bai, S.-H. Gary Chan", "title": "CNN-based Single Image Crowd Counting: Network Design, Loss Function and\n  Supervisory Signal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image crowd counting is a challenging computer vision problem with\nwide applications in public safety, city planning, traffic management, etc.\nThis survey is to provide a comprehensive summary of recent advanced crowd\ncounting techniques based on Convolutional Neural Network (CNN) via density map\nestimation. Our goals are to provide an up-to-date review of recent approaches,\nand educate new researchers in this field the design principles and trade-offs.\nAfter presenting publicly available datasets and evaluation metrics, we review\nthe recent advances with detailed comparisons on three major design modules for\ncrowd counting: deep neural network designs, loss functions, and supervisory\nsignals. We conclude the survey with some future directions.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 16:10:40 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Bai", "Haoyue", ""], ["Chan", "S. -H. Gary", ""]]}, {"id": "2012.15692", "submitter": "Zhengxia Zou", "authors": "Zhengxia Zou, Tianyang Shi, Yi Yuan, Zhenwei Shi", "title": "NeuralMagicEye: Learning to See and Understand the Scene Behind an\n  Autostereogram", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An autostereogram, a.k.a. magic eye image, is a single-image stereogram that\ncan create visual illusions of 3D scenes from 2D textures. This paper studies\nan interesting question that whether a deep CNN can be trained to recover the\ndepth behind an autostereogram and understand its content. The key to the\nautostereogram magic lies in the stereopsis - to solve such a problem, a model\nhas to learn to discover and estimate disparity from the quasi-periodic\ntextures. We show that deep CNNs embedded with disparity convolution, a novel\nconvolutional layer proposed in this paper that simulates stereopsis and\nencodes disparity, can nicely solve such a problem after being sufficiently\ntrained on a large 3D object dataset in a self-supervised fashion. We refer to\nour method as ``NeuralMagicEye''. Experiments show that our method can\naccurately recover the depth behind autostereograms with rich details and\ngradient smoothness. Experiments also show the completely different working\nmechanisms for autostereogram perception between neural networks and human\neyes. We hope this research can help people with visual impairments and those\nwho have trouble viewing autostereograms. Our code is available at\n\\url{https://jiupinjia.github.io/neuralmagiceye/}.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 16:17:47 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Zou", "Zhengxia", ""], ["Shi", "Tianyang", ""], ["Yuan", "Yi", ""], ["Shi", "Zhenwei", ""]]}, {"id": "2012.15712", "submitter": "Jiajun Deng", "authors": "Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou, Yanyong Zhang,\n  Houqiang Li", "title": "Voxel R-CNN: Towards High Performance Voxel-based 3D Object Detection", "comments": "AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances on 3D object detection heavily rely on how the 3D data are\nrepresented, \\emph{i.e.}, voxel-based or point-based representation. Many\nexisting high performance 3D detectors are point-based because this structure\ncan better retain precise point positions. Nevertheless, point-level features\nlead to high computation overheads due to unordered storage. In contrast, the\nvoxel-based structure is better suited for feature extraction but often yields\nlower accuracy because the input data are divided into grids. In this paper, we\ntake a slightly different viewpoint -- we find that precise positioning of raw\npoints is not essential for high performance 3D object detection and that the\ncoarse voxel granularity can also offer sufficient detection accuracy. Bearing\nthis view in mind, we devise a simple but effective voxel-based framework,\nnamed Voxel R-CNN. By taking full advantage of voxel features in a two stage\napproach, our method achieves comparable detection accuracy with\nstate-of-the-art point-based models, but at a fraction of the computation cost.\nVoxel R-CNN consists of a 3D backbone network, a 2D bird-eye-view (BEV) Region\nProposal Network and a detect head. A voxel RoI pooling is devised to extract\nRoI features directly from voxel features for further refinement. Extensive\nexperiments are conducted on the widely used KITTI Dataset and the more recent\nWaymo Open Dataset. Our results show that compared to existing voxel-based\nmethods, Voxel R-CNN delivers a higher detection accuracy while maintaining a\nreal-time frame processing rate, \\emph{i.e}., at a speed of 25 FPS on an NVIDIA\nRTX 2080 Ti GPU. The code is available at\n\\url{https://github.com/djiajunustc/Voxel-R-CNN}.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 17:02:46 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 16:25:48 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Deng", "Jiajun", ""], ["Shi", "Shaoshuai", ""], ["Li", "Peiwei", ""], ["Zhou", "Wengang", ""], ["Zhang", "Yanyong", ""], ["Li", "Houqiang", ""]]}, {"id": "2012.15766", "submitter": "Zhengsu Chen", "authors": "Zhengsu Chen, Jianwei Niu, Xuefeng Liu and Shaojie Tang", "title": "SelectScale: Mining More Patterns from Images via Selective and Soft\n  Dropout", "comments": "arXiv admin note: text overlap with arXiv:1810.09849 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have achieved remarkable success in\nimage recognition. Although the internal patterns of the input images are\neffectively learned by the CNNs, these patterns only constitute a small\nproportion of useful patterns contained in the input images. This can be\nattributed to the fact that the CNNs will stop learning if the learned patterns\nare enough to make a correct classification. Network regularization methods\nlike dropout and SpatialDropout can ease this problem. During training, they\nrandomly drop the features. These dropout methods, in essence, change the\npatterns learned by the networks, and in turn, forces the networks to learn\nother patterns to make the correct classification. However, the above methods\nhave an important drawback. Randomly dropping features is generally inefficient\nand can introduce unnecessary noise. To tackle this problem, we propose\nSelectScale. Instead of randomly dropping units, SelectScale selects the\nimportant features in networks and adjusts them during training. Using\nSelectScale, we improve the performance of CNNs on CIFAR and ImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 12:15:08 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Chen", "Zhengsu", ""], ["Niu", "Jianwei", ""], ["Liu", "Xuefeng", ""], ["Tang", "Shaojie", ""]]}, {"id": "2012.15772", "submitter": "Matthew Ng", "authors": "Matthew Ng, Fumin Guo, Labonny Biswas, Steffen E. Petersen, Stefan K.\n  Piechnik, Stefan Neubauer, Graham Wright", "title": "Estimating Uncertainty in Neural Networks for Cardiac MRI Segmentation:\n  A Benchmark Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Convolutional neural networks (CNNs) have demonstrated promise in automated\ncardiac magnetic resonance imaging segmentation. However, when using CNNs in a\nlarge real world dataset, it is important to quantify segmentation uncertainty\nin order to know which segmentations could be problematic. In this work, we\nperformed a systematic study of Bayesian and non-Bayesian methods for\nestimating uncertainty in segmentation neural networks. We evaluated Bayes by\nBackprop (BBB), Monte Carlo (MC) Dropout, and Deep Ensembles in terms of\nsegmentation accuracy, probability calibration, uncertainty on\nout-of-distribution images, and segmentation quality control. We tested these\nalgorithms on datasets with various distortions and observed that Deep\nEnsembles outperformed the other methods except for images with heavy noise\ndistortions. For segmentation quality control, we showed that segmentation\nuncertainty is correlated with segmentation accuracy. With the incorporation of\nuncertainty estimates, we were able to reduce the percentage of poor\nsegmentation to 5% by flagging 31% to 48% of the most uncertain images for\nmanual review, substantially lower than random review of the results without\nusing neural network uncertainty.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 17:46:52 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Ng", "Matthew", ""], ["Guo", "Fumin", ""], ["Biswas", "Labonny", ""], ["Petersen", "Steffen E.", ""], ["Piechnik", "Stefan K.", ""], ["Neubauer", "Stefan", ""], ["Wright", "Graham", ""]]}, {"id": "2012.15779", "submitter": "Egor Ershov I", "authors": "Egor Ershov, Alex Savchik, Ilya Semenkov, Nikola Bani\\'c, Karlo\n  Koscevi\\'c, Marko Suba\\v{s}i\\'c, Alexander Belokopytov, Zhihao Li, Arseniy\n  Terekhin, Daria Senshina, Artem Nikonorov, Yanlin Qian, Marco Buzzelli,\n  Riccardo Riva, Simone Bianco, Raimondo Schettini, Sven Lon\\v{c}ari\\'c, Dmitry\n  Nikolaev", "title": "Illumination Estimation Challenge: experience of past two years", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Illumination estimation is the essential step of computational color\nconstancy, one of the core parts of various image processing pipelines of\nmodern digital cameras. Having an accurate and reliable illumination estimation\nis important for reducing the illumination influence on the image colors. To\nmotivate the generation of new ideas and the development of new algorithms in\nthis field, the 2nd Illumination estimation challenge~(IEC\\#2) was conducted.\nThe main advantage of testing a method on a challenge over testing in on some\nof the known datasets is the fact that the ground-truth illuminations for the\nchallenge test images are unknown up until the results have been submitted,\nwhich prevents any potential hyperparameter tuning that may be biased.\n  The challenge had several tracks: general, indoor, and two-illuminant with\neach of them focusing on different parameters of the scenes. Other main\nfeatures of it are a new large dataset of images (about 5000) taken with the\nsame camera sensor model, a manual markup accompanying each image, diverse\ncontent with scenes taken in numerous countries under a huge variety of\nilluminations extracted by using the SpyderCube calibration object, and a\ncontest-like markup for the images from the Cube+ dataset that was used in\nIEC\\#1.\n  This paper focuses on the description of the past two challenges, algorithms\nwhich won in each track, and the conclusions that were drawn based on the\nresults obtained during the 1st and 2nd challenge that can be useful for\nsimilar future developments.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 17:59:19 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Ershov", "Egor", ""], ["Savchik", "Alex", ""], ["Semenkov", "Ilya", ""], ["Bani\u0107", "Nikola", ""], ["Koscevi\u0107", "Karlo", ""], ["Suba\u0161i\u0107", "Marko", ""], ["Belokopytov", "Alexander", ""], ["Li", "Zhihao", ""], ["Terekhin", "Arseniy", ""], ["Senshina", "Daria", ""], ["Nikonorov", "Artem", ""], ["Qian", "Yanlin", ""], ["Buzzelli", "Marco", ""], ["Riva", "Riccardo", ""], ["Bianco", "Simone", ""], ["Schettini", "Raimondo", ""], ["Lon\u010dari\u0107", "Sven", ""], ["Nikolaev", "Dmitry", ""]]}, {"id": "2012.15783", "submitter": "Saeed Khorram", "authors": "Saeed Khorram, Tyler Lawson, Fuxin Li", "title": "iGOS++: Integrated Gradient Optimized Saliency by Bilateral\n  Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The black-box nature of the deep networks makes the explanation for \"why\"\nthey make certain predictions extremely challenging. Saliency maps are one of\nthe most widely-used local explanation tools to alleviate this problem. One of\nthe primary approaches for generating saliency maps is by optimizing a mask\nover the input dimensions so that the output of the network is influenced the\nmost by the masking. However, prior work only studies such influence by\nremoving evidence from the input. In this paper, we present iGOS++, a framework\nto generate saliency maps that are optimized for altering the output of the\nblack-box system by either removing or preserving only a small fraction of the\ninput. Additionally, we propose to add a bilateral total variation term to the\noptimization that improves the continuity of the saliency map especially under\nhigh resolution and with thin object parts. The evaluation results from\ncomparing iGOS++ against state-of-the-art saliency map methods show significant\nimprovement in locating salient regions that are directly interpretable by\nhumans. We utilized iGOS++ in the task of classifying COVID-19 cases from x-ray\nimages and discovered that sometimes the CNN network is overfitted to the\ncharacters printed on the x-ray images when performing classification. Fixing\nthis issue by data cleansing significantly improved the precision and recall of\nthe classifier.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 18:04:12 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 19:56:55 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Khorram", "Saeed", ""], ["Lawson", "Tyler", ""], ["Li", "Fuxin", ""]]}, {"id": "2012.15814", "submitter": "Jiayuan Mao", "authors": "Ruocheng Wang, Jiayuan Mao, Samuel J. Gershman, Jiajun Wu", "title": "Language-Mediated, Object-Centric Representation Learning", "comments": "ACL 2021 Findings. First two authors contributed equally; last two\n  authors contributed equally. Project page: https://lang-orl.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Language-mediated, Object-centric Representation Learning (LORL),\na paradigm for learning disentangled, object-centric scene representations from\nvision and language. LORL builds upon recent advances in unsupervised object\ndiscovery and segmentation, notably MONet and Slot Attention. While these\nalgorithms learn an object-centric representation just by reconstructing the\ninput image, LORL enables them to further learn to associate the learned\nrepresentations to concepts, i.e., words for object categories, properties, and\nspatial relationships, from language input. These object-centric concepts\nderived from language facilitate the learning of object-centric\nrepresentations. LORL can be integrated with various unsupervised object\ndiscovery algorithms that are language-agnostic. Experiments show that the\nintegration of LORL consistently improves the performance of unsupervised\nobject discovery methods on two datasets via the help of language. We also show\nthat concepts learned by LORL, in conjunction with object discovery methods,\naid downstream tasks such as referring expression comprehension.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 18:36:07 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 04:37:54 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wang", "Ruocheng", ""], ["Mao", "Jiayuan", ""], ["Gershman", "Samuel J.", ""], ["Wu", "Jiajun", ""]]}, {"id": "2012.15823", "submitter": "Mehdi Bahri", "authors": "Mehdi Bahri, Ga\\'etan Bahl, Stefanos Zafeiriou", "title": "Binary Graph Neural Networks", "comments": "CVPR 2021 Camera-Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph Neural Networks (GNNs) have emerged as a powerful and flexible\nframework for representation learning on irregular data. As they generalize the\noperations of classical CNNs on grids to arbitrary topologies, GNNs also bring\nmuch of the implementation challenges of their Euclidean counterparts. Model\nsize, memory footprint, and energy consumption are common concerns for many\nreal-world applications. Network binarization allocates a single bit to\nparameters and activations, thus dramatically reducing the memory requirements\n(up to 32x compared to single-precision floating-point numbers) and maximizing\nthe benefits of fast SIMD instructions on modern hardware for measurable\nspeedups. However, in spite of the large body of work on binarization for\nclassical CNNs, this area remains largely unexplored in geometric deep\nlearning. In this paper, we present and evaluate different strategies for the\nbinarization of graph neural networks. We show that through careful design of\nthe models, and control of the training process, binary graph neural networks\ncan be trained at only a moderate cost in accuracy on challenging benchmarks.\nIn particular, we present the first dynamic graph neural network in Hamming\nspace, able to leverage efficient k-NN search on binary vectors to speed-up the\nconstruction of the dynamic graph. We further verify that the binary models\noffer significant savings on embedded devices. Our code is publicly available\non Github.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 18:48:58 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 23:48:56 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Bahri", "Mehdi", ""], ["Bahl", "Ga\u00e9tan", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2012.15827", "submitter": "Lucas Prado Osco", "authors": "Lucas Prado Osco, Mauro dos Santos de Arruda, Diogo Nunes\n  Gon\\c{c}alves, Alexandre Dias, Juliana Batistoti, Mauricio de Souza, Felipe\n  David Georges Gomes, Ana Paula Marques Ramos, L\\'ucio Andr\\'e de Castro\n  Jorge, Veraldo Liesenberg, Jonathan Li, Lingfei Ma, Jos\\'e Marcato Junior,\n  Wesley Nunes Gon\\c{c}alves", "title": "A CNN Approach to Simultaneously Count Plants and Detect Plantation-Rows\n  from UAV Imagery", "comments": "27 pages, 12 figures, 9 tables", "journal-ref": "ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING 174 (2021) 1-17", "doi": "10.1016/j.isprsjprs.2021.01.024", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we propose a novel deep learning method based on a\nConvolutional Neural Network (CNN) that simultaneously detects and geolocates\nplantation-rows while counting its plants considering highly-dense plantation\nconfigurations. The experimental setup was evaluated in a cornfield with\ndifferent growth stages and in a Citrus orchard. Both datasets characterize\ndifferent plant density scenarios, locations, types of crops, sensors, and\ndates. A two-branch architecture was implemented in our CNN method, where the\ninformation obtained within the plantation-row is updated into the plant\ndetection branch and retro-feed to the row branch; which are then refined by a\nMulti-Stage Refinement method. In the corn plantation datasets (with both\ngrowth phases, young and mature), our approach returned a mean absolute error\n(MAE) of 6.224 plants per image patch, a mean relative error (MRE) of 0.1038,\nprecision and recall values of 0.856, and 0.905, respectively, and an F-measure\nequal to 0.876. These results were superior to the results from other deep\nnetworks (HRNet, Faster R-CNN, and RetinaNet) evaluated with the same task and\ndataset. For the plantation-row detection, our approach returned precision,\nrecall, and F-measure scores of 0.913, 0.941, and 0.925, respectively. To test\nthe robustness of our model with a different type of agriculture, we performed\nthe same task in the citrus orchard dataset. It returned an MAE equal to 1.409\ncitrus-trees per patch, MRE of 0.0615, precision of 0.922, recall of 0.911, and\nF-measure of 0.965. For citrus plantation-row detection, our approach resulted\nin precision, recall, and F-measure scores equal to 0.965, 0.970, and 0.964,\nrespectively. The proposed method achieved state-of-the-art performance for\ncounting and geolocating plants and plant-rows in UAV images from different\ntypes of crops.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 18:51:17 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 16:47:17 GMT"}, {"version": "v3", "created": "Sun, 14 Feb 2021 18:02:01 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Osco", "Lucas Prado", ""], ["de Arruda", "Mauro dos Santos", ""], ["Gon\u00e7alves", "Diogo Nunes", ""], ["Dias", "Alexandre", ""], ["Batistoti", "Juliana", ""], ["de Souza", "Mauricio", ""], ["Gomes", "Felipe David Georges", ""], ["Ramos", "Ana Paula Marques", ""], ["Jorge", "L\u00facio Andr\u00e9 de Castro", ""], ["Liesenberg", "Veraldo", ""], ["Li", "Jonathan", ""], ["Ma", "Lingfei", ""], ["Junior", "Jos\u00e9 Marcato", ""], ["Gon\u00e7alves", "Wesley Nunes", ""]]}, {"id": "2012.15838", "submitter": "Sida Peng", "authors": "Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai,\n  Hujun Bao, Xiaowei Zhou", "title": "Neural Body: Implicit Neural Representations with Structured Latent\n  Codes for Novel View Synthesis of Dynamic Humans", "comments": "CVPR 2021. Project page: https://zju3dv.github.io/neuralbody/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the challenge of novel view synthesis for a human\nperformer from a very sparse set of camera views. Some recent works have shown\nthat learning implicit neural representations of 3D scenes achieves remarkable\nview synthesis quality given dense input views. However, the representation\nlearning will be ill-posed if the views are highly sparse. To solve this\nill-posed problem, our key idea is to integrate observations over video frames.\nTo this end, we propose Neural Body, a new human body representation which\nassumes that the learned neural representations at different frames share the\nsame set of latent codes anchored to a deformable mesh, so that the\nobservations across frames can be naturally integrated. The deformable mesh\nalso provides geometric guidance for the network to learn 3D representations\nmore efficiently. To evaluate our approach, we create a multi-view dataset\nnamed ZJU-MoCap that captures performers with complex motions. Experiments on\nZJU-MoCap show that our approach outperforms prior works by a large margin in\nterms of novel view synthesis quality. We also demonstrate the capability of\nour approach to reconstruct a moving person from a monocular video on the\nPeople-Snapshot dataset. The code and dataset are available at\nhttps://zju3dv.github.io/neuralbody/.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 18:55:38 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 14:13:59 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Peng", "Sida", ""], ["Zhang", "Yuanqing", ""], ["Xu", "Yinghao", ""], ["Wang", "Qianqian", ""], ["Shuai", "Qing", ""], ["Bao", "Hujun", ""], ["Zhou", "Xiaowei", ""]]}, {"id": "2012.15840", "submitter": "Li Zhang", "authors": "Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo,\n  Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip H.S. Torr, Li Zhang", "title": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective\n  with Transformers", "comments": "CVPR 2021. Project page at https://fudan-zvg.github.io/SETR/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most recent semantic segmentation methods adopt a fully-convolutional network\n(FCN) with an encoder-decoder architecture. The encoder progressively reduces\nthe spatial resolution and learns more abstract/semantic visual concepts with\nlarger receptive fields. Since context modeling is critical for segmentation,\nthe latest efforts have been focused on increasing the receptive field, through\neither dilated/atrous convolutions or inserting attention modules. However, the\nencoder-decoder based FCN architecture remains unchanged. In this paper, we aim\nto provide an alternative perspective by treating semantic segmentation as a\nsequence-to-sequence prediction task. Specifically, we deploy a pure\ntransformer (ie, without convolution and resolution reduction) to encode an\nimage as a sequence of patches. With the global context modeled in every layer\nof the transformer, this encoder can be combined with a simple decoder to\nprovide a powerful segmentation model, termed SEgmentation TRansformer (SETR).\nExtensive experiments show that SETR achieves new state of the art on ADE20K\n(50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on\nCityscapes. Particularly, we achieve the first position in the highly\ncompetitive ADE20K test server leaderboard on the day of submission.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 18:55:57 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 10:07:30 GMT"}, {"version": "v3", "created": "Sun, 25 Jul 2021 10:44:52 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zheng", "Sixiao", ""], ["Lu", "Jiachen", ""], ["Zhao", "Hengshuang", ""], ["Zhu", "Xiatian", ""], ["Luo", "Zekun", ""], ["Wang", "Yabiao", ""], ["Fu", "Yanwei", ""], ["Feng", "Jianfeng", ""], ["Xiang", "Tao", ""], ["Torr", "Philip H. S.", ""], ["Zhang", "Li", ""]]}, {"id": "2012.15846", "submitter": "Amogh Gudi", "authors": "Amogh Gudi, Marian Bittner, Jan van Gemert", "title": "Real-time Webcam Heart-Rate and Variability Estimation with Clean Ground\n  Truth for Evaluation", "comments": "Published in the MDPI Applied Sciences journal special issue Video\n  Analysis for Health Monitoring on December 2, 2020. arXiv admin note: text\n  overlap with arXiv:1909.01206", "journal-ref": "Applied Sciences. 2020; 10(23):8630", "doi": "10.3390/app10238630", "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote photo-plethysmography (rPPG) uses a camera to estimate a person's\nheart rate (HR). Similar to how heart rate can provide useful information about\na person's vital signs, insights about the underlying physio/psychological\nconditions can be obtained from heart rate variability (HRV). HRV is a measure\nof the fine fluctuations in the intervals between heart beats. However, this\nmeasure requires temporally locating heart beats with a high degree of\nprecision. We introduce a refined and efficient real-time rPPG pipeline with\nnovel filtering and motion suppression that not only estimates heart rates, but\nalso extracts the pulse waveform to time heart beats and measure heart rate\nvariability. This unsupervised method requires no rPPG specific training and is\nable to operate in real-time. We also introduce a new multi-modal video\ndataset, VicarPPG 2, specifically designed to evaluate rPPG algorithms on HR\nand HRV estimation. We validate and study our method under various conditions\non a comprehensive range of public and self-recorded datasets, showing\nstate-of-the-art results and providing useful insights into some unique\naspects. Lastly, we make available CleanerPPG, a collection of human-verified\nground truth peak/heart-beat annotations for existing rPPG datasets. These\nverified annotations should make future evaluations and benchmarking of rPPG\nalgorithms more accurate, standardized and fair.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 18:57:05 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Gudi", "Amogh", ""], ["Bittner", "Marian", ""], ["van Gemert", "Jan", ""]]}, {"id": "2012.15864", "submitter": "Ayaan Haque", "authors": "Ayaan Haque", "title": "EC-GAN: Low-Sample Classification using Semi-Supervised Algorithms and\n  GANs", "comments": "AAAI 2021; 7 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semi-supervised learning has been gaining attention as it allows for\nperforming image analysis tasks such as classification with limited labeled\ndata. Some popular algorithms using Generative Adversarial Networks (GANs) for\nsemi-supervised classification share a single architecture for classification\nand discrimination. However, this may require a model to converge to a separate\ndata distribution for each task, which may reduce overall performance. While\nprogress in semi-supervised learning has been made, less addressed are\nsmall-scale, fully-supervised tasks where even unlabeled data is unavailable\nand unattainable. We therefore, propose a novel GAN model namely External\nClassifier GAN (EC-GAN), that utilizes GANs and semi-supervised algorithms to\nimprove classification in fully-supervised regimes. Our method leverages a GAN\nto generate artificial data used to supplement supervised classification. More\nspecifically, we attach an external classifier, hence the name EC-GAN, to the\nGAN's generator, as opposed to sharing an architecture with the discriminator.\nOur experiments demonstrate that EC-GAN's performance is comparable to the\nshared architecture method, far superior to the standard data augmentation and\nregularization-based approach, and effective on a small, realistic dataset.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 05:58:00 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 17:13:04 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 21:24:33 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Haque", "Ayaan", ""]]}]