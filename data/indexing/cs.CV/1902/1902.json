[{"id": "1902.00038", "submitter": "Hedi Ben-Younes", "authors": "Hedi Ben-younes, R\\'emi Cadene, Nicolas Thome, Matthieu Cord", "title": "BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and\n  Visual Relationship Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal representation learning is gaining more and more interest within\nthe deep learning community. While bilinear models provide an interesting\nframework to find subtle combination of modalities, their number of parameters\ngrows quadratically with the input dimensions, making their practical\nimplementation within classical deep learning pipelines challenging. In this\npaper, we introduce BLOCK, a new multimodal fusion based on the\nblock-superdiagonal tensor decomposition. It leverages the notion of block-term\nranks, which generalizes both concepts of rank and mode ranks for tensors,\nalready used for multimodal fusion. It allows to define new ways for optimizing\nthe tradeoff between the expressiveness and complexity of the fusion model, and\nis able to represent very fine interactions between modalities while\nmaintaining powerful mono-modal representations. We demonstrate the practical\ninterest of our fusion model by using BLOCK for two challenging tasks: Visual\nQuestion Answering (VQA) and Visual Relationship Detection (VRD), where we\ndesign end-to-end learnable architectures for representing relevant\ninteractions between modalities. Through extensive experiments, we show that\nBLOCK compares favorably with respect to state-of-the-art multimodal fusion\nmodels for both VQA and VRD tasks. Our code is available at\nhttps://github.com/Cadene/block.bootstrap.pytorch.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 19:10:14 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 10:54:56 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Ben-younes", "Hedi", ""], ["Cadene", "R\u00e9mi", ""], ["Thome", "Nicolas", ""], ["Cord", "Matthieu", ""]]}, {"id": "1902.00100", "submitter": "Kyle Luther", "authors": "Kyle Luther, H. Sebastian Seung", "title": "Learning Metric Graphs for Neuron Segmentation In Electron Microscopy\n  Images", "comments": "5 pages, Accepted at IEEE ISBI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the deep metric learning approach to image segmentation, a convolutional\nnet densely generates feature vectors at the pixels of an image. Pairs of\nfeature vectors are trained to be similar or different, depending on whether\nthe corresponding pixels belong to same or different ground truth segments. To\nsegment a new image, the feature vectors are computed and clustered. Both\nempirically and theoretically, it is unclear whether or when deep metric\nlearning is superior to the more conventional approach of directly predicting\nan affinity graph with a convolutional net. We compare the two approaches using\nbrain images from serial section electron microscopy images, which constitute\nan especially challenging example of instance segmentation. We first show that\nseed-based postprocessing of the feature vectors, as originally proposed,\nproduces inferior accuracy because it is difficult for the convolutional net to\npredict feature vectors that remain uniform across large objects. Then we\nconsider postprocessing by thresholding a nearest neighbor graph followed by\nconnected components. In this case, segmentations from a \"metric graph\" turn\nout to be competitive or even superior to segmentations from a directly\npredicted affinity graph. To explain these findings theoretically, we invoke\nthe property that the metric function satisfies the triangle inequality. Then\nwe show with an example where this constraint suppresses noise, causing\nconnected components to more robustly segment a metric graph than an\nunconstrained affinity graph.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 22:16:58 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Luther", "Kyle", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1902.00105", "submitter": "Caixia Zhang", "authors": "Bo wang, Hao Hu, Caixia Zhang", "title": "Geometric Interpretation of side-sharing and point-sharing solutions in\n  the P3P Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the P3P problem could have 1, 2, 3 and at most 4\npositive solutions under different configurations among its 3 control points\nand the position of the optical center. Since in any real applications, the\nknowledge on the exact number of possible solutions is a prerequisite for\nselecting the right one among all the possible solutions, the study on the\nphenomenon of multiple solutions in the P3P problem has been an active topic .\nIn this work, we provide some new geometric interpretations on the\nmulti-solution phenomenon in the P3P problem, our main results include: (1):\nThe necessary and sufficient condition for the P3P problem to have a pair of\nside-sharing solutions is the two optical centers of the solutions both lie on\none of the 3 vertical planes to the base plane of control points; (2): The\nnecessary and sufficient condition for the P3P problem to have a pair of\npoint-sharing solutions is the two optical centers of the solutions both lie on\none of the 3 so-called skewed danger cylinders;(3): If the P3P problem has\nother solutions in addition to a pair of side-sharing ( point-sharing)\nsolutions, these remaining solutions must be a point-sharing ( side-sharing )\npair. In a sense, the side-sharing pair and the point-sharing pair are\ncompanion pairs. In sum, our results provide some new insights into the nature\nof the multi-solution phenomenon in the P3P problem, in addition to their\nacademic value, they could also be used as some theoretical guidance for\npractitioners in real applications to avoid occurrence of multiple solutions by\nproperly arranging the control points.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 18:41:34 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["wang", "Bo", ""], ["Hu", "Hao", ""], ["Zhang", "Caixia", ""]]}, {"id": "1902.00113", "submitter": "Da Li", "authors": "Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song and Timothy\n  M. Hospedales", "title": "Episodic Training for Domain Generalization", "comments": "ICCV'19 CR version and fix Table 5. Code is now available at\n  https://github.com/HAHA-DL/Episodic-DG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain generalization (DG) is the challenging and topical problem of learning\nmodels that generalize to novel testing domains with different statistics than\na set of known training domains. The simple approach of aggregating data from\nall source domains and training a single deep neural network end-to-end on all\nthe data provides a surprisingly strong baseline that surpasses many prior\npublished methods. In this paper, we build on this strong baseline by designing\nan episodic training procedure that trains a single deep network in a way that\nexposes it to the domain shift that characterises a novel domain at runtime.\nSpecifically, we decompose a deep network into feature extractor and classifier\ncomponents, and then train each component by simulating it interacting with a\npartner who is badly tuned for the current domain. This makes both components\nmore robust, ultimately leading to our networks producing state-of-the-art\nperformance on three DG benchmarks. Furthermore, we consider the pervasive\nworkflow of using an ImageNet trained CNN as a fixed feature extractor for\ndownstream recognition tasks. Using the Visual Decathlon benchmark, we\ndemonstrate that our episodic-DG training improves the performance of such a\ngeneral-purpose feature extractor by explicitly training a feature for\nrobustness to novel problems. This shows that DG training can benefit standard\npractice in computer vision.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 22:45:51 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 15:27:04 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 12:31:57 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Li", "Da", ""], ["Zhang", "Jianshu", ""], ["Yang", "Yongxin", ""], ["Liu", "Cong", ""], ["Song", "Yi-Zhe", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1902.00125", "submitter": "Zhaoyang Xu", "authors": "Zhaoyang Xu, Faranak Sobhani, Carlos Fernandez Moro and Qianni Zhang", "title": "US-net for robust and efficient nuclei instance segmentation", "comments": "To appear in ISBI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel neural network architecture, US-Net, for robust nuclei\ninstance segmentation in histopathology images. The proposed framework\nintegrates the nuclei detection and segmentation networks by sharing their\noutputs through the same foundation network, and thus enhancing the performance\nof both. The detection network takes into account the high-level semantic cues\nwith contextual information, while the segmentation network focuses more on the\nlow-level details like the edges. Extensive experiments reveal that our\nproposed framework can strengthen the performance of both branch networks in an\nintegrated architecture and outperforms most of the state-of-the-art nuclei\ndetection and segmentation networks.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 23:27:52 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Xu", "Zhaoyang", ""], ["Sobhani", "Faranak", ""], ["Moro", "Carlos Fernandez", ""], ["Zhang", "Qianni", ""]]}, {"id": "1902.00153", "submitter": "Yue Cao", "authors": "Bin Liu, Yue Cao, Mingsheng Long, Jianmin Wang, Jingdong Wang", "title": "Deep Triplet Quantization", "comments": "Accepted by ACM Multimedia 2018 as oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep hashing establishes efficient and effective image retrieval by\nend-to-end learning of deep representations and hash codes from similarity\ndata. We present a compact coding solution, focusing on deep learning to\nquantization approach that has shown superior performance over hashing\nsolutions for similarity retrieval. We propose Deep Triplet Quantization (DTQ),\na novel approach to learning deep quantization models from the similarity\ntriplets. To enable more effective triplet training, we design a new triplet\nselection approach, Group Hard, that randomly selects hard triplets in each\nimage group. To generate compact binary codes, we further apply a triplet\nquantization with weak orthogonality during triplet training. The quantization\nloss reduces the codebook redundancy and enhances the quantizability of deep\nrepresentations through back-propagation. Extensive experiments demonstrate\nthat DTQ can generate high-quality and compact binary codes, which yields\nstate-of-the-art image retrieval performance on three benchmark datasets,\nNUS-WIDE, CIFAR-10, and MS-COCO.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 02:29:35 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Liu", "Bin", ""], ["Cao", "Yue", ""], ["Long", "Mingsheng", ""], ["Wang", "Jianmin", ""], ["Wang", "Jingdong", ""]]}, {"id": "1902.00163", "submitter": "Mengmi Zhang", "authors": "Mengmi Zhang, Claire Tseng, Karla Montejo, Joseph Kwon, Gabriel\n  Kreiman", "title": "Lift-the-flap: what, where and when for context reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context reasoning is critical in a wide variety of applications where current\ninputs need to be interpreted in the light of previous experience and\nknowledge. Both spatial and temporal contextual information play a critical\nrole in the domain of visual recognition. Here we investigate spatial\nconstraints (what image features provide contextual information and where they\nare located), and temporal constraints (when different contextual cues matter)\nfor visual recognition. The task is to reason about the scene context and infer\nwhat a target object hidden behind a flap is in a natural image. To tackle this\nproblem, we first describe an online human psychophysics experiment recording\nactive sampling via mouse clicks in lift-the-flap games and identify clicking\npatterns and features which are diagnostic for high contextual reasoning\naccuracy. As a proof of the usefulness of these clicking patterns and visual\nfeatures, we extend a state-of-the-art recurrent model capable of attending to\nsalient context regions, dynamically integrating useful information, making\ninferences, and predicting class label for the target object over multiple\nclicks. The proposed model achieves human-level contextual reasoning accuracy,\nshares human-like sampling behavior and learns interpretable features for\ncontextual reasoning.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 03:37:17 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 01:34:56 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Zhang", "Mengmi", ""], ["Tseng", "Claire", ""], ["Montejo", "Karla", ""], ["Kwon", "Joseph", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "1902.00173", "submitter": "Kentaro Yoshioka", "authors": "Kentaro Yoshioka, Edward Lee, Simon Wong, Mark Horowitz", "title": "Dataset Culling: Towards Efficient Training Of Distillation-Based Domain\n  Specific Models", "comments": "accepted to IEEE ICIP 2019. 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time CNN-based object detection models for applications like\nsurveillance can achieve high accuracy but are computationally expensive.\nRecent works have shown 10 to 100x reduction in computation cost for inference\nby using domain-specific networks. However, prior works have focused on\ninference only. If the domain model requires frequent retraining, training\ncosts can pose a significant bottleneck. To address this, we propose Dataset\nCulling: a pipeline to reduce the size of the dataset for training, based on\nthe prediction difficulty. Images that are easy to classify are filtered out\nsince they contribute little to improving the accuracy. The difficulty is\nmeasured using our proposed confidence loss metric with little computational\noverhead. Dataset Culling is extended to optimize the image resolution to\nfurther improve training and inference costs. We develop fixed-angle,\nlong-duration video datasets across several domains, and we show that the\ndataset size can be culled by a factor of 300x to reduce the total training\ntime by 47x with no accuracy loss or even with slight improvement. Codes are\navailable: https://github.com/kentaroy47/DatasetCulling\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 04:23:32 GMT"}, {"version": "v2", "created": "Sun, 10 Feb 2019 08:52:34 GMT"}, {"version": "v3", "created": "Thu, 16 May 2019 09:30:16 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Yoshioka", "Kentaro", ""], ["Lee", "Edward", ""], ["Wong", "Simon", ""], ["Horowitz", "Mark", ""]]}, {"id": "1902.00176", "submitter": "Dominique Beaini", "authors": "Dominique Beaini, Sofiane Achiche, Fabrice Nonez, Olivier Brochu\n  Dufour, C\\'edric Leblond-M\\'enard, Mahdis Asaadi, Maxime Raison", "title": "Fast and Optimal Laplacian Solver for Gradient-Domain Image Editing\n  using Green Function Convolution", "comments": "17 pages, single column scientific paper. Patent submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In computer vision, the gradient and Laplacian of an image are used in\ndifferent applications, such as edge detection, feature extraction, and\nseamless image cloning. Computing the gradient of an image is straightforward\nsince numerical derivatives are available in most computer vision toolboxes.\nHowever, the reverse problem is more difficult, since computing an image from\nits gradient requires to solve the Laplacian equation, also called Poisson\nequation. Current discrete methods are either slow or require heavy parallel\ncomputing. The objective of this paper is to present a novel fast and robust\nmethod of solving the image gradient or Laplacian with minimal error, which can\nbe used for gradient domain editing. By using a single convolution based on a\nnumerical Green's function, the whole process is faster and straightforward to\nimplement with different computer vision libraries. It can also be optimized on\na GPU using fast Fourier transforms and can easily be generalized for an n\ndimension image. The tests show that, for images of resolution 801x1200, the\nproposed GFC can solve 100 Laplacian in parallel in around 1.0 milliseconds ms.\nThis is orders of magnitude faster than our nearest competitor which requires\n294ms for a single image. Furthermore, we prove mathematically and demonstrate\nempirically that the proposed method is the least error solver for gradient\ndomain editing. The developed method is also validated with examples of Poisson\nblending, gradient removal, and the proposed gradient domain merging GDM.\nFinally, we present how the GDM can be leveraged in future works for\nconvolutional neural networks CNN.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 04:36:56 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 00:41:44 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Beaini", "Dominique", ""], ["Achiche", "Sofiane", ""], ["Nonez", "Fabrice", ""], ["Dufour", "Olivier Brochu", ""], ["Leblond-M\u00e9nard", "C\u00e9dric", ""], ["Asaadi", "Mahdis", ""], ["Raison", "Maxime", ""]]}, {"id": "1902.00220", "submitter": "Ruixin Zhang", "authors": "Qiuyu Zhu, Ruixin Zhang", "title": "A Classification Supervised Auto-Encoder Based on Predefined\n  Evenly-Distributed Class Centroids", "comments": "16 pages,12 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classic variational autoencoders are used to learn complex data\ndistributions, that are built on standard function approximators. Especially,\nVAE has shown promise on a lot of complex task. In this paper, a new\nautoencoder model - classification supervised autoencoder (CSAE) based on\npredefined evenly-distributed class centroids (PEDCC) is proposed. Our method\nuses PEDCC of latent variables to train the network to ensure the maximization\nof inter-class distance and the minimization of inner-class distance. Instead\nof learning mean/variance of latent variables distribution and taking\nreparameterization of VAE, latent variables of CSAE are directly used to\nclassify and as input of decoder. In addition, a new loss function is proposed\nto combine the loss function of classification. Based on the basic structure of\nthe universal autoencoder, we realized the comprehensive optimal results of\nencoding, decoding, classification, and good model generalization performance\nat the same time. Theoretical advantages are reflected in experimental results.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 08:26:47 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 05:29:34 GMT"}, {"version": "v3", "created": "Fri, 10 Jan 2020 06:34:30 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Zhu", "Qiuyu", ""], ["Zhang", "Ruixin", ""]]}, {"id": "1902.00236", "submitter": "Yuval Bahat", "authors": "Yuval Bahat, Michal Irani and Gregory Shakhnarovich", "title": "Natural and Adversarial Error Detection using Invariance to Image\n  Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to distinguish between correct and incorrect image\nclassifications. Our approach can detect misclassifications which either occur\n$\\it{unintentionally}$ (\"natural errors\"), or due to\n$\\it{intentional~adversarial~attacks}$ (\"adversarial errors\"), both in a single\n$\\it{unified~framework}$. Our approach is based on the observation that\ncorrectly classified images tend to exhibit robust and consistent\nclassifications under certain image transformations (e.g., horizontal flip,\nsmall image translation, etc.). In contrast, incorrectly classified images\n(whether due to adversarial errors or natural errors) tend to exhibit large\nvariations in classification results under such transformations. Our approach\ndoes not require any modifications or retraining of the classifier, hence can\nbe applied to any pre-trained classifier. We further use state of the art\ntargeted adversarial attacks to demonstrate that even when the adversary has\nfull knowledge of our method, the adversarial distortion needed for bypassing\nour detector is $\\it{no~longer~imperceptible~to~the~human~eye}$. Our approach\nobtains state-of-the-art results compared to previous adversarial detection\nmethods, surpassing them by a large margin.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 09:00:54 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Bahat", "Yuval", ""], ["Irani", "Michal", ""], ["Shakhnarovich", "Gregory", ""]]}, {"id": "1902.00267", "submitter": "Shreyank N Gowda", "authors": "Shreyank N Gowda and Chun Yuan", "title": "ColorNet: Investigating the importance of color spaces for image\n  classification", "comments": null, "journal-ref": "Asian Conference on Computer Vision 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification is a fundamental application in computer vision.\nRecently, deeper networks and highly connected networks have shown state of the\nart performance for image classification tasks. Most datasets these days\nconsist of a finite number of color images. These color images are taken as\ninput in the form of RGB images and classification is done without modifying\nthem. We explore the importance of color spaces and show that color spaces\n(essentially transformations of original RGB images) can significantly affect\nclassification accuracy. Further, we show that certain classes of images are\nbetter represented in particular color spaces and for a dataset with a highly\nvarying number of classes such as CIFAR and Imagenet, using a model that\nconsiders multiple color spaces within the same model gives excellent levels of\naccuracy. Also, we show that such a model, where the input is preprocessed into\nmultiple color spaces simultaneously, needs far fewer parameters to obtain high\naccuracy for classification. For example, our model with 1.75M parameters\nsignificantly outperforms DenseNet 100-12 that has 12M parameters and gives\nresults comparable to Densenet-BC-190-40 that has 25.6M parameters for\nclassification of four competitive image classification datasets namely:\nCIFAR-10, CIFAR-100, SVHN and Imagenet. Our model essentially takes an RGB\nimage as input, simultaneously converts the image into 7 different color spaces\nand uses these as inputs to individual densenets. We use small and wide\ndensenets to reduce computation overhead and number of hyperparameters\nrequired. We obtain significant improvement on current state of the art results\non these datasets as well.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 10:35:18 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Gowda", "Shreyank N", ""], ["Yuan", "Chun", ""]]}, {"id": "1902.00274", "submitter": "Antonio Mazza", "authors": "Antonio Mazza, Francescopaolo Sica", "title": "Deep Learning Solutions for TanDEM-X-based Forest Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, deep learning (DL) has been successfully and massively\nemployed in computer vision for discriminative tasks, such as image\nclassification or object detection. This kind of problems are core to many\nremote sensing (RS) applications as well, though with domain-specific\npeculiarities. Therefore, there is a growing interest on the use of DL methods\nfor RS tasks. Here, we consider the forest/non-forest classification problem\nwith TanDEM-X data, and test two state-of-the-art DL models, suitably adapting\nthem to the specific task. Our experiments confirm the great potential of DL\nmethods for RS applications.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 11:12:30 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Mazza", "Antonio", ""], ["Sica", "Francescopaolo", ""]]}, {"id": "1902.00293", "submitter": "Wouter Van Gansbeke", "authors": "Wouter Van Gansbeke, Bert De Brabandere, Davy Neven, Marc Proesmans,\n  Luc Van Gool", "title": "End-to-end Lane Detection through Differentiable Least-Squares Fitting", "comments": "Accepted at ICCVW 2019 (CVRSUAD-Road Scene Understanding and\n  Autonomous Driving)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lane detection is typically tackled with a two-step pipeline in which a\nsegmentation mask of the lane markings is predicted first, and a lane line\nmodel (like a parabola or spline) is fitted to the post-processed mask next.\nThe problem with such a two-step approach is that the parameters of the network\nare not optimized for the true task of interest (estimating the lane curvature\nparameters) but for a proxy task (segmenting the lane markings), resulting in\nsub-optimal performance. In this work, we propose a method to train a lane\ndetector in an end-to-end manner, directly regressing the lane parameters. The\narchitecture consists of two components: a deep network that predicts a\nsegmentation-like weight map for each lane line, and a differentiable\nleast-squares fitting module that returns for each map the parameters of the\nbest-fitting curve in the weighted least-squares sense. These parameters can\nsubsequently be supervised with a loss function of choice. Our method relies on\nthe observation that it is possible to backpropagate through a least-squares\nfitting procedure. This leads to an end-to-end method where the features are\noptimized for the true task of interest: the network implicitly learns to\ngenerate features that prevent instabilities during the model fitting step, as\nopposed to two-step pipelines that need to handle outliers with heuristics.\nAdditionally, the system is not just a black box but offers a degree of\ninterpretability because the intermediately generated segmentation-like weight\nmaps can be inspected and visualized. Code and a video is available at\ngithub.com/wvangansbeke/LaneDetection_End2End.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 12:05:47 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 18:02:44 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 07:57:31 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Van Gansbeke", "Wouter", ""], ["De Brabandere", "Bert", ""], ["Neven", "Davy", ""], ["Proesmans", "Marc", ""], ["Van Gool", "Luc", ""]]}, {"id": "1902.00301", "submitter": "Oleksii Sidorov", "authors": "Oleksii Sidorov, Jon Yngve Hardeberg", "title": "Deep Hyperspectral Prior: Denoising, Inpainting, Super-Resolution", "comments": "Published in ICCV 2019 Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning algorithms have demonstrated state-of-the-art performance in\nvarious tasks of image restoration. This was made possible through the ability\nof CNNs to learn from large exemplar sets. However, the latter becomes an issue\nfor hyperspectral image processing where datasets commonly consist of just a\nfew images. In this work, we propose a new approach to denoising, inpainting,\nand super-resolution of hyperspectral image data using intrinsic properties of\na CNN without any training. The performance of the given algorithm is shown to\nbe comparable to the performance of trained networks, while its application is\nnot restricted by the availability of training data. This work is an extension\nof original \"deep prior\" algorithm to HSI domain and 3D-convolutional networks.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 12:20:38 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 06:55:58 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Sidorov", "Oleksii", ""], ["Hardeberg", "Jon Yngve", ""]]}, {"id": "1902.00311", "submitter": "Oleksii Sidorov", "authors": "Oleksii Sidorov, Congcong Wang, Faouzi Alaya Cheikh", "title": "Generative Smoke Removal", "comments": "Presented at NeurIPS Workshop and published in Proceedings of Machine\n  Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In minimally invasive surgery, the use of tissue dissection tools causes\nsmoke, which inevitably degrades the image quality. This could reduce the\nvisibility of the operation field for surgeons and introduces errors for the\ncomputer vision algorithms used in surgical navigation systems. In this paper,\nwe propose a novel approach for computational smoke removal using supervised\nimage-to-image translation. We demonstrate that straightforward application of\nexisting generative algorithms allows removing smoke but decreases image\nquality and introduces synthetic noise (grid-structure). Thus, we propose to\nsolve this issue by modification of GAN's architecture and adding perceptual\nimage quality metric to the loss function. Obtained results demonstrate that\nproposed method efficiently removes smoke as well as preserves perceptually\nsufficient image quality.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 13:05:10 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 06:47:45 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Sidorov", "Oleksii", ""], ["Wang", "Congcong", ""], ["Cheikh", "Faouzi Alaya", ""]]}, {"id": "1902.00313", "submitter": "Yuanzhi Liang", "authors": "Yuanzhi Liang, Yalong Bai, Wei Zhang, Xueming Qian, Li Zhu, Tao Mei", "title": "VrR-VG: Refocusing Visually-Relevant Relationships", "comments": "Accepted by ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relationships encode the interactions among individual instances, and play a\ncritical role in deep visual scene understanding. Suffering from the high\npredictability with non-visual information, existing methods tend to fit the\nstatistical bias rather than ``learning'' to ``infer'' the relationships from\nimages. To encourage further development in visual relationships, we propose a\nnovel method to automatically mine more valuable relationships by pruning\nvisually-irrelevant ones. We construct a new scene-graph dataset named\nVisually-Relevant Relationships Dataset (VrR-VG) based on Visual Genome.\nCompared with existing datasets, the performance gap between learnable and\nstatistical method is more significant in VrR-VG, and frequency-based analysis\ndoes not work anymore. Moreover, we propose to learn a relationship-aware\nrepresentation by jointly considering instances, attributes and relationships.\nBy applying the representation-aware feature learned on VrR-VG, the\nperformances of image captioning and visual question answering are\nsystematically improved with a large margin, which demonstrates the gain of our\ndataset and the features embedding schema. VrR-VG is available via\nhttp://vrr-vg.com/.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 13:10:05 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 07:24:33 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Liang", "Yuanzhi", ""], ["Bai", "Yalong", ""], ["Zhang", "Wei", ""], ["Qian", "Xueming", ""], ["Zhu", "Li", ""], ["Mei", "Tao", ""]]}, {"id": "1902.00334", "submitter": "Aythami Morales", "authors": "Aythami Morales, Julian Fierrez, Ruben Vera-Rodriguez, Ruben Tolosana", "title": "SensitiveNets: Learning Agnostic Representations with Application to\n  Face Images", "comments": "Accepted in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a novel privacy-preserving neural network feature\nrepresentation to suppress the sensitive information of a learned space while\nmaintaining the utility of the data. The new international regulation for\npersonal data protection forces data controllers to guarantee privacy and avoid\ndiscriminative hazards while managing sensitive data of users. In our approach,\nprivacy and discrimination are related to each other. Instead of existing\napproaches aimed directly at fairness improvement, the proposed feature\nrepresentation enforces the privacy of selected attributes. This way fairness\nis not the objective, but the result of a privacy-preserving learning method.\nThis approach guarantees that sensitive information cannot be exploited by any\nagent who process the output of the model, ensuring both privacy and equality\nof opportunity. Our method is based on an adversarial regularizer that\nintroduces a sensitive information removal function in the learning objective.\nThe method is evaluated on three different primary tasks (identity,\nattractiveness, and smiling) and three publicly available benchmarks. In\naddition, we present a new face annotation dataset with balanced distribution\nbetween genders and ethnic origins. The experiments demonstrate that it is\npossible to improve the privacy and equality of opportunity while retaining\ncompetitive performance independently of the task.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 14:01:43 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 08:41:21 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 19:44:13 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Morales", "Aythami", ""], ["Fierrez", "Julian", ""], ["Vera-Rodriguez", "Ruben", ""], ["Tolosana", "Ruben", ""]]}, {"id": "1902.00347", "submitter": "Markus Haltmeier", "authors": "Christoph Angermann, Markus Haltmeier, Ruth Steiger, Sergiy\n  Pereverzyev Jr, Elke Gizewski", "title": "Projection-Based 2.5D U-net Architecture for Fast Volumetric\n  Segmentation", "comments": "presented at the SAMPTA 2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks are state-of-the-art for various segmentation\ntasks. While for 2D images these networks are also computationally efficient,\n3D convolutions have huge storage requirements and require long training time.\nTo overcome this issue, we introduce a network structure for volumetric data\nwithout 3D convolutional layers. The main idea is to include maximum intensity\nprojections from different directions to transform the volumetric data to a\nsequence of images, where each image contains information of the full data. We\nthen apply 2D convolutions to these projection images and lift them again to\nvolumetric data using a trainable reconstruction algorithm.The proposed network\narchitecture has less storage requirements than network structures using 3D\nconvolutions. For a tested binary segmentation task, it even shows better\nperformance than the 3D U-net and can be trained much faster.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 14:19:00 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 16:34:40 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Angermann", "Christoph", ""], ["Haltmeier", "Markus", ""], ["Steiger", "Ruth", ""], ["Pereverzyev", "Sergiy", "Jr"], ["Gizewski", "Elke", ""]]}, {"id": "1902.00369", "submitter": "Yongpei Zhu", "authors": "Yongpei Zhu, Xuesheng Zhang, Kehong Yuan", "title": "Medical Image Super-Resolution Using a Generative Adversarial Network", "comments": "5 pages,2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the growing popularity of electronic medical records, electronic\nmedical record (EMR) data has exploded increasingly. It is very meaningful to\nretrieve high quality EMR in mass data. In this paper, an EMR value network\nwith retrieval function is constructed by taking stroke disease as the research\nobject. It mainly includes: 1) It establishes the electronic medical record\ndatabase and corresponding stroke knowledge graph. 2) The strategy of\nsimilarity measurement is included three parts(patients' chief complaint,\npathology results and medical images). Patients' chief complaints are text\ndata, mainly describing patients' symptoms and expressed in words or phrases,\nand patients' chief complaints are input in independent tick of various\nsymptoms. The data of the pathology results is a structured and digitized\nexpression, so the input method is the same as the patient's chief complaint;\nImage similarity adopts content-based image retrieval(CBIR) technology. 3) The\nanalytic hierarchy process (AHP) is used to establish the weights of the three\ntypes of data and then synthesize them into an indicator. The accuracy rate of\nsimilarity in top 5 was more than 85\\% based on EMR database with more 200\nstroke records using leave-one-out method. It will be the good tool for\nassistant diagnosis and doctor training, as good quality records are colleted\ninto the databases, like Doctor Watson, in the future.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 11:14:55 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 05:37:47 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 03:55:07 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Zhu", "Yongpei", ""], ["Zhang", "Xuesheng", ""], ["Yuan", "Kehong", ""]]}, {"id": "1902.00378", "submitter": "Yash Patel", "authors": "Yash Patel, Lluis Gomez, Mar\\c{c}al Rusi\\~nol, Dimosthenis Karatzas,\n  C.V. Jawahar", "title": "Self-Supervised Visual Representations for Cross-Modal Retrieval", "comments": "arXiv admin note: text overlap with arXiv:1807.02110", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal retrieval methods have been significantly improved in last years\nwith the use of deep neural networks and large-scale annotated datasets such as\nImageNet and Places. However, collecting and annotating such datasets requires\na tremendous amount of human effort and, besides, their annotations are usually\nlimited to discrete sets of popular visual classes that may not be\nrepresentative of the richer semantics found on large-scale cross-modal\nretrieval datasets. In this paper, we present a self-supervised cross-modal\nretrieval framework that leverages as training data the correlations between\nimages and text on the entire set of Wikipedia articles. Our method consists in\ntraining a CNN to predict: (1) the semantic context of the article in which an\nimage is more probable to appear as an illustration (global context), and (2)\nthe semantic context of its caption (local context). Our experiments\ndemonstrate that the proposed method is not only capable of learning\ndiscriminative visual representations for solving vision tasks like image\nclassification and object detection, but that the learned representations are\nbetter for cross-modal retrieval when compared to supervised pre-training of\nthe network on the ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 09:17:07 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Patel", "Yash", ""], ["Gomez", "Lluis", ""], ["Rusi\u00f1ol", "Mar\u00e7al", ""], ["Karatzas", "Dimosthenis", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1902.00383", "submitter": "Xiaofang Wang", "authors": "Shengcao Cao, Xiaofang Wang, Kris M. Kitani", "title": "Learnable Embedding Space for Efficient Neural Architecture Compression", "comments": "ICLR 2019 - Code available here:\n  https://github.com/Friedrich1006/ESNAC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to incrementally learn an embedding space over the domain\nof network architectures, to enable the careful selection of architectures for\nevaluation during compressed architecture search. Given a teacher network, we\nsearch for a compressed network architecture by using Bayesian Optimization\n(BO) with a kernel function defined over our proposed embedding space to select\narchitectures for evaluation. We demonstrate that our search algorithm can\nsignificantly outperform various baseline methods, such as random search and\nreinforcement learning (Ashok et al., 2018). The compressed architectures found\nby our method are also better than the state-of-the-art manually-designed\ncompact architecture ShuffleNet (Zhang et al., 2018). We also demonstrate that\nthe learned embedding space can be transferred to new settings for architecture\nsearch, such as a larger teacher network or a teacher network in a different\narchitecture family, without any training. Code is publicly available here:\nhttps://github.com/Friedrich1006/ESNAC .\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 14:54:17 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 15:03:04 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Cao", "Shengcao", ""], ["Wang", "Xiaofang", ""], ["Kitani", "Kris M.", ""]]}, {"id": "1902.00386", "submitter": "Thomas Sanchez", "authors": "Thomas Sanchez and Baran G\\\"ozc\\\"u and Ruud B. van Heeswijk and Armin\n  Eftekhari and Efe Il{\\i}cak and Tolga \\c{C}ukur and Volkan Cevher", "title": "Scalable Learning-Based Sampling Optimization for Compressive Dynamic\n  MRI", "comments": "13 pages, 16 figures, ICASSP 2020 - Session on \"Learning and\n  Optimization in Non-Convex Environments\". Code available at\n  https://github.com/t-sanchez/stochasticGreedyMRI.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing applied to magnetic resonance imaging (MRI) allows to\nreduce the scanning time by enabling images to be reconstructed from highly\nundersampled data. In this paper, we tackle the problem of designing a sampling\nmask for an arbitrary reconstruction method and a limited acquisition budget.\nNamely, we look for an optimal probability distribution from which a mask with\na fixed cardinality is drawn. We demonstrate that this problem admits a\ncompactly supported solution, which leads to a deterministic optimal sampling\nmask. We then propose a stochastic greedy algorithm that (i) provides an\napproximate solution to this problem, and (ii) resolves the scaling issues of\n[1,2]. We validate its performance on in vivo dynamic MRI with retrospective\nundersampling, showing that our method preserves the performance of [1,2] while\nreducing the computational burden by a factor close to 200.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 14:58:24 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 09:40:18 GMT"}, {"version": "v3", "created": "Wed, 20 Feb 2019 07:12:49 GMT"}, {"version": "v4", "created": "Fri, 18 Oct 2019 06:55:28 GMT"}, {"version": "v5", "created": "Mon, 16 Mar 2020 13:15:46 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Sanchez", "Thomas", ""], ["G\u00f6zc\u00fc", "Baran", ""], ["van Heeswijk", "Ruud B.", ""], ["Eftekhari", "Armin", ""], ["Il\u0131cak", "Efe", ""], ["\u00c7ukur", "Tolga", ""], ["Cevher", "Volkan", ""]]}, {"id": "1902.00423", "submitter": "Bj\\\"orn Barz", "authors": "Bj\\\"orn Barz, Joachim Denzler", "title": "Do We Train on Test Data? Purging CIFAR of Near-Duplicates", "comments": "Journal of Imaging", "journal-ref": "Journal of Imaging. 2020; 6(6):41", "doi": "10.3390/jimaging6060041", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CIFAR-10 and CIFAR-100 datasets are two of the most heavily benchmarked\ndatasets in computer vision and are often used to evaluate novel methods and\nmodel architectures in the field of deep learning. However, we find that 3.3%\nand 10% of the images from the test sets of these datasets have duplicates in\nthe training set. These duplicates are easily recognizable by memorization and\nmay, hence, bias the comparison of image recognition techniques regarding their\ngeneralization capability. To eliminate this bias, we provide the \"fair CIFAR\"\n(ciFAIR) dataset, where we replaced all duplicates in the test sets with new\nimages sampled from the same domain. We then re-evaluate the classification\nperformance of various popular state-of-the-art CNN architectures on these new\ntest sets to investigate whether recent research has overfitted to memorizing\ndata instead of learning abstract concepts. We find a significant drop in\nclassification accuracy of between 9% and 14% relative to the original\nperformance on the duplicate-free test set. The ciFAIR dataset and pre-trained\nmodels are available at https://cvjena.github.io/cifair/, where we also\nmaintain a leaderboard.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 16:00:34 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 16:29:07 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Barz", "Bj\u00f6rn", ""], ["Denzler", "Joachim", ""]]}, {"id": "1902.00469", "submitter": "Andrawes Al Bahou", "authors": "Andrawes Al Bahou, Christine Tanner, Orcun Goksel", "title": "SCATGAN for Reconstruction of Ultrasound Scatterers Using Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational simulation of ultrasound (US) echography is essential for\ntraining sonographers. Realistic simulation of US interaction with microscopic\ntissue structures is often modeled by a tissue representation in the form of\npoint scatterers, convolved with a spatially varying point spread function.\nThis yields a realistic US B-mode speckle texture, given that a scatterer\nrepresentation for a particular tissue type is readily available. This is often\nnot the case and scatterers are nontrivial to determine. In this work we\npropose to estimate scatterer maps from sample US B-mode images of a tissue, by\nformulating this inverse mapping problem as image translation, where we learn\nthe mapping with Generative Adversarial Networks, using a US simulation\nsoftware for training. We demonstrate robust reconstruction results, invariant\nto US viewing and imaging settings such as imaging direction and center\nfrequency. Our method is shown to generalize beyond the trained imaging\nsettings, demonstrated on in-vivo US data. Our inference runs orders of\nmagnitude faster than optimization-based techniques, enabling future extensions\nfor reconstructing 3D B-mode volumes with only linear computational complexity.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 17:38:17 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Bahou", "Andrawes Al", ""], ["Tanner", "Christine", ""], ["Goksel", "Orcun", ""]]}, {"id": "1902.00487", "submitter": "Dongfang Yang", "authors": "Dongfang Yang, Linhui Li, Keith Redmill, and \\\"Umit \\\"Ozg\\\"uner", "title": "Top-view Trajectories: A Pedestrian Dataset of Vehicle-Crowd Interaction\n  from Controlled Experiments and Crowded Campus", "comments": "This paper was accepted into the 30th IEEE Intelligent Vehicles\n  Symposium. Personal use of this material is permitted. Permission from IEEE\n  must be obtained for all other uses", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the collective motion of a group of pedestrians (a crowd) under\nthe vehicle influence is essential for the development of autonomous vehicles\nto deal with mixed urban scenarios where interpersonal interaction and\nvehicle-crowd interaction (VCI) are significant. This usually requires a model\nthat can describe individual pedestrian motion under the influence of nearby\npedestrians and the vehicle. This study proposed two pedestrian trajectory\ndatasets, CITR dataset and DUT dataset, so that the pedestrian motion models\ncan be further calibrated and verified, especially when vehicle influence on\npedestrians plays an important role. CITR dataset consists of experimentally\ndesigned fundamental VCI scenarios (front, back, and lateral VCIs) and provides\nunique ID for each pedestrian, which is suitable for exploring a specific\naspect of VCI. DUT dataset gives two ordinary and natural VCI scenarios in\ncrowded university campus, which can be used for more general purpose VCI\nexploration. The trajectories of pedestrians, as well as vehicles, were\nextracted by processing video frames that come from a down-facing camera\nmounted on a hovering drone as the recording equipment. The final trajectories\nof pedestrians and vehicles were refined by Kalman filters with linear\npoint-mass model and nonlinear bicycle model, respectively, in which\nxy-velocity of pedestrians and longitudinal speed and orientation of vehicles\nwere estimated. The statistics of the velocity magnitude distribution\ndemonstrated the validity of the proposed dataset. In total, there are\napproximate 340 pedestrian trajectories in CITR dataset and 1793 pedestrian\ntrajectories in DUT dataset. The dataset is available at GitHub.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 18:14:35 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 19:48:12 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Yang", "Dongfang", ""], ["Li", "Linhui", ""], ["Redmill", "Keith", ""], ["\u00d6zg\u00fcner", "\u00dcmit", ""]]}, {"id": "1902.00505", "submitter": "Aj Piergiovanni", "authors": "AJ Piergiovanni, Anelia Angelova, Michael S. Ryoo", "title": "Differentiable Grammars for Videos", "comments": null, "journal-ref": "AAAI-2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel algorithm which learns a formal regular grammar\nfrom real-world continuous data, such as videos. Learning latent terminals,\nnon-terminals, and production rules directly from continuous data allows the\nconstruction of a generative model capturing sequential structures with\nmultiple possibilities. Our model is fully differentiable, and provides easily\ninterpretable results which are important in order to understand the learned\nstructures. It outperforms the state-of-the-art on several challenging datasets\nand is more accurate for forecasting future activities in videos. We plan to\nopen-source the code. https://sites.google.com/view/differentiable-grammars\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 18:58:18 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 16:38:35 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Piergiovanni", "AJ", ""], ["Angelova", "Anelia", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "1902.00536", "submitter": "Jue Jiang Dr.", "authors": "Peter Klages, Ilyes Benslimane, Sadegh Riyahi, Jue Jiang, Margie Hunt,\n  Joe Deasy, Harini Veeraraghavan, Neelam Tyagi", "title": "Comparison of Patch-Based Conditional Generative Adversarial Neural Net\n  Models with Emphasis on Model Robustness for Use in Head and Neck Cases for\n  MR-Only planning", "comments": "submitted to PMB", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A total of twenty paired CT and MR images were used in this study to\ninvestigate two conditional generative adversarial networks, Pix2Pix, and Cycle\nGAN, for generating synthetic CT images for Headand Neck cancer cases. Ten of\nthe patient cases were used for training and included such common artifacts as\ndental implants; the remaining ten testing cases were used for testing and\nincluded a larger range of image features commonly found in clinical head and\nneck cases. These features included strong metal artifacts from dental\nimplants, one case with a metal implant, and one case with abnormal anatomy.\nThe original CT images were deformably registered to the mDixon FFE MR images\nto minimize the effects of processing the MR images. The sCT generation\naccuracy and robustness were evaluated using Mean Absolute Error (MAE) based on\nthe Hounsfield Units (HU) for three regions (whole body, bone, and air within\nthe body), Mean Error (ME) to observe systematic average offset errors in the\nsCT generation, and dosimetric evaluation of all clinically relevant\nstructures. For the test set the MAE for the Pix2Pix and Cycle GAN models were\n92.4 $\\pm$ 13.5 HU, and 100.7 $\\pm$ 14.6 HU, respectively, for the body region,\n166.3 $\\pm$ 31.8 HU, and 184 $\\pm$ 31.9 HU, respectively, for the bone region,\nand 183.7 $\\pm$ 41.3 HU and 185.4 $\\pm$ 37.9 HU for the air regions. The ME for\nPix2Pix and Cycle GAN were 21.0 $\\pm$ 11.8 HU and 37.5 $\\pm$ 14.9 HU,\nrespectively. Absolute Percent Mean/Max Dose Errors were less than 2% for the\nPTV and all critical structures for both models, and DRRs generated from these\nmodels looked qualitatively similar to CT generated DRRs showing these methods\nare promising for MR-only planning.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 19:42:17 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 21:13:34 GMT"}, {"version": "v3", "created": "Thu, 7 Feb 2019 19:07:57 GMT"}, {"version": "v4", "created": "Wed, 27 Feb 2019 18:14:56 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Klages", "Peter", ""], ["Benslimane", "Ilyes", ""], ["Riyahi", "Sadegh", ""], ["Jiang", "Jue", ""], ["Hunt", "Margie", ""], ["Deasy", "Joe", ""], ["Veeraraghavan", "Harini", ""], ["Tyagi", "Neelam", ""]]}, {"id": "1902.00541", "submitter": "Cory Cornelius", "authors": "Cory Cornelius, Nilaksh Das, Shang-Tse Chen, Li Chen, Michael E.\n  Kounavis, Duen Horng Chau", "title": "The Efficacy of SHIELD under Different Threat Models", "comments": "Appraisal paper of existing method accepted for oral presentation at\n  KDD LEMINCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this appraisal paper, we evaluate the efficacy of SHIELD, a\ncompression-based defense framework for countering adversarial attacks on image\nclassification models, which was published at KDD 2018. Here, we consider\nalternative threat models not studied in the original work, where we assume\nthat an adaptive adversary is aware of the ensemble defense approach, the\ndefensive pre-processing, and the architecture and weights of the models used\nin the ensemble. We define scenarios with varying levels of threat and\nempirically analyze the proposed defense by varying the degree of information\navailable to the attacker, spanning from a full white-box attack to the\ngray-box threat model described in the original work. To evaluate the\nrobustness of the defense against an adaptive attacker, we consider the\ntargeted-attack success rate of the Projected Gradient Descent (PGD) attack,\nwhich is a strong gradient-based adversarial attack proposed in adversarial\nmachine learning research. We also experiment with training the SHIELD ensemble\nfrom scratch, which is different from re-training using a pre-trained model as\ndone in the original work. We find that the targeted PGD attack has a success\nrate of 64.3% against the original SHIELD ensemble in the full white box\nscenario, but this drops to 48.9% if the models used in the ensemble are\ntrained from scratch instead of being retrained. Our experiments further reveal\nthat an ensemble whose models are re-trained indeed have higher correlation in\nthe cosine similarity space, and models that are trained from scratch are less\nvulnerable to targeted attacks in the white-box and gray-box scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 20:10:12 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 20:48:26 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Cornelius", "Cory", ""], ["Das", "Nilaksh", ""], ["Chen", "Shang-Tse", ""], ["Chen", "Li", ""], ["Kounavis", "Michael E.", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1902.00550", "submitter": "Haifa Alhasson", "authors": "Haifa F. Alhasson, Shuaa S. Alharbi, Boguslaw Obara", "title": "2D and 3D Vascular Structures Enhancement via Multiscale Fractional\n  Anisotropy Tensor", "comments": "ECCV 2018 Workshops,Munich, Germany, Sept. 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of vascular structures from noisy images is a fundamental\nprocess for extracting meaningful information in many applications. Most\nwell-known vascular enhancing techniques often rely on Hessian-based filters.\nThis paper investigates the feasibility and deficiencies of detecting\ncurve-like structures using a Hessian matrix. The main contribution is a novel\nenhancement function, which overcomes the deficiencies of established methods.\nOur approach has been evaluated quantitatively and qualitatively using\nsynthetic examples and a wide range of real 2D and 3D biomedical images.\nCompared with other existing approaches, the experimental results prove that\nour proposed approach achieves high-quality curvilinear structure enhancement.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 20:17:41 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Alhasson", "Haifa F.", ""], ["Alharbi", "Shuaa S.", ""], ["Obara", "Boguslaw", ""]]}, {"id": "1902.00577", "submitter": "Sascha Saralajew", "authors": "Sascha Saralajew and Lars Holdijk and Maike Rees and Thomas Villmann", "title": "Robustness of Generalized Learning Vector Quantization Models against\n  Adversarial Attacks", "comments": "to be published in 13th International Workshop on Self-Organizing\n  Maps and Learning Vector Quantization, Clustering and Data Visualization", "journal-ref": null, "doi": "10.1007/978-3-030-19642-4_19", "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks and the development of (deep) neural networks robust\nagainst them are currently two widely researched topics. The robustness of\nLearning Vector Quantization (LVQ) models against adversarial attacks has\nhowever not yet been studied to the same extent. We therefore present an\nextensive evaluation of three LVQ models: Generalized LVQ, Generalized Matrix\nLVQ and Generalized Tangent LVQ. The evaluation suggests that both Generalized\nLVQ and Generalized Tangent LVQ have a high base robustness, on par with the\ncurrent state-of-the-art in robust neural network methods. In contrast to this,\nGeneralized Matrix LVQ shows a high susceptibility to adversarial attacks,\nscoring consistently behind all other models. Additionally, our numerical\nevaluation indicates that increasing the number of prototypes per class\nimproves the robustness of the models.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 22:28:56 GMT"}, {"version": "v2", "created": "Sat, 9 Mar 2019 23:29:01 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Saralajew", "Sascha", ""], ["Holdijk", "Lars", ""], ["Rees", "Maike", ""], ["Villmann", "Thomas", ""]]}, {"id": "1902.00579", "submitter": "Zhe Gan", "authors": "Zhe Gan, Yu Cheng, Ahmed El Kholy, Linjie Li, Jingjing Liu, Jianfeng\n  Gao", "title": "Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog", "comments": "Accepted to ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new model for visual dialog, Recurrent Dual Attention\nNetwork (ReDAN), using multi-step reasoning to answer a series of questions\nabout an image. In each question-answering turn of a dialog, ReDAN infers the\nanswer progressively through multiple reasoning steps. In each step of the\nreasoning process, the semantic representation of the question is updated based\non the image and the previous dialog history, and the recurrently-refined\nrepresentation is used for further reasoning in the subsequent step. On the\nVisDial v1.0 dataset, the proposed ReDAN model achieves a new state-of-the-art\nof 64.47% NDCG score. Visualization on the reasoning process further\ndemonstrates that ReDAN can locate context-relevant visual and textual clues\nvia iterative refinement, which can lead to the correct answer step-by-step.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 22:48:26 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 05:54:02 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Gan", "Zhe", ""], ["Cheng", "Yu", ""], ["Kholy", "Ahmed El", ""], ["Li", "Linjie", ""], ["Liu", "Jingjing", ""], ["Gao", "Jianfeng", ""]]}, {"id": "1902.00615", "submitter": "Zhicheng Ding", "authors": "Zhicheng Ding, Edward Wong", "title": "Confidence Trigger Detection: An Approach to Build Real-time\n  Tracking-by-detection System", "comments": "9 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With deep learning based image analysis getting popular in recent years, a\nlot of multiple objects tracking applications are in demand. Some of these\napplications (e.g., surveillance camera, intelligent robotics, and autonomous\ndriving) require the system runs in real-time. Though recent proposed methods\nreach fairly high accuracy, the speed is still slower than real-time\napplication requirement. In order to increase tracking-by-detection system's\nspeed for real-time tracking, we proposed confidence trigger detection (CTD)\napproach which uses confidence of tracker to decide when to trigger object\ndetection. Using this approach, system can safely skip detection of images\nframes that objects barely move. We had studied the influence of different\nconfidences in three popular detectors separately. Though we found trade-off\nbetween speed and accuracy, our approach reaches higher accuracy at given\nspeed.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 01:52:53 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Ding", "Zhicheng", ""], ["Wong", "Edward", ""]]}, {"id": "1902.00617", "submitter": "Jingdong Wang", "authors": "Xiaojuan Wang, Ting Zhang, Guo-Jun Q, Jinhui Tang, Jingdong Wang", "title": "Supervised Quantization for Similarity Search", "comments": "CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of searching for semantically similar\nimages from a large database. We present a compact coding approach, supervised\nquantization. Our approach simultaneously learns feature selection that\nlinearly transforms the database points into a low-dimensional discriminative\nsubspace, and quantizes the data points in the transformed space. The\noptimization criterion is that the quantized points not only approximate the\ntransformed points accurately, but also are semantically separable: the points\nbelonging to a class lie in a cluster that is not overlapped with other\nclusters corresponding to other classes, which is formulated as a\nclassification problem. The experiments on several standard datasets show the\nsuperiority of our approach over the state-of-the art supervised hashing and\nunsupervised quantization algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 02:07:51 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Wang", "Xiaojuan", ""], ["Zhang", "Ting", ""], ["Q", "Guo-Jun", ""], ["Tang", "Jinhui", ""], ["Wang", "Jingdong", ""]]}, {"id": "1902.00623", "submitter": "Jingdong Wang", "authors": "Ting Zhang and Jingdong Wang", "title": "Collaborative Quantization for Cross-Modal Similarity Search", "comments": "CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal similarity search is a problem about designing a search system\nsupporting querying across content modalities, e.g., using an image to search\nfor texts or using a text to search for images. This paper presents a compact\ncoding solution for efficient search, with a focus on the quantization approach\nwhich has already shown the superior performance over the hashing solutions in\nthe single-modal similarity search. We propose a cross-modal quantization\napproach, which is among the early attempts to introduce quantization into\ncross-modal search. The major contribution lies in jointly learning the\nquantizers for both modalities through aligning the quantized representations\nfor each pair of image and text belonging to a document. In addition, our\napproach simultaneously learns the common space for both modalities in which\nquantization is conducted to enable efficient and effective search using the\nEuclidean distance computed in the common space with fast distance table\nlookup. Experimental results compared with several competitive algorithms over\nthree benchmark datasets demonstrate that the proposed approach achieves the\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 02:20:25 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Zhang", "Ting", ""], ["Wang", "Jingdong", ""]]}, {"id": "1902.00626", "submitter": "Marwan Mattar", "authors": "Marwan Mattar, Michael Ross, Erik Learned-Miller", "title": "Nonparametric Curve Alignment", "comments": "4 pages, IEEE International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP), 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Congealing is a flexible nonparametric data-driven framework for the joint\nalignment of data. It has been successfully applied to the joint alignment of\nbinary images of digits, binary images of object silhouettes, grayscale MRI\nimages, color images of cars and faces, and 3D brain volumes. This research\nenhances congealing to practically and effectively apply it to curve data. We\ndevelop a parameterized set of nonlinear transformations that allow us to apply\ncongealing to this type of data. We present positive results on aligning\nsynthetic and real curve data sets and conclude with a discussion on extending\nthis work to simultaneous alignment and clustering.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 02:29:24 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Mattar", "Marwan", ""], ["Ross", "Michael", ""], ["Learned-Miller", "Erik", ""]]}, {"id": "1902.00643", "submitter": "Shifeng Zhang", "authors": "Shifeng Zhang, Jianmin Li and Bo Zhang", "title": "Pairwise Teacher-Student Network for Semi-Supervised Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing method maps similar high-dimensional data to binary hashcodes with\nsmaller hamming distance, and it has received broad attention due to its low\nstorage cost and fast retrieval speed. Pairwise similarity is easily obtained\nand widely used for retrieval, and most supervised hashing algorithms are\ncarefully designed for the pairwise supervisions. As labeling all data pairs is\ndifficult, semi-supervised hashing is proposed which aims at learning efficient\ncodes with limited labeled pairs and abundant unlabeled ones. Existing methods\nbuild graphs to capture the structure of dataset, but they are not working well\nfor complex data as the graph is built based on the data representations and\ndetermining the representations of complex data is difficult. In this paper, we\npropose a novel teacher-student semi-supervised hashing framework in which the\nstudent is trained with the pairwise information produced by the teacher\nnetwork. The network follows the smoothness assumption, which achieves\nconsistent distances for similar data pairs so that the retrieval results are\nsimilar for neighborhood queries. Experiments on large-scale datasets show that\nthe proposed method reaches impressive gain over the supervised baselines and\nis superior to state-of-the-art semi-supervised hashing methods.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 05:21:26 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Zhang", "Shifeng", ""], ["Li", "Jianmin", ""], ["Zhang", "Bo", ""]]}, {"id": "1902.00669", "submitter": "Bairui Wang", "authors": "Bairui Wang, Lin Ma, Wei Zhang, Wenhao Jiang, Feng Zhang", "title": "Hierarchical Photo-Scene Encoder for Album Storytelling", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel model with a hierarchical photo-scene\nencoder and a reconstructor for the task of album storytelling. The photo-scene\nencoder contains two sub-encoders, namely the photo and scene encoders, which\nare stacked together and behave hierarchically to fully exploit the structure\ninformation of the photos within an album. Specifically, the photo encoder\ngenerates semantic representation for each photo while exploiting temporal\nrelationships among them. The scene encoder, relying on the obtained photo\nrepresentations, is responsible for detecting the scene changes and generating\nscene representations. Subsequently, the decoder dynamically and attentively\nsummarizes the encoded photo and scene representations to generate a sequence\nof album representations, based on which a story consisting of multiple\ncoherent sentences is generated. In order to fully extract the useful semantic\ninformation from an album, a reconstructor is employed to reproduce the\nsummarized album representations based on the hidden states of the decoder. The\nproposed model can be trained in an end-to-end manner, which results in an\nimproved performance over the state-of-the-arts on the public visual\nstorytelling (VIST) dataset. Ablation studies further demonstrate the\neffectiveness of the proposed hierarchical photo-scene encoder and\nreconstructor.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 08:42:58 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Wang", "Bairui", ""], ["Ma", "Lin", ""], ["Zhang", "Wei", ""], ["Jiang", "Wenhao", ""], ["Zhang", "Feng", ""]]}, {"id": "1902.00671", "submitter": "Mehmet Ozgur Turkoglu", "authors": "Mehmet Ozgur Turkoglu, William Thong, Luuk Spreeuwers, Berkay\n  Kicanaoglu", "title": "A Layer-Based Sequential Framework for Scene Generation with GANs", "comments": "This paper was accepted at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual world we sense, interpret and interact everyday is a complex\ncomposition of interleaved physical entities. Therefore, it is a very\nchallenging task to generate vivid scenes of similar complexity using\ncomputers. In this work, we present a scene generation framework based on\nGenerative Adversarial Networks (GANs) to sequentially compose a scene,\nbreaking down the underlying problem into smaller ones. Different than the\nexisting approaches, our framework offers an explicit control over the elements\nof a scene through separate background and foreground generators. Starting with\nan initially generated background, foreground objects then populate the scene\none-by-one in a sequential manner. Via quantitative and qualitative experiments\non a subset of the MS-COCO dataset, we show that our proposed framework\nproduces not only more diverse images but also copes better with affine\ntransformations and occlusion artifacts of foreground objects than its\ncounterparts.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 08:49:56 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Turkoglu", "Mehmet Ozgur", ""], ["Thong", "William", ""], ["Spreeuwers", "Luuk", ""], ["Kicanaoglu", "Berkay", ""]]}, {"id": "1902.00730", "submitter": "Fayez Lahoud", "authors": "Fayez Lahoud, Radhakrishna Achanta, Pablo M\\'arquez-Neila and Sabine\n  S\\\"usstrunk", "title": "Self-Binarizing Networks", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to train self-binarizing neural networks, that is,\nnetworks that evolve their weights and activations during training to become\nbinary. To obtain similar binary networks, existing methods rely on the sign\nactivation function. This function, however, has no gradients for non-zero\nvalues, which makes standard backpropagation impossible. To circumvent the\ndifficulty of training a network relying on the sign activation function, these\nmethods alternate between floating-point and binary representations of the\nnetwork during training, which is sub-optimal and inefficient. We approach the\nbinarization task by training on a unique representation involving a smooth\nactivation function, which is iteratively sharpened during training until it\nbecomes a binary representation equivalent to the sign activation function.\nAdditionally, we introduce a new technique to perform binary batch\nnormalization that simplifies the conventional batch normalization by\ntransforming it into a simple comparison operation. This is unlike existing\nmethods, which are forced to the retain the conventional floating-point-based\nbatch normalization. Our binary networks, apart from displaying advantages of\nlower memory and computation as compared to conventional floating-point and\nbinary networks, also show higher classification accuracy than existing\nstate-of-the-art methods on multiple benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 14:48:16 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Lahoud", "Fayez", ""], ["Achanta", "Radhakrishna", ""], ["M\u00e1rquez-Neila", "Pablo", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "1902.00749", "submitter": "Ji Zhu", "authors": "Ji Zhu, Hua Yang, Nian Liu, Minyoung Kim, Wenjun Zhang, Ming-Hsuan\n  Yang", "title": "Online Multi-Object Tracking with Dual Matching Attention Networks", "comments": "Accepted by ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an online Multi-Object Tracking (MOT) approach\nwhich integrates the merits of single object tracking and data association\nmethods in a unified framework to handle noisy detections and frequent\ninteractions between targets. Specifically, for applying single object tracking\nin MOT, we introduce a cost-sensitive tracking loss based on the\nstate-of-the-art visual tracker, which encourages the model to focus on hard\nnegative distractors during online learning. For data association, we propose\nDual Matching Attention Networks (DMAN) with both spatial and temporal\nattention mechanisms. The spatial attention module generates dual attention\nmaps which enable the network to focus on the matching patterns of the input\nimage pair, while the temporal attention module adaptively allocates different\nlevels of attention to different samples in the tracklet to suppress noisy\nobservations. Experimental results on the MOT benchmark datasets show that the\nproposed algorithm performs favorably against both online and offline trackers\nin terms of identity-preserving metrics.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 16:25:46 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Zhu", "Ji", ""], ["Yang", "Hua", ""], ["Liu", "Nian", ""], ["Kim", "Minyoung", ""], ["Zhang", "Wenjun", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1902.00760", "submitter": "Alessio Tonioni", "authors": "Alessio Tonioni and Luigi Di Stefano", "title": "Domain invariant hierarchical embedding for grocery products recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing packaged grocery products based solely on appearance is still an\nopen issue for modern computer vision systems due to peculiar challenges.\nFirstly, the number of different items to be recognized is huge (i.e., in the\norder of thousands) and rapidly changing over time. Moreover, there exist a\nsignificant domain shift between the images that should be recognized at test\ntime, taken in stores by cheap cameras, and those available for training,\nusually just one or a few studio-quality images per product. We propose an\nend-to-end architecture comprising a GAN to address the domain shift at\ntraining time and a deep CNN trained on the samples generated by the GAN to\nlearn an embedding of product images that enforces a hierarchy between product\ncategories. At test time, we perform recognition by means of K-NN search\nagainst a database consisting of just one reference image per product.\nExperiments addressing recognition of products present in the training datasets\nas well as different ones unseen at training time show that our approach\ncompares favourably to state-of-the-art methods on the grocery recognition task\nand generalize fairly well to similar ones.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 18:19:41 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Tonioni", "Alessio", ""], ["Di Stefano", "Luigi", ""]]}, {"id": "1902.00761", "submitter": "Shreyas Skandan Shivakumar", "authors": "Shreyas S. Shivakumar, Ty Nguyen, Ian D. Miller, Steven W. Chen, Vijay\n  Kumar and Camillo J. Taylor", "title": "DFuseNet: Deep Fusion of RGB and Sparse Depth Information for Image\n  Guided Dense Depth Completion", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a convolutional neural network that is designed to\nupsample a series of sparse range measurements based on the contextual cues\ngleaned from a high resolution intensity image. Our approach draws inspiration\nfrom related work on super-resolution and in-painting. We propose a novel\narchitecture that seeks to pull contextual cues separately from the intensity\nimage and the depth features and then fuse them later in the network. We argue\nthat this approach effectively exploits the relationship between the two\nmodalities and produces accurate results while respecting salient image\nstructures. We present experimental results to demonstrate that our approach is\ncomparable with state of the art methods and generalizes well across multiple\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 18:25:31 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 17:02:49 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Shivakumar", "Shreyas S.", ""], ["Nguyen", "Ty", ""], ["Miller", "Ian D.", ""], ["Chen", "Steven W.", ""], ["Kumar", "Vijay", ""], ["Taylor", "Camillo J.", ""]]}, {"id": "1902.00809", "submitter": "Moi Hoon Yap", "authors": "Manu Goyal and Amanda Oakley and Priyanka Bansal and Darren Dancey and\n  Moi Hoon Yap", "title": "Automatic Lesion Boundary Segmentation in Dermoscopic Images with\n  Ensemble Deep Learning Methods", "comments": "7 pages, 8 figures and 4 tables. arXiv admin note: text overlap with\n  arXiv:1711.10449", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of skin cancer, particularly melanoma, is crucial to enable\nadvanced treatment. Due to the rapid growth in the numbers of skin cancers,\nthere is a growing need of computerized analysis for skin lesions. The\nstate-of-the-art public available datasets for skin lesions are often\naccompanied with very limited amount of segmentation ground truth labeling as\nit is laborious and expensive. The lesion boundary segmentation is vital to\nlocate the lesion accurately in dermoscopic images and lesion diagnosis of\ndifferent skin lesion types. In this work, we propose the use of fully\nautomated deep learning ensemble methods for accurate lesion boundary\nsegmentation in dermoscopic images. We trained the Mask-RCNN and DeepLabv3+\nmethods on ISIC-2017 segmentation training set and evaluate the performance of\nthe ensemble networks on ISIC-2017 testing set. Our results showed that the\nbest proposed ensemble method segmented the skin lesions with Jaccard index of\n79.58% for the ISIC-2017 testing set. The proposed ensemble method outperformed\nFrCN, FCN, U-Net, and SegNet in Jaccard Index by 2.48%, 7.42%, 17.95%, and\n9.96% respectively. Furthermore, the proposed ensemble method achieved an\naccuracy of 95.6% for some representative clinically benign cases, 90.78% for\nthe melanoma cases, and 91.29% for the seborrheic keratosis cases on ISIC-2017\ntesting set, exhibiting better performance than FrCN, FCN, U-Net, and SegNet.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 23:17:19 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 11:02:06 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Goyal", "Manu", ""], ["Oakley", "Amanda", ""], ["Bansal", "Priyanka", ""], ["Dancey", "Darren", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "1902.00813", "submitter": "Yuejiang Liu", "authors": "Yuejiang Liu, Parth Kothari, Alexandre Alahi", "title": "Collaborative Sampling in Generative Adversarial Networks", "comments": "Accepted to AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard practice in Generative Adversarial Networks (GANs) discards the\ndiscriminator during sampling. However, this sampling method loses valuable\ninformation learned by the discriminator regarding the data distribution. In\nthis work, we propose a collaborative sampling scheme between the generator and\nthe discriminator for improved data generation. Guided by the discriminator,\nour approach refines the generated samples through gradient-based updates at a\nparticular layer of the generator, shifting the generator distribution closer\nto the real data distribution. Additionally, we present a practical\ndiscriminator shaping method that can smoothen the loss landscape provided by\nthe discriminator for effective sample refinement. Through extensive\nexperiments on synthetic and image datasets, we demonstrate that our proposed\nmethod can improve generated samples both quantitatively and qualitatively,\noffering a new degree of freedom in GAN sampling.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 23:43:25 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 12:18:25 GMT"}, {"version": "v3", "created": "Fri, 22 Nov 2019 15:54:24 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Liu", "Yuejiang", ""], ["Kothari", "Parth", ""], ["Alahi", "Alexandre", ""]]}, {"id": "1902.00820", "submitter": "Sarah Ostadabbas", "authors": "Amirreza Farnoosh, Behnaz Rezaei, and Sarah Ostadabbas", "title": "DeepPBM: Deep Probabilistic Background Model Estimation from Video\n  Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel unsupervised probabilistic model estimation of\nvisual background in video sequences using a variational autoencoder framework.\nDue to the redundant nature of the backgrounds in surveillance videos, visual\ninformation of the background can be compressed into a low-dimensional subspace\nin the encoder part of the variational autoencoder, while the highly variant\ninformation of its moving foreground gets filtered throughout its\nencoding-decoding process. Our deep probabilistic background model (DeepPBM)\nestimation approach is enabled by the power of deep neural networks in learning\ncompressed representations of video frames and reconstructing them back to the\noriginal domain. We evaluated the performance of our DeepPBM in background\nsubtraction on 9 surveillance videos from the background model challenge\n(BMC2012) dataset, and compared that with a standard subspace learning\ntechnique, robust principle component analysis (RPCA), which similarly\nestimates a deterministic low dimensional representation of the background in\nvideos and is widely used for this application. Our method outperforms RPCA on\nBMC2012 dataset with 23% in average in F-measure score, emphasizing that\nbackground subtraction using the trained model can be done in more than 10\ntimes faster.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 00:25:53 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Farnoosh", "Amirreza", ""], ["Rezaei", "Behnaz", ""], ["Ostadabbas", "Sarah", ""]]}, {"id": "1902.00835", "submitter": "Ken Li", "authors": "Hui Li, Ye Liu, Yan Qiu Chen", "title": "Automatic trajectory measurement of large numbers of crowded objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex motion patterns of natural systems, such as fish schools, bird\nflocks, and cell groups, have attracted great attention from scientists for\nyears. Trajectory measurement of individuals is vital for quantitative and\nhigh-throughput study of their collective behaviors. However, such data are\nrare mainly due to the challenges of detection and tracking of large numbers of\nobjects with similar visual features and frequent occlusions. We present an\nautomatic and effective framework to measure trajectories of large numbers of\ncrowded oval-shaped objects, such as fish and cells. We first use a novel dual\nellipse locator to detect the coarse position of each individual and then\npropose a variance minimization active contour method to obtain the optimal\nsegmentation results. For tracking, cost matrix of assignment between\nconsecutive frames is trainable via a random forest classifier with many\nspatial, texture, and shape features. The optimal trajectories are found for\nthe whole image sequence by solving two linear assignment problems. We evaluate\nthe proposed method on many challenging data sets.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 03:30:50 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Li", "Hui", ""], ["Liu", "Ye", ""], ["Chen", "Yan Qiu", ""]]}, {"id": "1902.00842", "submitter": "Anish Singhani", "authors": "Anish Singhani", "title": "Real-Time Freespace Segmentation on Autonomous Robots for Detection of\n  Obstacles and Drop-Offs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile robots navigating in indoor and outdoor environments must be able to\nidentify and avoid unsafe terrain. Although a significant amount of work has\nbeen done on the detection of standing obstacles (solid obstructions), not much\nwork has been done on the detection of negative obstacles (e.g. dropoffs,\nledges, downward stairs). We propose a method of terrain safety segmentation\nusing deep convolutional networks. Our custom semantic segmentation\narchitecture uses a single camera as input and creates a freespace map\ndistinguishing safe terrain and obstacles. We then show how this freespace map\ncan be used for real-time navigation on an indoor robot. The results show that\nour system generalizes well, is suitable for real-time operation, and runs at\naround 55 fps on a small indoor robot powered by a low-power embedded GPU.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 04:42:13 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Singhani", "Anish", ""]]}, {"id": "1902.00855", "submitter": "Shiba Kuanar", "authors": "Shiba Kuanar, K.R. Rao, Dwarikanath Mahapatra, Monalisa Bilas", "title": "Night Time Haze and Glow Removal using Deep Dilated Convolutional\n  Network", "comments": "13 pages, 10 figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the single image haze removal problem in a\nnighttime scene. The night haze removal is a severely ill-posed problem\nespecially due to the presence of various visible light sources with varying\ncolors and non-uniform illumination. These light sources are of different\nshapes and introduce noticeable glow in night scenes. To address these effects\nwe introduce a deep learning based DeGlow-DeHaze iterative architecture which\naccounts for varying color illumination and glows. First, our convolution\nneural network (CNN) based DeGlow model is able to remove the glow effect\nsignificantly and on top of it a separate DeHaze network is included to remove\nthe haze effect. For our recurrent network training, the hazy images and the\ncorresponding transmission maps are synthesized from the NYU depth datasets and\nconsequently restored a high-quality haze-free image. The experimental results\ndemonstrate that our hybrid CNN model outperforms other state-of-the-art\nmethods in terms of computation speed and image quality. We also show the\neffectiveness of our model on a number of real images and compare our results\nwith the existing night haze heuristic models.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 06:24:46 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Kuanar", "Shiba", ""], ["Rao", "K. R.", ""], ["Mahapatra", "Dwarikanath", ""], ["Bilas", "Monalisa", ""]]}, {"id": "1902.00918", "submitter": "Jie Zhang", "authors": "Jie Zhang, Xiaolong Wang, Dawei Li, Shalini Ghosh, Abhishek Kolagunda,\n  Yalin Wang", "title": "MICIK: MIning Cross-Layer Inherent Similarity Knowledge for Deep Model\n  Compression", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep model compression methods exploit the low-rank\napproximation and sparsity pruning to remove redundant parameters from a\nlearned hidden layer. However, they process each hidden layer individually\nwhile neglecting the common components across layers, and thus are not able to\nfully exploit the potential redundancy space for compression. To solve the\nabove problem and enable further compression of a model, removing the\ncross-layer redundancy and mining the layer-wise inheritance knowledge is\nnecessary. In this paper, we introduce a holistic model compression framework,\nnamely MIning Cross-layer Inherent similarity Knowledge (MICIK), to fully\nexcavate the potential redundancy space. The proposed MICIK framework\nsimultaneously, (1) learns the common and unique weight components across deep\nneural network layers to increase compression rate; (2) preserves the inherent\nsimilarity knowledge of nearby layers and distant layers to minimize the\naccuracy loss and (3) can be complementary to other existing compression\ntechniques such as knowledge distillation. Extensive experiments on large-scale\nconvolutional neural networks demonstrate that MICIK is superior over\nstate-of-the-art model compression approaches with 16X parameter reduction on\nVGG-16 and 6X on GoogLeNet, all without accuracy loss.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 16:26:21 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Zhang", "Jie", ""], ["Wang", "Xiaolong", ""], ["Li", "Dawei", ""], ["Ghosh", "Shalini", ""], ["Kolagunda", "Abhishek", ""], ["Wang", "Yalin", ""]]}, {"id": "1902.00927", "submitter": "Yandong Li", "authors": "Yunhui Guo, Yandong Li, Rogerio Feris, Liqiang Wang, Tajana Rosing", "title": "Depthwise Convolution is All You Need for Learning Multiple Visual\n  Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in designing models that can deal with images\nfrom different visual domains. If there exists a universal structure in\ndifferent visual domains that can be captured via a common parameterization,\nthen we can use a single model for all domains rather than one model per\ndomain. A model aware of the relationships between different domains can also\nbe trained to work on new domains with less resources. However, to identify the\nreusable structure in a model is not easy. In this paper, we propose a\nmulti-domain learning architecture based on depthwise separable convolution.\nThe proposed approach is based on the assumption that images from different\ndomains share cross-channel correlations but have domain-specific spatial\ncorrelations. The proposed model is compact and has minimal overhead when being\napplied to new domains. Additionally, we introduce a gating mechanism to\npromote soft sharing between different domains. We evaluate our approach on\nVisual Decathlon Challenge, a benchmark for testing the ability of multi-domain\nmodels. The experiments show that our approach can achieve the highest score\nwhile only requiring 50% of the parameters compared with the state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 16:58:19 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 18:45:10 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Guo", "Yunhui", ""], ["Li", "Yandong", ""], ["Feris", "Rogerio", ""], ["Wang", "Liqiang", ""], ["Rosing", "Tajana", ""]]}, {"id": "1902.01019", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Amirali Abdolrashidi", "title": "Deep-Emotion: Facial Expression Recognition Using Attentional\n  Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression recognition has been an active research area over the past\nfew decades, and it is still challenging due to the high intra-class variation.\n  Traditional approaches for this problem rely on hand-crafted features such as\nSIFT, HOG and LBP, followed by a classifier trained on a database of images or\nvideos.\n  Most of these works perform reasonably well on datasets of images captured in\na controlled condition, but fail to perform as good on more challenging\ndatasets with more image variation and partial faces.\n  In recent years, several works proposed an end-to-end framework for facial\nexpression recognition, using deep learning models.\n  Despite the better performance of these works, there still seems to be a\ngreat room for improvement.\n  In this work, we propose a deep learning approach based on attentional\nconvolutional network, which is able to focus on important parts of the face,\nand achieves significant improvement over previous models on multiple datasets,\nincluding FER-2013, CK+, FERG, and JAFFE.\n  We also use a visualization technique which is able to find important face\nregions for detecting different emotions, based on the classifier's output.\n  Through experimental results, we show that different emotions seems to be\nsensitive to different parts of the face.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 03:15:13 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Minaee", "Shervin", ""], ["Abdolrashidi", "Amirali", ""]]}, {"id": "1902.01031", "submitter": "Md Ashraful Alam Milton", "authors": "Md Ashraful Alam Milton", "title": "Towards Pedestrian Detection Using RetinaNet in ECCV 2018 Wider\n  Pedestrian Detection Challenge", "comments": "ECCV Wider pedestrian detection challenege submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The main essence of this paper is to investigate the performance of RetinaNet\nbased object detectors on pedestrian detection. Pedestrian detection is an\nimportant research topic as it provides a baseline for general object detection\nand has a great number of practical applications like autonomous car, robotics\nand Security camera. Though extensive research has made huge progress in\npedestrian detection, there are still many issues and open for more research\nand improvement. Recent deep learning based methods have shown state-of-the-art\nperformance in computer vision tasks such as image classification, object\ndetection, and segmentation. Wider pedestrian detection challenge aims at\nfinding improve solutions for pedestrian detection problem. In this paper, We\npropose a pedestrian detection system based on RetinaNet. Our solution has\nscored 0.4061 mAP. The code is available at\nhttps://github.com/miltonbd/ECCV_2018_pedestrian_detection_challenege.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 04:49:59 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Milton", "Md Ashraful Alam", ""]]}, {"id": "1902.01057", "submitter": "Dongyan Guo", "authors": "Dongyan Guo, Jun Wang, Weixuan Zhao, Ying Cui, Zhenhua Wang, Shengyong\n  Chen", "title": "End-to-end feature fusion siamese network for adaptive visual tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to observations, different visual objects have different salient\nfeatures in different scenarios. Even for the same object, its salient shape\nand appearance features may change greatly from time to time in a long-term\ntracking task. Motivated by them, we proposed an end-to-end feature fusion\nframework based on Siamese network, named FF-Siam, which can effectively fuse\ndifferent features for adaptive visual tracking. The framework consists of four\nlayers. A feature extraction layer is designed to extract the different\nfeatures of the target region and search region. The extracted features are\nthen put into a weight generation layer to obtain the channel weights, which\nindicate the importance of different feature channels. Both features and the\nchannel weights are utilized in a template generation layer to generate a\ndiscriminative template. Finally, the corresponding response maps created by\nthe convolution of the search region features and the template are applied with\na fusion layer to obtain the final response map for locating the target.\nExperimental results demonstrate that the proposed framework achieves\nstate-of-the-art performance on the popular Temple-Color, OTB50 and UAV123\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 07:28:20 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Guo", "Dongyan", ""], ["Wang", "Jun", ""], ["Zhao", "Weixuan", ""], ["Cui", "Ying", ""], ["Wang", "Zhenhua", ""], ["Chen", "Shengyong", ""]]}, {"id": "1902.01061", "submitter": "Brojeshwar Bhowmick", "authors": "Swapna Agarwal and Brojeshwar Bhowmick", "title": "3D point cloud registration with shape constraint", "comments": "Published in ICIP 2017", "journal-ref": null, "doi": "10.1109/ICIP.2017.8296672", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a shape-constrained iterative algorithm is proposed to\nregister a rigid template point-cloud to a given reference point-cloud. The\nalgorithm embeds a shape-based similarity constraint into the principle of\ngravitation. The shape-constrained gravitation, as induced by the reference,\ncontrols the movement of the template such that at each iteration, the template\nbetter aligns with the reference in terms of shape. This constraint enables the\nalignment in difficult conditions indtroduced by change (presence of outliers\nand/or missing parts), translation, rotation and scaling. We discuss efficient\nimplementation techniques with least manual intervention. The registration is\nshown to be useful for change detection in the 3D point-cloud. The algorithm is\ncompared with three state-of-the-art registration approaches. The experiments\nare done on both synthetic and real-world data. The proposed algorithm is shown\nto perform better in the presence of big rotation, structured and unstructured\noutliers and missing data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 07:44:54 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Agarwal", "Swapna", ""], ["Bhowmick", "Brojeshwar", ""]]}, {"id": "1902.01077", "submitter": "Suryansh Kumar", "authors": "Suryansh Kumar", "title": "Jumping Manifolds: Geometry Aware Dense Non-Rigid Structure from Motion", "comments": "New version with corrected typo. 10 Pages, 7 Figures, 1 Table.\n  Accepted for publication in IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2019. Acknowledgement added. Supplementary material is\n  available at https://suryanshkumar.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given dense image feature correspondences of a non-rigidly moving object\nacross multiple frames, this paper proposes an algorithm to estimate its 3D\nshape for each frame. To solve this problem accurately, the recent\nstate-of-the-art algorithm reduces this task to set of local linear subspace\nreconstruction and clustering problem using Grassmann manifold representation\n\\cite{kumar2018scalable}. Unfortunately, their method missed on some of the\ncritical issues associated with the modeling of surface deformations, for e.g.,\nthe dependence of a local surface deformation on its neighbors. Furthermore,\ntheir representation to group high dimensional data points inevitably introduce\nthe drawbacks of categorizing samples on the high-dimensional Grassmann\nmanifold \\cite{huang2015projection, harandi2014manifold}. Hence, to deal with\nsuch limitations with \\cite{kumar2018scalable}, we propose an algorithm that\njointly exploits the benefit of high-dimensional Grassmann manifold to perform\nreconstruction, and its equivalent lower-dimensional representation to infer\nsuitable clusters. To accomplish this, we project each Grassmannians onto a\nlower-dimensional Grassmann manifold which preserves and respects the\ndeformation of the structure w.r.t its neighbors. These Grassmann points in the\nlower-dimension then act as a representative for the selection of\nhigh-dimensional Grassmann samples to perform each local reconstruction. In\npractice, our algorithm provides a geometrically efficient way to solve dense\nNRSfM by switching between manifolds based on its benefit and usage.\nExperimental results show that the proposed algorithm is very effective in\nhandling noise with reconstruction accuracy as good as or better than the\ncompeting methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 08:19:46 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 10:52:32 GMT"}, {"version": "v3", "created": "Sun, 28 Apr 2019 01:24:05 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Kumar", "Suryansh", ""]]}, {"id": "1902.01078", "submitter": "Alexandros Stergiou MSc", "authors": "Alexandros Stergiou, Georgios Kapidis, Grigorios Kalliatakis, Christos\n  Chrysoulas, Remco Veltkamp and Ronald Poppe", "title": "Saliency Tubes: Visual Explanations for Spatio-Temporal Convolutions", "comments": null, "journal-ref": "IEEE International Conference on Image Processing (ICIP 2019)", "doi": "10.1109/ICIP.2019.8803153", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches have been established as the main methodology for\nvideo classification and recognition. Recently, 3-dimensional convolutions have\nbeen used to achieve state-of-the-art performance in many challenging video\ndatasets. Because of the high level of complexity of these methods, as the\nconvolution operations are also extended to additional dimension in order to\nextract features from them as well, providing a visualization for the signals\nthat the network interpret as informative, is a challenging task. An effective\nnotion of understanding the network's inner-workings would be to isolate the\nspatio-temporal regions on the video that the network finds most informative.\nWe propose a method called Saliency Tubes which demonstrate the foremost points\nand regions in both frame level and over time that are found to be the main\nfocus points of the network. We demonstrate our findings on widely used\ndatasets for third-person and egocentric action classification and enhance the\nset of methods and visualizations that improve 3D Convolutional Neural Networks\n(CNNs) intelligibility.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 08:19:50 GMT"}, {"version": "v2", "created": "Sun, 12 May 2019 10:38:02 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Stergiou", "Alexandros", ""], ["Kapidis", "Georgios", ""], ["Kalliatakis", "Grigorios", ""], ["Chrysoulas", "Christos", ""], ["Veltkamp", "Remco", ""], ["Poppe", "Ronald", ""]]}, {"id": "1902.01096", "submitter": "Xintong Han", "authors": "Xintong Han, Zuxuan Wu, Weilin Huang, Matthew R. Scott, Larry S. Davis", "title": "Compatible and Diverse Fashion Image Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual compatibility is critical for fashion analysis, yet is missing in\nexisting fashion image synthesis systems. In this paper, we propose to\nexplicitly model visual compatibility through fashion image inpainting. To this\nend, we present Fashion Inpainting Networks (FiNet), a two-stage image-to-image\ngeneration framework that is able to perform compatible and diverse inpainting.\nDisentangling the generation of shape and appearance to ensure photorealistic\nresults, our framework consists of a shape generation network and an appearance\ngeneration network. More importantly, for each generation network, we introduce\ntwo encoders interacting with one another to learn latent code in a shared\ncompatibility space. The latent representations are jointly optimized with the\ncorresponding generation network to condition the synthesis process,\nencouraging a diverse set of generated results that are visually compatible\nwith existing fashion garments. In addition, our framework is readily extended\nto clothing reconstruction and fashion transfer, with impressive results.\nExtensive experiments with comparisons with state-of-the-art approaches on\nfashion synthesis task quantitatively and qualitatively demonstrate the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 09:29:22 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 04:21:49 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Han", "Xintong", ""], ["Wu", "Zuxuan", ""], ["Huang", "Weilin", ""], ["Scott", "Matthew R.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1902.01115", "submitter": "Liang Zhu", "authors": "Liang Zhu, Zhijian Zhao, Chao Lu, Yining Lin, Yao Peng, Tangren Yao", "title": "Dual Path Multi-Scale Fusion Networks with Attention for Crowd Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of crowd counting in varying density scenes is an extremely\ndifficult challenge due to large scale variations. In this paper, we propose a\nnovel dual path multi-scale fusion network architecture with attention\nmechanism named SFANet that can perform accurate count estimation as well as\npresent high-resolution density maps for highly congested crowd scenes. The\nproposed SFANet contains two main components: a VGG backbone convolutional\nneural network (CNN) as the front-end feature map extractor and a dual path\nmulti-scale fusion networks as the back-end to generate density map. These dual\npath multi-scale fusion networks have the same structure, one path is\nresponsible for generating attention map by highlighting crowd regions in\nimages, the other path is responsible for fusing multi-scale features as well\nas attention map to generate the final high-quality high-resolution density\nmaps. SFANet can be easily trained in an end-to-end way by dual path joint\ntraining. We have evaluated our method on four crowd counting datasets\n(ShanghaiTech, UCF CC 50, UCSD and UCF-QRNF). The results demonstrate that with\nattention mechanism and multi-scale feature fusion, the proposed SFANet\nachieves the best performance on all these datasets and generates better\nquality density maps compared with other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 10:34:56 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Zhu", "Liang", ""], ["Zhao", "Zhijian", ""], ["Lu", "Chao", ""], ["Lin", "Yining", ""], ["Peng", "Yao", ""], ["Yao", "Tangren", ""]]}, {"id": "1902.01196", "submitter": "Saber Malekzadeh", "authors": "Fatemeh Jamal, Fatemeh Ahmadi, Mohammad Khanzadeh, Saber Malekzadeh", "title": "Application of image processing in optical method, Moire deflectometry\n  for investigating the optical properties of zinc oxide nanoparticle", "comments": "Submitted to \"Optic and laser technology\" journal", "journal-ref": null, "doi": "10.13140/RG.2.2.23059.32804", "report-no": null, "categories": "physics.ins-det cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays for measurement of refractive index of nanomaterials usually\nspectro-photometric and mechanical methods are used which are expensive and\nindirect. In this paper for measuring these parameters of zinc oxide\nnanomaterial with two different stabilizers, a simple optical method, Moire\ndeflectometry, which is based on wave front analysis and geometric optics is\nused. In the Moire deflectometry method, the beam of light from the laser diode\npasses through the sample. As a result of that, a change in the sample\nenvironment is observed as deflections of the fringes. By recording of these\ndeflections using CCD and image processing with MATLAB, the nanomaterials\nrefractive indices can be calculated. Due to the high accuracy of this method\nand improved the image processing code in Matlab, the smallest changes of the\nrefractive index in the sample can be measured. Digital Image processing is\nused for processing images in a way that features can be selected and being\nshown. The results obtained in this method show a good improvement over the\nother used methods.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 14:48:05 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Jamal", "Fatemeh", ""], ["Ahmadi", "Fatemeh", ""], ["Khanzadeh", "Mohammad", ""], ["Malekzadeh", "Saber", ""]]}, {"id": "1902.01220", "submitter": "Yatie Xiao", "authors": "Yatie Xiao, Chi-Man Pun", "title": "Adaptive Gradient for Adversarial Perturbations Generation", "comments": "arXiv admin note: text overlap with arXiv:1901.03706 The formula in\n  Algorithm 1 lacks important representations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks have achieved remarkable success in computer vision,\nnatural language processing, and audio tasks.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 10:47:21 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 02:33:55 GMT"}, {"version": "v3", "created": "Wed, 27 Feb 2019 03:25:51 GMT"}, {"version": "v4", "created": "Fri, 12 Apr 2019 13:05:50 GMT"}, {"version": "v5", "created": "Tue, 14 May 2019 07:17:28 GMT"}, {"version": "v6", "created": "Mon, 20 May 2019 01:34:19 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Xiao", "Yatie", ""], ["Pun", "Chi-Man", ""]]}, {"id": "1902.01275", "submitter": "Martin Sundermeyer", "authors": "Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, Manuel\n  Brucker, Rudolph Triebel", "title": "Implicit 3D Orientation Learning for 6D Object Detection from RGB Images", "comments": "Code available at: https://github.com/DLR-RM/AugmentedAutoencoder", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a real-time RGB-based pipeline for object detection and 6D pose\nestimation. Our novel 3D orientation estimation is based on a variant of the\nDenoising Autoencoder that is trained on simulated views of a 3D model using\nDomain Randomization. This so-called Augmented Autoencoder has several\nadvantages over existing methods: It does not require real, pose-annotated\ntraining data, generalizes to various test sensors and inherently handles\nobject and view symmetries. Instead of learning an explicit mapping from input\nimages to object poses, it provides an implicit representation of object\norientations defined by samples in a latent space. Our pipeline achieves\nstate-of-the-art performance on the T-LESS dataset both in the RGB and RGB-D\ndomain. We also evaluate on the LineMOD dataset where we can compete with other\nsynthetically trained approaches. We further increase performance by correcting\n3D orientation estimates to account for perspective errors when the object\ndeviates from the image center and show extended results.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 16:03:57 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 14:12:26 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Sundermeyer", "Martin", ""], ["Marton", "Zoltan-Csaba", ""], ["Durner", "Maximilian", ""], ["Brucker", "Manuel", ""], ["Triebel", "Rudolph", ""]]}, {"id": "1902.01293", "submitter": "Derek Phillips", "authors": "Derek J. Phillips, Juan Carlos Aragon, Anjali Roychowdhury, Regina\n  Madigan, Sunil Chintakindi, Mykel J. Kochenderfer", "title": "Real-time Prediction of Automotive Collision Risk from Monocular Video", "comments": "Submitted to IV2019. 7 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many automotive applications, such as Advanced Driver Assistance Systems\n(ADAS) for collision avoidance and warnings, require estimating the future\nautomotive risk of a driving scene. We present a low-cost system that predicts\nthe collision risk over an intermediate time horizon from a monocular video\nsource, such as a dashboard-mounted camera. The modular system includes\ncomponents for object detection, object tracking, and state estimation. We\nintroduce solutions to the object tracking and distance estimation problems.\nAdvanced approaches to the other tasks are used to produce real-time\npredictions of the automotive risk for the next 10 s at over 5 Hz. The system\nis designed such that alternative components can be substituted with minimal\neffort. It is demonstrated on common physical hardware, specifically an\noff-the-shelf gaming laptop and a webcam. We extend the framework to support\nabsolute speed estimation and more advanced risk estimation techniques.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 16:35:19 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Phillips", "Derek J.", ""], ["Aragon", "Juan Carlos", ""], ["Roychowdhury", "Anjali", ""], ["Madigan", "Regina", ""], ["Chintakindi", "Sunil", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "1902.01314", "submitter": "Abhijit Guha Roy", "authors": "Abhijit Guha Roy, Shayan Siddiqui, Sebastian P\\\"olsterl, Nassir Navab,\n  Christian Wachinger", "title": "'Squeeze & Excite' Guided Few-Shot Segmentation of Volumetric Images", "comments": "Accepted for publication at Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks enable highly accurate image segmentation, but require\nlarge amounts of manually annotated data for supervised training. Few-shot\nlearning aims to address this shortcoming by learning a new class from a few\nannotated support examples. We introduce, a novel few-shot framework, for the\nsegmentation of volumetric medical images with only a few annotated slices.\nCompared to other related works in computer vision, the major challenges are\nthe absence of pre-trained networks and the volumetric nature of medical scans.\nWe address these challenges by proposing a new architecture for few-shot\nsegmentation that incorporates 'squeeze & excite' blocks. Our two-armed\narchitecture consists of a conditioner arm, which processes the annotated\nsupport input and generates a task-specific representation. This representation\nis passed on to the segmenter arm that uses this information to segment the new\nquery image. To facilitate efficient interaction between the conditioner and\nthe segmenter arm, we propose to use 'channel squeeze & spatial excitation'\nblocks - a light-weight computational module - that enables heavy interaction\nbetween both the arms with negligible increase in model complexity. This\ncontribution allows us to perform image segmentation without relying on a\npre-trained model, which generally is unavailable for medical scans.\nFurthermore, we propose an efficient strategy for volumetric segmentation by\noptimally pairing a few slices of the support volume to all the slices of the\nquery volume. We perform experiments for organ segmentation on whole-body\ncontrast-enhanced CT scans from the Visceral Dataset. Our proposed model\noutperforms multiple baselines and existing approaches with respect to the\nsegmentation accuracy by a significant margin. The source code is available at\nhttps://github.com/abhi4ssj/few-shot-segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 17:11:32 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 12:51:22 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Roy", "Abhijit Guha", ""], ["Siddiqui", "Shayan", ""], ["P\u00f6lsterl", "Sebastian", ""], ["Navab", "Nassir", ""], ["Wachinger", "Christian", ""]]}, {"id": "1902.01338", "submitter": "Amelia Jim\\'enez-S\\'anchez", "authors": "Amelia Jim\\'enez-S\\'anchez, Anees Kazi, Shadi Albarqouni, Chlodwig\n  Kirchhoff, Peter Biberthaler, Nassir Navab, Sonja Kirchhoff, Diana Mateus", "title": "Precise Proximal Femur Fracture Classification for Interactive Training\n  and Surgical Planning", "comments": "Accepted at IPCAI 2020 and IJCARS", "journal-ref": null, "doi": "10.1007/s11548-020-02150-x", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the feasibility of a fully automatic computer-aided diagnosis\n(CAD) tool, based on deep learning, that localizes and classifies proximal\nfemur fractures on X-ray images according to the AO classification. The\nproposed framework aims to improve patient treatment planning and provide\nsupport for the training of trauma surgeon residents. A database of 1347\nclinical radiographic studies was collected. Radiologists and trauma surgeons\nannotated all fractures with bounding boxes, and provided a classification\naccording to the AO standard. The proposed CAD tool for the classification of\nradiographs into types \"A\", \"B\" and \"not-fractured\", reaches a F1-score of 87%\nand AUC of 0.95, when classifying fractures versus not-fractured cases it\nimproves up to 94% and 0.98. Prior localization of the fracture results in an\nimprovement with respect to full image classification. 100% of the predicted\ncenters of the region of interest are contained in the manually provided\nbounding boxes. The system retrieves on average 9 relevant images (from the\nsame class) out of 10 cases. Our CAD scheme localizes, detects and further\nclassifies proximal femur fractures achieving results comparable to\nexpert-level and state-of-the-art performance. Our auxiliary localization model\nwas highly accurate predicting the region of interest in the radiograph. We\nfurther investigated several strategies of verification for its adoption into\nthe daily clinical routine. A sensitivity analysis of the size of the ROI and\nimage retrieval as a clinical use case were presented.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 18:00:24 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 14:16:19 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Jim\u00e9nez-S\u00e1nchez", "Amelia", ""], ["Kazi", "Anees", ""], ["Albarqouni", "Shadi", ""], ["Kirchhoff", "Chlodwig", ""], ["Biberthaler", "Peter", ""], ["Navab", "Nassir", ""], ["Kirchhoff", "Sonja", ""], ["Mateus", "Diana", ""]]}, {"id": "1902.01374", "submitter": "Wei Liu", "authors": "Wei Liu, Xianxu Hou, Jiang Duan, Guoping Qiu", "title": "End-to-End Single Image Fog Removal using Enhanced Cycle Consistent\n  Adversarial Networks", "comments": "Submitted to IEEE Transactions on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image defogging is a classical and challenging problem in computer\nvision. Existing methods towards this problem mainly include handcrafted priors\nbased methods that rely on the use of the atmospheric degradation model and\nlearning based approaches that require paired fog-fogfree training example\nimages. In practice, however, prior-based methods are prone to failure due to\ntheir own limitations and paired training data are extremely difficult to\nacquire. Inspired by the principle of CycleGAN network, we have developed an\nend-to-end learning system that uses unpaired fog and fogfree training images,\nadversarial discriminators and cycle consistency losses to automatically\nconstruct a fog removal system. Similar to CycleGAN, our system has two\ntransformation paths; one maps fog images to a fogfree image domain and the\nother maps fogfree images to a fog image domain. Instead of one stage mapping,\nour system uses a two stage mapping strategy in each transformation path to\nenhance the effectiveness of fog removal. Furthermore, we make explicit use of\nprior knowledge in the networks by embedding the atmospheric degradation\nprinciple and a sky prior for mapping fogfree images to the fog images domain.\nIn addition, we also contribute the first real world nature fog-fogfree image\ndataset for defogging research. Our multiple real fog images dataset (MRFID)\ncontains images of 200 natural outdoor scenes. For each scene, there are one\nclear image and corresponding four foggy images of different fog densities\nmanually selected from a sequence of images taken by a fixed camera over the\ncourse of one year. Qualitative and quantitative comparison against several\nstate-of-the-art methods on both synthetic and real world images demonstrate\nthat our approach is effective and performs favorably for recovering a clear\nimage from a foggy image.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 18:43:07 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Liu", "Wei", ""], ["Hou", "Xianxu", ""], ["Duan", "Jiang", ""], ["Qiu", "Guoping", ""]]}, {"id": "1902.01400", "submitter": "Wajih Ullah Baig", "authors": "Wajih Ullah Baig, Adeel Ejaz, Umar Munir and Kashif Sardar", "title": "Partial Fingerprint Detection Using Core Point Location", "comments": "5 pages. 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Biometric identification, fingerprints based identification has been the\nwidely accepted mechanism. Automated fingerprints identification/verification\ntechniques are widely adopted in many civilian and forensic applications. In\nforensic applications fingerprints are usually incomplete, broken, unclear or\ndegraded which are known as partial fingerprints. Fingerprints\nidentification/verification largely suffer from the problem of handling partial\nfingerprints. In this paper a novel and simple approach is presented for\ndetecting partial fingerprints using core point location. Our techniques is\nparticularly useful during the acquisition stage as to determine whether a user\nneeds to re-align the finger to ensure a complete capture of fingerprint\narea.This technique is tested on FVC-2002 DB1A. The results are very accurate\nwhich are presented in the Results sections.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 18:02:41 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Baig", "Wajih Ullah", ""], ["Ejaz", "Adeel", ""], ["Munir", "Umar", ""], ["Sardar", "Kashif", ""]]}, {"id": "1902.01439", "submitter": "Chenge Li", "authors": "Chenge Li, Weixi Zhang, Yong Liu, Yao Wang", "title": "Very Long Term Field of View Prediction for 360-degree Video Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  360-degree videos have gained increasing popularity in recent years with the\ndevelopments and advances in Virtual Reality (VR) and Augmented Reality (AR)\ntechnologies. In such applications, a user only watches a video scene within a\nfield of view (FoV) centered in a certain direction. Predicting the future FoV\nin a long time horizon (more than seconds ahead) can help save bandwidth\nresources in on-demand video streaming while minimizing video freezing in\nnetworks with significant bandwidth variations. In this work, we treat the FoV\nprediction as a sequence learning problem, and propose to predict the target\nuser's future FoV not only based on the user's own past FoV center trajectory\nbut also other users' future FoV locations. We propose multiple prediction\nmodels based on two different FoV representations: one using FoV center\ntrajectories and another using equirectangular heatmaps that represent the FoV\ncenter distributions. Extensive evaluations with two public datasets\ndemonstrate that the proposed models can significantly outperform benchmark\nmodels, and other users' FoVs are very helpful for improving long-term\npredictions.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 19:43:40 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Li", "Chenge", ""], ["Zhang", "Weixi", ""], ["Liu", "Yong", ""], ["Wang", "Yao", ""]]}, {"id": "1902.01466", "submitter": "Chenge Li", "authors": "Chenge Li, Gregory Dobler, Xin Feng, Yao Wang", "title": "TrackNet: Simultaneous Object Detection and Tracking and Its Application\n  in Traffic Video Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection and object tracking are usually treated as two separate\nprocesses. Significant progress has been made for object detection in 2D images\nusing deep learning networks. The usual tracking-by-detection pipeline for\nobject tracking requires that the object is successfully detected in the first\nframe and all subsequent frames, and tracking is done by associating detection\nresults. Performing object detection and object tracking through a single\nnetwork remains a challenging open question. We propose a novel network\nstructure named trackNet that can directly detect a 3D tube enclosing a moving\nobject in a video segment by extending the faster R-CNN framework. A Tube\nProposal Network (TPN) inside the trackNet is proposed to predict the\nobjectness of each candidate tube and location parameters specifying the\nbounding tube. The proposed framework is applicable for detecting and tracking\nany object and in this paper, we focus on its application for traffic video\nanalysis. The proposed model is trained and tested on UA-DETRAC, a large\ntraffic video dataset available for multi-vehicle detection and tracking, and\nobtained very promising results.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 21:39:17 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Li", "Chenge", ""], ["Dobler", "Gregory", ""], ["Feng", "Xin", ""], ["Wang", "Yao", ""]]}, {"id": "1902.01496", "submitter": "Icaro Oliveira", "authors": "Icaro O. de Oliveira, Keiko V. O. Fonseca and Rodrigo Minetto", "title": "A Two-Stream Siamese Neural Network for Vehicle Re-Identification by\n  Using Non-Overlapping Cameras", "comments": "5 pages, 6 figures, To appear in IEEE International Conference on\n  Image Processing (ICIP), Sept. 22-25, 2019, Taipei, Taiwan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe in this paper a Two-Stream Siamese Neural Network for vehicle\nre-identification. The proposed network is fed simultaneously with small coarse\npatches of the vehicle shape's, with 96 x 96 pixels, in one stream, and fine\nfeatures extracted from license plate patches, easily readable by humans, with\n96 x 48 pixels, in the other one. Then, we combined the strengths of both\nstreams by merging the Siamese distance descriptors with a sequence of fully\nconnected layers, as an attempt to tackle a major problem in the field, false\nalarms caused by a huge number of car design and models with nearly the same\nappearance or by similar license plate strings. In our experiments, with 2\nhours of videos containing 2982 vehicles, extracted from two low-cost cameras\nin the same roadway, 546 ft away, we achieved a F-measure and accuracy of 92.6%\nand 98.7%, respectively. We show that the proposed network, available at\nhttps://github.com/icarofua/siamese-two-stream, outperforms other One-Stream\narchitectures, even if they use higher resolution image features.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 23:44:00 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 01:31:06 GMT"}, {"version": "v3", "created": "Thu, 4 Apr 2019 15:47:26 GMT"}, {"version": "v4", "created": "Wed, 15 May 2019 13:54:05 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["de Oliveira", "Icaro O.", ""], ["Fonseca", "Keiko V. O.", ""], ["Minetto", "Rodrigo", ""]]}, {"id": "1902.01522", "submitter": "Jialei Chen", "authors": "Jialei Chen, Yujia Xie, Kan Wang, Chuck Zhang, Mani A. Vannan, Ben\n  Wang, Zhen Qian", "title": "Active Image Synthesis for Efficient Labeling", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2020", "doi": "10.1109/TPAMI.2020.2993221", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The great success achieved by deep neural networks attracts increasing\nattention from the manufacturing and healthcare communities. However, the\nlimited availability of data and high costs of data collection are the major\nchallenges for the applications in those fields. We propose in this work AISEL,\nan active image synthesis method for efficient labeling to improve the\nperformance of the small-data learning tasks. Specifically, a complementary\nAISEL dataset is generated, with labels actively acquired via a physics-based\nmethod to incorporate underlining physical knowledge at hand. An important\ncomponent of our AISEL method is the bidirectional generative invertible\nnetwork (GIN), which can extract interpretable features from the training\nimages and generate physically meaningful virtual images. Our AISEL method then\nefficiently samples virtual images not only further exploits the uncertain\nregions, but also explores the entire image space. We then discuss the\ninterpretability of GIN both theoretically and experimentally, demonstrating\nclear visual improvements over the benchmarks. Finally, we demonstrate the\neffectiveness of our AISEL framework on aortic stenosis application, in which\nour method lower the labeling cost by $90\\%$ while achieving a $15\\%$\nimprovement in prediction accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 02:49:09 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 14:53:14 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 22:58:31 GMT"}, {"version": "v4", "created": "Tue, 4 Aug 2020 15:04:32 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chen", "Jialei", ""], ["Xie", "Yujia", ""], ["Wang", "Kan", ""], ["Zhang", "Chuck", ""], ["Vannan", "Mani A.", ""], ["Wang", "Ben", ""], ["Qian", "Zhen", ""]]}, {"id": "1902.01534", "submitter": "\\'Alvaro Parra", "authors": "\\'Alvaro Parra, Tat-Jun Chin, Frank Neumann, Tobias Friedrich,\n  Maximilian Katzmann", "title": "A Practical Maximum Clique Algorithm for Matching with Pairwise\n  Constraints", "comments": "Code and demo program are available in the supplementary material. 9\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular paradigm for 3D point cloud registration is by extracting 3D\nkeypoint correspondences, then estimating the registration function from the\ncorrespondences using a robust algorithm. However, many existing 3D keypoint\ntechniques tend to produce large proportions of erroneous correspondences or\noutliers, which significantly increases the cost of robust estimation. An\nalternative approach is to directly search for the subset of correspondences\nthat are pairwise consistent, without optimising the registration function.\nThis gives rise to the combinatorial problem of matching with pairwise\nconstraints. In this paper, we propose a very efficient maximum clique\nalgorithm to solve matching with pairwise constraints. Our technique combines\ntree searching with efficient bounding and pruning based on graph colouring. We\ndemonstrate that, despite the theoretical intractability, many real problem\ninstances can be solved exactly and quickly (seconds to minutes) with our\nalgorithm, which makes our approach an excellent alternative to standard robust\ntechniques for 3D registration.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 04:06:26 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 05:13:59 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Parra", "\u00c1lvaro", ""], ["Chin", "Tat-Jun", ""], ["Neumann", "Frank", ""], ["Friedrich", "Tobias", ""], ["Katzmann", "Maximilian", ""]]}, {"id": "1902.01549", "submitter": "Huu Le", "authors": "Huu Le, Tuan Hoang, Qianggong Zhang, Thanh-Toan Do, Anders Eriksson,\n  Michael Milford", "title": "SASSE: Scalable and Adaptable 6-DOF Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual localization has become a key enabling component of many place\nrecognition and SLAM systems. Contemporary research has primarily focused on\nimproving accuracy and precision-recall type metrics, with relatively little\nattention paid to a system's absolute storage scaling characteristics, its\nflexibility to adapt to available computational resources, and its longevity\nwith respect to easily incorporating newly learned or hand-crafted image\ndescriptors. Most significantly, improvement in one of these aspects typically\ncomes at the cost of others: for example, a snapshot-based system that achieves\nsub-linear storage cost typically provides no metric pose estimation, or, a\nhighly accurate pose estimation technique is often ossified in adapting to\nrecent advances in appearance-invariant features. In this paper, we present a\nnovel 6-DOF localization system that for the first time simultaneously achieves\nall the three characteristics: significantly sub-linear storage growth,\nagnosticism to image descriptors, and customizability to available storage and\ncomputational resources. The key features of our method are developed based on\na novel adaptation of multiple-label learning, together with effective\ndimensional reduction and learning techniques that enable simple and efficient\noptimization. We evaluate our system on several large benchmarking datasets and\nprovide detailed comparisons to state-of-the-art systems. The proposed method\ndemonstrates competitive accuracy with existing pose estimation methods while\nachieving better sub-linear storage scaling, significantly reduced absolute\nstorage requirements, and faster training and deployment speeds.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 05:13:54 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Le", "Huu", ""], ["Hoang", "Tuan", ""], ["Zhang", "Qianggong", ""], ["Do", "Thanh-Toan", ""], ["Eriksson", "Anders", ""], ["Milford", "Michael", ""]]}, {"id": "1902.01559", "submitter": "Hiromasa Fujihara", "authors": "Nguyen Van Quang, Hiromasa Fujihara", "title": "Revisiting a single-stage method for face detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although accurate, two-stage face detectors usually require more inference\ntime than single-stage detectors do. This paper proposes a simple yet effective\nsingle-stage model for real-time face detection with a prominently high\naccuracy. We build our single-stage model on the top of the ResNet-101 backbone\nand analyze some problems with the baseline single-stage detector in order to\ndesign several strategies for reducing the false positive rate. The design\nleverages the context information from the deeper layers in order to increase\nrecall rate while maintaining a low false positive rate. In addition, we reduce\nthe detection time by an improved inference procedure for decoding outputs\nfaster. The inference time of a VGA ($640{\\times}480$) image was only\napproximately 26 ms with a Titan X GPU. The effectiveness of our proposed\nmethod was evaluated on several face detection benchmarks (Wider Face, AFW,\nPascal Face, and FDDB). The experiments show that our method achieved\ncompetitive results on these popular datasets with a faster runtime than the\ncurrent best two-stage practices.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 06:02:20 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Van Quang", "Nguyen", ""], ["Fujihara", "Hiromasa", ""]]}, {"id": "1902.01626", "submitter": "Markus Suchi", "authors": "Markus Suchi, Timothy Patten, David Fischinger, Markus Vincze", "title": "EasyLabel: A Semi-Automatic Pixel-wise Object Annotation Tool for\n  Creating Robotic RGB-D Datasets", "comments": "7 pages, 8 figures, ICRA2019, Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing robot perception systems for recognizing objects in the real-world\nrequires computer vision algorithms to be carefully scrutinized with respect to\nthe expected operating domain. This demands large quantities of ground truth\ndata to rigorously evaluate the performance of algorithms. This paper presents\nthe EasyLabel tool for easily acquiring high quality ground truth annotation of\nobjects at the pixel-level in densely cluttered scenes. In a semi-automatic\nprocess, complex scenes are incrementally built and EasyLabel exploits depth\nchange to extract precise object masks at each step. We use this tool to\ngenerate the Object Cluttered Indoor Dataset (OCID) that captures diverse\nsettings of objects, background, context, sensor to scene distance, viewpoint\nangle and lighting conditions. OCID is used to perform a systematic comparison\nof existing object segmentation methods. The baseline comparison supports the\nneed for pixel- and object-wise annotation to progress robot vision towards\nrealistic applications. This insight reveals the usefulness of EasyLabel and\nOCID to better understand the challenges that robots face in the real-world.\n  Copyright 20XX IEEE. Personal use of this material is permitted. Permission\nfrom IEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or redistribution to\nservers or lists, or reuse of any copyrighted component of this work in other\nworks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 10:14:12 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 07:09:34 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Suchi", "Markus", ""], ["Patten", "Timothy", ""], ["Fischinger", "David", ""], ["Vincze", "Markus", ""]]}, {"id": "1902.01654", "submitter": "Guillaume Michel", "authors": "Guillaume Michel, Mohammed Amine Alaoui, Alice Lebois, Amal Feriani,\n  Mehdi Felhi", "title": "DVOLVER: Efficient Pareto-Optimal Neural Network Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic search of neural network architectures is a standing research\ntopic. In addition to the fact that it presents a faster alternative to\nhand-designed architectures, it can improve their efficiency and for instance\ngenerate Convolutional Neural Networks (CNN) adapted for mobile devices. In\nthis paper, we present a multi-objective neural architecture search method to\nfind a family of CNN models with the best accuracy and computational resources\ntradeoffs, in a search space inspired by the state-of-the-art findings in\nneural search. Our work, called Dvolver, evolves a population of architectures\nand iteratively improves an approximation of the optimal Pareto front. Applying\nDvolver on the model accuracy and on the number of floating points operations\nas objective functions, we are able to find, in only 2.5 days, a set of\ncompetitive mobile models on ImageNet. Amongst these models one architecture\nhas the same Top-1 accuracy on ImageNet as NASNet-A mobile with 8% less\nfloating point operations and another one has a Top-1 accuracy of 75.28% on\nImageNet exceeding by 0.28% the best MobileNetV2 model for the same\ncomputational resources.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 12:05:17 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Michel", "Guillaume", ""], ["Alaoui", "Mohammed Amine", ""], ["Lebois", "Alice", ""], ["Feriani", "Amal", ""], ["Felhi", "Mehdi", ""]]}, {"id": "1902.01728", "submitter": "Jin Liu", "authors": "Jin Liu and Sheng He", "title": "6D Object Pose Estimation without PnP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient end-to-end algorithm to tackle the\nproblem of estimating the 6D pose of objects from a single RGB image. Our\nsystem trains a fully convolutional network to regress the 3D rotation and the\n3D translation in region layer. On this basis, a special layer, Collinear\nEquation Layer, is added next to region layer to output the 2D projections of\nthe 3D bounding boxs corners. In the back propagation stage, the 6D pose\nnetwork are adjusted according to the error of the 2D projections. In the\ndetection phase, we directly output the position and pose through the region\nlayer. Besides, we introduce a novel and concise representation of 3D rotation\nto make the regression more precise and easier. Experiments show that our\nmethod outperforms base-line and state of the art methods both at accuracy and\nefficiency. In the LineMod dataset, our algorithm achieves less than 18\nms/object on a GeForce GTX 1080Ti GPU, while the translational error and\nrotational error are less than 1.67 cm and 2.5 degree.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 15:04:36 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 12:40:06 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Liu", "Jin", ""], ["He", "Sheng", ""]]}, {"id": "1902.01739", "submitter": "Stefan Becker", "authors": "Stefan Becker and Ronny Hug and Wolfgang H\\\"ubner and Michael Arens", "title": "An RNN-based IMM Filter Surrogate", "comments": "Accepted at Scandinavian Conference on Image Analysis (SCIA) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of varying dynamics of tracked objects, such as pedestrians, is\ntraditionally tackled with approaches like the Interacting Multiple Model (IMM)\nfilter using a Bayesian formulation. By following the current trend towards\nusing deep neural networks, in this paper an RNN-based IMM filter surrogate is\npresented. Similar to an IMM filter solution, the presented RNN-based model\nassigns a probability value to a performed dynamic and, based on them, puts out\na multi-modal distribution over future pedestrian trajectories. The evaluation\nis done on synthetic data, reflecting prototypical pedestrian maneuvers.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 15:21:53 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 07:48:38 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Becker", "Stefan", ""], ["Hug", "Ronny", ""], ["H\u00fcbner", "Wolfgang", ""], ["Arens", "Michael", ""]]}, {"id": "1902.01824", "submitter": "Suleyman Aslan", "authors": "S\\\"uleyman Aslan, U\\u{g}ur G\\\"ud\\\"ukbay, B. U\\u{g}ur T\\\"oreyin, A.\n  Enis \\c{C}etin", "title": "Deep Convolutional Generative Adversarial Networks Based Flame Detection\n  in Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time flame detection is crucial in video based surveillance systems. We\npropose a vision-based method to detect flames using Deep Convolutional\nGenerative Adversarial Neural Networks (DCGANs). Many existing supervised\nlearning approaches using convolutional neural networks do not take temporal\ninformation into account and require substantial amount of labeled data. In\norder to have a robust representation of sequences with and without flame, we\npropose a two-stage training of a DCGAN exploiting spatio-temporal flame\nevolution. Our training framework includes the regular training of a DCGAN with\nreal spatio-temporal images, namely, temporal slice images, and noise vectors,\nand training the discriminator separately using the temporal flame images\nwithout the generator. Experimental results show that the proposed method\neffectively detects flame in video with negligible false positive rates in\nreal-time.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 17:53:42 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Aslan", "S\u00fcleyman", ""], ["G\u00fcd\u00fckbay", "U\u011fur", ""], ["T\u00f6reyin", "B. U\u011fur", ""], ["\u00c7etin", "A. Enis", ""]]}, {"id": "1902.01831", "submitter": "Jos\\'e Miguel Buenaposada", "authors": "Roberto Valle (1), Jos\\'e M. Buenaposada (2), Antonio Vald\\'es (3),\n  Luis Baumela (1) ((1) Universidad Polit\\'ecnica de Madrid, (2) Universidad\n  Rey Juan Carlos, (3) Universidad Complutense de Madrid)", "title": "Face Alignment using a 3D Deeply-initialized Ensemble of Regression\n  Trees", "comments": "Accepted Version to Computer Vision and Image Understanding", "journal-ref": null, "doi": "10.1016/j.cviu.2019.102846", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face alignment algorithms locate a set of landmark points in images of faces\ntaken in unrestricted situations. State-of-the-art approaches typically fail or\nlose accuracy in the presence of occlusions, strong deformations, large pose\nvariations and ambiguous configurations. In this paper we present 3DDE, a\nrobust and efficient face alignment algorithm based on a coarse-to-fine cascade\nof ensembles of regression trees. It is initialized by robustly fitting a 3D\nface model to the probability maps produced by a convolutional neural network.\nWith this initialization we address self-occlusions and large face rotations.\nFurther, the regressor implicitly imposes a prior face shape on the solution,\naddressing occlusions and ambiguous face configurations. Its coarse-to-fine\nstructure tackles the combinatorial explosion of parts deformation. In the\nexperiments performed, 3DDE improves the state-of-the-art in 300W, COFW, AFLW\nand WFLW data sets. Finally, we perform cross-dataset experiments that reveal\nthe existence of a significant data set bias in these benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 18:07:17 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 10:15:08 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Valle", "Roberto", ""], ["Buenaposada", "Jos\u00e9 M.", ""], ["Vald\u00e9s", "Antonio", ""], ["Baumela", "Luis", ""]]}, {"id": "1902.01855", "submitter": "Svitlana Alkhimova", "authors": "Svitlana Alkhimova", "title": "Detection of perfusion ROI as a quality control in perfusion analysis", "comments": null, "journal-ref": "in Proc. Science, Research, Development. Technics and Technology,\n  Berlin, Germany, Jan 30, 2018, pp.57-59", "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In perfusion analysis automated approaches for image processing is preferable\ndue to reduce time-consuming tasks for radiologists. Assessment of perfusion\nresults quality is important step in development of algorithms for automated\nprocessing. One of them is an assessment of perfusion maps quality based on\ndetection of perfusion ROI.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 22:18:04 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Alkhimova", "Svitlana", ""]]}, {"id": "1902.01878", "submitter": "Sagar Sharma", "authors": "Sagar Sharma, Keke Chen", "title": "Disguised-Nets: Image Disguising for Privacy-preserving Outsourced Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning model developers often use cloud GPU resources to experiment\nwith large data and models that need expensive setups. However, this practice\nraises privacy concerns. Adversaries may be interested in: 1) personally\nidentifiable information or objects encoded in the training images, and 2) the\nmodels trained with sensitive data to launch model-based attacks. Learning deep\nneural networks (DNN) from encrypted data is still impractical due to the large\ntraining data and the expensive learning process. A few recent studies have\ntried to provide efficient, practical solutions to protect data privacy in\noutsourced deep-learning. However, we find out that they are vulnerable under\ncertain attacks. In this paper, we specifically identify two types of unique\nattacks on outsourced deep-learning: 1) the visual re-identification attack on\nthe training data, and 2) the class membership attack on the learned models,\nwhich can break existing privacy-preserving solutions. We develop an image\ndisguising approach to address these attacks and design a suite of methods to\nevaluate the levels of attack resilience for a privacy-preserving solution for\noutsourced deep learning. The experimental results show that our\nimage-disguising mechanisms can provide a high level of protection against the\ntwo attacks while still generating high-quality DNN models for image\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 19:20:02 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 04:31:54 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Sharma", "Sagar", ""], ["Chen", "Keke", ""]]}, {"id": "1902.01926", "submitter": "Nagender Aneja", "authors": "Sandhya Aneja, Nagender Aneja, Md Shohidul Islam", "title": "IoT Device Fingerprint using Deep Learning", "comments": null, "journal-ref": null, "doi": "10.1109/IOTAIS.2018.8600824", "report-no": null, "categories": "cs.NI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Device Fingerprinting (DFP) is the identification of a device without using\nits network or other assigned identities including IP address, Medium Access\nControl (MAC) address, or International Mobile Equipment Identity (IMEI)\nnumber. DFP identifies a device using information from the packets which the\ndevice uses to communicate over the network. Packets are received at a router\nand processed to extract the information. In this paper, we worked on the DFP\nusing Inter Arrival Time (IAT). IAT is the time interval between the two\nconsecutive packets received. This has been observed that the IAT is unique for\na device because of different hardware and the software used for the device.\nThe existing work on the DFP uses the statistical techniques to analyze the IAT\nand to further generate the information using which a device can be identified\nuniquely. This work presents a novel idea of DFP by plotting graphs of IAT for\npackets with each graph plotting 100 IATs and subsequently processing the\nresulting graphs for the identification of the device. This approach improves\nthe efficiency to identify a device DFP due to achieved benchmark of the deep\nlearning libraries in the image processing. We configured Raspberry Pi to work\nas a router and installed our packet sniffer application on the Raspberry Pi .\nThe packet sniffer application captured the packet information from the\nconnected devices in a log file. We connected two Apple devices iPad4 and\niPhone 7 Plus to the router and created IAT graphs for these two devices. We\nused Convolution Neural Network (CNN) to identify the devices and observed the\naccuracy of 86.7%.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 03:28:44 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Aneja", "Sandhya", ""], ["Aneja", "Nagender", ""], ["Islam", "Md Shohidul", ""]]}, {"id": "1902.01977", "submitter": "Arjun Desai", "authors": "Arjun D. Desai, Garry E. Gold, Brian A. Hargreaves, Akshay S.\n  Chaudhari", "title": "Technical Considerations for Semantic Segmentation in MRI using\n  Convolutional Neural Networks", "comments": "Submitted to Magnetic Resonance in Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-fidelity semantic segmentation of magnetic resonance volumes is critical\nfor estimating tissue morphometry and relaxation parameters in both clinical\nand research applications. While manual segmentation is accepted as the\ngold-standard, recent advances in deep learning and convolutional neural\nnetworks (CNNs) have shown promise for efficient automatic segmentation of soft\ntissues. However, due to the stochastic nature of deep learning and the\nmultitude of hyperparameters in training networks, predicting network behavior\nis challenging. In this paper, we quantify the impact of three factors\nassociated with CNN segmentation performance: network architecture, training\nloss functions, and training data characteristics. We evaluate the impact of\nthese variations on the segmentation of femoral cartilage and propose potential\nmodifications to CNN architectures and training protocols to train these models\nwith confidence.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 23:39:17 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Desai", "Arjun D.", ""], ["Gold", "Garry E.", ""], ["Hargreaves", "Brian A.", ""], ["Chaudhari", "Akshay S.", ""]]}, {"id": "1902.01980", "submitter": "Yueru Chen", "authors": "Yueru Chen, Yijing Yang, Min Zhang and C.-C. Jay Kuo", "title": "Semi-supervised learning via Feedforward-Designed Convolutional Neural\n  Networks", "comments": "5 pages, under review of ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A semi-supervised learning framework using the feedforward-designed\nconvolutional neural networks (FF-CNNs) is proposed for image classification in\nthis work. One unique property of FF-CNNs is that no backpropagation is used in\nmodel parameters determination. Since unlabeled data may not always enhance\nsemi-supervised learning, we define an effective quality score and use it to\nselect a subset of unlabeled data in the training process. We conduct\nexperiments on the MNIST, SVHN, and CIFAR-10 datasets, and show that the\nproposed semi-supervised FF-CNN solution outperforms the CNN trained by\nbackpropagation (BP-CNN) when the amount of labeled data is reduced.\nFurthermore, we develop an ensemble system that combines the output decision\nvectors of different semi-supervised FF-CNNs to boost classification accuracy.\nThe ensemble systems can achieve further performance gains on all three\nbenchmarking datasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 00:00:23 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Chen", "Yueru", ""], ["Yang", "Yijing", ""], ["Zhang", "Min", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1902.02037", "submitter": "Hao Wang", "authors": "Hao Wang, Chengzhi Mao, Hao He, Mingmin Zhao, Tommi S. Jaakkola, Dina\n  Katabi", "title": "Bidirectional Inference Networks: A Class of Deep Bayesian Networks for\n  Health Profiling", "comments": "Appeared at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of inferring the values of an arbitrary set of\nvariables (e.g., risk of diseases) given other observed variables (e.g.,\nsymptoms and diagnosed diseases) and high-dimensional signals (e.g., MRI images\nor EEG). This is a common problem in healthcare since variables of interest\noften differ for different patients. Existing methods including Bayesian\nnetworks and structured prediction either do not incorporate high-dimensional\nsignals or fail to model conditional dependencies among variables. To address\nthese issues, we propose bidirectional inference networks (BIN), which stich\ntogether multiple probabilistic neural networks, each modeling a conditional\ndependency. Predictions are then made via iteratively updating variables using\nbackpropagation (BP) to maximize corresponding posterior probability.\nFurthermore, we extend BIN to composite BIN (CBIN), which involves the\niterative prediction process in the training stage and improves both accuracy\nand computational efficiency by adaptively smoothing the optimization\nlandscape. Experiments on synthetic and real-world datasets (a sleep study and\na dermatology dataset) show that CBIN is a single model that can achieve\nstate-of-the-art performance and obtain better accuracy in most inference tasks\nthan multiple models each specifically trained for a different task.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 06:10:46 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Wang", "Hao", ""], ["Mao", "Chengzhi", ""], ["He", "Hao", ""], ["Zhao", "Mingmin", ""], ["Jaakkola", "Tommi S.", ""], ["Katabi", "Dina", ""]]}, {"id": "1902.02041", "submitter": "Sunghwan Joo", "authors": "Juyeon Heo, Sunghwan Joo, Taesup Moon", "title": "Fooling Neural Network Interpretations via Adversarial Model\n  Manipulation", "comments": null, "journal-ref": "NeurIPS 2019, ICCV workshop 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We ask whether the neural network interpretation methods can be fooled via\nadversarial model manipulation, which is defined as a model fine-tuning step\nthat aims to radically alter the explanations without hurting the accuracy of\nthe original models, e.g., VGG19, ResNet50, and DenseNet121. By incorporating\nthe interpretation results directly in the penalty term of the objective\nfunction for fine-tuning, we show that the state-of-the-art saliency map based\ninterpreters, e.g., LRP, Grad-CAM, and SimpleGrad, can be easily fooled with\nour model manipulation. We propose two types of fooling, Passive and Active,\nand demonstrate such foolings generalize well to the entire validation set as\nwell as transfer to other interpretation methods. Our results are validated by\nboth visually showing the fooled explanations and reporting quantitative\nmetrics that measure the deviations from the original explanations. We claim\nthat the stability of neural network interpretation method with respect to our\nadversarial model manipulation is an important criterion to check for\ndeveloping robust and reliable neural network interpretation method.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 06:28:09 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 08:31:14 GMT"}, {"version": "v3", "created": "Fri, 1 Nov 2019 00:16:17 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Heo", "Juyeon", ""], ["Joo", "Sunghwan", ""], ["Moon", "Taesup", ""]]}, {"id": "1902.02059", "submitter": "Abdolreza Rashno Dr.", "authors": "Abdolreza Rashno and Elyas Rashno", "title": "Content-based image retrieval system with most relevant features among\n  wavelet and color features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based image retrieval (CBIR) has become one of the most important\nresearch directions in the domain of digital data management. In this paper, a\nnew feature extraction schema including the norm of low frequency components in\nwavelet transformation and color features in RGB and HSV domains are proposed\nas representative feature vector for images in database followed by appropriate\nsimilarity measure for each feature type. In CBIR systems, retrieving results\nare so sensitive to image features. We address this problem with selection of\nmost relevant features among complete feature set by ant colony optimization\n(ACO)-based feature selection which minimize the number of features as well as\nmaximize F-measure in CBIR system. To evaluate the performance of our proposed\nCBIR system, it has been compared with three older proposed systems. Results\nshow that the precision and recall of our proposed system are higher than older\nones for the majority of image categories in Corel database.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 08:22:06 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Rashno", "Abdolreza", ""], ["Rashno", "Elyas", ""]]}, {"id": "1902.02067", "submitter": "Derui Derek Wang", "authors": "Derui Wang, Chaoran Li, Sheng Wen, Qing-Long Han, Surya Nepal, Xiangyu\n  Zhang, Yang Xiang", "title": "Daedalus: Breaking Non-Maximum Suppression in Object Detection via\n  Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates that Non-Maximum Suppression (NMS), which is commonly\nused in Object Detection (OD) tasks to filter redundant detection results, is\nno longer secure. Considering that NMS has been an integral part of OD systems,\nthwarting the functionality of NMS can result in unexpected or even lethal\nconsequences for such systems. In this paper, an adversarial example attack\nwhich triggers malfunctioning of NMS in end-to-end OD models is proposed. The\nattack, namely \\texttt{Daedalus}, compresses the dimensions of detection boxes\nto evade NMS. As a result, the final detection output contains extremely dense\nfalse positives. This can be fatal for many OD applications such as autonomous\nvehicles and surveillance systems. The attack can be generalised to different\nend-to-end OD models, such that the attack cripples various OD applications.\nFurthermore, a way to craft robust adversarial examples is developed by using\nan ensemble of popular detection models as the substitutes. Considering the\npervasive nature of model reusing in real-world OD scenarios, Daedalus examples\ncrafted based on an \\textit{ensemble of substitutes} can launch attacks without\nknowing the parameters of the victim models. Experimental results demonstrate\nthat the attack effectively stops NMS from filtering redundant bounding boxes.\nAs the evaluation results suggest, Daedalus increases the false positive rate\nin detection results to $99.9\\%$ and reduces the mean average precision scores\nto $0$, while maintaining a low cost of distortion on the original inputs. It\nis also demonstrated that the attack can be practically launched against\nreal-world OD systems via printed posters.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 08:58:37 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 05:16:10 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2020 05:29:56 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Wang", "Derui", ""], ["Li", "Chaoran", ""], ["Wen", "Sheng", ""], ["Han", "Qing-Long", ""], ["Nepal", "Surya", ""], ["Zhang", "Xiangyu", ""], ["Xiang", "Yang", ""]]}, {"id": "1902.02086", "submitter": "Tom Roussel", "authors": "Punarjay Chakravarty, Praveen Narayanan, Tom Roussel", "title": "GEN-SLAM: Generative Modeling for Monocular Simultaneous Localization\n  and Mapping", "comments": "Accepted for ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Deep Learning based system for the twin tasks of localization\nand obstacle avoidance essential to any mobile robot. Our system learns from\nconventional geometric SLAM, and outputs, using a single camera, the\ntopological pose of the camera in an environment, and the depth map of\nobstacles around it. We use a CNN to localize in a topological map, and a\nconditional VAE to output depth for a camera image, conditional on this\ntopological location estimation. We demonstrate the effectiveness of our\nmonocular localization and depth estimation system on simulated and real\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 09:50:51 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Chakravarty", "Punarjay", ""], ["Narayanan", "Praveen", ""], ["Roussel", "Tom", ""]]}, {"id": "1902.02102", "submitter": "Lars Maal{\\o}e", "authors": "Lars Maal{\\o}e, Marco Fraccaro, Valentin Li\\'evin, Ole Winther", "title": "BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the introduction of the variational autoencoder (VAE), probabilistic\nlatent variable models have received renewed attention as powerful generative\nmodels. However, their performance in terms of test likelihood and quality of\ngenerated samples has been surpassed by autoregressive models without\nstochastic units. Furthermore, flow-based models have recently been shown to be\nan attractive alternative that scales well to high-dimensional data. In this\npaper we close the performance gap by constructing VAE models that can\neffectively utilize a deep hierarchy of stochastic variables and model complex\ncovariance structures. We introduce the Bidirectional-Inference Variational\nAutoencoder (BIVA), characterized by a skip-connected generative model and an\ninference network formed by a bidirectional stochastic inference path. We show\nthat BIVA reaches state-of-the-art test likelihoods, generates sharp and\ncoherent natural images, and uses the hierarchy of latent variables to capture\ndifferent aspects of the data distribution. We observe that BIVA, in contrast\nto recent results, can be used for anomaly detection. We attribute this to the\nhierarchy of latent variables which is able to extract high-level semantic\nfeatures. Finally, we extend BIVA to semi-supervised classification tasks and\nshow that it performs comparably to state-of-the-art results by generative\nadversarial networks.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 10:36:57 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 12:21:18 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 16:48:34 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Maal\u00f8e", "Lars", ""], ["Fraccaro", "Marco", ""], ["Li\u00e9vin", "Valentin", ""], ["Winther", "Ole", ""]]}, {"id": "1902.02144", "submitter": "Dwarikanath Mahapatra", "authors": "Dwarikanath Mahapatra and Behzad Bozorgtabar", "title": "Progressive Generative Adversarial Networks for Medical Image Super\n  resolution", "comments": "NA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anatomical landmark segmentation and pathology localization are important\nsteps in automated analysis of medical images. They are particularly\nchallenging when the anatomy or pathology is small, as in retinal images and\ncardiac MRI, or when the image is of low quality due to device acquisition\nparameters as in magnetic resonance (MR) scanners. We propose an image\nsuper-resolution method using progressive generative adversarial networks\n(P-GAN) that can take as input a low-resolution image and generate a high\nresolution image of desired scaling factor. The super resolved images can be\nused for more accurate detection of landmarks and pathology. Our primary\ncontribution is in proposing a multistage model where the output image quality\nof one stage is progressively improved in the next stage by using a triplet\nloss function. The triplet loss enables stepwise image quality improvement by\nusing the output of the previous stage as the baseline. This facilitates\ngeneration of super resolved images of high scaling factor while maintaining\ngood image quality. Experimental results for image super-resolution show that\nour proposed multistage P-GAN outperforms competing methods and baseline GAN.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 12:38:53 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 02:59:05 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Mahapatra", "Dwarikanath", ""], ["Bozorgtabar", "Behzad", ""]]}, {"id": "1902.02166", "submitter": "Yuxin Hou", "authors": "Yuxin Hou, Arno Solin, Juho Kannala", "title": "Unstructured Multi-View Depth Estimation Using Mask-Based Multiplane\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method, MaskMVS, to solve depth estimation for\nunstructured multi-view image-pose pairs. In the plane-sweep procedure, the\ndepth planes are sampled by histogram matching that ensures covering the depth\nrange of interest. Unlike other plane-sweep methods, we do not rely on a cost\nmetric to explicitly build the cost volume, but instead infer a multiplane mask\nrepresentation which regularizes the learning. Compared to many previous\napproaches, we show that our method is lightweight and generalizes well without\nrequiring excessive training. We outperform the current state-of-the-art and\nshow results on the sun3d, scenes11, MVS, and RGBD test data sets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 13:26:03 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 12:07:01 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Hou", "Yuxin", ""], ["Solin", "Arno", ""], ["Kannala", "Juho", ""]]}, {"id": "1902.02205", "submitter": "Anjany Kumar Sekuboyina", "authors": "Anjany Sekuboyina, Markus Rempfler, Alexander Valentinitsch, Bjoern H.\n  Menze, Jan S. Kirschke", "title": "Labelling Vertebrae with 2D Reformations of Multidetector CT Images: An\n  Adversarial Approach for Incorporating Prior Knowledge of Spine Anatomy", "comments": "Published in Radiology:AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To use and test a labelling algorithm that operates on\ntwo-dimensional (2D) reformations, rather than three-dimensional (3D) data to\nlocate and identify vertebrae.\n  Methods: We improved the Btrfly Net (described by Sekuboyina et al) that\nworks on sagittal and coronal maximum intensity projections (MIP) and augmented\nit with two additional components: spine-localization and adversarial a\npriori-learning. Furthermore, we explored two variants of adversarial training\nschemes that incorporated the anatomical a priori knowledge into the Btrfly\nNet. We investigated the superiority of the proposed approach for labelling\nvertebrae on three datasets: a public benchmarking dataset of 302 CT scans and\ntwo in-house datasets with a total of 238 CT scans. We employed Wilcoxon\nsigned-rank test to compute the statistical significance of the improvement in\nperformance observed due to various architectural components in our approach.\n  Results: On the public dataset, our approach using the described\nBtrfly(pe-eb) network performed on par with current state-of-the-art methods\nachieving a statistically significant (p < .001) vertebrae identification rate\nof 88.5+/-0.2 % and localization distances of less than 7-mm. On the in-house\ndatasets that had a higher inter-scan data variability, we obtained an\nidentification rate of 85.1+/-1.2%.\n  Conclusion: An identification performance comparable to existing 3D\napproaches was achieved when labelling vertebrae on 2D MIPs. The performance\nwas further improved using the proposed adversarial training regime that\neffectively enforced local spine a priori knowledge during training. Lastly,\nspine-localization increased the generalizability of our approach by\nhomogenizing the content in the MIPs.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 14:34:35 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 09:05:40 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 13:50:23 GMT"}, {"version": "v4", "created": "Fri, 26 Feb 2021 11:13:14 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Sekuboyina", "Anjany", ""], ["Rempfler", "Markus", ""], ["Valentinitsch", "Alexander", ""], ["Menze", "Bjoern H.", ""], ["Kirschke", "Jan S.", ""]]}, {"id": "1902.02248", "submitter": "Anant Gupta", "authors": "Anant Gupta, Srivas Venkatesh, Sumit Chopra, Christian Ledig", "title": "Generative Image Translation for Data Augmentation of Bone Lesion\n  Pathology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insufficient training data and severe class imbalance are often limiting\nfactors when developing machine learning models for the classification of rare\ndiseases. In this work, we address the problem of classifying bone lesions from\nX-ray images by increasing the small number of positive samples in the training\nset. We propose a generative data augmentation approach based on a\ncycle-consistent generative adversarial network that synthesizes bone lesions\non images without pathology. We pose the generative task as an image-patch\ntranslation problem that we optimize specifically for distinct bones (humerus,\ntibia, femur). In experimental results, we confirm that the described method\nmitigates the class imbalance problem in the binary classification task of bone\nlesion detection. We show that the augmented training sets enable the training\nof superior classifiers achieving better performance on a held-out test set.\nAdditionally, we demonstrate the feasibility of transfer learning and apply a\ngenerative model that was trained on one body part to another.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 15:57:39 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Gupta", "Anant", ""], ["Venkatesh", "Srivas", ""], ["Chopra", "Sumit", ""], ["Ledig", "Christian", ""]]}, {"id": "1902.02256", "submitter": "Kaveh Fathian", "authors": "Kaveh Fathian, Kasra Khosoussi, Yulun Tian, Parker Lusk, Jonathan P.\n  How", "title": "CLEAR: A Consistent Lifting, Embedding, and Alignment Rectification\n  Algorithm for Multi-View Data Association", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many robotics applications require alignment and fusion of observations\nobtained at multiple views to form a global model of the environment. Multi-way\ndata association methods provide a mechanism to improve alignment accuracy of\npairwise associations and ensure their consistency. However, existing methods\nthat solve this computationally challenging problem are often too slow for\nreal-time applications. Furthermore, some of the existing techniques can\nviolate the cycle consistency principle, thus drastically reducing the fusion\naccuracy. This work presents the CLEAR (Consistent Lifting, Embedding, and\nAlignment Rectification) algorithm to address these issues. By leveraging\ninsights from the multi-way matching and spectral graph clustering literature,\nCLEAR provides cycle consistent and accurate solutions in a computationally\nefficient manner. Numerical experiments on both synthetic and real datasets are\ncarried out to demonstrate the scalability and superior performance of our\nalgorithm in real-world problems. This algorithmic framework can provide\nsignificant improvement in the accuracy and efficiency of existing discrete\nassignment problems, which traditionally use pairwise (but potentially\ninconsistent) correspondences. An implementation of CLEAR is made publicly\navailable online.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 16:12:57 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 21:39:56 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 23:08:51 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Fathian", "Kaveh", ""], ["Khosoussi", "Kasra", ""], ["Tian", "Yulun", ""], ["Lusk", "Parker", ""], ["How", "Jonathan P.", ""]]}, {"id": "1902.02342", "submitter": "Dongming Wei", "authors": "Dongming Wei, Zhengwang Wu, Gang Li, Xiaohuan Cao, Dinggang Shen, Qian\n  Wang", "title": "Deep Morphological Simplification Network (MS-Net) for Guided\n  Registration of Brain Magnetic Resonance Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Deformable brain MR image registration is challenging due to large\ninter-subject anatomical variation. For example, the highly complex cortical\nfolding pattern makes it hard to accurately align corresponding cortical\nstructures of individual images. In this paper, we propose a novel deep\nlearning way to simplify the difficult registration problem of brain MR images.\nMethods: We train a morphological simplification network (MS-Net), which can\ngenerate a \"simple\" image with less anatomical details based on the \"complex\"\ninput. With MS-Net, the complexity of the fixed image or the moving image under\nregistration can be reduced gradually, thus building an individual\n(simplification) trajectory represented by MS-Net outputs. Since the generated\nimages at the ends of the two trajectories (of the fixed and moving images) are\nso simple and very similar in appearance, they are easy to register. Thus, the\ntwo trajectories can act as a bridge to link the fixed and the moving images,\nand guide their registration. Results: Our experiments show that the proposed\nmethod can achieve highly accurate registration performance on different\ndatasets (i.e., NIREP, LPBA, IBSR, CUMC, and MGH). Moreover, the method can be\nalso easily transferred across diverse image datasets and obtain superior\naccuracy on surface alignment. Conclusion and Significance: We propose MS-Net\nas a powerful and flexible tool to simplify brain MR images and their\nregistration. To our knowledge, this is the first work to simplify brain MR\nimage registration by deep learning, instead of estimating deformation field\ndirectly.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 18:58:58 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Wei", "Dongming", ""], ["Wu", "Zhengwang", ""], ["Li", "Gang", ""], ["Cao", "Xiaohuan", ""], ["Shen", "Dinggang", ""], ["Wang", "Qian", ""]]}, {"id": "1902.02394", "submitter": "Dengxin Dai", "authors": "Ankit Dhall, Dengxin Dai and Luc Van Gool", "title": "Real-time 3D Traffic Cone Detection for Autonomous Driving", "comments": "IEEE Intelligent Vehicles Symposium (IV'19). arXiv admin note: text\n  overlap with arXiv:1809.10548", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considerable progress has been made in semantic scene understanding of road\nscenes with monocular cameras. It is, however, mainly related to certain\nclasses such as cars and pedestrians. This work investigates traffic cones, an\nobject class crucial for traffic control in the context of autonomous vehicles.\n3D object detection using images from a monocular camera is intrinsically an\nill-posed problem. In this work, we leverage the unique structure of traffic\ncones and propose a pipelined approach to the problem. Specifically, we first\ndetect cones in images by a tailored 2D object detector; then, the spatial\narrangement of keypoints on a traffic cone are detected by our deep structural\nregression network, where the fact that the cross-ratio is projection invariant\nis leveraged for network regularization; finally, the 3D position of cones is\nrecovered by the classical Perspective n-Point algorithm. Extensive experiments\nshow that our approach can accurately detect traffic cones and estimate their\nposition in the 3D world in real time. The proposed method is also deployed on\na real-time, critical system. It runs efficiently on the low-power Jetson TX2,\nproviding accurate 3D position estimates, allowing a race-car to map and drive\nautonomously on an unseen track indicated by traffic cones. With the help of\nrobust and accurate perception, our race-car won both Formula Student\nCompetitions held in Italy and Germany in 2018, cruising at a top-speed of 54\nkmph. Visualization of the complete pipeline, mapping and navigation can be\nfound on our project page.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 20:45:30 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 21:17:37 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Dhall", "Ankit", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "1902.02449", "submitter": "Se Young Chun", "authors": "Byung Hyun Lee and Se Young Chun", "title": "Empirically Accelerating Scaled Gradient Projection Using Deep Neural\n  Network For Inverse Problems In Image Processing", "comments": "10 pages, 6 figures, 3 tables, ICASSP 2021, this is a long version of\n  it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep neural networks (DNNs) have shown advantages in accelerating\noptimization algorithms. One approach is to unfold finite number of iterations\nof conventional optimization algorithms and to learn parameters in the\nalgorithms. However, these are forward methods and are indeed neither iterative\nnor convergent. Here, we present a novel DNN-based convergent iterative\nalgorithm that accelerates conventional optimization algorithms. We train a DNN\nto yield parameters in scaled gradient projection method. So far, these\nparameters have been chosen heuristically, but have shown to be crucial for\ngood empirical performance. In simulation results, the proposed method\nsignificantly improves the empirical convergence rate over conventional\noptimization methods for various large-scale inverse problems in image\nprocessing.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 02:19:53 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 05:18:41 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 03:00:39 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Lee", "Byung Hyun", ""], ["Chun", "Se Young", ""]]}, {"id": "1902.02452", "submitter": "Se Young Chun", "authors": "Magauiya Zhussip, Shakarim Soltanayev, Se Young Chun", "title": "Extending Stein's unbiased risk estimator to train deep denoisers with\n  correlated pairs of noisy images", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Stein's unbiased risk estimator (SURE) has been applied to\nunsupervised training of deep neural network Gaussian denoisers that\noutperformed classical non-deep learning based denoisers and yielded comparable\nperformance to those trained with ground truth. While SURE requires only one\nnoise realization per image for training, it does not take advantage of having\nmultiple noise realizations per image when they are available (e.g., two\nuncorrelated noise realizations per image for Noise2Noise). Here, we propose an\nextended SURE (eSURE) to train deep denoisers with correlated pairs of noise\nrealizations per image and applied it to the case with two uncorrelated\nrealizations per image to achieve better performance than SURE based method and\ncomparable results to Noise2Noise. Then, we further investigated the case with\nimperfect ground truth (i.e., mild noise in ground truth) that may be obtained\nconsidering painstaking, time-consuming, and even expensive processes of\ncollecting ground truth images with multiple noisy images. For the case of\ngenerating noisy training data by adding synthetic noise to imperfect ground\ntruth to yield correlated pairs of images, our proposed eSURE based training\nmethod outperformed conventional SURE based method as well as Noise2Noise.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 02:44:55 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 11:33:32 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Zhussip", "Magauiya", ""], ["Soltanayev", "Shakarim", ""], ["Chun", "Se Young", ""]]}, {"id": "1902.02476", "submitter": "Andrew Wilson", "authors": "Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, Andrew\n  Gordon Wilson", "title": "A Simple Baseline for Bayesian Uncertainty in Deep Learning", "comments": "Published at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose\napproach for uncertainty representation and calibration in deep learning.\nStochastic Weight Averaging (SWA), which computes the first moment of\nstochastic gradient descent (SGD) iterates with a modified learning rate\nschedule, has recently been shown to improve generalization in deep learning.\nWith SWAG, we fit a Gaussian using the SWA solution as the first moment and a\nlow rank plus diagonal covariance also derived from the SGD iterates, forming\nan approximate posterior distribution over neural network weights; we then\nsample from this Gaussian distribution to perform Bayesian model averaging. We\nempirically find that SWAG approximates the shape of the true posterior, in\naccordance with results describing the stationary distribution of SGD iterates.\nMoreover, we demonstrate that SWAG performs well on a wide variety of tasks,\nincluding out of sample detection, calibration, and transfer learning, in\ncomparison to many popular alternatives including MC dropout, KFAC Laplace,\nSGLD, and temperature scaling.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 05:15:46 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 08:28:19 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Maddox", "Wesley", ""], ["Garipov", "Timur", ""], ["Izmailov", "Pavel", ""], ["Vetrov", "Dmitry", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1902.02497", "submitter": "Xinrui Cui", "authors": "Xinrui Cui, Dan Wang, and Z. Jane Wang", "title": "CHIP: Channel-wise Disentangled Interpretation of Deep Convolutional\n  Neural Networks", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the widespread applications of deep convolutional neural networks\n(DCNNs), it becomes increasingly important for DCNNs not only to make accurate\npredictions but also to explain how they make their decisions. In this work, we\npropose a CHannel-wise disentangled InterPretation (CHIP) model to give the\nvisual interpretation to the predictions of DCNNs. The proposed model distills\nthe class-discriminative importance of channels in networks by utilizing the\nsparse regularization. Here, we first introduce the network perturbation\ntechnique to learn the model. The proposed model is capable to not only distill\nthe global perspective knowledge from networks but also present the\nclass-discriminative visual interpretation for specific predictions of\nnetworks. It is noteworthy that the proposed model is able to interpret\ndifferent layers of networks without re-training. By combining the distilled\ninterpretation knowledge in different layers, we further propose the Refined\nCHIP visual interpretation that is both high-resolution and\nclass-discriminative. Experimental results on the standard dataset demonstrate\nthat the proposed model provides promising visual interpretation for the\npredictions of networks in image classification task compared with existing\nvisual interpretation methods. Besides, the proposed method outperforms related\napproaches in the application of ILSVRC 2015 weakly-supervised localization\ntask.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 07:17:03 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 19:01:48 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Cui", "Xinrui", ""], ["Wang", "Dan", ""], ["Wang", "Z. Jane", ""]]}, {"id": "1902.02513", "submitter": "Massimiliano Gargiulo", "authors": "Massimiliano Gargiulo", "title": "Advances on CNN-based super-resolution of Sentinel-2 images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to their temporal-spatial coverage and free access, Sentinel-2 images\nare very interesting for the community. However, a relatively coarse spatial\nresolution, compared to that of state-of-the-art commercial products, motivates\nthe study of super-resolution techniques to mitigate such a limitation.\nSpecifically, thirtheen bands are sensed simultaneously but at different\nspatial resolutions: 10, 20, and 60 meters depending on the spectral location.\nHere, building upon our previous convolutional neural network (CNN) based\nmethod, we propose an improved CNN solution to super-resolve the 20-m\nresolution bands benefiting spatial details conveyed by the accompanying 10-m\nspectral bands.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 08:09:46 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Gargiulo", "Massimiliano", ""]]}, {"id": "1902.02530", "submitter": "Sunghwan Joo", "authors": "Sunghwan Joo, Sungmin Cha, and Taesup Moon", "title": "DoPAMINE: Double-sided Masked CNN for Pixel Adaptive Multiplicative\n  Noise Despeckling", "comments": "AAAI 2019 Camera Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DoPAMINE, a new neural network based multiplicative noise\ndespeckling algorithm. Our algorithm is inspired by Neural AIDE (N-AIDE), which\nis a recently proposed neural adaptive image denoiser. While the original\nN-AIDE was designed for the additive noise case, we show that the same\nframework, i.e., adaptively learning a network for pixel-wise affine denoisers\nby minimizing an unbiased estimate of MSE, can be applied to the multiplicative\nnoise case as well. Moreover, we derive a double-sided masked CNN architecture\nwhich can control the variance of the activation values in each layer and\nconverge fast to high denoising performance during supervised training. In the\nexperimental results, we show our DoPAMINE possesses high adaptivity via\nfine-tuning the network parameters based on the given noisy image and achieves\nsignificantly better despeckling results compared to SAR-DRN, a\nstate-of-the-art CNN-based algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 09:08:18 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Joo", "Sunghwan", ""], ["Cha", "Sungmin", ""], ["Moon", "Taesup", ""]]}, {"id": "1902.02544", "submitter": "Shlomo Bugdary", "authors": "Shlomo Bugdary, Shay Maymon", "title": "Online Clustering by Penalized Weighted GMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the dawn of the Big Data era, data sets are growing rapidly. Data is\nstreaming from everywhere - from cameras, mobile phones, cars, and other\nelectronic devices. Clustering streaming data is a very challenging problem.\nUnlike the traditional clustering algorithms where the dataset can be stored\nand scanned multiple times, clustering streaming data has to satisfy\nconstraints such as limit memory size, real-time response, unknown data\nstatistics and an unknown number of clusters. In this paper, we present a novel\nonline clustering algorithm which can be used to cluster streaming data without\nknowing the number of clusters a priori. Results on both synthetic and real\ndatasets show that the proposed algorithm produces partitions which are close\nto what you could get if you clustered the whole data at one time.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 09:50:08 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Bugdary", "Shlomo", ""], ["Maymon", "Shay", ""]]}, {"id": "1902.02586", "submitter": "Ahmed Taha", "authors": "Ahmed Taha, Yi-Ting Chen, Teruhisa Misu, Abhinav Shrivastava, Larry\n  Davis", "title": "Unsupervised Data Uncertainty Learning in Visual Retrieval Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an unsupervised formulation to estimate heteroscedastic\nuncertainty in retrieval systems. We propose an extension to triplet loss that\nmodels data uncertainty for each input. Besides improving performance, our\nformulation models local noise in the embedding space. It quantifies input\nuncertainty and thus enhances interpretability of the system. This helps\nidentify noisy observations in query and search databases. Evaluation on both\nimage and video retrieval applications highlight the utility of our approach.\nWe highlight our efficiency in modeling local noise using two real-world\ndatasets: Clothing1M and Honda Driving datasets. Qualitative results illustrate\nour ability in identifying confusing scenarios in various domains. Uncertainty\nlearning also enables data cleaning by detecting noisy training labels.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 12:31:50 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Taha", "Ahmed", ""], ["Chen", "Yi-Ting", ""], ["Misu", "Teruhisa", ""], ["Shrivastava", "Abhinav", ""], ["Davis", "Larry", ""]]}, {"id": "1902.02593", "submitter": "Dean Zadok", "authors": "Nir Diamant, Dean Zadok, Chaim Baskin, Eli Schwartz, Alex M. Bronstein", "title": "Beholder-GAN: Generation and Beautification of Facial Images with\n  Conditioning on Their Beauty Level", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beauty is in the eye of the beholder. This maxim, emphasizing the\nsubjectivity of the perception of beauty, has enjoyed a wide consensus since\nancient times. In the digitalera, data-driven methods have been shown to be\nable to predict human-assigned beauty scores for facial images. In this work,\nwe augment this ability and train a generative model that generates faces\nconditioned on a requested beauty score. In addition, we show how this trained\ngenerator can be used to beautify an input face image. By doing so, we achieve\nan unsupervised beautification model, in the sense that it relies on no ground\ntruth target images.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 12:44:49 GMT"}, {"version": "v2", "created": "Sun, 10 Feb 2019 13:08:42 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 20:33:47 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Diamant", "Nir", ""], ["Zadok", "Dean", ""], ["Baskin", "Chaim", ""], ["Schwartz", "Eli", ""], ["Bronstein", "Alex M.", ""]]}, {"id": "1902.02597", "submitter": "Adrien Lagrange", "authors": "Adrien Lagrange, Mathieu Fauvel, St\\'ephane May, Jos\\'e Bioucas-Dias\n  and Nicolas Dobigeon", "title": "Matrix Cofactorization for Joint Representation Learning and Supervised\n  Classification -- Application to Hyperspectral Image Analysis", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2019.12.068", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised classification and representation learning are two widely used\nclasses of methods to analyze multivariate images. Although complementary,\nthese methods have been scarcely considered jointly in a hierarchical modeling.\nIn this paper, a method coupling these two approaches is designed using a\nmatrix cofactorization formulation. Each task is modeled as a factorization\nmatrix problem and a term relating both coding matrices is then introduced to\ndrive an appropriate coupling. The link can be interpreted as a clustering\noperation over a low-dimensional representation vectors. The attribution\nvectors of the clustering are then used as features vectors for the\nclassification task, i.e., the coding vectors of the corresponding\nfactorization problem. A proximal gradient descent algorithm, ensuring\nconvergence to a critical point of the objective function, is then derived to\nsolve the resulting non-convex non-smooth optimization problem. An evaluation\nof the proposed method is finally conducted both on synthetic and real data in\nthe specific context of hyperspectral image interpretation, unifying two\nstandard analysis techniques, namely unmixing and classification.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 12:54:56 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 09:28:56 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2020 18:40:04 GMT"}, {"version": "v4", "created": "Thu, 13 Feb 2020 18:53:51 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Lagrange", "Adrien", ""], ["Fauvel", "Mathieu", ""], ["May", "St\u00e9phane", ""], ["Bioucas-Dias", "Jos\u00e9", ""], ["Dobigeon", "Nicolas", ""]]}, {"id": "1902.02678", "submitter": "Daan de Geus", "authors": "Daan de Geus, Panagiotis Meletis, Gijs Dubbelman", "title": "Single Network Panoptic Segmentation for Street Scene Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a single deep neural network for panoptic\nsegmentation, for which the goal is to provide each individual pixel of an\ninput image with a class label, as in semantic segmentation, as well as a\nunique identifier for specific objects in an image, following instance\nsegmentation. Our network makes joint semantic and instance segmentation\npredictions and combines these to form an output in the panoptic format. This\nhas two main benefits: firstly, the entire panoptic prediction is made in one\npass, reducing the required computation time and resources; secondly, by\nlearning the tasks jointly, information is shared between the two tasks,\nthereby improving performance. Our network is evaluated on two street scene\ndatasets: Cityscapes and Mapillary Vistas. By leveraging information exchange\nand improving the merging heuristics, we increase the performance of the single\nnetwork, and achieve a score of 23.9 on the Panoptic Quality (PQ) metric on\nMapillary Vistas validation, with an input resolution of 640 x 900 pixels. On\nCityscapes validation, our method achieves a PQ score of 45.9 with an input\nresolution of 512 x 1024 pixels. Moreover, our method decreases the prediction\ntime by a factor of 2 with respect to separate networks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 15:18:57 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["de Geus", "Daan", ""], ["Meletis", "Panagiotis", ""], ["Dubbelman", "Gijs", ""]]}, {"id": "1902.02693", "submitter": "Alessandro Corbetta", "authors": "Joost Visser, Alessandro Corbetta, Vlado Menkovski, Federico Toschi", "title": "StampNet: unsupervised multi-class object discovery", "comments": null, "journal-ref": "IEEE International Conference on Image Processing (ICIP 2019), pp.\n  2951-2955, 2019", "doi": "10.1109/ICIP.2019.8803767", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised object discovery in images involves uncovering recurring\npatterns that define objects and discriminates them against the background.\nThis is more challenging than image clustering as the size and the location of\nthe objects are not known: this adds additional degrees of freedom and\nincreases the problem complexity. In this work, we propose StampNet, a novel\nautoencoding neural network that localizes shapes (objects) over a simple\nbackground in images and categorizes them simultaneously. StampNet consists of\na discrete latent space that is used to categorize objects and to determine the\nlocation of the objects. The object categories are formed during the training,\nresulting in the discovery of a fixed set of objects. We present a set of\nexperiments that demonstrate that StampNet is able to localize and cluster\nmultiple overlapping shapes with varying complexity including the digits from\nthe MNIST dataset. We also present an application of StampNet in the\nlocalization of pedestrians in overhead depth-maps.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 15:35:55 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Visser", "Joost", ""], ["Corbetta", "Alessandro", ""], ["Menkovski", "Vlado", ""], ["Toschi", "Federico", ""]]}, {"id": "1902.02711", "submitter": "Vianney Loing", "authors": "Vianney Loing, Renaud Marlet, Mathieu Aubry", "title": "Virtual Training for a Real Application: Accurate Object-Robot Relative\n  Localization without Calibration", "comments": null, "journal-ref": "Int J Comput Vis (2018) 126: 1045", "doi": "10.1007/s11263-018-1102-6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localizing an object accurately with respect to a robot is a key step for\nautonomous robotic manipulation. In this work, we propose to tackle this task\nknowing only 3D models of the robot and object in the particular case where the\nscene is viewed from uncalibrated cameras -- a situation which would be typical\nin an uncontrolled environment, e.g., on a construction site. We demonstrate\nthat this localization can be performed very accurately, with millimetric\nerrors, without using a single real image for training, a strong advantage\nsince acquiring representative training data is a long and expensive process.\nOur approach relies on a classification Convolutional Neural Network (CNN)\ntrained using hundreds of thousands of synthetically rendered scenes with\nrandomized parameters. To evaluate our approach quantitatively and make it\ncomparable to alternative approaches, we build a new rich dataset of real robot\nimages with accurately localized blocks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 16:17:01 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Loing", "Vianney", ""], ["Marlet", "Renaud", ""], ["Aubry", "Mathieu", ""]]}, {"id": "1902.02729", "submitter": "Tycho Van Der Ouderaa", "authors": "Tycho F.A. van der Ouderaa, Daniel E. Worrall", "title": "Reversible GANs for Memory-efficient Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Pix2pix and CycleGAN losses have vastly improved the qualitative and\nquantitative visual quality of results in image-to-image translation tasks. We\nextend this framework by exploring approximately invertible architectures which\nare well suited to these losses. These architectures are approximately\ninvertible by design and thus partially satisfy cycle-consistency before\ntraining even begins. Furthermore, since invertible architectures have constant\nmemory complexity in depth, these models can be built arbitrarily deep. We are\nable to demonstrate superior quantitative output on the Cityscapes and Maps\ndatasets at near constant memory budget.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 17:03:05 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["van der Ouderaa", "Tycho F. A.", ""], ["Worrall", "Daniel E.", ""]]}, {"id": "1902.02752", "submitter": "Alexandre Kaspar", "authors": "Alexandre Kaspar, Tae-Hyun Oh, Liane Makatura, Petr Kellnhofer,\n  Jacqueline Aslarus and Wojciech Matusik", "title": "Neural Inverse Knitting: From Images to Manufacturing Instructions", "comments": "Project page: http://deepknitting.csail.mit.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the recent potential of mass customization brought by\nwhole-garment knitting machines, we introduce the new problem of automatic\nmachine instruction generation using a single image of the desired physical\nproduct, which we apply to machine knitting. We propose to tackle this problem\nby directly learning to synthesize regular machine instructions from real\nimages. We create a cured dataset of real samples with their instruction\ncounterpart and propose to use synthetic images to augment it in a novel way.\nWe theoretically motivate our data mixing framework and show empirical results\nsuggesting that making real images look more synthetic is beneficial in our\nproblem setup.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 18:05:17 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 19:32:16 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Kaspar", "Alexandre", ""], ["Oh", "Tae-Hyun", ""], ["Makatura", "Liane", ""], ["Kellnhofer", "Petr", ""], ["Aslarus", "Jacqueline", ""], ["Matusik", "Wojciech", ""]]}, {"id": "1902.02771", "submitter": "S.H. Shabbeer Basha", "authors": "S.H. Shabbeer Basha, Shiv Ram Dubey, Viswanath Pulabaigari, Snehasis\n  Mukherjee", "title": "Impact of Fully Connected Layers on Performance of Convolutional Neural\n  Networks for Image Classification", "comments": "This paper is accepted for publication in Neurocomputing Journal", "journal-ref": null, "doi": "10.1016/j.neucom.2019.10.008", "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Convolutional Neural Networks (CNNs), in domains like computer vision,\nmostly reduced the need for handcrafted features due to its ability to learn\nthe problem-specific features from the raw input data. However, the selection\nof dataset-specific CNN architecture, which mostly performed by either\nexperience or expertise is a time-consuming and error-prone process. To\nautomate the process of learning a CNN architecture, this paper attempts at\nfinding the relationship between Fully Connected (FC) layers with some of the\ncharacteristics of the datasets. The CNN architectures, and recently datasets\nalso, are categorized as deep, shallow, wide, etc. This paper tries to\nformalize these terms along with answering the following questions. (i) What is\nthe impact of deeper/shallow architectures on the performance of the CNN w.r.t.\nFC layers?, (ii) How the deeper/wider datasets influence the performance of CNN\nw.r.t. FC layers?, and (iii) Which kind of architecture (deeper/ shallower) is\nbetter suitable for which kind of (deeper/ wider) datasets. To address these\nfindings, we have performed experiments with three CNN architectures having\ndifferent depths. The experiments are conducted by varying the number of FC\nlayers. We used four widely used datasets including CIFAR-10, CIFAR-100, Tiny\nImageNet, and CRCHistoPhenotypes to justify our findings in the context of the\nimage classification problem. The source code of this research is available at\nhttps://github.com/shabbeersh/Impact-of-FC-layers.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 07:42:26 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 04:35:05 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 05:28:01 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Basha", "S. H. Shabbeer", ""], ["Dubey", "Shiv Ram", ""], ["Pulabaigari", "Viswanath", ""], ["Mukherjee", "Snehasis", ""]]}, {"id": "1902.02777", "submitter": "Ivan Bajic", "authors": "Jianglin Fu, Saeed Ranjbar Alvar, Ivan V. Bajic, and Rodney G. Vaughan", "title": "FDDB-360: Face Detection in 360-degree Fisheye Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  360-degree cameras offer the possibility to cover a large area, for example\nan entire room, without using multiple distributed vision sensors. However,\ngeometric distortions introduced by their lenses make computer vision problems\nmore challenging. In this paper we address face detection in 360-degree fisheye\nimages. We show how a face detector trained on regular images can be re-trained\nfor this purpose, and we also provide a 360-degree fisheye-like version of the\npopular FDDB face detection dataset, which we call FDDB-360.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 18:59:10 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Fu", "Jianglin", ""], ["Alvar", "Saeed Ranjbar", ""], ["Bajic", "Ivan V.", ""], ["Vaughan", "Rodney G.", ""]]}, {"id": "1902.02804", "submitter": "Yuhong Li", "authors": "Yuhong Li, Xiaofan Zhang", "title": "SiamVGG: Visual Tracking using Deeper Siamese Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, we have seen a rapid development of Deep Neural Network (DNN) based\nvisual tracking solutions. Some trackers combine the DNN-based solutions with\nDiscriminative Correlation Filters (DCF) to extract semantic features and\nsuccessfully deliver the state-of-the-art tracking accuracy. However, these\nsolutions are highly compute-intensive, which require long processing time,\nresulting unsecured real-time performance. To deliver both high accuracy and\nreliable real-time performance, we propose a novel tracker called SiamVGG. It\ncombines a Convolutional Neural Network (CNN) backbone and a cross-correlation\noperator, and takes advantage of the features from exemplary images for more\naccurate object tracking.\n  The architecture of SiamVGG is customized from VGG-16, with the parameters\nshared by both exemplary images and desired input video frames.\n  We demonstrate the proposed SiamVGG on OTB-2013/50/100 and VOT 2015/2016/2017\ndatasets with the state-of-the-art accuracy while maintaining a decent\nreal-time performance of 50 FPS running on a GTX 1080Ti. Our design can achieve\n2% higher Expected Average Overlap (EAO) compared to the ECO and C-COT in\nVOT2017 Challenge.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 19:08:34 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 22:00:10 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Li", "Yuhong", ""], ["Zhang", "Xiaofan", ""]]}, {"id": "1902.02826", "submitter": "Abinaya Manimaran", "authors": "Thiyagarajan Ramanathan, Abinaya Manimaran, Suya You, C-C Jay Kuo", "title": "Robustness Of Saak Transform Against Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification is vulnerable to adversarial attacks. This work\ninvestigates the robustness of Saak transform against adversarial attacks\ntowards high performance image classification. We develop a complete image\nclassification system based on multi-stage Saak transform. In the Saak\ntransform domain, clean and adversarial images demonstrate different\ndistributions at different spectral dimensions. Selection of the spectral\ndimensions at every stage can be viewed as an automatic denoising process.\nMotivated by this observation, we carefully design strategies of feature\nextraction, representation and classification that increase adversarial\nrobustness. The performances with well-known datasets and attacks are\ndemonstrated by extensive experimental evaluations.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 20:11:03 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Ramanathan", "Thiyagarajan", ""], ["Manimaran", "Abinaya", ""], ["You", "Suya", ""], ["Kuo", "C-C Jay", ""]]}, {"id": "1902.02831", "submitter": "Emanuel Aldea", "authors": "Jennifer Vandoni, Emanuel Aldea and Sylvie Le H\\'egarat-Mascle", "title": "Evaluating Crowd Density Estimators via Their Uncertainty Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we use the Belief Function Theory which extends the\nprobabilistic framework in order to provide uncertainty bounds to different\ncategories of crowd density estimators. Our method allows us to compare the\nmulti-scale performance of the estimators, and also to characterize their\nreliability for crowd monitoring applications requiring varying degrees of\nprudence.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 20:22:15 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Vandoni", "Jennifer", ""], ["Aldea", "Emanuel", ""], ["H\u00e9garat-Mascle", "Sylvie Le", ""]]}, {"id": "1902.02841", "submitter": "Steven Schwarcz", "authors": "Steven Schwarcz, Thomas Pollard", "title": "3D Human Pose Estimation from Deep Multi-View 2D Pose", "comments": null, "journal-ref": "2018 24th International Conference on Pattern Recognition (ICPR),\n  Beijing, 2018, pp. 2326-2331", "doi": "10.1109/ICPR.2018.8545631", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human pose estimation - the process of recognizing a human's limb positions\nand orientations in a video - has many important applications including\nsurveillance, diagnosis of movement disorders, and computer animation. While\ndeep learning has lead to great advances in 2D and 3D pose estimation from\nsingle video sources, the problem of estimating 3D human pose from multiple\nvideo sensors with overlapping fields of view has received less attention. When\nthe application allows use of multiple cameras, 3D human pose estimates may be\ngreatly improved through fusion of multi-view pose estimates and observation of\nlimbs that are fully or partially occluded in some views. Past approaches to\nmulti-view 3D pose estimation have used probabilistic graphical models to\nreason over constraints, including per-image pose estimates, temporal\nsmoothness, and limb length. In this paper, we present a pipeline for\nmulti-view 3D pose estimation of multiple individuals which combines a\nstate-of-art 2D pose detector with a factor graph of 3D limb constraints\noptimized with belief propagation. We evaluate our results on the TUM-Campus\nand Shelf datasets for multi-person 3D pose estimation and show that our system\nsignificantly out-performs the previous state-of-the-art with a simpler model\nof limb dependency.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 20:55:19 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Schwarcz", "Steven", ""], ["Pollard", "Thomas", ""]]}, {"id": "1902.02845", "submitter": "Tiago Carvalho Dr.", "authors": "Rodrigo Bresan, Allan Pinto, Anderson Rocha, Carlos Beluzo, Tiago\n  Carvalho", "title": "FaceSpoof Buster: a Presentation Attack Detector Based on Intrinsic\n  Image Properties and Deep Learning", "comments": "7 pages, 1 figure, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the adoption of face recognition for biometric authentication\nsystems is usual, mainly because this is one of the most accessible biometric\nmodalities. Techniques that rely on trespassing these kind of systems by using\na forged biometric sample, such as a printed paper or a recorded video of a\ngenuine access, are known as presentation attacks, but may be also referred in\nthe literature as face spoofing. Presentation attack detection is a crucial\nstep for preventing this kind of unauthorized accesses into restricted areas\nand/or devices. In this paper, we propose a novel approach which relies in a\ncombination between intrinsic image properties and deep neural networks to\ndetect presentation attack attempts. Our method explores depth, salience and\nillumination maps, associated with a pre-trained Convolutional Neural Network\nin order to produce robust and discriminant features. Each one of these\nproperties are individually classified and, in the end of the process, they are\ncombined by a meta learning classifier, which achieves outstanding results on\nthe most popular datasets for PAD. Results show that proposed method is able to\noverpass state-of-the-art results in an inter-dataset protocol, which is\ndefined as the most challenging in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 21:11:07 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Bresan", "Rodrigo", ""], ["Pinto", "Allan", ""], ["Rocha", "Anderson", ""], ["Beluzo", "Carlos", ""], ["Carvalho", "Tiago", ""]]}, {"id": "1902.02870", "submitter": "Valsamis Ntouskos", "authors": "Lorenzo Mauro, Francesco Puja, Simone Grazioso, Valsamis Ntouskos,\n  Marta Sanzari, Edoardo Alati, Fiora Pirri", "title": "Visual search and recognition for robot task execution and monitoring", "comments": null, "journal-ref": "Frontiers in Artificial Intelligence and Applications 310 (2018)\n  94-109", "doi": "10.3233/978-1-61499-929-4-94", "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual search of relevant targets in the environment is a crucial robot\nskill. We propose a preliminary framework for the execution monitor of a robot\ntask, taking care of the robot attitude to visually searching the environment\nfor targets involved in the task. Visual search is also relevant to recover\nfrom a failure. The framework exploits deep reinforcement learning to acquire a\n\"common sense\" scene structure and it takes advantage of a deep convolutional\nnetwork to detect objects and relevant relations holding between them. The\nframework builds on these methods to introduce a vision-based execution\nmonitoring, which uses classical planning as a backbone for task execution.\nExperiments show that with the proposed vision-based execution monitor the\nrobot can complete simple tasks and can recover from failures in autonomy.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 22:35:51 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Mauro", "Lorenzo", ""], ["Puja", "Francesco", ""], ["Grazioso", "Simone", ""], ["Ntouskos", "Valsamis", ""], ["Sanzari", "Marta", ""], ["Alati", "Edoardo", ""], ["Pirri", "Fiora", ""]]}, {"id": "1902.02877", "submitter": "Valsamis Ntouskos", "authors": "Lorenzo Mauro, Edoardo Alati, Marta Sanzari, Valsamis Ntouskos,\n  Gianluca Massimiani, Fiora Pirri", "title": "Deep execution monitor for robot assistive tasks", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-11024-6_11", "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a novel approach to high-level robot task execution for a robot\nassistive task. In this work we explore the problem of learning to predict the\nnext subtask by introducing a deep model for both sequencing goals and for\nvisually evaluating the state of a task. We show that deep learning for\nmonitoring robot tasks execution very well supports the interconnection between\ntask-level planning and robot operations. These solutions can also cope with\nthe natural non-determinism of the execution monitor. We show that a deep\nexecution monitor leverages robot performance. We measure the improvement\ntaking into account some robot helping tasks performed at a warehouse.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 23:02:47 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Mauro", "Lorenzo", ""], ["Alati", "Edoardo", ""], ["Sanzari", "Marta", ""], ["Ntouskos", "Valsamis", ""], ["Massimiani", "Gianluca", ""], ["Pirri", "Fiora", ""]]}, {"id": "1902.02882", "submitter": "Pingfan Song", "authors": "Pingfan Song, Yonina C. Eldar, Gal Mazor, Miguel Rodrigues", "title": "HYDRA: Hybrid Deep Magnetic Resonance Fingerprinting", "comments": null, "journal-ref": null, "doi": "10.1002/mp.13727", "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Magnetic resonance fingerprinting (MRF) methods typically rely on\ndictio-nary matching to map the temporal MRF signals to quantitative tissue\nparameters. Such approaches suffer from inherent discretization errors, as well\nas high computational complexity as the dictionary size grows. To alleviate\nthese issues, we propose a HYbrid Deep magnetic ResonAnce fingerprinting\napproach, referred to as HYDRA.\n  Methods: HYDRA involves two stages: a model-based signature restoration phase\nand a learning-based parameter restoration phase. Signal restoration is\nimplemented using low-rank based de-aliasing techniques while parameter\nrestoration is performed using a deep nonlocal residual convolutional neural\nnetwork. The designed network is trained on synthesized MRF data simulated with\nthe Bloch equations and fast imaging with steady state precession (FISP)\nsequences. In test mode, it takes a temporal MRF signal as input and produces\nthe corresponding tissue parameters.\n  Results: We validated our approach on both synthetic data and anatomical data\ngenerated from a healthy subject. The results demonstrate that, in contrast to\nconventional dictionary-matching based MRF techniques, our approach\nsignificantly improves inference speed by eliminating the time-consuming\ndictionary matching operation, and alleviates discretization errors by\noutputting continuous-valued parameters. We further avoid the need to store a\nlarge dictionary, thus reducing memory requirements.\n  Conclusions: Our approach demonstrates advantages in terms of inference\nspeed, accuracy and storage requirements over competing MRF methods\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 23:15:24 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 15:44:04 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Song", "Pingfan", ""], ["Eldar", "Yonina C.", ""], ["Mazor", "Gal", ""], ["Rodrigues", "Miguel", ""]]}, {"id": "1902.02905", "submitter": "Stephen Odaibo", "authors": "Stephen G. Odaibo, Mikelson MomPremier, Richard Y. Hwang, Salman J.\n  Yousuf, Steven L. Williams, Joshua Grant", "title": "Mobile Artificial Intelligence Technology for Detecting Macula Edema and\n  Subretinal Fluid on OCT Scans: Initial Results from the DATUM alpha Study", "comments": "Initial results of the DATUM alpha Study were initially presented on\n  August 13th 2018 in the Keynote Address at the 116th National Medical\n  Association Annual Meeting & Scientific Assembly's New Innovations in\n  Ophthalmology Session. The results were also presented on September 21st 2018\n  in a Podium Lecture during Alumni Day at the University of Michigan--Ann\n  Arbor Kellogg Eye Center", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.AI cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) is necessary to address the large and growing\ndeficit in retina and healthcare access globally. And mobile AI diagnostic\nplatforms running in the Cloud may effectively and efficiently distribute such\nAI capability. Here we sought to evaluate the feasibility of Cloud-based mobile\nartificial intelligence for detection of retinal disease. And to evaluate the\naccuracy of a particular such system for detection of subretinal fluid (SRF)\nand macula edema (ME) on OCT scans. A multicenter retrospective image analysis\nwas conducted in which board-certified ophthalmologists with fellowship\ntraining in retina evaluated OCT images of the macula. They noted the presence\nor absence of ME or SRF, then compared their assessment to that obtained from\nFluid Intelligence, a mobile AI app that detects SRF and ME on OCT scans.\nInvestigators consecutively selected retinal OCTs, while making effort to\nbalance the number of scans with retinal fluid and scans without. Exclusion\ncriteria included poor scan quality, ambiguous features, macula holes,\nretinoschisis, and dense epiretinal membranes. Accuracy in the form of\nsensitivity and specificity of the AI mobile App was determined by comparing\nits assessments to those of the retina specialists. At the time of this\nsubmission, five centers have completed their initial studies. This consists of\na total of 283 OCT scans of which 155 had either ME or SRF (\"wet\") and 128 did\nnot (\"dry\"). The sensitivity ranged from 82.5% to 97% with a weighted average\nof 89.3%. The specificity ranged from 52% to 100% with a weighted average of\n81.23%. CONCLUSION: Cloud-based Mobile AI technology is feasible for the\ndetection retinal disease. In particular, Fluid Intelligence (alpha version),\nis sufficiently accurate as a screening tool for SRF and ME, especially in\nunderserved areas. Further studies and technology development is needed.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 01:15:23 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 23:50:23 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Odaibo", "Stephen G.", ""], ["MomPremier", "Mikelson", ""], ["Hwang", "Richard Y.", ""], ["Yousuf", "Salman J.", ""], ["Williams", "Steven L.", ""], ["Grant", "Joshua", ""]]}, {"id": "1902.02910", "submitter": "Ting-Wu Chin", "authors": "Ting-Wu Chin, Ruizhou Ding, Diana Marculescu", "title": "AdaScale: Towards Real-time Video Object Detection Using Adaptive\n  Scaling", "comments": "Accepted to SysML 2019 (http://www.sysml.cc/) as oral contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In vision-enabled autonomous systems such as robots and autonomous cars,\nvideo object detection plays a crucial role, and both its speed and accuracy\nare important factors to provide reliable operation. The key insight we show in\nthis paper is that speed and accuracy are not necessarily a trade-off when it\ncomes to image scaling. Our results show that re-scaling the image to a lower\nresolution will sometimes produce better accuracy. Based on this observation,\nwe propose a novel approach, dubbed AdaScale, which adaptively selects the\ninput image scale that improves both accuracy and speed for video object\ndetection. To this end, our results on ImageNet VID and mini\nYouTube-BoundingBoxes datasets demonstrate 1.3 points and 2.7 points mAP\nimprovement with 1.6x and 1.8x speedup, respectively. Additionally, we improve\nstate-of-the-art video acceleration work by an extra 1.25x speedup with\nslightly better mAP on ImageNet VID dataset.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 01:31:02 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Chin", "Ting-Wu", ""], ["Ding", "Ruizhou", ""], ["Marculescu", "Diana", ""]]}, {"id": "1902.02919", "submitter": "Maneet Singh", "authors": "Maneet Singh, Richa Singh, Arun Ross", "title": "A Comprehensive Overview of Biometric Fusion", "comments": "Accepted for publication in Information Fusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of a biometric system that relies on a single biometric\nmodality (e.g., fingerprints only) is often stymied by various factors such as\npoor data quality or limited scalability. Multibiometric systems utilize the\nprinciple of fusion to combine information from multiple sources in order to\nimprove recognition accuracy whilst addressing some of the limitations of\nsingle-biometric systems. The past two decades have witnessed the development\nof a large number of biometric fusion schemes. This paper presents an overview\nof biometric fusion with specific focus on three questions: what to fuse, when\nto fuse, and how to fuse. A comprehensive review of techniques incorporating\nancillary information in the biometric recognition pipeline is also presented.\nIn this regard, the following topics are discussed: (i) incorporating data\nquality in the biometric recognition pipeline; (ii) combining soft biometric\nattributes with primary biometric identifiers; (iii) utilizing contextual\ninformation to improve biometric recognition accuracy; and (iv) performing\ncontinuous authentication using ancillary information. In addition, the use of\ninformation fusion principles for presentation attack detection and\nmultibiometric cryptosystems is also discussed. Finally, some of the research\nchallenges in biometric fusion are enumerated. The purpose of this article is\nto provide readers a comprehensive overview of the role of information fusion\nin biometrics.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 02:15:15 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Singh", "Maneet", ""], ["Singh", "Richa", ""], ["Ross", "Arun", ""]]}, {"id": "1902.02922", "submitter": "Nasim Nematzadeh", "authors": "Nasim Nematzadeh, David M. W. Powers, Trent Lewis", "title": "Informing Computer Vision with Optical Illusions", "comments": "8 pages, 4 figures. Submitted to IJCNN2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Illusions are fascinating and immediately catch people's attention and\ninterest, but they are also valuable in terms of giving us insights into human\ncognition and perception. A good theory of human perception should be able to\nexplain the illusion, and a correct theory will actually give quantifiable\nresults. We investigate here the efficiency of a computational filtering model\nutilised for modelling the lateral inhibition of retinal ganglion cells and\ntheir responses to a range of Geometric Illusions using isotropic Differences\nof Gaussian filters. This study explores the way in which illusions have been\nexplained and shows how a simple standard model of vision based on classical\nreceptive fields can predict the existence of these illusions as well as the\ndegree of effect. A fundamental contribution of this work is to link bottom-up\nprocesses to higher level perception and cognition consistent with Marr's\ntheory of vision and edge map representation.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 03:06:29 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Nematzadeh", "Nasim", ""], ["Powers", "David M. W.", ""], ["Lewis", "Trent", ""]]}, {"id": "1902.02923", "submitter": "Guizhong Liu", "authors": "Weiqiang Li and Guizhong Liu", "title": "A Single-shot Object Detector with Feature Aggragation and Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many real applications, it is equally important to detect objects\naccurately and quickly. In this paper, we propose an accurate and efficient\nsingle shot object detector with feature aggregation and enhancement (FAENet).\nOur motivation is to enhance and exploit the shallow and deep feature maps of\nthe whole network simultaneously. To achieve it we introduce a pair of novel\nfeature aggregation modules and two feature enhancement blocks, and integrate\nthem into the original structure of SSD. Extensive experiments on both the\nPASCAL VOC and MS COCO datasets demonstrate that the proposed method achieves\nmuch higher accuracy than SSD. In addition, our method performs better than the\nstate-of-the-art one-stage detector RefineDet on small objects and can run at a\nfaster speed.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 03:08:12 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 11:07:12 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Li", "Weiqiang", ""], ["Liu", "Guizhong", ""]]}, {"id": "1902.02947", "submitter": "Danilo Vasconcellos  Vargas", "authors": "Danilo Vasconcellos Vargas, Jiawei Su", "title": "Understanding the One-Pixel Attack: Propagation Maps and Locality\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks were shown to be vulnerable to single pixel\nmodifications. However, the reason behind such phenomena has never been\nelucidated. Here, we propose Propagation Maps which show the influence of the\nperturbation in each layer of the network. Propagation Maps reveal that even in\nextremely deep networks such as Resnet, modification in one pixel easily\npropagates until the last layer. In fact, this initial local perturbation is\nalso shown to spread becoming a global one and reaching absolute difference\nvalues that are close to the maximum value of the original feature maps in a\ngiven layer. Moreover, we do a locality analysis in which we demonstrate that\nnearby pixels of the perturbed one in the one-pixel attack tend to share the\nsame vulnerability, revealing that the main vulnerability lies in neither\nneurons nor pixels but receptive fields. Hopefully, the analysis conducted in\nthis work together with a new technique called propagation maps shall shed\nlight into the inner workings of other adversarial samples and be the basis of\nnew defense systems to come.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 06:06:01 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Vargas", "Danilo Vasconcellos", ""], ["Su", "Jiawei", ""]]}, {"id": "1902.03051", "submitter": "Zizhao Zhang", "authors": "Zizhao Zhang, Adriana Romero, Matthew J. Muckley, Pascal Vincent, Lin\n  Yang, Michal Drozdzal", "title": "Reducing Uncertainty in Undersampled MRI Reconstruction with Active\n  Acquisition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of MRI reconstruction is to restore a high fidelity image from\npartially observed measurements. This partial view naturally induces\nreconstruction uncertainty that can only be reduced by acquiring additional\nmeasurements. In this paper, we present a novel method for MRI reconstruction\nthat, at inference time, dynamically selects the measurements to take and\niteratively refines the prediction in order to best reduce the reconstruction\nerror and, thus, its uncertainty. We validate our method on a large scale knee\nMRI dataset, as well as on ImageNet. Results show that (1) our system\nsuccessfully outperforms active acquisition baselines; (2) our uncertainty\nestimates correlate with error maps; and (3) our ResNet-based architecture\nsurpasses standard pixel-to-pixel models in the task of MRI reconstruction. The\nproposed method not only shows high-quality reconstructions but also paves the\nroad towards more applicable solutions for accelerating MRI.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 12:24:57 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Zhang", "Zizhao", ""], ["Romero", "Adriana", ""], ["Muckley", "Matthew J.", ""], ["Vincent", "Pascal", ""], ["Yang", "Lin", ""], ["Drozdzal", "Michal", ""]]}, {"id": "1902.03057", "submitter": "Hamidreza Kasaei", "authors": "Hamidreza Kasaei", "title": "OrthographicNet: A Deep Transfer Learning Approach for 3D Object\n  Recognition in Open-Ended Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, service robots are appearing more and more in our daily life. For\nthis type of robot, open-ended object category learning and recognition is\nnecessary since no matter how extensive the training data used for batch\nlearning, the robot might be faced with a new object when operating in a\nreal-world environment. In this work, we present OrthographicNet, a\nConvolutional Neural Network (CNN)-based model, for 3D object recognition in\nopen-ended domains. In particular, OrthographicNet generates a global rotation-\nand scale-invariant representation for a given 3D object, enabling robots to\nrecognize the same or similar objects seen from different perspectives.\nExperimental results show that our approach yields significant improvements\nover the previous state-of-the-art approaches concerning object recognition\nperformance and scalability in open-ended scenarios. Moreover, OrthographicNet\ndemonstrates the capability of learning new categories from very few examples\non-site. Regarding real-time performance, three real-world demonstrations\nvalidate the promising performance of the proposed architecture.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 12:39:00 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 11:32:40 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2020 17:37:11 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Kasaei", "Hamidreza", ""]]}, {"id": "1902.03070", "submitter": "Xi-Le Zhao", "authors": "Wen-Hao Xu, Xi-Le Zhao, Michael Ng", "title": "A Fast Algorithm for Cosine Transform Based Tensor Singular Value\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a lot of research into tensor singular value\ndecomposition (t-SVD) by using discrete Fourier transform (DFT) matrix. The\nmain aims of this paper are to propose and study tensor singular value\ndecomposition based on the discrete cosine transform (DCT) matrix. The\nadvantages of using DCT are that (i) the complex arithmetic is not involved in\nthe cosine transform based tensor singular value decomposition, so the\ncomputational cost required can be saved; (ii) the intrinsic reflexive boundary\ncondition along the tubes in the third dimension of tensors is employed, so its\nperformance would be better than that by using the periodic boundary condition\nin DFT. We demonstrate that the tensor product between two tensors by using DCT\ncan be equivalent to the multiplication between a block Toeplitz-plus-Hankel\nmatrix and a block vector. Numerical examples of low-rank tensor completion are\nfurther given to illustrate that the efficiency by using DCT is two times\nfaster than that by using DFT and also the errors of video and multispectral\nimage completion by using DCT are smaller than those by using DFT.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 13:21:37 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Xu", "Wen-Hao", ""], ["Zhao", "Xi-Le", ""], ["Ng", "Michael", ""]]}, {"id": "1902.03084", "submitter": "Jun Liu", "authors": "Jun Liu, Amir Shahroudy, Gang Wang, Ling-Yu Duan, Alex C. Kot", "title": "Skeleton-Based Online Action Prediction Using Scale Selection Network", "comments": "This paper has been accepted by T-PAMI. DOI:\n  10.1109/TPAMI.2019.2898954", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action prediction is to recognize the class label of an ongoing activity when\nonly a part of it is observed. In this paper, we focus on online action\nprediction in streaming 3D skeleton sequences. A dilated convolutional network\nis introduced to model the motion dynamics in temporal dimension via a sliding\nwindow over the temporal axis. Since there are significant temporal scale\nvariations in the observed part of the ongoing action at different time steps,\na novel window scale selection method is proposed to make our network focus on\nthe performed part of the ongoing action and try to suppress the possible\nincoming interference from the previous actions at each step. An activation\nsharing scheme is also proposed to handle the overlapping computations among\nthe adjacent time steps, which enables our framework to run more efficiently.\nMoreover, to enhance the performance of our framework for action prediction\nwith the skeletal input data, a hierarchy of dilated tree convolutions are also\ndesigned to learn the multi-level structured semantic representations over the\nskeleton joints at each frame. Our proposed approach is evaluated on four\nchallenging datasets. The extensive experiments demonstrate the effectiveness\nof our method for skeleton-based online action prediction.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 14:04:25 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 10:46:28 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Liu", "Jun", ""], ["Shahroudy", "Amir", ""], ["Wang", "Gang", ""], ["Duan", "Ling-Yu", ""], ["Kot", "Alex C.", ""]]}, {"id": "1902.03088", "submitter": "Hasan Asy'ari Arief", "authors": "Hasan Asyari Arief, Ulf Geir Indahl, Geir-Harald Strand, H{\\aa}vard\n  Tveite", "title": "Addressing Overfitting on Pointcloud Classification using Atrous XCRF", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing Volume 155,\n  September 2019, Pages 90-101", "doi": "10.1016/j.isprsjprs.2019.07.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in techniques for automated classification of pointcloud data\nintroduce great opportunities for many new and existing applications. However,\nwith a limited number of labeled points, automated classification by a machine\nlearning model is prone to overfitting and poor generalization. The present\npaper addresses this problem by inducing controlled noise (on a trained model)\ngenerated by invoking conditional random field similarity penalties using\nnearby features. The method is called Atrous XCRF and works by forcing a\ntrained model to respect the similarity penalties provided by unlabeled data.\nIn a benchmark study carried out using the ISPRS 3D labeling dataset, our\ntechnique achieves 84.97% in term of overall accuracy, and 71.05% in term of F1\nscore. The result is on par with the current best model for the benchmark\ndataset and has the highest value in term of F1 score.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 14:20:42 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Arief", "Hasan Asyari", ""], ["Indahl", "Ulf Geir", ""], ["Strand", "Geir-Harald", ""], ["Tveite", "H\u00e5vard", ""]]}, {"id": "1902.03091", "submitter": "Chaitanya Kaul", "authors": "Chaitanya Kaul, Suresh Manandhar, Nick Pears", "title": "FocusNet: An attention-based Fully Convolutional Network for Medical\n  Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel technique to incorporate attention within convolutional\nneural networks using feature maps generated by a separate convolutional\nautoencoder. Our attention architecture is well suited for incorporation with\ndeep convolutional networks. We evaluate our model on benchmark segmentation\ndatasets in skin cancer segmentation and lung lesion segmentation. Results show\nhighly competitive performance when compared with U-Net and it's residual\nvariant.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 14:24:36 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Kaul", "Chaitanya", ""], ["Manandhar", "Suresh", ""], ["Pears", "Nick", ""]]}, {"id": "1902.03120", "submitter": "Maryam Sultana", "authors": "Maryam Sultana, Soon Ki Jung", "title": "Illumination Invariant Foreground Object Segmentation using ForeGANs", "comments": "3 pages, 1 figure. arXiv admin note: text overlap with\n  arXiv:1811.01526", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The foreground segmentation algorithms suffer performance degradation in the\npresence of various challenges such as dynamic backgrounds, and various\nillumination conditions. To handle these challenges, we present a foreground\nsegmentation method, based on generative adversarial network (GAN). We aim to\nsegment foreground objects in the presence of two aforementioned major\nchallenges in background scenes in real environments. To address this problem,\nour presented GAN model is trained on background image samples with dynamic\nchanges, after that for testing the GAN model has to generate the same\nbackground sample as test sample with similar conditions via back-propagation\ntechnique. The generated background sample is then subtracted from the given\ntest sample to segment foreground objects. The comparison of our proposed\nmethod with five state-of-the-art methods highlights the strength of our\nalgorithm for foreground segmentation in the presence of challenging dynamic\nbackground scenario.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 12:01:56 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 13:54:39 GMT"}, {"version": "v3", "created": "Sat, 5 Oct 2019 08:36:44 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Sultana", "Maryam", ""], ["Jung", "Soon Ki", ""]]}, {"id": "1902.03122", "submitter": "Oindrila Saha", "authors": "Oindrila Saha, Rachana Sathish, Debdoot Sheet", "title": "Fully Convolutional Neural Network for Semantic Segmentation of\n  Anatomical Structure and Pathologies in Colour Fundus Images Associated with\n  Diabetic Retinopathy", "comments": "arXiv admin note: text overlap with arXiv:1511.00561 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Diabetic retinopathy (DR) is the most common form of diabetic eye disease.\nRetinopathy can affect all diabetic patients and becomes particularly\ndangerous, increasing the risk of blindness, if it is left untreated. The\nsuccess rate of its curability solemnly depends on diagnosis at an early stage.\nThe development of automated computer aided disease diagnosis tools could help\nin faster detection of symptoms with a wider reach and reasonable cost. This\npaper proposes a method for the automated segmentation of retinal lesions and\noptic disk in fundus images using a deep fully convolutional neural network for\nsemantic segmentation. This trainable segmentation pipeline consists of an\nencoder network, a corresponding decoder network followed by pixel-wise\nclassification to segment microaneurysms, hemorrhages, hard exudates, soft\nexudates, optic disk from background. The network was trained using Binary\ncross entropy criterion with Sigmoid as the last layer, while during an\nadditional SoftMax layer was used for boosting response of single class. The\nperformance of the proposed method is evaluated using sensitivity, positive\nprediction value (PPV) and accuracy as the metrices. Further, the position of\nthe Optic disk is localised using the segmented output map.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 10:37:40 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Saha", "Oindrila", ""], ["Sathish", "Rachana", ""], ["Sheet", "Debdoot", ""]]}, {"id": "1902.03129", "submitter": "Amirata Ghorbani", "authors": "Amirata Ghorbani, James Wexler, James Zou, Been Kim", "title": "Towards Automatic Concept-based Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability has become an important topic of research as more machine\nlearning (ML) models are deployed and widely used to make important decisions.\n  Most of the current explanation methods provide explanations through feature\nimportance scores, which identify features that are important for each\nindividual input. However, how to systematically summarize and interpret such\nper sample feature importance scores itself is challenging. In this work, we\npropose principles and desiderata for \\emph{concept} based explanation, which\ngoes beyond per-sample features to identify higher-level human-understandable\nconcepts that apply across the entire dataset. We develop a new algorithm, ACE,\nto automatically extract visual concepts. Our systematic experiments\ndemonstrate that \\alg discovers concepts that are human-meaningful, coherent\nand important for the neural network's predictions.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 03:18:54 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 18:53:03 GMT"}, {"version": "v3", "created": "Tue, 8 Oct 2019 09:28:43 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Ghorbani", "Amirata", ""], ["Wexler", "James", ""], ["Zou", "James", ""], ["Kim", "Been", ""]]}, {"id": "1902.03151", "submitter": "Priyadarshini Panda", "authors": "Priyadarshini Panda, Indranil Chakraborty and Kaushik Roy", "title": "Discretization based Solutions for Secure Machine Learning against\n  Adversarial Attacks", "comments": "8 pages, 8 Figures, 6 Tables", "journal-ref": "IEEE Access, 2019", "doi": "10.1109/ACCESS.2019.2919463", "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are perturbed inputs that are designed (from a deep\nlearning network's (DLN) parameter gradients) to mislead the DLN during test\ntime. Intuitively, constraining the dimensionality of inputs or parameters of a\nnetwork reduces the 'space' in which adversarial examples exist. Guided by this\nintuition, we demonstrate that discretization greatly improves the robustness\nof DLNs against adversarial attacks. Specifically, discretizing the input space\n(or allowed pixel levels from 256 values or 8-bit to 4 values or 2-bit)\nextensively improves the adversarial robustness of DLNs for a substantial range\nof perturbations for minimal loss in test accuracy. Furthermore, we find that\nBinary Neural Networks (BNNs) and related variants are intrinsically more\nrobust than their full precision counterparts in adversarial scenarios.\nCombining input discretization with BNNs furthers the robustness even waiving\nthe need for adversarial training for certain magnitude of perturbation values.\nWe evaluate the effect of discretization on MNIST, CIFAR10, CIFAR100 and\nImagenet datasets. Across all datasets, we observe maximal adversarial\nresistance with 2-bit input discretization that incurs an adversarial accuracy\nloss of just ~1-2% as compared to clean test accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 15:38:24 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 18:15:55 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Panda", "Priyadarshini", ""], ["Chakraborty", "Indranil", ""], ["Roy", "Kaushik", ""]]}, {"id": "1902.03192", "submitter": "Panagiotis Mousouliotis", "authors": "Panagiotis G. Mousouliotis and Loukas P. Petrou", "title": "Software-Defined FPGA Accelerator Design for Mobile Deep Learning\n  Applications", "comments": "Accepted to be presented in the 15th International Symposium on\n  Applied Reconfigurable Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the field of deep learning has received great attention by the\nscientific community and it is used to provide improved solutions to many\ncomputer vision problems. Convolutional neural networks (CNNs) have been\nsuccessfully used to attack problems such as object recognition, object\ndetection, semantic segmentation, and scene understanding. The rapid\ndevelopment of deep learning goes hand by hand with the adaptation of GPUs for\naccelerating its processes, such as network training and inference. Even though\nFPGA design exists long before the use of GPUs for accelerating computations\nand despite the fact that high-level synthesis (HLS) tools are getting more\nattractive, the adaptation of FPGAs for deep learning research and application\ndevelopment is poor due to the requirement of hardware design related\nexpertise. This work presents a workflow for deep learning mobile application\nacceleration on small low-cost low-power FPGA devices using HLS tools. This\nworkflow eases the design of an improved version of the SqueezeJet accelerator\nused for the speedup of mobile-friendly low-parameter ImageNet class CNNs, such\nas the SqueezeNet v1.1 and the ZynqNet. Additionally, the workflow includes the\ndevelopment of an HLS-driven analytical model which is used for performance\nestimation of the accelerator. This model can be also used to direct the design\nprocess and lead to future design improvements and optimizations.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 17:08:39 GMT"}, {"version": "v2", "created": "Sun, 24 Mar 2019 17:23:57 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Mousouliotis", "Panagiotis G.", ""], ["Petrou", "Loukas P.", ""]]}, {"id": "1902.03227", "submitter": "Sanjana Srivastava", "authors": "Sanjana Srivastava, Guy Ben-Yosef, Xavier Boix", "title": "Minimal Images in Deep Neural Networks: Fragile Object Recognition in\n  Natural Images", "comments": "International Conference on Learning Representations (ICLR) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human ability to recognize objects is impaired when the object is not\nshown in full. \"Minimal images\" are the smallest regions of an image that\nremain recognizable for humans. Ullman et al. 2016 show that a slight\nmodification of the location and size of the visible region of the minimal\nimage produces a sharp drop in human recognition accuracy. In this paper, we\ndemonstrate that such drops in accuracy due to changes of the visible region\nare a common phenomenon between humans and existing state-of-the-art deep\nneural networks (DNNs), and are much more prominent in DNNs. We found many\ncases where DNNs classified one region correctly and the other incorrectly,\nthough they only differed by one row or column of pixels, and were often bigger\nthan the average human minimal image size. We show that this phenomenon is\nindependent from previous works that have reported lack of invariance to minor\nmodifications in object location in DNNs. Our results thus reveal a new failure\nmode of DNNs that also affects humans to a much lesser degree. They expose how\nfragile DNN recognition ability is for natural images even without adversarial\npatterns being introduced. Bringing the robustness of DNNs in natural images to\nthe human level remains an open challenge for the community.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 18:36:49 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Srivastava", "Sanjana", ""], ["Ben-Yosef", "Guy", ""], ["Boix", "Xavier", ""]]}, {"id": "1902.03233", "submitter": "Onur Ozdemir", "authors": "Onur Ozdemir, Rebecca L. Russell, Andrew A. Berlin", "title": "A 3D Probabilistic Deep Learning System for Detection and Diagnosis of\n  Lung Cancer Using Low-Dose CT Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new computer aided detection and diagnosis system for lung\ncancer screening with low-dose CT scans that produces meaningful probability\nassessments. Our system is based entirely on 3D convolutional neural networks\nand achieves state-of-the-art performance for both lung nodule detection and\nmalignancy classification tasks on the publicly available LUNA16 and Kaggle\nData Science Bowl challenges. While nodule detection systems are typically\ndesigned and optimized on their own, we find that it is important to consider\nthe coupling between detection and diagnosis components. Exploiting this\ncoupling allows us to develop an end-to-end system that has higher and more\nrobust performance and eliminates the need for a nodule detection false\npositive reduction stage. Furthermore, we characterize model uncertainty in our\ndeep learning systems, a first for lung CT analysis, and show that we can use\nthis to provide well-calibrated classification probabilities for both nodule\ndetection and patient malignancy diagnosis. These calibrated probabilities\ninformed by model uncertainty can be used for subsequent risk-based decision\nmaking towards diagnostic interventions or disease treatments, as we\ndemonstrate using a probability-based patient referral strategy to further\nimprove our results.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 18:53:27 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 01:03:32 GMT"}, {"version": "v3", "created": "Tue, 21 Jan 2020 02:27:50 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Ozdemir", "Onur", ""], ["Russell", "Rebecca L.", ""], ["Berlin", "Andrew A.", ""]]}, {"id": "1902.03253", "submitter": "Alceu Emanuel Bissoto", "authors": "Alceu Bissoto, F\\'abio Perez, Eduardo Valle and Sandra Avila", "title": "Skin Lesion Synthesis with Generative Adversarial Networks", "comments": "Conference: ISIC Skin Image Analysis Workshop and Challenge @ MICCAI\n  2018", "journal-ref": null, "doi": "10.1007/978-3-030-01201-4_32", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin cancer is by far the most common type of cancer. Early detection is the\nkey to increase the chances for successful treatment significantly. Currently,\nDeep Neural Networks are the state-of-the-art results on automated skin cancer\nclassification. To push the results further, we need to address the lack of\nannotated data, which is expensive and require much effort from specialists. To\nbypass this problem, we propose using Generative Adversarial Networks for\ngenerating realistic synthetic skin lesion images. To the best of our\nknowledge, our results are the first to show visually-appealing synthetic\nimages that comprise clinically-meaningful information.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 19:03:41 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Bissoto", "Alceu", ""], ["Perez", "F\u00e1bio", ""], ["Valle", "Eduardo", ""], ["Avila", "Sandra", ""]]}, {"id": "1902.03284", "submitter": "Pedro Marrero", "authors": "Pedro D. Marrero Fernandez, Fidel A. Guerrero Pe\\~na, Tsang Ing Ren,\n  Alexandre Cunha", "title": "FERAtt: Facial Expression Recognition with Attention Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new end-to-end network architecture for facial expression\nrecognition with an attention model. It focuses attention in the human face and\nuses a Gaussian space representation for expression recognition. We devise this\narchitecture based on two fundamental complementary components: (1) facial\nimage correction and attention and (2) facial expression representation and\nclassification. The first component uses an encoder-decoder style network and a\nconvolutional feature extractor that are pixel-wise multiplied to obtain a\nfeature attention map. The second component is responsible for obtaining an\nembedded representation and classification of the facial expression. We propose\na loss function that creates a Gaussian structure on the representation space.\nTo demonstrate the proposed method, we create two larger and more comprehensive\nsynthetic datasets using the traditional BU3DFE and CK+ facial datasets. We\ncompared results with the PreActResNet18 baseline. Our experiments on these\ndatasets have shown the superiority of our approach in recognizing facial\nexpressions.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 20:39:53 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Fernandez", "Pedro D. Marrero", ""], ["Pe\u00f1a", "Fidel A. Guerrero", ""], ["Ren", "Tsang Ing", ""], ["Cunha", "Alexandre", ""]]}, {"id": "1902.03326", "submitter": "Bhav Ashok", "authors": "Anubhav Ashok", "title": "Architecture Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel approach to model compression termed\nArchitecture Compression. Instead of operating on the weight or filter space of\nthe network like classical model compression methods, our approach operates on\nthe architecture space. A 1-D CNN encoder-decoder is trained to learn a mapping\nfrom discrete architecture space to a continuous embedding and back.\nAdditionally, this embedding is jointly trained to regress accuracy and\nparameter count in order to incorporate information about the architecture's\neffectiveness on the dataset. During the compression phase, we first encode the\nnetwork and then perform gradient descent in continuous space to optimize a\ncompression objective function that maximizes accuracy and minimizes parameter\ncount. The final continuous feature is then mapped to a discrete architecture\nusing the decoder. We demonstrate the merits of this approach on visual\nrecognition tasks such as CIFAR-10, CIFAR-100, Fashion-MNIST and SVHN and\nachieve a greater than 20x compression on CIFAR-10.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 23:26:12 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 08:42:42 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2019 09:10:49 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Ashok", "Anubhav", ""]]}, {"id": "1902.03334", "submitter": "Tomas Hodan", "authors": "Tomas Hodan, Vibhav Vineet, Ran Gal, Emanuel Shalev, Jon Hanzelka,\n  Treb Connell, Pedro Urbina, Sudipta N. Sinha, Brian Guenter", "title": "Photorealistic Image Synthesis for Object Instance Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to synthesize highly photorealistic images of 3D\nobject models, which we use to train a convolutional neural network for\ndetecting the objects in real images. The proposed approach has three key\ningredients: (1) 3D object models are rendered in 3D models of complete scenes\nwith realistic materials and lighting, (2) plausible geometric configuration of\nobjects and cameras in a scene is generated using physics simulations, and (3)\nhigh photorealism of the synthesized images achieved by physically based\nrendering. When trained on images synthesized by the proposed approach, the\nFaster R-CNN object detector achieves a 24% absolute improvement of mAP@.75IoU\non Rutgers APC and 11% on LineMod-Occluded datasets, compared to a baseline\nwhere the training images are synthesized by rendering object models on top of\nrandom photographs. This work is a step towards being able to effectively train\nobject detectors without capturing or annotating any real images. A dataset of\n600K synthetic images with ground truth annotations for various computer vision\ntasks will be released on the project website: thodan.github.io/objectsynth.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 00:14:46 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Hodan", "Tomas", ""], ["Vineet", "Vibhav", ""], ["Gal", "Ran", ""], ["Shalev", "Emanuel", ""], ["Hanzelka", "Jon", ""], ["Connell", "Treb", ""], ["Urbina", "Pedro", ""], ["Sinha", "Sudipta N.", ""], ["Guenter", "Brian", ""]]}, {"id": "1902.03346", "submitter": "Mohammad Saad Billah", "authors": "Mohammad Billah, Farzana Rahman, Arash Maskooki, Michael Todd, Matthew\n  Barth, Jay A. Farrell", "title": "Challenges in Partially-Automated Roadway Feature Mapping Using Mobile\n  Laser Scanning and Vehicle Trajectory Data", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connected vehicle and driver's assistance applications are greatly\nfacilitated by Enhanced Digital Maps (EDMs) that represent roadway features\n(e.g., lane edges or centerlines, stop bars). Due to the large number of\nsignalized intersections and miles of roadway, manual development of EDMs on a\nglobal basis is not feasible. Mobile Terrestrial Laser Scanning (MTLS) is the\npreferred data acquisition method to provide data for automated EDM\ndevelopment. Such systems provide an MTLS trajectory and a point cloud for the\nroadway environment. The challenge is to automatically convert these data into\nan EDM. This article presents a new processing and feature extraction method,\nexperimental demonstration providing SAE-J2735 map messages for eleven example\nintersections, and a discussion of the results that points out remaining\nchallenges and suggests directions for future research.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 01:18:36 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Billah", "Mohammad", ""], ["Rahman", "Farzana", ""], ["Maskooki", "Arash", ""], ["Todd", "Michael", ""], ["Barth", "Matthew", ""], ["Farrell", "Jay A.", ""]]}, {"id": "1902.03361", "submitter": "Houpu Yao", "authors": "Houpu Yao, Malcolm Regan, Yezhou Yang, Yi Ren", "title": "Image Decomposition and Classification through a Generative Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate in this paper that a generative model can be designed to\nperform classification tasks under challenging settings, including adversarial\nattacks and input distribution shifts. Specifically, we propose a conditional\nvariational autoencoder that learns both the decomposition of inputs and the\ndistributions of the resulting components. During test, we jointly optimize the\nlatent variables of the generator and the relaxed component labels to find the\nbest match between the given input and the output of the generator. The model\ndemonstrates promising performance at recognizing overlapping components from\nthe multiMNIST dataset, and novel component combinations from a traffic sign\ndataset. Experiments also show that the proposed model achieves high robustness\non MNIST and NORB datasets, in particular for high-strength gradient attacks\nand non-gradient attacks.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 02:40:04 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Yao", "Houpu", ""], ["Regan", "Malcolm", ""], ["Yang", "Yezhou", ""], ["Ren", "Yi", ""]]}, {"id": "1902.03368", "submitter": "Noel Codella", "authors": "Noel Codella, Veronica Rotemberg, Philipp Tschandl, M. Emre Celebi,\n  Stephen Dusza, David Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris,\n  Michael Marchetti, Harald Kittler, and Allan Halpern", "title": "Skin Lesion Analysis Toward Melanoma Detection 2018: A Challenge Hosted\n  by the International Skin Imaging Collaboration (ISIC)", "comments": "https://challenge2018.isic-archive.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work summarizes the results of the largest skin image analysis challenge\nin the world, hosted by the International Skin Imaging Collaboration (ISIC), a\nglobal partnership that has organized the world's largest public repository of\ndermoscopic images of skin. The challenge was hosted in 2018 at the Medical\nImage Computing and Computer Assisted Intervention (MICCAI) conference in\nGranada, Spain. The dataset included over 12,500 images across 3 tasks. 900\nusers registered for data download, 115 submitted to the lesion segmentation\ntask, 25 submitted to the lesion attribute detection task, and 159 submitted to\nthe disease classification task. Novel evaluation protocols were established,\nincluding a new test for segmentation algorithm performance, and a test for\nalgorithm ability to generalize. Results show that top segmentation algorithms\nstill fail on over 10% of images on average, and algorithms with equal\nperformance on test data can have different abilities to generalize. This is an\nimportant consideration for agencies regulating the growing set of machine\nlearning tools in the healthcare domain, and sets a new standard for future\npublic challenges in healthcare.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 04:18:10 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 17:36:27 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Codella", "Noel", ""], ["Rotemberg", "Veronica", ""], ["Tschandl", "Philipp", ""], ["Celebi", "M. Emre", ""], ["Dusza", "Stephen", ""], ["Gutman", "David", ""], ["Helba", "Brian", ""], ["Kalloo", "Aadi", ""], ["Liopyris", "Konstantinos", ""], ["Marchetti", "Michael", ""], ["Kittler", "Harald", ""], ["Halpern", "Allan", ""]]}, {"id": "1902.03377", "submitter": "Guangcun Shan", "authors": "Weikuang Li, Tian Wang, Chuanyun Wang, Guangcun Shan, Mengyi Zhang and\n  Hichem Snoussi", "title": "Region based Ensemble Learning Network for Fine-grained Classification", "comments": "6 pages, 3 figures, 2018 Chinese Automation Congress (CAC)", "journal-ref": null, "doi": "10.1109/CAC.2018.8623687", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As an important research topic in computer vision, fine-grained\nclassification which aims to recognition subordinate-level categories has\nattracted significant attention. We propose a novel region based ensemble\nlearning network for fine-grained classification. Our approach contains a\ndetection module and a module for classification. The detection module is based\non the faster R-CNN framework to locate the semantic regions of the object. The\nclassification module using an ensemble learning method, which trains a set of\nsub-classifiers for different semantic regions and combines them together to\nget a stronger classifier. In the evaluation, we implement experiments on the\nCUB-2011 dataset and the result of experiments proves our method s efficient\nfor fine-grained classification. We also extend our approach to remote scene\nrecognition and evaluate it on the NWPU-RESISC45 dataset.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 06:11:32 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Li", "Weikuang", ""], ["Wang", "Tian", ""], ["Wang", "Chuanyun", ""], ["Shan", "Guangcun", ""], ["Zhang", "Mengyi", ""], ["Snoussi", "Hichem", ""]]}, {"id": "1902.03380", "submitter": "C. H. Huck Yang", "authors": "Chao-Han Huck Yang, Yi-Chieh Liu, Pin-Yu Chen, Xiaoli Ma, Yi-Chang\n  James Tsai", "title": "When Causal Intervention Meets Adversarial Examples and Image Masking\n  for Deep Neural Networks", "comments": "Noted our camera-ready version has changed the title. \"When Causal\n  Intervention Meets Adversarial Examples and Image Masking for Deep Neural\n  Networks\" as the v3 official paper title in IEEE Proceeding. Please use it in\n  your formal reference. Accepted at IEEE ICIP 2019. Pytorch code has released\n  on https://github.com/jjaacckkyy63/Causal-Intervention-AE-wAdvImg", "journal-ref": "2019 26th IEEE International Conference on Image Processing\n  (ICIP). IEEE", "doi": null, "report-no": "pages={3811--3815}", "categories": "cs.CV cs.AI cs.LG cs.SC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Discovering and exploiting the causality in deep neural networks (DNNs) are\ncrucial challenges for understanding and reasoning causal effects (CE) on an\nexplainable visual model. \"Intervention\" has been widely used for recognizing a\ncausal relation ontologically. In this paper, we propose a causal inference\nframework for visual reasoning via do-calculus. To study the intervention\neffects on pixel-level features for causal reasoning, we introduce pixel-wise\nmasking and adversarial perturbation. In our framework, CE is calculated using\nfeatures in a latent space and perturbed prediction from a DNN-based model. We\nfurther provide the first look into the characteristics of discovered CE of\nadversarially perturbed images generated by gradient-based methods\n\\footnote{~~https://github.com/jjaacckkyy63/Causal-Intervention-AE-wAdvImg}.\nExperimental results show that CE is a competitive and robust index for\nunderstanding DNNs when compared with conventional methods such as\nclass-activation mappings (CAMs) on the Chest X-Ray-14 dataset for\nhuman-interpretable feature(s) (e.g., symptom) reasoning. Moreover, CE holds\npromises for detecting adversarial examples as it possesses distinct\ncharacteristics in the presence of adversarial perturbations.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 06:44:13 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 19:11:24 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 15:07:42 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Yang", "Chao-Han Huck", ""], ["Liu", "Yi-Chieh", ""], ["Chen", "Pin-Yu", ""], ["Ma", "Xiaoli", ""], ["Tsai", "Yi-Chang James", ""]]}, {"id": "1902.03442", "submitter": "Senthil Yogamani", "authors": "Michal Uricar, Pavel Krizek, David Hurych, Ibrahim Sobh, Senthil\n  Yogamani and Patrick Denny", "title": "Yes, we GAN: Applying Adversarial Techniques for Autonomous Driving", "comments": "Accepted for publication in Electronic Imaging, Autonomous Vehicles\n  and Machines 2019. arXiv admin note: text overlap with arXiv:1606.05908 by\n  other authors", "journal-ref": null, "doi": "10.2352/ISSN.2470-1173.2019.15.AVM-048", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GAN) have gained a lot of popularity from\ntheir introduction in 2014 till present. Research on GAN is rapidly growing and\nthere are many variants of the original GAN focusing on various aspects of deep\nlearning. GAN are perceived as the most impactful direction of machine learning\nin the last decade. This paper focuses on the application of GAN in autonomous\ndriving including topics such as advanced data augmentation, loss function\nlearning, semi-supervised learning, etc. We formalize and review key\napplications of adversarial techniques and discuss challenges and open problems\nto be addressed.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 16:42:47 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 18:22:01 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Uricar", "Michal", ""], ["Krizek", "Pavel", ""], ["Hurych", "David", ""], ["Sobh", "Ibrahim", ""], ["Yogamani", "Senthil", ""], ["Denny", "Patrick", ""]]}, {"id": "1902.03451", "submitter": "Adnane Boukhayma", "authors": "Adnane Boukhayma, Rodrigo de Bem, Philip H.S. Torr", "title": "3D Hand Shape and Pose from Images in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this work the first end-to-end deep learning based method that\npredicts both 3D hand shape and pose from RGB images in the wild. Our network\nconsists of the concatenation of a deep convolutional encoder, and a fixed\nmodel-based decoder. Given an input image, and optionally 2D joint detections\nobtained from an independent CNN, the encoder predicts a set of hand and view\nparameters. The decoder has two components: A pre-computed articulated mesh\ndeformation hand model that generates a 3D mesh from the hand parameters, and a\nre-projection module controlled by the view parameters that projects the\ngenerated hand into the image domain. We show that using the shape and pose\nprior knowledge encoded in the hand model within a deep learning framework\nyields state-of-the-art performance in 3D pose prediction from images on\nstandard benchmarks, and produces geometrically valid and plausible 3D\nreconstructions. Additionally, we show that training with weak supervision in\nthe form of 2D joint annotations on datasets of images in the wild, in\nconjunction with full supervision in the form of 3D joint annotations on\nlimited available datasets allows for good generalization to 3D shape and pose\npredictions on images in the wild.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 17:30:16 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Boukhayma", "Adnane", ""], ["de Bem", "Rodrigo", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1902.03459", "submitter": "Marcin Kopaczka", "authors": "Marcin Kopaczka, Justus Schock, Dorit Merhof", "title": "Super-realtime facial landmark detection and shape fitting by deep\n  regression of shape model parameters", "comments": "https://github.com/justusschock/shapenet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for highly efficient landmark detection that combines\ndeep convolutional neural networks with well established model-based fitting\nalgorithms. Motivated by established model-based fitting methods such as active\nshapes, we use a PCA of the landmark positions to allow generative modeling of\nfacial landmarks. Instead of computing the model parameters using iterative\noptimization, the PCA is included in a deep neural network using a novel layer\ntype. The network predicts model parameters in a single forward pass, thereby\nallowing facial landmark detection at several hundreds of frames per second.\nOur architecture allows direct end-to-end training of a model-based landmark\ndetection method and shows that deep neural networks can be used to reliably\npredict model parameters directly without the need for an iterative\noptimization. The method is evaluated on different datasets for facial landmark\ndetection and medical image segmentation. PyTorch code is freely available at\nhttps://github.com/justusschock/shapenet\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 17:59:07 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Kopaczka", "Marcin", ""], ["Schock", "Justus", ""], ["Merhof", "Dorit", ""]]}, {"id": "1902.03466", "submitter": "Jos\\'e Solomon", "authors": "Jose Solomon and Francois Charette", "title": "Hierarchical Multi-task Deep Neural Network Architecture for End-to-End\n  Driving", "comments": "18 pages, 17 plots and figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel hierarchical Deep Neural Network (DNN) model is presented to address\nthe task of end-to-end driving. The model consists of a master classifier\nnetwork which determines the driving task required from an input stereo image\nand directs said image to one of a set of subservient network regression models\nthat perform inference and output a steering command. These subservient\nnetworks are designed and trained for a specific driving task: straightaway,\nswerve maneuver, tight turn, gradual turn, and chicane. Using this modular\nnetwork strategy allows for two primary advantages: an overall reduction in the\namount of data required to train the complete system, and for model tailoring\nwhere more complex models can be used for more challenging tasks while\nsimplified networks can handle more mundane tasks. It is this latter facet of\nthe model that makes the approach attractive to a number of applications beyond\nthe current vehicle steering strategy.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 18:23:51 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 01:26:54 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Solomon", "Jose", ""], ["Charette", "Francois", ""]]}, {"id": "1902.03471", "submitter": "Mohammad Samar Ansari", "authors": "Asra Aslam and Mohd. Samar Ansari", "title": "Depth-Map Generation using Pixel Matching in Stereoscopic Pair of Images", "comments": "5 Pages, pre-print, to be submitted to a conference later", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern day multimedia content generation and dissemination is moving towards\nthe presentation of more and more `realistic' scenarios. The switch from\n2-dimensional (2D) to 3-dimensional (3D) has been a major driving force in that\ndirection. Over the recent past, a large number of approaches have been\nproposed for creating 3D images/videos most of which are based on the\ngeneration of depth-maps. This paper presents a new algorithm for obtaining\ndepth information pertaining to a depicted scene from a set of available pair\nof stereoscopic images. The proposed algorithm performs a pixel-to-pixel\nmatching of the two images in the stereo pair for estimation of depth. It is\nshown that the obtained depth-maps show improvements over the reported\ncounterparts.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 18:44:35 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 12:51:11 GMT"}, {"version": "v3", "created": "Wed, 15 May 2019 10:29:37 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Aslam", "Asra", ""], ["Ansari", "Mohd. Samar", ""]]}, {"id": "1902.03477", "submitter": "Brenden Lake", "authors": "Brenden M. Lake, Ruslan Salakhutdinov, Joshua B. Tenenbaum", "title": "The Omniglot challenge: a 3-year progress report", "comments": "In press at Current Opinion in Behavioral Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three years ago, we released the Omniglot dataset for one-shot learning,\nalong with five challenge tasks and a computational model that addresses these\ntasks. The model was not meant to be the final word on Omniglot; we hoped that\nthe community would build on our work and develop new approaches. In the time\nsince, we have been pleased to see wide adoption of the dataset. There has been\nnotable progress on one-shot classification, but researchers have adopted new\nsplits and procedures that make the task easier. There has been less progress\non the other four tasks. We conclude that recent approaches are still far from\nhuman-like concept learning on Omniglot, a challenge that requires performing\nmany tasks with a single model.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 19:13:31 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 20:01:27 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Lake", "Brenden M.", ""], ["Salakhutdinov", "Ruslan", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1902.03493", "submitter": "Mohammad Tofighi", "authors": "Yuelong Li, Mohammad Tofighi, Junyi Geng, Vishal Monga, and Yonina C.\n  Eldar", "title": "Deep Algorithm Unrolling for Blind Image Deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind image deblurring remains a topic of enduring interest. Learning based\napproaches, especially those that employ neural networks have emerged to\ncomplement traditional model based methods and in many cases achieve vastly\nenhanced performance. That said, neural network approaches are generally\nempirically designed and the underlying structures are difficult to interpret.\nIn recent years, a promising technique called algorithm unrolling has been\ndeveloped that has helped connect iterative algorithms such as those for sparse\ncoding to neural network architectures. However, such connections have not been\nmade yet for blind image deblurring. In this paper, we propose a neural network\narchitecture based on this idea. We first present an iterative algorithm that\nmay be considered as a generalization of the traditional total-variation\nregularization method in the gradient domain. We then unroll the algorithm to\nconstruct a neural network for image deblurring which we refer to as Deep\nUnrolling for Blind Deblurring (DUBLID). Key algorithm parameters are learned\nwith the help of training images. Our proposed deep network DUBLID achieves\nsignificant practical performance gains while enjoying interpretability at the\nsame time. Extensive experimental results show that DUBLID outperforms many\nstate-of-the-art methods and in addition is computationally faster.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 21:19:35 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 20:40:09 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 16:40:59 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Li", "Yuelong", ""], ["Tofighi", "Mohammad", ""], ["Geng", "Junyi", ""], ["Monga", "Vishal", ""], ["Eldar", "Yonina C.", ""]]}, {"id": "1902.03510", "submitter": "Xiaohui Yang", "authors": "Xiao-Hui Yang, Li Tian, Yun-Mei Chen, Li-Jun Yang, Shuang Xu, and\n  Wen-Ming Wu", "title": "Inverse Projection Representation and Category Contribution Rate for\n  Robust Tumor Recognition", "comments": "14 pages, 19 figures, 10 tables", "journal-ref": "IEEE/ACM Transactions on Computational Biology and Bioinformatics,\n  2018", "doi": "10.1109/TCBB.2018.2886334", "report-no": null, "categories": "q-bio.QM cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation based classification (SRC) methods have achieved\nremarkable results. SRC, however, still suffer from requiring enough training\nsamples, insufficient use of test samples and instability of representation. In\nthis paper, a stable inverse projection representation based classification\n(IPRC) is presented to tackle these problems by effectively using test samples.\nAn IPR is firstly proposed and its feasibility and stability are analyzed. A\nclassification criterion named category contribution rate is constructed to\nmatch the IPR and complete classification. Moreover, a statistical measure is\nintroduced to quantify the stability of representation-based classification\nmethods. Based on the IPRC technique, a robust tumor recognition framework is\npresented by interpreting microarray gene expression data, where a two-stage\nhybrid gene selection method is introduced to select informative genes.\nFinally, the functional analysis of candidate's pathogenicity-related genes is\ngiven. Extensive experiments on six public tumor microarray gene expression\ndatasets demonstrate the proposed technique is competitive with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 23:07:22 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 04:07:28 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Yang", "Xiao-Hui", ""], ["Tian", "Li", ""], ["Chen", "Yun-Mei", ""], ["Yang", "Li-Jun", ""], ["Xu", "Shuang", ""], ["Wu", "Wen-Ming", ""]]}, {"id": "1902.03514", "submitter": "Ayan Kumar Bhunia", "authors": "Sauradip Nag, Ayan Kumar Bhunia, Aishik Konwer, Partha Pratim Roy", "title": "Facial Micro-Expression Spotting and Recognition using Time Contrasted\n  Feature with Visual Memory", "comments": "International Conference on Acoustics, Speech, and Signal\n  Processing(ICASSP), 2019", "journal-ref": null, "doi": "10.1109/ICASSP.2019.8683737", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial micro-expressions are sudden involuntary minute muscle movements which\nreveal true emotions that people try to conceal. Spotting a micro-expression\nand recognizing it is a major challenge owing to its short duration and\nintensity. Many works pursued traditional and deep learning based approaches to\nsolve this issue but compromised on learning low-level features and higher\naccuracy due to unavailability of datasets. This motivated us to propose a\nnovel joint architecture of spatial and temporal network which extracts\ntime-contrasted features from the feature maps to contrast out micro-expression\nfrom rapid muscle movements. The usage of time contrasted features greatly\nimproved the spotting of micro-expression from inconspicuous facial movements.\nAlso, we include a memory module to predict the class and intensity of the\nmicro-expression across the temporal frames of the micro-expression clip. Our\nmethod achieves superior performance in comparison to other conventional\napproaches on CASMEII dataset.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 23:46:01 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 02:23:37 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Nag", "Sauradip", ""], ["Bhunia", "Ayan Kumar", ""], ["Konwer", "Aishik", ""], ["Roy", "Partha Pratim", ""]]}, {"id": "1902.03524", "submitter": "Stephen Balaban", "authors": "Stephen Balaban", "title": "Deep learning and face recognition: the state of the art", "comments": "Published May 15th 2015 in the Proc. SPIE 9457, Biometric and\n  Surveillance Technology for Human and Activity Identification XII, 94570B;\n  Ioannis A. Kakadiaris; Ajay Kumar; Walter J. Scheirer, Editor(s)", "journal-ref": "Proc. SPIE 9457, Biometric and Surveillance Technology for Human\n  and Activity Identification XII, 94570B (15 May 2015)", "doi": "10.1117/12.2181526", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have established themselves as a dominant\ntechnique in machine learning. DNNs have been top performers on a wide variety\nof tasks including image classification, speech recognition, and face\nrecognition. Convolutional neural networks (CNNs) have been used in nearly all\nof the top performing methods on the Labeled Faces in the Wild (LFW) dataset.\nIn this talk and accompanying paper, I attempt to provide a review and summary\nof the deep learning techniques used in the state-of-the-art. In addition, I\nhighlight the need for both larger and more challenging public datasets to\nbenchmark these systems. The high accuracy (99.63% for FaceNet at the time of\npublishing) and utilization of outside data (hundreds of millions of images in\nthe case of Google's FaceNet) suggest that current face verification benchmarks\nsuch as LFW may not be challenging enough, nor provide enough data, for current\ntechniques. There exist a variety of organizations with mobile photo sharing\napplications that would be capable of releasing a very large scale and highly\ndiverse dataset of facial images captured on mobile devices. Such an \"ImageNet\nfor Face Recognition\" would likely receive a warm welcome from researchers and\npractitioners alike.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 01:07:15 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Balaban", "Stephen", ""]]}, {"id": "1902.03565", "submitter": "Ran He", "authors": "Ran He, Jie Cao, Lingxiao Song, Zhenan Sun, Tieniu Tan", "title": "Cross-spectral Face Completion for NIR-VIS Heterogeneous Face\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near infrared-visible (NIR-VIS) heterogeneous face recognition refers to the\nprocess of matching NIR to VIS face images. Current heterogeneous methods try\nto extend VIS face recognition methods to the NIR spectrum by synthesizing VIS\nimages from NIR images. However, due to self-occlusion and sensing gap, NIR\nface images lose some visible lighting contents so that they are always\nincomplete compared to VIS face images. This paper models high resolution\nheterogeneous face synthesis as a complementary combination of two components,\na texture inpainting component and pose correction component. The inpainting\ncomponent synthesizes and inpaints VIS image textures from NIR image textures.\nThe correction component maps any pose in NIR images to a frontal pose in VIS\nimages, resulting in paired NIR and VIS textures. A warping procedure is\ndeveloped to integrate the two components into an end-to-end deep network. A\nfine-grained discriminator and a wavelet-based discriminator are designed to\nsupervise intra-class variance and visual quality respectively. One UV loss,\ntwo adversarial losses and one pixel loss are imposed to ensure synthesis\nresults. We demonstrate that by attaching the correction component, we can\nsimplify heterogeneous face synthesis from one-to-many unpaired image\ntranslation to one-to-one paired image translation, and minimize spectral and\npose discrepancy during heterogeneous recognition. Extensive experimental\nresults show that our network not only generates high-resolution VIS face\nimages and but also facilitates the accuracy improvement of heterogeneous face\nrecognition.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 10:20:38 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["He", "Ran", ""], ["Cao", "Jie", ""], ["Song", "Lingxiao", ""], ["Sun", "Zhenan", ""], ["Tan", "Tieniu", ""]]}, {"id": "1902.03570", "submitter": "Deshraj Yadav", "authors": "Deshraj Yadav, Rishabh Jain, Harsh Agrawal, Prithvijit Chattopadhyay,\n  Taranjeet Singh, Akash Jain, Shiv Baran Singh, Stefan Lee, Dhruv Batra", "title": "EvalAI: Towards Better Evaluation Systems for AI Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce EvalAI, an open source platform for evaluating and comparing\nmachine learning (ML) and artificial intelligence algorithms (AI) at scale.\nEvalAI is built to provide a scalable solution to the research community to\nfulfill the critical need of evaluating machine learning models and agents\nacting in an environment against annotations or with a human-in-the-loop. This\nwill help researchers, students, and data scientists to create, collaborate,\nand participate in AI challenges organized around the globe. By simplifying and\nstandardizing the process of benchmarking these models, EvalAI seeks to lower\nthe barrier to entry for participating in the global scientific effort to push\nthe frontiers of machine learning and artificial intelligence, thereby\nincreasing the rate of measurable progress in this domain.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 10:34:54 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Yadav", "Deshraj", ""], ["Jain", "Rishabh", ""], ["Agrawal", "Harsh", ""], ["Chattopadhyay", "Prithvijit", ""], ["Singh", "Taranjeet", ""], ["Jain", "Akash", ""], ["Singh", "Shiv Baran", ""], ["Lee", "Stefan", ""], ["Batra", "Dhruv", ""]]}, {"id": "1902.03582", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Xingzhi Yue, Neofytos Dimitriou, Ognjen Arandjelovic", "title": "Colorectal Cancer Outcome Prediction from H&E Whole Slide Images using\n  Machine Learning and Automatically Inferred Phenotype Profiles", "comments": "2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital pathology (DP) is a new research area which falls under the broad\numbrella of health informatics. Owing to its potential for major public health\nimpact, in recent years DP has been attracting much research attention.\nNevertheless, a wide breadth of significant conceptual and technical challenges\nremain, few of them greater than those encountered in the field of oncology.\nThe automatic analysis of digital pathology slides of cancerous tissues is\nparticularly problematic due to the inherent heterogeneity of the disease,\nextremely large images, amongst numerous others. In this paper we introduce a\nnovel machine learning based framework for the prediction of colorectal cancer\noutcome from whole digitized haematoxylin & eosin (H&E) stained histopathology\nslides. Using a real-world data set we demonstrate the effectiveness of the\nmethod and present a detailed analysis of its different elements which\ncorroborate its ability to extract and learn salient, discriminative, and\nclinically meaningful content.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 12:05:02 GMT"}, {"version": "v2", "created": "Sat, 9 Mar 2019 18:18:52 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Yue", "Xingzhi", ""], ["Dimitriou", "Neofytos", ""], ["Arandjelovic", "Ognjen", ""]]}, {"id": "1902.03585", "submitter": "Huazhu Fu", "authors": "Huazhu Fu and Yanwu Xu and Stephen Lin and Damon Wing Kee Wong and\n  Mani Baskaran and Meenakshi Mahesh and Tin Aung and Jiang Liu", "title": "Angle-Closure Detection in Anterior Segment OCT based on Multi-Level\n  Deep Network", "comments": "9 pages, accepted by IEEE Transactions on Cybernetics", "journal-ref": null, "doi": "10.1109/TCYB.2019.2897162", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Irreversible visual impairment is often caused by primary angle-closure\nglaucoma, which could be detected via Anterior Segment Optical Coherence\nTomography (AS-OCT). In this paper, an automated system based on deep learning\nis presented for angle-closure detection in AS-OCT images. Our system learns a\ndiscriminative representation from training data that captures subtle visual\ncues not modeled by handcrafted features. A Multi-Level Deep Network (MLDN) is\nproposed to formulate this learning, which utilizes three particular AS-OCT\nregions based on clinical priors: the global anterior segment structure, local\niris region, and anterior chamber angle (ACA) patch. In our method, a sliding\nwindow based detector is designed to localize the ACA region, which addresses\nACA detection as a regression task. Then, three parallel sub-networks are\napplied to extract AS-OCT representations for the global image and at\nclinically-relevant local regions. Finally, the extracted deep features of\nthese sub-networks are concatenated into one fully connected layer to predict\nthe angle-closure detection result. In the experiments, our system is shown to\nsurpass previous detection methods and other deep learning systems on two\nclinical AS-OCT datasets.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 12:28:52 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Fu", "Huazhu", ""], ["Xu", "Yanwu", ""], ["Lin", "Stephen", ""], ["Wong", "Damon Wing Kee", ""], ["Baskaran", "Mani", ""], ["Mahesh", "Meenakshi", ""], ["Aung", "Tin", ""], ["Liu", "Jiang", ""]]}, {"id": "1902.03589", "submitter": "Sumanth Chennupati", "authors": "Ganesh Sistu, Isabelle Leang, Sumanth Chennupati, Ciaran Hughes,\n  Stefan Milz, Senthil Yogamani and Samir Rawashdeh", "title": "NeurAll: Towards a Unified Model for Visual Perception in Automated\n  Driving", "comments": "Accepted for Oral Presentation at IEEE Intelligent Transportation\n  Systems Conference (ITSC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are successfully used for the important\nautomotive visual perception tasks including object recognition, motion and\ndepth estimation, visual SLAM, etc. However, these tasks are typically\nindependently explored and modeled. In this paper, we propose a joint\nmulti-task network design for learning several tasks simultaneously. Our main\nmotivation is the computational efficiency achieved by sharing the expensive\ninitial convolutional layers between all tasks. Indeed, the main bottleneck in\nautomated driving systems is the limited processing power available on\ndeployment hardware. There is also some evidence for other benefits in\nimproving accuracy for some tasks and easing development effort. It also offers\nscalability to add more tasks leveraging existing features and achieving better\ngeneralization. We survey various CNN based solutions for visual perception\ntasks in automated driving. Then we propose a unified CNN model for the\nimportant tasks and discuss several advanced optimization and architecture\ndesign techniques to improve the baseline model. The paper is partly review and\npartly positional with demonstration of several preliminary results promising\nfor future research. We first demonstrate results of multi-stream learning and\nauxiliary learning which are important ingredients to scale to a large\nmulti-task model. Finally, we implement a two-stream three-task network which\nperforms better in many cases compared to their corresponding single-task\nmodels, while maintaining network size.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 12:45:49 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 16:39:56 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Sistu", "Ganesh", ""], ["Leang", "Isabelle", ""], ["Chennupati", "Sumanth", ""], ["Hughes", "Ciaran", ""], ["Milz", "Stefan", ""], ["Yogamani", "Senthil", ""], ["Rawashdeh", "Samir", ""]]}, {"id": "1902.03601", "submitter": "Patrick Mannion", "authors": "Patrick Mannion", "title": "Vulnerable road user detection: state-of-the-art and open challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correctly identifying vulnerable road users (VRUs), e.g. cyclists and\npedestrians, remains one of the most challenging environment perception tasks\nfor autonomous vehicles (AVs). This work surveys the current state-of-the-art\nin VRU detection, covering topics such as benchmarks and datasets, object\ndetection techniques and relevant machine learning algorithms. The article\nconcludes with a discussion of remaining open challenges and promising future\nresearch directions for this domain.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 13:51:47 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Mannion", "Patrick", ""]]}, {"id": "1902.03604", "submitter": "Paul Voigtlaender", "authors": "Paul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon Luiten, Berin\n  Balachandar Gnana Sekar, Andreas Geiger, Bastian Leibe", "title": "MOTS: Multi-Object Tracking and Segmentation", "comments": "CVPR 2019 camera-ready version", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n  2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the popular task of multi-object tracking to multi-object\ntracking and segmentation (MOTS). Towards this goal, we create dense\npixel-level annotations for two existing tracking datasets using a\nsemi-automatic annotation procedure. Our new annotations comprise 65,213 pixel\nmasks for 977 distinct objects (cars and pedestrians) in 10,870 video frames.\nFor evaluation, we extend existing multi-object tracking metrics to this new\ntask. Moreover, we propose a new baseline method which jointly addresses\ndetection, tracking, and segmentation with a single convolutional network. We\ndemonstrate the value of our datasets by achieving improvements in performance\nwhen training on MOTS annotations. We believe that our datasets, metrics and\nbaseline will become a valuable resource towards developing multi-object\ntracking approaches that go beyond 2D bounding boxes. We make our annotations,\ncode, and models available at https://www.vision.rwth-aachen.de/page/mots.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 14:01:22 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 15:01:48 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Voigtlaender", "Paul", ""], ["Krause", "Michael", ""], ["Osep", "Aljosa", ""], ["Luiten", "Jonathon", ""], ["Sekar", "Berin Balachandar Gnana", ""], ["Geiger", "Andreas", ""], ["Leibe", "Bastian", ""]]}, {"id": "1902.03618", "submitter": "Nils Gessert", "authors": "Nils Gessert and Matthias Schl\\\"uter and Sarah Latus and Veronika\n  Volgger and Christian Betz and Alexander Schlaefer", "title": "Towards Automatic Lesion Classification in the Upper Aerodigestive Tract\n  Using OCT and Deep Transfer Learning Methods", "comments": "Accepted for publication at CARS 2019", "journal-ref": null, "doi": "10.1007/s11548-019-01969-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of cancer is crucial for treatment and overall patient\nsurvival. In the upper aerodigestive tract (UADT) the gold standard for\nidentification of malignant tissue is an invasive biopsy. Recently,\nnon-invasive imaging techniques such as confocal laser microscopy and optical\ncoherence tomography (OCT) have been used for tissue assessment. In particular,\nin a recent study experts classified lesions in the UADT with respect to their\ninvasiveness using OCT images only. As the results were promising, automatic\nclassification of lesions might be feasible which could assist experts in their\ndecision making. Therefore, we address the problem of automatic lesion\nclassification from OCT images. This task is very challenging as the available\ndataset is extremely small and the data quality is limited. However, as similar\nissues are typical in many clinical scenarios we study to what extent deep\nlearning approaches can still be trained and used for decision support.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 15:14:43 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Gessert", "Nils", ""], ["Schl\u00fcter", "Matthias", ""], ["Latus", "Sarah", ""], ["Volgger", "Veronika", ""], ["Betz", "Christian", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "1902.03619", "submitter": "Victoria Fernandez Abrevaya", "authors": "Victoria Fernandez Abrevaya, Adnane Boukhayma, Stefanie Wuhrer, Edmond\n  Boyer", "title": "A Decoupled 3D Facial Shape Model by Adversarial Training", "comments": "camera-ready version for ICCV'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven generative 3D face models are used to compactly encode facial\nshape data into meaningful parametric representations. A desirable property of\nthese models is their ability to effectively decouple natural sources of\nvariation, in particular identity and expression. While factorized\nrepresentations have been proposed for that purpose, they are still limited in\nthe variability they can capture and may present modeling artifacts when\napplied to tasks such as expression transfer. In this work, we explore a new\ndirection with Generative Adversarial Networks and show that they contribute to\nbetter face modeling performances, especially in decoupling natural factors,\nwhile also achieving more diverse samples. To train the model we introduce a\nnovel architecture that combines a 3D generator with a 2D discriminator that\nleverages conventional CNNs, where the two components are bridged by a geometry\nmapping layer. We further present a training scheme, based on auxiliary\nclassifiers, to explicitly disentangle identity and expression attributes.\nThrough quantitative and qualitative results on standard face datasets, we\nillustrate the benefits of our model and demonstrate that it outperforms\ncompeting state of the art methods in terms of decoupling and diversity.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 15:15:44 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 14:02:33 GMT"}, {"version": "v3", "created": "Sat, 7 Sep 2019 17:19:26 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Abrevaya", "Victoria Fernandez", ""], ["Boukhayma", "Adnane", ""], ["Wuhrer", "Stefanie", ""], ["Boyer", "Edmond", ""]]}, {"id": "1902.03634", "submitter": "John See", "authors": "Sze-Teng Liong and Y.S. Gan and John See and Huai-Qian Khor and\n  Yen-Chang Huang", "title": "Shallow Triple Stream Three-dimensional CNN (STSTNet) for\n  Micro-expression Recognition", "comments": "5 pages, 1 figure, Accepted and published in IEEE FG 2019", "journal-ref": null, "doi": "10.1109/FG.2019.8756567", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent year, state-of-the-art for facial micro-expression recognition\nhave been significantly advanced by deep neural networks. The robustness of\ndeep learning has yielded promising performance beyond that of traditional\nhandcrafted approaches. Most works in literature emphasized on increasing the\ndepth of networks and employing highly complex objective functions to learn\nmore features. In this paper, we design a Shallow Triple Stream\nThree-dimensional CNN (STSTNet) that is computationally light whilst capable of\nextracting discriminative high level features and details of micro-expressions.\nThe network learns from three optical flow features (i.e., optical strain,\nhorizontal and vertical optical flow fields) computed based on the onset and\napex frames of each video. Our experimental results demonstrate the\neffectiveness of the proposed STSTNet, which obtained an unweighted average\nrecall rate of 0.7605 and unweighted F1-score of 0.7353 on the composite\ndatabase consisting of 442 samples from the SMIC, CASME II and SAMM databases.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 17:26:39 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 10:45:38 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Liong", "Sze-Teng", ""], ["Gan", "Y. S.", ""], ["See", "John", ""], ["Khor", "Huai-Qian", ""], ["Huang", "Yen-Chang", ""]]}, {"id": "1902.03646", "submitter": "David V\\'azquez", "authors": "Guillem Cucurull, Perouz Taslakian, David Vazquez", "title": "Context-Aware Visual Compatibility Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do we determine whether two or more clothing items are compatible or\nvisually appealing? Part of the answer lies in understanding of visual\naesthetics, and is biased by personal preferences shaped by social attitudes,\ntime, and place. In this work we propose a method that predicts compatibility\nbetween two items based on their visual features, as well as their context. We\ndefine context as the products that are known to be compatible with each of\nthese item. Our model is in contrast to other metric learning approaches that\nrely on pairwise comparisons between item features alone. We address the\ncompatibility prediction problem using a graph neural network that learns to\ngenerate product embeddings conditioned on their context. We present results\nfor two prediction tasks (fill in the blank and outfit compatibility) tested on\ntwo fashion datasets Polyvore and Fashion-Gen, and on a subset of the Amazon\ndataset; we achieve state of the art results when using context information and\nshow how test performance improves as more context is used.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 18:14:15 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 14:17:03 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Cucurull", "Guillem", ""], ["Taslakian", "Perouz", ""], ["Vazquez", "David", ""]]}, {"id": "1902.03680", "submitter": "Ryutaro Tanno", "authors": "Ryutaro Tanno, Ardavan Saeedi, Swami Sankaranarayanan, Daniel C.\n  Alexander, Nathan Silberman", "title": "Learning From Noisy Labels By Regularized Estimation Of Annotator\n  Confusion", "comments": "CVPR 2019, code snippets included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predictive performance of supervised learning algorithms depends on the\nquality of labels. In a typical label collection process, multiple annotators\nprovide subjective noisy estimates of the \"truth\" under the influence of their\nvarying skill-levels and biases. Blindly treating these noisy labels as the\nground truth limits the accuracy of learning algorithms in the presence of\nstrong disagreement. This problem is critical for applications in domains such\nas medical imaging where both the annotation cost and inter-observer\nvariability are high. In this work, we present a method for simultaneously\nlearning the individual annotator model and the underlying true label\ndistribution, using only noisy observations. Each annotator is modeled by a\nconfusion matrix that is jointly estimated along with the classifier\npredictions. We propose to add a regularization term to the loss function that\nencourages convergence to the true annotator confusion matrix. We provide a\ntheoretical argument as to how the regularization is essential to our approach\nboth for the case of single annotator and multiple annotators. Despite the\nsimplicity of the idea, experiments on image classification tasks with both\nsimulated and real labels show that our method either outperforms or performs\non par with the state-of-the-art methods and is capable of estimating the\nskills of annotators even with a single label available per image.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 23:01:33 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 12:14:05 GMT"}, {"version": "v3", "created": "Mon, 17 Jun 2019 07:34:07 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Tanno", "Ryutaro", ""], ["Saeedi", "Ardavan", ""], ["Sankaranarayanan", "Swami", ""], ["Alexander", "Daniel C.", ""], ["Silberman", "Nathan", ""]]}, {"id": "1902.03682", "submitter": "Kenneth Vecchio PhD", "authors": "Kevin Kaufmann, Chaoyi Zhu, Alexander S. Rosengarten, Daniel\n  Maryanovsky, Tyler J. Harrington, Eduardo Marin, and Kenneth S. Vecchio", "title": "Paradigm shift in electron-based crystallography via machine learning", "comments": "35 pages, 17 figures with extended data included", "journal-ref": "Science 367 (2020) 564-568", "doi": "10.1126/science.aay3062", "report-no": null, "categories": "cond-mat.mtrl-sci cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately determining the crystallographic structure of a material, organic\nor inorganic, is a critical primary step in material development and analysis.\nThe most common practices involve analysis of diffraction patterns produced in\nlaboratory XRD, TEM, and synchrotron X-ray sources. However, these techniques\nare slow, require careful sample preparation, can be difficult to access, and\nare prone to human error during analysis. This paper presents a newly developed\nmethodology that represents a paradigm change in electron diffraction-based\nstructure analysis techniques, with the potential to revolutionize multiple\ncrystallography-related fields. A machine learning-based approach for rapid and\nautonomous identification of the crystal structure of metals and alloys,\nceramics, and geological specimens, without any prior knowledge of the sample,\nis presented and demonstrated utilizing the electron backscatter diffraction\n(EBSD) technique. Electron backscatter diffraction patterns are collected from\nmaterials with well-known crystal structures, then a deep neural network model\nis constructed for classification to a specific Bravais lattice or point group.\nThe applicability of this approach is evaluated on diffraction patterns from\nsamples unknown to the computer without any human input or data filtering. This\nis in comparison to traditional Hough transform EBSD, which requires that you\nhave already determined the phases present in your sample. The internal\noperations of the neural network are elucidated through visualizing the\nsymmetry features learned by the convolutional neural network. It is determined\nthat the model looks for the same features a crystallographer would use, even\nthough it is not explicitly programmed to do so. This study opens the door to\nfully automated, high-throughput determination of crystal structures via\nseveral electron-based diffraction techniques.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 23:16:41 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Kaufmann", "Kevin", ""], ["Zhu", "Chaoyi", ""], ["Rosengarten", "Alexander S.", ""], ["Maryanovsky", "Daniel", ""], ["Harrington", "Tyler J.", ""], ["Marin", "Eduardo", ""], ["Vecchio", "Kenneth S.", ""]]}, {"id": "1902.03715", "submitter": "Achal Dave", "authors": "Achal Dave, Pavel Tokmakov, Deva Ramanan", "title": "Towards Segmenting Anything That Moves", "comments": "Website: http://www.achaldave.com/projects/anything-that-moves/.\n  Code: https://github.com/achalddave/segment-any-moving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and segmenting individual objects, regardless of their category, is\ncrucial for many applications such as action detection or robotic interaction.\nWhile this problem has been well-studied under the classic formulation of\nspatio-temporal grouping, state-of-the-art approaches do not make use of\nlearning-based methods. To bridge this gap, we propose a simple learning-based\napproach for spatio-temporal grouping. Our approach leverages motion cues from\noptical flow as a bottom-up signal for separating objects from each other.\nMotion cues are then combined with appearance cues that provide a generic\nobjectness prior for capturing the full extent of objects. We show that our\napproach outperforms all prior work on the benchmark FBMS dataset. One\npotential worry with learning-based methods is that they might overfit to the\nparticular type of objects that they have been trained on. To address this\nconcern, we propose two new benchmarks for generic, moving object detection,\nand show that our model matches top-down methods on common categories, while\nsignificantly out-performing both top-down and bottom-up methods on\nnever-before-seen categories.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 03:40:48 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 20:18:11 GMT"}, {"version": "v3", "created": "Tue, 10 Sep 2019 21:15:14 GMT"}, {"version": "v4", "created": "Wed, 1 Apr 2020 01:19:41 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Dave", "Achal", ""], ["Tokmakov", "Pavel", ""], ["Ramanan", "Deva", ""]]}, {"id": "1902.03729", "submitter": "Rafael Munoz-Salinas", "authors": "Rafael Munoz-Salinas, Rafael Medina-Carnicer", "title": "UcoSLAM: Simultaneous Localization and Mapping by Fusion of KeyPoints\n  and Squared Planar Markers", "comments": "Paper submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach for Simultaneous Localization and\nMapping by fusing natural and artificial landmarks. Most of the SLAM approaches\nuse natural landmarks (such as keypoints). However, they are unstable over\ntime, repetitive in many cases or insufficient for a robust tracking (e.g. in\nindoor buildings). On the other hand, other approaches have employed artificial\nlandmarks (such as squared fiducial markers) placed in the environment to help\ntracking and relocalization. We propose a method that integrates both\napproaches in order to achieve long-term robust tracking in many scenarios.\n  Our method has been compared to the start-of-the-art methods ORB-SLAM2 and\nLDSO in the public dataset Kitti, Euroc-MAV, TUM and SPM, obtaining better\nprecision, robustness and speed. Our tests also show that the combination of\nmarkers and keypoints achieves better accuracy than each one of them\nindependently.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 04:48:38 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Munoz-Salinas", "Rafael", ""], ["Medina-Carnicer", "Rafael", ""]]}, {"id": "1902.03739", "submitter": "Nasim Nematzadeh", "authors": "Nasim Nematzadeh, David M. W. Powers", "title": "Prediction of Dashed Caf\\'e Wall illusion by the Classical Receptive\n  Field Model", "comments": "6 pages, 6 figures. Accepted in ICECCE2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The Caf\\'e Wall illusion is one of a class of tilt illusions where lines that\nare parallel appear to be tilted. We demonstrate that a simple Differences of\nGaussian model provides an explanatory mechanism for the illusory tilt\nperceived in a family of Caf\\'e Wall illusion generalizes to the dashed\nversions of Caf\\'e Wall. Our explanation models the visual mechanisms in low\nlevel stages and the lateral inhibition of simple cells that can reveal tilt\ncues in Geometrical distortion illusions such as Tile illusions particularly\nCaf\\'e Wall illusions. For this, we simulate the activations of the\nretinal/cortical simple cells in responses to these patterns based on a\nClassical Receptive Field (CRF) model (referred to as Vis-CRF) to explain tilt\neffects in these illusions. Previously, it was assumed that all these visual\nexperiences of tilt arise from the orientation selectivity properties described\nfor more complex cortical cells. An estimation of an overall tilt angle\nperceived in these illusions is based on the integration of the local tilts\ndetected by simple cells which is presumed to be a key mechanism utilized by\nthe complex cells to create our final perception of tilt.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 03:23:22 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 07:59:53 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Nematzadeh", "Nasim", ""], ["Powers", "David M. W.", ""]]}, {"id": "1902.03747", "submitter": "\\'Alvaro Parra", "authors": "\\'Alvaro Parra, Tat-Jun Chin, Anders Eriksson, Ian Reid", "title": "Visual SLAM: Why Bundle Adjust?", "comments": "Accepted to ICRA 2019", "journal-ref": null, "doi": "10.1109/ICRA.2019.8793749", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bundle adjustment plays a vital role in feature-based monocular SLAM. In many\nmodern SLAM pipelines, bundle adjustment is performed to estimate the 6DOF\ncamera trajectory and 3D map (3D point cloud) from the input feature tracks.\nHowever, two fundamental weaknesses plague SLAM systems based on bundle\nadjustment. First, the need to carefully initialise bundle adjustment means\nthat all variables, in particular the map, must be estimated as accurately as\npossible and maintained over time, which makes the overall algorithm\ncumbersome. Second, since estimating the 3D structure (which requires\nsufficient baseline) is inherent in bundle adjustment, the SLAM algorithm will\nencounter difficulties during periods of slow motion or pure rotational motion.\n  We propose a different SLAM optimisation core: instead of bundle adjustment,\nwe conduct rotation averaging to incrementally optimise only camera\norientations. Given the orientations, we estimate the camera positions and 3D\npoints via a quasi-convex formulation that can be solved efficiently and\nglobally optimally. Our approach not only obviates the need to estimate and\nmaintain the positions and 3D map at keyframe rate (which enables simpler SLAM\nsystems), it is also more capable of handling slow motions or pure rotational\nmotions.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 06:58:38 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 03:28:08 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Parra", "\u00c1lvaro", ""], ["Chin", "Tat-Jun", ""], ["Eriksson", "Anders", ""], ["Reid", "Ian", ""]]}, {"id": "1902.03748", "submitter": "Junwei Liang", "authors": "Junwei Liang, Lu Jiang, Juan Carlos Niebles, Alexander Hauptmann, Li\n  Fei-Fei", "title": "Peeking into the Future: Predicting Future Person Activities and\n  Locations in Videos", "comments": "In CVPR 2019. Code, models and more results are available at:\n  https://next.cs.cmu.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deciphering human behaviors to predict their future paths/trajectories and\nwhat they would do from videos is important in many applications. Motivated by\nthis idea, this paper studies predicting a pedestrian's future path jointly\nwith future activities. We propose an end-to-end, multi-task learning system\nutilizing rich visual features about human behavioral information and\ninteraction with their surroundings. To facilitate the training, the network is\nlearned with an auxiliary task of predicting future location in which the\nactivity will happen. Experimental results demonstrate our state-of-the-art\nperformance over two public benchmarks on future trajectory prediction.\nMoreover, our method is able to produce meaningful future activity prediction\nin addition to the path. The result provides the first empirical evidence that\njoint modeling of paths and activities benefits future path prediction.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 07:02:18 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 21:12:59 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 22:53:41 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Liang", "Junwei", ""], ["Jiang", "Lu", ""], ["Niebles", "Juan Carlos", ""], ["Hauptmann", "Alexander", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1902.03751", "submitter": "Ramprasaath R. Selvaraju", "authors": "Ramprasaath R. Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini\n  Ghosh, Larry Heck, Dhruv Batra, Devi Parikh", "title": "Taking a HINT: Leveraging Explanations to Make Vision and Language\n  Models More Grounded", "comments": "Published at ICCV'2019", "journal-ref": "The IEEE International Conference on Computer Vision (ICCV) 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many vision and language models suffer from poor visual grounding - often\nfalling back on easy-to-learn language priors rather than basing their\ndecisions on visual concepts in the image. In this work, we propose a generic\napproach called Human Importance-aware Network Tuning (HINT) that effectively\nleverages human demonstrations to improve visual grounding. HINT encourages\ndeep networks to be sensitive to the same input regions as humans. Our approach\noptimizes the alignment between human attention maps and gradient-based network\nimportances - ensuring that models learn not just to look at but rather rely on\nvisual concepts that humans found relevant for a task when making predictions.\nWe apply HINT to Visual Question Answering and Image Captioning tasks,\noutperforming top approaches on splits that penalize over-reliance on language\npriors (VQA-CP and robust captioning) using human attention demonstrations for\njust 6% of the training data.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 07:28:03 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 10:30:28 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Selvaraju", "Ramprasaath R.", ""], ["Lee", "Stefan", ""], ["Shen", "Yilin", ""], ["Jin", "Hongxia", ""], ["Ghosh", "Shalini", ""], ["Heck", "Larry", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1902.03765", "submitter": "Patrick Wenzel", "authors": "Qadeer Khan, Torsten Sch\\\"on, Patrick Wenzel", "title": "Latent Space Reinforcement Learning for Steering Angle Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-free reinforcement learning has recently been shown to successfully\nlearn navigation policies from raw sensor data. In this work, we address the\nproblem of learning driving policies for an autonomous agent in a high-fidelity\nsimulator. Building upon recent research that applies deep reinforcement\nlearning to navigation problems, we present a modular deep reinforcement\nlearning approach to predict the steering angle of the car from raw images. The\nfirst module extracts a low-dimensional latent semantic representation of the\nimage. The control module trained with reinforcement learning takes the latent\nvector as input to predict the correct steering angle. The experimental results\nhave showed that our method is capable of learning to maneuver the car without\nany human control signals.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 08:14:34 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Khan", "Qadeer", ""], ["Sch\u00f6n", "Torsten", ""], ["Wenzel", "Patrick", ""]]}, {"id": "1902.03771", "submitter": "Xin Jin", "authors": "Jin Xin, Wang Yuhui, Tan Xiaoyang", "title": "Pornographic Image Recognition via Weighted Multiple Instance Learning", "comments": "9 pages, 3 figures", "journal-ref": "IEEE transactions on cybernetics, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of Internet, recognizing pornographic images is of great\nsignificance for protecting children's physical and mental health. However,\nthis task is very challenging as the key pornographic contents (e.g., breast\nand private part) in an image often lie in local regions of small size. In this\npaper, we model each image as a bag of regions, and follow a multiple instance\nlearning (MIL) approach to train a generic region-based recognition model.\nSpecifically, we take into account the region's degree of pornography, and make\nthree main contributions. First, we show that based on very few annotations of\nthe key pornographic contents in a training image, we can generate a bag of\nproperly sized regions, among which the potential positive regions usually\ncontain useful contexts that can aid recognition. Second, we present a simple\nquantitative measure of a region's degree of pornography, which can be used to\nweigh the importance of different regions in a positive image. Third, we\nformulate the recognition task as a weighted MIL problem under the\nconvolutional neural network framework, with a bag probability function\nintroduced to combine the importance of different regions. Experiments on our\nnewly collected large scale dataset demonstrate the effectiveness of the\nproposed method, achieving an accuracy with 97.52% true positive rate at 1%\nfalse positive rate, tested on 100K pornographic images and 100K normal images.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 08:24:53 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Xin", "Jin", ""], ["Yuhui", "Wang", ""], ["Xiaoyang", "Tan", ""]]}, {"id": "1902.03777", "submitter": "Patrick Wenzel", "authors": "Qadeer Khan, Torsten Sch\\\"on, Patrick Wenzel", "title": "Semantic Label Reduction Techniques for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation maps can be used as input to models for maneuvering the\ncontrols of a car. However, not all labels may be necessary for making the\ncontrol decision. One would expect that certain labels such as road lanes or\nsidewalks would be more critical in comparison with labels for vegetation or\nbuildings which may not have a direct influence on the car's driving decision.\nIn this appendix, we evaluate and quantify how sensitive and important the\ndifferent semantic labels are for controlling the car. Labels that do not\ninfluence the driving decision are remapped to other classes, thereby\nsimplifying the task by reducing to only labels critical for driving of the\nvehicle.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 08:38:26 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Khan", "Qadeer", ""], ["Sch\u00f6n", "Torsten", ""], ["Wenzel", "Patrick", ""]]}, {"id": "1902.03782", "submitter": "Jianxin Lin", "authors": "Jianxin Lin, Zhibo Chen, Yingce Xia, Sen Liu, Tao Qin, Jiebo Luo", "title": "Exploring Explicit Domain Supervision for Latent Space Disentanglement\n  in Unpaired Image-to-Image Translation", "comments": "Accepted by IEEE Transaction on Pattern Analysis and Machine\n  Intelligence (TPAMI).13 pages, 11 figures, 7 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation tasks have been widely investigated with\nGenerative Adversarial Networks (GANs). However, existing approaches are mostly\ndesigned in an unsupervised manner while little attention has been paid to\ndomain information within unpaired data. In this paper, we treat domain\ninformation as explicit supervision and design an unpaired image-to-image\ntranslation framework, Domain-supervised GAN (DosGAN), which takes the first\nstep towards the exploration of explicit domain supervision. In contrast to\nrepresenting domain characteristics using different generators or domain codes,\nwe pre-train a classification network to explicitly classify the domain of an\nimage. After pre-training, this network is used to extract the domain-specific\nfeatures of each image. Such features, together with the domain-independent\nfeatures extracted by another encoder (shared across different domains), are\nused to generate image in target domain. Extensive experiments on multiple\nfacial attribute translation, multiple identity translation, multiple season\ntranslation and conditional edges-to-shoes/handbags demonstrate the\neffectiveness of our method. In addition, we can transfer the domain-specific\nfeature extractor obtained on the Facescrub dataset with domain supervision\ninformation to unseen domains, such as faces in the CelebA dataset. We also\nsucceed in achieving conditional translation with any two images in CelebA,\nwhile previous models like StarGAN cannot handle this task.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 09:07:30 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 03:22:18 GMT"}, {"version": "v3", "created": "Tue, 1 Oct 2019 02:54:46 GMT"}, {"version": "v4", "created": "Tue, 22 Oct 2019 08:24:52 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Lin", "Jianxin", ""], ["Chen", "Zhibo", ""], ["Xia", "Yingce", ""], ["Liu", "Sen", ""], ["Qin", "Tao", ""], ["Luo", "Jiebo", ""]]}, {"id": "1902.03791", "submitter": "Suryansh Kumar", "authors": "Suryansh Kumar, Ram Srivatsav Ghorakavi, Yuchao Dai, Hongdong Li", "title": "Dense Depth Estimation of a Complex Dynamic Scene without Explicit 3D\n  Motion Estimation", "comments": "Extension to our ICCV'17 work. 10 pages, 10 Figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent geometric methods need reliable estimates of 3D motion parameters to\nprocure accurate dense depth map of a complex dynamic scene from monocular\nimages \\cite{kumar2017monocular, ranftl2016dense}. Generally, to estimate\n\\textbf{precise} measurements of relative 3D motion parameters and to validate\nits accuracy using image data is a challenging task. In this work, we propose\nan alternative approach that circumvents the 3D motion estimation requirement\nto obtain a dense depth map of a dynamic scene. Given per-pixel optical flow\ncorrespondences between two consecutive frames and, the sparse depth prior for\nthe reference frame, we show that, we can effectively recover the dense depth\nmap for the successive frames without solving for 3D motion parameters. Our\nmethod assumes a piece-wise planar model of a dynamic scene, which undergoes\nrigid transformation locally, and as-rigid-as-possible transformation globally\nbetween two successive frames. Under our assumption, we can avoid the explicit\nestimation of 3D rotation and translation to estimate scene depth. In essence,\nour formulation provides an unconventional way to think and recover the dense\ndepth map of a complex dynamic scene which is incremental and motion free in\nnature. Our proposed method does not make object level or any other high-level\nprior assumption about the dynamic scene, as a result, it is applicable to a\nwide range of scenarios. Experimental results on the benchmarks dataset show\nthe competence of our approach for multiple frames.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 09:45:52 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 10:15:21 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Kumar", "Suryansh", ""], ["Ghorakavi", "Ram Srivatsav", ""], ["Dai", "Yuchao", ""], ["Li", "Hongdong", ""]]}, {"id": "1902.03804", "submitter": "Claudio Ferrari", "authors": "Claudio Ferrari, Stefano Berretti, Alberto Del Bimbo", "title": "Additional Baseline Metrics for the paper \"Extended YouTube Faces: a\n  Dataset for Heterogeneous Open-Set Face Identification\"", "comments": "3 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we provide additional and corrected results for the paper\n\"Extended YouTube Faces: a Dataset for Heterogeneous Open-Set Face\nIdentification\". After further investigations, we discovered and corrected\nwrongly labeled images and incorrect identities. This forced us to re-generate\nthe evaluation protocol for the new data; in doing so, we also reproduced and\nextended the experimental results with other standard metrics and measures used\nin the literature. The reader can refer to the original paper for additional\ndetails regarding the data collection procedure and recognition pipeline.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 10:26:21 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Ferrari", "Claudio", ""], ["Berretti", "Stefano", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1902.03817", "submitter": "Grigorios Kalliatakis", "authors": "Grigorios Kalliatakis, Shoaib Ehsan, Maria Fasli and Klaus D.\n  McDonald-Maier", "title": "GET-AID: Visual Recognition of Human Rights Abuses via Global Emotional\n  Traits", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of social media and big data, the use of visual evidence to\ndocument conflict and human rights abuse has become an important element for\nhuman rights organizations and advocates. In this paper, we address the task of\ndetecting two types of human rights abuses in challenging, everyday photos: (1)\nchild labour, and (2) displaced populations. We propose a novel model that is\ndriven by a human-centric approach. Our hypothesis is that the emotional state\nof a person -- how positive or pleasant an emotion is, and the control level of\nthe situation by the person -- are powerful cues for perceiving potential human\nrights violations. To exploit these cues, our model learns to predict global\nemotional traits over a given image based on the joint analysis of every\ndetected person and the whole scene. By integrating these predictions with a\ndata-driven convolutional neural network (CNN) classifier, our system\nefficiently infers potential human rights abuses in a clean, end-to-end system\nwe call GET-AID (from Global Emotional Traits for Abuse IDentification).\nExtensive experiments are performed to verify our method on the recently\nintroduced subset of Human Rights Archive (HRA) dataset (2 violation categories\nwith the same number of positive and negative samples), where we show\nquantitatively compelling results. Compared with previous works and the sole\nuse of a CNN classifier, this paper improves the coverage up to 23.73% for\nchild labour and 57.21% for displaced populations. Our dataset, codes and\ntrained models are available online at https://github.com/GKalliatakis/GET-AID.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 11:15:09 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Kalliatakis", "Grigorios", ""], ["Ehsan", "Shoaib", ""], ["Fasli", "Maria", ""], ["McDonald-Maier", "Klaus D.", ""]]}, {"id": "1902.03830", "submitter": "Saurabh Saini", "authors": "Saurabh Saini, P. J. Narayanan", "title": "Semantic Hierarchical Priors for Intrinsic Image Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsic Image Decomposition (IID) is a challenging and interesting computer\nvision problem with various applications in several fields. We present novel\nsemantic priors and an integrated approach for single image IID that involves\nanalyzing image at three hierarchical context levels. Local context priors\ncapture scene properties at each pixel within a small neighbourhood. Mid-level\ncontext priors encode object level semantics. Global context priors establish\ncorrespondences at the scene level. Our semantic priors are designed on both\nfixed and flexible regions, using selective search method and Convolutional\nNeural Network features. Our IID method is an iterative multistage optimization\nscheme and consists of two complementary formulations: $L_2$ smoothing for\nshading and $L_1$ sparsity for reflectance. Experiments and analysis of our\nmethod indicate the utility of our semantic priors and structured hierarchical\nanalysis in an IID framework. We compare our method with other contemporary IID\nsolutions and show results with lesser artifacts. Finally, we highlight that\nproper choice and encoding of prior knowledge can produce competitive results\neven when compared to end-to-end deep learning IID methods, signifying the\nimportance of such priors. We believe that the insights and techniques\npresented in this paper would be useful in the future IID research.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 11:46:52 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 09:35:34 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Saini", "Saurabh", ""], ["Narayanan", "P. J.", ""]]}, {"id": "1902.03842", "submitter": "Ramon Campos", "authors": "Ramon Giostri Campos and Evandro Ottoni Teatini Salles", "title": "Robust statistics and no-reference image quality assessment in Curvelet\n  domain", "comments": "Article published in the XIV Workshop of Computer Vision, ISBN:\n  978-85-7455-514-0, this version has 7 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper uses robust statistics and curvelet transform to learn a\ngeneral-purpose no-reference (NR) image quality assessment (IQA) model. The new\napproach, here called M1, competes with the Curvelet Quality Assessment\nproposed in 2014 (Curvelet2014). The central idea is to use descriptors based\non robust statistics to extract features and predict the human opinion about\ndegraded images. To show the consistency of the method the model is tested with\n3 different datasets, LIVE IQA, TID2013 and CSIQ. To test evaluation, it is\nused the Wilcoxon test to verify the statistical significance of results and\npromote an accurate comparison between new model M1 and Curvelet2014. The\nresults show a gain when robust statistics are used as descriptor.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 12:30:01 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Campos", "Ramon Giostri", ""], ["Salles", "Evandro Ottoni Teatini", ""]]}, {"id": "1902.03871", "submitter": "Ruiqi Gao", "authors": "Ruiqi Gao, Jianwen Xie, Siyuan Huang, Yufan Ren, Song-Chun Zhu and\n  Ying Nian Wu", "title": "Learning vector representation of local content and matrix\n  representation of local motion, with implications for V1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a representational model for image pair such as\nconsecutive video frames that are related by local pixel displacements, in the\nhope that the model may shed light on motion perception in primary visual\ncortex (V1). The model couples the following two components. (1) The vector\nrepresentations of local contents of images. (2) The matrix representations of\nlocal pixel displacements caused by the relative motions between the agent and\nthe objects in the 3D scene. When the image frame undergoes changes due to\nlocal pixel displacements, the vectors are multiplied by the matrices that\nrepresent the local displacements. Our experiments show that our model can\nlearn to infer local motions. Moreover, the model can learn Gabor-like filter\npairs of quadrature phases.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 08:09:19 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 23:22:38 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 05:51:22 GMT"}, {"version": "v4", "created": "Sun, 1 Dec 2019 09:18:13 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Gao", "Ruiqi", ""], ["Xie", "Jianwen", ""], ["Huang", "Siyuan", ""], ["Ren", "Yufan", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1902.03932", "submitter": "Andrew Wilson", "authors": "Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, Andrew Gordon\n  Wilson", "title": "Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning", "comments": "Published at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The posteriors over neural network weights are high dimensional and\nmultimodal. Each mode typically characterizes a meaningfully different\nrepresentation of the data. We develop Cyclical Stochastic Gradient MCMC\n(SG-MCMC) to automatically explore such distributions. In particular, we\npropose a cyclical stepsize schedule, where larger steps discover new modes,\nand smaller steps characterize each mode. We also prove non-asymptotic\nconvergence of our proposed algorithm. Moreover, we provide extensive\nexperimental results, including ImageNet, to demonstrate the scalability and\neffectiveness of cyclical SG-MCMC in learning complex multimodal distributions,\nespecially for fully Bayesian inference with modern deep neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 15:03:30 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 20:49:28 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Zhang", "Ruqi", ""], ["Li", "Chunyuan", ""], ["Zhang", "Jianyi", ""], ["Chen", "Changyou", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1902.03938", "submitter": "Sanghyeon Na", "authors": "Sanghyeon Na, Seungjoo Yoo, Jaegul Choo", "title": "MISO: Mutual Information Loss with Stochastic Style Representations for\n  Multimodal Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unpaired multimodal image-to-image translation is a task of translating a\ngiven image in a source domain into diverse images in the target domain,\novercoming the limitation of one-to-one mapping. Existing multimodal\ntranslation models are mainly based on the disentangled representations with an\nimage reconstruction loss. We propose two approaches to improve multimodal\ntranslation quality. First, we use a content representation from the source\ndomain conditioned on a style representation from the target domain. Second,\nrather than using a typical image reconstruction loss, we design MILO (Mutual\nInformation LOss), a new stochastically-defined loss function based on\ninformation theory. This loss function directly reflects the interpretation of\nlatent variables as a random variable. We show that our proposed model Mutual\nInformation with StOchastic Style Representation(MISO) achieves\nstate-of-the-art performance through extensive experiments on various\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 15:15:45 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Na", "Sanghyeon", ""], ["Yoo", "Seungjoo", ""], ["Choo", "Jaegul", ""]]}, {"id": "1902.03954", "submitter": "Kong Zhaoming", "authors": "Zhaoming Kong and Xiaowei Yang", "title": "Color Image and Multispectral Image Denoising Using Block Diagonal\n  Representation", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2907478", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filtering images of more than one channel is challenging in terms of both\nefficiency and effectiveness. By grouping similar patches to utilize the\nself-similarity and sparse linear approximation of natural images, recent\nnonlocal and transform-domain methods have been widely used in color and\nmultispectral image (MSI) denoising. Many related methods focus on the modeling\nof group level correlation to enhance sparsity, which often resorts to a\nrecursive strategy with a large number of similar patches. The importance of\nthe patch level representation is understated. In this paper, we mainly\ninvestigate the influence and potential of representation at patch level by\nconsidering a general formulation with block diagonal matrix. We further show\nthat by training a proper global patch basis, along with a local principal\ncomponent analysis transform in the grouping dimension, a simple\ntransform-threshold-inverse method could produce very competitive results. Fast\nimplementation is also developed to reduce computational complexity. Extensive\nexperiments on both simulated and real datasets demonstrate its robustness,\neffectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 15:50:10 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Kong", "Zhaoming", ""], ["Yang", "Xiaowei", ""]]}, {"id": "1902.04038", "submitter": "Lakshmanan Nataraj", "authors": "M. Goebel, A. Flenner, L. Nataraj, B.S. Manjunath", "title": "Deep Learning Methods for Event Verification and Image Repurposing\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authenticity of images posted on social media is an issue of growing\nconcern. Many algorithms have been developed to detect manipulated images, but\nfew have investigated the ability of deep neural network based approaches to\nverify the authenticity of image labels, such as event names. In this paper, we\npropose several novel methods to predict if an image was captured at one of\nseveral noteworthy events. We use a set of images from several recorded events\nsuch as storms, marathons, protests, and other large public gatherings. Two\nstrategies of applying pre-trained Imagenet network for event verification are\npresented, with two modifications for each strategy. The first method uses the\nfeatures from the last convolutional layer of a pre-trained network as input to\na classifier. We also consider the effects of tuning the convolutional weights\nof the pre-trained network to improve classification. The second method\ncombines many features extracted from smaller scales and uses the output of a\npre-trained network as the input to a second classifier. For both methods, we\ninvestigated several different classifiers and tested many different\npre-trained networks. Our experiments demonstrate both these approaches are\neffective for event verification and image re-purposing detection. The\nclassification at the global scale tends to marginally outperform our tested\nlocal methods and fine tuning the network further improves the results.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 18:32:30 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Goebel", "M.", ""], ["Flenner", "A.", ""], ["Nataraj", "L.", ""], ["Manjunath", "B. S.", ""]]}, {"id": "1902.04042", "submitter": "Youngkyoon Jang", "authors": "Youngkyoon Jang and Hatice Gunes and Ioannis Patras", "title": "Registration-free Face-SSD: Single shot analysis of smiles, facial\n  attributes, and affect in the wild", "comments": "14 pages, 9 figures, 8 tables, accepted for Elsevier CVIU 2019", "journal-ref": null, "doi": "10.1016/j.cviu.2019.01.006", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel single shot face-related task analysis\nmethod, called Face-SSD, for detecting faces and for performing various\nface-related (classification/regression) tasks including smile recognition,\nface attribute prediction and valence-arousal estimation in the wild. Face-SSD\nuses a Fully Convolutional Neural Network (FCNN) to detect multiple faces of\ndifferent sizes and recognise/regress one or more face-related classes.\nFace-SSD has two parallel branches that share the same low-level filters, one\nbranch dealing with face detection and the other one with face analysis tasks.\nThe outputs of both branches are spatially aligned heatmaps that are produced\nin parallel - therefore Face-SSD does not require that face detection, facial\nregion extraction, size normalisation, and facial region processing are\nperformed in subsequent steps. Our contributions are threefold: 1) Face-SSD is\nthe first network to perform face analysis without relying on pre-processing\nsuch as face detection and registration in advance - Face-SSD is a simple and a\nsingle FCNN architecture simultaneously performing face detection and\nface-related task analysis - those are conventionally treated as separate\nconsecutive tasks; 2) Face-SSD is a generalised architecture that is applicable\nfor various face analysis tasks without modifying the network structure - this\nis in contrast to designing task-specific architectures; and 3) Face-SSD\nachieves real-time performance (21 FPS) even when detecting multiple faces and\nrecognising multiple classes in a given image. Experimental results show that\nFace-SSD achieves state-of-the-art performance in various face analysis tasks\nby reaching a recognition accuracy of 95.76% for smile detection, 90.29% for\nattribute prediction, and Root Mean Square (RMS) error of 0.44 and 0.39 for\nvalence and arousal estimation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 18:43:31 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Jang", "Youngkyoon", ""], ["Gunes", "Hatice", ""], ["Patras", "Ioannis", ""]]}, {"id": "1902.04049", "submitter": "Nabil Ibtehaz", "authors": "Nabil Ibtehaz, M. Sohel Rahman", "title": "MultiResUNet : Rethinking the U-Net Architecture for Multimodal\n  Biomedical Image Segmentation", "comments": null, "journal-ref": null, "doi": "10.1016/j.neunet.2019.08.025", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years Deep Learning has brought about a breakthrough in Medical\nImage Segmentation. U-Net is the most prominent deep network in this regard,\nwhich has been the most popular architecture in the medical imaging community.\nDespite outstanding overall performance in segmenting multimodal medical\nimages, from extensive experimentations on challenging datasets, we found out\nthat the classical U-Net architecture seems to be lacking in certain aspects.\nTherefore, we propose some modifications to improve upon the already\nstate-of-the-art U-Net model. Hence, following the modifications we develop a\nnovel architecture MultiResUNet as the potential successor to the successful\nU-Net architecture. We have compared our proposed architecture MultiResUNet\nwith the classical U-Net on a vast repertoire of multimodal medical images.\nAlbeit slight improvements in the cases of ideal images, a remarkable gain in\nperformance has been attained for challenging images. We have evaluated our\nmodel on five different datasets, each with their own unique challenges, and\nhave obtained a relative improvement in performance of 10.15%, 5.07%, 2.63%,\n1.41%, and 0.62% respectively.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 18:50:11 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Ibtehaz", "Nabil", ""], ["Rahman", "M. Sohel", ""]]}, {"id": "1902.04051", "submitter": "Hyungtae Lee", "authors": "Hyungtae Lee and Sungmin Eum and Heesung Kwon", "title": "S-DOD-CNN: Doubly Injecting Spatially-Preserved Object Information for\n  Event Recognition", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel event recognition approach called Spatially-preserved\nDoubly-injected Object Detection CNN (S-DOD-CNN), which incorporates the\nspatially preserved object detection information in both a direct and an\nindirect way. Indirect injection is carried out by simply sharing the weights\nbetween the object detection modules and the event recognition module.\nMeanwhile, our novelty lies in the fact that we have preserved the spatial\ninformation for the direct injection. Once multiple regions-of-intereset (RoIs)\nare acquired, their feature maps are computed and then projected onto a\nspatially-preserving combined feature map using one of the four RoI Projection\napproaches we present. In our architecture, combined feature maps are generated\nfor object detection which are directly injected to the event recognition\nmodule. Our method provides the state-of-the-art accuracy for malicious event\nrecognition.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 18:52:59 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 00:35:19 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Lee", "Hyungtae", ""], ["Eum", "Sungmin", ""], ["Kwon", "Heesung", ""]]}, {"id": "1902.04062", "submitter": "Tao Sun", "authors": "Tao Sun, Dongsheng Li, Hao Jiang, Zhe Quan", "title": "Iteratively reweighted penalty alternating minimization methods with\n  continuation for image deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a class of nonconvex problems with linear\nconstraints appearing frequently in the area of image processing. We solve this\nproblem by the penalty method and propose the iteratively reweighted\nalternating minimization algorithm. To speed up the algorithm, we also apply\nthe continuation strategy to the penalty parameter. A convergence result is\nproved for the algorithm. Compared with the nonconvex ADMM, the proposed\nalgorithm enjoys both theoretical and computational advantages like weaker\nconvergence requirements and faster speed. Numerical results demonstrate the\nefficiency of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 08:41:35 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Sun", "Tao", ""], ["Li", "Dongsheng", ""], ["Jiang", "Hao", ""], ["Quan", "Zhe", ""]]}, {"id": "1902.04099", "submitter": "Balamurali Murugesan", "authors": "Balamurali Murugesan, Kaushik Sarveswaran, Sharath M Shankaranarayana,\n  Keerthi Ram, Mohanasankar Sivaprakasam", "title": "Psi-Net: Shape and boundary aware joint multi-task deep network for\n  medical image segmentation", "comments": "Accepted at EMBC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is a primary task in many medical applications. Recently,\nmany deep networks derived from U-Net have been extensively used in various\nmedical image segmentation tasks. However, in most of the cases, networks\nsimilar to U-net produce coarse and non-smooth segmentations with lots of\ndiscontinuities. To improve and refine the performance of U-Net like networks,\nwe propose the use of parallel decoders which along with performing the mask\npredictions also perform contour prediction and distance map estimation. The\ncontour and distance map aid in ensuring smoothness in the segmentation\npredictions. To facilitate joint training of three tasks, we propose a novel\narchitecture called Psi-Net with a single encoder and three parallel decoders\n(thus having a shape of $\\Psi$), one decoder to learns the segmentation mask\nprediction and other two decoders to learn the auxiliary tasks of contour\ndetection and distance map estimation. The learning of these auxiliary tasks\nhelps in capturing the shape and the boundary information. We also propose a\nnew joint loss function for the proposed architecture. The loss function\nconsists of a weighted combination of Negative Log likelihood and Mean Square\nError loss. We have used two publicly available datasets: 1) Origa dataset for\nthe task of optic cup and disc segmentation and 2) Endovis segment dataset for\nthe task of polyp segmentation to evaluate our model. We have conducted\nextensive experiments using our network to show our model gives better results\nin terms of segmentation, boundary and shape metrics.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 19:11:49 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 07:55:03 GMT"}, {"version": "v3", "created": "Wed, 14 Aug 2019 17:17:37 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Murugesan", "Balamurali", ""], ["Sarveswaran", "Kaushik", ""], ["Shankaranarayana", "Sharath M", ""], ["Ram", "Keerthi", ""], ["Sivaprakasam", "Mohanasankar", ""]]}, {"id": "1902.04103", "submitter": "Zhi Zhang", "authors": "Zhi Zhang, Tong He, Hang Zhang, Zhongyue Zhang, Junyuan Xie, Mu Li", "title": "Bag of Freebies for Training Object Detection Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training heuristics greatly improve various image classification model\naccuracies~\\cite{he2018bag}. Object detection models, however, have more\ncomplex neural network structures and optimization targets. The training\nstrategies and pipelines dramatically vary among different models. In this\nworks, we explore training tweaks that apply to various models including Faster\nR-CNN and YOLOv3. These tweaks do not change the model architectures,\ntherefore, the inference costs remain the same. Our empirical results\ndemonstrate that, however, these freebies can improve up to 5% absolute\nprecision compared to state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 19:20:09 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 23:12:03 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2019 18:04:05 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Zhang", "Zhi", ""], ["He", "Tong", ""], ["Zhang", "Hang", ""], ["Zhang", "Zhongyue", ""], ["Xie", "Junyuan", ""], ["Li", "Mu", ""]]}, {"id": "1902.04139", "submitter": "Veeru Talreja", "authors": "Veeru Talreja, Fariborz Taherkhani, Matthew C. Valenti, and Nasser M.\n  Nasrabadi", "title": "Using Deep Cross Modal Hashing and Error Correcting Codes for Improving\n  the Efficiency of Attribute Guided Facial Image Retrieval", "comments": "To be published in Proc. IEEE Global SIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With benefits of fast query speed and low storage cost, hashing-based image\nretrieval approaches have garnered considerable attention from the research\ncommunity. In this paper, we propose a novel Error-Corrected Deep Cross Modal\nHashing (CMH-ECC) method which uses a bitmap specifying the presence of certain\nfacial attributes as an input query to retrieve relevant face images from the\ndatabase. In this architecture, we generate compact hash codes using an\nend-to-end deep learning module, which effectively captures the inherent\nrelationships between the face and attribute modality. We also integrate our\ndeep learning module with forward error correction codes to further reduce the\ndistance between different modalities of the same subject. Specifically, the\nproperties of deep hashing and forward error correction codes are exploited to\ndesign a cross modal hashing framework with high retrieval performance.\nExperimental results using two standard datasets with facial attributes-image\nmodalities indicate that our CMH-ECC face image retrieval model outperforms\nmost of the current attribute-based face image retrieval approaches.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 20:42:52 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Talreja", "Veeru", ""], ["Taherkhani", "Fariborz", ""], ["Valenti", "Matthew C.", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1902.04144", "submitter": "Marcos Eduardo Valle", "authors": "Alex Santana dos Santos and Marcos Eduardo Valle", "title": "Max-C and Min-D Projection Autoassociative Fuzzy Morphological Memories:\n  Theory and an Application for Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-C and min-D projection autoassociative fuzzy morphological memories\n(max-C and min-D PAFMMs) are two layer feedforward fuzzy morphological neural\nnetworks able to implement an associative memory designed for the storage and\nretrieval of finite fuzzy sets or vectors on a hypercube. In this paper we\naddress the main features of these autoassociative memories, which include\nunlimited absolute storage capacity, fast retrieval of stored items, few\nspurious memories, and an excellent tolerance to either dilative noise or\nerosive noise. Particular attention is given to the so-called PAFMM of Zadeh\nwhich, besides performing no floating-point operations, exhibit the largest\nnoise tolerance among max-C and min-D PAFMMs. Computational experiments reveal\nthat Zadeh's max-C PFAMM, combined with a noise masking strategy, yields a fast\nand robust classifier with strong potential for face recognition.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 21:00:43 GMT"}, {"version": "v2", "created": "Sat, 31 Aug 2019 00:46:32 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Santos", "Alex Santana dos", ""], ["Valle", "Marcos Eduardo", ""]]}, {"id": "1902.04147", "submitter": "C. H. Huck Yang", "authors": "Yi-Chieh Liu, Hao-Hsiang Yang, Chao-Han Huck Yang, Jia-Hong Huang,\n  Meng Tian, Hiromasa Morikawa, Yi-Chang James Tsai, Jesper Tegner", "title": "Synthesizing New Retinal Symptom Images by Multiple Generative Models", "comments": null, "journal-ref": "AI for Retinal Image Analysis Workshop ACCV 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Age-Related Macular Degeneration (AMD) is an asymptomatic retinal disease\nwhich may result in loss of vision. There is limited access to high-quality\nrelevant retinal images and poor understanding of the features defining\nsub-classes of this disease. Motivated by recent advances in machine learning\nwe specifically explore the potential of generative modeling, using Generative\nAdversarial Networks (GANs) and style transferring, to facilitate clinical\ndiagnosis and disease understanding by feature extraction. We design an\nanalytic pipeline which first generates synthetic retinal images from clinical\nimages; a subsequent verification step is applied. In the synthesizing step we\nmerge GANs (DCGANs and WGANs architectures) and style transferring for the\nimage generation, whereas the verified step controls the accuracy of the\ngenerated images. We find that the generated images contain sufficient\npathological details to facilitate ophthalmologists' task of disease\nclassification and in discovery of disease relevant features. In particular,\nour system predicts the drusen and geographic atrophy sub-classes of AMD.\nFurthermore, the performance using CFP images for GANs outperforms the\nclassification based on using only the original clinical dataset. Our results\nare evaluated using existing classifier of retinal diseases and class activated\nmaps, supporting the predictive power of the synthetic images and their utility\nfor feature extraction. Our code examples are available online.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 21:07:14 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Liu", "Yi-Chieh", ""], ["Yang", "Hao-Hsiang", ""], ["Yang", "Chao-Han Huck", ""], ["Huang", "Jia-Hong", ""], ["Tian", "Meng", ""], ["Morikawa", "Hiromasa", ""], ["Tsai", "Yi-Chang James", ""], ["Tegner", "Jesper", ""]]}, {"id": "1902.04149", "submitter": "Veeru Talreja", "authors": "Veeru Talreja, Sobhan Soleymani, Matthew C. Valenti, and Nasser M.\n  Nasrabadi", "title": "Learning to Authenticate with Deep Multibiometric Hashing and Neural\n  Network Decoding", "comments": "To be published in Proc. IEEE ICC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel multimodal deep hashing neural decoder\n(MDHND) architecture, which integrates a deep hashing framework with a neural\nnetwork decoder (NND) to create an effective multibiometric authentication\nsystem. The MDHND consists of two separate modules: a multimodal deep hashing\n(MDH) module, which is used for feature-level fusion and binarization of\nmultiple biometrics, and a neural network decoder (NND) module, which is used\nto refine the intermediate binary codes generated by the MDH and compensate for\nthe difference between enrollment and probe biometrics (variations in pose,\nillumination, etc.). Use of NND helps to improve the performance of the overall\nmultimodal authentication system. The MDHND framework is trained in 3 steps\nusing joint optimization of the two modules. In Step 1, the MDH parameters are\ntrained and learned to generate a shared multimodal latent code; in Step 2, the\nlatent codes from Step 1 are passed through a conventional error-correcting\ncode (ECC) decoder to generate the ground truth to train a neural network\ndecoder (NND); in Step 3, the NND decoder is trained using the ground truth\nfrom Step 2 and the MDH and NND are jointly optimized. Experimental results on\na standard multimodal dataset demonstrate the superiority of our method\nrelative to other current multimodal authentication systems\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 21:08:10 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 17:48:25 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2019 15:54:15 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Talreja", "Veeru", ""], ["Soleymani", "Sobhan", ""], ["Valenti", "Matthew C.", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1902.04161", "submitter": "Gopalakrishnan Srinivasan", "authors": "Gopalakrishnan Srinivasan and Kaushik Roy", "title": "ReStoCNet: Residual Stochastic Binary Convolutional Spiking Neural\n  Network for Memory-Efficient Neuromorphic Computing", "comments": "27 pages, 11 figures, and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose ReStoCNet, a residual stochastic multilayer\nconvolutional Spiking Neural Network (SNN) composed of binary kernels, to\nreduce the synaptic memory footprint and enhance the computational efficiency\nof SNNs for complex pattern recognition tasks. ReStoCNet consists of an input\nlayer followed by stacked convolutional layers for hierarchical input feature\nextraction, pooling layers for dimensionality reduction, and fully-connected\nlayer for inference. In addition, we introduce residual connections between the\nstacked convolutional layers to improve the hierarchical feature learning\ncapability of deep SNNs. We propose Spike Timing Dependent Plasticity (STDP)\nbased probabilistic learning algorithm, referred to as Hybrid-STDP (HB-STDP),\nincorporating Hebbian and anti-Hebbian learning mechanisms, to train the binary\nkernels forming ReStoCNet in a layer-wise unsupervised manner. We demonstrate\nthe efficacy of ReStoCNet and the presented HB-STDP based unsupervised training\nmethodology on the MNIST and CIFAR-10 datasets. We show that residual\nconnections enable the deeper convolutional layers to self-learn useful\nhigh-level input features and mitigate the accuracy loss observed in deep SNNs\ndevoid of residual connections. The proposed ReStoCNet offers >20x kernel\nmemory compression compared to full-precision (32-bit) SNN while yielding high\nenough classification accuracy on the chosen pattern recognition tasks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 21:54:48 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Srinivasan", "Gopalakrishnan", ""], ["Roy", "Kaushik", ""]]}, {"id": "1902.04186", "submitter": "Hiroyuki Kasai", "authors": "Hiroyuki Kasai and Bamdev Mishra", "title": "Riemannian joint dimensionality reduction and dictionary learning on\n  symmetric positive definite manifold", "comments": "European Signal Processing Conference (EUSIPCO 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary leaning (DL) and dimensionality reduction (DR) are powerful tools\nto analyze high-dimensional noisy signals. This paper presents a proposal of a\nnovel Riemannian joint dimensionality reduction and dictionary learning\n(R-JDRDL) on symmetric positive definite (SPD) manifolds for classification\ntasks. The joint learning considers the interaction between dimensionality\nreduction and dictionary learning procedures by connecting them into a unified\nframework. We exploit a Riemannian optimization framework for solving DL and DR\nproblems jointly. Finally, we demonstrate that the proposed R-JDRDL outperforms\nexisting state-of-the-arts algorithms when used for image classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 23:49:03 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Kasai", "Hiroyuki", ""], ["Mishra", "Bamdev", ""]]}, {"id": "1902.04202", "submitter": "Siwei Lyu", "authors": "Yuezun Li and Siwei Lyu", "title": "De-identification without losing faces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training of deep learning models for computer vision requires large image or\nvideo datasets from real world. Often, in collecting such datasets, we need to\nprotect the privacy of the people captured in the images or videos, while still\npreserve the useful attributes such as facial expressions. In this work, we\ndescribe a new face de-identification method that can preserve essential facial\nattributes in the faces while concealing the identities. Our method takes\nadvantage of the recent advances in face attribute transfer models, while\nmaintaining a high visual quality. Instead of changing factors of the original\nfaces or synthesizing faces completely, our method use a trained facial\nattribute transfer model to map non-identity related facial attributes to the\nface of donors, who are a small number (usually 2 to 3) of consented subjects.\nUsing the donors' faces ensures that the natural appearance of the synthesized\nfaces, while ensuring the identity of the synthesized faces are changed. On the\nother hand, the FATM blends the donors' facial attributes to those of the\noriginal faces to diversify the appearance of the synthesized faces.\nExperimental results on several sets of images and videos demonstrate the\neffectiveness of our face de-ID algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 01:15:15 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Li", "Yuezun", ""], ["Lyu", "Siwei", ""]]}, {"id": "1902.04207", "submitter": "Mustansar Fiaz", "authors": "Mustansar Fiaz, Kamran Ali, Abdul Rehman, M. Junaid Gul, Soon Ki Jung", "title": "Brain MRI Segmentation using Rule-Based Hybrid Approach", "comments": "8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image segmentation being a substantial component of image processing\nplays a significant role to analyze gross anatomy, to locate an infirmity and\nto plan the surgical procedures. Segmentation of brain Magnetic Resonance\nImaging (MRI) is of considerable importance for the accurate diagnosis.\nHowever, precise and accurate segmentation of brain MRI is a challenging task.\nHere, we present an efficient framework for segmentation of brain MR images.\nFor this purpose, Gabor transform method is used to compute features of brain\nMRI. Then, these features are classified by using four different classifiers\ni.e., Incremental Supervised Neural Network (ISNN), K-Nearest Neighbor (KNN),\nProbabilistic Neural Network (PNN), and Support Vector Machine (SVM).\nPerformance of these classifiers is investigated over different images of brain\nMRI and the variation in the performance of these classifiers is observed for\ndifferent brain tissues. Thus, we proposed a rule-based hybrid approach to\nsegment brain MRI. Experimental results show that the performance of these\nclassifiers varies over each tissue MRI and the proposed rule-based hybrid\napproach exhibits better segmentation of brain MRI tissues.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 01:30:18 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Fiaz", "Mustansar", ""], ["Ali", "Kamran", ""], ["Rehman", "Abdul", ""], ["Gul", "M. Junaid", ""], ["Jung", "Soon Ki", ""]]}, {"id": "1902.04208", "submitter": "Xuezhe Ma", "authors": "Xuezhe Ma, Xiang Kong, Shanghang Zhang, Eduard Hovy", "title": "MaCow: Masked Convolutional Generative Flow", "comments": "In Proceedings of Thirty-third Conference on Neural Information\n  Processing Systems (NeurIPS-2019)", "journal-ref": "NeurIPS 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow-based generative models, conceptually attractive due to tractability of\nboth the exact log-likelihood computation and latent-variable inference, and\nefficiency of both training and sampling, has led to a number of impressive\nempirical successes and spawned many advanced variants and theoretical\ninvestigations. Despite their computational efficiency, the density estimation\nperformance of flow-based generative models significantly falls behind those of\nstate-of-the-art autoregressive models. In this work, we introduce masked\nconvolutional generative flow (MaCow), a simple yet effective architecture of\ngenerative flow using masked convolution. By restricting the local connectivity\nin a small kernel, MaCow enjoys the properties of fast and stable training, and\nefficient sampling, while achieving significant improvements over Glow for\ndensity estimation on standard image benchmarks, considerably narrowing the gap\nto autoregressive models.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 01:31:06 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 18:34:51 GMT"}, {"version": "v3", "created": "Sat, 8 Jun 2019 18:46:40 GMT"}, {"version": "v4", "created": "Sat, 15 Jun 2019 15:33:07 GMT"}, {"version": "v5", "created": "Sun, 27 Oct 2019 00:45:11 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Ma", "Xuezhe", ""], ["Kong", "Xiang", ""], ["Zhang", "Shanghang", ""], ["Hovy", "Eduard", ""]]}, {"id": "1902.04213", "submitter": "Chaorui Deng", "authors": "Chaorui Deng, Qi Wu, Guanghui Xu, Zhuliang Yu, Yanwu Xu, Kui Jia,\n  Mingkui Tan", "title": "You Only Look & Listen Once: Towards Fast and Accurate Visual Grounding", "comments": "Wrong experimental results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Grounding (VG) aims to locate the most relevant region in an image,\nbased on a flexible natural language query but not a pre-defined label, thus it\ncan be a more useful technique than object detection in practice. Most\nstate-of-the-art methods in VG operate in a two-stage manner, wherein the first\nstage an object detector is adopted to generate a set of object proposals from\nthe input image and the second stage is simply formulated as a cross-modal\nmatching problem that finds the best match between the language query and all\nregion proposals. This is rather inefficient because there might be hundreds of\nproposals produced in the first stage that need to be compared in the second\nstage, not to mention this strategy performs inaccurately. In this paper, we\npropose an simple, intuitive and much more elegant one-stage detection based\nmethod that joints the region proposal and matching stage as a single detection\nnetwork. The detection is conditioned on the input query with a stack of novel\nRelation-to-Attention modules that transform the image-to-query relationship to\nan relation map, which is used to predict the bounding box directly without\nproposing large numbers of useless region proposals. During the inference, our\napproach is about 20x ~ 30x faster than previous methods and, remarkably, it\nachieves 18% ~ 41% absolute performance improvement on top of the\nstate-of-the-art results on several benchmark datasets. We release our code and\nall the pre-trained models at https://github.com/openblack/rvg.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 02:04:49 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 03:32:37 GMT"}, {"version": "v3", "created": "Sun, 17 Mar 2019 06:05:50 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Deng", "Chaorui", ""], ["Wu", "Qi", ""], ["Xu", "Guanghui", ""], ["Yu", "Zhuliang", ""], ["Xu", "Yanwu", ""], ["Jia", "Kui", ""], ["Tan", "Mingkui", ""]]}, {"id": "1902.04236", "submitter": "Vignesh R", "authors": "Vignesh Ravichandran, Balamurali Murugesan, Vaishali Balakarthikeyan,\n  Sharath M Shankaranarayana, Keerthi Ram, Preejith S.P, Jayaraj Joseph and\n  Mohanasankar Sivaprakasam", "title": "RespNet: A deep learning model for extraction of respiration from\n  photoplethysmogram", "comments": "Under review at EMBC", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respiratory ailments afflict a wide range of people and manifests itself\nthrough conditions like asthma and sleep apnea. Continuous monitoring of\nchronic respiratory ailments is seldom used outside the intensive care ward due\nto the large size and cost of the monitoring system. While Electrocardiogram\n(ECG) based respiration extraction is a validated approach, its adoption is\nlimited by access to a suitable continuous ECG monitor. Recently, due to the\nwidespread adoption of wearable smartwatches with in-built Photoplethysmogram\n(PPG) sensor, it is being considered as a viable candidate for continuous and\nunobtrusive respiration monitoring. Research in this domain, however, has been\npredominantly focussed on estimating respiration rate from PPG. In this work, a\nnovel end-to-end deep learning network called RespNet is proposed to perform\nthe task of extracting the respiration signal from a given input PPG as opposed\nto extracting respiration rate. The proposed network was trained and tested on\ntwo different datasets utilizing different modalities of reference respiration\nsignal recordings. Also, the similarity and performance of the proposed network\nagainst two conventional signal processing approaches for extracting\nrespiration signal were studied. The proposed method was tested on two\nindependent datasets with a Mean Squared Error of 0.262 and 0.145. The\nCross-Correlation coefficient of the respective datasets were found to be 0.933\nand 0.931. The reported errors and similarity was found to be better than\nconventional approaches. The proposed approach would aid clinicians to provide\ncomprehensive evaluation of sleep-related respiratory conditions and chronic\nrespiratory ailments while being comfortable and inexpensive for the patient.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 04:36:34 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 10:11:24 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Ravichandran", "Vignesh", ""], ["Murugesan", "Balamurali", ""], ["Balakarthikeyan", "Vaishali", ""], ["Shankaranarayana", "Sharath M", ""], ["Ram", "Keerthi", ""], ["P", "Preejith S.", ""], ["Joseph", "Jayaraj", ""], ["Sivaprakasam", "Mohanasankar", ""]]}, {"id": "1902.04244", "submitter": "Wenxi Liu", "authors": "Dengsheng Chen, Wenxi Liu, You Huang, Tong Tong and Yuanlong Yu", "title": "Enhancement Mask for Hippocampus Detection and Segmentation", "comments": "This paper has been published in the proceedings of IEEE\n  International Conference on Information and Automation 2018", "journal-ref": "2018 IEEE International Conference on Information and Automation\n  (ICIA)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection and segmentation of the hippocampal structures in volumetric brain\nimages is a challenging problem in the area of medical imaging. In this paper,\nwe propose a two-stage 3D fully convolutional neural network that efficiently\ndetects and segments the hippocampal structures. In particular, our approach\nfirst localizes the hippocampus from the whole volumetric image while obtaining\na proposal for a rough segmentation. After localization, we apply the proposal\nas an enhancement mask to extract the fine structure of the hippocampus. The\nproposed method has been evaluated on a public dataset and compares with\nstate-of-the-art approaches. Results indicate the effectiveness of the proposed\nmethod, which yields mean Dice Similarity Coefficients (i.e. DSC) of $0.897$\nand $0.900$ for the left and right hippocampus, respectively. Furthermore,\nextensive experiments manifest that the proposed enhancement mask layer has\nremarkable benefits for accelerating training process and obtaining more\naccurate segmentation results.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 05:25:29 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Chen", "Dengsheng", ""], ["Liu", "Wenxi", ""], ["Huang", "You", ""], ["Tong", "Tong", ""], ["Yu", "Yuanlong", ""]]}, {"id": "1902.04258", "submitter": "Brian Wandell", "authors": "Zhenyi Liu, Minghao Shen, Jiaqi Zhang, Shuangting Liu, Henryk\n  Blasinski, Trisha Lian, Brian Wandell", "title": "A system for generating complex physically accurate sensor images for\n  automotive applications", "comments": "5 pages, 10 figures, IS&T Electronic Imaging conference 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an open-source simulator that creates sensor irradiance and\nsensor images of typical automotive scenes in urban settings. The purpose of\nthe system is to support camera design and testing for automotive applications.\nThe user can specify scene parameters (e.g., scene type, road type, traffic\ndensity, time of day) to assemble a large number of random scenes from graphics\nassets stored in a database. The sensor irradiance is generated using\nquantitative computer graphics methods, and the sensor images are created using\nimage systems sensor simulation. The synthetic sensor images have pixel level\nannotations; hence, they can be used to train and evaluate neural networks for\nimaging tasks, such as object detection and classification. The end-to-end\nsimulation system supports quantitative assessment, from scene to camera to\nnetwork accuracy, for automotive applications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 06:50:47 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Liu", "Zhenyi", ""], ["Shen", "Minghao", ""], ["Zhang", "Jiaqi", ""], ["Liu", "Shuangting", ""], ["Blasinski", "Henryk", ""], ["Lian", "Trisha", ""], ["Wandell", "Brian", ""]]}, {"id": "1902.04272", "submitter": "Patrick Wenzel", "authors": "Qadeer Khan, Torsten Sch\\\"on, Patrick Wenzel", "title": "Towards Self-Supervised High Level Sensor Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a framework to control a self-driving car by fusing\nraw information from RGB images and depth maps. A deep neural network\narchitecture is used for mapping the vision and depth information,\nrespectively, to steering commands. This fusion of information from two sensor\nsources allows to provide redundancy and fault tolerance in the presence of\nsensor failures. Even if one of the input sensors fails to produce the correct\noutput, the other functioning sensor would still be able to maneuver the car.\nSuch redundancy is crucial in the critical application of self-driving cars.\nThe experimental results have showed that our method is capable of learning to\nuse the relevant sensor information even when one of the sensors fail without\nany explicit signal.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 07:53:55 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Khan", "Qadeer", ""], ["Sch\u00f6n", "Torsten", ""], ["Wenzel", "Patrick", ""]]}, {"id": "1902.04294", "submitter": "Hojun Lee", "authors": "Jaeyoung Yoo, Hojun Lee, Nojun Kwak", "title": "Density Estimation and Incremental Learning of Latent Vector for\n  Generative Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we treat the image generation task using the autoencoder, a\nrepresentative latent model. Unlike many studies regularizing the latent\nvariable's distribution by assuming a manually specified prior, we approach the\nimage generation task using an autoencoder by directly estimating the latent\ndistribution. To do this, we introduce 'latent density estimator' which\ncaptures latent distribution explicitly and propose its structure. In addition,\nwe propose an incremental learning strategy of latent variables so that the\nautoencoder learns important features of data by using the structural\ncharacteristics of under-complete autoencoder without an explicit\nregularization term in the objective function. Through experiments, we show the\neffectiveness of the proposed latent density estimator and the incremental\nlearning strategy of latent variables. We also show that our generative model\ngenerates images with improved visual quality compared to previous generative\nmodels based on autoencoders.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 09:41:36 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Yoo", "Jaeyoung", ""], ["Lee", "Hojun", ""], ["Kwak", "Nojun", ""]]}, {"id": "1902.04311", "submitter": "Jonas L\\\"ohdefink", "authors": "Jonas L\\\"ohdefink, Andreas B\\\"ar, Nico M. Schmidt, Fabian H\\\"uger,\n  Peter Schlicht, Tim Fingscheidt", "title": "GAN- vs. JPEG2000 Image Compression for Distributed Automotive\n  Perception: Higher Peak SNR Does Not Mean Better Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high amount of sensors required for autonomous driving poses enormous\nchallenges on the capacity of automotive bus systems. There is a need to\nunderstand tradeoffs between bitrate and perception performance. In this paper,\nwe compare the image compression standards JPEG, JPEG2000, and WebP to a modern\nencoder/decoder image compression approach based on generative adversarial\nnetworks (GANs). We evaluate both the pure compression performance using\ntypical metrics such as peak signal-to-noise ratio (PSNR), structural\nsimilarity (SSIM) and others, but also the performance of a subsequent\nperception function, namely a semantic segmentation (characterized by the mean\nintersection over union (mIoU) measure). Not surprisingly, for all investigated\ncompression methods, a higher bitrate means better results in all investigated\nquality metrics. Interestingly, however, we show that the semantic segmentation\nmIoU of the GAN autoencoder in the highly relevant low-bitrate regime (at\n0.0625 bit/pixel) is better by 3.9% absolute than JPEG2000, although the latter\nstill is considerably better in terms of PSNR (5.91 dB difference). This effect\ncan greatly be enlarged by training the semantic segmentation model with images\noriginating from the decoder, so that the mIoU using the segmentation model\ntrained by GAN reconstructions exceeds the use of the model trained with\noriginal images by almost 20% absolute. We conclude that distributed perception\nin future autonomous driving will most probably not provide a solution to the\nautomotive bus capacity bottleneck by using standard compression schemes such\nas JPEG2000, but requires modern coding approaches, with the GAN\nencoder/decoder method being a promising candidate.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 10:09:37 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["L\u00f6hdefink", "Jonas", ""], ["B\u00e4r", "Andreas", ""], ["Schmidt", "Nico M.", ""], ["H\u00fcger", "Fabian", ""], ["Schlicht", "Peter", ""], ["Fingscheidt", "Tim", ""]]}, {"id": "1902.04356", "submitter": "Mohammad Kamalzare", "authors": "Mohammad Kamalzare, Reza Kahani, Alireza Talebpour, Ahmad\n  Mahmoudi-Aznaveh", "title": "The effect of scene context on weakly supervised semantic segmentation", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image semantic segmentation is parsing image into several partitions in such\na way that each region of which involves a semantic concept. In a weakly\nsupervised manner, since only image-level labels are available, discriminating\nobjects from the background is challenging, and in some cases, much more\ndifficult. More specifically, some objects which are commonly seen in one\nspecific scene (e.g. 'train' typically is seen on 'railroad track') are much\nmore likely to be confused. In this paper, we propose a method to add the\ntarget-specific scenes in order to overcome the aforementioned problem.\nActually, we propose a scene recommender which suggests to add some specific\nscene contexts to the target dataset in order to train the model more\naccurately. It is notable that this idea could be a complementary part of the\nbaselines of many other methods. The experiments validate the effectiveness of\nthe proposed method for the objects for which the scene context is added.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 12:28:12 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 08:45:28 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Kamalzare", "Mohammad", ""], ["Kahani", "Reza", ""], ["Talebpour", "Alireza", ""], ["Mahmoudi-Aznaveh", "Ahmad", ""]]}, {"id": "1902.04378", "submitter": "Arash Akbarinia", "authors": "Arash Akbarinia and Karl R. Gegenfurtner", "title": "Manifestation of Image Contrast in Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contrast is subject to dramatic changes across the visual field, depending on\nthe source of light and scene configurations. Hence, the human visual system\nhas evolved to be more sensitive to contrast than absolute luminance. This\nfeature is equally desired for machine vision: the ability to recognise\npatterns even when aspects of them are transformed due to variation in local\nand global contrast. In this work, we thoroughly investigate the impact of\nimage contrast on prominent deep convolutional networks, both during the\ntraining and testing phase. The results of conducted experiments testify to an\nevident deterioration in the accuracy of all state-of-the-art networks at\nlow-contrast images. We demonstrate that \"contrast-augmentation\" is a\nsufficient condition to endow a network with invariance to contrast. This\npractice shows no negative side effects, quite the contrary, it might allow a\nmodel to refrain from other illuminance related over-fittings. This ability can\nalso be achieved by a short fine-tuning procedure, which opens new lines of\ninvestigation on mechanisms involved in two networks whose weights are over\n99.9% correlated, yet astonishingly produce utterly different outcomes. Our\nfurther analysis suggests that the optimisation algorithm is an influential\nfactor, however with a significantly lower effect; and while the choice of an\narchitecture manifests a negligible impact on this phenomenon, the first layers\nappear to be more critical.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 13:35:26 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Akbarinia", "Arash", ""], ["Gegenfurtner", "Karl R.", ""]]}, {"id": "1902.04422", "submitter": "Andrew Webb", "authors": "Andrew M. Webb, Charles Reynolds, Wenlin Chen, Henry Reeve, Dan-Andrei\n  Iliescu, Mikel Lujan, Gavin Brown", "title": "To Ensemble or Not Ensemble: When does End-To-End Training Fail?", "comments": "Code: https://github.com/grey-area/modular-loss-experiments. Preprint\n  updated to reflect version accepted for publication at ECML", "journal-ref": null, "doi": "10.13140/RG.2.2.28091.46880", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-End training (E2E) is becoming more and more popular to train complex\nDeep Network architectures. An interesting question is whether this trend will\ncontinue-are there any clear failure cases for E2E training? We study this\nquestion in depth, for the specific case of E2E training an ensemble of\nnetworks. Our strategy is to blend the gradient smoothly in between two\nextremes: from independent training of the networks, up to to full E2E\ntraining. We find clear failure cases, where over-parameterized models cannot\nbe trained E2E. A surprising result is that the optimum can sometimes lie in\nbetween the two, neither an ensemble or an E2E system. The work also uncovers\nlinks to Dropout, and raises questions around the nature of ensemble diversity\nand multi-branch networks.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 14:56:06 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 11:15:03 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 10:25:34 GMT"}, {"version": "v4", "created": "Thu, 6 Aug 2020 09:48:03 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Webb", "Andrew M.", ""], ["Reynolds", "Charles", ""], ["Chen", "Wenlin", ""], ["Reeve", "Henry", ""], ["Iliescu", "Dan-Andrei", ""], ["Lujan", "Mikel", ""], ["Brown", "Gavin", ""]]}, {"id": "1902.04478", "submitter": "Chen Liu", "authors": "Chen Liu, Yasutaka Furukawa", "title": "MASC: Multi-scale Affinity with Sparse Convolution for 3D Instance\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for 3D instance segmentation based on sparse\nconvolution and point affinity prediction, which indicates the likelihood of\ntwo points belonging to the same instance. The proposed network, built upon\nsubmanifold sparse convolution [3], processes a voxelized point cloud and\npredicts semantic scores for each occupied voxel as well as the affinity\nbetween neighboring voxels at different scales. A simple yet effective\nclustering algorithm segments points into instances based on the predicted\naffinity and the mesh topology. The semantic for each instance is determined by\nthe semantic prediction. Experiments show that our method outperforms the\nstate-of-the-art instance segmentation methods by a large margin on the widely\nused ScanNet benchmark [2]. We share our code publicly at\nhttps://github.com/art-programmer/MASC.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 16:30:54 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Liu", "Chen", ""], ["Furukawa", "Yasutaka", ""]]}, {"id": "1902.04487", "submitter": "Diedre Carmo", "authors": "Diedre Carmo, Bruna Silva, Clarissa Yasuda, Let\\'icia Rittner, Roberto\n  Lotufo", "title": "Extended 2D Consensus Hippocampus Segmentation", "comments": "This was published as an extended abstract in MIDL 2019\n  [arXiv:1907.08612]. An alpha version of the code is available at\n  https://github.com/dscarmo/e2dhipseg. More experiments on improvements to the\n  method and code are ongoing. Future updates are to be expected. A new, more\n  complete paper is published in arXiv:2001.05058", "journal-ref": null, "doi": null, "report-no": "MIDL/2019/ExtendedAbstract/Sygx97DaKV", "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Hippocampus segmentation plays a key role in diagnosing various brain\ndisorders such as Alzheimer's disease, epilepsy, multiple sclerosis, cancer,\ndepression and others. Nowadays, segmentation is still mainly performed\nmanually by specialists. Segmentation done by experts is considered to be a\ngold-standard when evaluating automated methods, buts it is a time consuming\nand arduos task, requiring specialized personnel. In recent years, efforts have\nbeen made to achieve reliable automated segmentation. For years the best\nperforming authomatic methods were multi atlas based with around 80-85% Dice\ncoefficient and very time consuming, but machine learning methods are recently\nrising with promising time and accuracy performance. A method for volumetric\nhippocampus segmentation is presented, based on the consensus of tri-planar\nU-Net inspired fully convolutional networks (FCNNs), with some modifications,\nincluding residual connections, VGG weight transfers, batch normalization and a\npatch extraction technique employing data from neighbor patches. A study on the\nimpact of our modifications to the classical U-Net architecture was performed.\nOur method achieves cutting edge performance in our dataset, with around 96%\nvolumetric Dice accuracy in our test data. In a public validation dataset,\nHARP, we achieve 87.48% DICE. GPU execution time is in the order of seconds per\nvolume, and source code is publicly available. Also, masks are shown to be\nsimilar to other recent state-of-the-art hippocampus segmentation methods in a\nthird dataset, without manual annotations.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 16:44:35 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 16:11:41 GMT"}, {"version": "v3", "created": "Mon, 29 Jul 2019 20:21:52 GMT"}, {"version": "v4", "created": "Wed, 11 Mar 2020 00:26:48 GMT"}, {"version": "v5", "created": "Wed, 13 May 2020 22:01:47 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Carmo", "Diedre", ""], ["Silva", "Bruna", ""], ["Yasuda", "Clarissa", ""], ["Rittner", "Let\u00edcia", ""], ["Lotufo", "Roberto", ""]]}, {"id": "1902.04502", "submitter": "Rudra P K Poudel", "authors": "Rudra P K Poudel, Stephan Liwicki, Roberto Cipolla", "title": "Fast-SCNN: Fast Semantic Segmentation Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The encoder-decoder framework is state-of-the-art for offline semantic image\nsegmentation. Since the rise in autonomous systems, real-time computation is\nincreasingly desirable. In this paper, we introduce fast segmentation\nconvolutional neural network (Fast-SCNN), an above real-time semantic\nsegmentation model on high resolution image data (1024x2048px) suited to\nefficient computation on embedded devices with low memory. Building on existing\ntwo-branch methods for fast segmentation, we introduce our `learning to\ndownsample' module which computes low-level features for multiple resolution\nbranches simultaneously. Our network combines spatial detail at high resolution\nwith deep features extracted at lower resolution, yielding an accuracy of 68.0%\nmean intersection over union at 123.5 frames per second on Cityscapes. We also\nshow that large scale pre-training is unnecessary. We thoroughly validate our\nmetric in experiments with ImageNet pre-training and the coarse labeled data of\nCityscapes. Finally, we show even faster computation with competitive results\non subsampled inputs, without any network modifications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 17:08:26 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Poudel", "Rudra P K", ""], ["Liwicki", "Stephan", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1902.04541", "submitter": "Xi Wang", "authors": "Xi Wang, Albert Chern, Marc Alexa", "title": "Center of circle after perspective transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based glint-free eye tracking commonly estimates gaze direction based\non the pupil center. The boundary of the pupil is fitted with an ellipse and\nthe euclidean center of the ellipse in the image is taken as the center of the\npupil. However, the center of the pupil is generally not mapped to the center\nof the ellipse by the projective camera transformation. This error resulting\nfrom using a point that is not the true center of the pupil directly affects\neye tracking accuracy. We investigate the underlying geometric problem of\ndetermining the center of a circular object based on its projective image. The\nmain idea is to exploit two concentric circles -- in the application scenario\nthese are the pupil and the iris. We show that it is possible to computed the\ncenter and the ratio of the radii from the mapped concentric circles with a\ndirect method that is fast and robust in practice. We evaluate our method on\nsynthetically generated data and find that it improves systematically over\nusing the center of the fitted ellipse. Apart from applications of eye tracking\nwe estimate that our approach will be useful in other tracking applications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 18:38:04 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Wang", "Xi", ""], ["Chern", "Albert", ""], ["Alexa", "Marc", ""]]}, {"id": "1902.04570", "submitter": "Alessandro Bay", "authors": "Alessandro Bay, Panagiotis Sidiropoulos, Eduard Vazquez, Michele\n  Sasdelli", "title": "Real-time tracker with fast recovery from target loss", "comments": "arXiv admin note: substantial text overlap with arXiv:1806.07844", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a variation of a state-of-the-art real-time\ntracker (CFNet), which adds to the original algorithm robustness to target loss\nwithout a significant computational overhead. The new method is based on the\nassumption that the feature map can be used to estimate the tracking confidence\nmore accurately. When the confidence is low, we avoid updating the object's\nposition through the feature map; instead, the tracker passes to a single-frame\nfailure mode, during which the patch's low-level visual content is used to\nswiftly update the object's position, before recovering from the target loss in\nthe next frame. The experimental evidence provided by evaluating the method on\nseveral tracking datasets validates both the theoretical assumption that the\nfeature map is associated to tracking confidence, and that the proposed\nimplementation can achieve target recovery in multiple scenarios, without\ncompromising the real-time performance.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 11:28:15 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Bay", "Alessandro", ""], ["Sidiropoulos", "Panagiotis", ""], ["Vazquez", "Eduard", ""], ["Sasdelli", "Michele", ""]]}, {"id": "1902.04604", "submitter": "Edward Collier", "authors": "Edward Collier, Kate Duffy, Sangram Ganguly, Geri Madanguit, Subodh\n  Kalia, Gayaka Shreekant, Ramakrishna Nemani, Andrew Michaelis, Shuang Li,\n  Auroop Ganguly, Supratik Mukhopadhyay", "title": "Progressively Growing Generative Adversarial Networks for High\n  Resolution Semantic Segmentation of Satellite Images", "comments": "Accepted too and presented at DMESS 2018 as part of IEEE ICDM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has proven to be useful in classification and segmentation\nof images. In this paper, we evaluate a training methodology for pixel-wise\nsegmentation on high resolution satellite images using progressive growing of\ngenerative adversarial networks. We apply our model to segmenting building\nrooftops and compare these results to conventional methods for rooftop\nsegmentation. We present our findings using the SpaceNet version 2 dataset.\nProgressive GAN training achieved a test accuracy of 93% compared to 89% for\ntraditional GAN training.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 19:39:07 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Collier", "Edward", ""], ["Duffy", "Kate", ""], ["Ganguly", "Sangram", ""], ["Madanguit", "Geri", ""], ["Kalia", "Subodh", ""], ["Shreekant", "Gayaka", ""], ["Nemani", "Ramakrishna", ""], ["Michaelis", "Andrew", ""], ["Li", "Shuang", ""], ["Ganguly", "Auroop", ""], ["Mukhopadhyay", "Supratik", ""]]}, {"id": "1902.04615", "submitter": "Taco Cohen", "authors": "Taco S. Cohen, Maurice Weiler, Berkay Kicanaoglu, Max Welling", "title": "Gauge Equivariant Convolutional Networks and the Icosahedral CNN", "comments": "Proceedings of the International Conference on Machine Learning\n  (ICML), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The principle of equivariance to symmetry transformations enables a\ntheoretically grounded approach to neural network architecture design.\nEquivariant networks have shown excellent performance and data efficiency on\nvision and medical imaging problems that exhibit symmetries. Here we show how\nthis principle can be extended beyond global symmetries to local gauge\ntransformations. This enables the development of a very general class of\nconvolutional neural networks on manifolds that depend only on the intrinsic\ngeometry, and which includes many popular methods from equivariant and\ngeometric deep learning. We implement gauge equivariant CNNs for signals\ndefined on the surface of the icosahedron, which provides a reasonable\napproximation of the sphere. By choosing to work with this very regular\nmanifold, we are able to implement the gauge equivariant convolution using a\nsingle conv2d call, making it a highly scalable and practical alternative to\nSpherical CNNs. Using this method, we demonstrate substantial improvements over\nprevious methods on the task of segmenting omnidirectional images and global\nclimate patterns.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 17:01:05 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 09:50:05 GMT"}, {"version": "v3", "created": "Mon, 13 May 2019 23:03:52 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Cohen", "Taco S.", ""], ["Weiler", "Maurice", ""], ["Kicanaoglu", "Berkay", ""], ["Welling", "Max", ""]]}, {"id": "1902.04684", "submitter": "Owen Mayer", "authors": "Owen Mayer, Matthew C. Stamm", "title": "Forensic Similarity for Digital Images", "comments": "16 pages, Accepted for publication with IEEE T-IFS (IEEE Transactions\n  on Information Forensics and Security, 2019)", "journal-ref": "IEEE Transactions on Information Forensics and Security (2019)\n  Volume: 15, Pages: 1331 - 1346", "doi": "10.1109/TIFS.2019.2924552", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new digital image forensics approach called\nforensic similarity, which determines whether two image patches contain the\nsame forensic trace or different forensic traces. One benefit of this approach\nis that prior knowledge, e.g. training samples, of a forensic trace are not\nrequired to make a forensic similarity decision on it in the future. To do\nthis, we propose a two part deep-learning system composed of a CNN-based\nfeature extractor and a three-layer neural network, called the similarity\nnetwork. This system maps pairs of image patches to a score indicating whether\nthey contain the same or different forensic traces. We evaluated system\naccuracy of determining whether two image patches were 1) captured by the same\nor different camera model, 2) manipulated by the same or different editing\noperation, and 3) manipulated by the same or different manipulation parameter,\ngiven a particular editing operation. Experiments demonstrate applicability to\na variety of forensic traces, and importantly show efficacy on \"unknown\"\nforensic traces that were not used to train the system. Experiments also show\nthat the proposed system significantly improves upon prior art, reducing error\nrates by more than half. Furthermore, we demonstrated the utility of the\nforensic similarity approach in two practical applications: forgery detection\nand localization, and database consistency verification.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 00:21:02 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 18:15:23 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Mayer", "Owen", ""], ["Stamm", "Matthew C.", ""]]}, {"id": "1902.04705", "submitter": "Sijie Shen", "authors": "Yongjie Liu, Sijie Shen", "title": "Self-adaptive Single and Multi-illuminant Estimation Framework based on\n  Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Illuminant estimation plays a key role in digital camera pipeline system, it\naims at reducing color casting effect due to the influence of non-white\nilluminant. Recent researches handle this task by using Convolution Neural\nNetwork (CNN) as a mapping function from input image to a single illumination\nvector. However, global mapping approaches are difficult to deal with scenes\nunder multi-light-sources. In this paper, we proposed a self-adaptive single\nand multi-illuminant estimation framework, which includes the following\nnovelties: (1) Learning local self-adaptive kernels from the entire image for\nilluminant estimation with encoder-decoder CNN structure; (2) Providing\nconfidence measurement for the prediction; (3) Clustering-based iterative\nfitting for computing single and multi-illumination vectors. The proposed\nglobal-to-local aggregation is able to predict multi-illuminant regionally by\nutilizing global information instead of training in patches, as well as brings\nsignificant improvement for single illuminant estimation. We outperform the\nstate-of-the-art methods on standard benchmarks with the largest relative\nimprovement of 16%. In addition, we collect a dataset contains over 13k images\nfor illuminant estimation and evaluation. The code and dataset is available on\nhttps://github.com/LiamLYJ/KPF_WB\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 02:12:55 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Liu", "Yongjie", ""], ["Shen", "Sijie", ""]]}, {"id": "1902.04713", "submitter": "Lei Bi", "authors": "Lei Bi, Yuyu Guo, Qian Wang, Dagan Feng, Michael Fulham, Jinman Kim", "title": "Automated Segmentation of the Optic Disk and Cup using Dual-Stage Fully\n  Convolutional Networks", "comments": "REFUGE Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated segmentation of the optic cup and disk on retinal fundus images is\nfundamental for the automated detection / analysis of glaucoma. Traditional\nsegmentation approaches depend heavily upon hand-crafted features and a priori\nknowledge of the user. As such, these methods are difficult to be adapt to the\nclinical environment. Recently, deep learning methods based on fully\nconvolutional networks (FCNs) have been successful in resolving segmentation\nproblems. However, the reliance on large annotated training data is problematic\nwhen dealing with medical images. If a sufficient amount of annotated training\ndata to cover all possible variations is not available, FCNs do not provide\naccurate segmentation. In addition, FCNs have a large receptive field in the\nconvolutional layers, and hence produce coarse outputs of boundaries. Hence, we\npropose a new fully automated method that we refer to as a dual-stage fully\nconvolutional networks (DSFCN). Our approach leverages deep residual\narchitectures and FCNs and learns and infers the location of the optic cup and\ndisk in a step-wise manner with fine-grained details. During training, our\napproach learns from the training data and the estimated results derived from\nthe previous iteration. The ability to learn from the previous iteration\noptimizes the learning of the optic cup and the disk boundaries. During testing\n(prediction), DSFCN uses test (input) images and the estimated probability map\nderived from previous iterations to gradually improve the segmentation\naccuracy. Our method achieved an average Dice co-efficient of 0.8488 and 0.9441\nfor optic cup and disk segmentation and an area under curve (AUC) of 0.9513 for\nglaucoma detection.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 02:29:59 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Bi", "Lei", ""], ["Guo", "Yuyu", ""], ["Wang", "Qian", ""], ["Feng", "Dagan", ""], ["Fulham", "Michael", ""], ["Kim", "Jinman", ""]]}, {"id": "1902.04729", "submitter": "Jiaxiang(Tom) Jiang", "authors": "Jiaxiang Jiang, Po-Yu Kao, Samuel A. Belteton, Daniel B. Szymanski,\n  B.S. Manjunath", "title": "Accurate 3D Cell Segmentation using Deep Feature and CRF Refinement", "comments": "5 pages, 5 figures, 3 tables", "journal-ref": "2019 IEEE International Conference on Image Processing (ICIP)", "doi": "10.1109/ICIP.2019.8803095", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of accurately identifying cell boundaries and\nlabeling individual cells in confocal microscopy images, specifically, 3D image\nstacks of cells with tagged cell membranes. Precise identification of cell\nboundaries, their shapes, and quantifying inter-cellular space leads to a\nbetter understanding of cell morphogenesis. Towards this, we outline a cell\nsegmentation method that uses a deep neural network architecture to extract a\nconfidence map of cell boundaries, followed by a 3D watershed algorithm and a\nfinal refinement using a conditional random field. In addition to improving the\naccuracy of segmentation compared to other state-of-the-art methods, the\nproposed approach also generalizes well to different datasets without the need\nto retrain the network for each dataset. Detailed experimental results are\nprovided, and the source code is available on GitHub.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 03:38:46 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Jiang", "Jiaxiang", ""], ["Kao", "Po-Yu", ""], ["Belteton", "Samuel A.", ""], ["Szymanski", "Daniel B.", ""], ["Manjunath", "B. S.", ""]]}, {"id": "1902.04755", "submitter": "Jian Zhao", "authors": "Jian Zhao, Jianshu Li, Xiaoguang Tu, Fang Zhao, Yuan Xin, Junliang\n  Xing, Hengzhu Liu, Shuicheng Yan, Jiashi Feng", "title": "Multi-Prototype Networks for Unconstrained Set-based Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the challenging unconstrained set-based face\nrecognition problem where each subject face is instantiated by a set of media\n(images and videos) instead of a single image. Naively aggregating information\nfrom all the media within a set would suffer from the large intra-set variance\ncaused by heterogeneous factors (e.g., varying media modalities, poses and\nilluminations) and fail to learn discriminative face representations. A novel\nMulti-Prototype Network (MPNet) model is thus proposed to learn multiple\nprototype face representations adaptively from the media sets. Each learned\nprototype is representative for the subject face under certain condition in\nterms of pose, illumination and media modality. Instead of handcrafting the set\npartition for prototype learning, MPNet introduces a Dense SubGraph (DSG)\nlearning sub-net that implicitly untangles inconsistent media and learns a\nnumber of representative prototypes. Qualitative and quantitative experiments\nclearly demonstrate superiority of the proposed model over state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 06:02:46 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 01:00:29 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 02:36:10 GMT"}, {"version": "v4", "created": "Sat, 23 Mar 2019 03:54:17 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Zhao", "Jian", ""], ["Li", "Jianshu", ""], ["Tu", "Xiaoguang", ""], ["Zhao", "Fang", ""], ["Xin", "Yuan", ""], ["Xing", "Junliang", ""], ["Liu", "Hengzhu", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1902.04856", "submitter": "Arif Ahmed Sk", "authors": "Sk. Arif Ahmed, Debi Prosad Dogra, Heeseung Choi, Seungho Chae and\n  Ig-Jae Kim", "title": "Person Re-identification in Videos by Analyzing Spatio-Temporal Tubes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical person re-identification frameworks search for k best matches in a\ngallery of images that are often collected in varying conditions. The gallery\nmay contain image sequences when re-identification is done on videos. However,\nsuch a process is time consuming as re-identification has to be carried out\nmultiple times. In this paper, we extract spatio-temporal sequences of frames\n(referred to as tubes) of moving persons and apply a multi-stage processing to\nmatch a given query tube with a gallery of stored tubes recorded through other\ncameras. Initially, we apply a binary classifier to remove noisy images from\nthe input query tube. In the next step, we use a key-pose detection-based query\nminimization. This reduces the length of the query tube by removing redundant\nframes. Finally, a 3-stage hierarchical re-identification framework is used to\nrank the output tubes as per the matching scores. Experiments with publicly\navailable video re-identification datasets reveal that our framework is better\nthan state-of-the-art methods. It ranks the tubes with an increased CMC\naccuracy of 6-8% across multiple datasets. Also, our method significantly\nreduces the number of false positives. A new video re-identification dataset,\nnamed Tube-based Reidentification Video Dataset (TRiViD), has been prepared\nwith an aim to help the re-identification research community\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 10:55:53 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Ahmed", "Sk. Arif", ""], ["Dogra", "Debi Prosad", ""], ["Choi", "Heeseung", ""], ["Chae", "Seungho", ""], ["Kim", "Ig-Jae", ""]]}, {"id": "1902.04886", "submitter": "Angelo Porrello", "authors": "Luca Bergamini, Angelo Porrello, Andrea Capobianco Dondona, Ercole Del\n  Negro, Mauro Mattioli, Nicola D'Alterio, Simone Calderara", "title": "Multi-views Embedding for Cattle Re-identification", "comments": "8 pages, 3 figures. Accepted in the 14th International Conference on\n  SIGNAL IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS (SITIS-2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  People re-identification task has seen enormous improvements in the latest\nyears, mainly due to the development of better image features extraction from\ndeep Convolutional Neural Networks (CNN) and the availability of large\ndatasets. However, little research has been conducted on animal identification\nand re-identification, even if this knowledge may be useful in a rich variety\nof different scenarios. Here, we tackle cattle re-identification exploiting\ndeep CNN and show how this task is poorly related with the human one,\npresenting unique challenges that makes it far from being solved. We present\nvarious baselines, both based on deep architectures or on standard machine\nlearning algorithms, and compared them with our solution. Finally, a rich\nablation study has been conducted to further investigate the unique\npeculiarities of this task.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 13:16:50 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Bergamini", "Luca", ""], ["Porrello", "Angelo", ""], ["Dondona", "Andrea Capobianco", ""], ["Del Negro", "Ercole", ""], ["Mattioli", "Mauro", ""], ["D'Alterio", "Nicola", ""], ["Calderara", "Simone", ""]]}, {"id": "1902.04893", "submitter": "Junghoon Seo", "authors": "Beomsu Kim, Junghoon Seo, SeungHyun Jeon, Jamyoung Koo, Jeongyeol\n  Choe, Taegyun Jeon", "title": "Why are Saliency Maps Noisy? Cause of and Solution to Noisy Saliency\n  Maps", "comments": "Accepted at the 2019 ICCV Workshop on Interpreting and Explaining\n  Visual AI Models (VXAI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency Map, the gradient of the score function with respect to the input,\nis the most basic technique for interpreting deep neural network decisions.\nHowever, saliency maps are often visually noisy. Although several hypotheses\nwere proposed to account for this phenomenon, there are few works that provide\nrigorous analyses of noisy saliency maps. In this paper, we firstly propose a\nnew hypothesis that noise may occur in saliency maps when irrelevant features\npass through ReLU activation functions. Then, we propose Rectified Gradient, a\nmethod that alleviates this problem through layer-wise thresholding during\nbackpropagation. Experiments with neural networks trained on CIFAR-10 and\nImageNet showed effectiveness of our method and its superiority to other\nattribution methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 13:25:39 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 05:36:09 GMT"}, {"version": "v3", "created": "Sat, 14 Sep 2019 15:55:32 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Kim", "Beomsu", ""], ["Seo", "Junghoon", ""], ["Jeon", "SeungHyun", ""], ["Koo", "Jamyoung", ""], ["Choe", "Jeongyeol", ""], ["Jeon", "Taegyun", ""]]}, {"id": "1902.04902", "submitter": "Bin Song", "authors": "Yinghua Li, Bin Song, Jie Guo, Xiaojiang Du, Mohsen Guizani", "title": "Super-Resolution of Brain MRI Images using Overcomplete Dictionaries and\n  Nonlocal Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the Magnetic Resonance Imaging (MRI) images have limited and\nunsatisfactory resolutions due to various constraints such as physical,\ntechnological and economic considerations. Super-resolution techniques can\nobtain high-resolution MRI images. The traditional methods obtained the\nresolution enhancement of brain MRI by interpolations, affecting the accuracy\nof the following diagnose process. The requirement for brain image quality is\nfast increasing. In this paper, we propose an image super-resolution (SR)\nmethod based on overcomplete dictionaries and inherent similarity of an image\nto recover the high-resolution (HR) image from a single low-resolution (LR)\nimage. We explore the nonlocal similarity of the image to tentatively search\nfor similar blocks in the whole image and present a joint reconstruction method\nbased on compressive sensing (CS) and similarity constraints. The sparsity and\nself-similarity of the image blocks are taken as the constraints. The proposed\nmethod is summarized in the following steps. First, a dictionary classification\nmethod based on the measurement domain is presented. The image blocks are\nclassified into smooth, texture and edge parts by analyzing their features in\nthe measurement domain. Then, the corresponding dictionaries are trained using\nthe classified image blocks. Equally important, in the reconstruction part, we\nuse the CS reconstruction method to recover the HR brain MRI image, considering\nboth nonlocal similarity and the sparsity of an image as the constraints. This\nmethod performs better both visually and quantitatively than some existing\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 13:39:45 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Li", "Yinghua", ""], ["Song", "Bin", ""], ["Guo", "Jie", ""], ["Du", "Xiaojiang", ""], ["Guizani", "Mohsen", ""]]}, {"id": "1902.04943", "submitter": "Feng Liu", "authors": "Feng Liu, Luan Tran, Xiaoming Liu", "title": "3D Face Modeling From Diverse Raw Scan Data", "comments": "to appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional 3D face models learn a latent representation of faces using\nlinear subspaces from limited scans of a single database. The main roadblock of\nbuilding a large-scale face model from diverse 3D databases lies in the lack of\ndense correspondence among raw scans. To address these problems, this paper\nproposes an innovative framework to jointly learn a nonlinear face model from a\ndiverse set of raw 3D scan databases and establish dense point-to-point\ncorrespondence among their scans. Specifically, by treating input scans as\nunorganized point clouds, we explore the use of PointNet architectures for\nconverting point clouds to identity and expression feature representations,\nfrom which the decoder networks recover their 3D face shapes. Further, we\npropose a weakly supervised learning approach that does not require\ncorrespondence label for the scans. We demonstrate the superior dense\ncorrespondence and representation power of our proposed method, and its\ncontribution to single-image 3D face reconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 15:04:12 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 02:03:05 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 20:02:49 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Liu", "Feng", ""], ["Tran", "Luan", ""], ["Liu", "Xiaoming", ""]]}, {"id": "1902.04955", "submitter": "Arif Ahmed Sk", "authors": "Sk. Arif Ahmed, Debi Prosad Dogra, Samarjit Kar, Partha Pratim Roy,\n  Dilip K. Prasad", "title": "Can We Automate Diagrammatic Reasoning?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to solve diagrammatic reasoning (DR) can be a challenging but\ninteresting problem to the computer vision research community. It is believed\nthat next generation pattern recognition applications should be able to\nsimulate human brain to understand and analyze reasoning of images. However,\ndue to the lack of benchmarks of diagrammatic reasoning, the present research\nprimarily focuses on visual reasoning that can be applied to real-world\nobjects. In this paper, we present a diagrammatic reasoning dataset that\nprovides a large variety of DR problems. In addition, we also propose a\nKnowledge-based Long Short Term Memory (KLSTM) to solve diagrammatic reasoning\nproblems. Our proposed analysis is arguably the first work in this research\narea. Several state-of-the-art learning frameworks have been used to compare\nwith the proposed KLSTM framework in the present context. Preliminary results\nindicate that the domain is highly related to computer vision and pattern\nrecognition research with several challenging avenues.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 15:43:11 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Ahmed", "Sk. Arif", ""], ["Dogra", "Debi Prosad", ""], ["Kar", "Samarjit", ""], ["Roy", "Partha Pratim", ""], ["Prasad", "Dilip K.", ""]]}, {"id": "1902.04987", "submitter": "Christoph Heindl", "authors": "Christoph Heindl, Sebastian Zambal, Thomas Ponitz, Andreas Pichler and\n  Josef Scharinger", "title": "3D Robot Pose Estimation from 2D Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the task of locating articulated poses of multiple\nrobots in images. Our approach simultaneously infers the number of robots in a\nscene, identifies joint locations and estimates sparse depth maps around joint\nlocations. The proposed method applies staged convolutional feature detectors\nto 2D image inputs and computes robot instance masks using a recurrent network\narchitecture. In addition, regression maps of most likely joint locations in\npixel coordinates together with depth information are computed. Compositing 3D\nrobot joint kinematics is accomplished by applying masks to joint readout maps.\nOur end-to-end formulation is in contrast to previous work in which the\ncomposition of robot joints into kinematics is performed in a separate\npost-processing step. Despite the fact that our models are trained on\nartificial data, we demonstrate generalizability to real world images.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 16:26:31 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Heindl", "Christoph", ""], ["Zambal", "Sebastian", ""], ["Ponitz", "Thomas", ""], ["Pichler", "Andreas", ""], ["Scharinger", "Josef", ""]]}, {"id": "1902.04992", "submitter": "Fabio Maria Carlucci", "authors": "Fabio Maria Carlucci", "title": "Learning to see across Domains and Modalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has raised hopes and expectations as a general solution for\nmany applications; indeed it has proven effective, but it also showed a strong\ndependence on large quantities of data. Luckily, it has been shown that, even\nwhen data is scarce, a successful model can be trained by reusing prior\nknowledge. Thus, developing techniques for transfer learning, in its broadest\ndefinition, is a crucial element towards the deployment of effective and\naccurate intelligent systems. This thesis will focus on a family of transfer\nlearning methods applied to the task of visual object recognition, specifically\nimage classification. Transfer learning is a general term, and specific\nsettings have been given specific names: when the learner has only access to\nunlabeled data from the a target domain and labeled data from a different\ndomain (the source), the problem is known as that of \"unsupervised domain\nadaptation\" (DA). The first part of this work will focus on three methods for\nthis setting: one of these methods deals with features, one with images while\nthe third one uses both. The second part will focus on the real life issues of\nrobotic perception, specifically RGB-D recognition. Robotic platforms are\nusually not limited to color perception; very often they also carry a Depth\ncamera. Unfortunately, the depth modality is rarely used for visual recognition\ndue to the lack of pretrained models from which to transfer and little data to\ntrain one on from scratch. Two methods for dealing with this scenario will be\npresented: one using synthetic data and the other exploiting cross-modality\ntransfer learning.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 16:32:32 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Carlucci", "Fabio Maria", ""]]}, {"id": "1902.04997", "submitter": "Gruber Tobias", "authors": "Tobias Gruber, Frank Julca-Aguilar, Mario Bijelic, Werner Ritter,\n  Klaus Dietmayer, Felix Heide", "title": "Gated2Depth: Real-time Dense Lidar from Gated Images", "comments": "ICCV 2019 (oral), Authorship changed due to ICCV policy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an imaging framework which converts three images from a gated\ncamera into high-resolution depth maps with depth accuracy comparable to pulsed\nlidar measurements. Existing scanning lidar systems achieve low spatial\nresolution at large ranges due to mechanically-limited angular sampling rates,\nrestricting scene understanding tasks to close-range clusters with dense\nsampling. Moreover, today's pulsed lidar scanners suffer from high cost, power\nconsumption, large form-factors, and they fail in the presence of strong\nbackscatter. We depart from point scanning and demonstrate that it is possible\nto turn a low-cost CMOS gated imager into a dense depth camera with at least\n80m range - by learning depth from three gated images. The proposed\narchitecture exploits semantic context across gated slices, and is trained on a\nsynthetic discriminator loss without the need of dense depth labels. The\nproposed replacement for scanning lidar systems is real-time, handles\nback-scatter and provides dense depth at long ranges. We validate our approach\nin simulation and on real-world data acquired over 4,000km driving in northern\nEurope. Data and code are available at https://github.com/gruberto/Gated2Depth.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 16:41:52 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 16:35:35 GMT"}, {"version": "v3", "created": "Sun, 27 Oct 2019 12:56:14 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Gruber", "Tobias", ""], ["Julca-Aguilar", "Frank", ""], ["Bijelic", "Mario", ""], ["Ritter", "Werner", ""], ["Dietmayer", "Klaus", ""], ["Heide", "Felix", ""]]}, {"id": "1902.05020", "submitter": "Shengyu Zhao", "authors": "Shengyu Zhao, Tingfung Lau, Ji Luo, Eric I-Chao Chang, Yan Xu", "title": "Unsupervised 3D End-to-End Medical Image Registration with Volume\n  Tweening Network", "comments": "Journal of Biomedical and Health Informatics", "journal-ref": "IEEE Journal of Biomedical and Health Informatics (Volume: 24,\n  Issue: 5, May 2020)", "doi": "10.1109/JBHI.2019.2951024", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D medical image registration is of great clinical importance. However,\nsupervised learning methods require a large amount of accurately annotated\ncorresponding control points (or morphing), which are very difficult to obtain.\nUnsupervised learning methods ease the burden of manual annotation by\nexploiting unlabeled data without supervision. In this paper, we propose a new\nunsupervised learning method using convolutional neural networks under an\nend-to-end framework, Volume Tweening Network (VTN), for 3D medical image\nregistration. We propose three innovative technical components: (1) An\nend-to-end cascading scheme that resolves large displacement; (2) An efficient\nintegration of affine registration network; and (3) An additional invertibility\nloss that encourages backward consistency. Experiments demonstrate that our\nalgorithm is 880x faster (or 3.3x faster without GPU acceleration) than\ntraditional optimization-based methods and achieves state-of-theart performance\nin medical image registration.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 17:30:00 GMT"}, {"version": "v2", "created": "Sat, 28 Sep 2019 11:50:21 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 13:54:32 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Zhao", "Shengyu", ""], ["Lau", "Tingfung", ""], ["Luo", "Ji", ""], ["Chang", "Eric I-Chao", ""], ["Xu", "Yan", ""]]}, {"id": "1902.05068", "submitter": "Zhanyu Ma", "authors": "Zhanyu Ma, Jalil Taghia, Jun Guo", "title": "On the Convergence of Extended Variational Inference for Non-Gaussian\n  Statistical Models", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference (VI) is a widely used framework in Bayesian estimation.\nFor most of the non-Gaussian statistical models, it is infeasible to find an\nanalytically tractable solution to estimate the posterior distributions of the\nparameters. Recently, an improved framework, namely the extended variational\ninference (EVI), has been introduced and applied to derive analytically\ntractable solution by employing lower-bound approximation to the variational\nobjective function. Two conditions required for EVI implementation, namely the\nweak condition and the strong condition, are discussed and compared in this\npaper. In practical implementation, the convergence of the EVI depends on the\nselection of the lower-bound approximation, no matter with the weak condition\nor the strong condition. In general, two approximation strategies, the single\nlower-bound (SLB) approximation and the multiple lower-bounds (MLB)\napproximation, can be applied to carry out the lower-bound approximation. To\nclarify the differences between the SLB and the MLB, we will also discuss the\nconvergence properties of the aforementioned two approximations. Extensive\ncomparisons are made based on some existing EVI-based non-Gaussian statistical\nmodels. Theoretical analysis are conducted to demonstrate the differences\nbetween the weak and the strong conditions. Qualitative and quantitative\nexperimental results are presented to show the advantages of the SLB\napproximation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 07:35:54 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 15:13:08 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Ma", "Zhanyu", ""], ["Taghia", "Jalil", ""], ["Guo", "Jun", ""]]}, {"id": "1902.05093", "submitter": "Tien-Ju Yang", "authors": "Tien-Ju Yang, Maxwell D. Collins, Yukun Zhu, Jyh-Jing Hwang, Ting Liu,\n  Xiao Zhang, Vivienne Sze, George Papandreou, Liang-Chieh Chen", "title": "DeeperLab: Single-Shot Image Parser", "comments": "20 pages. The code of the proposed Parsing Covering metric is\n  available at http://deeperlab.mit.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a single-shot, bottom-up approach for whole image parsing. Whole\nimage parsing, also known as Panoptic Segmentation, generalizes the tasks of\nsemantic segmentation for 'stuff' classes and instance segmentation for 'thing'\nclasses, assigning both semantic and instance labels to every pixel in an\nimage. Recent approaches to whole image parsing typically employ separate\nstandalone modules for the constituent semantic and instance segmentation tasks\nand require multiple passes of inference. Instead, the proposed DeeperLab image\nparser performs whole image parsing with a significantly simpler, fully\nconvolutional approach that jointly addresses the semantic and instance\nsegmentation tasks in a single-shot manner, resulting in a streamlined system\nthat better lends itself to fast processing. For quantitative evaluation, we\nuse both the instance-based Panoptic Quality (PQ) metric and the proposed\nregion-based Parsing Covering (PC) metric, which better captures the image\nparsing quality on 'stuff' classes and larger object instances. We report\nexperimental results on the challenging Mapillary Vistas dataset, in which our\nsingle model achieves 31.95% (val) / 31.6% PQ (test) and 55.26% PC (val) with 3\nframes per second (fps) on GPU or near real-time speed (22.6 fps on GPU) with\nreduced accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 19:34:07 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 23:22:42 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Yang", "Tien-Ju", ""], ["Collins", "Maxwell D.", ""], ["Zhu", "Yukun", ""], ["Hwang", "Jyh-Jing", ""], ["Liu", "Ting", ""], ["Zhang", "Xiao", ""], ["Sze", "Vivienne", ""], ["Papandreou", "George", ""], ["Chen", "Liang-Chieh", ""]]}, {"id": "1902.05176", "submitter": "Ashis Banerjee", "authors": "Behnoosh Parsa, Ekta U. Samani, Rose Hendrix, Cameron Devine, Shashi\n  M. Singh, Santosh Devasia, and Ashis G. Banerjee", "title": "Toward Ergonomic Risk Prediction via Segmentation of Indoor Object\n  Manipulation Actions Using Spatiotemporal Convolutional Networks", "comments": null, "journal-ref": null, "doi": "10.1109/LRA.2019.2925305", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated real-time prediction of the ergonomic risks of manipulating objects\nis a key unsolved challenge in developing effective human-robot collaboration\nsystems for logistics and manufacturing applications. We present a foundational\nparadigm to address this challenge by formulating the problem as one of action\nsegmentation from RGB-D camera videos. Spatial features are first learned using\na deep convolutional model from the video frames, which are then fed\nsequentially to temporal convolutional networks to semantically segment the\nframes into a hierarchy of actions, which are either ergonomically safe,\nrequire monitoring, or need immediate attention. For performance evaluation, in\naddition to an open-source kitchen dataset, we collected a new dataset\ncomprising twenty individuals picking up and placing objects of varying weights\nto and from cabinet and table locations at various heights. Results show very\nhigh (87-94)\\% F1 overlap scores among the ground truth and predicted frame\nlabels for videos lasting over two minutes and consisting of a large number of\nactions.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 00:53:07 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 04:39:37 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Parsa", "Behnoosh", ""], ["Samani", "Ekta U.", ""], ["Hendrix", "Rose", ""], ["Devine", "Cameron", ""], ["Singh", "Shashi M.", ""], ["Devasia", "Santosh", ""], ["Banerjee", "Ashis G.", ""]]}, {"id": "1902.05194", "submitter": "Martin Bertran", "authors": "Natalia Martinez, Martin Bertran, Guillermo Sapiro, Hau-Tieng Wu", "title": "Non-contact photoplethysmogram and instantaneous heart rate estimation\n  from infrared face video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting the instantaneous heart rate (iHR) from face videos has been well\nstudied in recent years. It is well known that changes in skin color due to\nblood flow can be captured using conventional cameras. One of the main\nlimitations of methods that rely on this principle is the need of an\nillumination source. Moreover, they have to be able to operate under different\nlight conditions. One way to avoid these constraints is using infrared cameras,\nallowing the monitoring of iHR under low light conditions. In this work, we\npresent a simple, principled signal extraction method that recovers the iHR\nfrom infrared face videos. We tested the procedure on 7 participants, for whom\nwe recorded an electrocardiogram simultaneously with their infrared face video.\nWe checked that the recovered signal matched the ground truth iHR, showing that\ninfrared is a promising alternative to conventional video imaging for heart\nrate monitoring, especially in low light conditions. Code is available at\nhttps://github.com/natalialmg/IR_iHR\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 03:01:28 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Martinez", "Natalia", ""], ["Bertran", "Martin", ""], ["Sapiro", "Guillermo", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "1902.05211", "submitter": "Kourosh Meshgi", "authors": "Kourosh Meshgi, Maryam Sadat Mirzaei, Shigeyuki Oba", "title": "Long and Short Memory Balancing in Visual Co-Tracking using Q-Learning", "comments": "Submitted to ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Employing one or more additional classifiers to break the self-learning loop\nin tracing-by-detection has gained considerable attention. Most of such\ntrackers merely utilize the redundancy to address the accumulating label error\nin the tracking loop, and suffer from high computational complexity as well as\ntracking challenges that may interrupt all classifiers (e.g. temporal\nocclusions). We propose the active co-tracking framework, in which the main\nclassifier of the tracker labels samples of the video sequence, and only\nconsults auxiliary classifier when it is uncertain. Based on the source of the\nuncertainty and the differences of two classifiers (e.g. accuracy, speed,\nupdate frequency, etc.), different policies should be taken to exchange the\ninformation between two classifiers. Here, we introduce a reinforcement\nlearning approach to find the appropriate policy by considering the state of\nthe tracker in a specific sequence. The proposed method yields promising\nresults in comparison to the best tracking-by-detection approaches.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 04:17:29 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Meshgi", "Kourosh", ""], ["Mirzaei", "Maryam Sadat", ""], ["Oba", "Shigeyuki", ""]]}, {"id": "1902.05247", "submitter": "Zhidong LIang", "authors": "Zhidong Liang, Ming Yang, Chunxiang Wang", "title": "3D Graph Embedding Learning with a Structure-aware Loss Function for\n  Point Cloud Semantic Instance Segmentation", "comments": null, "journal-ref": "Title: 3D Instance Embedding Learning With a Structure-Aware Loss\n  Function for Point Cloud Segmentation; Published in: IEEE Robotics and\n  Automation Letters ( Volume: 5, Issue: 3, July 2020)", "doi": "10.1109/LRA.2020.3004802", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel approach for 3D semantic instance segmentation\non point clouds. A 3D convolutional neural network called submanifold sparse\nconvolutional network is used to generate semantic predictions and instance\nembeddings simultaneously. To obtain discriminative embeddings for each 3D\ninstance, a structure-aware loss function is proposed which considers both the\nstructure information and the embedding information. To get more consistent\nembeddings for each 3D instance, attention-based k nearest neighbour (KNN) is\nproposed to assign different weights for different neighbours. Based on the\nattention-based KNN, we add a graph convolutional network after the sparse\nconvolutional network to get refined embeddings. Our network can be trained\nend-to-end. A simple mean-shift algorithm is utilized to cluster refined\nembeddings to get final instance predictions. As a result, our framework can\noutput both the semantic prediction and the instance prediction. Experiments\nshow that our approach outperforms all state-of-art methods on ScanNet\nbenchmark and NYUv2 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 07:29:40 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Liang", "Zhidong", ""], ["Yang", "Ming", ""], ["Wang", "Chunxiang", ""]]}, {"id": "1902.05255", "submitter": "Jan Egger", "authors": "J\\\"urgen Wallner, Irene Mischak, Jan Egger", "title": "Computed tomography data collection of the complete human mandible and\n  valid clinical ground truth models", "comments": "14 pages, 8 Figures, 4 Tables, 55 References", "journal-ref": "Wallner, J. et al. Computed tomography data collection of the\n  complete human mandible and valid clinical ground truth models. Sci. Data.\n  6:190003 https://doi.org/10.1038/sdata.2019.3 (2019)", "doi": "10.1038/sdata.2019.3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based algorithmic software segmentation is an increasingly important\ntopic in many medical fields. Algorithmic segmentation is used for medical\nthree-dimensional visualization, diagnosis or treatment support, especially in\ncomplex medical cases. However, accessible medical databases are limited, and\nvalid medical ground truth databases for the evaluation of algorithms are rare\nand usually comprise only a few images. Inaccuracy or invalidity of medical\nground truth data and image-based artefacts also limit the creation of such\ndatabases, which is especially relevant for CT data sets of the\nmaxillomandibular complex. This contribution provides a unique and accessible\ndata set of the complete mandible, including 20 valid ground truth segmentation\nmodels originating from 10 CT scans from clinical practice without artefacts or\nfaulty slices. From each CT scan, two 3D ground truth models were created by\nclinical experts through independent manual slice-by-slice segmentation, and\nthe models were statistically compared to prove their validity. These data\ncould be used to conduct serial image studies of the human mandible, evaluating\nsegmentation algorithms and developing adequate image tools.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 08:10:39 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Wallner", "J\u00fcrgen", ""], ["Mischak", "Irene", ""], ["Egger", "Jan", ""]]}, {"id": "1902.05291", "submitter": "Michael Ying Yang", "authors": "Yanpeng Cao, Dayan Guan, Yulun Wu, Jiangxin Yang, Yanlong Cao, Michael\n  Ying Yang", "title": "Box-level Segmentation Supervised Deep Neural Networks for Accurate and\n  Real-time Multispectral Pedestrian Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective fusion of complementary information captured by multi-modal sensors\n(visible and infrared cameras) enables robust pedestrian detection under\nvarious surveillance situations (e.g. daytime and nighttime). In this paper, we\npresent a novel box-level segmentation supervised learning framework for\naccurate and real-time multispectral pedestrian detection by incorporating\nfeatures extracted in visible and infrared channels. Specifically, our method\ntakes pairs of aligned visible and infrared images with easily obtained\nbounding box annotations as input and estimates accurate prediction maps to\nhighlight the existence of pedestrians. It offers two major advantages over the\nexisting anchor box based multispectral detection methods. Firstly, it\novercomes the hyperparameter setting problem occurred during the training phase\nof anchor box based detectors and can obtain more accurate detection results,\nespecially for small and occluded pedestrian instances. Secondly, it is capable\nof generating accurate detection results using small-size input images, leading\nto improvement of computational efficiency for real-time autonomous driving\napplications. Experimental results on KAIST multispectral dataset show that our\nproposed method outperforms state-of-the-art approaches in terms of both\naccuracy and speed.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 10:25:26 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Cao", "Yanpeng", ""], ["Guan", "Dayan", ""], ["Wu", "Yulun", ""], ["Yang", "Jiangxin", ""], ["Cao", "Yanlong", ""], ["Yang", "Michael Ying", ""]]}, {"id": "1902.05300", "submitter": "Vegard Antun", "authors": "Vegard Antun, Francesco Renna, Clarice Poon, Ben Adcock and Anders C.\n  Hansen", "title": "On instabilities of deep learning in image reconstruction - Does AI come\n  at a cost?", "comments": null, "journal-ref": "Proc. Natl. Acad. Sci. USA, 2020", "doi": "10.1073/pnas.1907377117", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning, due to its unprecedented success in tasks such as image\nclassification, has emerged as a new tool in image reconstruction with\npotential to change the field. In this paper we demonstrate a crucial\nphenomenon: deep learning typically yields unstablemethods for image\nreconstruction. The instabilities usually occur in several forms: (1) tiny,\nalmost undetectable perturbations, both in the image and sampling domain, may\nresult in severe artefacts in the reconstruction, (2) a small structural\nchange, for example a tumour, may not be captured in the reconstructed image\nand (3) (a counterintuitive type of instability) more samples may yield poorer\nperformance. Our new stability test with algorithms and easy to use software\ndetects the instability phenomena. The test is aimed at researchers to test\ntheir networks for instabilities and for government agencies, such as the Food\nand Drug Administration (FDA), to secure safe use of deep learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 11:14:58 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Antun", "Vegard", ""], ["Renna", "Francesco", ""], ["Poon", "Clarice", ""], ["Adcock", "Ben", ""], ["Hansen", "Anders C.", ""]]}, {"id": "1902.05316", "submitter": "Soomin Seo", "authors": "Soomin Seo, Sehwan Ki, Munchurl Kim", "title": "A Novel Just-Noticeable-Difference-based Saliency-Channel Attention\n  Residual Network for Full-Reference Image Quality Predictions", "comments": "Accepted for publication in IEEE Transactions on Circuits and Systems\n  for Video Technology (TCSVT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, due to the strength of deep convolutional neural networks (CNN),\nmany CNN-based image quality assessment (IQA) models have been studied.\nHowever, previous CNN-based IQA models likely have yet to utilize the\ncharacteristics of the human visual system (HVS) fully for IQA problems when\nthey simply entrust everything to the CNN, expecting it to learn from a\ntraining dataset. However, in this paper, we propose a novel saliency-channel\nattention residual network based on the just-noticeable-difference (JND)\nconcept for full-reference image quality assessments (FR-IQA). It is referred\nto as JND-SalCAR and shows significant improvements in large IQA datasets with\nvarious types of distortion. The proposed JND-SalCAR effectively learns how to\nincorporate human psychophysical characteristics, such as visual saliency and\nJND, into image quality predictions. In the proposed network, a SalCAR block is\ndevised so that perceptually important features can be extracted with the help\nof saliency-based spatial attention and channel attention schemes. In addition,\na saliency map serves as a guideline for predicting a patch weight map in order\nto afford stable training of end-to-end optimization for the JND-SalCAR. To the\nbest of our knowledge, our work presents the first HVS-inspired trainable\nFR-IQA network that considers both visual saliency and the JND characteristics\nof the HVS. When the visual saliency map and the JND probability map are\nexplicitly given as priors, they can be usefully combined to predict IQA scores\nrated by humans more precisely, eventually leading to performance improvements\nand faster convergence. The experimental results show that the proposed\nJND-SalCAR significantly outperforms all recent state-of-the-art FR-IQA methods\non large IQA datasets in terms of the Spearman rank order coefficient (SRCC)\nand the Pearson linear correlation coefficient (PLCC).\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 11:50:49 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 12:51:10 GMT"}, {"version": "v3", "created": "Tue, 7 May 2019 08:08:33 GMT"}, {"version": "v4", "created": "Fri, 16 Oct 2020 06:34:33 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Seo", "Soomin", ""], ["Ki", "Sehwan", ""], ["Kim", "Munchurl", ""]]}, {"id": "1902.05340", "submitter": "Shuvendu Rana", "authors": "Shuvendu Rana, Rory Hampson, Gordon Dobie", "title": "Breast Cancer: Model Reconstruction and Image Registration from\n  Segmented Deformed Image using Visual and Force based Analysis", "comments": "12 pages, 16 figures", "journal-ref": "IEEE Transactions on Medical Imaging 2019", "doi": "10.1109/TMI.2019.2946629", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Breast lesion localization using tactile imaging is a new and developing\ndirection in medical science. To achieve the goal, proper image reconstruction\nand image registration can be a valuable asset. In this paper, a new approach\nof the segmentation-based image surface reconstruction algorithm is used to\nreconstruct the surface of a breast phantom. In breast tissue, the sub-dermal\nvein network is used as a distinguishable pattern for reconstruction. The\nproposed image capturing device contacts the surface of the phantom, and\nsurface deformation will occur due to applied force at the time of scanning. A\nnovel force based surface rectification system is used to reconstruct a\ndeformed surface image to its original structure. For the construction of the\nfull surface from rectified images, advanced affine scale-invariant feature\ntransform (A-SIFT) is proposed to reduce the affine effect in time when data\ncapturing. Camera position based image stitching approach is applied to\nconstruct the final original non-rigid surface. The proposed model is validated\nin theoretical models and real scenarios, to demonstrate its advantages with\nrespect to competing methods. The result of the proposed method, applied to\npath reconstruction, ends with a positioning accuracy of 99.7%\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 13:09:20 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 14:59:01 GMT"}, {"version": "v3", "created": "Tue, 8 Oct 2019 07:44:49 GMT"}, {"version": "v4", "created": "Tue, 15 Oct 2019 07:28:27 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Rana", "Shuvendu", ""], ["Hampson", "Rory", ""], ["Dobie", "Gordon", ""]]}, {"id": "1902.05341", "submitter": "Wonjik Kim", "authors": "Wonjik Kim, Masayuki Tanaka, Masatoshi Okutomi, Yoko Sasaki", "title": "Automatic Labeled LiDAR Data Generation based on Precise Human Model", "comments": "Accepted at ICRA2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following improvements in deep neural networks, state-of-the-art networks\nhave been proposed for human recognition using point clouds captured by LiDAR.\nHowever, the performance of these networks strongly depends on the training\ndata. An issue with collecting training data is labeling. Labeling by humans is\nnecessary to obtain the ground truth label; however, labeling requires huge\ncosts. Therefore, we propose an automatic labeled data generation pipeline, for\nwhich we can change any parameters or data generation environments. Our\napproach uses a human model named Dhaiba and a background of Miraikan and\nconsequently generated realistic artificial data. We present 500k+ data\ngenerated by the proposed pipeline. This paper also describes the specification\nof the pipeline and data details with evaluations of various approaches.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 13:11:14 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Kim", "Wonjik", ""], ["Tanaka", "Masayuki", ""], ["Okutomi", "Masatoshi", ""], ["Sasaki", "Yoko", ""]]}, {"id": "1902.05343", "submitter": "Weiya Ren", "authors": "Weiya Ren", "title": "Study of dynamical system based obstacle avoidance via manipulating\n  orthogonal coordinates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the general problem of obstacle avoidance based on\ndynamical system. The modulation matrix is developed by introducing orthogonal\ncoordinates, which makes the modulation matrix more reasonable. The new\ntrajectory's direction can be represented by the linear combination of\northogonal coordinates. A orthogonal coordinates manipulating approach is\nproposed by introducing rotating matrix to solve the local minimal problem and\nprovide more reasonable motions in 3-D or higher dimension space. The proposed\nmethod also provide a solution for patrolling around a convex shape.\nExperimental results on several designed dynamical systems demonstrate the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 13:16:19 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 13:35:13 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Ren", "Weiya", ""]]}, {"id": "1902.05356", "submitter": "Wouter Van Gansbeke", "authors": "Wouter Van Gansbeke, Davy Neven, Bert De Brabandere and Luc Van Gool", "title": "Sparse and noisy LiDAR completion with RGB guidance and uncertainty", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a new method to accurately complete sparse LiDAR maps\nguided by RGB images. For autonomous vehicles and robotics the use of LiDAR is\nindispensable in order to achieve precise depth predictions. A multitude of\napplications depend on the awareness of their surroundings, and use depth cues\nto reason and react accordingly. On the one hand, monocular depth prediction\nmethods fail to generate absolute and precise depth maps. On the other hand,\nstereoscopic approaches are still significantly outperformed by LiDAR based\napproaches. The goal of the depth completion task is to generate dense depth\npredictions from sparse and irregular point clouds which are mapped to a 2D\nplane. We propose a new framework which extracts both global and local\ninformation in order to produce proper depth maps. We argue that simple depth\ncompletion does not require a deep network. However, we additionally propose a\nfusion method with RGB guidance from a monocular camera in order to leverage\nobject information and to correct mistakes in the sparse input. This improves\nthe accuracy significantly. Moreover, confidence masks are exploited in order\nto take into account the uncertainty in the depth predictions from each\nmodality. This fusion method outperforms the state-of-the-art and ranks first\non the KITTI depth completion benchmark. Our code with visualizations is\navailable.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 13:55:22 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Van Gansbeke", "Wouter", ""], ["Neven", "Davy", ""], ["De Brabandere", "Bert", ""], ["Van Gool", "Luc", ""]]}, {"id": "1902.05373", "submitter": "Xu Zhao", "authors": "Xu Zhao, Zongli Jiang", "title": "A Tangent Distance Preserving Dimensionality Reduction Algorithm", "comments": "Signal and Image Processing (SIP 2008)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of nonlinear dimensionality reduction.\nUnlike existing methods, such as LLE, ISOMAP, which attempt to unfold the true\nmanifold in the low dimensional space, our algorithm tries to preserve the\nnonlinear structure of the manifold, and shows how the manifold is folded in\nthe high dimensional space. We call this method Tangent Distance Preserving\nMapping (TDPM). TDPM uses tangent distance instead of geodesic distance, and\nthen applies MDS to the tangent distance matrix to map the manifold into a low\ndimensional space in which we can get its nonlinear structure.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 01:52:42 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Zhao", "Xu", ""], ["Jiang", "Zongli", ""]]}, {"id": "1902.05376", "submitter": "Guangcun Shan Prof.", "authors": "Guangcun Shan, Hongyu Wang and Wei Liang", "title": "Robust Encoder-Decoder Learning Framework towards Offline Handwritten\n  Mathematical Expression Recognition Based on Multi-Scale Deep Neural Network", "comments": "11 pages, 16 figures", "journal-ref": "Sci China Inf Sci, 2021, 64(3): 139101, doi:\n  10.1007/s11432-018-9824-9", "doi": "10.1007/s11432-018-9824-9", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Offline handwritten mathematical expression recognition is a challenging\ntask, because handwritten mathematical expressions mainly have two problems in\nthe process of recognition. On one hand, it is how to correctly recognize\ndifferent mathematical symbols. On the other hand, it is how to correctly\nrecognize the two-dimensional structure existing in mathematical expressions.\nInspired by recent work in deep learning, a new neural network model that\ncombines a Multi-Scale convolutional neural network (CNN) with an Attention\nrecurrent neural network (RNN) is proposed to identify two-dimensional\nhandwritten mathematical expressions as one-dimensional LaTeX sequences. As a\nresult, the model proposed in the present work has achieved a WER error of\n25.715% and ExpRate of 28.216%.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 03:29:49 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 09:29:27 GMT"}, {"version": "v3", "created": "Thu, 28 May 2020 11:06:38 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Shan", "Guangcun", ""], ["Wang", "Hongyu", ""], ["Liang", "Wei", ""]]}, {"id": "1902.05377", "submitter": "Yuxuan Liang", "authors": "Yuxuan Liang, Kun Ouyang, Lin Jing, Sijie Ruan, Ye Liu, Junbo Zhang,\n  David S. Rosenblum, Yu Zheng", "title": "UrbanFM: Inferring Fine-Grained Urban Flows", "comments": null, "journal-ref": null, "doi": "10.1145/3292500.3330646", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban flow monitoring systems play important roles in smart city efforts\naround the world. However, the ubiquitous deployment of monitoring devices,\nsuch as CCTVs, induces a long-lasting and enormous cost for maintenance and\noperation. This suggests the need for a technology that can reduce the number\nof deployed devices, while preventing the degeneration of data accuracy and\ngranularity. In this paper, we aim to infer the real-time and fine-grained\ncrowd flows throughout a city based on coarse-grained observations. This task\nis challenging due to two reasons: the spatial correlations between coarse- and\nfine-grained urban flows, and the complexities of external impacts. To tackle\nthese issues, we develop a method entitled UrbanFM based on deep neural\nnetworks. Our model consists of two major parts: 1) an inference network to\ngenerate fine-grained flow distributions from coarse-grained inputs by using a\nfeature extraction module and a novel distributional upsampling module; 2) a\ngeneral fusion subnet to further boost the performance by considering the\ninfluences of different external factors. Extensive experiments on two\nreal-world datasets, namely TaxiBJ and HappyValley, validate the effectiveness\nand efficiency of our method compared to seven baselines, demonstrating the\nstate-of-the-art performance of our approach on the fine-grained urban flow\ninference problem.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 08:22:18 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Liang", "Yuxuan", ""], ["Ouyang", "Kun", ""], ["Jing", "Lin", ""], ["Ruan", "Sijie", ""], ["Liu", "Ye", ""], ["Zhang", "Junbo", ""], ["Rosenblum", "David S.", ""], ["Zheng", "Yu", ""]]}, {"id": "1902.05378", "submitter": "Manuel Lagunas", "authors": "Manuel Lagunas, Elena Garces and Diego Gutierrez", "title": "Learning icons appearance similarity", "comments": "12 pages, 11 figures", "journal-ref": "Multimedia Tools and Applications, pages: 1-19, year: 2018,\n  publisher: Springer", "doi": "10.1007/s11042-018-6628-7", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting an optimal set of icons is a crucial step in the pipeline of visual\ndesign to structure and navigate through content. However, designing the icons\nsets is usually a difficult task for which expert knowledge is required. In\nthis work, to ease the process of icon set selection to the users, we propose a\nsimilarity metric which captures the properties of style and visual identity.\nWe train a Siamese Neural Network with an online dataset of icons organized in\nvisually coherent collections that are used to adaptively sample training data\nand optimize the training process. As the dataset contains noise, we further\ncollect human-rated information on the perception of icon's similarity which\nwill be used for evaluating and testing the proposed model. We present several\nresults and applications based on searches, kernel visualizations and optimized\nset proposals that can be helpful for designers and non-expert users while\nexploring large collections of icons.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 14:17:26 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Lagunas", "Manuel", ""], ["Garces", "Elena", ""], ["Gutierrez", "Diego", ""]]}, {"id": "1902.05379", "submitter": "Greg Olmschenk", "authors": "Greg Olmschenk, Hao Tang, Zhigang Zhu", "title": "Improving Dense Crowd Counting Convolutional Neural Networks using\n  Inverse k-Nearest Neighbor Maps and Multiscale Upsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gatherings of thousands to millions of people frequently occur for an\nenormous variety of events, and automated counting of these high-density crowds\nis useful for safety, management, and measuring significance of an event. In\nthis work, we show that the regularly accepted labeling scheme of crowd density\nmaps for training deep neural networks is less effective than our alternative\ninverse k-nearest neighbor (i$k$NN) maps, even when used directly in existing\nstate-of-the-art network structures. We also provide a new network architecture\nMUD-i$k$NN, which uses multi-scale upsampling via transposed convolutions to\ntake full advantage of the provided i$k$NN labeling. This upsampling combined\nwith the i$k$NN maps further improves crowd counting accuracy. Our new network\narchitecture performs favorably in comparison with the state-of-the-art.\nHowever, our labeling and upsampling techniques are generally applicable to\nexisting crowd counting architectures.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 22:05:47 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 21:07:05 GMT"}, {"version": "v3", "created": "Fri, 29 Mar 2019 20:59:03 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Olmschenk", "Greg", ""], ["Tang", "Hao", ""], ["Zhu", "Zhigang", ""]]}, {"id": "1902.05380", "submitter": "Tao Li", "authors": "Xudong Liu, Tao Li, Hao Peng, Iris Chuoying Ouyang, Taehwan Kim,\n  Ruizhe Wang", "title": "Understanding Beauty via Deep Facial Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of beauty has been debated by philosophers and psychologists for\ncenturies, but most definitions are subjective and metaphysical, and deficit in\naccuracy, generality, and scalability. In this paper, we present a novel study\non mining beauty semantics of facial attributes based on big data, with an\nattempt to objectively construct descriptions of beauty in a quantitative\nmanner. We first deploy a deep convolutional neural network (CNN) to extract\nfacial attributes, and then investigate correlations between these features and\nattractiveness on two large-scale datasets labelled with beauty scores. Not\nonly do we discover the secrets of beauty verified by statistical significance\ntests, our findings also align perfectly with existing psychological studies\nthat, e.g., small nose, high cheekbones, and femininity contribute to\nattractiveness. We further leverage these high-level representations to\noriginal images by a generative adversarial network (GAN). Beauty enhancements\nafter synthesis are visually compelling and statistically convincing verified\nby a user survey of 10,000 data points.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 22:51:21 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 03:44:24 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Liu", "Xudong", ""], ["Li", "Tao", ""], ["Peng", "Hao", ""], ["Ouyang", "Iris Chuoying", ""], ["Kim", "Taehwan", ""], ["Wang", "Ruizhe", ""]]}, {"id": "1902.05386", "submitter": "Andrej Joki\\'c", "authors": "Andrej Jokic, Nikola Vukovic", "title": "License Plate Recognition with Compressive Sensing Based Feature\n  Extraction", "comments": "Student paper submitted to The 8th Mediterranean Conference on\n  Embedded Computing - MECO'2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  License plate recognition is the key component to many automatic traffic\ncontrol systems. It enables the automatic identification of vehicles in many\napplications. Such systems must be able to identify vehicles from images taken\nin various conditions including low light, rain, snow, etc. In order to reduce\nthe complexity and cost of the hardware required for such devices, the\nalgorithm should be as efficient as possible. This paper proposes a license\nplate recognition system which uses a new approach based on compressive sensing\ntechniques for dimensionality reduction and feature extraction. Dimensionality\nreduction will enable precise classification with less training data while\ndemanding less computational power. Based on the extracted features, character\nrecognition and classification is done by a Support Vector Machine classifier.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 11:58:56 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Jokic", "Andrej", ""], ["Vukovic", "Nikola", ""]]}, {"id": "1902.05387", "submitter": "Alexander Boxer", "authors": "Seth Zuckerman, Timothy Klein, Alexander Boxer, Christopher Goldman,\n  Brian Lang", "title": "Simultaneous x, y Pixel Estimation and Feature Extraction for Multiple\n  Small Objects in a Scene: A Description of the ALIEN Network", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep-learning network that detects multiple small objects\n(hundreds to thousands) in a scene while simultaneously estimating their x,y\npixel locations together with a characteristic feature-set (for instance,\ntarget orientation and color). All estimations are performed in a single,\nforward pass which makes implementing the network fast and efficient. In this\npaper, we describe the architecture of our network --- nicknamed ALIEN --- and\ndetail its performance when applied to vehicle detection.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 16:45:32 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Zuckerman", "Seth", ""], ["Klein", "Timothy", ""], ["Boxer", "Alexander", ""], ["Goldman", "Christopher", ""], ["Lang", "Brian", ""]]}, {"id": "1902.05388", "submitter": "Slavko Kovacevic", "authors": "Slavko Kovacevic, Vuko Djaletic, Jelena Vukovic", "title": "Face Recognition using Compressive Sensing", "comments": "Student paper submitted to The 8th Mediterranean Conference on\n  Embedded Computing - MECO'2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the Compressive Sensing implementation in the Face\nRecognition problem. Compressive Sensing is new approach in signal processing\nwith a single goal to recover signal from small set of available samples.\nCompressive Sensing finds its usage in many real applications as it lowers the\nmemory demand and acquisition time, and therefore allows dealing with huge data\nin the fastest manner. In this paper, the undersampled signal is recovered\nusing the algorithm based on Total Variation minimization. The theory is\nverified with an experimental results using different percentage of signal\nsamples.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 11:21:12 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Kovacevic", "Slavko", ""], ["Djaletic", "Vuko", ""], ["Vukovic", "Jelena", ""]]}, {"id": "1902.05389", "submitter": "Jovan Radonjic", "authors": "Dejan Brajovic, Kristina Tomovic, Jovan Radonjic", "title": "Fingerprint Recognition under Missing Image Pixels Scenario", "comments": "Student paper, submitted to The 8th Mediterranean Conference on\n  Embedded Computing - MECO'2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work observed the problem of fingerprint image recognition in the case\nof missing pixels from the original image. The possibility of missing pixels\nrecovery is tested by applying the Compressive Sensing approach. Namely,\ndifferent percentage of missing pixels is observed and the image reconstruction\nis done by applying commonly used approach for sparse image reconstruction. The\ntheory is verified by experiments, showing successful image reconstruction and\nlater person identification even if less then 90% of the image pixels is\nmissing.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 09:42:17 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Brajovic", "Dejan", ""], ["Tomovic", "Kristina", ""], ["Radonjic", "Jovan", ""]]}, {"id": "1902.05390", "submitter": "Akanksha Joshi", "authors": "Abhishek Gangwar, Akanksha Joshi, Padmaja Joshi, R. Raghavendra", "title": "DeepIrisNet2: Learning Deep-IrisCodes from Scratch for\n  Segmentation-Robust Visible Wavelength and Near Infrared Iris Recognition", "comments": "10 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first, introduce a deep learning based framework named as DeepIrisNet2 for\nvisible spectrum and NIR Iris representation. The framework can work without\nclassical iris normalization step or very accurate iris segmentation; allowing\nto work under non-ideal situation. The framework contains spatial transformer\nlayers to handle deformation and supervision branches after certain\nintermediate layers to mitigate overfitting. In addition, we present a dual CNN\niris segmentation pipeline comprising of a iris/pupil bounding boxes detection\nnetwork and a semantic pixel-wise segmentation network. Furthermore, to get\ncompact templates, we present a strategy to generate binary iris codes using\nDeepIrisNet2. Since, no ground truth dataset are available for CNN training for\niris segmentation, We build large scale hand labeled datasets and make them\npublic; i) iris, pupil bounding boxes, ii) labeled iris texture. The networks\nare evaluated on challenging ND-IRIS-0405, UBIRIS.v2, MICHE-I, and CASIA v4\nInterval datasets. Proposed approach significantly improves the\nstate-of-the-art and achieve outstanding performance surpassing all previous\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 08:07:37 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Gangwar", "Abhishek", ""], ["Joshi", "Akanksha", ""], ["Joshi", "Padmaja", ""], ["Raghavendra", "R.", ""]]}, {"id": "1902.05391", "submitter": "Weisi Guo", "authors": "Arya Pamuncak, Weisi Guo, Ahmed Soliman Khaled, Irwanda Laory", "title": "Deep Learning for Bridge Load Capacity Estimation in Post-Disaster and\n  -Conflict Zones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many post-disaster and -conflict regions do not have sufficient data on their\ntransportation infrastructure assets, hindering both mobility and\nreconstruction. In particular, as the number of aging and deteriorating bridges\nincrease, it is necessary to quantify their load characteristics in order to\ninform maintenance and prevent failure. The load carrying capacity and the\ndesign load are considered as the main aspects of any civil structures. Human\nexamination can be costly and slow when expertise is lacking in challenging\nscenarios. In this paper, we propose to employ deep learning as method to\nestimate the load carrying capacity from crowd sourced images. A new\nconvolutional neural network architecture is trained on data from over 6000\nbridges, which will benefit future research and applications. We tackle\nsignificant variations in the dataset (e.g. class interval, image completion,\nimage colour) and quantify their impact on the prediction accuracy, precision,\nrecall and F1 score. Finally, practical optimisation is performed by converting\nmulticlass classification into binary classification to achieve a promising\nfield use performance.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 23:44:17 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Pamuncak", "Arya", ""], ["Guo", "Weisi", ""], ["Khaled", "Ahmed Soliman", ""], ["Laory", "Irwanda", ""]]}, {"id": "1902.05392", "submitter": "Serhan G\\\"ul", "authors": "Talmaj Marin\\v{c}, Vignesh Srinivasan, Serhan G\\\"ul, Cornelius Hellge,\n  Wojciech Samek", "title": "Multi-Kernel Prediction Networks for Denoising of Burst Images", "comments": "5 pages, 4 figures", "journal-ref": "2019 IEEE International Conference on Image Processing (ICIP), pp.\n  2404-2408", "doi": "10.1109/ICIP.2019.8803335", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In low light or short-exposure photography the image is often corrupted by\nnoise. While longer exposure helps reduce the noise, it can produce blurry\nresults due to the object and camera motion. The reconstruction of a noise-less\nimage is an ill posed problem. Recent approaches for image denoising aim to\npredict kernels which are convolved with a set of successively taken images\n(burst) to obtain a clear image. We propose a deep neural network based\napproach called Multi-Kernel Prediction Networks (MKPN) for burst image\ndenoising. MKPN predicts kernels of not just one size but of varying sizes and\nperforms fusion of these different kernels resulting in one kernel per pixel.\nThe advantages of our method are two fold: (a) the different sized kernels help\nin extracting different information from the image which results in better\nreconstruction and (b) kernel fusion assures retaining of the extracted\ninformation while maintaining computational efficiency. Experimental results\nreveal that MKPN outperforms state-of-the-art on our synthetic datasets with\ndifferent noise levels.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 22:29:09 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 14:57:46 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Marin\u010d", "Talmaj", ""], ["Srinivasan", "Vignesh", ""], ["G\u00fcl", "Serhan", ""], ["Hellge", "Cornelius", ""], ["Samek", "Wojciech", ""]]}, {"id": "1902.05394", "submitter": "Guoqiang Zhang", "authors": "Guoqiang Zhang, Haopeng Li and Fabian Wenger", "title": "Object Detection and 3D Estimation via an FMCW Radar Using a Fully\n  Convolutional Network", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers object detection and 3D estimation using an FMCW radar.\nThe state-of-the-art deep learning framework is employed instead of using\ntraditional signal processing. In preparing the radar training data, the ground\ntruth of an object orientation in 3D space is provided by conducting image\nanalysis, of which the images are obtained through a coupled camera to the\nradar device. To ensure successful training of a fully convolutional network\n(FCN), we propose a normalization method, which is found to be essential to be\napplied to the radar signal before feeding into the neural network. The system\nafter proper training is able to first detect the presence of an object in an\nenvironment. If it does, the system then further produces an estimation of its\n3D position. Experimental results show that the proposed system can be\nsuccessfully trained and employed for detecting a car and further estimating\nits 3D position in a noisy environment.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 10:56:20 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Zhang", "Guoqiang", ""], ["Li", "Haopeng", ""], ["Wenger", "Fabian", ""]]}, {"id": "1902.05395", "submitter": "Wanming Huang", "authors": "Wanming Huang, Yida Xu, Ian Oppermann", "title": "Realistic Image Generation using Region-phrase Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Generative Adversarial Network (GAN) has recently been applied to\ngenerate synthetic images from text. Despite significant advances, most current\nstate-of-the-art algorithms are regular-grid region based; when attention is\nused, it is mainly applied between individual regular-grid regions and a word.\nThese approaches are sufficient to generate images that contain a single object\nin its foreground, such as a \"bird\" or \"flower\". However, natural languages\noften involve complex foreground objects and the background may also constitute\na variable portion of the generated image. Therefore, the regular-grid based\nimage attention weights may not necessarily concentrate on the intended\nforeground region(s), which in turn, results in an unnatural looking image.\nAdditionally, individual words such as \"a\", \"blue\" and \"shirt\" do not\nnecessarily provide a full visual context unless they are applied together. For\nthis reason, in our paper, we proposed a novel method in which we introduced an\nadditional set of attentions between true-grid regions and word phrases. The\ntrue-grid region is derived using a set of auxiliary bounding boxes. These\nauxiliary bounding boxes serve as superior location indicators to where the\nalignment and attention should be drawn with the word phrases. Word phrases are\nderived from analysing Part-of-Speech (POS) results. We perform experiments on\nthis novel network architecture using the Microsoft Common Objects in Context\n(MSCOCO) dataset and the model generates $256 \\times 256$ conditioned on a\nshort sentence description. Our proposed approach is capable of generating more\nrealistic images compared with the current state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 11:23:00 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Huang", "Wanming", ""], ["Xu", "Yida", ""], ["Oppermann", "Ian", ""]]}, {"id": "1902.05396", "submitter": "Krishna Chaitanya", "authors": "Krishna Chaitanya, Neerav Karani, Christian Baumgartner, Olivio\n  Donati, Anton Becker, Ender Konukoglu", "title": "Semi-Supervised and Task-Driven Data Augmentation", "comments": "13 pages, 3 figures, 1 table, This article has been accepted at the\n  26th international conference on Information Processing in Medical Imaging\n  (IPMI) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised deep learning methods for segmentation require large amounts of\nlabelled training data, without which they are prone to overfitting, not\ngeneralizing well to unseen images. In practice, obtaining a large number of\nannotations from clinical experts is expensive and time-consuming. One way to\naddress scarcity of annotated examples is data augmentation using random\nspatial and intensity transformations. Recently, it has been proposed to use\ngenerative models to synthesize realistic training examples, complementing the\nrandom augmentation. So far, these methods have yielded limited gains over the\nrandom augmentation. However, there is potential to improve the approach by (i)\nexplicitly modeling deformation fields (non-affine spatial transformation) and\nintensity transformations and (ii) leveraging unlabelled data during the\ngenerative process. With this motivation, we propose a novel task-driven data\naugmentation method where to synthesize new training examples, a generative\nnetwork explicitly models and applies deformation fields and additive intensity\nmasks on existing labelled data, modeling shape and intensity variations,\nrespectively. Crucially, the generative model is optimized to be conducive to\nthe task, in this case segmentation, and constrained to match the distribution\nof images observed from labelled and unlabelled samples. Furthermore, explicit\nmodeling of deformation fields allow synthesizing segmentation masks and images\nin exact correspondence by simply applying the generated transformation to an\ninput image and the corresponding annotation. Our experiments on cardiac\nmagnetic resonance images (MRI) showed that, for the task of segmentation in\nsmall training data scenarios, the proposed method substantially outperforms\nconventional augmentation techniques.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 11:21:23 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 10:08:23 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Chaitanya", "Krishna", ""], ["Karani", "Neerav", ""], ["Baumgartner", "Christian", ""], ["Donati", "Olivio", ""], ["Becker", "Anton", ""], ["Konukoglu", "Ender", ""]]}, {"id": "1902.05399", "submitter": "Mohammad Tofighi", "authors": "Yuelong Li, Mohammad Tofighi, Vishal Monga and Yonina C. Eldar", "title": "An Algorithm Unrolling Approach to Deep Image Deblurring", "comments": "IEEE International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While neural networks have achieved vastly enhanced performance over\ntraditional iterative methods in many cases, they are generally empirically\ndesigned and the underlying structures are difficult to interpret. The\nalgorithm unrolling approach has helped connect iterative algorithms to neural\nnetwork architectures. However, such connections have not been made yet for\nblind image deblurring. In this paper, we propose a neural network architecture\nthat advances this idea. We first present an iterative algorithm that may be\nconsidered a generalization of the traditional total-variation regularization\nmethod on the gradient domain, and subsequently unroll the half-quadratic\nsplitting algorithm to construct a neural network. Our proposed deep network\nachieves significant practical performance gains while enjoying\ninterpretability at the same time. Experimental results show that our approach\noutperforms many state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 21:19:11 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 18:58:12 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Li", "Yuelong", ""], ["Tofighi", "Mohammad", ""], ["Monga", "Vishal", ""], ["Eldar", "Yonina C.", ""]]}, {"id": "1902.05400", "submitter": "Shayan Jawed", "authors": "Shayan Jawed, Eya Boumaiza, Josif Grabocka, Lars Schmidt-Thieme", "title": "Data-Driven Vehicle Trajectory Forecasting", "comments": "Published in ECML KNOWMe: 2nd International Workshop on Knowledge\n  Discovery from Mobility and Transportation Systems 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An active area of research is to increase the safety of self-driving\nvehicles. Although safety cannot be guarenteed completely, the capability of a\nvehicle to predict the future trajectories of its surrounding vehicles could\nhelp ensure this notion of safety to a greater deal. We cast the trajectory\nforecast problem in a multi-time step forecasting problem and develop a\nConvolutional Neural Network based approach to learn from trajectory sequences\ngenerated from completely raw dataset in real-time. Results show improvement\nover baselines.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 12:09:48 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Jawed", "Shayan", ""], ["Boumaiza", "Eya", ""], ["Grabocka", "Josif", ""], ["Schmidt-Thieme", "Lars", ""]]}, {"id": "1902.05401", "submitter": "Thiago Vinicius Machado De Souza", "authors": "Thiago V.M. Souza and Cleber Zanchettin", "title": "Improving Deep Image Clustering With Spatial Transformer Layers", "comments": null, "journal-ref": "Artificial Neural Networks and Machine Learning -- ICANN 2019:\n  Text and Time Series:641--654,2019", "doi": "10.1007/978-3-030-30490-4_51", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image clustering is an important but challenging task in machine learning. As\nin most image processing areas, the latest improvements came from models based\non the deep learning approach. However, classical deep learning methods have\nproblems to deal with spatial image transformations like scale and rotation. In\nthis paper, we propose the use of visual attention techniques to reduce this\nproblem in image clustering methods. We evaluate the combination of a deep\nimage clustering model called Deep Adaptive Clustering (DAC) with the Spatial\nTransformer Networks (STN). The proposed model is evaluated in the datasets\nMNIST and FashionMNIST and outperformed the baseline model.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 01:56:24 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 13:43:23 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Souza", "Thiago V. M.", ""], ["Zanchettin", "Cleber", ""]]}, {"id": "1902.05402", "submitter": "James Murphy", "authors": "James M. Murphy and Mauro Maggioni", "title": "Spectral-Spatial Diffusion Geometry for Hyperspectral Image Clustering", "comments": null, "journal-ref": null, "doi": "10.1109/LGRS.2019.2943001", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An unsupervised learning algorithm to cluster hyperspectral image (HSI) data\nis proposed that exploits spatially-regularized random walks. Markov diffusions\nare defined on the space of HSI spectra with transitions constrained to near\nspatial neighbors. The explicit incorporation of spatial regularity into the\ndiffusion construction leads to smoother random processes that are more adapted\nfor unsupervised machine learning than those based on spectra alone. The\nregularized diffusion process is subsequently used to embed the\nhigh-dimensional HSI into a lower dimensional space through diffusion\ndistances. Cluster modes are computed using density estimation and diffusion\ndistances, and all other points are labeled according to these modes. The\nproposed method has low computational complexity and performs competitively\nagainst state-of-the-art HSI clustering algorithms on real data. In particular,\nthe proposed spatial regularization confers an empirical advantage over\nnon-regularized methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 13:28:30 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Murphy", "James M.", ""], ["Maggioni", "Mauro", ""]]}, {"id": "1902.05408", "submitter": "Bob D. de Vos", "authors": "Bob D. de Vos, Jelmer M. Wolterink, Tim Leiner, Pim A. de Jong,\n  Nikolas Lessmann, Ivana Isgum", "title": "Direct Automatic Coronary Calcium Scoring in Cardiac and Chest CT", "comments": "IEEE Transactions on Medical Imaging (In press)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular disease (CVD) is the global leading cause of death. A strong\nrisk factor for CVD events is the amount of coronary artery calcium (CAC). To\nmeet demands of the increasing interest in quantification of CAC, i.e. coronary\ncalcium scoring, especially as an unrequested finding for screening and\nresearch, automatic methods have been proposed. Current automatic calcium\nscoring methods are relatively computationally expensive and only provide\nscores for one type of CT. To address this, we propose a computationally\nefficient method that employs two ConvNets: the first performs registration to\nalign the fields of view of input CTs and the second performs direct regression\nof the calcium score, thereby circumventing time-consuming intermediate CAC\nsegmentation. Optional decision feedback provides insight in the regions that\ncontributed to the calcium score. Experiments were performed using 903 cardiac\nCT and 1,687 chest CT scans. The method predicted calcium scores in less than\n0.3 s. Intra-class correlation coefficient between predicted and manual calcium\nscores was 0.98 for both cardiac and chest CT. The method showed almost perfect\nagreement between automatic and manual CVD risk categorization in both\ndatasets, with a linearly weighted Cohen's kappa of 0.95 in cardiac CT and 0.93\nin chest CT. Performance is similar to that of state-of-the-art methods, but\nthe proposed method is hundreds of times faster. By providing visual feedback,\ninsight is given in the decision process, making it readily implementable in\nclinical and research settings.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 21:50:21 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["de Vos", "Bob D.", ""], ["Wolterink", "Jelmer M.", ""], ["Leiner", "Tim", ""], ["de Jong", "Pim A.", ""], ["Lessmann", "Nikolas", ""], ["Isgum", "Ivana", ""]]}, {"id": "1902.05411", "submitter": "Ram Krishna Pandey", "authors": "Ram Krishna Pandey, Souvik Karmakar, A G Ramakrishnan and Nabagata\n  Saha", "title": "Improving Facial Emotion Recognition Systems Using Gradient and\n  Laplacian Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we have proposed several enhancements to improve the\nperformance of any facial emotion recognition (FER) system. We believe that the\nchanges in the positions of the fiducial points and the intensities capture the\ncrucial information regarding the emotion of a face image. We propose the use\nof the gradient and the Laplacian of the input image together with the original\ninput into a convolutional neural network (CNN). These modifications help the\nnetwork learn additional information from the gradient and Laplacian of the\nimages. However, the plain CNN is not able to extract this information from the\nraw images. We have performed a number of experiments on two well known\ndatasets KDEF and FERplus. Our approach enhances the already high performance\nof state-of-the-art FER systems by 3 to 5%.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 10:00:34 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Pandey", "Ram Krishna", ""], ["Karmakar", "Souvik", ""], ["Ramakrishnan", "A G", ""], ["Saha", "Nabagata", ""]]}, {"id": "1902.05413", "submitter": "Bo Feng", "authors": "Fanbo Sun, Zhixiang Gu, Bo Feng", "title": "Yelp Food Identification via Image Feature Extraction and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Yelp has been one of the most popular local service search engine in US since\n2004. It is powered by crowd-sourced text reviews and photo reviews. Restaurant\ncustomers and business owners upload photo images to Yelp, including reviewing\nor advertising either food, drinks, or inside and outside decorations. It is\nobviously not so effective that labels for food photos rely on human editors,\nwhich is an issue should be addressed by innovative machine learning\napproaches. In this paper, we present a simple but effective approach which can\nidentify up to ten kinds of food via raw photos from the challenge dataset. We\nuse 1) image pre-processing techniques, including filtering and image\naugmentation, 2) feature extraction via convolutional neural networks (CNN),\nand 3) three ways of classification algorithms. Then, we illustrate the\nclassification accuracy by tuning parameters for augmentations, CNN, and\nclassification. Our experimental results show this simple but effective\napproach to identify up to 10 food types from images.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 19:34:34 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Sun", "Fanbo", ""], ["Gu", "Zhixiang", ""], ["Feng", "Bo", ""]]}, {"id": "1902.05414", "submitter": "Marc Aubreville", "authors": "Marc Aubreville, Christof A. Bertram, Christian Marzahl, Corinne\n  Gurtner, Martina Dettwiler, Anja Schmidt, Florian Bartenschlager, Sophie\n  Merz, Marco Fragoso, Olivia Kershaw, Robert Klopfleisch and Andreas Maier", "title": "Deep learning algorithms out-perform veterinary pathologists in\n  detecting the mitotically most active tumor region", "comments": "13 pages, 7 figures", "journal-ref": "Sci Rep. 2020 Oct 5;10(1):16447", "doi": "10.1038/s41598-020-73246-2", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Manual count of mitotic figures, which is determined in the tumor region with\nthe highest mitotic activity, is a key parameter of most tumor grading schemes.\nIt can be, however, strongly dependent on the area selection due to uneven\nmitotic figure distribution in the tumor section.We aimed to assess the\nquestion, how significantly the area selection could impact the mitotic count,\nwhich has a known high inter-rater disagreement. On a data set of 32 whole\nslide images of H&E-stained canine cutaneous mast cell tumor, fully annotated\nfor mitotic figures, we asked eight veterinary pathologists (five\nboard-certified, three in training) to select a field of interest for the\nmitotic count. To assess the potential difference on the mitotic count, we\ncompared the mitotic count of the selected regions to the overall distribution\non the slide.Additionally, we evaluated three deep learning-based methods for\nthe assessment of highest mitotic density: In one approach, the model would\ndirectly try to predict the mitotic count for the presented image patches as a\nregression task. The second method aims at deriving a segmentation mask for\nmitotic figures, which is then used to obtain a mitotic density. Finally, we\nevaluated a two-stage object-detection pipeline based on state-of-the-art\narchitectures to identify individual mitotic figures. We found that the\npredictions by all models were, on average, better than those of the experts.\nThe two-stage object detector performed best and outperformed most of the human\npathologists on the majority of tumor cases. The correlation between the\npredicted and the ground truth mitotic count was also best for this approach\n(0.963 to 0.979). Further, we found considerable differences in position\nselection between pathologists, which could partially explain the high variance\nthat has been reported for the manual mitotic count.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 17:37:20 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 10:16:34 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 05:49:22 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Aubreville", "Marc", ""], ["Bertram", "Christof A.", ""], ["Marzahl", "Christian", ""], ["Gurtner", "Corinne", ""], ["Dettwiler", "Martina", ""], ["Schmidt", "Anja", ""], ["Bartenschlager", "Florian", ""], ["Merz", "Sophie", ""], ["Fragoso", "Marco", ""], ["Kershaw", "Olivia", ""], ["Klopfleisch", "Robert", ""], ["Maier", "Andreas", ""]]}, {"id": "1902.05429", "submitter": "Bin Song", "authors": "Sijia Chen, Bin Song, Xiaojiang Du, Nadra Guizani", "title": "Structured Bayesian Compression for Deep models in mobile enabled\n  devices for connected healthcare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Models, typically Deep neural networks, have millions of parameters,\nanalyze medical data accurately, yet in a time-consuming method. However,\nenergy cost effectiveness and computational efficiency are important for\nprerequisites developing and deploying mobile-enabled devices, the mainstream\ntrend in connected healthcare.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 09:09:52 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Chen", "Sijia", ""], ["Song", "Bin", ""], ["Du", "Xiaojiang", ""], ["Guizani", "Nadra", ""]]}, {"id": "1902.05431", "submitter": "Siyan Tao", "authors": "Siyan Tao, Yao Guo, Chuang Zhu, Huang Chen, Yue Zhang, Jie Yang and\n  Jun Liu", "title": "Highly Efficient Follicular Segmentation in Thyroid Cytopathological\n  Whole Slide Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method for highly efficient follicular\nsegmentation of thyroid cytopathological WSIs. Firstly, we propose a hybrid\nsegmentation architecture, which integrates a classifier into Deeplab V3 by\nadding a branch. A large amount of the WSI segmentation time is saved by\nskipping the irrelevant areas using the classification branch. Secondly, we\nmerge the low scale fine features into the original atrous spatial pyramid\npooling (ASPP) in Deeplab V3 to accurately represent the details in\ncytopathological images. Thirdly, our hybrid model is trained by a\ncriterion-oriented adaptive loss function, which leads the model converging\nmuch faster. Experimental results on a collection of thyroid patches\ndemonstrate that the proposed model reaches 80.9% on the segmentation accuracy.\nBesides, 93% time is reduced for the WSI segmentation by using our proposed\nmethod, and the WSI-level accuracy achieves 53.4%.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 10:16:54 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Tao", "Siyan", ""], ["Guo", "Yao", ""], ["Zhu", "Chuang", ""], ["Chen", "Huang", ""], ["Zhang", "Yue", ""], ["Yang", "Jie", ""], ["Liu", "Jun", ""]]}, {"id": "1902.05433", "submitter": "Swetava Ganguli", "authors": "Swetava Ganguli, Jared Dunnmon, Darren Hau", "title": "Predicting Food Security Outcomes Using Convolutional Neural Networks\n  (CNNs) for Satellite Tasking", "comments": "Research performed as part of the Sustainability and Artificial\n  Intelligence Laboratory (SAIL) at Stanford University. Second revised version\n  corrects typographical errors and adds a few references", "journal-ref": null, "doi": null, "report-no": "Prepared as submission for final project of the Fall 2016 offering\n  of CS 221 Artificial Intelligence at Stanford University", "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Obtaining reliable data describing local Food Security Metrics (FSM) at a\ngranularity that is informative to policy-makers requires expensive and\nlogistically difficult surveys, particularly in the developing world. We train\na CNN on publicly available satellite data describing land cover classification\nand use both transfer learning and direct training to build a model for FSM\nprediction purely from satellite imagery data. We then propose efficient\ntasking algorithms for high resolution satellite assets via transfer learning,\nMarkovian search algorithms, and Bayesian networks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 02:22:12 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 19:47:36 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Ganguli", "Swetava", ""], ["Dunnmon", "Jared", ""], ["Hau", "Darren", ""]]}, {"id": "1902.05437", "submitter": "Sirin Haddad", "authors": "Sirin Haddad, Meiqing Wu, He Wei, Siew Kei Lam", "title": "Situation-Aware Pedestrian Trajectory Prediction with Spatio-Temporal\n  Attention Model", "comments": null, "journal-ref": "in 24th Computer Vision Winter Workshop (CVWW), 2019, pp. 4-13", "doi": "10.3217/978-3-85125-652-9", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pedestrian trajectory prediction is essential for collision avoidance in\nautonomous driving and robot navigation. However, predicting a pedestrian's\ntrajectory in crowded environments is non-trivial as it is influenced by other\npedestrians' motion and static structures that are present in the scene. Such\nhuman-human and human-space interactions lead to non-linearities in the\ntrajectories. In this paper, we present a new spatio-temporal graph based Long\nShort-Term Memory (LSTM) network for predicting pedestrian trajectory in\ncrowded environments, which takes into account the interaction with static\n(physical objects) and dynamic (other pedestrians) elements in the scene. Our\nresults are based on two widely-used datasets to demonstrate that the proposed\nmethod outperforms the state-of-the-art approaches in human trajectory\nprediction. In particular, our method leads to a reduction in Average\nDisplacement Error (ADE) and Final Displacement Error (FDE) of up to 55% and\n61% respectively over state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 17:57:50 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Haddad", "Sirin", ""], ["Wu", "Meiqing", ""], ["Wei", "He", ""], ["Lam", "Siew Kei", ""]]}, {"id": "1902.05488", "submitter": "Ke Yang", "authors": "Ke Yang, Xiaolong Shen, Peng Qiao, Shijie Li, Dongsheng Li, Yong Dou", "title": "Exploring Frame Segmentation Networks for Temporal Action Localization", "comments": "Accepted by Journal of Visual Communication and Image Representation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action localization is an important task of computer vision. Though\nmany methods have been proposed, it still remains an open question how to\npredict the temporal location of action segments precisely. Most\nstate-of-the-art works train action classifiers on video segments\npre-determined by action proposal. However, recent work found that a desirable\nmodel should move beyond segment-level and make dense predictions at a fine\ngranularity in time to determine precise temporal boundaries. In this paper, we\npropose a Frame Segmentation Network (FSN) that places a temporal CNN on top of\nthe 2D spatial CNNs. Spatial CNNs are responsible for abstracting semantics in\nspatial dimension while temporal CNN is responsible for introducing temporal\ncontext information and performing dense predictions. The proposed FSN can make\ndense predictions at frame-level for a video clip using both spatial and\ntemporal context information. FSN is trained in an end-to-end manner, so the\nmodel can be optimized in spatial and temporal domain jointly. We also adapt\nFSN to use it in weakly supervised scenario (WFSN), where only video level\nlabels are provided when training. Experiment results on public dataset show\nthat FSN achieves superior performance in both frame-level action localization\nand temporal action localization.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 16:46:34 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Yang", "Ke", ""], ["Shen", "Xiaolong", ""], ["Qiao", "Peng", ""], ["Li", "Shijie", ""], ["Li", "Dongsheng", ""], ["Dou", "Yong", ""]]}, {"id": "1902.05492", "submitter": "Colin Samplawski", "authors": "Colin Samplawski, Heesung Kwon, Erik Learned-Miller, Benjamin M.\n  Marlin", "title": "Integrating Propositional and Relational Label Side Information for\n  Hierarchical Zero-Shot Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) is one of the most extreme forms of learning from\nscarce labeled data. It enables predicting that images belong to classes for\nwhich no labeled training instances are available. In this paper, we present a\nnew ZSL framework that leverages both label attribute side information and a\nsemantic label hierarchy. We present two methods, lifted zero-shot prediction\nand a custom conditional random field (CRF) model, that integrate both forms of\nside information. We propose benchmark tasks for this framework that focus on\nmaking predictions across a range of semantic levels. We show that lifted\nzero-shot prediction can dramatically outperform baseline methods when making\npredictions within specified semantic levels, and that the probability\ndistribution provided by the CRF model can be leveraged to yield further\nperformance improvements when making unconstrained predictions over the\nhierarchy.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 16:54:19 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Samplawski", "Colin", ""], ["Kwon", "Heesung", ""], ["Learned-Miller", "Erik", ""], ["Marlin", "Benjamin M.", ""]]}, {"id": "1902.05498", "submitter": "Thomio Watanabe", "authors": "Thomio Watanabe and Denis Wolf", "title": "Instance Segmentation as Image Segmentation Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The instance segmentation problem intends to precisely detect and delineate\nobjects in images. Most of the current solutions rely on deep convolutional\nneural networks but despite this fact proposed solutions are very diverse. Some\nsolutions approach the problem as a network problem, where they use several\nnetworks or specialize a single network to solve several tasks. A different\napproach tries to solve the problem as an annotation problem, where the\ninstance information is encoded in a mathematical representation. This work\nproposes a solution based in the DCME technique to solve the instance\nsegmentation with a single segmentation network. Different from others, the\nsegmentation network decoder is not specialized in a multi-task network.\nInstead, the network encoder is repurposed to classify image objects, reducing\nthe computational cost of the solution.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 10:53:25 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Watanabe", "Thomio", ""], ["Wolf", "Denis", ""]]}, {"id": "1902.05509", "submitter": "Maxim Berman", "authors": "Maxim Berman, Herv\\'e J\\'egou, Andrea Vedaldi, Iasonas Kokkinos,\n  Matthijs Douze", "title": "MultiGrain: a unified image embedding for classes and instances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MultiGrain is a network architecture producing compact vector representations\nthat are suited both for image classification and particular object retrieval.\nIt builds on a standard classification trunk. The top of the network produces\nan embedding containing coarse and fine-grained information, so that images can\nbe recognized based on the object class, particular object, or if they are\ndistorted copies. Our joint training is simple: we minimize a cross-entropy\nloss for classification and a ranking loss that determines if two images are\nidentical up to data augmentation, with no need for additional labels. A key\ncomponent of MultiGrain is a pooling layer that takes advantage of\nhigh-resolution images with a network trained at a lower resolution.\n  When fed to a linear classifier, the learned embeddings provide\nstate-of-the-art classification accuracy. For instance, we obtain 79.4% top-1\naccuracy with a ResNet-50 learned on Imagenet, which is a +1.8% absolute\nimprovement over the AutoAugment method. When compared with the cosine\nsimilarity, the same embeddings perform on par with the state-of-the-art for\nimage retrieval at moderate resolutions.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 17:32:21 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 08:07:29 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Berman", "Maxim", ""], ["J\u00e9gou", "Herv\u00e9", ""], ["Vedaldi", "Andrea", ""], ["Kokkinos", "Iasonas", ""], ["Douze", "Matthijs", ""]]}, {"id": "1902.05528", "submitter": "Ricardo Borsoi", "authors": "Ricardo Augusto Borsoi, Tales Imbiriba, Jos\\'e Carlos Moreira Bermudez", "title": "Deep Generative Endmember Modeling: An Application to Unsupervised\n  Spectral Unmixing", "comments": null, "journal-ref": null, "doi": "10.1109/TCI.2019.2948726", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Endmember (EM) spectral variability can greatly impact the performance of\nstandard hyperspectral image analysis algorithms. Extended parametric models\nhave been successfully applied to account for the EM spectral variability.\nHowever, these models still lack the compromise between flexibility and\nlow-dimensional representation that is necessary to properly explore the fact\nthat spectral variability is often confined to a low-dimensional manifold in\nreal scenes. In this paper we propose to learn a spectral variability model\ndirectly form the observed data, instead of imposing it \\emph{a priori}. This\nis achieved through a deep generative EM model, which is estimated using a\nvariational autoencoder (VAE). The encoder and decoder that compose the\ngenerative model are trained using pure pixel information extracted directly\nfrom the observed image, what allows for an unsupervised formulation. The\nproposed EM model is applied to the solution of a spectral unmixing problem,\nwhich we cast as an alternating nonlinear least-squares problem that is solved\niteratively with respect to the abundances and to the low-dimensional\nrepresentations of the EMs in the latent space of the deep generative model.\nSimulations using both synthetic and real data indicate that the proposed\nstrategy can outperform the competing state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 18:08:22 GMT"}, {"version": "v2", "created": "Sat, 31 Aug 2019 17:22:07 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 15:24:26 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Borsoi", "Ricardo Augusto", ""], ["Imbiriba", "Tales", ""], ["Bermudez", "Jos\u00e9 Carlos Moreira", ""]]}, {"id": "1902.05542", "submitter": "Tianhe Yu", "authors": "Tianhe Yu, Gleb Shevchuk, Dorsa Sadigh, Chelsea Finn", "title": "Unsupervised Visuomotor Control through Distributional Planning Networks", "comments": "Videos available at https://sites.google.com/view/dpn-public/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While reinforcement learning (RL) has the potential to enable robots to\nautonomously acquire a wide range of skills, in practice, RL usually requires\nmanual, per-task engineering of reward functions, especially in real world\nsettings where aspects of the environment needed to compute progress are not\ndirectly accessible. To enable robots to autonomously learn skills, we instead\nconsider the problem of reinforcement learning without access to rewards. We\naim to learn an unsupervised embedding space under which the robot can measure\nprogress towards a goal for itself. Our approach explicitly optimizes for a\nmetric space under which action sequences that reach a particular state are\noptimal when the goal is the final state reached. This enables learning\neffective and control-centric representations that lead to more autonomous\nreinforcement learning algorithms. Our experiments on three simulated\nenvironments and two real-world manipulation problems show that our method can\nlearn effective goal metrics from unlabeled interaction, and use the learned\ngoal metrics for autonomous reinforcement learning.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 18:54:54 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Yu", "Tianhe", ""], ["Shevchuk", "Gleb", ""], ["Sadigh", "Dorsa", ""], ["Finn", "Chelsea", ""]]}, {"id": "1902.05546", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Chris Lu, Trevor Darrell, Phillip Isola, Alexei A.\n  Efros", "title": "Learning to Control Self-Assembling Morphologies: A Study of\n  Generalization via Modularity", "comments": "NeurIPS 2019 (Spotlight). Videos at\n  https://pathak22.github.io/modular-assemblies/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary sensorimotor learning approaches typically start with an\nexisting complex agent (e.g., a robotic arm), which they learn to control. In\ncontrast, this paper investigates a modular co-evolution strategy: a collection\nof primitive agents learns to dynamically self-assemble into composite bodies\nwhile also learning to coordinate their behavior to control these bodies. Each\nprimitive agent consists of a limb with a motor attached at one end. Limbs may\nchoose to link up to form collectives. When a limb initiates a link-up action,\nand there is another limb nearby, the latter is magnetically connected to the\n'parent' limb's motor. This forms a new single agent, which may further link\nwith other agents. In this way, complex morphologies can emerge, controlled by\na policy whose architecture is in explicit correspondence with the morphology.\nWe evaluate the performance of these dynamic and modular agents in simulated\nenvironments. We demonstrate better generalization to test-time changes both in\nthe environment, as well as in the structure of the agent, compared to static\nand monolithic baselines. Project video and code are available at\nhttps://pathak22.github.io/modular-assemblies/\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 18:59:05 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 21:35:27 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Pathak", "Deepak", ""], ["Lu", "Chris", ""], ["Darrell", "Trevor", ""], ["Isola", "Phillip", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1902.05581", "submitter": "Wenju Xu", "authors": "Wenju Xu and Shawn Keshmiri and Guanghui Wang", "title": "Adversarially Approximated Autoencoder for Image Generation and\n  Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized autoencoders learn the latent codes, a structure with the\nregularization under the distribution, which enables them the capability to\ninfer the latent codes given observations and generate new samples given the\ncodes. However, they are sometimes ambiguous as they tend to produce\nreconstructions that are not necessarily faithful reproduction of the inputs.\nThe main reason is to enforce the learned latent code distribution to match a\nprior distribution while the true distribution remains unknown. To improve the\nreconstruction quality and learn the latent space a manifold structure, this\nwork present a novel approach using the adversarially approximated autoencoder\n(AAAE) to investigate the latent codes with adversarial approximation. Instead\nof regularizing the latent codes by penalizing on the distance between the\ndistributions of the model and the target, AAAE learns the autoencoder flexibly\nand approximates the latent space with a simpler generator. The ratio is\nestimated using generative adversarial network (GAN) to enforce the similarity\nof the distributions. Additionally, the image space is regularized with an\nadditional adversarial regularizer. The proposed approach unifies two deep\ngenerative models for both latent space inference and diverse generation. The\nlearning scheme is realized without regularization on the latent codes, which\nalso encourages faithful reconstruction. Extensive validation experiments on\nfour real-world datasets demonstrate the superior performance of AAAE. In\ncomparison to the state-of-the-art approaches, AAAE generates samples with\nbetter quality and shares the properties of regularized autoencoder with a nice\nlatent manifold structure.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 19:54:13 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Xu", "Wenju", ""], ["Keshmiri", "Shawn", ""], ["Wang", "Guanghui", ""]]}, {"id": "1902.05582", "submitter": "Hongxu Yang", "authors": "Hongxu Yang, Caifeng Shan, Alexander F. Kolen, Peter H.N. de With", "title": "Improving Catheter Segmentation & Localization in 3D Cardiac Ultrasound\n  Using Direction-Fused FCN", "comments": "ISBI 2019 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and accurate catheter detection in cardiac catheterization using\nharmless 3D ultrasound (US) can improve the efficiency and outcome of the\nintervention. However, the low image quality of US requires extra training for\nsonographers to localize the catheter. In this paper, we propose a catheter\ndetection method based on a pre-trained VGG network, which exploits 3D\ninformation through re-organized cross-sections to segment the catheter by a\nshared fully convolutional network (FCN), which is called a Direction-Fused FCN\n(DF-FCN). Based on the segmented image of DF-FCN, the catheter can be localized\nby model fitting. Our experiments show that the proposed method can\nsuccessfully detect an ablation catheter in a challenging ex-vivo 3D US\ndataset, which was collected on the porcine heart. Extensive analysis shows\nthat the proposed method achieves a Dice score of 57.7%, which offers at least\nan 11.8 % improvement when compared to state-of-the-art instrument detection\nmethods. Due to the improved segmentation performance by the DF-FCN, the\ncatheter can be localized with an error of only 1.4 mm.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 19:57:14 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Yang", "Hongxu", ""], ["Shan", "Caifeng", ""], ["Kolen", "Alexander F.", ""], ["de With", "Peter H. N.", ""]]}, {"id": "1902.05586", "submitter": "Brent Lagesse", "authors": "Cody Burkard, Brent Lagesse", "title": "Can Intelligent Hyperparameter Selection Improve Resistance to\n  Adversarial Examples?", "comments": "37 pages, 11 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks and Deep Learning classification systems in\ngeneral have been shown to be vulnerable to attack by specially crafted data\nsamples that appear to belong to one class but are instead classified as\nanother, commonly known as adversarial examples. A variety of attack strategies\nhave been proposed to craft these samples; however, there is no standard model\nthat is used to compare the success of each type of attack. Furthermore, there\nis no literature currently available that evaluates how common hyperparameters\nand optimization strategies may impact a model's ability to resist these\nsamples. This research bridges that lack of awareness and provides a means for\nthe selection of training and model parameters in future research on evasion\nattacks against convolutional neural networks. The findings of this work\nindicate that the selection of model hyperparameters does impact the ability of\na model to resist attack, although they alone cannot prevent the existence of\nadversarial examples.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 20:11:12 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Burkard", "Cody", ""], ["Lagesse", "Brent", ""]]}, {"id": "1902.05611", "submitter": "Swetava Ganguli", "authors": "Swetava Ganguli, Pedro Garzon, Noa Glaser", "title": "GeoGAN: A Conditional GAN with Reconstruction and Style Loss to Generate\n  Standard Layer of Maps from Satellite Images", "comments": "Version 2 of paper submitted incorporating minor revisions. Corrected\n  typographical errors and added some additional references", "journal-ref": null, "doi": null, "report-no": "Final report for research conducted as part of the Fall 2018\n  offering of CS 236 Deep Generative Models at Stanford University", "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatically generating maps from satellite images is an important task.\nThere is a body of literature which tries to address this challenge. We created\na more expansive survey of the task by experimenting with different models and\nadding new loss functions to improve results. We created a database of pairs of\nsatellite images and the corresponding map of the area. Our model translates\nthe satellite image to the corresponding standard layer map image using three\nmain model architectures: (i) a conditional Generative Adversarial Network\n(GAN) which compresses the images down to a learned embedding, (ii) a generator\nwhich is trained as a normalizing flow (RealNVP) model, and (iii) a conditional\nGAN where the generator translates via a series of convolutions to the standard\nlayer of a map and the discriminator input is the concatenation of the\nreal/generated map and the satellite image. Model (iii) was by far the most\npromising of three models. To improve the results we also added a\nreconstruction loss and style transfer loss in addition to the GAN losses. The\nthird model architecture produced the best quality of sampled images. In\ncontrast to the other generative model where evaluation of the model is a\nchallenging problem. since we have access to the real map for a given satellite\nimage, we are able to assign a quantitative metric to the quality of the\ngenerated images in addition to inspecting them visually. While we are\ncontinuing to work on increasing the accuracy of the model, one challenge has\nbeen the coarse resolution of the data which upper-bounds the quality of the\nresults of our model. Nevertheless, as will be seen in the results, the\ngenerated map is more accurate in the features it produces since the generator\narchitecture demands a pixel-wise image translation/pixel-wise coloring. A\nvideo presentation summarizing this paper is available at:\nhttps://youtu.be/Ur0flOX-Ji0\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 21:41:18 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 18:54:53 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Ganguli", "Swetava", ""], ["Garzon", "Pedro", ""], ["Glaser", "Noa", ""]]}, {"id": "1902.05655", "submitter": "Naveed Akhtar Dr.", "authors": "Fouzia Altaf, Syed M. S. Islam, Naveed Akhtar, Naeem K. Janjua", "title": "Going Deep in Medical Image Analysis: Concepts, Methods, Challenges and\n  Future Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical Image Analysis is currently experiencing a paradigm shift due to Deep\nLearning. This technology has recently attracted so much interest of the\nMedical Imaging community that it led to a specialized conference in `Medical\nImaging with Deep Learning' in the year 2018. This article surveys the recent\ndevelopments in this direction, and provides a critical review of the related\nmajor aspects. We organize the reviewed literature according to the underlying\nPattern Recognition tasks, and further sub-categorize it following a taxonomy\nbased on human anatomy. This article does not assume prior knowledge of Deep\nLearning and makes a significant contribution in explaining the core Deep\nLearning concepts to the non-experts in the Medical community. Unique to this\nstudy is the Computer Vision/Machine Learning perspective taken on the advances\nof Deep Learning in Medical Imaging. This enables us to single out `lack of\nappropriately annotated large-scale datasets' as the core challenge (among\nother challenges) in this research direction. We draw on the insights from the\nsister research fields of Computer Vision, Pattern Recognition and Machine\nLearning etc.; where the techniques of dealing with such challenges have\nalready matured, to provide promising directions for the Medical Imaging\ncommunity to fully harness Deep Learning in the future.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 01:05:25 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Altaf", "Fouzia", ""], ["Islam", "Syed M. S.", ""], ["Akhtar", "Naveed", ""], ["Janjua", "Naeem K.", ""]]}, {"id": "1902.05657", "submitter": "Somdip Dey Mr.", "authors": "Somdip Dey, Amit K. Singh, Dilip K. Prasad, Klaus D. McDonald-Maier", "title": "TMAV: Temporal Motionless Analysis of Video using CNN in MPSoC", "comments": "11 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing video for traffic categorization is an important pillar of\nIntelligent Transport Systems. However, it is difficult to analyze and predict\ntraffic based on image frames because the representation of each frame may vary\nsignificantly within a short time period. This also would inaccurately\nrepresent the traffic over a longer period of time such as the case of video.\nWe propose a novel bio-inspired methodology that integrates analysis of the\nprevious image frames of the video to represent the analysis of the current\nimage frame, the same way a human being analyzes the current situation based on\npast experience. In our proposed methodology, called IRON-MAN (Integrated\nRational prediction and Motionless ANalysis), we utilize Bayesian update on top\nof the individual image frame analysis in the videos and this has resulted in\nhighly accurate prediction of Temporal Motionless Analysis of the Videos (TMAV)\nfor most of the chosen test cases. The proposed approach could be used for TMAV\nusing Convolutional Neural Network (CNN) for applications where the number of\nobjects in an image is the deciding factor for prediction and results also show\nthat our proposed approach outperforms the state-of-the-art for the chosen test\ncase. We also introduce a new metric named, Energy Consumption per Training\nImage (ECTI). Since, different CNN based models have different training\ncapability and computing resource utilization, some of the models are more\nsuitable for embedded device implementation than the others, and ECTI metric is\nuseful to assess the suitability of using a CNN model in multi-processor\nsystems-on-chips (MPSoCs) with a focus on energy consumption and reliability in\nterms of lifespan of the embedded device using these MPSoCs.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 01:14:04 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 15:16:30 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Dey", "Somdip", ""], ["Singh", "Amit K.", ""], ["Prasad", "Dilip K.", ""], ["McDonald-Maier", "Klaus D.", ""]]}, {"id": "1902.05659", "submitter": "Julian Yarkony", "authors": "Margret Keuper, Jovita Lukasik, Maneesh Singh, Julian Yarkony", "title": "Massively Parallel Benders Decomposition for Correlation Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of graph partitioning for image segmentation using\ncorrelation clustering (CC), which we treat as an integer linear program (ILP).\nWe reformulate optimization in the ILP so as to admit efficient optimization\nvia Benders decomposition, a classic technique from operations research. Our\nBenders decomposition formulation has many subproblems, each associated with a\nnode in the CC instance's graph, which are solved in parallel. Each Benders\nsubproblem enforces the cycle inequalities corresponding to the negative weight\nedges attached to its corresponding node in the CC instance. We generate\nMagnanti-Wong Benders rows in addition to standard Benders rows, to accelerate\noptimization. Our Benders decomposition approach provides a promising new\navenue to accelerate optimization for CC, and allows for massive\nparallelization.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 01:50:07 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 17:44:22 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Keuper", "Margret", ""], ["Lukasik", "Jovita", ""], ["Singh", "Maneesh", ""], ["Yarkony", "Julian", ""]]}, {"id": "1902.05660", "submitter": "Meet Pragnesh Shah", "authors": "Meet Shah, Xinlei Chen, Marcus Rohrbach, Devi Parikh", "title": "Cycle-Consistency for Robust Visual Question Answering", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress in Visual Question Answering over the years,\nrobustness of today's VQA models leave much to be desired. We introduce a new\nevaluation protocol and associated dataset (VQA-Rephrasings) and show that\nstate-of-the-art VQA models are notoriously brittle to linguistic variations in\nquestions. VQA-Rephrasings contains 3 human-provided rephrasings for 40k\nquestions spanning 40k images from the VQA v2.0 validation dataset. As a step\ntowards improving robustness of VQA models, we propose a model-agnostic\nframework that exploits cycle consistency. Specifically, we train a model to\nnot only answer a question, but also generate a question conditioned on the\nanswer, such that the answer predicted for the generated question is the same\nas the ground truth answer to the original question. Without the use of\nadditional annotations, we show that our approach is significantly more robust\nto linguistic variations than state-of-the-art VQA models, when evaluated on\nthe VQA-Rephrasings dataset. In addition, our approach outperforms\nstate-of-the-art approaches on the standard VQA and Visual Question Generation\ntasks on the challenging VQA v2.0 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 02:07:18 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Shah", "Meet", ""], ["Chen", "Xinlei", ""], ["Rohrbach", "Marcus", ""], ["Parikh", "Devi", ""]]}, {"id": "1902.05672", "submitter": "Hao Zhu", "authors": "Hao Zhu, Mantang Guo, Hongdong Li, Qing Wang, Antonio Robles-Kelly", "title": "Breaking the Spatio-Angular Trade-off for Light Field Super-Resolution\n  via LSTM Modelling on Epipolar Plane Images", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light-field cameras (LFC) have received increasing attention due to their\nwide-spread applications. However, current LFCs suffer from the well-known\nspatio-angular trade-off, which is considered as an inherent and fundamental\nlimit for LFC designs. In this paper, by doing a detailed geometrical optical\nanalysis of the sampling process in an LFC, we show that the effective sampling\nresolution is generally higher than the number of micro-lenses. This\ncontribution makes it theoretically possible to break the resolution trade-off.\nOur second contribution is an epipolar plane image (EPI) based super-resolution\nmethod, which can super-resolve the spatial and angular dimensions\nsimultaneously. We prove that the light field is a 2D series, thus, a\nspecifically designed CNN-LSTM network is proposed to capture the continuity\nproperty of the EPI. Rather than leveraging semantic information, our network\nfocuses on extracting geometric continuity in the EPI. This gives our method an\nimproved generalization ability and makes it applicable to a wide range of\npreviously unseen scenes. Experiments on both synthetic and real light fields\ndemonstrate the improvements over state-of-the-art, especially in large\ndisparity areas.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 03:36:00 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Zhu", "Hao", ""], ["Guo", "Mantang", ""], ["Li", "Hongdong", ""], ["Wang", "Qing", ""], ["Robles-Kelly", "Antonio", ""]]}, {"id": "1902.05687", "submitter": "Zhiming Zhou", "authors": "Zhiming Zhou, Jiadong Liang, Yuxuan Song, Lantao Yu, Hongwei Wang,\n  Weinan Zhang, Yong Yu, Zhihua Zhang", "title": "Lipschitz Generative Adversarial Nets", "comments": "Published as a conference paper at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the convergence of generative adversarial networks\n(GANs) from the perspective of the informativeness of the gradient of the\noptimal discriminative function. We show that GANs without restriction on the\ndiscriminative function space commonly suffer from the problem that the\ngradient produced by the discriminator is uninformative to guide the generator.\nBy contrast, Wasserstein GAN (WGAN), where the discriminative function is\nrestricted to 1-Lipschitz, does not suffer from such a gradient\nuninformativeness problem. We further show in the paper that the model with a\ncompact dual form of Wasserstein distance, where the Lipschitz condition is\nrelaxed, may also theoretically suffer from this issue. This implies the\nimportance of Lipschitz condition and motivates us to study the general\nformulation of GANs with Lipschitz constraint, which leads to a new family of\nGANs that we call Lipschitz GANs (LGANs). We show that LGANs guarantee the\nexistence and uniqueness of the optimal discriminative function as well as the\nexistence of a unique Nash equilibrium. We prove that LGANs are generally\ncapable of eliminating the gradient uninformativeness problem. According to our\nempirical analysis, LGANs are more stable and generate consistently higher\nquality samples compared with WGAN.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 05:19:21 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 11:45:44 GMT"}, {"version": "v3", "created": "Thu, 16 May 2019 04:59:42 GMT"}, {"version": "v4", "created": "Mon, 24 Jun 2019 07:55:51 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Zhou", "Zhiming", ""], ["Liang", "Jiadong", ""], ["Song", "Yuxuan", ""], ["Yu", "Lantao", ""], ["Wang", "Hongwei", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1902.05694", "submitter": "Wei Wang", "authors": "Wenming Yang, Wei Wang, Xuechen Zhang, Shuifa Sun, Qingmin Liao", "title": "Lightweight Feature Fusion Network for Single Image Super-Resolution", "comments": "Accepted by IEEE Signal Processing Letters (Volume:26, Issue:4, April\n  2019)", "journal-ref": null, "doi": "10.1109/LSP.2018.2890770", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution(SISR) has witnessed great progress as\nconvolutional neural network(CNN) gets deeper and wider. However, enormous\nparameters hinder its application to real world problems. In this letter, We\npropose a lightweight feature fusion network (LFFN) that can fully explore\nmulti-scale contextual information and greatly reduce network parameters while\nmaximizing SISR results. LFFN is built on spindle blocks and a softmax feature\nfusion module (SFFM). Specifically, a spindle block is composed of a dimension\nextension unit, a feature exploration unit and a feature refinement unit. The\ndimension extension layer expands low dimension to high dimension and\nimplicitly learns the feature maps which is suitable for the next unit. The\nfeature exploration unit performs linear and nonlinear feature exploration\naimed at different feature maps. The feature refinement layer is used to fuse\nand refine features. SFFM fuses the features from different modules in a\nself-adaptive learning manner with softmax function, making full use of\nhierarchical information with a small amount of parameter cost. Both\nqualitative and quantitative experiments on benchmark datasets show that LFFN\nachieves favorable performance against state-of-the-art methods with similar\nparameters.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 05:55:45 GMT"}, {"version": "v2", "created": "Sat, 13 Apr 2019 08:42:55 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Yang", "Wenming", ""], ["Wang", "Wei", ""], ["Zhang", "Xuechen", ""], ["Sun", "Shuifa", ""], ["Liao", "Qingmin", ""]]}, {"id": "1902.05703", "submitter": "James Harrison", "authors": "Sandeep Chinchali, Apoorva Sharma, James Harrison, Amine Elhafsi,\n  Daniel Kang, Evgenya Pergament, Eyal Cidon, Sachin Katti, Marco Pavone", "title": "Network Offloading Policies for Cloud Robotics: a Learning-based\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's robotic systems are increasingly turning to computationally expensive\nmodels such as deep neural networks (DNNs) for tasks like localization,\nperception, planning, and object detection. However, resource-constrained\nrobots, like low-power drones, often have insufficient on-board compute\nresources or power reserves to scalably run the most accurate, state-of-the art\nneural network compute models. Cloud robotics allows mobile robots the benefit\nof offloading compute to centralized servers if they are uncertain locally or\nwant to run more accurate, compute-intensive models. However, cloud robotics\ncomes with a key, often understated cost: communicating with the cloud over\ncongested wireless networks may result in latency or loss of data. In fact,\nsending high data-rate video or LIDAR from multiple robots over congested\nnetworks can lead to prohibitive delay for real-time applications, which we\nmeasure experimentally. In this paper, we formulate a novel Robot Offloading\nProblem --- how and when should robots offload sensing tasks, especially if\nthey are uncertain, to improve accuracy while minimizing the cost of cloud\ncommunication? We formulate offloading as a sequential decision making problem\nfor robots, and propose a solution using deep reinforcement learning. In both\nsimulations and hardware experiments using state-of-the art vision DNNs, our\noffloading strategy improves vision task performance by between 1.3-2.6x of\nbenchmark offloading strategies, allowing robots the potential to significantly\ntranscend their on-board sensing accuracy but with limited cost of cloud\ncommunication.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 06:34:31 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Chinchali", "Sandeep", ""], ["Sharma", "Apoorva", ""], ["Harrison", "James", ""], ["Elhafsi", "Amine", ""], ["Kang", "Daniel", ""], ["Pergament", "Evgenya", ""], ["Cidon", "Eyal", ""], ["Katti", "Sachin", ""], ["Pavone", "Marco", ""]]}, {"id": "1902.05811", "submitter": "Qiao Zheng", "authors": "Qiao Zheng, Herv\\'e Delingette, Kenneth Fung, Steffen E. Petersen,\n  Nicholas Ayache", "title": "Unsupervised shape and motion analysis of 3822 cardiac 4D MRIs of UK\n  Biobank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform unsupervised analysis of image-derived shape and motion features\nextracted from 3822 cardiac 4D MRIs of the UK Biobank. First, with a feature\nextraction method previously published based on deep learning models, we\nextract from each case 9 feature values characterizing both the cardiac shape\nand motion. Second, a feature selection is performed to remove highly\ncorrelated feature pairs. Third, clustering is carried out using a Gaussian\nmixture model on the selected features. After analysis, we identify two small\nclusters which probably correspond to two pathological categories. Further\nconfirmation using a trained classification model and dimensionality reduction\ntools is carried out to support this discovery. Moreover, we examine the\ndifferences between the other large clusters and compare our measures with the\nground-truth.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 13:56:04 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Zheng", "Qiao", ""], ["Delingette", "Herv\u00e9", ""], ["Fung", "Kenneth", ""], ["Petersen", "Steffen E.", ""], ["Ayache", "Nicholas", ""]]}, {"id": "1902.05818", "submitter": "Rui Cao", "authors": "Rui Cao, Qian Zhang, Jiasong Zhu, Qing Li, Qingquan Li, Bozhi Liu, and\n  Guoping Qiu", "title": "Enhancing Remote Sensing Image Retrieval with Triplet Deep Metric\n  Learning Network", "comments": "5 pages, 7 figures, 3 tables", "journal-ref": "International Journal of Remote Sensing, 2020, Vol. 41, No. 2, pp.\n  740-751", "doi": "10.1080/2150704X.2019.1647368", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growing of remotely sensed imagery data, there is a high\ndemand for effective and efficient image retrieval tools to manage and exploit\nsuch data. In this letter, we present a novel content-based remote sensing\nimage retrieval method based on Triplet deep metric learning convolutional\nneural network (CNN). By constructing a Triplet network with metric learning\nobjective function, we extract the representative features of the images in a\nsemantic space in which images from the same class are close to each other\nwhile those from different classes are far apart. In such a semantic space,\nsimple metric measures such as Euclidean distance can be used directly to\ncompare the similarity of images and effectively retrieve images of the same\nclass. We also investigate a supervised and an unsupervised learning methods\nfor reducing the dimensionality of the learned semantic features. We present\ncomprehensive experimental results on two publicly available remote sensing\nimage retrieval datasets and show that our method significantly outperforms\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 14:29:02 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Cao", "Rui", ""], ["Zhang", "Qian", ""], ["Zhu", "Jiasong", ""], ["Li", "Qing", ""], ["Li", "Qingquan", ""], ["Liu", "Bozhi", ""], ["Qiu", "Guoping", ""]]}, {"id": "1902.05829", "submitter": "Vassilis Pitsikalis", "authors": "Nikolaos Gkanatsios, Vassilis Pitsikalis, Petros Koutras, Athanasia\n  Zlatintsi, Petros Maragos", "title": "Deeply Supervised Multimodal Attentional Translation Embeddings for\n  Visual Relationship Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting visual relationships, i.e. <Subject, Predicate, Object> triplets,\nis a challenging Scene Understanding task approached in the past via linguistic\npriors or spatial information in a single feature branch. We introduce a new\ndeeply supervised two-branch architecture, the Multimodal Attentional\nTranslation Embeddings, where the visual features of each branch are driven by\na multimodal attentional mechanism that exploits spatio-linguistic similarities\nin a low-dimensional space. We present a variety of experiments comparing\nagainst all related approaches in the literature, as well as by re-implementing\nand fine-tuning several of them. Results on the commonly employed VRD dataset\n[1] show that the proposed method clearly outperforms all others, while we also\njustify our claims both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 14:53:40 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Gkanatsios", "Nikolaos", ""], ["Pitsikalis", "Vassilis", ""], ["Koutras", "Petros", ""], ["Zlatintsi", "Athanasia", ""], ["Maragos", "Petros", ""]]}, {"id": "1902.05839", "submitter": "Janek Gr\\\"ohl", "authors": "Janek Gr\\\"ohl, Thomas Kirchner, Tim Adler, Lena Maier-Hein", "title": "Estimation of blood oxygenation with learned spectral decoloring for\n  quantitative photoacoustic imaging (LSD-qPAI)", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main applications of photoacoustic (PA) imaging is the recovery of\nfunctional tissue properties, such as blood oxygenation (sO2). This is\ntypically achieved by linear spectral unmixing of relevant chromophores from\nmultispectral photoacoustic images. Despite the progress that has been made\ntowards quantitative PA imaging (qPAI), most sO2 estimation methods yield poor\nresults in realistic settings. In this work, we tackle the challenge by\nemploying learned spectral decoloring for quantitative photoacoustic imaging\n(LSD-qPAI) to obtain quantitative estimates for blood oxygenation. LSD-qPAI\ncomputes sO2 directly from pixel-wise initial pressure spectra Sp0, which are\nvectors comprised of the initial pressure at the same spatial location over all\nrecorded wavelengths. Initial results suggest that LSD-qPAI is able to obtain\naccurate sO2 estimates directly from multispectral photoacoustic measurements\nin silico and plausible estimates in vivo.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 15:15:11 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Gr\u00f6hl", "Janek", ""], ["Kirchner", "Thomas", ""], ["Adler", "Tim", ""], ["Maier-Hein", "Lena", ""]]}, {"id": "1902.05872", "submitter": "Michael Jones", "authors": "Bharathkumar Ramachandra, Michael Jones", "title": "Street Scene: A new dataset and evaluation protocol for video anomaly\n  detection", "comments": "accepted to WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in video anomaly detection research is currently slowed by small\ndatasets that lack a wide variety of activities as well as flawed evaluation\ncriteria. This paper aims to help move this research effort forward by\nintroducing a large and varied new dataset called Street Scene, as well as two\nnew evaluation criteria that provide a better estimate of how an algorithm will\nperform in practice. In addition to the new dataset and evaluation criteria, we\npresent two variations of a novel baseline video anomaly detection algorithm\nand show they are much more accurate on Street Scene than two state-of-the-art\nalgorithms from the literature.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 16:18:36 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 18:04:15 GMT"}, {"version": "v3", "created": "Fri, 24 Jan 2020 15:25:39 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Ramachandra", "Bharathkumar", ""], ["Jones", "Michael", ""]]}, {"id": "1902.05908", "submitter": "Naimul Khan", "authors": "Naimul Mefraz Khan, Nabila Abraham, Ling Guan", "title": "Machine Learning on Biomedical Images: Interactive Learning, Transfer\n  Learning, Class Imbalance, and Beyond", "comments": "Accepted at IEEE MIPR 2019. arXiv admin note: text overlap with\n  arXiv:1810.07842", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we highlight three issues that limit performance of machine\nlearning on biomedical images, and tackle them through 3 case studies: 1)\nInteractive Machine Learning (IML): we show how IML can drastically improve\nexploration time and quality of direct volume rendering. 2) transfer learning:\nwe show how transfer learning along with intelligent pre-processing can result\nin better Alzheimer's diagnosis using a much smaller training set 3) data\nimbalance: we show how our novel focal Tversky loss function can provide better\nsegmentation results taking into account the imbalanced nature of segmentation\ndatasets. The case studies are accompanied by in-depth analytical discussion of\nresults with possible future directions.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 21:23:07 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Khan", "Naimul Mefraz", ""], ["Abraham", "Nabila", ""], ["Guan", "Ling", ""]]}, {"id": "1902.05947", "submitter": "Fereshteh Sadeghi", "authors": "Fereshteh Sadeghi", "title": "DIViS: Domain Invariant Visual Servoing for Collision-Free Goal Reaching", "comments": "Supplementary videos: https://fsadeghi.github.io/DIViS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots should understand both semantics and physics to be functional in the\nreal world. While robot platforms provide means for interacting with the\nphysical world they cannot autonomously acquire object-level semantics without\nneeding human. In this paper, we investigate how to minimize human effort and\nintervention to teach robots perform real world tasks that incorporate\nsemantics. We study this question in the context of visual servoing of mobile\nrobots and propose DIViS, a Domain Invariant policy learning approach for\ncollision free Visual Servoing. DIViS incorporates high level semantics from\npreviously collected static human-labeled datasets and learns collision free\nservoing entirely in simulation and without any real robot data. However, DIViS\ncan directly be deployed on a real robot and is capable of servoing to the\nuser-specified object categories while avoiding collisions in the real world.\nDIViS is not constrained to be queried by the final view of goal but rather is\nrobust to servo to image goals taken from initial robot view with high\nocclusions without this impairing its ability to maintain a collision free\npath. We show the generalization capability of DIViS on real mobile robots in\nmore than 90 real world test scenarios with various unseen object goals in\nunstructured environments. DIViS is compared to prior approaches via real world\nexperiments and rigorous tests in simulation. For supplementary videos, see:\n\\href{https://fsadeghi.github.io/DIViS}{https://fsadeghi.github.io/DIViS}\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 18:57:05 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Sadeghi", "Fereshteh", ""]]}, {"id": "1902.05974", "submitter": "Simos Gerasimou", "authors": "Hasan Ferit Eniser, Simos Gerasimou, Alper Sen", "title": "DeepFault: Fault Localization for Deep Neural Networks", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are increasingly deployed in safety-critical\napplications including autonomous vehicles and medical diagnostics. To reduce\nthe residual risk for unexpected DNN behaviour and provide evidence for their\ntrustworthy operation, DNNs should be thoroughly tested. The DeepFault whitebox\nDNN testing approach presented in our paper addresses this challenge by\nemploying suspiciousness measures inspired by fault localization to establish\nthe hit spectrum of neurons and identify suspicious neurons whose weights have\nnot been calibrated correctly and thus are considered responsible for\ninadequate DNN performance. DeepFault also uses a suspiciousness-guided\nalgorithm to synthesize new inputs, from correctly classified inputs, that\nincrease the activation values of suspicious neurons. Our empirical evaluation\non several DNN instances trained on MNIST and CIFAR-10 datasets shows that\nDeepFault is effective in identifying suspicious neurons. Also, the inputs\nsynthesized by DeepFault closely resemble the original inputs, exercise the\nidentified suspicious neurons and are highly adversarial.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 19:42:45 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Eniser", "Hasan Ferit", ""], ["Gerasimou", "Simos", ""], ["Sen", "Alper", ""]]}, {"id": "1902.05978", "submitter": "Baris Gecer", "authors": "Baris Gecer, Stylianos Ploumpis, Irene Kotsia, and Stefanos Zafeiriou", "title": "GANFIT: Generative Adversarial Network Fitting for High Fidelity 3D Face\n  Reconstruction", "comments": "CVPR 2019 camera ready; Check project page:\n  https://github.com/barisgecer/ganfit for full resolution results and more", "journal-ref": "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR), Long Beach, CA, USA, 2019, pp. 1155-1164", "doi": "10.1109/CVPR.2019.00125", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the past few years, a lot of work has been done towards reconstructing the\n3D facial structure from single images by capitalizing on the power of Deep\nConvolutional Neural Networks (DCNNs). In the most recent works, differentiable\nrenderers were employed in order to learn the relationship between the facial\nidentity features and the parameters of a 3D morphable model for shape and\ntexture. The texture features either correspond to components of a linear\ntexture space or are learned by auto-encoders directly from in-the-wild images.\nIn all cases, the quality of the facial texture reconstruction of the\nstate-of-the-art methods is still not capable of modeling textures in high\nfidelity. In this paper, we take a radically different approach and harness the\npower of Generative Adversarial Networks (GANs) and DCNNs in order to\nreconstruct the facial texture and shape from single images. That is, we\nutilize GANs to train a very powerful generator of facial texture in UV space.\nThen, we revisit the original 3D Morphable Models (3DMMs) fitting approaches\nmaking use of non-linear optimization to find the optimal latent parameters\nthat best reconstruct the test image but under a new perspective. We optimize\nthe parameters with the supervision of pretrained deep identity features\nthrough our end-to-end differentiable framework. We demonstrate excellent\nresults in photorealistic and identity preserving 3D face reconstructions and\nachieve for the first time, to the best of our knowledge, facial texture\nreconstruction with high-frequency details.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 19:53:45 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 19:05:21 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Gecer", "Baris", ""], ["Ploumpis", "Stylianos", ""], ["Kotsia", "Irene", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1902.05985", "submitter": "Nono Heryana", "authors": "Rini Mayasari, Nono Heryana", "title": "Reduce Noise in Computed Tomography Image using Adaptive Gaussian Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One image processing application that is very helpful for humans is to\nimprove image quality, poor image quality makes the image more difficult to\ninterpret because the information conveyed by the image is reduced. In the\nprocess of the acquisition of medical images, the resulting image has decreased\nquality (degraded) due to external factors and medical equipment used. For this\nreason, it is necessary to have an image processing process to improve the\nquality of medical images, so that later it is expected to help facilitate\nmedical personnel in analyzing and translating medical images, which will lead\nto an improvement in the quality of diagnosis. In this study, an analysis will\nbe carried out to improve the quality of medical images with noise reduction\nwith the Gaussian Filter Method. Next, it is carried out, and tested against\nmedical images, in this case, the lung photo image. The test image is given\nnoise in the form of impulse salt & pepper and adaptive Gaussian then analyzed\nits performance qualitatively by comparing the output filter image, noise\nimage, and the original image by naked eye.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 13:02:00 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Mayasari", "Rini", ""], ["Heryana", "Nono", ""]]}, {"id": "1902.06042", "submitter": "Jiangmiao Pang", "authors": "Jiangmiao Pang, Cong Li, Jianping Shi, Zhihai Xu, Huajun Feng", "title": "R$^2$-CNN: Fast Tiny Object Detection in Large-Scale Remote Sensing\n  Images", "comments": "13 pages. Accepted to IEEE Transactions on Geoscience and Remote\n  Sensing", "journal-ref": null, "doi": "10.1109/TGRS.2019.2899955", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the convolutional neural network has brought impressive\nimprovements for object detection. However, detecting tiny objects in\nlarge-scale remote sensing images still remains challenging. First, the extreme\nlarge input size makes the existing object detection solutions too slow for\npractical use. Second, the massive and complex backgrounds cause serious false\nalarms. Moreover, the ultratiny objects increase the difficulty of accurate\ndetection. To tackle these problems, we propose a unified and self-reinforced\nnetwork called remote sensing region-based convolutional neural network\n($\\mathcal{R}^2$-CNN), composing of backbone Tiny-Net, intermediate global\nattention block, and final classifier and detector. Tiny-Net is a lightweight\nresidual structure, which enables fast and powerful features extraction from\ninputs. Global attention block is built upon Tiny-Net to inhibit false\npositives. Classifier is then used to predict the existence of targets in each\npatch, and detector is followed to locate them accurately if available. The\nclassifier and detector are mutually reinforced with end-to-end training, which\nfurther speed up the process and avoid false alarms. Effectiveness of\n$\\mathcal{R}^2$-CNN is validated on hundreds of GF-1 images and GF-2 images\nthat are 18 000 $\\times$ 18 192 pixels, 2.0-m resolution, and 27 620 $\\times$\n29 200 pixels, 0.8-m resolution, respectively. Specifically, we can process a\nGF-1 image in 29.4 s on Titian X just with single thread. According to our\nknowledge, no previous solution can detect the tiny object on such huge remote\nsensing images gracefully. We believe that it is a significant step toward\npractical real-time remote sensing systems.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 04:59:13 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 17:25:19 GMT"}, {"version": "v3", "created": "Sat, 30 Mar 2019 11:43:11 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Pang", "Jiangmiao", ""], ["Li", "Cong", ""], ["Shi", "Jianping", ""], ["Xu", "Zhihai", ""], ["Feng", "Huajun", ""]]}, {"id": "1902.06057", "submitter": "Fang Wan", "authors": "Fang Wan, Pengxu Wei, Zhenjun Han, Jianbin Jiao and Qixiang Ye", "title": "Min-Entropy Latent Model for Weakly Supervised Object Detection", "comments": "Accepted by TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised object detection is a challenging task when provided with\nimage category supervision but required to learn, at the same time, object\nlocations and object detectors. The inconsistency between the weak supervision\nand learning objectives introduces significant randomness to object locations\nand ambiguity to detectors. In this paper, a min-entropy latent model (MELM) is\nproposed for weakly supervised object detection. Min-entropy serves as a model\nto learn object locations and a metric to measure the randomness of object\nlocalization during learning. It aims to principally reduce the variance of\nlearned instances and alleviate the ambiguity of detectors. MELM is decomposed\ninto three components including proposal clique partition, object clique\ndiscovery, and object localization. MELM is optimized with a recurrent learning\nalgorithm, which leverages continuation optimization to solve the challenging\nnon-convexity problem. Experiments demonstrate that MELM significantly improves\nthe performance of weakly supervised object detection, weakly supervised object\nlocalization, and image classification, against the state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 07:32:39 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Wan", "Fang", ""], ["Wei", "Pengxu", ""], ["Han", "Zhenjun", ""], ["Jiao", "Jianbin", ""], ["Ye", "Qixiang", ""]]}, {"id": "1902.06061", "submitter": "Devansh Bisla", "authors": "Devansh Bisla, Anna Choromanska, Jennifer A. Stein, David Polsky,\n  Russell Berman", "title": "Towards Automated Melanoma Detection with Deep Learning: Data\n  Purification and Augmentation", "comments": "Accepted to CVPR ISIC Workshop - 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Melanoma is one of the ten most common cancers in the US. Early detection is\ncrucial for survival, but often the cancer is diagnosed in the fatal stage.\nDeep learning has the potential to improve cancer detection rates, but its\napplicability to melanoma detection is compromised by the limitations of the\navailable skin lesion databases, which are small, heavily imbalanced, and\ncontain images with occlusions. We build deep-learning-based tools for data\npurification and augmentation to counter-act these limitations. The developed\ntools can be utilized in a deep learning system for lesion classification and\nwe show how to build such a system. The system heavily relies on the processing\nunit for removing image occlusions and the data generation unit, based on\ngenerative adversarial networks, for populating scarce lesion classes, or\nequivalently creating virtual patients with pre-defined types of lesions. We\nempirically verify our approach and show that incorporating these two units\ninto melanoma detection system results in the superior performance over common\nbaselines.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 07:57:12 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 18:58:58 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Bisla", "Devansh", ""], ["Choromanska", "Anna", ""], ["Stein", "Jennifer A.", ""], ["Polsky", "David", ""], ["Berman", "Russell", ""]]}, {"id": "1902.06066", "submitter": "Varshaneya V", "authors": "Varshaneya V, Balasubramanian S and Darshan Gera", "title": "RES-SE-NET: Boosting Performance of Resnets by Enhancing\n  Bridge-connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  One of the ways to train deep neural networks effectively is to use residual\nconnections. Residual connections can be classified as being either identity\nconnections or bridge-connections with a reshaping convolution. Empirical\nobservations on CIFAR-10 and CIFAR-100 datasets using a baseline Resnet model,\nwith bridge-connections removed, have shown a significant reduction in\naccuracy. This reduction is due to lack of contribution, in the form of feature\nmaps, by the bridge-connections. Hence bridge-connections are vital for Resnet.\nHowever, all feature maps in the bridge-connections are considered to be\nequally important. In this work, an upgraded architecture \"Res-SE-Net\" is\nproposed to further strengthen the contribution from the bridge-connections by\nquantifying the importance of each feature map and weighting them accordingly\nusing Squeeze-and-Excitation (SE) block. It is demonstrated that Res-SE-Net\ngeneralizes much better than Resnet and SE-Resnet on the benchmark CIFAR-10 and\nCIFAR-100 datasets.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 08:25:16 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["V", "Varshaneya", ""], ["S", "Balasubramanian", ""], ["Gera", "Darshan", ""]]}, {"id": "1902.06068", "submitter": "Zhihao Wang", "authors": "Zhihao Wang, Jian Chen, Steven C.H. Hoi", "title": "Deep Learning for Image Super-resolution: A Survey", "comments": "Accepted by IEEE TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Super-Resolution (SR) is an important class of image processing\ntechniques to enhance the resolution of images and videos in computer vision.\nRecent years have witnessed remarkable progress of image super-resolution using\ndeep learning techniques. This article aims to provide a comprehensive survey\non recent advances of image super-resolution using deep learning approaches. In\ngeneral, we can roughly group the existing studies of SR techniques into three\nmajor categories: supervised SR, unsupervised SR, and domain-specific SR. In\naddition, we also cover some other important issues, such as publicly available\nbenchmark datasets and performance evaluation metrics. Finally, we conclude\nthis survey by highlighting several future directions and open issues which\nshould be further addressed by the community in the future.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 08:39:36 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2020 04:43:33 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Wang", "Zhihao", ""], ["Chen", "Jian", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "1902.06082", "submitter": "Christian Lessig", "authors": "Christian Lessig", "title": "Local Fourier Slice Photography", "comments": "images with reduced quality (please contact the author for a high\n  resolution version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field cameras provide intriguing possibilities, such as post-capture\nrefocus or the ability to synthesize images from novel viewpoints. This comes,\nhowever, at the price of significant storage requirements. Compression\ntechniques can be used to reduce these but refocusing and reconstruction\nrequire so far again a dense pixel representation. To avoid this, we introduce\nlocal Fourier slice photography that allows for refocused image reconstruction\ndirectly from a sparse wavelet representation of a light field, either to\nobtain an image or a compressed representation of it. The result is made\npossible by wavelets that respect the \"slicing's\" intrinsic structure and\nenable us to derive exact reconstruction filters for the refocused image in\nclosed form. Image reconstruction then amounts to applying these filters to the\nlight field's wavelet coefficients, and hence no reconstruction of a dense\npixel representation is required. We demonstrate that this substantially\nreduces storage requirements and also computation times. We furthermore analyze\nthe computational complexity of our algorithm and show that it scales linearly\nwith the size of the reconstructed region and the non-negligible wavelet\ncoefficients, i.e. with the visual complexity.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 10:37:00 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 20:22:56 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Lessig", "Christian", ""]]}, {"id": "1902.06085", "submitter": "Meiyu Li", "authors": "Meiyu Li, Hailiang Tang, Michael D. Chan, Xiaobo Zhou, and Xiaohua\n  Qian", "title": "DC-AL GAN: Pseudoprogression and True Tumor Progression of Glioblastoma\n  Multiform Image Classification Based on DCGAN and AlexNet", "comments": null, "journal-ref": null, "doi": "10.1002/mp.14003", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudoprogression (PsP) occurs in 20-30% of patients with glioblastoma\nmultiforme (GBM) after receiving the standard treatment. In the course of\npost-treatment magnetic resonance imaging (MRI), PsP exhibits similarities in\nshape and intensity to the true tumor progression (TTP) of GBM. So, these\nsimilarities pose challenges on the differentiation of these types of\nprogression and hence the selection of the appropriate clinical treatment\nstrategy. In this paper, we introduce DC-AL GAN, a novel feature learning\nmethod based on deep convolutional generative adversarial network (DCGAN) and\nAlexNet, to discriminate between PsP and TTP in MRI images. Due to the\nadversarial relationship between the generator and the discriminator of DCGAN,\nhigh-level discriminative features of PsP and TTP can be derived for the\ndiscriminator with AlexNet. Also, a feature fusion scheme is used to combine\nhigher-layer features with lower-layer information, leading to more powerful\nfeatures that are used for effectively discriminating between PsP and TTP. The\nexperimental results show that DC-AL GAN achieves desirable PsP and TTP\nclassification performance that is superior to other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 10:43:33 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 13:54:13 GMT"}, {"version": "v3", "created": "Wed, 15 May 2019 05:54:26 GMT"}, {"version": "v4", "created": "Sat, 18 May 2019 05:48:12 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Li", "Meiyu", ""], ["Tang", "Hailiang", ""], ["Chan", "Michael D.", ""], ["Zhou", "Xiaobo", ""], ["Qian", "Xiaohua", ""]]}, {"id": "1902.06105", "submitter": "Qilin Li", "authors": "Qilin Li, Senjian An, Ling Li, Wanquan Liu", "title": "Semi-supervised Learning on Graph with an Alternating Diffusion Process", "comments": "7 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based semi-supervised learning usually involves two separate stages,\nconstructing an affinity graph and then propagating labels for transductive\ninference on the graph. It is suboptimal to solve them independently, as the\ncorrelation between the affinity graph and labels are not fully exploited. In\nthis paper, we integrate the two stages into one unified framework by\nformulating the graph construction as a regularized function estimation problem\nsimilar to label propagation. We propose an alternating diffusion process to\nsolve the two problems simultaneously, which allows us to learn the graph and\nunknown labels in an iterative fashion. With the proposed framework, we are\nable to adequately leverage both the given labels and estimated labels to\nconstruct a better graph, and effectively propagate labels on such a dynamic\ngraph updated simultaneously with the newly obtained labels. Extensive\nexperiments on various real-world datasets have demonstrated the superiority of\nthe proposed method compared to other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 14:26:47 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Li", "Qilin", ""], ["An", "Senjian", ""], ["Li", "Ling", ""], ["Liu", "Wanquan", ""]]}, {"id": "1902.06130", "submitter": "Diane Genest", "authors": "Diane Genest (LIGM), Marc L\\'eonard, Jean Cousty (LIGM), No\\'emie De\n  Croz\\'e (RCO), Hugues Talbot (LIGM)", "title": "Atlas-based automated detection of swim bladder in Medaka embryo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fish embryo models are increasingly being used both for the assessment of\nchemicals efficacy and potential toxicity. This article proposes a methodology\nto automatically detect the swim bladder on 2D images of Medaka fish embryos\nseen either in dorsal view or in lateral view. After embryo segmentation and\nfor each studied orientation, the method builds an atlas of a healthy embryo.\nThis atlas is then used to define the region of interest and to guide the swim\nbladder segmentation with a discrete globally optimal active contour.\nDescriptors are subsequently designed from this segmentation. An automated\nrandom forest clas-sifier is built from these descriptors in order to classify\nembryos with and without a swim bladder. The proposed method is assessed on a\ndataset of 261 images, containing 202 embryos with a swim bladder (where 196\nare in dorsal view and 6 are in lateral view) and 59 without (where 43 are in\ndorsal view and 16 are in lateral view). We obtain an average precision rate of\n95% in the total dataset following 5-fold cross-validation.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 17:30:53 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Genest", "Diane", "", "LIGM"], ["L\u00e9onard", "Marc", "", "LIGM"], ["Cousty", "Jean", "", "LIGM"], ["De Croz\u00e9", "No\u00e9mie", "", "RCO"], ["Talbot", "Hugues", "", "LIGM"]]}, {"id": "1902.06131", "submitter": "Jiayang Sun", "authors": "Jang Ik Cho, Xiaofeng Wang, Yifan Xu and Jiayang Sun", "title": "LISA: a MATLAB package for Longitudinal Image Sequence Analysis", "comments": "18 pages, 17 figures made from 35 png files", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large sequences of images (or movies) can now be obtained on an unprecedented\nscale, which poses fundamental challenges to the existing image analysis\ntechniques. The challenges include heterogeneity, (automatic) alignment,\nmultiple comparisons, potential artifacts, and hidden noises. This paper\nintroduces our MATLAB package, Longitudinal Image Sequence Analysis (LISA), as\na one-stop ensemble of image processing and analysis tool for comparing a\ngeneral class of images from either different times, sessions, or subjects.\nGiven two contrasting sequences of images, the image processing in LISA starts\nwith selecting a region of interest in two representative images, followed by\nautomatic or manual segmentation and registration. Automatic segmentation\nde-noises an image using a mixture of Gaussian distributions of the pixel\nintensity values, while manual segmentation applies a user-chosen intensity\ncut-off value to filter out noises. Automatic registration aligns the\ncontrasting images based on a mid-line regression whereas manual registration\nlines up the images along a reference line formed by two user-selected points.\nThe processed images are then rendered for simultaneous statistical comparisons\nto generate D, S, T, and P-maps. The D map represents a curated difference of\ncontrasting images, the S map is the non-parametrically smoothed differences,\nthe T map presents the variance-adjusted, smoothed differences, and the P-map\nprovides multiplicity-controlled p-values. These maps reveal the regions with\nsignificant differences due to either longitudinal, subject-specific, or\ntreatment changes. A user can skip the image processing step to dive directly\ninto the statistical analysis step if the images have already been processed.\nHence, LISA offers flexibility in applying other image pre-processing tools.\nLISA also has a parallel computing option for high definition images.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 17:50:19 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Cho", "Jang Ik", ""], ["Wang", "Xiaofeng", ""], ["Xu", "Yifan", ""], ["Sun", "Jiayang", ""]]}, {"id": "1902.06148", "submitter": "Begum Demir Prof. Dr.", "authors": "Gencer Sumbul, Marcela Charfuelan, Beg\\\"um Demir, Volker Markl", "title": "BigEarthNet: A Large-Scale Benchmark Archive For Remote Sensing Image\n  Understanding", "comments": "Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS) 2019", "journal-ref": null, "doi": "10.1109/IGARSS.2019.8900532", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the BigEarthNet that is a new large-scale multi-label\nSentinel-2 benchmark archive. The BigEarthNet consists of 590,326 Sentinel-2\nimage patches, each of which is a section of i) 120x120 pixels for 10m bands;\nii) 60x60 pixels for 20m bands; and iii) 20x20 pixels for 60m bands. Unlike\nmost of the existing archives, each image patch is annotated by multiple\nland-cover classes (i.e., multi-labels) that are provided from the CORINE Land\nCover database of the year 2018 (CLC 2018). The BigEarthNet is significantly\nlarger than the existing archives in remote sensing (RS) and thus is much more\nconvenient to be used as a training source in the context of deep learning.\nThis paper first addresses the limitations of the existing archives and then\ndescribes the properties of the BigEarthNet. Experimental results obtained in\nthe framework of RS image scene classification problems show that a shallow\nConvolutional Neural Network (CNN) architecture trained on the BigEarthNet\nprovides much higher accuracy compared to a state-of-the-art CNN model\npre-trained on the ImageNet (which is a very popular large-scale benchmark\narchive in computer vision). The BigEarthNet opens up promising directions to\nadvance operational RS applications and research in massive Sentinel-2 image\narchives.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 20:06:55 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 17:00:35 GMT"}, {"version": "v3", "created": "Thu, 6 Jun 2019 20:56:02 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Sumbul", "Gencer", ""], ["Charfuelan", "Marcela", ""], ["Demir", "Beg\u00fcm", ""], ["Markl", "Volker", ""]]}, {"id": "1902.06155", "submitter": "Jos van de Wolfshaar", "authors": "Jos van de Wolfshaar, Andrzej Pronobis", "title": "Deep Generalized Convolutional Sum-Product Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sum-Product Networks (SPNs) are hierarchical, graphical models that combine\nbenefits of deep learning and probabilistic modeling. SPNs offer unique\nadvantages to applications demanding exact probabilistic inference over\nhigh-dimensional, noisy inputs. Yet, compared to convolutional neural nets,\nthey struggle with capturing complex spatial relationships in image data. To\nalleviate this issue, we introduce Deep Generalized Convolutional Sum-Product\nNetworks (DGC-SPNs), which encode spatial features in a way similar to CNNs,\nwhile preserving the validity of the probabilistic SPN model. As opposed to\nexisting SPN-based image representations, DGC-SPNs allow for overlapping\nconvolution patches through a novel parameterization of dilations and strides,\nresulting in significantly improved feature coverage and feature resolution.\nDGC-SPNs substantially outperform other SPN architectures across several visual\ndatasets and for both generative and discriminative tasks, including image\ninpainting and classification. These contributions are reinforced by the first\nsimple, scalable, and GPU-optimized implementation of SPNs, integrated with the\nwidely used Keras/TensorFlow framework. The resulting model is fully\nprobabilistic and versatile, yet efficient and straightforward to apply in\npractical applications in place of traditional deep nets.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 20:55:53 GMT"}, {"version": "v2", "created": "Sun, 8 Sep 2019 08:21:21 GMT"}, {"version": "v3", "created": "Tue, 10 Sep 2019 21:34:50 GMT"}, {"version": "v4", "created": "Tue, 22 Sep 2020 19:09:17 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["van de Wolfshaar", "Jos", ""], ["Pronobis", "Andrzej", ""]]}, {"id": "1902.06162", "submitter": "Longlong Jing", "authors": "Longlong Jing, Yingli Tian", "title": "Self-supervised Visual Feature Learning with Deep Neural Networks: A\n  Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale labeled data are generally required to train deep neural networks\nin order to obtain better performance in visual feature learning from images or\nvideos for computer vision applications. To avoid extensive cost of collecting\nand annotating large-scale datasets, as a subset of unsupervised learning\nmethods, self-supervised learning methods are proposed to learn general image\nand video features from large-scale unlabeled data without using any\nhuman-annotated labels. This paper provides an extensive review of deep\nlearning-based self-supervised general visual feature learning methods from\nimages or videos. First, the motivation, general pipeline, and terminologies of\nthis field are described. Then the common deep neural network architectures\nthat used for self-supervised learning are summarized. Next, the main\ncomponents and evaluation metrics of self-supervised learning methods are\nreviewed followed by the commonly used image and video datasets and the\nexisting self-supervised visual feature learning methods. Finally, quantitative\nperformance comparisons of the reviewed methods on benchmark datasets are\nsummarized and discussed for both image and video feature learning. At last,\nthis paper is concluded and lists a set of promising future directions for\nself-supervised visual feature learning.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 21:30:18 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Jing", "Longlong", ""], ["Tian", "Yingli", ""]]}, {"id": "1902.06182", "submitter": "Mohammadreza Javanmardi", "authors": "Mohammadreza Javanmardi, Xiaojun Qi", "title": "Structured Group Local Sparse Tracker", "comments": "This manuscript is submitted to IET Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation is considered as a viable solution to visual tracking.\nIn this paper, we propose a structured group local sparse tracker (SGLST),\nwhich exploits local patches inside target candidates in the particle filter\nframework. Unlike the conventional local sparse trackers, the proposed\noptimization model in SGLST not only adopts local and spatial information of\nthe target candidates but also attains the spatial layout structure among them\nby employing a group-sparsity regularization term. To solve the optimization\nmodel, we propose an efficient numerical algorithm consisting of two\nsubproblems with the closed-form solutions. Both qualitative and quantitative\nevaluations on the benchmarks of challenging image sequences demonstrate the\nsuperior performance of the proposed tracker against several state-of-the-art\ntrackers.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 01:06:58 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 02:16:02 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Javanmardi", "Mohammadreza", ""], ["Qi", "Xiaojun", ""]]}, {"id": "1902.06197", "submitter": "Sanli Tang", "authors": "Sanli Tang, Fan He, Xiaolin Huang, Jie Yang", "title": "Online PCB Defect Detector On A New PCB Defect Dataset", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous works for PCB defect detection based on image difference and image\nprocessing techniques have already achieved promising performance. However,\nthey sometimes fall short because of the unaccounted defect patterns or\nover-sensitivity about some hyper-parameters. In this work, we design a deep\nmodel that accurately detects PCB defects from an input pair of a detect-free\ntemplate and a defective tested image. A novel group pyramid pooling module is\nproposed to efficiently extract features of a large range of resolutions, which\nare merged by group to predict PCB defect of corresponding scales. To train the\ndeep model, a dataset is established, namely DeepPCB, which contains 1,500\nimage pairs with annotations including positions of 6 common types of PCB\ndefects. Experiment results validate the effectiveness and efficiency of the\nproposed model by achieving $98.6\\%$ mAP @ 62 FPS on DeepPCB dataset. This\ndataset is now available at: https://github.com/tangsanli5201/DeepPCB.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 03:39:24 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Tang", "Sanli", ""], ["He", "Fan", ""], ["Huang", "Xiaolin", ""], ["Yang", "Jie", ""]]}, {"id": "1902.06202", "submitter": "Sarah Tymochko", "authors": "Sarah Tymochko, Elizabeth Munch, Jason Dunion, Kristen Corbosiero, and\n  Ryan Torn", "title": "Using Persistent Homology to Quantify a Diurnal Cycle in Hurricane Felix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diurnal cycle of tropical cyclones (TCs) is a daily cycle in clouds that\nappears in satellite images and may have implications for TC structure and\nintensity. The diurnal pattern can be seen in infrared (IR) satellite imagery\nas cyclical pulses in the cloud field that propagate radially outward from the\ncenter of nearly all Atlantic-basin TCs. These diurnal pulses, a distinguishing\ncharacteristic of the TC diurnal cycle, begin forming in the storm's inner core\nnear sunset each day and appear as a region of cooling cloud-top temperatures.\nThe area of cooling takes on a ring-like appearance as cloud-top warming occurs\non its inside edge and the cooling moves away from the storm overnight,\nreaching several hundred kilometers from the circulation center by the\nfollowing afternoon. The state-of-the-art TC diurnal cycle measurement has a\nlimited ability to analyze the behavior beyond qualitative observations. We\npresent a method for quantifying the TC diurnal cycle using one-dimensional\npersistent homology, a tool from Topological Data Analysis, by tracking maximum\npersistence and quantifying the cycle using the discrete Fourier transform.\nUsing Geostationary Operational Environmental Satellite IR imagery data from\nHurricane Felix (2007), our method is able to detect an approximate daily\ncycle.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 04:31:12 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 19:24:53 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Tymochko", "Sarah", ""], ["Munch", "Elizabeth", ""], ["Dunion", "Jason", ""], ["Corbosiero", "Kristen", ""], ["Torn", "Ryan", ""]]}, {"id": "1902.06218", "submitter": "Hongyin Ni", "authors": "Hongyin Ni and Fengping Li", "title": "Fast Pedestrian Detection based on T-CENTRIST in infrared image", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Pedestrian detection is a research hotspot and a difficult issue in the\ncomputer vision such as the Intelligent Surveillance System, the Intelligent\nTransport System, robotics, and automotive safety. However, the human body's\nposition, angle, and dress in a video scene are complicated and changeable,\nwhich have a great influence on the detection accuracy. In this paper, through\nthe analysis on the pros and cons of Census Transform Histogram (CENTRIST), a\nnovel feature is presented for human detection Ternary CENTRIST (T-CENTRIST).\nThe T-CENTRIST feature takes the relationship between each pixel and its\nneighborhood pixels into account. Meanwhile, it also considers the relevancy\namong these neighborhood pixels. Therefore, the proposed feature description\nmethod can reflect the silhouette of pedestrian more adequately and accurately\nthan that of CENTRIST. Second, we propose a fast pedestrian detection framework\nbased on T-CENTRIST in infrared image, which introduces the idea of extended\nblocks and the integral image. Finally, experimental results verify the\neffectiveness of the proposed pedestrian detection method.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 07:20:59 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 01:19:36 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Ni", "Hongyin", ""], ["Li", "Fengping", ""]]}, {"id": "1902.06221", "submitter": "Gaochang Wu", "authors": "Gaochang Wu and Yebin Liu and Lu Fang and Tianyou Chai", "title": "LapEPI-Net: A Laplacian Pyramid EPI structure for Learning-based Dense\n  Light Field Reconstruction", "comments": "10 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For dense sampled light field (LF) reconstruction problem, existing\napproaches focus on a depth-free framework to achieve non-Lambertian\nperformance. However, they trap in the trade-off \"either aliasing or blurring\"\nproblem, i.e., pre-filtering the aliasing components (caused by the angular\nsparsity of the input LF) always leads to a blurry result. In this paper, we\nintend to solve this challenge by introducing an elaborately designed epipolar\nplane image (EPI) structure within a learning-based framework. Specifically, we\nstart by analytically showing that decreasing the spatial scale of an EPI shows\nhigher efficiency in addressing the aliasing problem than simply adopting\npre-filtering. Accordingly, we design a Laplacian Pyramid EPI (LapEPI)\nstructure that contains both low spatial scale EPI (for aliasing) and\nhigh-frequency residuals (for blurring) to solve the trade-off problem. We then\npropose a novel network architecture for the LapEPI structure, termed as\nLapEPI-net. To ensure the non-Lambertian performance, we adopt a\ntransfer-learning strategy by first pre-training the network with natural\nimages then fine-tuning it with unstructured LFs. Extensive experiments\ndemonstrate the high performance and robustness of the proposed approach for\ntackling the aliasing-or-blurring problem as well as the non-Lambertian\nreconstruction.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 08:28:31 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Wu", "Gaochang", ""], ["Liu", "Yebin", ""], ["Fang", "Lu", ""], ["Chai", "Tianyou", ""]]}, {"id": "1902.06222", "submitter": "Weize Quan", "authors": "Weize Quan, Dong-Ming Yan, Kai Wang, Xiaopeng Zhang and Denis Pellerin", "title": "Detecting Colorized Images via Convolutional Neural Networks: Toward\n  High Accuracy and Good Generalization", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image colorization achieves more and more realistic results with the\nincreasing computation power of recent deep learning techniques. It becomes\nmore difficult to identify the fake colorized images by human eyes. In this\nwork, we propose a novel forensic method to distinguish between natural images\n(NIs) and colorized images (CIs) based on convolutional neural network (CNN).\nOur method is able to achieve high classification accuracy and cope with the\nchallenging scenario of blind detection, i.e., no training sample is available\nfrom \"unknown\" colorization algorithm that we may encounter during the testing\nphase. This blind detection performance can be regarded as a generalization\nperformance. First, we design and implement a base network, which can attain\nbetter performance in terms of classification accuracy and generalization (in\nmost cases) compared with state-of-the-art methods. Furthermore, we design a\nnew branch, which analyzes smaller regions of extracted features, and insert it\ninto the above base network. Consequently, our network can not only improve the\nclassification accuracy, but also enhance the generalization in the vast\nmajority of cases. To further improve the performance of blind detection, we\npropose to automatically construct negative samples through linear\ninterpolation of paired natural and colorized images. Then, we progressively\ninsert these negative samples into the original training dataset and continue\nto train the network. Experimental results demonstrate that our method can\nachieve stable and high generalization performance when tested against\ndifferent state-of-the-art colorization algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 08:40:48 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Quan", "Weize", ""], ["Yan", "Dong-Ming", ""], ["Wang", "Kai", ""], ["Zhang", "Xiaopeng", ""], ["Pellerin", "Denis", ""]]}, {"id": "1902.06255", "submitter": "Guang-Yu Nie", "authors": "Guang-Yu Nie, Yun Liu, Cong Wang, Yue Liu, Yongtian Wang", "title": "Exploring Stereovision-Based 3-D Scene Reconstruction for Augmented\n  Reality", "comments": "To be published in IEEE VR2019 Conference as a Poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional (3-D) scene reconstruction is one of the key techniques in\nAugmented Reality (AR), which is related to the integration of image processing\nand display systems of complex information. Stereo matching is a computer\nvision based approach for 3-D scene reconstruction. In this paper, we explore\nan improved stereo matching network, SLED-Net, in which a Single Long\nEncoder-Decoder is proposed to replace the stacked hourglass network in PSM-Net\nfor better contextual information learning. We compare SLED-Net to\nstate-of-the-art methods recently published, and demonstrate its superior\nperformance on Scene Flow and KITTI2015 test sets.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 13:09:16 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Nie", "Guang-Yu", ""], ["Liu", "Yun", ""], ["Wang", "Cong", ""], ["Liu", "Yue", ""], ["Wang", "Yongtian", ""]]}, {"id": "1902.06258", "submitter": "Muli Yang", "authors": "De Xie, Muli Yang, Cheng Deng, Wei Liu and Dacheng Tao", "title": "Fully-Featured Attribute Transfer", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image attribute transfer aims to change an input image to a target one with\nexpected attributes, which has received significant attention in recent years.\nHowever, most of the existing methods lack the ability to de-correlate the\ntarget attributes and irrelevant information, i.e., the other attributes and\nbackground information, thus often suffering from blurs and artifacts. To\naddress these issues, we propose a novel Attribute Manifold Encoding GAN\n(AME-GAN) for fully-featured attribute transfer, which can modify and adjust\nevery detail in the images. Specifically, our method divides the input image\ninto image attribute part and image background part on manifolds, which are\ncontrolled by attribute latent variables and background latent variables\nrespectively. Through enforcing attribute latent variables to Gaussian\ndistributions and background latent variables to uniform distributions\nrespectively, the attribute transfer procedure becomes controllable and image\ngeneration is more photo-realistic. Furthermore, we adopt a conditional\nmulti-scale discriminator to render accurate and high-quality target attribute\nimages. Experimental results on three popular datasets demonstrate the\nsuperiority of our proposed method in both performances of the attribute\ntransfer and image generation quality.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 13:20:24 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Xie", "De", ""], ["Yang", "Muli", ""], ["Deng", "Cheng", ""], ["Liu", "Wei", ""], ["Tao", "Dacheng", ""]]}, {"id": "1902.06285", "submitter": "Xialei Liu", "authors": "Xialei Liu, Joost van de Weijer, Andrew D. Bagdanov", "title": "Exploiting Unlabeled Data in CNNs by Self-supervised Learning to Rank", "comments": "Accepted at TPAMI. (Keywords: Learning from rankings, image quality\n  assessment, crowd counting, active learning). arXiv admin note: text overlap\n  with arXiv:1803.03095", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2899857", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For many applications the collection of labeled data is expensive laborious.\nExploitation of unlabeled data during training is thus a long pursued objective\nof machine learning. Self-supervised learning addresses this by positing an\nauxiliary task (different, but related to the supervised task) for which data\nis abundantly available. In this paper, we show how ranking can be used as a\nproxy task for some regression problems. As another contribution, we propose an\nefficient backpropagation technique for Siamese networks which prevents the\nredundant computation introduced by the multi-branch network architecture. We\napply our framework to two regression problems: Image Quality Assessment (IQA)\nand Crowd Counting. For both we show how to automatically generate ranked image\nsets from unlabeled data. Our results show that networks trained to regress to\nthe ground truth targets for labeled data and to simultaneously learn to rank\nunlabeled data obtain significantly better, state-of-the-art results for both\nIQA and crowd counting. In addition, we show that measuring network uncertainty\non the self-supervised proxy task is a good measure of informativeness of\nunlabeled data. This can be used to drive an algorithm for active learning and\nwe show that this reduces labeling effort by up to 50%.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 16:18:34 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Liu", "Xialei", ""], ["van de Weijer", "Joost", ""], ["Bagdanov", "Andrew D.", ""]]}, {"id": "1902.06292", "submitter": "Sercan Arik", "authors": "Sercan O. Arik and Tomas Pfister", "title": "ProtoAttend: Attention-Based Prototypical Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel inherently interpretable machine learning method that\nbases decisions on few relevant examples that we call prototypes. Our method,\nProtoAttend, can be integrated into a wide range of neural network\narchitectures including pre-trained models. It utilizes an attention mechanism\nthat relates the encoded representations to samples in order to determine\nprototypes. The resulting model outperforms state of the art in three high\nimpact problems without sacrificing accuracy of the original model: (1) it\nenables high-quality interpretability that outputs samples most relevant to the\ndecision-making (i.e. a sample-based interpretability method); (2) it achieves\nstate of the art confidence estimation by quantifying the mismatch across\nprototype labels; and (3) it obtains state of the art in distribution mismatch\ndetection. All this can be achieved with minimal additional test time and a\npractically viable training time computational cost.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 17:12:07 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 23:54:08 GMT"}, {"version": "v3", "created": "Fri, 2 Aug 2019 16:45:51 GMT"}, {"version": "v4", "created": "Thu, 26 Sep 2019 01:39:46 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Arik", "Sercan O.", ""], ["Pfister", "Tomas", ""]]}, {"id": "1902.06323", "submitter": "Svitlana Alkhimova", "authors": "Svitlana M Alkhimova", "title": "Automated Detection of Regions of Interest for Brain Perfusion MR Images", "comments": null, "journal-ref": "Research Bulletin of the National Technical University of Ukraine\"\n  Kyiv Politechnic Institute\" 5 (2018): 14-21", "doi": "10.20535/1810-0546.2018.5.146185", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images with abnormal brain anatomy produce problems for automatic\nsegmentation techniques, and as a result poor ROI detection affects both\nquantitative measurements and visual assessment of perfusion data. This paper\npresents a new approach for fully automated and relatively accurate ROI\ndetection from dynamic susceptibility contrast perfusion magnetic resonance and\ncan therefore be applied excellently in the perfusion analysis. In the proposed\napproach the segmentation output is a binary mask of perfusion ROI that has\nzero values for air pixels, pixels that represent non-brain tissues, and\ncerebrospinal fluid pixels. The process of binary mask producing starts with\nextracting low intensity pixels by thresholding. Optimal low-threshold value is\nsolved by obtaining intensity pixels information from the approximate\nanatomical brain location. Holes filling algorithm and binary region growing\nalgorithm are used to remove falsely detected regions and produce region of\nonly brain tissues. Further, CSF pixels extraction is provided by thresholding\nof high intensity pixels from region of only brain tissues. Each time-point\nimage of the perfusion sequence is used for adjustment of CSF pixels location.\nThe segmentation results were compared with the manual segmentation performed\nby experienced radiologists, considered as the reference standard for\nevaluation of proposed approach. On average of 120 images the segmentation\nresults have a good agreement with the reference standard. All detected\nperfusion ROIs were deemed by two experienced radiologists as satisfactory\nenough for clinical use. The results show that proposed approach is suitable to\nbe used for perfusion ROI detection from DSC head scans. Segmentation tool\nbased on the proposed approach can be implemented as a part of any automatic\nbrain image processing system for clinical use.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 20:47:22 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Alkhimova", "Svitlana M", ""]]}, {"id": "1902.06326", "submitter": "Bin Yang", "authors": "Bin Yang, Wenjie Luo, Raquel Urtasun", "title": "PIXOR: Real-time 3D Object Detection from Point Clouds", "comments": "Update of CVPR2018 paper: correct timing, fix typos, add\n  acknowledgement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We address the problem of real-time 3D object detection from point clouds in\nthe context of autonomous driving. Computation speed is critical as detection\nis a necessary component for safety. Existing approaches are, however,\nexpensive in computation due to high dimensionality of point clouds. We utilize\nthe 3D data more efficiently by representing the scene from the Bird's Eye View\n(BEV), and propose PIXOR, a proposal-free, single-stage detector that outputs\noriented 3D object estimates decoded from pixel-wise neural network\npredictions. The input representation, network architecture, and model\noptimization are especially designed to balance high accuracy and real-time\nefficiency. We validate PIXOR on two datasets: the KITTI BEV object detection\nbenchmark, and a large-scale 3D vehicle detection benchmark. In both datasets\nwe show that the proposed detector surpasses other state-of-the-art methods\nnotably in terms of Average Precision (AP), while still runs at >28 FPS.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 21:17:55 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 17:28:03 GMT"}, {"version": "v3", "created": "Sat, 2 Mar 2019 02:43:26 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Yang", "Bin", ""], ["Luo", "Wenjie", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1902.06328", "submitter": "Jinyong Hou", "authors": "Jinyong Hou, Xuejie Ding, Jeremiah D. Deng, Stephen Cranefield", "title": "Unsupervised Domain Adaptation using Deep Networks with Cross-Grafted\n  Stacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep domain adaptation methods used in computer vision have mainly\nfocused on learning discriminative and domain-invariant features across\ndifferent domains. In this paper, we present a novel approach that bridges the\ndomain gap by projecting the source and target domains into a common\nassociation space through an unsupervised ``cross-grafted representation\nstacking'' (CGRS) mechanism. Specifically, we construct variational\nauto-encoders (VAE) for the two domains, and form bidirectional associations by\ncross-grafting the VAEs' decoder stacks. Furthermore, generative adversarial\nnetworks (GAN) are employed for label alignment (LA), mapping the target domain\ndata to the known label space of the source domain. The overall adaptation\nprocess hence consists of three phases: feature representation learning by\nVAEs, association generation, and association label alignment by GANs.\nExperimental results demonstrate that our CGRS-LA approach outperforms the\nstate-of-the-art on a number of unsupervised domain adaptation benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 21:29:06 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 02:25:47 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Hou", "Jinyong", ""], ["Ding", "Xuejie", ""], ["Deng", "Jeremiah D.", ""], ["Cranefield", "Stephen", ""]]}, {"id": "1902.06334", "submitter": "Dogancan Temel", "authors": "Mohit Prabhushankar and Gukyeong Kwon and Dogancan Temel and Ghassan\n  AlRegib", "title": "Semantically Interpretable and Controllable Filter Sets", "comments": "5 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we generate and control semantically interpretable filters\nthat are directly learned from natural images in an unsupervised fashion. Each\nsemantic filter learns a visually interpretable local structure in conjunction\nwith other filters. The significance of learning these interpretable filter\nsets is demonstrated on two contrasting applications. The first application is\nimage recognition under progressive decolorization, in which recognition\nalgorithms should be color-insensitive to achieve a robust performance. The\nsecond application is image quality assessment where objective methods should\nbe sensitive to color degradations. In the proposed work, the sensitivity and\nlack thereof are controlled by weighing the semantic filters based on the local\nstructures they represent. To validate the proposed approach, we utilize the\nCURE-TSR dataset for image recognition and the TID 2013 dataset for image\nquality assessment. We show that the proposed semantic filter set achieves\nstate-of-the-art performances in both datasets while maintaining its robustness\nacross progressive distortions.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 21:49:41 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Prabhushankar", "Mohit", ""], ["Kwon", "Gukyeong", ""], ["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1902.06347", "submitter": "Pedro Pereira", "authors": "Pedro M. M. Pereira, Rui Fonseca-Pinto, Rui Pedro Paiva, Luis M. N.\n  Tavora, Pedro A. A. Assuncao, Sergio M. M. de Faria", "title": "Accurate Segmentation of Dermoscopic Images based on Local Binary\n  Pattern Clustering", "comments": "submitted to MIPRO DC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation is a key stage in dermoscopic image processing, where the\naccuracy of the border line that defines skin lesions is of utmost importance\nfor subsequent algorithms (e.g., classification) and computer-aided early\ndiagnosis of serious medical conditions. This paper proposes a novel\nsegmentation method based on Local Binary Patterns (LBP), where LBP and K-Means\nclustering are combined to achieve a detailed delineation in dermoscopic\nimages. In comparison with usual dermatologist-like segmentation (i.e., the\navailable ground-truth), the proposed method is capable of finding more\nrealistic borders of skin lesions, i.e., with much more detail. The results\nalso exhibit reduced variability amongst different performance measures and\nthey are consistent across different images. The proposed method can be applied\nfor cell-based like segmentation adapted to the lesion border growing\nspecificities. Hence, the method is suitable to follow the growth dynamics\nassociated with the lesion border geometry in skin melanocytic images.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 23:12:15 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 15:21:35 GMT"}, {"version": "v3", "created": "Wed, 20 Feb 2019 18:23:51 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Pereira", "Pedro M. M.", ""], ["Fonseca-Pinto", "Rui", ""], ["Paiva", "Rui Pedro", ""], ["Tavora", "Luis M. N.", ""], ["Assuncao", "Pedro A. A.", ""], ["de Faria", "Sergio M. M.", ""]]}, {"id": "1902.06362", "submitter": "Ali Hatamizadeh", "authors": "Abdullah-Al-Zubaer Imran, Ali Hatamizadeh, Shilpa P. Ananth, Xiaowei\n  Ding, Demetri Terzopoulos and Nima Tajbakhsh", "title": "Automatic Segmentation of Pulmonary Lobes Using a Progressive Dense\n  V-Network", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-00889-5_32", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable and automatic segmentation of lung lobes is important for diagnosis,\nassessment, and quantification of pulmonary diseases. The existing techniques\nare prohibitively slow, undesirably rely on prior (airway/vessel) segmentation,\nand/or require user interactions for optimal results. This work presents a\nreliable, fast, and fully automated lung lobe segmentation based on a\nprogressive dense V-network (PDV-Net). The proposed method can segment lung\nlobes in one forward pass of the network, with an average runtime of 2 seconds\nusing 1 Nvidia Titan XP GPU, eliminating the need for any prior atlases, lung\nsegmentation or any subsequent user intervention. We evaluated our model using\n84 chest CT scans from the LIDC and 154 pathological cases from the LTRC\ndatasets. Our model achieved a Dice score of $0.939 \\pm 0.02$ for the LIDC test\nset and $0.950 \\pm 0.01$ for the LTRC test set, significantly outperforming a\n2D U-net model and a 3D dense V-net. We further evaluated our model against 55\ncases from the LOLA11 challenge, obtaining an average Dice score of 0.935---a\nperformance level competitive to the best performing team with an average score\nof 0.938. Our extensive robustness analyses also demonstrate that our model can\nreliably segment both healthy and pathological lung lobes in CT scans from\ndifferent vendors, and that our model is robust against configurations of CT\nscan reconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 00:37:22 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Imran", "Abdullah-Al-Zubaer", ""], ["Hatamizadeh", "Ali", ""], ["Ananth", "Shilpa P.", ""], ["Ding", "Xiaowei", ""], ["Terzopoulos", "Demetri", ""], ["Tajbakhsh", "Nima", ""]]}, {"id": "1902.06379", "submitter": "Yuan Wang", "authors": "Yuan Wang, Yang Yu, Ming Liu", "title": "PointIT: A Fast Tracking Framework Based on 3D Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently most popular tracking frameworks focus on 2D image sequences. They\nseldom track the 3D object in point clouds. In this paper, we propose PointIT,\na fast, simple tracking method based on 3D on-road instance segmentation.\nFirstly, we transform 3D LiDAR data into the spherical image with the size of\n64 x 512 x 4 and feed it into instance segment model to get the predicted\ninstance mask for each class. Then we use MobileNet as our primary encoder\ninstead of the original ResNet to reduce the computational complexity. Finally,\nwe extend the Sort algorithm with this instance framework to realize tracking\nin the 3D LiDAR point cloud data. The model is trained on the spherical images\ndataset with the corresponding instance label masks which are provided by KITTI\n3D Object Track dataset. According to the experiment results, our network can\nachieve on Average Precision (AP) of 0.617 and the performance of\nmulti-tracking task has also been improved.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 02:39:08 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Wang", "Yuan", ""], ["Yu", "Yang", ""], ["Liu", "Ming", ""]]}, {"id": "1902.06382", "submitter": "Chengcheng Li", "authors": "Chengcheng Li, Zi Wang, Xiangyang Wang, Hairong Qi", "title": "Single-shot Channel Pruning Based on Alternating Direction Method of\n  Multipliers", "comments": "Submitted to ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Channel pruning has been identified as an effective approach to constructing\nefficient network structures. Its typical pipeline requires iterative pruning\nand fine-tuning. In this work, we propose a novel single-shot channel pruning\napproach based on alternating direction methods of multipliers (ADMM), which\ncan eliminate the need for complex iterative pruning and fine-tuning procedure\nand achieve a target compression ratio with only one run of pruning and\nfine-tuning. To the best of our knowledge, this is the first study of\nsingle-shot channel pruning. The proposed method introduces filter-level\nsparsity during training and can achieve competitive performance with a simple\nheuristic pruning criterion (L1-norm). Extensive evaluations have been\nconducted with various widely-used benchmark architectures and image datasets\nfor object classification purpose. The experimental results on classification\naccuracy show that the proposed method can outperform state-of-the-art network\npruning works under various scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 03:07:59 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Li", "Chengcheng", ""], ["Wang", "Zi", ""], ["Wang", "Xiangyang", ""], ["Qi", "Hairong", ""]]}, {"id": "1902.06383", "submitter": "Leslie Tiong", "authors": "Leslie Ching Ow Tiong, Andrew Beng Jin Teoh, Yunli Lee", "title": "Periocular Recognition in the Wild with Orthogonal Combination of Local\n  Binary Coded Pattern in Dual-stream Convolutional Neural Network", "comments": "Accepted in International Conference On Biometrics 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the advancements made in the periocular recognition, the dataset\nand periocular recognition in the wild remains a challenge. In this paper, we\npropose a multilayer fusion approach by means of a pair of shared parameters\n(dual-stream) convolutional neural network where each network accepts RGB data\nand a novel colour-based texture descriptor, namely Orthogonal\nCombination-Local Binary Coded Pattern (OC-LBCP) for periocular recognition in\nthe wild. Specifically, two distinct late-fusion layers are introduced in the\ndual-stream network to aggregate the RGB data and OC-LBCP. Thus, the network\nbeneficial from this new feature of the late-fusion layers for accuracy\nperformance gain. We also introduce and share a new dataset for periocular in\nthe wild, namely Ethnic-ocular dataset for benchmarking. The proposed network\nhas also been assessed on one publicly available dataset, namely UBIPr. The\nproposed network outperforms several competing approaches on these datasets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 03:08:04 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 05:25:05 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Tiong", "Leslie Ching Ow", ""], ["Teoh", "Andrew Beng Jin", ""], ["Lee", "Yunli", ""]]}, {"id": "1902.06385", "submitter": "Chengcheng Li", "authors": "Zi Wang, Chengcheng Li, Dali Wang, Xiangyang Wang, Hairong Qi", "title": "Speeding up convolutional networks pruning with coarse ranking", "comments": "Submitted to ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Channel-based pruning has achieved significant successes in accelerating deep\nconvolutional neural network, whose pipeline is an iterative three-step\nprocedure: ranking, pruning and fine-tuning. However, this iterative procedure\nis computationally expensive. In this study, we present a novel computationally\nefficient channel pruning approach based on the coarse ranking that utilizes\nthe intermediate results during fine-tuning to rank the importance of filters,\nbuilt upon state-of-the-art works with data-driven ranking criteria. The goal\nof this work is not to propose a single improved approach built upon a specific\nchannel pruning method, but to introduce a new general framework that works for\na series of channel pruning methods. Various benchmark image datasets\n(CIFAR-10, ImageNet, Birds-200, and Flowers-102) and network architectures\n(AlexNet and VGG-16) are utilized to evaluate the proposed approach for object\nclassification purpose. Experimental results show that the proposed method can\nachieve almost identical performance with the corresponding state-of-the-art\nworks (baseline) while our ranking time is negligibly short. In specific, with\nthe proposed method, 75% and 54% of the total computation time for the whole\npruning procedure can be reduced for AlexNet on CIFAR-10, and for VGG-16 on\nImageNet, respectively. Our approach would significantly facilitate pruning\npractice, especially on resource-constrained platforms.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 03:13:16 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Wang", "Zi", ""], ["Li", "Chengcheng", ""], ["Wang", "Dali", ""], ["Wang", "Xiangyang", ""], ["Qi", "Hairong", ""]]}, {"id": "1902.06426", "submitter": "Max Allan", "authors": "Max Allan, Alex Shvets, Thomas Kurmann, Zichen Zhang, Rahul Duggal,\n  Yun-Hsuan Su, Nicola Rieke, Iro Laina, Niveditha Kalavakonda, Sebastian\n  Bodenstedt, Luis Herrera, Wenqi Li, Vladimir Iglovikov, Huoling Luo, Jian\n  Yang, Danail Stoyanov, Lena Maier-Hein, Stefanie Speidel, Mahdi Azizian", "title": "2017 Robotic Instrument Segmentation Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mainstream computer vision and machine learning, public datasets such as\nImageNet, COCO and KITTI have helped drive enormous improvements by enabling\nresearchers to understand the strengths and limitations of different algorithms\nvia performance comparison. However, this type of approach has had limited\ntranslation to problems in robotic assisted surgery as this field has never\nestablished the same level of common datasets and benchmarking methods. In 2015\na sub-challenge was introduced at the EndoVis workshop where a set of robotic\nimages were provided with automatically generated annotations from robot\nforward kinematics. However, there were issues with this dataset due to the\nlimited background variation, lack of complex motion and inaccuracies in the\nannotation. In this work we present the results of the 2017 challenge on\nrobotic instrument segmentation which involved 10 teams participating in\nbinary, parts and type based segmentation of articulated da Vinci robotic\ninstruments.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 07:08:36 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 17:01:02 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Allan", "Max", ""], ["Shvets", "Alex", ""], ["Kurmann", "Thomas", ""], ["Zhang", "Zichen", ""], ["Duggal", "Rahul", ""], ["Su", "Yun-Hsuan", ""], ["Rieke", "Nicola", ""], ["Laina", "Iro", ""], ["Kalavakonda", "Niveditha", ""], ["Bodenstedt", "Sebastian", ""], ["Herrera", "Luis", ""], ["Li", "Wenqi", ""], ["Iglovikov", "Vladimir", ""], ["Luo", "Huoling", ""], ["Yang", "Jian", ""], ["Stoyanov", "Danail", ""], ["Maier-Hein", "Lena", ""], ["Speidel", "Stefanie", ""], ["Azizian", "Mahdi", ""]]}, {"id": "1902.06455", "submitter": "Tao Zhang", "authors": "Zhongnian Li, Tao Zhang, Peng Wan and Daoqiang Zhang", "title": "SEGAN: Structure-Enhanced Generative Adversarial Network for Compressed\n  Sensing MRI Reconstruction", "comments": "9 pages,5 figures, Proceedings of the Association for the Advancement\n  of Artificial Intelligence (AAAI-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are powerful tools for reconstructing\nCompressed Sensing Magnetic Resonance Imaging (CS-MRI). However most recent\nworks lack exploration of structure information of MRI images that is crucial\nfor clinical diagnosis. To tackle this problem, we propose the\nStructure-Enhanced GAN (SEGAN) that aims at restoring structure information at\nboth local and global scale. SEGAN defines a new structure regularization\ncalled Patch Correlation Regularization (PCR) which allows for efficient\nextraction of structure information. In addition, to further enhance the\nability to uncover structure information, we propose a novel generator SU-Net\nby incorporating multiple-scale convolution filters into each layer. Besides,\nwe theoretically analyze the convergence of stochastic factors contained in\ntraining process. Experimental results show that SEGAN is able to learn target\nstructure information and achieves state-of-the-art performance for CS-MRI\nreconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 08:28:50 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 07:47:19 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Li", "Zhongnian", ""], ["Zhang", "Tao", ""], ["Wan", "Peng", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "1902.06467", "submitter": "Manuel Soriano-Trigueros", "authors": "N. Atienza, M. J. Jimenez, M. Soriano-Trigueros", "title": "Stable Topological Summaries for Analyzing the Organization of Cells in\n  a Packed Tissue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use Topological Data Analysis tools for studying the inner organization of\ncells in segmented images of epithelial tissues. More specifically, for each\nsegmented image, we compute different persistence barcodes, which codify\nlifetime of homology classes (persistent homology) along different filtrations\n(increasing nested sequences of simplicial complexes) that are built from the\nregions representing the cells in the tissue. We use a complete and\nwell-grounded set of numerical variables over those persistence barcodes, also\nknown as topological summaries. A novel combination of normalization methods\nfor both, the set of input segmented images and the produced barcodes, allows\nto prove stability results for those variables with respect to small changes in\nthe input, as well as invariance to image scale. Our study provides new\ninsights to this problem, such as a possible novel indicator for the\ndevelopment of the drosophila wing disc tissue or the importance of centroids\ndistribution to differentiate some tissues from their CVT-path counterpart (a\nmathematical model of epithelia based on Voronoi diagrams). We also show how\nthe use of topological summaries may improve the classification accuracy of\nepithelial images using Random Forests algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 09:03:07 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 16:57:45 GMT"}, {"version": "v3", "created": "Wed, 27 Feb 2019 08:16:15 GMT"}, {"version": "v4", "created": "Fri, 17 May 2019 14:23:54 GMT"}, {"version": "v5", "created": "Tue, 18 May 2021 12:33:52 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Atienza", "N.", ""], ["Jimenez", "M. J.", ""], ["Soriano-Trigueros", "M.", ""]]}, {"id": "1902.06506", "submitter": "Youngjoo Kim", "authors": "Youngjoo Kim, Peng Wang, Lyudmila Mihaylova", "title": "Structural Recurrent Neural Network for Traffic Speed Prediction", "comments": "Accepted and revised, to be presented in International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP) on May 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have recently demonstrated the traffic prediction\ncapability with the time series data obtained by sensors mounted on road\nsegments. However, capturing spatio-temporal features of the traffic data often\nrequires a significant number of parameters to train, increasing computational\nburden. In this work we demonstrate that embedding topological information of\nthe road network improves the process of learning traffic features. We use a\ngraph of a vehicular road network with recurrent neural networks (RNNs) to\ninfer the interaction between adjacent road segments as well as the temporal\ndynamics. The topology of the road network is converted into a spatio-temporal\ngraph to form a structural RNN (SRNN). The proposed approach is validated over\ntraffic speed data from the road network of the city of Santander in Spain. The\nexperiment shows that the graph-based method outperforms the state-of-the-art\nmethods based on spatio-temporal images, requiring much fewer parameters to\ntrain.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 10:49:04 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Kim", "Youngjoo", ""], ["Wang", "Peng", ""], ["Mihaylova", "Lyudmila", ""]]}, {"id": "1902.06543", "submitter": "David Tellez", "authors": "David Tellez, Geert Litjens, Peter Bandi, Wouter Bulten, John-Melle\n  Bokhorst, Francesco Ciompi, Jeroen van der Laak", "title": "Quantifying the effects of data augmentation and stain color\n  normalization in convolutional neural networks for computational pathology", "comments": "Accepted in the Medical Image Analysis journal", "journal-ref": null, "doi": "10.1016/j.media.2019.101544", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stain variation is a phenomenon observed when distinct pathology laboratories\nstain tissue slides that exhibit similar but not identical color appearance.\nDue to this color shift between laboratories, convolutional neural networks\n(CNNs) trained with images from one lab often underperform on unseen images\nfrom the other lab. Several techniques have been proposed to reduce the\ngeneralization error, mainly grouped into two categories: stain color\naugmentation and stain color normalization. The former simulates a wide variety\nof realistic stain variations during training, producing stain-invariant CNNs.\nThe latter aims to match training and test color distributions in order to\nreduce stain variation. For the first time, we compared some of these\ntechniques and quantified their effect on CNN classification performance using\na heterogeneous dataset of hematoxylin and eosin histopathology images from 4\norgans and 9 pathology laboratories. Additionally, we propose a novel\nunsupervised method to perform stain color normalization using a neural\nnetwork. Based on our experimental results, we provide practical guidelines on\nhow to use stain color augmentation and stain color normalization in future\ncomputational pathology applications.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 12:36:58 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 14:03:56 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Tellez", "David", ""], ["Litjens", "Geert", ""], ["Bandi", "Peter", ""], ["Bulten", "Wouter", ""], ["Bokhorst", "John-Melle", ""], ["Ciompi", "Francesco", ""], ["van der Laak", "Jeroen", ""]]}, {"id": "1902.06550", "submitter": "Bojian Yin", "authors": "Bojian Yin, Siebren Schaafsma, Henk Corporaal, H. Steven Scholte,\n  Sander M. Bohte", "title": "LocalNorm: Robust Image Classification through Dynamically Regularized\n  Normalization", "comments": "14 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While modern convolutional neural networks achieve outstanding accuracy on\nmany image classification tasks, they are, compared to humans, much more\nsensitive to image degradation. Here, we describe a variant of Batch\nNormalization, LocalNorm, that regularizes the normalization layer in the\nspirit of Dropout while dynamically adapting to the local image intensity and\ncontrast at test-time. We show that the resulting deep neural networks are much\nmore resistant to noise-induced image degradation, improving accuracy by up to\nthree times, while achieving the same or slightly better accuracy on\nnon-degraded classical benchmarks. In computational terms, LocalNorm adds\nnegligible training cost and little or no cost at inference time, and can be\napplied to already-trained networks in a straightforward manner.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 12:58:23 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 09:04:54 GMT"}, {"version": "v3", "created": "Mon, 4 Mar 2019 10:21:13 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Yin", "Bojian", ""], ["Schaafsma", "Siebren", ""], ["Corporaal", "Henk", ""], ["Scholte", "H. Steven", ""], ["Bohte", "Sander M.", ""]]}, {"id": "1902.06554", "submitter": "Junhao Cai", "authors": "Junhao Cai, Hui Cheng, Zhanpeng Zhang, Jingcheng Su", "title": "MetaGrasp: Data Efficient Grasping by Affordance Interpreter Network", "comments": "7 pages, 10 figures, IEEE International Conference on Robotics and\n  Automation 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven approach for grasping shows significant advance recently. But\nthese approaches usually require much training data. To increase the efficiency\nof grasping data collection, this paper presents a novel grasp training system\nincluding the whole pipeline from data collection to model inference. The\nsystem can collect effective grasp sample with a corrective strategy assisted\nby antipodal grasp rule, and we design an affordance interpreter network to\npredict pixelwise grasp affordance map. We define graspability, ungraspability\nand background as grasp affordances. The key advantage of our system is that\nthe pixel-level affordance interpreter network trained with only a small number\nof grasp samples under antipodal rule can achieve significant performance on\ntotally unseen objects and backgrounds. The training sample is only collected\nin simulation. Extensive qualitative and quantitative experiments demonstrate\nthe accuracy and robustness of our proposed approach. In the real-world grasp\nexperiments, we achieve a grasp success rate of 93% on a set of household items\nand 91% on a set of adversarial items with only about 6,300 simulated samples.\nWe also achieve 87% accuracy in clutter scenario. Although the model is trained\nusing only RGB image, when changing the background textures, it also performs\nwell and can achieve even 94% accuracy on the set of adversarial objects, which\noutperforms current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 13:05:36 GMT"}, {"version": "v2", "created": "Sat, 23 Feb 2019 01:25:51 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Cai", "Junhao", ""], ["Cheng", "Hui", ""], ["Zhang", "Zhanpeng", ""], ["Su", "Jingcheng", ""]]}, {"id": "1902.06557", "submitter": "Sarah Alotaibi", "authors": "Sarah Alotaibi and William A. P. Smith", "title": "Decomposing multispectral face images into diffuse and specular shading\n  and biophysical parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel biophysical and dichromatic reflectance model that\nefficiently characterises spectral skin reflectance. We show how to fit the\nmodel to multispectral face images enabling high quality estimation of diffuse\nand specular shading as well as biophysical parameter maps (melanin and\nhaemoglobin). Our method works from a single image without requiring complex\ncontrolled lighting setups yet provides quantitatively accurate reconstructions\nand qualitatively convincing decomposition and editing.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 13:09:35 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Alotaibi", "Sarah", ""], ["Smith", "William A. P.", ""]]}, {"id": "1902.06585", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Jinsol Lee and Ghassan AlRegib", "title": "Object Recognition under Multifarious Conditions: A Reliability Analysis\n  and A Feature Similarity-based Performance Estimation", "comments": "5 pages, 3 figures, 1 table", "journal-ref": "IEEE International Conference on Image Processing, Taipei, Taiwan,\n  2019", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the reliability of online recognition\nplatforms, Amazon Rekognition and Microsoft Azure, with respect to changes in\nbackground, acquisition device, and object orientation. We focus on platforms\nthat are commonly used by the public to better understand their real-world\nperformances. To assess the variation in recognition performance, we perform a\ncontrolled experiment by changing the acquisition conditions one at a time. We\nuse three smartphones, one DSLR, and one webcam to capture side views and\noverhead views of objects in a living room, an office, and photo studio setups.\nMoreover, we introduce a framework to estimate the recognition performance with\nrespect to backgrounds and orientations. In this framework, we utilize both\nhandcrafted features based on color, texture, and shape characteristics and\ndata-driven features obtained from deep neural networks. Experimental results\nshow that deep learning-based image representations can estimate the\nrecognition performance variation with a Spearman's rank-order correlation of\n0.94 under multifarious acquisition conditions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 14:27:25 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 07:36:56 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Temel", "Dogancan", ""], ["Lee", "Jinsol", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1902.06634", "submitter": "Alexander Kroner", "authors": "Alexander Kroner, Mario Senden, Kurt Driessens, Rainer Goebel", "title": "Contextual Encoder-Decoder Network for Visual Saliency Prediction", "comments": "Accepted Manuscript", "journal-ref": "Neural Networks, 2020, Volume 129, Pages 261-270, ISSN 0893-6080", "doi": "10.1016/j.neunet.2020.05.004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting salient regions in natural images requires the detection of\nobjects that are present in a scene. To develop robust representations for this\nchallenging task, high-level visual features at multiple spatial scales must be\nextracted and augmented with contextual information. However, existing models\naimed at explaining human fixation maps do not incorporate such a mechanism\nexplicitly. Here we propose an approach based on a convolutional neural network\npre-trained on a large-scale image classification task. The architecture forms\nan encoder-decoder structure and includes a module with multiple convolutional\nlayers at different dilation rates to capture multi-scale features in parallel.\nMoreover, we combine the resulting representations with global scene\ninformation for accurately predicting visual saliency. Our model achieves\ncompetitive and consistent results across multiple evaluation metrics on two\npublic saliency benchmarks and we demonstrate the effectiveness of the\nsuggested approach on five datasets and selected examples. Compared to state of\nthe art approaches, the network is based on a lightweight image classification\nbackbone and hence presents a suitable choice for applications with limited\ncomputational resources, such as (virtual) robotic systems, to estimate human\nfixations across complex natural scenes.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 16:15:25 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 15:41:55 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 11:35:47 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Kroner", "Alexander", ""], ["Senden", "Mario", ""], ["Driessens", "Kurt", ""], ["Goebel", "Rainer", ""]]}, {"id": "1902.06676", "submitter": "Stephen Odaibo", "authors": "Stephen G. Odaibo, M.D., M.S.(Math), M.S.(Comp. Sci.)", "title": "Generative Adversarial Networks Synthesize Realistic OCT Images of the\n  Retina", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report, to our knowledge, the first end-to-end application of Generative\nAdversarial Networks (GANs) towards the synthesis of Optical Coherence\nTomography (OCT) images of the retina. Generative models have gained recent\nattention for the increasingly realistic images they can synthesize, given a\nsampling of a data type. In this paper, we apply GANs to a sampling\ndistribution of OCTs of the retina. We observe the synthesis of realistic OCT\nimages depicting recognizable pathology such as macular holes, choroidal\nneovascular membranes, myopic degeneration, cystoid macular edema, and central\nserous retinopathy amongst others. This represents the first such report of its\nkind. Potential applications of this new technology include for surgical\nsimulation, for treatment planning, for disease prognostication, and for\naccelerating the development of new drugs and surgical procedures to treat\nretinal disease.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 17:47:53 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Odaibo", "Stephen G.", "", "Math"], ["D.", "M.", "", "Math"], ["S.", "M.", "", "Math"], ["S.", "M.", "", "Math"]]}, {"id": "1902.06701", "submitter": "Swalpa Kumar Roy Mr.", "authors": "Swalpa Kumar Roy, Gopal Krishna, Shiv Ram Dubey and Bidyut B.\n  Chaudhuri", "title": "HybridSN: Exploring 3D-2D CNN Feature Hierarchy for Hyperspectral Image\n  Classification", "comments": "Published in IEEE Geoscience and Remote Sensing Letters", "journal-ref": null, "doi": "10.1109/LGRS.2019.2918719", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral image (HSI) classification is widely used for the analysis of\nremotely sensed images. Hyperspectral imagery includes varying bands of images.\nConvolutional Neural Network (CNN) is one of the most frequently used deep\nlearning based methods for visual data processing. The use of CNN for HSI\nclassification is also visible in recent works. These approaches are mostly\nbased on 2D CNN. Whereas, the HSI classification performance is highly\ndependent on both spatial and spectral information. Very few methods have\nutilized the 3D CNN because of increased computational complexity. This letter\nproposes a Hybrid Spectral Convolutional Neural Network (HybridSN) for HSI\nclassification. Basically, the HybridSN is a spectral-spatial 3D-CNN followed\nby spatial 2D-CNN. The 3D-CNN facilitates the joint spatial-spectral feature\nrepresentation from a stack of spectral bands. The 2D-CNN on top of the 3D-CNN\nfurther learns more abstract level spatial representation. Moreover, the use of\nhybrid CNNs reduces the complexity of the model compared to 3D-CNN alone. To\ntest the performance of this hybrid approach, very rigorous HSI classification\nexperiments are performed over Indian Pines, Pavia University and Salinas Scene\nremote sensing datasets. The results are compared with the state-of-the-art\nhand-crafted as well as end-to-end deep learning based methods. A very\nsatisfactory performance is obtained using the proposed HybridSN for HSI\nclassification. The source code can be found at\n\\url{https://github.com/gokriznastic/HybridSN}.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 18:14:26 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 03:03:43 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 06:05:46 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Roy", "Swalpa Kumar", ""], ["Krishna", "Gopal", ""], ["Dubey", "Shiv Ram", ""], ["Chaudhuri", "Bidyut B.", ""]]}, {"id": "1902.06729", "submitter": "Daeyun Shin", "authors": "Daeyun Shin, Zhile Ren, Erik B. Sudderth, Charless C. Fowlkes", "title": "3D Scene Reconstruction with Multi-layer Depth and Epipolar Transformers", "comments": "Accepted at ICCV 2019. Paper title changed. Project web page:\n  https://research.dshin.org/iccv19/multi-layer-depth", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of automatically reconstructing a complete 3D model of\na scene from a single RGB image. This challenging task requires inferring the\nshape of both visible and occluded surfaces. Our approach utilizes\nviewer-centered, multi-layer representation of scene geometry adapted from\nrecent methods for single object shape completion. To improve the accuracy of\nview-centered representations for complex scenes, we introduce a novel\n\"Epipolar Feature Transformer\" that transfers convolutional network features\nfrom an input view to other virtual camera viewpoints, and thus better covers\nthe 3D scene geometry. Unlike existing approaches that first detect and\nlocalize objects in 3D, and then infer object shape using category-specific\nmodels, our approach is fully convolutional, end-to-end differentiable, and\navoids the resolution and memory limitations of voxel representations. We\ndemonstrate the advantages of multi-layer depth representations and epipolar\nfeature transformers on the reconstruction of a large database of indoor\nscenes.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 18:55:22 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 17:25:32 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Shin", "Daeyun", ""], ["Ren", "Zhile", ""], ["Sudderth", "Erik B.", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1902.06804", "submitter": "Alexander Wong", "authors": "Raymond Bond, Ansgar Koene, Alan Dix, Jennifer Boger, Maurice D.\n  Mulvenna, Mykola Galushka, Bethany Waterhouse Bradley, Fiona Browne, Hui\n  Wang, and Alexander Wong", "title": "Democratisation of Usable Machine Learning in Computer Vision", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many industries are now investing heavily in data science and automation to\nreplace manual tasks and/or to help with decision making, especially in the\nrealm of leveraging computer vision to automate many monitoring, inspection,\nand surveillance tasks. This has resulted in the emergence of the 'data\nscientist' who is conversant in statistical thinking, machine learning (ML),\ncomputer vision, and computer programming. However, as ML becomes more\naccessible to the general public and more aspects of ML become automated,\napplications leveraging computer vision are increasingly being created by\nnon-experts with less opportunity for regulatory oversight. This points to the\noverall need for more educated responsibility for these lay-users of usable ML\ntools in order to mitigate potentially unethical ramifications. In this paper,\nwe undertake a SWOT analysis to study the strengths, weaknesses, opportunities,\nand threats of building usable ML tools for mass adoption for important areas\nleveraging ML such as computer vision. The paper proposes a set of data science\nliteracy criteria for educating and supporting lay-users in the responsible\ndevelopment and deployment of ML applications.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 21:22:45 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Bond", "Raymond", ""], ["Koene", "Ansgar", ""], ["Dix", "Alan", ""], ["Boger", "Jennifer", ""], ["Mulvenna", "Maurice D.", ""], ["Galushka", "Mykola", ""], ["Bradley", "Bethany Waterhouse", ""], ["Browne", "Fiona", ""], ["Wang", "Hui", ""], ["Wong", "Alexander", ""]]}, {"id": "1902.06806", "submitter": "Amy Tabb", "authors": "Philipe A. Dias and Zhou Shen and Amy Tabb and Henry Medeiros", "title": "FreeLabel: A Publicly Available Annotation Tool based on Freehand Traces", "comments": "Accepted and presented at 2019 IEEE Winter Conference on Applications\n  of Computer Vision (WACV). 10 pages", "journal-ref": "2019 IEEE Winter Conference on Applications of Computer Vision\n  (WACV)", "doi": "10.1109/WACV.2019.00010", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale annotation of image segmentation datasets is often prohibitively\nexpensive, as it usually requires a huge number of worker hours to obtain\nhigh-quality results. Abundant and reliable data has been, however, crucial for\nthe advances on image understanding tasks achieved by deep learning models. In\nthis paper, we introduce FreeLabel, an intuitive open-source web interface that\nallows users to obtain high-quality segmentation masks with just a few freehand\nscribbles, in a matter of seconds. The efficacy of FreeLabel is quantitatively\ndemonstrated by experimental results on the PASCAL dataset as well as on a\ndataset from the agricultural domain. Designed to benefit the computer vision\ncommunity, FreeLabel can be used for both crowdsourced or private annotation\nand has a modular structure that can be easily adapted for any image dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 21:47:39 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 15:34:45 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Dias", "Philipe A.", ""], ["Shen", "Zhou", ""], ["Tabb", "Amy", ""], ["Medeiros", "Henry", ""]]}, {"id": "1902.06820", "submitter": "Alex Zihao Zhu", "authors": "Alex Zihao Zhu, Ziyun Wang, Kostas Daniilidis", "title": "Motion Equivariant Networks for Event Cameras with the Temporal\n  Normalization Transform", "comments": "8 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel transformation for events from an event\ncamera that is equivariant to optical flow under convolutions in the 3-D\nspatiotemporal domain. Events are generated by changes in the image, which are\ntypically due to motion, either of the camera or the scene. As a result,\ndifferent motions result in a different set of events. For learning based tasks\nbased on a static scene such as classification which directly use the events,\nwe must either rely on the learning method to learn the underlying object\ndistinct from the motion, or to memorize all possible motions for each object\nwith extensive data augmentation. Instead, we propose a novel transformation of\nthe input event data which normalizes the $x$ and $y$ positions by the\ntimestamp of each event. We show that this transformation generates a\nrepresentation of the events that is equivariant to this motion when the\noptical flow is constant, allowing a deep neural network to learn the\nclassification task without the need for expensive data augmentation. We test\nour method on the event based N-MNIST dataset, as well as a novel dataset\nN-MOVING-MNIST, with significantly more variety in motion compared to the\nstandard N-MNIST dataset. In all sequences, we demonstrate that our transformed\nnetwork is able to achieve similar or better performance compared to a network\nwith a standard volumetric event input, and performs significantly better when\nthe test set has a larger set of motions than seen at training.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 22:21:59 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Zhu", "Alex Zihao", ""], ["Wang", "Ziyun", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1902.06822", "submitter": "Yoni Choukroun", "authors": "Yoni Choukroun, Eli Kravchik, Fan Yang, Pavel Kisilev", "title": "Low-bit Quantization of Neural Networks for Efficient Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent machine learning methods use increasingly large deep neural networks\nto achieve state of the art results in various tasks. The gains in performance\ncome at the cost of a substantial increase in computation and storage\nrequirements. This makes real-time implementations on limited resources\nhardware a challenging task. One popular approach to address this challenge is\nto perform low-bit precision computations via neural network quantization.\nHowever, aggressive quantization generally entails a severe penalty in terms of\naccuracy, and often requires retraining of the network, or resorting to higher\nbit precision quantization. In this paper, we formalize the linear quantization\ntask as a Minimum Mean Squared Error (MMSE) problem for both weights and\nactivations, allowing low-bit precision inference without the need for full\nnetwork retraining. The main contributions of our approach are the\noptimizations of the constrained MSE problem at each layer of the network, the\nhardware aware partitioning of the network parameters, and the use of multiple\nlow precision quantized tensors for poorly approximated layers. The proposed\napproach allows 4 bits integer (INT4) quantization for deployment of pretrained\nmodels on limited hardware resources. Multiple experiments on various network\narchitectures show that the suggested method yields state of the art results\nwith minimal loss of tasks accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 22:28:34 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 08:12:15 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Choukroun", "Yoni", ""], ["Kravchik", "Eli", ""], ["Yang", "Fan", ""], ["Kisilev", "Pavel", ""]]}, {"id": "1902.06835", "submitter": "Michael Zollh\\\"ofer", "authors": "Michael Zollh\\\"ofer", "title": "Commodity RGB-D Sensors: Data Acquisition", "comments": "Contributed chapter to a book on \"RGB-D Image Analysis and\n  Processing\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past ten years we have seen a democratization of range sensing\ntechnology. While previously range sensors have been highly expensive and only\naccessible to a few domain experts, such sensors are nowadays ubiquitous and\ncan even be found in the latest generation of mobile devices, e.g., current\nsmartphones. This democratization of range sensing technology was started with\nthe release of the Microsoft Kinect, and since then many different commodity\nrange sensors followed its lead, such as the Primesense Carmine, Asus Xtion\nPro, and the Structure Sensor from Occipital. The availability of cheap range\nsensing technology led to a big leap in research, especially in the context of\nmore powerful static and dynamic reconstruction techniques, starting from 3D\nscanning applications, such as KinectFusion, to highly accurate face and body\ntracking approaches. In this chapter, we have a detailed look into the\ndifferent types of existing range sensors. We discuss the two fundamental types\nof commodity range sensing techniques in detail, namely passive and active\nsensing, and we explore the principles these technologies are based on. Our\nfocus is on modern active commodity range sensors based on time-of-flight and\nstructured light. We conclude by discussing the noise characteristics, working\nranges, and types of errors made by the different sensing modalities.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 23:17:19 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Zollh\u00f6fer", "Michael", ""]]}, {"id": "1902.06838", "submitter": "Youngjoo Jo", "authors": "Youngjoo Jo, Jongyoul Park", "title": "SC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch\n  and Color", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel image editing system that generates images as the user\nprovides free-form mask, sketch and color as an input. Our system consist of a\nend-to-end trainable convolutional network. Contrary to the existing methods,\nour system wholly utilizes free-form user input with color and shape. This\nallows the system to respond to the user's sketch and color input, using it as\na guideline to generate an image. In our particular work, we trained network\nwith additional style loss which made it possible to generate realistic\nresults, despite large portions of the image being removed. Our proposed\nnetwork architecture SC-FEGAN is well suited to generate high quality synthetic\nimage using intuitive user inputs.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 23:28:30 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Jo", "Youngjoo", ""], ["Park", "Jongyoul", ""]]}, {"id": "1902.06854", "submitter": "Qingqiu Huang", "authors": "Chen Change Loy, Dahua Lin, Wanli Ouyang, Yuanjun Xiong, Shuo Yang,\n  Qingqiu Huang, Dongzhan Zhou, Wei Xia, Quanquan Li, Ping Luo, Junjie Yan,\n  Jianfeng Wang, Zuoxin Li, Ye Yuan, Boxun Li, Shuai Shao, Gang Yu, Fangyun\n  Wei, Xiang Ming, Dong Chen, Shifeng Zhang, Cheng Chi, Zhen Lei, Stan Z. Li,\n  Hongkai Zhang, Bingpeng Ma, Hong Chang, Shiguang Shan, Xilin Chen, Wu Liu,\n  Boyan Zhou, Huaxiong Li, Peng Cheng, Tao Mei, Artem Kukharenko, Artem\n  Vasenin, Nikolay Sergievskiy, Hua Yang, Liangqi Li, Qiling Xu, Yuan Hong, Lin\n  Chen, Mingjun Sun, Yirong Mao, Shiying Luo, Yongjun Li, Ruiping Wang,\n  Qiaokang Xie, Ziyang Wu, Lei Lu, Yiheng Liu, Wengang Zhou", "title": "WIDER Face and Pedestrian Challenge 2018: Methods and Results", "comments": "Report of ECCV 2018 workshop: WIDER Face and Pedestrian Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a review of the 2018 WIDER Challenge on Face and\nPedestrian. The challenge focuses on the problem of precise localization of\nhuman faces and bodies, and accurate association of identities. It comprises of\nthree tracks: (i) WIDER Face which aims at soliciting new approaches to advance\nthe state-of-the-art in face detection, (ii) WIDER Pedestrian which aims to\nfind effective and efficient approaches to address the problem of pedestrian\ndetection in unconstrained environments, and (iii) WIDER Person Search which\npresents an exciting challenge of searching persons across 192 movies. In\ntotal, 73 teams made valid submissions to the challenge tracks. We summarize\nthe winning solutions for all three tracks. and present discussions on open\nproblems and potential research directions in these topics.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 01:09:56 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Loy", "Chen Change", ""], ["Lin", "Dahua", ""], ["Ouyang", "Wanli", ""], ["Xiong", "Yuanjun", ""], ["Yang", "Shuo", ""], ["Huang", "Qingqiu", ""], ["Zhou", "Dongzhan", ""], ["Xia", "Wei", ""], ["Li", "Quanquan", ""], ["Luo", "Ping", ""], ["Yan", "Junjie", ""], ["Wang", "Jianfeng", ""], ["Li", "Zuoxin", ""], ["Yuan", "Ye", ""], ["Li", "Boxun", ""], ["Shao", "Shuai", ""], ["Yu", "Gang", ""], ["Wei", "Fangyun", ""], ["Ming", "Xiang", ""], ["Chen", "Dong", ""], ["Zhang", "Shifeng", ""], ["Chi", "Cheng", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""], ["Zhang", "Hongkai", ""], ["Ma", "Bingpeng", ""], ["Chang", "Hong", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""], ["Liu", "Wu", ""], ["Zhou", "Boyan", ""], ["Li", "Huaxiong", ""], ["Cheng", "Peng", ""], ["Mei", "Tao", ""], ["Kukharenko", "Artem", ""], ["Vasenin", "Artem", ""], ["Sergievskiy", "Nikolay", ""], ["Yang", "Hua", ""], ["Li", "Liangqi", ""], ["Xu", "Qiling", ""], ["Hong", "Yuan", ""], ["Chen", "Lin", ""], ["Sun", "Mingjun", ""], ["Mao", "Yirong", ""], ["Luo", "Shiying", ""], ["Li", "Yongjun", ""], ["Wang", "Ruiping", ""], ["Xie", "Qiaokang", ""], ["Wu", "Ziyang", ""], ["Lu", "Lei", ""], ["Liu", "Yiheng", ""], ["Zhou", "Wengang", ""]]}, {"id": "1902.06857", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Tariq Alshawi and Min-Hung Chen and Ghassan AlRegib", "title": "Challenging Environments for Traffic Sign Detection: Reliability\n  Assessment under Inclement Conditions", "comments": "26 pages, 22 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art algorithms successfully localize and recognize traffic signs\nover existing datasets, which are limited in terms of challenging condition\ntype and severity. Therefore, it is not possible to estimate the performance of\ntraffic sign detection algorithms under overlooked challenging conditions.\nAnother shortcoming of existing datasets is the limited utilization of temporal\ninformation and the unavailability of consecutive frames and annotations. To\novercome these shortcomings, we generated the CURE-TSD video dataset and hosted\nthe first IEEE Video and Image Processing (VIP) Cup within the IEEE Signal\nProcessing Society. In this paper, we provide a detailed description of the\nCURE-TSD dataset, analyze the characteristics of the top performing algorithms,\nand provide a performance benchmark. Moreover, we investigate the robustness of\nthe benchmarked algorithms with respect to sign size, challenge type and\nseverity. Benchmarked algorithms are based on state-of-the-art and custom\nconvolutional neural networks that achieved a precision of 0.55 and a recall of\n0.32, F0.5 score of 0.48 and F2 score of 0.35. Experimental results show that\nbenchmarked algorithms are highly sensitive to tested challenging conditions,\nwhich result in an average performance drop of 0.17 in terms of precision and a\nperformance drop of 0.28 in recall under severe conditions. The dataset is\npublicly available at https://github.com/olivesgatech/CURE-TSD.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 01:46:57 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 21:23:28 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Temel", "Dogancan", ""], ["Alshawi", "Tariq", ""], ["Chen", "Min-Hung", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1902.06871", "submitter": "Jorge Camargo", "authors": "Sergio Acosta, Jorge E. Camargo", "title": "Predicting city safety perception based on visual image content", "comments": "CIARP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safety perception measurement has been a subject of interest in many cities\nof the world. This is due to its social relevance, and to its effect on some\nlocal economic activities. Even though people safety perception is a subjective\ntopic, sometimes it is possible to find out common patterns given a restricted\ngeographical and sociocultural context. This paper presents an approach that\nmakes use of image processing and machine learning techniques to detect with\nhigh accuracy urban environment patterns that could affect citizen's safety\nperception.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 02:59:44 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Acosta", "Sergio", ""], ["Camargo", "Jorge E.", ""]]}, {"id": "1902.06913", "submitter": "Shaojie Xu", "authors": "Shaojie Xu, Sihan Zeng, Justin Romberg", "title": "Fast Compressive Sensing Recovery Using Generative Models with\n  Structured Latent Variables", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2019.8683641", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have significantly improved the visual quality and\naccuracy on compressive sensing recovery. In this paper, we propose an\nalgorithm for signal reconstruction from compressed measurements with image\npriors captured by a generative model. We search and constrain on latent\nvariable space to make the method stable when the number of compressed\nmeasurements is extremely limited. We show that, by exploiting certain\nstructures of the latent variables, the proposed method produces improved\nreconstruction accuracy and preserves realistic and non-smooth features in the\nimage. Our algorithm achieves high computation speed by projecting between the\noriginal signal space and the latent variable space in an alternating fashion.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 06:18:59 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 02:27:16 GMT"}, {"version": "v3", "created": "Fri, 8 Nov 2019 17:08:57 GMT"}, {"version": "v4", "created": "Thu, 19 Mar 2020 16:16:54 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Xu", "Shaojie", ""], ["Zeng", "Sihan", ""], ["Romberg", "Justin", ""]]}, {"id": "1902.06919", "submitter": "Yafei Song", "authors": "Yafei Song, Yonghong Tian, Gang Wang, Mingyang Li", "title": "2D LiDAR Map Prediction via Estimating Motion Flow with GRU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a significant problem to predict the 2D LiDAR map at next moment for\nrobotics navigation and path-planning. To tackle this problem, we resort to the\nmotion flow between adjacent maps, as motion flow is a powerful tool to process\nand analyze the dynamic data, which is named optical flow in video processing.\nHowever, unlike video, which contains abundant visual features in each frame, a\n2D LiDAR map lacks distinctive local features. To alleviate this challenge, we\npropose to estimate the motion flow based on deep neural networks inspired by\nits powerful representation learning ability in estimating the optical flow of\nthe video. To this end, we design a recurrent neural network based on gated\nrecurrent unit, which is named LiDAR-FlowNet. As a recurrent neural network can\nencode the temporal dynamic information, our LiDAR-FlowNet can estimate motion\nflow between the current map and the unknown next map only from the current\nframe and previous frames. A self-supervised strategy is further designed to\ntrain the LiDAR-FlowNet model effectively, while no training data need to be\nmanually annotated. With the estimated motion flow, it is straightforward to\npredict the 2D LiDAR map at the next moment. Experimental results verify the\neffectiveness of our LiDAR-FlowNet as well as the proposed training strategy.\nThe results of the predicted LiDAR map also show the advantages of our motion\nflow based method.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 06:47:49 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Song", "Yafei", ""], ["Tian", "Yonghong", ""], ["Wang", "Gang", ""], ["Li", "Mingyang", ""]]}, {"id": "1902.06923", "submitter": "Xueqing Deng", "authors": "Xueqing Deng, Yi Zhu and Shawn Newsam", "title": "Using Conditional Generative Adversarial Networks to Generate\n  Ground-Level Views From Overhead Imagery", "comments": "5 pages. arXiv admin note: text overlap with arXiv:1806.05129", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper develops a deep-learning framework to synthesize a ground-level\nview of a location given an overhead image. We propose a novel conditional\ngenerative adversarial network (cGAN) in which the trained generator generates\nrealistic looking and representative ground-level images using overhead imagery\nas auxiliary information. The generator is an encoder-decoder network which\nallows us to compare low- and high-level features as well as their\nconcatenation for encoding the overhead imagery. We also demonstrate how our\nframework can be used to perform land cover classification by modifying the\ntrained cGAN to extract features from overhead imagery. This is interesting\nbecause, although we are using this modified cGAN as a feature extractor for\noverhead imagery, it incorporates knowledge of how locations look from the\nground.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 06:58:36 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Deng", "Xueqing", ""], ["Zhu", "Yi", ""], ["Newsam", "Shawn", ""]]}, {"id": "1902.06924", "submitter": "Ha Son Vu", "authors": "Ha Son Vu, Daisuke Ueta, Kiyoshi Hashimoto, Kazuki Maeno, Sugiri\n  Pranata and Sheng Mei Shen", "title": "Anomaly Detection with Adversarial Dual Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised and unsupervised Generative Adversarial Networks (GAN)-based\nmethods have been gaining popularity in anomaly detection task recently.\nHowever, GAN training is somewhat challenging and unstable. Inspired from\nprevious work in GAN-based image generation, we introduce a GAN-based anomaly\ndetection framework - Adversarial Dual Autoencoders (ADAE) - consists of two\nautoencoders as generator and discriminator to increase training stability. We\nalso employ discriminator reconstruction error as anomaly score for better\ndetection performance. Experiments across different datasets of varying\ncomplexity show strong evidence of a robust model that can be used in different\nscenarios, one of which is brain tumor detection.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 06:59:30 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Vu", "Ha Son", ""], ["Ueta", "Daisuke", ""], ["Hashimoto", "Kiyoshi", ""], ["Maeno", "Kazuki", ""], ["Pranata", "Sugiri", ""], ["Shen", "Sheng Mei", ""]]}, {"id": "1902.06927", "submitter": "Kele Xu", "authors": "Chaojie Zhao and Peng Zhang and Jian Zhu and Chengrui Wu and Huaimin\n  Wang and Kele Xu", "title": "Predicting tongue motion in unlabeled ultrasound videos using\n  convolutional LSTM neural network", "comments": "Accepted by ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A challenge in speech production research is to predict future tongue\nmovements based on a short period of past tongue movements. This study tackles\nspeaker-dependent tongue motion prediction problem in unlabeled ultrasound\nvideos with convolutional long short-term memory (ConvLSTM) networks. The model\nhas been tested on two different ultrasound corpora. ConvLSTM outperforms\n3-dimensional convolutional neural network (3DCNN) in predicting the\n9\\textsuperscript{th} frames based on 8 preceding frames, and also demonstrates\ngood capacity to predict only the tongue contours in future frames. Further\ntests reveal that ConvLSTM can also learn to predict tongue movements in more\ndistant frames beyond the immediately following frames. Our codes are available\nat: https://github.com/shuiliwanwu/ConvLstm-ultrasound-videos.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 07:11:28 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Zhao", "Chaojie", ""], ["Zhang", "Peng", ""], ["Zhu", "Jian", ""], ["Wu", "Chengrui", ""], ["Wang", "Huaimin", ""], ["Xu", "Kele", ""]]}, {"id": "1902.06942", "submitter": "Wei Zheng", "authors": "Zhenyu Wang, Wei Zheng, Chunfeng Song", "title": "Air Quality Measurement Based on Double-Channel Convolutional Neural\n  Network Ensemble Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental air quality affects people's life, obtaining real-time and\naccurate environmental air quality has a profound guiding significance for the\ndevelopment of social activities. At present, environmental air quality\nmeasurement mainly adopts the method that setting air quality detector at\nspecific monitoring points in cities and timing sampling analysis, which is\neasy to be restricted by time and space factors. Some air quality measurement\nalgorithms related to deep learning mostly adopt a single convolutional neural\nnetwork to train the whole image, which will ignore the difference of different\nparts of the image. In this paper, we propose a method for air quality\nmeasurement based on double-channel convolutional neural network ensemble\nlearning to solve the problem of feature extraction for different parts of\nenvironmental images. Our method mainly includes two aspects: ensemble learning\nof double-channel convolutional neural network and self-learning weighted\nfeature fusion. We constructed a double-channel convolutional neural network,\nused each channel to train different parts of the environment images for\nfeature extraction. We propose a feature weight self-learning method, which\nweights and concatenates the extracted feature vectors, and uses the fused\nfeature vectors to measure air quality. Our method can be applied to the two\ntasks of air quality grade measurement and air quality index (AQI) measurement.\nMoreover, we build an environmental image dataset of random time and location\ncondition. The experiments show that our method can achieve nearly 82% accuracy\nand a small mean absolute error (MAE) on our test dataset. At the same time,\nthrough comparative experiment, we proved that our proposed method gained\nconsiderable improvement in performance compared with single channel\nconvolutional neural network air quality measurements.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 08:05:54 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 03:38:56 GMT"}, {"version": "v3", "created": "Thu, 28 Feb 2019 09:39:00 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Wang", "Zhenyu", ""], ["Zheng", "Wei", ""], ["Song", "Chunfeng", ""]]}, {"id": "1902.06964", "submitter": "Ankita Shukla", "authors": "Ankita Shukla, Shagun Uppal, Sarthak Bhagat, Saket Anand, Pavan Turaga", "title": "Geometry of Deep Generative Models for Disentangled Representations", "comments": "Accepted at ICVGIP, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models like variational autoencoders approximate the\nintrinsic geometry of high dimensional data manifolds by learning\nlow-dimensional latent-space variables and an embedding function. The geometric\nproperties of these latent spaces has been studied under the lens of Riemannian\ngeometry; via analysis of the non-linearity of the generator function. In new\ndevelopments, deep generative models have been used for learning semantically\nmeaningful `disentangled' representations; that capture task relevant\nattributes while being invariant to other attributes. In this work, we explore\nthe geometry of popular generative models for disentangled representation\nlearning. We use several metrics to compare the properties of latent spaces of\ndisentangled representation models in terms of class separability and curvature\nof the latent-space. The results we obtain establish that the class\ndistinguishable features in the disentangled latent space exhibits higher\ncurvature as opposed to a variational autoencoder. We evaluate and compare the\ngeometry of three such models with variational autoencoder on two different\ndatasets. Further, our results show that distances and interpolation in the\nlatent space are significantly improved with Riemannian metrics derived from\nthe curvature of the space. We expect these results will have implications on\nunderstanding how deep-networks can be made more robust, generalizable, as well\nas interpretable.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 09:29:57 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Shukla", "Ankita", ""], ["Uppal", "Shagun", ""], ["Bhagat", "Sarthak", ""], ["Anand", "Saket", ""], ["Turaga", "Pavan", ""]]}, {"id": "1902.06967", "submitter": "Asal Baragchizadeh", "authors": "Kimberley D. Orsten-Hooge, Asal Baragchizadeh, Thomas P. Karnowski,\n  David S. Bolme, Regina Ferrell, Parisa R. Jesudasen, Carlos D. Castillo, and\n  Alice J. O'Toole", "title": "Evaluating the Effectiveness of Automated Identity Masking (AIM) Methods\n  with Human Perception and a Deep Convolutional Neural Network (CNN)", "comments": "*K.O.H and A.B. contributed equally to this work.10 pages, 4 tables,\n  7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face de-identification algorithms have been developed in response to the\nprevalent use of public video recordings and surveillance cameras. Here, we\nevaluated the success of identity masking in the context of monitoring drivers\nas they actively operate a motor vehicle. We studied the effectiveness of eight\nde-identification algorithms using human perceivers and a state-of-the-art deep\nconvolutional neural network (CNN). We used a standard face recognition\nexperiment in which human subjects studied high-resolution (studio-style)\nimages to learn driver identities. Subjects were tested subsequently on their\nability to recognize those identities in low-resolution videos depicting the\ndrivers operating a motor vehicle. The videos were in either unmasked format,\nor were masked by one of the eight de-identification algorithms. All masking\nalgorithms lowered identification accuracy substantially, relative to the\nunmasked video. In all cases, identifications were made with stringent decision\ncriteria indicating the subjects had low confidence in their decisions. When\nmatching the identities in high-resolution still images to those in the masked\nvideos, the CNN performed at chance. Next, we examined CNN performance on the\nsame task, but using the unmasked videos and their masked counterparts. In this\ncase, the network scored surprisingly well on a subset of mask conditions. We\nconclude that carefully tested de-identification approaches, used alone or in\ncombination, can be an effective tool for protecting the privacy of individuals\ncaptured in videos. We note that no approach is equally effective in masking\nall stimuli, and that future work should examine possible methods for\ndetermining the most effective mask per individual stimulus.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 09:34:12 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 15:32:28 GMT"}, {"version": "v3", "created": "Mon, 4 Nov 2019 17:38:58 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Orsten-Hooge", "Kimberley D.", ""], ["Baragchizadeh", "Asal", ""], ["Karnowski", "Thomas P.", ""], ["Bolme", "David S.", ""], ["Ferrell", "Regina", ""], ["Jesudasen", "Parisa R.", ""], ["Castillo", "Carlos D.", ""], ["O'Toole", "Alice J.", ""]]}, {"id": "1902.07017", "submitter": "Xiaojie Li", "authors": "Xiaojie Li, Lu Yang, Qing Song, Fuqiang Zhou", "title": "Detector-in-Detector: Multi-Level Analysis for Human-Parts", "comments": "13 pages, 4 figures, accepted by ACCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based person, hand or face detection approaches have achieved\nincredible success in recent years with the development of deep convolutional\nneural network (CNN). In this paper, we take the inherent correlation between\nthe body and body parts into account and propose a new framework to boost up\nthe detection performance of the multi-level objects. In particular, we adopt a\nregion-based object detection structure with two carefully designed detectors\nto separately pay attention to the human body and body parts in a\ncoarse-to-fine manner, which we call Detector-in-Detector network (DID-Net).\nThe first detector is designed to detect human body, hand, and face. The second\ndetector, based on the body detection results of the first detector, mainly\nfocus on the detection of small hand and face inside each body. The framework\nis trained in an end-to-end way by optimizing a multi-task loss. Due to the\nlack of human body, face and hand detection dataset, we have collected and\nlabeled a new large dataset named Human-Parts with 14,962 images and 106,879\nannotations. Experiments show that our method can achieve excellent performance\non Human-Parts.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 12:23:59 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Li", "Xiaojie", ""], ["Yang", "Lu", ""], ["Song", "Qing", ""], ["Zhou", "Fuqiang", ""]]}, {"id": "1902.07069", "submitter": "Ryan Wen Liu", "authors": "Qiaoling Shu, Chuansheng Wu, Zhe Xiao, Ryan Wen Liu", "title": "Variational Regularized Transmission Refinement for Image Dehazing", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-quality dehazing performance is highly dependent upon the accurate\nestimation of transmission map. In this work, the coarse estimation version is\nfirst obtained by weightedly fusing two different transmission maps, which are\ngenerated from foreground and sky regions, respectively. A hybrid variational\nmodel with promoted regularization terms is then proposed to assisting in\nrefining transmission map. The resulting complicated optimization problem is\neffectively solved via an alternating direction algorithm. The final haze-free\nimage can be effectively obtained according to the refined transmission map and\natmospheric scattering model. Our dehazing framework has the capacity of\npreserving important image details while suppressing undesirable artifacts,\neven for hazy images with large sky regions. Experiments on both synthetic and\nrealistic images have illustrated that the proposed method is competitive with\nor even outperforms the state-of-the-art dehazing techniques under different\nimaging conditions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 14:28:57 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Shu", "Qiaoling", ""], ["Wu", "Chuansheng", ""], ["Xiao", "Zhe", ""], ["Liu", "Ryan Wen", ""]]}, {"id": "1902.07090", "submitter": "Ryan Wen Liu", "authors": "Zhaoyang Sun, Shengwu Xiong, Ryan Wen Liu", "title": "Directional Regularized Tensor Modeling for Video Rain Streaks Removal", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outdoor videos sometimes contain unexpected rain streaks due to the rainy\nweather, which bring negative effects on subsequent computer vision\napplications, e.g., video surveillance, object recognition and tracking, etc.\nIn this paper, we propose a directional regularized tensor-based video\nderaining model by taking into consideration the arbitrary direction of rain\nstreaks. In particular, the sparsity of rain streaks in spatial and derivative\ndomains, the spatiotemporal sparsity and low-rank property of video background\nare incorporated into the proposed method. Different from many previous methods\nunder the assumption of vertically falling rain streaks, we consider a more\nrealistic assumption that all the rain streaks in a video fall in an\napproximately similar arbitrary direction. The resulting complicated\noptimization problem will be effectively solved through an alternating\ndirection method. Comprehensive experiments on both synthetic and realistic\ndatasets have demonstrated the superiority of the proposed deraining method.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 15:10:33 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Sun", "Zhaoyang", ""], ["Xiong", "Shengwu", ""], ["Liu", "Ryan Wen", ""]]}, {"id": "1902.07208", "submitter": "Chiyuan Zhang", "authors": "Maithra Raghu, Chiyuan Zhang, Jon Kleinberg and Samy Bengio", "title": "Transfusion: Understanding Transfer Learning for Medical Imaging", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning from natural image datasets, particularly ImageNet, using\nstandard large models and corresponding pretrained weights has become a\nde-facto method for deep learning applications to medical imaging. However,\nthere are fundamental differences in data sizes, features and task\nspecifications between natural image classification and the target medical\ntasks, and there is little understanding of the effects of transfer. In this\npaper, we explore properties of transfer learning for medical imaging. A\nperformance evaluation on two large scale medical imaging tasks shows that\nsurprisingly, transfer offers little benefit to performance, and simple,\nlightweight models can perform comparably to ImageNet architectures.\nInvestigating the learned representations and features, we find that some of\nthe differences from transfer learning are due to the over-parametrization of\nstandard models rather than sophisticated feature reuse. We isolate where\nuseful feature reuse occurs, and outline the implications for more efficient\nmodel exploration. We also explore feature independent benefits of transfer\narising from weight scalings.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 01:54:53 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 00:37:48 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 20:16:30 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Raghu", "Maithra", ""], ["Zhang", "Chiyuan", ""], ["Kleinberg", "Jon", ""], ["Bengio", "Samy", ""]]}, {"id": "1902.07262", "submitter": "Roy Shilkrot", "authors": "Roy Shilkrot, Zhi Chai and Minh Hoai", "title": "BusyHands: A Hand-Tool Interaction Database for Assembly Tasks Semantic\n  Segmentation", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual segmentation has seen tremendous advancement recently with ready\nsolutions for a wide variety of scene types, including human hands and other\nbody parts. However, focus on segmentation of human hands while performing\ncomplex tasks, such as manual assembly, is still severely lacking. Segmenting\nhands from tools, work pieces, background and other body parts is extremely\ndifficult because of self-occlusions and intricate hand grips and poses. In\nthis paper we introduce BusyHands, a large open dataset of pixel-level\nannotated images of hands performing 13 different tool-based assembly tasks,\nfrom both real-world captures and virtual-world renderings. A total of 7906\nsamples are included in our first-in-kind dataset, with both RGB and depth\nimages as obtained from a Kinect V2 camera and Blender. We evaluate several\nstate-of-the-art semantic segmentation methods on our dataset as a proposed\nperformance benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 19:59:46 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Shilkrot", "Roy", ""], ["Chai", "Zhi", ""], ["Hoai", "Minh", ""]]}, {"id": "1902.07283", "submitter": "Hamid Behjat", "authors": "Sevil Maghsadhagh, Anders Eklund, Hamid Behjat", "title": "Graph Spectral Characterization of Brain Cortical Morphology", "comments": "arXiv admin note: substantial text overlap with arXiv:1810.10339", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human brain cortical layer has a convoluted morphology that is unique to\neach individual. Characterization of the cortical morphology is necessary in\nlongitudinal studies of structural brain change, as well as in discriminating\nindividuals in health and disease. A method for encoding the cortical\nmorphology in the form of a graph is presented. The design of graphs that\nencode the global cerebral hemisphere cortices as well as localized cortical\nregions is proposed. Spectral metrics derived from these graphs are then\nstudied and proposed as descriptors of cortical morphology. As proof-of-concept\nof their applicability in characterizing cortical morphology, the metrics are\nstudied in the context of hemispheric asymmetry as well as gender dependent\ndiscrimination of cortical morphology.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 21:04:26 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Maghsadhagh", "Sevil", ""], ["Eklund", "Anders", ""], ["Behjat", "Hamid", ""]]}, {"id": "1902.07289", "submitter": "Yilin Liu", "authors": "Yilin Liu, Gengyan Zhao, Brendon M. Nacewicz, Nagesh Adluru, Gregory\n  R. Kirk, Peter A Ferrazzano, Martin Styner, Andrew L. Alexander", "title": "Accurate Automatic Segmentation of Amygdala Subnuclei and Modeling of\n  Uncertainty via Bayesian Fully Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have improved the segmentation accuracy of\nsubcortical brain structures, which would be useful in neuroimaging studies of\nmany neurological disorders. However, most of the previous deep learning work\ndoes not investigate the specific difficulties that exist in segmenting\nextremely small but important brain regions such as the amygdala and its\nsubregions. To tackle this challenging task, a novel 3D Bayesian fully\nconvolutional neural network was developed to apply a dilated dualpathway\napproach that retains fine details and utilizes both local and more global\ncontextual information to automatically segment the amygdala and its subregions\nat high precision. The proposed method provides insights on network design and\nsampling strategy that target segmentations of small 3D structures. In\nparticular, this study confirms that a large context, enabled by a large field\nof view, is beneficial for segmenting small objects; furthermore, precise\ncontextual information enabled by dilated convolutions allows for better\nboundary localization, which is critical for examining the morphology of the\nstructure. In addition, it is demonstrated that the uncertainty information\nestimated from our network may be leveraged to identify atypicality in data.\nOur method was compared with two state-of-the-art deep learning models and a\ntraditional multi-atlas approach, and exhibited excellent performance as\nmeasured both by Dice overlap as well as average symmetric surface distance. To\nthe best of our knowledge, this work is the first deep learning-based approach\nthat targets the subregions of the amygdala.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 21:16:55 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Liu", "Yilin", ""], ["Zhao", "Gengyan", ""], ["Nacewicz", "Brendon M.", ""], ["Adluru", "Nagesh", ""], ["Kirk", "Gregory R.", ""], ["Ferrazzano", "Peter A", ""], ["Styner", "Martin", ""], ["Alexander", "Andrew L.", ""]]}, {"id": "1902.07296", "submitter": "Zbigniew Wojna", "authors": "Mate Kisantal, Zbigniew Wojna, Jakub Murawski, Jacek Naruniec,\n  Kyunghyun Cho", "title": "Augmentation for small object detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, object detection has experienced impressive progress.\nDespite these improvements, there is still a significant gap in the performance\nbetween the detection of small and large objects. We analyze the current\nstate-of-the-art model, Mask-RCNN, on a challenging dataset, MS COCO. We show\nthat the overlap between small ground-truth objects and the predicted anchors\nis much lower than the expected IoU threshold. We conjecture this is due to two\nfactors; (1) only a few images are containing small objects, and (2) small\nobjects do not appear enough even within each image containing them. We thus\npropose to oversample those images with small objects and augment each of those\nimages by copy-pasting small objects many times. It allows us to trade off the\nquality of the detector on large objects with that on small objects. We\nevaluate different pasting augmentation strategies, and ultimately, we achieve\n9.7\\% relative improvement on the instance segmentation and 7.1\\% on the object\ndetection of small objects, compared to the current state of the art method on\nMS COCO.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 21:47:31 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Kisantal", "Mate", ""], ["Wojna", "Zbigniew", ""], ["Murawski", "Jakub", ""], ["Naruniec", "Jacek", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1902.07304", "submitter": "Jacek Komorowski", "authors": "Jacek Komorowski, Grzegorz Kurzejamski, Grzegorz Sarwas", "title": "DeepBall: Deep Neural-Network Ball Detector", "comments": "Conference: VISAPP 2019", "journal-ref": "VISIGRAPP 2019 - Proceedings of the 14th International Joint\n  Conference on Computer Vision, Imaging and Computer Graphics Theory and\n  Applications, vol. 5, 2019, SciTePress, ISBN 978-989-758-354-4, p. 297-304", "doi": "10.5220/0007348902970304", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes a deep network based object detector specialized for ball\ndetection in long shot videos. Due to its fully convolutional design, the\nmethod operates on images of any size and produces \\emph{ball confidence map}\nencoding the position of detected ball. The network uses hypercolumn concept,\nwhere feature maps from different hierarchy levels of the deep convolutional\nnetwork are combined and jointly fed to the convolutional classification layer.\nThis allows boosting the detection accuracy as larger visual context around the\nobject of interest is taken into account. The method achieves state-of-the-art\nresults when tested on publicly available ISSIA-CNR Soccer Dataset.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 21:57:48 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Komorowski", "Jacek", ""], ["Kurzejamski", "Grzegorz", ""], ["Sarwas", "Grzegorz", ""]]}, {"id": "1902.07323", "submitter": "Zbigniew Wojna", "authors": "Stephen Morrell, Zbigniew Wojna, Can Son Khoo, Sebastien Ourselin,\n  Juan Eugenio Iglesias", "title": "Large-scale mammography CAD with Deformable Conv-Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep learning methods for image processing are evolving into\nincreasingly complex meta-architectures with a growing number of modules. Among\nthem, region-based fully convolutional networks (R-FCN) and deformable\nconvolutional nets (DCN) can improve CAD for mammography: R-FCN optimizes for\nspeed and low consumption of memory, which is crucial for processing the high\nresolutions of to 50 micrometers used by radiologists. Deformable convolution\nand pooling can model a wide range of mammographic findings of different\nmorphology and scales, thanks to their versatility. In this study, we present a\nneural net architecture based on R-FCN / DCN, that we have adapted from the\nnatural image domain to suit mammograms -- particularly their larger image size\n-- without compromising resolution. We trained the network on a large, recently\nreleased dataset (Optimam) including 6,500 cancerous mammograms. By combining\nour modern architecture with such a rich dataset, we achieved an area under the\nROC curve of 0.879 for breast-wise detection in the DREAMS challenge (130,000\nwithheld images), which surpassed all other submissions in the competitive\nphase.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 22:18:22 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Morrell", "Stephen", ""], ["Wojna", "Zbigniew", ""], ["Khoo", "Can Son", ""], ["Ourselin", "Sebastien", ""], ["Iglesias", "Juan Eugenio", ""]]}, {"id": "1902.07327", "submitter": "Sixue Gong Miss", "authors": "Sixue Gong, Yichun Shi, and Anil K. Jain", "title": "Video Face Recognition: Component-wise Feature Aggregation Network\n  (C-FAN)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to video face recognition. Our component-wise\nfeature aggregation network (C-FAN) accepts a set of face images of a subject\nas an input, and outputs a single feature vector as the face representation of\nthe set for the recognition task. The whole network is trained in two steps:\n(i) train a base CNN for still image face recognition; (ii) add an aggregation\nmodule to the base network to learn the quality value for each feature\ncomponent, which adaptively aggregates deep feature vectors into a single\nvector to represent the face in a video. C-FAN automatically learns to retain\nsalient face features with high quality scores while suppressing features with\nlow quality scores. The experimental results on three benchmark datasets,\nYouTube Faces, IJB-A, and IJB-S show that the proposed C-FAN network is capable\nof generating a compact feature vector with 512 dimensions for a video sequence\nby efficiently aggregating feature vectors of all the video frames to achieve\nstate of the art performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 22:23:59 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 21:58:20 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2019 22:12:34 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Gong", "Sixue", ""], ["Shi", "Yichun", ""], ["Jain", "Anil K.", ""]]}, {"id": "1902.07367", "submitter": "Xiao Guo", "authors": "Xiao Guo, Jongmoo Choi", "title": "Human Motion Prediction via Learning Local Structure Representations and\n  Temporal Dependencies", "comments": "Accepted by AAAI19; Updated with the open source link", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion prediction from motion capture data is a classical problem in\nthe computer vision, and conventional methods take the holistic human body as\ninput. These methods ignore the fact that, in various human activities,\ndifferent body components (limbs and the torso) have distinctive\ncharacteristics in terms of the moving pattern. In this paper, we argue local\nrepresentations on different body components should be learned separately and,\nbased on such idea, propose a network, Skeleton Network (SkelNet), for\nlong-term human motion prediction. Specifically, at each time-step, local\nstructure representations of input (human body) are obtained via SkelNet's\nbranches of component-specific layers, then the shared layer uses local spatial\nrepresentations to predict the future human pose. Our SkelNet is the first to\nuse local structure representations for predicting the human motion. Then, for\nshort-term human motion prediction, we propose the second network, named as\nSkeleton Temporal Network (Skel-TNet). Skel-TNet consists of three components:\nSkelNet and a Recurrent Neural Network, they have advantages in learning\nspatial and temporal dependencies for predicting human motion, respectively; a\nfeed-forward network that outputs the final estimation. Our methods achieve\npromising results on the Human3.6M dataset and the CMU motion capture dataset.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 01:39:23 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 19:37:11 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Guo", "Xiao", ""], ["Choi", "Jongmoo", ""]]}, {"id": "1902.07370", "submitter": "Haichao Shi", "authors": "Xiao-Yu Zhang, Haichao Shi, Changsheng Li, Kai Zheng, Xiaobin Zhu,\n  Lixin Duan", "title": "Learning Transferable Self-attentive Representations for Action\n  Recognition in Untrimmed Videos with Weak Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition in videos has attracted a lot of attention in the past\ndecade. In order to learn robust models, previous methods usually assume videos\nare trimmed as short sequences and require ground-truth annotations of each\nvideo frame/sequence, which is quite costly and time-consuming. In this paper,\ngiven only video-level annotations, we propose a novel weakly supervised\nframework to simultaneously locate action frames as well as recognize actions\nin untrimmed videos. Our proposed framework consists of two major components.\nFirst, for action frame localization, we take advantage of the self-attention\nmechanism to weight each frame, such that the influence of background frames\ncan be effectively eliminated. Second, considering that there are trimmed\nvideos publicly available and also they contain useful information to leverage,\nwe present an additional module to transfer the knowledge from trimmed videos\nfor improving the classification performance in untrimmed ones. Extensive\nexperiments are conducted on two benchmark datasets (i.e., THUMOS14 and\nActivityNet1.3), and experimental results clearly corroborate the efficacy of\nour method.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 01:58:42 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Zhang", "Xiao-Yu", ""], ["Shi", "Haichao", ""], ["Li", "Changsheng", ""], ["Zheng", "Kai", ""], ["Zhu", "Xiaobin", ""], ["Duan", "Lixin", ""]]}, {"id": "1902.07385", "submitter": "David Alexandre", "authors": "David Alexandre, Chih-Peng Chang, Wen-Hsiao Peng, Hsueh-Ming Hang", "title": "An Autoencoder-based Learned Image Compressor: Description of Challenge\n  Proposal by NCTU", "comments": "Published in CVPR 2018: Workshop And Challenge On Learned Image\n  Compression", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a lossy image compression system using the deep-learning\nautoencoder structure to participate in the Challenge on Learned Image\nCompression (CLIC) 2018. Our autoencoder uses the residual blocks with skip\nconnections to reduce the correlation among image pixels and condense the input\nimage into a set of feature maps, a compact representation of the original\nimage. The bit allocation and bitrate control are implemented by using the\nimportance maps and quantizer. The importance maps are generated by a separate\nneural net in the encoder. The autoencoder and the importance net are trained\njointly based on minimizing a weighted sum of mean squared error, MS-SSIM, and\na rate estimate. Our aim is to produce reconstructed images with good\nsubjective quality subject to the 0.15 bits-per-pixel constraint.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 03:16:55 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Alexandre", "David", ""], ["Chang", "Chih-Peng", ""], ["Peng", "Wen-Hsiao", ""], ["Hang", "Hsueh-Ming", ""]]}, {"id": "1902.07401", "submitter": "Bill Yang Cai", "authors": "Bill Yang Cai, Ricardo Alvarez, Michelle Sit, F\\'abio Duarte, and\n  Carlo Ratti", "title": "Deep Learning Based Video System for Accurate and Real-Time Parking\n  Measurement", "comments": "Accepted for publication in IEEE Internet of Things Journal, Special\n  Issue on Enabling a Smart City: IoT Meets AI", "journal-ref": null, "doi": "10.1109/jiot.2019.2902887", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parking spaces are costly to build, parking payments are difficult to\nenforce, and drivers waste an excessive amount of time searching for empty\nlots. Accurate quantification would inform developers and municipalities in\nspace allocation and design, while real-time measurements would provide drivers\nand parking enforcement with information that saves time and resources. In this\npaper, we propose an accurate and real-time video system for future Internet of\nThings (IoT) and smart cities applications. Using recent developments in deep\nconvolutional neural networks (DCNNs) and a novel vehicle tracking filter, we\ncombine information across multiple image frames in a video sequence to remove\nnoise introduced by occlusions and detection failures. We demonstrate that our\nsystem achieves higher accuracy than pure image-based instance segmentation,\nand is comparable in performance to industry benchmark systems that utilize\nmore expensive sensors such as radar. Furthermore, our system shows significant\npotential in its scalability to a city-wide scale and also in the richness of\nits output that goes beyond traditional binary occupancy statistics.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 04:49:13 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Cai", "Bill Yang", ""], ["Alvarez", "Ricardo", ""], ["Sit", "Michelle", ""], ["Duarte", "F\u00e1bio", ""], ["Ratti", "Carlo", ""]]}, {"id": "1902.07402", "submitter": "Lu Tan", "authors": "Lu Tan, Ling Li, Wanquan Liu, Jie Sun, Min Zhang", "title": "A Novel Euler's Elastica based Segmentation Approach for Noisy Images\n  via using the Progressive Hedging Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Euler's Elastica based unsupervised segmentation models have strong\ncapability of completing the missing boundaries for existing objects in a clean\nimage, but they are not working well for noisy images. This paper aims to\nestablish a Euler's Elastica based approach that properly deals with random\nnoises to improve the segmentation performance for noisy images. We solve the\ncorresponding optimization problem via using the progressive hedging algorithm\n(PHA) with a step length suggested by the alternating direction method of\nmultipliers (ADMM). Technically, all the simplified convex versions of the\nsubproblems derived from the major framework of PHA can be obtained by using\nthe curvature weighted approach and the convex relaxation method. Then an\nalternating optimization strategy is applied with the merits of using some\npowerful accelerating techniques including the fast Fourier transform (FFT) and\ngeneralized soft threshold formulas. Extensive experiments have been conducted\non both synthetic and real images, which validated some significant gains of\nthe proposed segmentation models and demonstrated the advantages of the\ndeveloped algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 04:50:42 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Tan", "Lu", ""], ["Li", "Ling", ""], ["Liu", "Wanquan", ""], ["Sun", "Jie", ""], ["Zhang", "Min", ""]]}, {"id": "1902.07430", "submitter": "Muhammad Usman", "authors": "Muhammad Usman, Muhammad Umar Farooq, Siddique Latif, Muhammad Asim,\n  and Junaid Qadir", "title": "Motion Corrected Multishot MRI Reconstruction Using Generative Networks\n  with Sensitivity Encoding", "comments": "This paper has been published in Scientific Reports Journal", "journal-ref": null, "doi": "10.1038/s41598-020-61705-9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multishot Magnetic Resonance Imaging (MRI) is a promising imaging modality\nthat can produce a high-resolution image with relatively less data acquisition\ntime. The downside of multishot MRI is that it is very sensitive to subject\nmotion and even small amounts of motion during the scan can produce artifacts\nin the final MR image that may cause misdiagnosis. Numerous efforts have been\nmade to address this issue; however, all of these proposals are limited in\nterms of how much motion they can correct and the required computational time.\nIn this paper, we propose a novel generative networks based conjugate gradient\nSENSE (CG-SENSE) reconstruction framework for motion correction in multishot\nMRI. The proposed framework first employs CG-SENSE reconstruction to produce\nthe motion-corrupted image and then a generative adversarial network (GAN) is\nused to correct the motion artifacts. The proposed method has been rigorously\nevaluated on synthetically corrupted data on varying degrees of motion, numbers\nof shots, and encoding trajectories. Our analyses (both quantitative as well as\nqualitative/visual analysis) establishes that the proposed method significantly\nrobust and outperforms state-of-the-art motion correction techniques and also\nreduces severalfold of computational times.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 07:23:28 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 11:54:58 GMT"}, {"version": "v3", "created": "Sun, 24 Mar 2019 07:13:05 GMT"}, {"version": "v4", "created": "Thu, 31 Oct 2019 12:20:48 GMT"}, {"version": "v5", "created": "Mon, 2 Dec 2019 13:08:58 GMT"}, {"version": "v6", "created": "Thu, 12 Mar 2020 02:09:20 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Usman", "Muhammad", ""], ["Farooq", "Muhammad Umar", ""], ["Latif", "Siddique", ""], ["Asim", "Muhammad", ""], ["Qadir", "Junaid", ""]]}, {"id": "1902.07438", "submitter": "Abdul Basit", "authors": "Abdul Basit", "title": "Dynamic Matrix Decomposition for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a technique for the automatic analysis of different actions in\nvideos in order to detect the presence of interested activities is of high\nsignificance nowadays. In this paper, we explore a robust and dynamic\nappearance technique for the purpose of identifying different action\nactivities. We also exploit a low-rank and structured sparse matrix\ndecomposition (LSMD) method to better model these activities.. Our method is\neffective in encoding localized spatio-temporal features which enables the\nanalysis of local motion taking place in the video. Our proposed model use\nadjacent frame differences as the input to the method thereby forcing it to\ncapture the changes occurring in the video. The performance of our model is\ntested on a benchmark dataset in terms of detection accuracy. Results achieved\nwith our model showed the promising capability of our model in detecting action\nactivities.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 07:39:45 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Basit", "Abdul", ""]]}, {"id": "1902.07458", "submitter": "Alice Yang", "authors": "Alice Yi Yang and Ling Cheng", "title": "Long-Bone Fracture Detection using Artificial Neural Networks based on\n  Line Features of X-ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two line-based fracture detection scheme are developed and discussed, namely\nStandard line-based fracture detection and Adaptive Differential Parameter\nOptimized (ADPO) line-based fracture detection. The purpose for the two\nline-based fracture detection schemes is to detect fractured lines from X-ray\nimages using extracted features based on recognised patterns to differentiate\nfractured lines from non-fractured lines. The difference between the two\nschemes is the detection of detailed lines. The ADPO scheme optimizes the\nparameters of the Probabilistic Hough Transform, such that granule lines within\nthe fractured regions are detected, whereas the Standard scheme is unable to\ndetect them. The lines are detected using the Probabilistic Hough Function, in\nwhich the detected lines are a representation of the image edge objects. The\nlines are given in the form of points, (x,y), which includes the starting and\nending point. Based on the given line points, 13 features are extracted from\neach line, as a summary of line information. These features are used for\nfracture and non-fracture classification of the detected lines. The\nclassification is carried out by the Artificial Neural Network (ANN). There are\ntwo evaluations that are employed to evaluate both the entirety of the system\nand the ANN. The Standard Scheme is capable of achieving an average accuracy of\n74.25%, whilst the ADPO scheme achieved an average accuracy of 74.4%. The ADPO\nscheme is opted for over the Standard scheme, however it can be further\nimproved with detected contours and its extracted features.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 08:51:41 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Yang", "Alice Yi", ""], ["Cheng", "Ling", ""]]}, {"id": "1902.07463", "submitter": "Yu Xing", "authors": "Yu Xing, Shuang Liang, Lingzhi Sui, Xijie Jia, Jiantao Qiu, Xin Liu,\n  Yushun Wang, Yu Wang, and Yi Shan", "title": "DNNVM : End-to-End Compiler Leveraging Heterogeneous Optimizations on\n  FPGA-based CNN Accelerators", "comments": "18 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolutional neural network (CNN) has become a state-of-the-art method\nfor several artificial intelligence domains in recent years. The increasingly\ncomplex CNN models are both computation-bound and I/O-bound. FPGA-based\naccelerators driven by custom instruction set architecture (ISA) achieve a\nbalance between generality and efficiency, but there is much on them left to be\noptimized. We propose the full-stack compiler DNNVM, which is an integration of\noptimizers for graphs, loops and data layouts, and an assembler, a runtime\nsupporter and a validation environment. The DNNVM works in the context of deep\nlearning frameworks and transforms CNN models into the directed acyclic graph:\nXGraph. Based on XGraph, we transform the optimization challenges for both the\ndata layout and pipeline into graph-level problems. DNNVM enumerates all\npotentially profitable fusion opportunities by a heuristic subgraph isomorphism\nalgorithm to leverage pipeline and data layout optimizations, and searches for\nthe best choice of execution strategies of the whole computing graph. On the\nXilinx ZU2 @330 MHz and ZU9 @330 MHz, we achieve equivalently state-of-the-art\nperformance on our benchmarks by na\\\"ive implementations without optimizations,\nand the throughput is further improved up to 1.26x by leveraging heterogeneous\noptimizations in DNNVM. Finally, with ZU9 @330 MHz, we achieve state-of-the-art\nperformance for VGG and ResNet50. We achieve a throughput of 2.82 TOPs/s and an\nenergy efficiency of 123.7 GOPs/s/W for VGG. Additionally, we achieve 1.38\nTOPs/s for ResNet50 and 1.41 TOPs/s for GoogleNet.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 09:30:17 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 09:41:31 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Xing", "Yu", ""], ["Liang", "Shuang", ""], ["Sui", "Lingzhi", ""], ["Jia", "Xijie", ""], ["Qiu", "Jiantao", ""], ["Liu", "Xin", ""], ["Wang", "Yushun", ""], ["Wang", "Yu", ""], ["Shan", "Yi", ""]]}, {"id": "1902.07473", "submitter": "Yan-Bo Lin", "authors": "Yan-Bo Lin, Yu-Jhe Li, Yu-Chiang Frank Wang", "title": "Dual-modality seq2seq network for audio-visual event localization", "comments": "Accepted in ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio-visual event localization requires one to identify theevent which is\nboth visible and audible in a video (eitherat a frame or video level). To\naddress this task, we pro-pose a deep neural network named Audio-Visual\nsequence-to-sequence dual network (AVSDN). By jointly taking bothaudio and\nvisual features at each time segment as inputs, ourproposed model learns global\nand local event information ina sequence to sequence manner, which can be\nrealized in ei-ther fully supervised or weakly supervised settings.\nEmpiricalresults confirm that our proposed method performs favorablyagainst\nrecent deep learning approaches in both settings.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 09:48:30 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 07:08:41 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Lin", "Yan-Bo", ""], ["Li", "Yu-Jhe", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "1902.07474", "submitter": "Domen Tabernik", "authors": "Domen Tabernik, Matej Kristan and Ale\\v{s} Leonardis", "title": "Spatially-Adaptive Filter Units for Compact and Efficient Deep Neural\n  Networks", "comments": "Accepted for publication in International Journal of Computer Vision,\n  Jan 02 2020", "journal-ref": null, "doi": "10.1007/s11263-019-01282-1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks excel in a number of computer vision tasks. One\nof their most crucial architectural elements is the effective receptive field\nsize, that has to be manually set to accommodate a specific task. Standard\nsolutions involve large kernels, down/up-sampling and dilated convolutions.\nThese require testing a variety of dilation and down/up-sampling factors and\nresult in non-compact representations and excessive number of parameters. We\naddress this issue by proposing a new convolution filter composed of displaced\naggregation units (DAU). DAUs learn spatial displacements and adapt the\nreceptive field sizes of individual convolution filters to a given problem,\nthus eliminating the need for hand-crafted modifications. DAUs provide a\nseamless substitution of convolutional filters in existing state-of-the-art\narchitectures, which we demonstrate on AlexNet, ResNet50, ResNet101, DeepLab\nand SRN-DeblurNet. The benefits of this design are demonstrated on a variety of\ncomputer vision tasks and datasets, such as image classification (ILSVRC 2012),\nsemantic segmentation (PASCAL VOC 2011, Cityscape) and blind image de-blurring\n(GOPRO). Results show that DAUs efficiently allocate parameters resulting in up\nto four times more compact networks at similar or better performance.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 09:49:55 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 18:11:44 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Tabernik", "Domen", ""], ["Kristan", "Matej", ""], ["Leonardis", "Ale\u0161", ""]]}, {"id": "1902.07476", "submitter": "Sercan T\\\"urkmen", "authors": "Sercan T\\\"urkmen, Janne Heikkil\\\"a", "title": "An efficient solution for semantic segmentation: ShuffleNet V2 with\n  atrous separable convolutions", "comments": "12 pages, 6 figures, 5 tables", "journal-ref": null, "doi": "10.1007/978-3-030-20205-7_4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assigning a label to each pixel in an image, namely semantic segmentation,\nhas been an important task in computer vision, and has applications in\nautonomous driving, robotic navigation, localization, and scene understanding.\nFully convolutional neural networks have proved to be a successful solution for\nthe task over the years but most of the work being done focuses primarily on\naccuracy. In this paper, we present a computationally efficient approach to\nsemantic segmentation, while achieving a high mean intersection over union\n(mIOU), 70.33% on Cityscapes challenge. The network proposed is capable of\nrunning real-time on mobile devices. In addition, we make our code and model\nweights publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 09:50:47 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 17:18:58 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["T\u00fcrkmen", "Sercan", ""], ["Heikkil\u00e4", "Janne", ""]]}, {"id": "1902.07511", "submitter": "Luca Morreale", "authors": "Luca Morreale and Andrea Romanoni and Matteo Matteucci", "title": "Dense 3D Visual Mapping via Semantic Simplification", "comments": "7 pages; accepted at International Conference on Robotics and\n  Automation (IEEE) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense 3D visual mapping estimates as many as possible pixel depths, for each\nimage. This results in very dense point clouds that often contain redundant and\nnoisy information, especially for surfaces that are roughly planar, for\ninstance, the ground or the walls in the scene. In this paper we leverage on\nsemantic image segmentation to discriminate which regions of the scene require\nsimplification and which should be kept at high level of details. We propose\nfour different point cloud simplification methods which decimate the perceived\npoint cloud by relying on class-specific local and global statistics still\nmaintaining more points in the proximity of class boundaries to preserve the\ninfra-class edges and discontinuities. 3D dense model is obtained by fusing the\npoint clouds in a 3D Delaunay Triangulation to deal with variable point cloud\ndensity. In the experimental evaluation we have shown that, by leveraging on\nsemantics, it is possible to simplify the model and diminish the noise\naffecting the point clouds.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 11:23:44 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Morreale", "Luca", ""], ["Romanoni", "Andrea", ""], ["Matteucci", "Matteo", ""]]}, {"id": "1902.07519", "submitter": "Shujun Wang", "authors": "Shujun Wang, Lequan Yu, Xin Yang, Chi-Wing Fu and Pheng-Ann Heng", "title": "Patch-based Output Space Adversarial Learning for Joint Optic Disc and\n  Cup Segmentation", "comments": "IEEE Transactions on Medical Imaging (In press)", "journal-ref": null, "doi": "10.1109/TMI.2019.2899910", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glaucoma is a leading cause of irreversible blindness. Accurate segmentation\nof the optic disc (OD) and cup (OC) from fundus images is beneficial to\nglaucoma screening and diagnosis. Recently, convolutional neural networks\ndemonstrate promising progress in joint OD and OC segmentation. However,\naffected by the domain shift among different datasets, deep networks are\nseverely hindered in generalizing across different scanners and institutions.\nIn this paper, we present a novel patchbased Output Space Adversarial Learning\nframework (pOSAL) to jointly and robustly segment the OD and OC from different\nfundus image datasets. We first devise a lightweight and efficient segmentation\nnetwork as a backbone. Considering the specific morphology of OD and OC, a\nnovel morphology-aware segmentation loss is proposed to guide the network to\ngenerate accurate and smooth segmentation. Our pOSAL framework then exploits\nunsupervised domain adaptation to address the domain shift challenge by\nencouraging the segmentation in the target domain to be similar to the source\nones. Since the whole-segmentationbased adversarial loss is insufficient to\ndrive the network to capture segmentation details, we further design the pOSAL\nin a patch-based fashion to enable fine-grained discrimination on local\nsegmentation details. We extensively evaluate our pOSAL framework and\ndemonstrate its effectiveness in improving the segmentation performance on\nthree public retinal fundus image datasets, i.e., Drishti-GS, RIM-ONE-r3, and\nREFUGE. Furthermore, our pOSAL framework achieved the first place in the OD and\nOC segmentation tasks in MICCAI 2018 Retinal Fundus Glaucoma Challenge.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 11:37:59 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Wang", "Shujun", ""], ["Yu", "Lequan", ""], ["Yang", "Xin", ""], ["Fu", "Chi-Wing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1902.07521", "submitter": "Bernhard Schmitzer", "authors": "Bernhard Schmitzer, Klaus P. Sch\\\"afers, Benedikt Wirth", "title": "Dynamic Cell Imaging in PET with Optimal Transport Regularization", "comments": "Revised version, to appear in IEEE Trans Med Imaging. Supplementary\n  material attached as last page", "journal-ref": null, "doi": "10.1109/TMI.2019.2953773", "report-no": null, "categories": "cs.CV eess.IV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel dynamic image reconstruction method from PET listmode data\nthat could be particularly suited to tracking single or small numbers of cells.\nIn contrast to conventional PET reconstruction our method combines the\ninformation from all detected events not only to reconstruct the dynamic\nevolution of the radionuclide distribution, but also to improve the\nreconstruction at each single time point by enforcing temporal consistency.\nThis is achieved via optimal transport regularization where in principle, among\nall possible temporally evolving radionuclide distributions consistent with the\nPET measurement, the one is chosen with least kinetic motion energy. The\nreconstruction is found by convex optimization so that there is no dependence\non the initialization of the method. We study its behaviour on simulated data\nof a human PET system and demonstrate its robustness even in settings with very\nlow radioactivity. In contrast to previously reported cell tracking algorithms,\nour technique is oblivious to the number of tracked cells. Without any\nadditional complexity one or multiple cells can be reconstructed, and the model\nautomatically determines the number of particles. For instance, four\nradiolabelled cells moving at a velocity of 3.1 mm/s and a PET recorded count\nrate of 1.1 cps (for each cell) could be simultaneously tracked with a tracking\naccuracy of 5.3 mm inside a simulated human body.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 11:45:14 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 09:28:19 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Schmitzer", "Bernhard", ""], ["Sch\u00e4fers", "Klaus P.", ""], ["Wirth", "Benedikt", ""]]}, {"id": "1902.07582", "submitter": "Zhengchun Liu", "authors": "Zhengchun Liu, Tekin Bicer, Rajkumar Kettimuthu, Doga Gursoy,\n  Francesco De Carlo, Ian Foster", "title": "TomoGAN: Low-Dose Synchrotron X-Ray Tomography with Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": "10.1364/JOSAA.375595", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchrotron-based x-ray tomography is a noninvasive imaging technique that\nallows for reconstructing the internal structure of materials at high spatial\nresolutions from tens of micrometers to a few nanometers. In order to resolve\nsample features at smaller length scales, however, a higher radiation dose is\nrequired. Therefore, the limitation on the achievable resolution is set\nprimarily by noise at these length scales. We present \\TOMOGAN{}, a denoising\ntechnique based on generative adversarial networks, for improving the quality\nof reconstructed images for low-dose imaging conditions. We evaluate our\napproach in two photon-budget-limited experimental conditions: (1) sufficient\nnumber of low-dose projections (based on Nyquist sampling), and (2)\ninsufficient or limited number of high-dose projections. In both cases the\nangular sampling is assumed to be isotropic, and the photon budget throughout\nthe experiment is fixed based on the maximum allowable radiation dose on the\nsample. Evaluation with both simulated and experimental datasets shows that our\napproach can significantly reduce noise in reconstructed images, improving the\nstructural similarity score of simulation and experimental data from 0.18 to\n0.9 and from 0.18 to 0.41, respectively. Furthermore, the quality of the\nreconstructed images with filtered back projection followed by our denoising\napproach exceeds that of reconstructions with the simultaneous iterative\nreconstruction technique, showing the computational superiority of our\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 14:55:37 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 16:35:07 GMT"}, {"version": "v3", "created": "Sat, 9 Mar 2019 00:15:58 GMT"}, {"version": "v4", "created": "Fri, 20 Dec 2019 18:23:51 GMT"}, {"version": "v5", "created": "Mon, 30 Dec 2019 17:41:08 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Liu", "Zhengchun", ""], ["Bicer", "Tekin", ""], ["Kettimuthu", "Rajkumar", ""], ["Gursoy", "Doga", ""], ["De Carlo", "Francesco", ""], ["Foster", "Ian", ""]]}, {"id": "1902.07593", "submitter": "Roozbeh Rajabi", "authors": "Sara Khoshsokhan, Roozbeh Rajabi, Hadi Zayyani", "title": "Sparsity Constrained Distributed Unmixing of Hyperspectral Data", "comments": "one column, 17 pages, 10 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral unmixing (SU) is a technique to characterize mixed pixels in\nhyperspectral images measured by remote sensors. Most of the spectral unmixing\nalgorithms are developed using the linear mixing models. To estimate endmembers\nand fractional abundance matrices in a blind problem, nonnegative matrix\nfactorization (NMF) and its developments are widely used in the SU problem. One\nof the constraints which was added to NMF is sparsity, that was regularized by\nLq norm. In this paper, a new algorithm based on distributed optimization is\nsuggested for spectral unmixing. In the proposed algorithm, a network including\nsingle-node clusters is employed. Each pixel in the hyperspectral images is\nconsidered as a node in this network. The sparsity constrained distributed\nunmixing is optimized with diffusion least mean p-power (LMP) strategy, and\nthen the update equations for fractional abundance and signature matrices are\nobtained. Afterwards the proposed algorithm is analyzed for different values of\nLMP power and Lq norms. Simulation results based on defined performance metrics\nillustrate the advantage of the proposed algorithm in spectral unmixing of\nhyperspectral data compared with other methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 15:16:06 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Khoshsokhan", "Sara", ""], ["Rajabi", "Roozbeh", ""], ["Zayyani", "Hadi", ""]]}, {"id": "1902.07602", "submitter": "Jianze Li", "authors": "Jianze Li, Xiao-Ping Zhang, Tuan Tran", "title": "Point cloud denoising based on tensor Tucker decomposition", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new algorithm for point cloud denoising based on\nthe tensor Tucker decomposition. We first represent the local surface patches\nof a noisy point cloud to be matrices by their distances to a reference point,\nand stack the similar patch matrices to be a 3rd order tensor. Then we use the\nTucker decomposition to compress this patch tensor to be a core tensor of\nsmaller size. We consider this core tensor as the frequency domain and remove\nthe noise by manipulating the hard thresholding. Finally, all the fibers of the\ndenoised patch tensor are placed back, and the average is taken if there are\nmore than one estimators overlapped. The experimental evaluation shows that the\nproposed algorithm outperforms the state-of-the-art graph Laplacian regularized\n(GLR) algorithm when the Gaussian noise is high ($\\sigma=0.1$), and the GLR\nalgorithm is better in lower noise cases ($\\sigma=0.04, 0.05, 0.08$).\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 15:41:25 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 07:12:42 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Li", "Jianze", ""], ["Zhang", "Xiao-Ping", ""], ["Tran", "Tuan", ""]]}, {"id": "1902.07623", "submitter": "Gavin Weiguang Ding", "authors": "Gavin Weiguang Ding and Luyu Wang and Xiaomeng Jin", "title": "advertorch v0.1: An Adversarial Robustness Toolbox based on PyTorch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  advertorch is a toolbox for adversarial robustness research. It contains\nvarious implementations for attacks, defenses and robust training methods.\nadvertorch is built on PyTorch (Paszke et al., 2017), and leverages the\nadvantages of the dynamic computational graph to provide concise and efficient\nreference implementations. The code is licensed under the LGPL license and is\nopen sourced at https://github.com/BorealisAI/advertorch .\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 16:18:37 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Ding", "Gavin Weiguang", ""], ["Wang", "Luyu", ""], ["Jin", "Xiaomeng", ""]]}, {"id": "1902.07630", "submitter": "Mehryar Emambakhsh", "authors": "Mehryar Emambakhsh, Alessandro Bay and Eduard Vazquez", "title": "Filtering Point Targets via Online Learning of Motion Models", "comments": "arXiv admin note: text overlap with arXiv:1806.06594", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filtering point targets in highly cluttered and noisy data frames can be very\nchallenging, especially for complex target motions. Fixed motion models can\nfail to provide accurate predictions, while learning based algorithm can be\ndifficult to design (due to the variable number of targets), slow to train and\ndependent on separate train/test steps. To address these issues, this paper\nproposes a multi-target filtering algorithm which learns the motion models, on\nthe fly, using a recurrent neural network with a long short-term memory\narchitecture, as a regression block. The target state predictions are then\ncorrected using a novel data association algorithm, with a low computational\ncomplexity. The proposed algorithm is evaluated over synthetic and real point\ntarget filtering scenarios, demonstrating a remarkable performance over highly\ncluttered data sequences.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 16:36:52 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Emambakhsh", "Mehryar", ""], ["Bay", "Alessandro", ""], ["Vazquez", "Eduard", ""]]}, {"id": "1902.07651", "submitter": "Victor Boutin", "authors": "Victor Boutin, Angelo Franciosini, Frederic Chavane, Franck Ruffier,\n  Laurent Perrinet", "title": "Sparse Deep Predictive Coding captures contour integration capabilities\n  of the early visual system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both neurophysiological and psychophysical experiments have pointed out the\ncrucial role of recurrent and feedback connections to process context-dependent\ninformation in the early visual cortex. While numerous models have accounted\nfor feedback effects at either neural or representational level, none of them\nwere able to bind those two levels of analysis. Is it possible to describe\nfeedback effects at both levels using the same model? We answer this question\nby combining Predictive Coding (PC) and Sparse Coding (SC) into a hierarchical\nand convolutional framework. In this Sparse Deep Predictive Coding (SDPC)\nmodel, the SC component models the internal recurrent processing within each\nlayer, and the PC component describes the interactions between layers using\nfeedforward and feedback connections. Here, we train a 2-layered SDPC on two\ndifferent databases of images, and we interpret it as a model of the early\nvisual system (V1 & V2). We first demonstrate that once the training has\nconverged, SDPC exhibits oriented and localized receptive fields in V1 and more\ncomplex features in V2. Second, we analyze the effects of feedback on the\nneural organization beyond the classical receptive field of V1 neurons using\ninteraction maps. These maps are similar to association fields and reflect the\nGestalt principle of good continuation. We demonstrate that feedback signals\nreorganize interaction maps and modulate neural activity to promote contour\nintegration. Third, we demonstrate at the representational level that the SDPC\nfeedback connections are able to overcome noise in input images. Therefore, the\nSDPC captures the association field principle at the neural level which results\nin better disambiguation of blurred images at the representational level.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 17:06:00 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 07:43:28 GMT"}, {"version": "v3", "created": "Thu, 17 Oct 2019 15:46:21 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Boutin", "Victor", ""], ["Franciosini", "Angelo", ""], ["Chavane", "Frederic", ""], ["Ruffier", "Franck", ""], ["Perrinet", "Laurent", ""]]}, {"id": "1902.07653", "submitter": "Julio Cezar Silveira Jacques Junior", "authors": "Julio C. S. Jacques Junior, Cagri Ozcinar, Marina Marjanovic, Xavier\n  Bar\\'o, Gholamreza Anbarjafari, and Sergio Escalera", "title": "On the effect of age perception biases for real age regression", "comments": "Accepted in the 14th IEEE International Conference on Automatic Face\n  and Gesture Recognition (FG 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic age estimation from facial images represents an important task in\ncomputer vision. This paper analyses the effect of gender, age, ethnic, makeup\nand expression attributes of faces as sources of bias to improve deep apparent\nage prediction. Following recent works where it is shown that apparent age\nlabels benefit real age estimation, rather than direct real to real age\nregression, our main contribution is the integration, in an end-to-end\narchitecture, of face attributes for apparent age prediction with an additional\nloss for real age regression. Experimental results on the APPA-REAL dataset\nindicate the proposed network successfully take advantage of the adopted\nattributes to improve both apparent and real age estimation. Our model\noutperformed a state-of-the-art architecture proposed to separately address\napparent and real age regression. Finally, we present preliminary results and\ndiscussion of a proof of concept application using the proposed model to\nregress the apparent age of an individual based on the gender of an external\nobserver.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 17:07:44 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Junior", "Julio C. S. Jacques", ""], ["Ozcinar", "Cagri", ""], ["Marjanovic", "Marina", ""], ["Bar\u00f3", "Xavier", ""], ["Anbarjafari", "Gholamreza", ""], ["Escalera", "Sergio", ""]]}, {"id": "1902.07668", "submitter": "Mohammadreza Javanmardi", "authors": "Mohammadreza Javanmardi, Amir Hossein Farzaneh, Xiaojun Qi", "title": "Robust Structured Group Local Sparse Tracker Using Deep Features", "comments": "This submission is similar version of Structured Group Local Sparse\n  Tracker arXiv:1902.06182", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation has recently been successfully applied in visual\ntracking. It utilizes a set of templates to represent target candidates and\nfind the best one with the minimum reconstruction error as the tracking result.\nIn this paper, we propose a robust deep features-based structured group local\nsparse tracker (DF-SGLST), which exploits the deep features of local patches\ninside target candidates and represents them by a set of templates in the\nparticle filter framework. Unlike the conventional local sparse trackers, the\nproposed optimization model in DF-SGLST employs a group-sparsity regularization\nterm to seamlessly adopt local and spatial information of the target candidates\nand attain the spatial layout structure among them. To solve the optimization\nmodel, we propose an efficient and fast numerical algorithm that consists of\ntwo subproblems with the closed-form solutions. Different evaluations in terms\nof success and precision on the benchmarks of challenging image sequences\n(e.g., OTB50 and OTB100) demonstrate the superior performance of the proposed\ntracker against several state-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 08:43:51 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 06:06:59 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Javanmardi", "Mohammadreza", ""], ["Farzaneh", "Amir Hossein", ""], ["Qi", "Xiaojun", ""]]}, {"id": "1902.07687", "submitter": "Hengtao Guo", "authors": "Hengtao Guo, Uwe Kruger, Ge Wang, Mannudeep K. Kalra, Pingkun Yan", "title": "Knowledge-based Analysis for Mortality Prediction from CT Images", "comments": "Accepted for publication in IEEE Journal of Biomedical and Health\n  Informatics (JBHI)", "journal-ref": "IEEE Journal of Biomedical and Health Informatics, 2019", "doi": "10.1109/JBHI.2019.2946066", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have highlighted the high correlation between cardiovascular\ndiseases (CVD) and lung cancer, and both are associated with significant\nmorbidity and mortality. Low-Dose CT (LCDT) scans have led to significant\nimprovements in the accuracy of lung cancer diagnosis and thus the reduction of\ncancer deaths. However, the high correlation between lung cancer and CVD has\nnot been well explored for mortality prediction. This paper introduces a\nknowledge-based analytical method using deep convolutional neural network (CNN)\nfor all-cause mortality prediction. The underlying approach combines structural\nimage features extracted from CNNs, based on LDCT volume in different scale,\nand clinical knowledge obtained from quantitative measurements, to\ncomprehensively predict the mortality risk of lung cancer screening subjects.\nThe introduced method is referred to here as the Knowledge-based Analysis of\nMortality Prediction Network, or KAMP-Net. It constitutes a collaborative\nframework that utilizes both imaging features and anatomical information,\ninstead of completely relying on automatic feature extraction. Our work\ndemonstrates the feasibility of incorporating quantitative clinical\nmeasurements to assist CNNs in all-cause mortality prediction from chest LDCT\nimages. The results of this study confirm that radiologist defined features are\nan important complement to CNNs to achieve a more comprehensive feature\nextraction. Thus, the proposed KAMP-Net has shown to achieve a superior\nperformance when compared to other methods. Our code is available at\nhttps://github.com/DIAL-RPI/KAMP-Net.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 18:15:57 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 20:34:12 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Guo", "Hengtao", ""], ["Kruger", "Uwe", ""], ["Wang", "Ge", ""], ["Kalra", "Mannudeep K.", ""], ["Yan", "Pingkun", ""]]}, {"id": "1902.07762", "submitter": "Ondrej Skopek", "authors": "Lukas Jendele, Ondrej Skopek, Anton S. Becker, Ender Konukoglu", "title": "Adversarial Augmentation for Enhancing Classification of Mammography\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Supervised deep learning relies on the assumption that enough training data\nis available, which presents a problem for its application to several fields,\nlike medical imaging. On the example of a binary image classification task\n(breast cancer recognition), we show that pretraining a generative model for\nmeaningful image augmentation helps enhance the performance of the resulting\nclassifier. By augmenting the data, performance on downstream classification\ntasks could be improved even with a relatively small training set. We show that\nthis \"adversarial augmentation\" yields promising results compared to classical\nimage augmentation on the example of breast cancer classification.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 20:13:24 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Jendele", "Lukas", ""], ["Skopek", "Ondrej", ""], ["Becker", "Anton S.", ""], ["Konukoglu", "Ender", ""]]}, {"id": "1902.07766", "submitter": "Xingtong Liu", "authors": "Xingtong Liu, Ayushi Sinha, Masaru Ishii, Gregory D. Hager, Austin\n  Reiter, Russell H. Taylor, Mathias Unberath", "title": "Dense Depth Estimation in Monocular Endoscopy with Self-supervised\n  Learning Methods", "comments": "Accepted to IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a self-supervised approach to training convolutional neural\nnetworks for dense depth estimation from monocular endoscopy data without a\npriori modeling of anatomy or shading. Our method only requires monocular\nendoscopic videos and a multi-view stereo method, e.g., structure from motion,\nto supervise learning in a sparse manner. Consequently, our method requires\nneither manual labeling nor patient computed tomography (CT) scan in the\ntraining and application phases. In a cross-patient experiment using CT scans\nas groundtruth, the proposed method achieved submillimeter mean residual error.\nIn a comparison study to recent self-supervised depth estimation methods\ndesigned for natural video on in vivo sinus endoscopy data, we demonstrate that\nthe proposed approach outperforms the previous methods by a large margin. The\nsource code for this work is publicly available online at\nhttps://github.com/lppllppl920/EndoscopyDepthEstimation-Pytorch.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 20:25:22 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 02:28:19 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Liu", "Xingtong", ""], ["Sinha", "Ayushi", ""], ["Ishii", "Masaru", ""], ["Hager", "Gregory D.", ""], ["Reiter", "Austin", ""], ["Taylor", "Russell H.", ""], ["Unberath", "Mathias", ""]]}, {"id": "1902.07776", "submitter": "Diego Gragnaniello", "authors": "Diego Gragnaniello, Francesco Marra, Giovanni Poggi and Luisa\n  Verdoliva", "title": "Perceptual Quality-preserving Black-Box Attack against Deep Learning\n  Image Classifiers", "comments": "8 pages, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks provide unprecedented performance in all image\nclassification problems, taking advantage of huge amounts of data available for\ntraining. Recent studies, however, have shown their vulnerability to\nadversarial attacks, spawning an intense research effort in this field. With\nthe aim of building better systems, new countermeasures and stronger attacks\nare proposed by the day. On the attacker's side, there is growing interest for\nthe realistic black-box scenario, in which the user has no access to the neural\nnetwork parameters. The problem is to design efficient attacks which mislead\nthe neural network without compromising image quality. In this work, we propose\nto perform the black-box attack along a low-distortion path, so as to improve\nboth the attack efficiency and the perceptual quality of the adversarial image.\nNumerical experiments on real-world systems prove the effectiveness of the\nproposed approach, both in benchmark classification tasks and in key\napplications in biometrics and forensics.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 20:57:40 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 12:22:58 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2020 07:37:46 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Gragnaniello", "Diego", ""], ["Marra", "Francesco", ""], ["Poggi", "Giovanni", ""], ["Verdoliva", "Luisa", ""]]}, {"id": "1902.07810", "submitter": "Sagi Eppel", "authors": "Sagi Eppel", "title": "Class-independent sequential full image segmentation, using a\n  convolutional net that finds a segment within an attention region, given a\n  pointer pixel within this segment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This work examines the use of a fully convolutional net (FCN) to find an\nimage segment, given a pixel within this segment region. The net receives an\nimage, a point in the image and a region of interest (RoI ) mask. The net\noutput is a binary mask of the segment in which the point is located. The\nregion where the segment can be found is contained within the input RoI mask.\nFull image segmentation can be achieved by running this net sequentially,\nregion-by-region on the image, and stitching the output segments into a single\nsegmentation map. This simple method addresses two major challenges of image\nsegmentation: 1) Segmentation of unknown categories that were not included in\nthe training set. 2) Segmentation of both individual object instances (things)\nand non-objects (stuff), such as sky and vegetation. Hence, if the pointer\npixel is located within a person in a group, the net will output a mask that\ncovers that individual person; if the pointer point is located within the sky\nregion, the net returns the region of the sky in the image. This is true even\nif no example for sky or person appeared in the training set. The net was\ntested and trained on the COCO panoptic dataset and achieved 67% IOU for\nsegmentation of familiar classes (that were part of the net training set) and\n53% IOU for segmentation of unfamiliar classes (that were not included in the\ntraining).\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 23:35:49 GMT"}, {"version": "v2", "created": "Sat, 23 Feb 2019 12:02:27 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Eppel", "Sagi", ""]]}, {"id": "1902.07837", "submitter": "Jianda Sheng", "authors": "Zhihui Su, Ming Ye, Guohui Zhang, Lei Dai, Jianda Sheng", "title": "Cascade Feature Aggregation for Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation plays an important role in many computer vision tasks\nand has been studied for many decades. However, due to complex appearance\nvariations from poses, illuminations, occlusions and low resolutions, it still\nremains a challenging problem. Taking the advantage of high-level semantic\ninformation from deep convolutional neural networks is an effective way to\nimprove the accuracy of human pose estimation. In this paper, we propose a\nnovel Cascade Feature Aggregation (CFA) method, which cascades several\nhourglass networks for robust human pose estimation. Features from different\nstages are aggregated to obtain abundant contextual information, leading to\nrobustness to poses, partial occlusions and low resolution. Moreover, results\nfrom different stages are fused to further improve the localization accuracy.\nThe extensive experiments on MPII datasets and LIP datasets demonstrate that\nour proposed CFA outperforms the state-of-the-art and achieves the best\nperformance on the state-of-the-art benchmark MPII.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 01:34:27 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 09:19:52 GMT"}, {"version": "v3", "created": "Mon, 20 May 2019 14:38:37 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Su", "Zhihui", ""], ["Ye", "Ming", ""], ["Zhang", "Guohui", ""], ["Dai", "Lei", ""], ["Sheng", "Jianda", ""]]}, {"id": "1902.07864", "submitter": "Ramakrishna Vedantam", "authors": "Ramakrishna Vedantam, Karan Desai, Stefan Lee, Marcus Rohrbach, Dhruv\n  Batra, Devi Parikh", "title": "Probabilistic Neural-symbolic Models for Interpretable Visual Question\n  Answering", "comments": "ICML 2019 Camera Ready + Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of probabilistic neural-symbolic models, that have\nsymbolic functional programs as a latent, stochastic variable. Instantiated in\nthe context of visual question answering, our probabilistic formulation offers\ntwo key conceptual advantages over prior neural-symbolic models for VQA.\nFirstly, the programs generated by our model are more understandable while\nrequiring lesser number of teaching examples. Secondly, we show that one can\npose counterfactual scenarios to the model, to probe its beliefs on the\nprograms that could lead to a specified answer given an image. Our results on\nthe CLEVR and SHAPES datasets verify our hypotheses, showing that the model\ngets better program (and answer) prediction accuracy even in the low data\nregime, and allows one to probe the coherence and consistency of reasoning\nperformed.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 04:55:56 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 22:12:00 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Vedantam", "Ramakrishna", ""], ["Desai", "Karan", ""], ["Lee", "Stefan", ""], ["Rohrbach", "Marcus", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1902.07877", "submitter": "Lei Li", "authors": "Lei Li, Fuping Wu, Guang Yang, Lingchao Xu, Tom Wong, Raad Mohiaddin,\n  David Firmin, Jennifer Keegan, Xiahai Zhuang", "title": "Atrial Scar Quantification via Multi-scale CNN in the Graph-cuts\n  Framework", "comments": "13 pages, 10 figures, submitted to Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Late gadolinium enhancement magnetic resonance imaging (LGE MRI) appears to\nbe a promising alternative for scar assessment in patients with atrial\nfibrillation (AF). Automating the quantification and analysis of atrial scars\ncan be challenging due to the low image quality. In this work, we propose a\nfully automated method based on the graph-cuts framework, where the potentials\nof the graph are learned on a surface mesh of the left atrium (LA) using a\nmulti-scale convolutional neural network (MS-CNN). For validation, we have\nemployed fifty-eight images with manual delineations. MS-CNN, which can\nefficiently incorporate both the local and global texture information of the\nimages, has been shown to evidently improve the segmentation accuracy of the\nproposed graph-cuts based method. The segmentation could be further improved\nwhen the contribution between the t-link and n-link weights of the graph is\nbalanced. The proposed method achieves a mean accuracy of 0.856 +- 0.033 and\nmean Dice score of 0.702 +- 0.071 for LA scar quantification. Compared with the\nconventional methods, which are based on the manual delineation of LA for\ninitialization, our method is fully automatic and has demonstrated\nsignificantly better Dice score and accuracy (p < 0.01). The method is\npromising and can be useful in diagnosis and prognosis of AF.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 06:13:35 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Li", "Lei", ""], ["Wu", "Fuping", ""], ["Yang", "Guang", ""], ["Xu", "Lingchao", ""], ["Wong", "Tom", ""], ["Mohiaddin", "Raad", ""], ["Firmin", "David", ""], ["Keegan", "Jennifer", ""], ["Zhuang", "Xiahai", ""]]}, {"id": "1902.07880", "submitter": "Lei Li", "authors": "Xiahai Zhuang, Lei Li, Christian Payer, Darko Stern, Martin Urschler,\n  Mattias P. Heinrich, Julien Oster, Chunliang Wang, Orjan Smedby, Cheng Bian,\n  Xin Yang, Pheng-Ann Heng, Aliasghar Mortazi, Ulas Bagci, Guanyu Yang,\n  Chenchen Sun, Gaetan Galisot, Jean-Yves Ramel, Thierry Brouard, Qianqian\n  Tong, Weixin Si, Xiangyun Liao, Guodong Zeng, Zenglin Shi, Guoyan Zheng,\n  Chengjia Wang, Tom MacGillivray, David Newby, Kawal Rhode, Sebastien\n  Ourselin, Raad Mohiaddin, Jennifer Keegan, David Firmin, Guang Yang", "title": "Evaluation of Algorithms for Multi-Modality Whole Heart Segmentation: An\n  Open-Access Grand Challenge", "comments": "14 pages, 7 figures, sumitted to Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge of whole heart anatomy is a prerequisite for many clinical\napplications. Whole heart segmentation (WHS), which delineates substructures of\nthe heart, can be very valuable for modeling and analysis of the anatomy and\nfunctions of the heart. However, automating this segmentation can be arduous\ndue to the large variation of the heart shape, and different image qualities of\nthe clinical data. To achieve this goal, a set of training data is generally\nneeded for constructing priors or for training. In addition, it is difficult to\nperform comparisons between different methods, largely due to differences in\nthe datasets and evaluation metrics used. This manuscript presents the\nmethodologies and evaluation results for the WHS algorithms selected from the\nsubmissions to the Multi-Modality Whole Heart Segmentation (MM-WHS) challenge,\nin conjunction with MICCAI 2017. The challenge provides 120 three-dimensional\ncardiac images covering the whole heart, including 60 CT and 60 MRI volumes,\nall acquired in clinical environments with manual delineation. Ten algorithms\nfor CT data and eleven algorithms for MRI data, submitted from twelve groups,\nhave been evaluated. The results show that many of the deep learning (DL) based\nmethods achieved high accuracy, even though the number of training datasets was\nlimited. A number of them also reported poor results in the blinded evaluation,\nprobably due to overfitting in their training. The conventional algorithms,\nmainly based on multi-atlas segmentation, demonstrated robust and stable\nperformance, even though the accuracy is not as good as the best DL method in\nCT segmentation. The challenge, including the provision of the annotated\ntraining data and the blinded evaluation for submitted algorithms on the test\ndata, continues as an ongoing benchmarking resource via its homepage\n(\\url{www.sdspeople.fudan.edu.cn/zhuangxiahai/0/mmwhs/}).\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 06:18:30 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Zhuang", "Xiahai", ""], ["Li", "Lei", ""], ["Payer", "Christian", ""], ["Stern", "Darko", ""], ["Urschler", "Martin", ""], ["Heinrich", "Mattias P.", ""], ["Oster", "Julien", ""], ["Wang", "Chunliang", ""], ["Smedby", "Orjan", ""], ["Bian", "Cheng", ""], ["Yang", "Xin", ""], ["Heng", "Pheng-Ann", ""], ["Mortazi", "Aliasghar", ""], ["Bagci", "Ulas", ""], ["Yang", "Guanyu", ""], ["Sun", "Chenchen", ""], ["Galisot", "Gaetan", ""], ["Ramel", "Jean-Yves", ""], ["Brouard", "Thierry", ""], ["Tong", "Qianqian", ""], ["Si", "Weixin", ""], ["Liao", "Xiangyun", ""], ["Zeng", "Guodong", ""], ["Shi", "Zenglin", ""], ["Zheng", "Guoyan", ""], ["Wang", "Chengjia", ""], ["MacGillivray", "Tom", ""], ["Newby", "David", ""], ["Rhode", "Kawal", ""], ["Ourselin", "Sebastien", ""], ["Mohiaddin", "Raad", ""], ["Keegan", "Jennifer", ""], ["Firmin", "David", ""], ["Yang", "Guang", ""]]}, {"id": "1902.07891", "submitter": "Guilei Hu", "authors": "Guilei Hu, Yang Xiao, Zhiguo Cao, Lubin Meng, Zhiwen Fang, Joey Tianyi\n  Zhou, Junsong Yuan", "title": "Towards Real-time Eyeblink Detection in The Wild:Dataset,Theory and\n  Practices", "comments": null, "journal-ref": "IEEE Transactions on Information Forensics and Security 2019", "doi": "10.1109/TIFS.2019.29599778", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective and real-time eyeblink detection is of wide-range applications,\nsuch as deception detection, drive fatigue detection, face anti-spoofing, etc.\nAlthough numerous of efforts have already been paid, most of them focus on\naddressing the eyeblink detection problem under the constrained indoor\nconditions with the relative consistent subject and environment setup.\nNevertheless, towards the practical applications eyeblink detection in the wild\nis more required, and of greater challenges. However, to our knowledge this has\nnot been well studied before. In this paper, we shed the light to this research\ntopic. A labelled eyeblink in the wild dataset (i.e., HUST-LEBW) of 673\neyeblink video samples (i.e., 381 positives, and 292 negatives) is first\nestablished by us. These samples are captured from the unconstrained movies,\nwith the dramatic variation on human attribute, human pose, illumination\ncondition, imaging configuration, etc. Then, we formulate eyeblink detection\ntask as a spatial-temporal pattern recognition problem. After locating and\ntracking human eye using SeetaFace engine and KCF tracker respectively, a\nmodified LSTM model able to capture the multi-scale temporal information is\nproposed to execute eyeblink verification. A feature extraction approach that\nreveals appearance and motion characteristics simultaneously is also proposed.\nThe experiments on HUST-LEBW reveal the superiority and efficiency of our\napproach. It also verifies that, the existing eyeblink detection methods cannot\nachieve satisfactory performance in the wild.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 07:15:19 GMT"}, {"version": "v2", "created": "Sun, 5 May 2019 05:38:58 GMT"}, {"version": "v3", "created": "Wed, 18 Dec 2019 12:10:35 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Hu", "Guilei", ""], ["Xiao", "Yang", ""], ["Cao", "Zhiguo", ""], ["Meng", "Lubin", ""], ["Fang", "Zhiwen", ""], ["Zhou", "Joey Tianyi", ""], ["Yuan", "Junsong", ""]]}, {"id": "1902.07897", "submitter": "Alice Yang", "authors": "Alice Yi Yang and Ling Cheng", "title": "Long-Bone Fracture Detection using Artificial Neural Networks based on\n  Contour Features of X-ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The following paper proposes two contour-based fracture detection schemes.\nThe development of the contour-based fracture is based on the line-based\nfracture detection schemes proposed in arXiv:1902.07458. Existing Computer\nAided Diagnosis (CAD) systems commonly employs Convolutional Neural Networks\n(CNN), although the cost to obtain a high accuracy is the amount of training\ndata required. The purpose of the proposed schemes is to obtain a high\nclassification accuracy with a reduced number of training data through the use\nof detected contours in X-ray images. There are two contour-based fracture\ndetection schemes. The first is the Standard Contour Histogram Feature-Based\n(CHFB) and the second is the improved CHFB scheme. The difference between the\ntwo schemes is the removal of the surrounding detected flesh contours from the\nleg region in the improved CHFB scheme. The flesh contours are automatically\nclassified as non-fractures. The contours are further refined to give a precise\nrepresentation of the image edge objects. A total of 19 features are extracted\nfrom each refined contour. 8 out of the 19 features are based on the number of\noccurrences for particular detected gradients in the contour. Moreover, the\noccurrence of the 0-degree gradient in the contours are employed for the\nseparation of the knee, leg and foot region. The features are a summary\nrepresentation of the contour, in which it is used as inputs into the\nArtificial Neural Network (ANN). Both Standard CHFB and improved CHFB schemes\nare evaluated with the same experimental set-ups. The average system accuracy\nfor the Standard CHFB scheme is 80.7%, whilst the improved CHFB scheme has an\naverage accuracy of 82.98%. Additionally, the hierarchical clustering technique\nis adopted to highlight the fractured region within the X-ray image, using\nextracted 0-degree gradients from fractured contours.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 07:48:10 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Yang", "Alice Yi", ""], ["Cheng", "Ling", ""]]}, {"id": "1902.07902", "submitter": "Guiying Zhang", "authors": "Guiying Zhang, Yuxin Cui, Yong Zhao and Jianjun Hu", "title": "ComplexFace: a Multi-Representation Approach for Image Classification\n  with Small Dataset", "comments": "This paper includes 12 pages,6 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art face recognition algorithms are able to achieve good\nperformance when sufficient training images are provided. Unfortunately, the\nnumber of facial images is limited in some real face recognition applications.\nIn this paper, we propose ComplexFace, a novel and effective algorithm for face\nrecognition with limited samples using complex number based data augmentation.\nThe algorithm first generates new representations from original samples and\nthen fuse both into complex numbers, which avoids the difficulty of weight\nsetting in other fusion approaches. A test sample can then be expressed by the\nlinear combination of all the training samples, which mapped the sample to the\nnew representation space for classification by the kernel function. The\ncollaborative representation based classifier is then built to make\npredictions. Extensive experiments on the Georgia Tech (GT) face database and\nthe ORL face database show that our algorithm significantly outperforms\nexisting methods: the average errors of previous approaches ranging from 31.66%\nto 41.75% are reduced to 14.54% over the GT database; the average errors of\nprevious approaches ranging from 5.21% to 10.99% are reduced to 1.67% over the\nORL database. In other words, our algorithm has decreased the average errors by\nup to 84.80% on the ORL database.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 07:52:56 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 11:26:48 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhang", "Guiying", ""], ["Cui", "Yuxin", ""], ["Zhao", "Yong", ""], ["Hu", "Jianjun", ""]]}, {"id": "1902.07967", "submitter": "Jun Li", "authors": "Jun Li, Daoyu Lin, Yang Wang, Guangluan Xu, Chibiao Ding", "title": "Deep Discriminative Representation Learning with Attention Map for Scene\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning powerful discriminative features for remote sensing image scene\nclassification is a challenging computer vision problem. In the past, most\nclassification approaches were based on handcrafted features. However, most\nrecent approaches to remote sensing scene classification are based on\nConvolutional Neural Networks (CNNs). The de facto practice when learning these\nCNN models is only to use original RGB patches as input with training performed\non large amounts of labeled data (ImageNet). In this paper, we show class\nactivation map (CAM) encoded CNN models, codenamed DDRL-AM, trained using\noriginal RGB patches and attention map based class information provide\ncomplementary information to the standard RGB deep models. To the best of our\nknowledge, we are the first to investigate attention information encoded CNNs.\nAdditionally, to enhance the discriminability, we further employ a recently\ndeveloped object function called \"center loss,\" which has proved to be very\nuseful in face recognition. Finally, our framework provides attention guidance\nto the model in an end-to-end fashion. Extensive experiments on two benchmark\ndatasets show that our approach matches or exceeds the performance of other\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 11:09:18 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Li", "Jun", ""], ["Lin", "Daoyu", ""], ["Wang", "Yang", ""], ["Xu", "Guangluan", ""], ["Ding", "Chibiao", ""]]}, {"id": "1902.07971", "submitter": "Markus Haltmeier", "authors": "Nadja Gruber, Stephan Antholzer, Werner Jaschke, Christian Kremser and\n  Markus Haltmeier", "title": "A Joint Deep Learning Approach for Automated Liver and Tumor\n  Segmentation", "comments": "To appear in the SAMPTA 2019 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hepatocellular carcinoma (HCC) is the most common type of primary liver\ncancer in adults, and the most common cause of death of people suffering from\ncirrhosis. The segmentation of liver lesions in CT images allows assessment of\ntumor load, treatment planning, prognosis and monitoring of treatment response.\nManual segmentation is a very time-consuming task and in many cases, prone to\ninaccuracies and automatic tools for tumor detection and segmentation are\ndesirable. In this paper, we compare two network architectures, one that is\ncomposed of one neural network and manages the segmentation task in one step\nand one that consists of two consecutive fully convolutional neural networks.\nThe first network segments the liver whereas the second network segments the\nactual tumor inside the liver. Our networks are trained on a subset of the LiTS\n(Liver Tumor Segmentation) Challenge and evaluated on data.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 11:20:11 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 12:48:16 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Gruber", "Nadja", ""], ["Antholzer", "Stephan", ""], ["Jaschke", "Werner", ""], ["Kremser", "Christian", ""], ["Haltmeier", "Markus", ""]]}, {"id": "1902.07985", "submitter": "Shuming Jiao", "authors": "Yang Gao, Shuming Jiao, Juncheng Fang, Ting Lei, Zhenwei Xie, Xiaocong\n  Yuan", "title": "Multiple-image encryption and hiding with an optical diffractive neural\n  network", "comments": null, "journal-ref": null, "doi": "10.1016/j.optcom.2020.125476", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cascaded phase-only mask architecture (or an optical diffractive neural\nnetwork) can be employed for different optical information processing tasks\nsuch as pattern recognition, orbital angular momentum (OAM) mode conversion,\nimage salience detection and image encryption. However, for optical encryption\nand watermarking applications, such a system usually cannot process multiple\npairs of input images and output images simultaneously. In our proposed scheme,\nmultiple input images can be simultaneously fed to an optical diffractive\nneural network (DNN) system and each corresponding output image will be\ndisplayed in a non-overlap sub-region in the output imaging plane. Each input\nimage undergoes a different optical transform in an independent channel within\nthe same system. The multiple cascaded phase masks in the system can be\neffectively optimized by a wavefront matching algorithm. Similar to recent\noptical pattern recognition and mode conversion works, the orthogonality\nproperty is employed to design a multiplexed DNN.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 11:55:09 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 23:35:13 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Gao", "Yang", ""], ["Jiao", "Shuming", ""], ["Fang", "Juncheng", ""], ["Lei", "Ting", ""], ["Xie", "Zhenwei", ""], ["Yuan", "Xiaocong", ""]]}, {"id": "1902.07987", "submitter": "Jan Kieseler", "authors": "Shah Rukh Qasim, Jan Kieseler, Yutaro Iiyama, Maurizio Pierini", "title": "Learning representations of irregular particle-detector geometry with\n  distance-weighted graph networks", "comments": "10p, v2: published version", "journal-ref": "Eur. Phys. J. C, 79 7 (2019) 608", "doi": "10.1140/epjc/s10052-019-7113-9", "report-no": null, "categories": "physics.data-an cs.CV hep-ex stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of graph networks to deal with irregular-geometry\ndetectors in the context of particle reconstruction. Thanks to their\nrepresentation-learning capabilities, graph networks can exploit the full\ndetector granularity, while natively managing the event sparsity and\narbitrarily complex detector geometries. We introduce two distance-weighted\ngraph network architectures, dubbed GarNet and GravNet layers, and apply them\nto a typical particle reconstruction task. The performance of the new\narchitectures is evaluated on a data set of simulated particle interactions on\na toy model of a highly granular calorimeter, loosely inspired by the endcap\ncalorimeter to be installed in the CMS detector for the High-Luminosity LHC\nphase. We study the clustering of energy depositions, which is the basis for\ncalorimetric particle reconstruction, and provide a quantitative comparison to\nalternative approaches. The proposed algorithms provide an interesting\nalternative to existing methods, offering equally performing or less\nresource-demanding solutions with less underlying assumptions on the detector\ngeometry and, consequently, the possibility to generalize to other detectors.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 11:57:12 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 15:43:05 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Qasim", "Shah Rukh", ""], ["Kieseler", "Jan", ""], ["Iiyama", "Yutaro", ""], ["Pierini", "Maurizio", ""]]}, {"id": "1902.07995", "submitter": "Shibiao Xu", "authors": "Yong Zhao and Shibiao Xu and Shuhui Bu and Hongkai Jiang and Pengcheng\n  Han", "title": "GSLAM: A General SLAM Framework and Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SLAM technology has recently seen many successes and attracted the attention\nof high-technological companies. However, how to unify the interface of\nexisting or emerging algorithms, and effectively perform benchmark about the\nspeed, robustness and portability are still problems. In this paper, we propose\na novel SLAM platform named GSLAM, which not only provides evaluation\nfunctionality, but also supplies useful toolkit for researchers to quickly\ndevelop their own SLAM systems. The core contribution of GSLAM is an universal,\ncross-platform and full open-source SLAM interface for both research and\ncommercial usage, which is aimed to handle interactions with input dataset,\nSLAM implementation, visualization and applications in an unified framework.\nThrough this platform, users can implement their own functions for better\nperformance with plugin form and further boost the application to practical\nusage of the SLAM.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 12:10:28 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Zhao", "Yong", ""], ["Xu", "Shibiao", ""], ["Bu", "Shuhui", ""], ["Jiang", "Hongkai", ""], ["Han", "Pengcheng", ""]]}, {"id": "1902.08052", "submitter": "Uthman Baroudi Dr", "authors": "Uthman Baroudi, M. Alharbi, K. Alhouty, H. Baafeef, K. Alofi", "title": "Cloud-Based Autonomous Indoor Navigation: A Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this case study, we design, integrate and implement a cloud-enabled\nautonomous robotic navigation system. The system has the following features:\nmap generation and robot coordination via cloud service and video streaming to\nallow online monitoring and control in case of emergency. The system has been\ntested to generate a map for a long corridor using two modes: manual and\nautonomous. The autonomous mode has shown more accurate map. In addition, the\nfield experiments confirm the benefit of offloading the heavy computation to\nthe cloud by significantly shortening the time required to build the map.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 13:56:31 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Baroudi", "Uthman", ""], ["Alharbi", "M.", ""], ["Alhouty", "K.", ""], ["Baafeef", "H.", ""], ["Alofi", "K.", ""]]}, {"id": "1902.08123", "submitter": "Fernando Alonso-Fernandez", "authors": "Fernando Alonso-Fernandez, Kiran B. Raja, R. Raghavendra, Cristoph\n  Busch, Josef Bigun, Ruben Vera-Rodriguez, Julian Fierrez", "title": "Cross-Sensor Periocular Biometrics for Partial Face Recognition in a\n  Global Pandemic: Comparative Benchmark and Novel Multialgorithmic Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The massive availability of cameras results in a wide variability of imaging\nconditions, producing large intra-class variations and a significant\nperformance drop if heterogeneous images are compared for person recognition.\nHowever, as biometrics is deployed, it is common to replace damaged or obsolete\nhardware, or to exchange information between heterogeneous applications.\nVariations in spectral bands can also occur. For example, surveillance face\nimages (typically acquired in the visible spectrum, VIS) may need to be\ncompared against a legacy iris database (typically acquired in near-infrared,\nNIR). Here, we propose a multialgorithmic approach to cope with periocular\nimages from different sensors. With face masks in the front line against\nCOVID-19, periocular recognition is regaining popularity since it is the only\nface region that remains visible. We integrate different comparators with a\nfusion scheme based on linear logistic regression, in which scores are\nrepresented by log-likelihood ratios. This allows easy interpretation of scores\nand the use of Bayes thresholds for optimal decision-making, since scores from\ndifferent comparators are in the same probabilistic range. We evaluate our\napproach in the context of the Cross-Eyed Competition, whose aim was to compare\nrecognition approaches when NIR and VIS periocular images are matched. Our\napproach achieves EER=0.2% and FRR of just 0.47% at FAR=0.01%, representing the\nbest overall approach of the competition. Experiments are also reported with a\ndatabase of VIS images from different smartphones. We also discuss the impact\nof template size and computation times, with the most computationally heavy\ncomparator playing an important role in the results. Lastly, the proposed\nmethod is shown to outperform other popular fusion approaches, such as the\naverage of scores, SVMs or Random Forest.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 16:24:40 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 17:53:19 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2020 23:15:33 GMT"}, {"version": "v4", "created": "Wed, 28 Oct 2020 17:56:48 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Alonso-Fernandez", "Fernando", ""], ["Raja", "Kiran B.", ""], ["Raghavendra", "R.", ""], ["Busch", "Cristoph", ""], ["Bigun", "Josef", ""], ["Vera-Rodriguez", "Ruben", ""], ["Fierrez", "Julian", ""]]}, {"id": "1902.08128", "submitter": "QiKui Zhu", "authors": "Qikui Zhu, Bo Du, Pingkun Yan", "title": "Boundary-weighted Domain Adaptive Neural Network for Prostate MR Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2019.2935018", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of the prostate from magnetic resonance (MR) images\nprovides useful information for prostate cancer diagnosis and treatment.\nHowever, automated prostate segmentation from 3D MR images still faces several\nchallenges. For instance, a lack of clear edge between the prostate and other\nanatomical structures makes it challenging to accurately extract the\nboundaries. The complex background texture and large variation in size, shape\nand intensity distribution of the prostate itself make segmentation even\nfurther complicated. With deep learning, especially convolutional neural\nnetworks (CNNs), emerging as commonly used methods for medical image\nsegmentation, the difficulty in obtaining large number of annotated medical\nimages for training CNNs has become much more pronounced that ever before.\nSince large-scale dataset is one of the critical components for the success of\ndeep learning, lack of sufficient training data makes it difficult to fully\ntrain complex CNNs. To tackle the above challenges, in this paper, we propose a\nboundary-weighted domain adaptive neural network (BOWDA-Net). To make the\nnetwork more sensitive to the boundaries during segmentation, a\nboundary-weighted segmentation loss (BWL) is proposed. Furthermore, an advanced\nboundary-weighted transfer leaning approach is introduced to address the\nproblem of small medical imaging datasets. We evaluate our proposed model on\nthe publicly available MICCAI 2012 Prostate MR Image Segmentation (PROMISE12)\nchallenge dataset. Our experimental results demonstrate that the proposed model\nis more sensitive to boundary information and outperformed other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 16:27:35 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 02:00:05 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Zhu", "Qikui", ""], ["Du", "Bo", ""], ["Yan", "Pingkun", ""]]}, {"id": "1902.08134", "submitter": "Csaba Botos", "authors": "Botos Csaba, Adnane Boukhayma, Viveka Kulharia, Andr\\'as Horv\\'ath,\n  Philip H. S. Torr", "title": "Domain Partitioning Network", "comments": "18 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard adversarial training involves two agents, namely a generator and a\ndiscriminator, playing a mini-max game. However, even if the players converge\nto an equilibrium, the generator may only recover a part of the target data\ndistribution, in a situation commonly referred to as mode collapse. In this\nwork, we present the Domain Partitioning Network (DoPaNet), a new approach to\ndeal with mode collapse in generative adversarial learning. We employ multiple\ndiscriminators, each encouraging the generator to cover a different part of the\ntarget distribution. To ensure these parts do not overlap and collapse into the\nsame mode, we add a classifier as a third agent in the game. The classifier\ndecides which discriminator the generator is trained against for each sample.\nThrough experiments on toy examples and real images, we show the merits of\nDoPaNet in covering the real distribution and its superiority with respect to\nthe competing methods. Besides, we also show that we can control the modes from\nwhich samples are generated using DoPaNet.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 16:45:20 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Csaba", "Botos", ""], ["Boukhayma", "Adnane", ""], ["Kulharia", "Viveka", ""], ["Horv\u00e1th", "Andr\u00e1s", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1902.08137", "submitter": "Jochen Laubrock", "authors": "David Dubray and Jochen Laubrock", "title": "Deep CNN-based Speech Balloon Detection and Segmentation for Comic Books", "comments": "10 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a method for the automated detection and segmentation of speech\nballoons in comic books, including their carrier and tails. Our method is based\non a deep convolutional neural network that was trained on annotated pages of\nthe Graphic Narrative Corpus. More precisely, we are using a fully\nconvolutional network approach inspired by the U-Net architecture, combined\nwith a VGG-16 based encoder. The trained model delivers state-of-the-art\nperformance with an F1-score of over 0.94. Qualitative results suggest that\nwiggly tails, curved corners, and even illusory contours do not pose a major\nproblem. Furthermore, the model has learned to distinguish speech balloons from\ncaptions. We compare our model to earlier results and discuss some possible\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 16:49:12 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Dubray", "David", ""], ["Laubrock", "Jochen", ""]]}, {"id": "1902.08192", "submitter": "Yoojin Choi", "authors": "Yoojin Choi, Mostafa El-Khamy, Jungwon Lee", "title": "Jointly Sparse Convolutional Neural Networks in Dual Spatial-Winograd\n  Domains", "comments": "IEEE ICASSP 2019. arXiv admin note: substantial text overlap with\n  arXiv:1805.08303", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the optimization of deep convolutional neural networks (CNNs)\nsuch that they provide good performance while having reduced complexity if\ndeployed on either conventional systems with spatial-domain convolution or\nlower-complexity systems designed for Winograd convolution. The proposed\nframework produces one compressed model whose convolutional filters can be made\nsparse either in the spatial domain or in the Winograd domain. Hence, the\ncompressed model can be deployed universally on any platform, without need for\nre-training on the deployed platform. To get a better compression ratio, the\nsparse model is compressed in the spatial domain that has a fewer number of\nparameters. From our experiments, we obtain $24.2\\times$ and $47.7\\times$\ncompressed models for ResNet-18 and AlexNet trained on the ImageNet dataset,\nwhile their computational cost is also reduced by $4.5\\times$ and $5.1\\times$,\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 01:03:04 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Choi", "Yoojin", ""], ["El-Khamy", "Mostafa", ""], ["Lee", "Jungwon", ""]]}, {"id": "1902.08224", "submitter": "Tianming Wang", "authors": "Chandrajit Bajaj, Tianming Wang", "title": "Blind Hyperspectral-Multispectral Image Fusion via Graph Laplacian\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fusing a low-resolution hyperspectral image (HSI) and a high-resolution\nmultispectral image (MSI) of the same scene leads to a super-resolution image\n(SRI), which is information rich spatially and spectrally. In this paper, we\nsuper-resolve the HSI using the graph Laplacian defined on the MSI. Unlike many\nexisting works, we don't assume prior knowledge about the spatial degradation\nfrom SRI to HSI, nor a perfectly aligned HSI and MSI pair. Our algorithm\nprogressively alternates between finding the blur kernel and fusing HSI with\nMSI, generating accurate estimations of the blur kernel and the SRI at\nconvergence. Experiments on various datasets demonstrate the advantages of the\nproposed algorithm in the quality of fusion and its capability in dealing with\nunknown spatial degradation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 19:27:59 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Bajaj", "Chandrajit", ""], ["Wang", "Tianming", ""]]}, {"id": "1902.08231", "submitter": "Peng Chu", "authors": "Peng Chu, Heng Fan, Chiu C Tan, and Haibin Ling", "title": "Online Multi-Object Tracking with Instance-Aware Tracker and Dynamic\n  Model Refreshment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progresses in model-free single object tracking (SOT) algorithms have\nlargely inspired applying SOT to \\emph{multi-object tracking} (MOT) to improve\nthe robustness as well as relieving dependency on external detector. However,\nSOT algorithms are generally designed for distinguishing a target from its\nenvironment, and hence meet problems when a target is spatially mixed with\nsimilar objects as observed frequently in MOT. To address this issue, in this\npaper we propose an instance-aware tracker to integrate SOT techniques for MOT\nby encoding awareness both within and between target models. In particular, we\nconstruct each target model by fusing information for distinguishing target\nboth from background and other instances (tracking targets). To conserve\nuniqueness of all target models, our instance-aware tracker considers response\nmaps from all target models and assigns spatial locations exclusively to\noptimize the overall accuracy. Another contribution we make is a dynamic model\nrefreshing strategy learned by a convolutional neural network. This strategy\nhelps to eliminate initialization noise as well as to adapt to the variation of\ntarget size and appearance. To show the effectiveness of the proposed approach,\nit is evaluated on the popular MOT15 and MOT16 challenge benchmarks. On both\nbenchmarks, our approach achieves the best overall performances in comparison\nwith published results.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 19:50:18 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Chu", "Peng", ""], ["Fan", "Heng", ""], ["Tan", "Chiu C", ""], ["Ling", "Haibin", ""]]}, {"id": "1902.08236", "submitter": "Riqiang Gao", "authors": "Jiachen Wang, Riqiang Gao, Yuankai Huo, Shunxing Bao, Yunxi Xiong,\n  Sanja L. Antic, Travis J. Osterman, Pierre P. Massion, Bennett A. Landman", "title": "Lung Cancer Detection using Co-learning from Chest CT Images and\n  Clinical Demographics", "comments": "SPIE Medical Image, oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of lung cancer is essential in reducing mortality. Recent\nstudies have demonstrated the clinical utility of low-dose computed tomography\n(CT) to detect lung cancer among individuals selected based on very limited\nclinical information. However, this strategy yields high false positive rates,\nwhich can lead to unnecessary and potentially harmful procedures. To address\nsuch challenges, we established a pipeline that co-learns from detailed\nclinical demographics and 3D CT images. Toward this end, we leveraged data from\nthe Consortium for Molecular and Cellular Characterization of Screen-Detected\nLesions (MCL), which focuses on early detection of lung cancer. A 3D\nattention-based deep convolutional neural net (DCNN) is proposed to identify\nlung cancer from the chest CT scan without prior anatomical location of the\nsuspicious nodule. To improve upon the non-invasive discrimination between\nbenign and malignant, we applied a random forest classifier to a dataset\nintegrating clinical information to imaging data. The results show that the AUC\nobtained from clinical demographics alone was 0.635 while the attention network\nalone reached an accuracy of 0.687. In contrast when applying our proposed\npipeline integrating clinical and imaging variables, we reached an AUC of 0.787\non the testing dataset. The proposed network both efficiently captures\nanatomical information for classification and also generates attention maps\nthat explain the features that drive performance.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 20:01:35 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Wang", "Jiachen", ""], ["Gao", "Riqiang", ""], ["Huo", "Yuankai", ""], ["Bao", "Shunxing", ""], ["Xiong", "Yunxi", ""], ["Antic", "Sanja L.", ""], ["Osterman", "Travis J.", ""], ["Massion", "Pierre P.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1902.08276", "submitter": "Michael Andrews", "authors": "Michael Andrews, John Alison, Sitong An, Patrick Bryant, Bjorn Burkle,\n  Sergei Gleyzer, Meenakshi Narain, Manfred Paulini, Barnabas Poczos, Emanuele\n  Usai", "title": "End-to-End Jet Classification of Quarks and Gluons with the CMS Open\n  Data", "comments": "10 pages, 5 figures, 7 tables; v2: published version", "journal-ref": "Nucl. Instrum. Methods Phys. Res. A 977, 164304 (2020)", "doi": "10.1016/j.nima.2020.164304", "report-no": null, "categories": "hep-ex cs.CV cs.LG physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe the construction of end-to-end jet image classifiers based on\nsimulated low-level detector data to discriminate quark- vs. gluon-initiated\njets with high-fidelity simulated CMS Open Data. We highlight the importance of\nprecise spatial information and demonstrate competitive performance to existing\nstate-of-the-art jet classifiers. We further generalize the end-to-end approach\nto event-level classification of quark vs. gluon di-jet QCD events. We compare\nthe fully end-to-end approach to using hand-engineered features and demonstrate\nthat the end-to-end algorithm is robust against the effects of underlying event\nand pile-up.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 21:43:09 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 23:42:48 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Andrews", "Michael", ""], ["Alison", "John", ""], ["An", "Sitong", ""], ["Bryant", "Patrick", ""], ["Burkle", "Bjorn", ""], ["Gleyzer", "Sergei", ""], ["Narain", "Meenakshi", ""], ["Paulini", "Manfred", ""], ["Poczos", "Barnabas", ""], ["Usai", "Emanuele", ""]]}, {"id": "1902.08336", "submitter": "Gavin Weiguang Ding", "authors": "Gavin Weiguang Ding, Kry Yik Chau Lui, Xiaomeng Jin, Luyu Wang,\n  Ruitong Huang", "title": "On the Sensitivity of Adversarial Robustness to Input Data Distributions", "comments": "ICLR 2019, Seventh International Conference on Learning\n  Representations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are vulnerable to small adversarial perturbations. Existing\nliterature largely focused on understanding and mitigating the vulnerability of\nlearned models. In this paper, we demonstrate an intriguing phenomenon about\nthe most popular robust training method in the literature, adversarial\ntraining: Adversarial robustness, unlike clean accuracy, is sensitive to the\ninput data distribution. Even a semantics-preserving transformations on the\ninput data distribution can cause a significantly different robustness for the\nadversarial trained model that is both trained and evaluated on the new\ndistribution. Our discovery of such sensitivity on data distribution is based\non a study which disentangles the behaviors of clean accuracy and robust\naccuracy of the Bayes classifier. Empirical investigations further confirm our\nfinding. We construct semantically-identical variants for MNIST and CIFAR10\nrespectively, and show that standardly trained models achieve comparable clean\naccuracies on them, but adversarially trained models achieve significantly\ndifferent robustness accuracies. This counter-intuitive phenomenon indicates\nthat input data distribution alone can affect the adversarial robustness of\ntrained neural networks, not necessarily the tasks themselves. Lastly, we\ndiscuss the practical implications on evaluating adversarial robustness, and\nmake initial attempts to understand this complex phenomenon.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 02:03:17 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Ding", "Gavin Weiguang", ""], ["Lui", "Kry Yik Chau", ""], ["Jin", "Xiaomeng", ""], ["Wang", "Luyu", ""], ["Huang", "Ruitong", ""]]}, {"id": "1902.08347", "submitter": "Jie Chen", "authors": "Min Zhao, Jie Chen, Zhe He", "title": "A laboratory-created dataset with ground-truth for hyperspectral\n  unmixing evaluation", "comments": "13 pages, 13 figures, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral unmixing is an important and challenging problem in hyperspectral\ndata processing. This topic has been extensively studied and a variety of\nunmixing algorithms have been proposed in the literature. However, the lack of\npublicly available dataset with ground-truth makes it difficult to evaluate and\ncompare the performance of unmixing algorithms in a quantitative and objective\nmanner. Most of the existing works rely on the use of numerical synthetic data\nand an intuitive inspection of the results of real data. To alleviate this\ndilemma, in this study, we design several experimental scenes in our\nlaboratory, including printed checkerboards, mixed quartz sands, and reflection\nwith a vertical board. A dataset is then created by imaging these scenes with\nthe hyperspectral camera in our laboratory, providing 36 mixtures with more\nthan 130, 000 pixels with 256 wavelength bands ranging from 400nm to 1000nm.\nThe experimental settings are strictly controlled so that pure material\nspectral signatures and material compositions are known. To the best of our\nknowledge, this dataset is the first publicly available dataset created in a\nsystematic manner with ground-truth for spectral unmixing. Some typical linear\nand nonlinear unmixing algorithms are also tested with this dataset and lead to\nmeaningful results.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 02:51:12 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Zhao", "Min", ""], ["Chen", "Jie", ""], ["He", "Zhe", ""]]}, {"id": "1902.08355", "submitter": "Sang-Woo Lee", "authors": "Sang-Woo Lee, Tong Gao, Sohee Yang, Jaejun Yoo, Jung-Woo Ha", "title": "Large-Scale Answerer in Questioner's Mind for Visual Dialog Question\n  Generation", "comments": "Accepted for ICLR 2019. Camera ready version. Our code is publically\n  available: https://github.com/naver/aqm-plus", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answerer in Questioner's Mind (AQM) is an information-theoretic framework\nthat has been recently proposed for task-oriented dialog systems. AQM benefits\nfrom asking a question that would maximize the information gain when it is\nasked. However, due to its intrinsic nature of explicitly calculating the\ninformation gain, AQM has a limitation when the solution space is very large.\nTo address this, we propose AQM+ that can deal with a large-scale problem and\nask a question that is more coherent to the current context of the dialog. We\nevaluate our method on GuessWhich, a challenging task-oriented visual dialog\nproblem, where the number of candidate classes is near 10K. Our experimental\nresults and ablation studies show that AQM+ outperforms the state-of-the-art\nmodels by a remarkable margin with a reasonable approximation. In particular,\nthe proposed AQM+ reduces more than 60% of error as the dialog proceeds, while\nthe comparative algorithms diminish the error by less than 6%. Based on our\nresults, we argue that AQM+ is a general task-oriented dialog algorithm that\ncan be applied for non-yes-or-no responses.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 03:46:53 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Lee", "Sang-Woo", ""], ["Gao", "Tong", ""], ["Yang", "Sohee", ""], ["Yoo", "Jaejun", ""], ["Ha", "Jung-Woo", ""]]}, {"id": "1902.08431", "submitter": "Vanderson Martins do Rosario", "authors": "Vanderson Martins do Rosario, Edson Borin, Mauricio Breternitz Jr", "title": "The Multi-Lane Capsule Network (MLCN)", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2019.2915661", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Multi-Lane Capsule Networks (MLCN), which are a separable and\nresource efficient organization of Capsule Networks (CapsNet) that allows\nparallel processing, while achieving high accuracy at reduced cost. A MLCN is\ncomposed of a number of (distinct) parallel lanes, each contributing to a\ndimension of the result, trained using the routing-by-agreement organization of\nCapsNet. Our results indicate similar accuracy with a much reduced cost in\nnumber of parameters for the Fashion-MNIST and Cifar10 datsets. They also\nindicate that the MLCN outperforms the original CapsNet when using a proposed\nnovel configuration for the lanes. MLCN also has faster training and inference\ntimes, being more than two-fold faster than the original CapsNet in the same\naccelerator.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 10:44:55 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Rosario", "Vanderson Martins do", ""], ["Borin", "Edson", ""], ["Breternitz", "Mauricio", "Jr"]]}, {"id": "1902.08527", "submitter": "Xiaoxiao He", "authors": "Xiaoxiao He, Chaowei Tan, Yuting Qiao, Virak Tan, Dimitris Metaxas,\n  Kang Li", "title": "Effective 3D Humerus and Scapula Extraction using Low-contrast and\n  High-shape-variability MR Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For the initial shoulder preoperative diagnosis, it is essential to obtain a\nthree-dimensional (3D) bone mask from medical images, e.g., magnetic resonance\n(MR). However, obtaining high-resolution and dense medical scans is both costly\nand time-consuming. In addition, the imaging parameters for each 3D scan may\nvary from time to time and thus increase the variance between images.\nTherefore, it is practical to consider the bone extraction on low-resolution\ndata which may influence imaging contrast and make the segmentation work\ndifficult. In this paper, we present a joint segmentation for the humerus and\nscapula bones on a small dataset with low-contrast and high-shape-variability\n3D MR images. The proposed network has a deep end-to-end architecture to obtain\nthe initial 3D bone masks. Because the existing scarce and inaccurate\nhuman-labeled ground truth, we design a self-reinforced learning strategy to\nincrease performance. By comparing with the non-reinforced segmentation and a\nclassical multi-atlas method with joint label fusion, the proposed approach\nobtains better results.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 15:16:25 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["He", "Xiaoxiao", ""], ["Tan", "Chaowei", ""], ["Qiao", "Yuting", ""], ["Tan", "Virak", ""], ["Metaxas", "Dimitris", ""], ["Li", "Kang", ""]]}, {"id": "1902.08546", "submitter": "Xin Fu", "authors": "Xin Fu, Jia Yan, Cien Fan", "title": "Image Aesthetics Assessment Using Composite Features from off-the-Shelf\n  Deep Models", "comments": "Accepted by ICIP 2018", "journal-ref": null, "doi": "10.1109/ICIP.2018.8451133", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have recently achieved great success on\nimage aesthetics assessment task. In this paper, we propose an efficient method\nwhich takes the global, local and scene-aware information of images into\nconsideration and exploits the composite features extracted from corresponding\npretrained deep learning models to classify the derived features with support\nvector machine. Contrary to popular methods that require fine-tuning or\ntraining a new model from scratch, our training-free method directly takes the\ndeep features generated by off-the-shelf models for image classification and\nscene recognition. Also, we analyzed the factors that could influence the\nperformance from two aspects: the architecture of the deep neural network and\nthe contribution of local and scene-aware information. It turns out that deep\nresidual network could produce more aesthetics-aware image representation and\ncomposite features lead to the improvement of overall performance. Experiments\non common large-scale aesthetics assessment benchmarks demonstrate that our\nmethod outperforms the state-of-the-art results in photo aesthetics assessment.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 16:14:53 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Fu", "Xin", ""], ["Yan", "Jia", ""], ["Fan", "Cien", ""]]}, {"id": "1902.08553", "submitter": "Xin Fu", "authors": "Xin Fu, Chengkai Zhang, Xiang Peng, Lihua Jian, Zheng Liu", "title": "Towards end-to-end pulsed eddy current classification and regression\n  with CNN", "comments": "Accepted by IEEE I2MTC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pulsed eddy current (PEC) is an effective electromagnetic non-destructive\ninspection (NDI) technique for metal materials, which has already been widely\nadopted in detecting cracking and corrosion in some multi-layer structures.\nAutomatically inspecting the defects in these structures would be conducive to\nfurther analysis and treatment of them. In this paper, we propose an effective\nend-to-end model using convolutional neural networks (CNN) to learn effective\nfeatures from PEC data. Specifically, we construct a multi-task generic model,\nbased on 1D CNN, to predict both the class and depth of flaws simultaneously.\nExtensive experiments demonstrate our model is capable of handling both\nclassification and regression tasks on PEC data. Our proposed model obtains\nhigher accuracy and lower error compared to other standard methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 16:39:20 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Fu", "Xin", ""], ["Zhang", "Chengkai", ""], ["Peng", "Xiang", ""], ["Jian", "Lihua", ""], ["Liu", "Zheng", ""]]}, {"id": "1902.08570", "submitter": "Huilin Qu", "authors": "Huilin Qu and Loukas Gouskos", "title": "ParticleNet: Jet Tagging via Particle Clouds", "comments": "11 pages, 4 figures; v3: updated to match the version published in\n  PRD; Code available at https://github.com/hqucms/ParticleNet", "journal-ref": "Phys. Rev. D 101, 056019 (2020)", "doi": "10.1103/PhysRevD.101.056019", "report-no": null, "categories": "hep-ph cs.CV hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to represent a jet is at the core of machine learning on jet physics.\nInspired by the notion of point clouds, we propose a new approach that\nconsiders a jet as an unordered set of its constituent particles, effectively a\n\"particle cloud\". Such a particle cloud representation of jets is efficient in\nincorporating raw information of jets and also explicitly respects the\npermutation symmetry. Based on the particle cloud representation, we propose\nParticleNet, a customized neural network architecture using Dynamic Graph\nConvolutional Neural Network for jet tagging problems. The ParticleNet\narchitecture achieves state-of-the-art performance on two representative jet\ntagging benchmarks and is improved significantly over existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 17:26:39 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 01:57:56 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 15:57:41 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Qu", "Huilin", ""], ["Gouskos", "Loukas", ""]]}, {"id": "1902.08670", "submitter": "Xingyu Li", "authors": "Xingyu Li, Marko Radulovic, Ksenija Kanjer, Konstantinos N.\n  Plataniotis", "title": "Discriminative Pattern Mining for Breast Cancer Histopathology Image\n  Classification via Fully Convolutional Autoencoder", "comments": null, "journal-ref": "IEEE Access, vol. 7, 2019", "doi": "10.1109/ACCESS.2019.2904245", "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate diagnosis of breast cancer in histopathology images is challenging\ndue to the heterogeneity of cancer cell growth as well as of a variety of\nbenign breast tissue proliferative lesions. In this paper, we propose a\npractical and self-interpretable invasive cancer diagnosis solution. With\nminimum annotation information, the proposed method mines contrast patterns\nbetween normal and malignant images in unsupervised manner and generates a\nprobability map of abnormalities to verify its reasoning. Particularly, a fully\nconvolutional autoencoder is used to learn the dominant structural patterns\namong normal image patches. Patches that do not share the characteristics of\nthis normal population are detected and analyzed by one-class support vector\nmachine and 1-layer neural network. We apply the proposed method to a public\nbreast cancer image set. Our results, in consultation with a senior\npathologist, demonstrate that the proposed method outperforms existing methods.\nThe obtained probability map could benefit the pathology practice by providing\nvisualized verification data and potentially leads to a better understanding of\ndata-driven diagnosis solutions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 21:22:52 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 17:43:11 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 15:35:08 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Li", "Xingyu", ""], ["Radulovic", "Marko", ""], ["Kanjer", "Ksenija", ""], ["Plataniotis", "Konstantinos N.", ""]]}, {"id": "1902.08716", "submitter": "Ling Zhang", "authors": "Ling Zhang, Le Lu, Xiaosong Wang, Robert M. Zhu, Mohammadhadi Bagheri,\n  Ronald M. Summers, Jianhua Yao", "title": "Spatio-Temporal Convolutional LSTMs for Tumor Growth Prediction by\n  Learning 4D Longitudinal Patient Data", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prognostic tumor growth modeling via volumetric medical imaging observations\ncan potentially lead to better outcomes of tumor treatment and surgical\nplanning. Recent advances of convolutional networks have demonstrated higher\naccuracy than traditional mathematical models in predicting future tumor\nvolumes. This indicates that deep learning-based techniques may have great\npotentials on addressing such problem. However, current 2D patch-based modeling\napproaches cannot make full use of the spatio-temporal imaging context of the\ntumor's longitudinal 4D (3D + time) data. Moreover, they are incapable to\npredict clinically-relevant tumor properties, other than volumes. In this\npaper, we exploit to formulate the tumor growth process through convolutional\nLong Short-Term Memory (ConvLSTM) that extract tumor's static imaging\nappearances and capture its temporal dynamic changes within a single network.\nWe extend ConvLSTM into the spatio-temporal domain (ST-ConvLSTM) by jointly\nlearning the inter-slice 3D contexts and the longitudinal or temporal dynamics\nfrom multiple patient studies. Our approach can incorporate other non-imaging\npatient information in an end-to-end trainable manner. Experiments are\nconducted on the largest 4D longitudinal tumor dataset of 33 patients to date.\nResults validate that the ST-ConvLSTM produces a Dice score of 83.2%+-5.1% and\na RVD of 11.2%+-10.8%, both significantly outperforming (p<0.05) other compared\nmethods of linear model, ConvLSTM, and generative adversarial network (GAN)\nunder the metric of predicting future tumor volumes. Additionally, our new\nmethod enables the prediction of both cell density and CT intensity numbers.\nLast, we demonstrate the generalizability of ST-ConvLSTM by employing it in 4D\nmedical image segmentation task, which achieves an averaged Dice score of\n86.3+-1.2% for left-ventricle segmentation in 4D ultrasound with 3 seconds per\npatient.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 02:05:50 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 18:59:14 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Zhang", "Ling", ""], ["Lu", "Le", ""], ["Wang", "Xiaosong", ""], ["Zhu", "Robert M.", ""], ["Bagheri", "Mohammadhadi", ""], ["Summers", "Ronald M.", ""], ["Yao", "Jianhua", ""]]}, {"id": "1902.08722", "submitter": "Hadi Salman", "authors": "Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang", "title": "A Convex Relaxation Barrier to Tight Robustness Verification of Neural\n  Networks", "comments": "Poster at the 33rd Conference on Neural Information Processing\n  Systems (NeurIPS 2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification of neural networks enables us to gauge their robustness against\nadversarial attacks. Verification algorithms fall into two categories: exact\nverifiers that run in exponential time and relaxed verifiers that are efficient\nbut incomplete. In this paper, we unify all existing LP-relaxed verifiers, to\nthe best of our knowledge, under a general convex relaxation framework. This\nframework works for neural networks with diverse architectures and\nnonlinearities and covers both primal and dual views of robustness\nverification. We further prove strong duality between the primal and dual\nproblems under very mild conditions. Next, we perform large-scale experiments,\namounting to more than 22 CPU-years, to obtain exact solution to the\nconvex-relaxed problem that is optimal within our framework for ReLU networks.\nWe find the exact solution does not significantly improve upon the gap between\nPGD and existing relaxed verifiers for various networks trained normally or\nrobustly on MNIST and CIFAR datasets. Our results suggest there is an inherent\nbarrier to tight verification for the large class of methods captured by our\nframework. We discuss possible causes of this barrier and potential future\ndirections for bypassing it. Our code and trained models are available at\nhttp://github.com/Hadisalman/robust-verify-benchmark .\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 03:01:51 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 18:53:39 GMT"}, {"version": "v3", "created": "Fri, 3 May 2019 02:17:14 GMT"}, {"version": "v4", "created": "Sun, 3 Nov 2019 09:31:37 GMT"}, {"version": "v5", "created": "Fri, 10 Jan 2020 00:20:07 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Salman", "Hadi", ""], ["Yang", "Greg", ""], ["Zhang", "Huan", ""], ["Hsieh", "Cho-Jui", ""], ["Zhang", "Pengchuan", ""]]}, {"id": "1902.08785", "submitter": "Renjie Xie", "authors": "Renjie Xie, Yanzhi Chen, Yan Wo, Qiao Wang", "title": "A Deep, Information-theoretic Framework for Robust Biometric Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) have been a de facto standard for nowadays\nbiometric recognition solutions. A serious, but still overlooked problem in\nthese DNN-based recognition systems is their vulnerability against adversarial\nattacks. Adversarial attacks can easily cause the output of a DNN system to\ngreatly distort with only tiny changes in its input. Such distortions can\npotentially lead to an unexpected match between a valid biometric and a\nsynthetic one constructed by a strategic attacker, raising security issue. In\nthis work, we show how this issue can be resolved by learning robust biometric\nfeatures through a deep, information-theoretic framework, which builds upon the\nrecent deep variational information bottleneck method but is carefully adapted\nto biometric recognition tasks. Empirical evaluation demonstrates that our\nmethod not only offers stronger robustness against adversarial attacks but also\nprovides better recognition performance over state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 12:26:13 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Xie", "Renjie", ""], ["Chen", "Yanzhi", ""], ["Wo", "Yan", ""], ["Wang", "Qiao", ""]]}, {"id": "1902.08788", "submitter": "Yuedong Chen", "authors": "Yuedong Chen, Jianfeng Wang, Shikai Chen, Zhongchao Shi, Jianfei Cai", "title": "Facial Motion Prior Networks for Facial Expression Recognition", "comments": "VCIP 2019, Oral. Code is available at\n  https://github.com/donydchen/FMPN-FER", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based facial expression recognition (FER) has received a lot of\nattention in the past few years. Most of the existing deep learning based FER\nmethods do not consider domain knowledge well, which thereby fail to extract\nrepresentative features. In this work, we propose a novel FER framework, named\nFacial Motion Prior Networks (FMPN). Particularly, we introduce an addition\nbranch to generate a facial mask so as to focus on facial muscle moving\nregions. To guide the facial mask learning, we propose to incorporate prior\ndomain knowledge by using the average differences between neutral faces and the\ncorresponding expressive faces as the training guidance. Extensive experiments\non three facial expression benchmark datasets demonstrate the effectiveness of\nthe proposed method, compared with the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 13:26:45 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 03:14:57 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Chen", "Yuedong", ""], ["Wang", "Jianfeng", ""], ["Chen", "Shikai", ""], ["Shi", "Zhongchao", ""], ["Cai", "Jianfei", ""]]}, {"id": "1902.08793", "submitter": "Chi Zhang", "authors": "Chi Zhang, Kai Qiao, Linyuan Wang, Li Tong, Guoen Hu, Ruyuan Zhang,\n  Bin Yan", "title": "A visual encoding model based on deep neural networks and transfer\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Building visual encoding models to accurately predict visual\nresponses is a central challenge for current vision-based brain-machine\ninterface techniques. To achieve high prediction accuracy on neural signals,\nvisual encoding models should include precise visual features and appropriate\nprediction algorithms. Most existing visual encoding models employ hand-craft\nvisual features (e.g., Gabor wavelets or semantic labels) or data-driven\nfeatures (e.g., features extracted from deep neural networks (DNN)). They also\nassume a linear mapping between feature representation to brain activity.\nHowever, it remains unknown whether such linear mapping is sufficient for\nmaximizing prediction accuracy. New Method: We construct a new visual encoding\nframework to predict cortical responses in a benchmark functional magnetic\nresonance imaging (fMRI) dataset. In this framework, we employ the transfer\nlearning technique to incorporate a pre-trained DNN (i.e., AlexNet) and train a\nnonlinear mapping from visual features to brain activity. This nonlinear\nmapping replaces the conventional linear mapping and is supposed to improve\nprediction accuracy on brain activity. Results: The proposed framework can\nsignificantly predict responses of over 20% voxels in early visual areas (i.e.,\nV1-lateral occipital region, LO) and achieve unprecedented prediction accuracy.\nComparison with Existing Methods: Comparing to two conventional visual encoding\nmodels, we find that the proposed encoding model shows consistent higher\nprediction accuracy in all early visual areas, especially in relatively\nanterior visual areas (i.e., V4 and LO). Conclusions: Our work proposes a new\nframework to utilize pre-trained visual features and train non-linear mappings\nfrom visual features to brain activity.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 14:13:39 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Zhang", "Chi", ""], ["Qiao", "Kai", ""], ["Wang", "Linyuan", ""], ["Tong", "Li", ""], ["Hu", "Guoen", ""], ["Zhang", "Ruyuan", ""], ["Yan", "Bin", ""]]}, {"id": "1902.08802", "submitter": "Harshit Sikchi", "authors": "Sumit Agarwal, Harshit S. Sikchi, Suparna Rooj, Shubhobrata\n  Bhattacharya and Aurobinda Routray", "title": "Illumination-invariant Face recognition by fusing thermal and visual\n  images via gradient transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition in real life situations like low illumination condition is\nstill an open challenge in biometric security. It is well established that the\nstate-of-the-art methods in face recognition provide low accuracy in the case\nof poor illumination. In this work, we propose an algorithm for a more robust\nillumination invariant face recognition using a multi-modal approach. We\npropose a new dataset consisting of aligned faces of thermal and visual images\nof a hundred subjects. We then apply face detection on thermal images using the\nbiggest blob extraction method and apply them for fusing images of different\nmodalities for the purpose of face recognition. An algorithm is proposed to\nimplement fusion of thermal and visual images. We reason for why relying on\nonly one modality can give erroneous results. We use a lighter and faster CNN\nmodel called MobileNet for the purpose of face recognition with faster\ninferencing and to be able to be use it in real time biometric systems. We test\nour proposed method on our own created dataset to show that real-time face\nrecognition on fused images shows far better results than using visual or\nthermal images separately.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 15:13:16 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Agarwal", "Sumit", ""], ["Sikchi", "Harshit S.", ""], ["Rooj", "Suparna", ""], ["Bhattacharya", "Shubhobrata", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1902.08888", "submitter": "Gholamreza Haffari", "authors": "Faik Aydin and Maggie Zhang and Michelle Ananda-Rajah and Gholamreza\n  Haffari", "title": "Medical Multimodal Classifiers Under Scarce Data Condition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data is one of the essential ingredients to power deep learning research.\nSmall datasets, especially specific to medical institutes, bring challenges to\ndeep learning training stage. This work aims to develop a practical deep\nmultimodal that can classify patients into abnormal and normal categories\naccurately as well as assist radiologists to detect visual and textual\nanomalies by locating areas of interest. The detection of the anomalies is\nachieved through a novel technique which extends the integrated gradients\nmethodology with an unsupervised clustering algorithm. This technique also\nintroduces a tuning parameter which trades off true positive signals to denoise\nfalse positive signals in the detection process. To overcome the challenges of\nthe small training dataset which only has 3K frontal X-ray images and medical\nreports in pairs, we have adopted transfer learning for the multimodal which\nconcatenates the layers of image and text submodels. The image submodel was\ntrained on the vast ChestX-ray14 dataset, while the text submodel transferred a\npertained word embedding layer from a hospital-specific corpus. Experimental\nresults show that our multimodal improves the accuracy of the classification by\n4% and 7% on average of 50 epochs, compared to the individual text and image\nmodel, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 03:38:41 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Aydin", "Faik", ""], ["Zhang", "Maggie", ""], ["Ananda-Rajah", "Michelle", ""], ["Haffari", "Gholamreza", ""]]}, {"id": "1902.08897", "submitter": "Ram Srivatsav Ghorakavi", "authors": "Ram Srivatsav Ghorakavi", "title": "TBNet:Pulmonary Tuberculosis Diagnosing System using Deep Neural\n  Networks", "comments": "9 pages, 8 figures, 2 Numerical table, 1 algorithm table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tuberculosis is a deadly infectious disease prevalent around the world. Due\nto the lack of proper technology in place, the early detection of this disease\nis unattainable. Also, the available methods to detect Tuberculosis is not\nup-to a commendable standards due to their dependency on unnecessary features,\nthis make such technology obsolete for a reliable health-care technology. In\nthis paper, I propose a deep-learning based system which diagnoses tuberculosis\nbased on the important features in Chest X-rays along with original chest\nX-rays. Employing our system will accelerate the process of tuberculosis\ndiagnosis by overcoming the need to perform the time-consuming sputum-based\ntesting method (Diagnostic Microbiology). In contrast to the previous methods\n\\cite{kant2018towards, melendez2016automated}, our work utilizes the\nstate-of-the-art ResNet \\cite{he2016deep} with proper data augmentation using\ntraditional robust features like Haar \\cite{viola2005detecting,viola2001rapid}\nand LBP \\cite{ojala1994performance,ojala1996comparative}. I observed that such\na procedure enhances the rate of tuberculosis detection to a highly\nsatisfactory level. Our work uses the publicly available pulmonary chest X-ray\ndataset to train our network \\cite{jaeger2014two}. Nevertheless, the publicly\navailable dataset is very small and is inadequate to achieve the best accuracy.\nTo overcome this issue I have devised an intuitive feature based data\naugmentation pipeline. Our approach shall help the deep neural network\n\\cite{lecun2015deep,he2016deep,krizhevsky2012imagenet} to focus its training on\ntuberculosis affected regions making it more robust and accurate, when compared\nto other conventional methods that use procedures like mirroring and rotation.\nBy using our simple yet powerful techniques, I observed a 10\\% boost in\nperformance accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 05:45:29 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Ghorakavi", "Ram Srivatsav", ""]]}, {"id": "1902.08900", "submitter": "Zhenglin Geng", "authors": "Zhenglin Geng, Chen Cao, Sergey Tulyakov", "title": "3D Guided Fine-Grained Face Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for fine-grained face manipulation. Given a face image\nwith an arbitrary expression, our method can synthesize another arbitrary\nexpression by the same person. This is achieved by first fitting a 3D face\nmodel and then disentangling the face into a texture and a shape. We then learn\ndifferent networks in these two spaces. In the texture space, we use a\nconditional generative network to change the appearance, and carefully design\ninput formats and loss functions to achieve the best results. In the shape\nspace, we use a fully connected network to predict the accurate shapes and use\nthe available depth data for supervision. Both networks are conditioned on\nexpression coefficients rather than discrete labels, allowing us to generate an\nunlimited amount of expressions. We show the superiority of this disentangling\napproach through both quantitative and qualitative studies. In a user study,\nour method is preferred in 85% of cases when compared to the most recent work.\nWhen compared to the ground truth, annotators cannot reliably distinguish\nbetween our synthesized images and real images, preferring our method in 53% of\nthe cases.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 06:47:45 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Geng", "Zhenglin", ""], ["Cao", "Chen", ""], ["Tulyakov", "Sergey", ""]]}, {"id": "1902.08913", "submitter": "Mario Bijelic", "authors": "Mario Bijelic, Tobias Gruber, Fahim Mannan, Florian Kraus, Werner\n  Ritter, Klaus Dietmayer, Felix Heide", "title": "Seeing Through Fog Without Seeing Fog: Deep Multimodal Sensor Fusion in\n  Unseen Adverse Weather", "comments": null, "journal-ref": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fusion of multimodal sensor streams, such as camera, lidar, and radar\nmeasurements, plays a critical role in object detection for autonomous\nvehicles, which base their decision making on these inputs. While existing\nmethods exploit redundant information in good environmental conditions, they\nfail in adverse weather where the sensory streams can be asymmetrically\ndistorted. These rare \"edge-case\" scenarios are not represented in available\ndatasets, and existing fusion architectures are not designed to handle them. To\naddress this challenge we present a novel multimodal dataset acquired in over\n10,000km of driving in northern Europe. Although this dataset is the first\nlarge multimodal dataset in adverse weather, with 100k labels for lidar,\ncamera, radar, and gated NIR sensors, it does not facilitate training as\nextreme weather is rare. To this end, we present a deep fusion network for\nrobust fusion without a large corpus of labeled training data covering all\nasymmetric distortions. Departing from proposal-level fusion, we propose a\nsingle-shot model that adaptively fuses features, driven by measurement\nentropy. We validate the proposed method, trained on clean data, on our\nextensive validation dataset. Code and data are available here\nhttps://github.com/princeton-computational-imaging/SeeingThroughFog.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 10:05:18 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 07:51:36 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 14:23:45 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Bijelic", "Mario", ""], ["Gruber", "Tobias", ""], ["Mannan", "Fahim", ""], ["Kraus", "Florian", ""], ["Ritter", "Werner", ""], ["Dietmayer", "Klaus", ""], ["Heide", "Felix", ""]]}, {"id": "1902.08915", "submitter": "Yiwei Zhang", "authors": "Yiwei Zhang, Chunbiao Zhu, Ge Li, Yuan Zhao, Haifeng Shen", "title": "Bi-Skip: A Motion Deblurring Network Using Self-paced Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fast and effective motion deblurring method has great application values in\nreal life. This work presents an innovative approach in which a self-paced\nlearning is combined with GAN to deblur image. First, We explain that a proper\ngenerator can be used as deep priors and point out that the solution for\npixel-based loss is not same with the one for perception-based loss. By using\nthese ideas as starting points, a Bi-Skip network is proposed to improve the\ngenerating ability and a bi-level loss is adopted to solve the problem that\ncommon conditions are non-identical. Second, considering that the complex\nmotion blur will perturb the network in the training process, a self-paced\nmechanism is adopted to enhance the robustness of the network. Through\nextensive evaluations on both qualitative and quantitative criteria, it is\ndemonstrated that our approach has a competitive advantage over\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 10:28:04 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Zhang", "Yiwei", ""], ["Zhu", "Chunbiao", ""], ["Li", "Ge", ""], ["Zhao", "Yuan", ""], ["Shen", "Haifeng", ""]]}, {"id": "1902.08985", "submitter": "Marc Aubreville", "authors": "Marc Aubreville, Miguel Goncalves, Christian Knipfer, Nicolai Oetter,\n  Helmut Neumann, Florian Stelzle, Christopher Bohr, Andreas Maier", "title": "Transferability of Deep Learning Algorithms for Malignancy Detection in\n  Confocal Laser Endomicroscopy Images from Different Anatomical Locations of\n  the Upper Gastrointestinal Tract", "comments": "Erratum for version 1, correcting the number of CLE image sequences\n  used in one data set", "journal-ref": "BIOSTEC 2018: Biomedical Engineering Systems and Technologies", "doi": "10.1007/978-3-030-29196-9_4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Squamous Cell Carcinoma (SCC) is the most common cancer type of the\nepithelium and is often detected at a late stage. Besides invasive diagnosis of\nSCC by means of biopsy and histo-pathologic assessment, Confocal Laser\nEndomicroscopy (CLE) has emerged as noninvasive method that was successfully\nused to diagnose SCC in vivo. For interpretation of CLE images, however,\nextensive training is required, which limits its applicability and use in\nclinical practice of the method. To aid diagnosis of SCC in a broader scope,\nautomatic detection methods have been proposed. This work compares two methods\nwith regard to their applicability in a transfer learning sense, i.e. training\non one tissue type (from one clinical team) and applying the learnt\nclassification system to another entity (different anatomy, different clinical\nteam). Besides a previously proposed, patch-based method based on convolutional\nneural networks, a novel classification method on image level (based on a\npre-trained Inception V.3 network with dedicated preprocessing and\ninterpretation of class activation maps) is proposed and evaluated. The newly\npresented approach improves recognition performance, yielding accuracies of\n91.63% on the first data set (oral cavity) and 92.63% on a joint data set. The\ngeneralization from oral cavity to the second data set (vocal folds) lead to\nsimilar area-under-the-ROC curve values than a direct training on the vocal\nfolds data set, indicating good generalization.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 17:38:25 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 13:38:45 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Aubreville", "Marc", ""], ["Goncalves", "Miguel", ""], ["Knipfer", "Christian", ""], ["Oetter", "Nicolai", ""], ["Neumann", "Helmut", ""], ["Stelzle", "Florian", ""], ["Bohr", "Christopher", ""], ["Maier", "Andreas", ""]]}, {"id": "1902.08994", "submitter": "S. M. Kamrul Hasan", "authors": "S. M. Kamrul Hasan and Cristian A. Linte", "title": "U-NetPlus: A Modified Encoder-Decoder U-Net Architecture for Semantic\n  and Instance Segmentation of Surgical Instrument", "comments": "7 pages, 6 figures, IEEE conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conventional therapy approaches limit surgeons' dexterity control due to\nlimited field-of-view. With the advent of robot-assisted surgery, there has\nbeen a paradigm shift in medical technology for minimally invasive surgery.\nHowever, it is very challenging to track the position of the surgical\ninstruments in a surgical scene, and accurate detection & identification of\nsurgical tools is paramount. Deep learning-based semantic segmentation in\nframes of surgery videos has the potential to facilitate this task. In this\nwork, we modify the U-Net architecture named U-NetPlus, by introducing a\npre-trained encoder and re-design the decoder part, by replacing the transposed\nconvolution operation with an upsampling operation based on nearest-neighbor\n(NN) interpolation. To further improve performance, we also employ a very fast\nand flexible data augmentation technique. We trained the framework on 8 x 225\nframe sequences of robotic surgical videos, available through the MICCAI 2017\nEndoVis Challenge dataset and tested it on 8 x 75 frame and 2 x 300 frame\nvideos. Using our U-NetPlus architecture, we report a 90.20% DICE for binary\nsegmentation, 76.26% DICE for instrument part segmentation, and 46.07% for\ninstrument type (i.e., all instruments) segmentation, outperforming the results\nof previous techniques implemented and tested on these data.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 18:57:19 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Hasan", "S. M. Kamrul", ""], ["Linte", "Cristian A.", ""]]}, {"id": "1902.09023", "submitter": "Timo Gerasimow", "authors": "Jun Nishimura, Timo Gerasimow, Sushma Rao, Aleksandar Sutic,\n  Chyuan-Tyng Wu, Gilad Michael", "title": "Automatic ISP image quality tuning using non-linear optimization", "comments": "5 pages, 2018 25th IEEE International Conference on Image Processing\n  (ICIP), 2471-2475", "journal-ref": "2018 25th IEEE International Conference on Image Processing\n  (ICIP), 2471-2475", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Signal Processor (ISP) comprises of various blocks to reconstruct image\nsensor raw data to final image consumed by human visual system or computer\nvision applications. Each block typically has many tuning parameters due to the\ncomplexity of the operation. These need to be hand tuned by Image Quality (IQ)\nexperts, which takes considerable amount of time. In this paper, we present an\nautomatic IQ tuning using nonlinear optimization and automatic reference\ngeneration algorithms. The proposed method can produce high quality IQ in\nminutes as compared with weeks of hand-tuned results by IQ experts. In\naddition, the proposed method can work with any algorithms without being aware\nof their specific implementation. It was found successful on multiple different\nprocessing blocks such as noise reduction, demosaic, and sharpening.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 22:06:07 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Nishimura", "Jun", ""], ["Gerasimow", "Timo", ""], ["Rao", "Sushma", ""], ["Sutic", "Aleksandar", ""], ["Wu", "Chyuan-Tyng", ""], ["Michael", "Gilad", ""]]}, {"id": "1902.09063", "submitter": "Amber Simpson", "authors": "Amber L. Simpson, Michela Antonelli, Spyridon Bakas, Michel Bilello,\n  Keyvan Farahani, Bram van Ginneken, Annette Kopp-Schneider, Bennett A.\n  Landman, Geert Litjens, Bjoern Menze, Olaf Ronneberger, Ronald M. Summers,\n  Patrick Bilic, Patrick F. Christ, Richard K. G. Do, Marc Gollub, Jennifer\n  Golia-Pernicka, Stephan H. Heckers, William R. Jarnagin, Maureen K. McHugo,\n  Sandy Napel, Eugene Vorontsov, Lena Maier-Hein, and M. Jorge Cardoso", "title": "A large annotated medical image dataset for the development and\n  evaluation of segmentation algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of medical images aims to associate a pixel with a\nlabel in a medical image without human initialization. The success of semantic\nsegmentation algorithms is contingent on the availability of high-quality\nimaging data with corresponding labels provided by experts. We sought to create\na large collection of annotated medical image datasets of various clinically\nrelevant anatomies available under open source license to facilitate the\ndevelopment of semantic segmentation algorithms. Such a resource would allow:\n1) objective assessment of general-purpose segmentation methods through\ncomprehensive benchmarking and 2) open and free access to medical image data\nfor any researcher interested in the problem domain. Through a\nmulti-institutional effort, we generated a large, curated dataset\nrepresentative of several highly variable segmentation tasks that was used in a\ncrowd-sourced challenge - the Medical Segmentation Decathlon held during the\n2018 Medical Image Computing and Computer Aided Interventions Conference in\nGranada, Spain. Here, we describe these ten labeled image datasets so that\nthese data may be effectively reused by the research community.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 02:34:48 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Simpson", "Amber L.", ""], ["Antonelli", "Michela", ""], ["Bakas", "Spyridon", ""], ["Bilello", "Michel", ""], ["Farahani", "Keyvan", ""], ["van Ginneken", "Bram", ""], ["Kopp-Schneider", "Annette", ""], ["Landman", "Bennett A.", ""], ["Litjens", "Geert", ""], ["Menze", "Bjoern", ""], ["Ronneberger", "Olaf", ""], ["Summers", "Ronald M.", ""], ["Bilic", "Patrick", ""], ["Christ", "Patrick F.", ""], ["Do", "Richard K. G.", ""], ["Gollub", "Marc", ""], ["Golia-Pernicka", "Jennifer", ""], ["Heckers", "Stephan H.", ""], ["Jarnagin", "William R.", ""], ["McHugo", "Maureen K.", ""], ["Napel", "Sandy", ""], ["Vorontsov", "Eugene", ""], ["Maier-Hein", "Lena", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "1902.09080", "submitter": "Chengju Zhou", "authors": "Chengju Zhou, Meiqing Wu, Siew-Kei Lam", "title": "SSA-CNN: Semantic Self-Attention CNN for Pedestrian Detection", "comments": "wrong setting in CityPersons experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection plays an important role in many applications such as\nautonomous driving. We propose a method that explores semantic segmentation\nresults as self-attention cues to significantly improve the pedestrian\ndetection performance. Specifically, a multi-task network is designed to\njointly learn semantic segmentation and pedestrian detection from image\ndatasets with weak box-wise annotations. The semantic segmentation feature maps\nare concatenated with corresponding convolution features maps to provide more\ndiscriminative features for pedestrian detection and pedestrian classification.\nBy jointly learning segmentation and detection, our proposed pedestrian\nself-attention mechanism can effectively identify pedestrian regions and\nsuppress backgrounds. In addition, we propose to incorporate semantic attention\ninformation from multi-scale layers into deep convolution neural network to\nboost pedestrian detection. Experiment results show that the proposed method\nachieves the best detection performance with MR of 6.27% on Caltech dataset and\nobtain competitive performance on CityPersons dataset while maintaining high\ncomputational efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 04:13:25 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 05:06:55 GMT"}, {"version": "v3", "created": "Thu, 6 Jun 2019 01:28:28 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Zhou", "Chengju", ""], ["Wu", "Meiqing", ""], ["Lam", "Siew-Kei", ""]]}, {"id": "1902.09085", "submitter": "Zihao Wang", "authors": "Zihao W. Wang, Vibhav Vineet, Francesco Pittaluga, Sudipta Sinha,\n  Oliver Cossairt, Sing Bing Kang", "title": "Privacy-Preserving Action Recognition using Coded Aperture Videos", "comments": "CVCOPS2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The risk of unauthorized remote access of streaming video from networked\ncameras underlines the need for stronger privacy safeguards. We propose a\nlens-free coded aperture camera system for human action recognition that is\nprivacy-preserving. While coded aperture systems exist, we believe ours is the\nfirst system designed for action recognition without the need for image\nrestoration as an intermediate step. Action recognition is done using a deep\nnetwork that takes in as input, non-invertible motion features between pairs of\nframes computed using phase correlation and log-polar transformation. Phase\ncorrelation encodes translation while the log polar transformation encodes\nin-plane rotation and scaling. We show that the translation features are\nindependent of the coded aperture design, as long as its spectral response\nwithin the bandwidth has no zeros. Stacking motion features computed on frames\nat multiple different strides in the video can improve accuracy. Preliminary\nresults on simulated data based on a subset of the UCF and NTU datasets are\npromising. We also describe our prototype lens-free coded aperture camera\nsystem, and results for real captured videos are mixed.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 04:44:34 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 23:40:00 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Wang", "Zihao W.", ""], ["Vineet", "Vibhav", ""], ["Pittaluga", "Francesco", ""], ["Sinha", "Sudipta", ""], ["Cossairt", "Oliver", ""], ["Kang", "Sing Bing", ""]]}, {"id": "1902.09103", "submitter": "Tianwei Shen", "authors": "Tianwei Shen, Zixin Luo, Lei Zhou, Hanyu Deng, Runze Zhang, Tian Fang,\n  Long Quan", "title": "Beyond Photometric Loss for Self-Supervised Ego-Motion Estimation", "comments": "Accepted by ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate relative pose is one of the key components in visual odometry (VO)\nand simultaneous localization and mapping (SLAM). Recently, the self-supervised\nlearning framework that jointly optimizes the relative pose and target image\ndepth has attracted the attention of the community. Previous works rely on the\nphotometric error generated from depths and poses between adjacent frames,\nwhich contains large systematic error under realistic scenes due to reflective\nsurfaces and occlusions. In this paper, we bridge the gap between geometric\nloss and photometric loss by introducing the matching loss constrained by\nepipolar geometry in a self-supervised framework. Evaluated on the KITTI\ndataset, our method outperforms the state-of-the-art unsupervised ego-motion\nestimation methods by a large margin. The code and data are available at\nhttps://github.com/hlzz/DeepMatchVO.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 06:22:52 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Shen", "Tianwei", ""], ["Luo", "Zixin", ""], ["Zhou", "Lei", ""], ["Deng", "Hanyu", ""], ["Zhang", "Runze", ""], ["Fang", "Tian", ""], ["Quan", "Long", ""]]}, {"id": "1902.09104", "submitter": "Yuan Hu", "authors": "Yuan Hu, Yunpeng Chen, Xiang Li and Jiashi Feng", "title": "Dynamic Feature Fusion for Semantic Edge Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Features from multiple scales can greatly benefit the semantic edge detection\ntask if they are well fused. However, the prevalent semantic edge detection\nmethods apply a fixed weight fusion strategy where images with different\nsemantics are forced to share the same weights, resulting in universal fusion\nweights for all images and locations regardless of their different semantics or\nlocal context. In this work, we propose a novel dynamic feature fusion strategy\nthat assigns different fusion weights for different input images and locations\nadaptively. This is achieved by a proposed weight learner to infer proper\nfusion weights over multi-level features for each location of the feature map,\nconditioned on the specific input. In this way, the heterogeneity in\ncontributions made by different locations of feature maps and input images can\nbe better considered and thus help produce more accurate and sharper edge\npredictions. We show that our model with the novel dynamic feature fusion is\nsuperior to fixed weight fusion and also the na\\\"ive location-invariant weight\nfusion methods, via comprehensive experiments on benchmarks Cityscapes and SBD.\nIn particular, our method outperforms all existing well established methods and\nachieves new state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 06:36:13 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Hu", "Yuan", ""], ["Chen", "Yunpeng", ""], ["Li", "Xiang", ""], ["Feng", "Jiashi", ""]]}, {"id": "1902.09107", "submitter": "Abinaya Manimaran", "authors": "Abinaya Manimaran, Thiyagarajan Ramanathan, Suya You, C-C Jay Kuo", "title": "Visualization, Discriminability and Applications of Interpretable Saak\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the power of Saak features as an effort towards\ninterpretable deep learning. Being inspired by the operations of convolutional\nlayers of convolutional neural networks, multi-stage Saak transform was\nproposed. Based on this foundation, we provide an in-depth examination on Saak\nfeatures, which are coefficients of the Saak transform, by analyzing their\nproperties through visualization and demonstrating their applications in image\nclassification. Being similar to CNN features, Saak features at later stages\nhave larger receptive fields, yet they are obtained in a one-pass feedforward\nmanner without backpropagation. The whole feature extraction process is\ntransparent and is of extremely low complexity. The discriminant power of Saak\nfeatures is demonstrated, and their classification performance in three\nwell-known datasets (namely, MNIST, CIFAR-10 and STL-10) is shown by\nexperimental results.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 06:43:49 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 04:20:21 GMT"}, {"version": "v3", "created": "Sun, 3 Mar 2019 05:24:56 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Manimaran", "Abinaya", ""], ["Ramanathan", "Thiyagarajan", ""], ["You", "Suya", ""], ["Kuo", "C-C Jay", ""]]}, {"id": "1902.09130", "submitter": "Chenyang Si", "authors": "Chenyang Si, Wentao Chen, Wei Wang, Liang Wang, Tieniu Tan", "title": "An Attention Enhanced Graph Convolutional LSTM Network for\n  Skeleton-Based Action Recognition", "comments": "Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skeleton-based action recognition is an important task that requires the\nadequate understanding of movement characteristics of a human action from the\ngiven skeleton sequence. Recent studies have shown that exploring spatial and\ntemporal features of the skeleton sequence is vital for this task.\nNevertheless, how to effectively extract discriminative spatial and temporal\nfeatures is still a challenging problem. In this paper, we propose a novel\nAttention Enhanced Graph Convolutional LSTM Network (AGC-LSTM) for human action\nrecognition from skeleton data. The proposed AGC-LSTM can not only capture\ndiscriminative features in spatial configuration and temporal dynamics but also\nexplore the co-occurrence relationship between spatial and temporal domains. We\nalso present a temporal hierarchical architecture to increases temporal\nreceptive fields of the top AGC-LSTM layer, which boosts the ability to learn\nthe high-level semantic representation and significantly reduces the\ncomputation cost. Furthermore, to select discriminative spatial information,\nthe attention mechanism is employed to enhance information of key joints in\neach AGC-LSTM layer. Experimental results on two datasets are provided: NTU\nRGB+D dataset and Northwestern-UCLA dataset. The comparison results demonstrate\nthe effectiveness of our approach and show that our approach outperforms the\nstate-of-the-art methods on both datasets.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 08:08:50 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 06:38:38 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Si", "Chenyang", ""], ["Chen", "Wentao", ""], ["Wang", "Wei", ""], ["Wang", "Liang", ""], ["Tan", "Tieniu", ""]]}, {"id": "1902.09135", "submitter": "Longfei Ren", "authors": "Longfei Ren, Chengjing Wang, Peipei Tang, and Zheng Ma", "title": "A Dual Symmetric Gauss-Seidel Alternating Direction Method of\n  Multipliers for Hyperspectral Sparse Unmixing", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV eess.IV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since sparse unmixing has emerged as a promising approach to hyperspectral\nunmixing, some spatial-contextual information in the hyperspectral images has\nbeen exploited to improve the performance of the unmixing recently. The total\nvariation (TV) has been widely used to promote the spatial homogeneity as well\nas the smoothness between adjacent pixels. However, the computation task for\nhyperspectral sparse unmixing with a TV regularization term is heavy. Besides,\nthe convergence of the primal alternating direction method of multipliers\n(ADMM) for the hyperspectral sparse unmixing with a TV regularization term has\nnot been explained in details. In this paper, we design an efficient and\nconvergent dual symmetric Gauss-Seidel ADMM (sGS-ADMM) for hyperspectral sparse\nunmixing with a TV regularization term. We also present the global convergence\nand local linear convergence rate analysis for this algorithm. As demonstrated\nin numerical experiments, our algorithm can obviously improve the efficiency of\nthe unmixing compared with the state-of-the-art algorithm. More importantly, we\ncan obtain images with higher quality.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 08:28:01 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 11:33:28 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Ren", "Longfei", ""], ["Wang", "Chengjing", ""], ["Tang", "Peipei", ""], ["Ma", "Zheng", ""]]}, {"id": "1902.09145", "submitter": "Pengpeng Liu", "authors": "Pengpeng Liu, Irwin King, Michael R.Lyu, Jia Xu", "title": "DDFlow: Learning Optical Flow with Unlabeled Data Distillation", "comments": "8 pages, AAAI 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DDFlow, a data distillation approach to learning optical flow\nestimation from unlabeled data. The approach distills reliable predictions from\na teacher network, and uses these predictions as annotations to guide a student\nnetwork to learn optical flow. Unlike existing work relying on hand-crafted\nenergy terms to handle occlusion, our approach is data-driven, and learns\noptical flow for occluded pixels. This enables us to train our model with a\nmuch simpler loss function, and achieve a much higher accuracy. We conduct a\nrigorous evaluation on the challenging Flying Chairs, MPI Sintel, KITTI 2012\nand 2015 benchmarks, and show that our approach significantly outperforms all\nexisting unsupervised learning methods, while running at real time.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 08:58:10 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Liu", "Pengpeng", ""], ["King", "Irwin", ""], ["Lyu", "Michael R.", ""], ["Xu", "Jia", ""]]}, {"id": "1902.09159", "submitter": "Silas {\\O}rting", "authors": "Silas {\\O}rting, Andrew Doyle, Arno van Hilten, Matthias Hirth, Oana\n  Inel, Christopher R. Madan, Panagiotis Mavridis, Helen Spiers, Veronika\n  Cheplygina", "title": "A Survey of Crowdsourcing in Medical Image Analysis", "comments": "Submitted to Human Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid advances in image processing capabilities have been seen across many\ndomains, fostered by the application of machine learning algorithms to\n\"big-data\". However, within the realm of medical image analysis, advances have\nbeen curtailed, in part, due to the limited availability of large-scale,\nwell-annotated datasets. One of the main reasons for this is the high cost\noften associated with producing large amounts of high-quality meta-data.\nRecently, there has been growing interest in the application of crowdsourcing\nfor this purpose; a technique that has proven effective for creating\nlarge-scale datasets across a range of disciplines, from computer vision to\nastrophysics. Despite the growing popularity of this approach, there has not\nyet been a comprehensive literature review to provide guidance to researchers\nconsidering using crowdsourcing methodologies in their own medical imaging\nanalysis. In this survey, we review studies applying crowdsourcing to the\nanalysis of medical images, published prior to July 2018. We identify common\napproaches, challenges and considerations, providing guidance of utility to\nresearchers adopting this approach. Finally, we discuss future opportunities\nfor development within this emerging domain.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 09:21:09 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 12:47:16 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["\u00d8rting", "Silas", ""], ["Doyle", "Andrew", ""], ["van Hilten", "Arno", ""], ["Hirth", "Matthias", ""], ["Inel", "Oana", ""], ["Madan", "Christopher R.", ""], ["Mavridis", "Panagiotis", ""], ["Spiers", "Helen", ""], ["Cheplygina", "Veronika", ""]]}, {"id": "1902.09173", "submitter": "Feng Ji", "authors": "Feng Ji, Jielong Yang, Qiang Zhang, and Wee Peng Tay", "title": "GFCN: A New Graph Convolutional Network Based on Parallel Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In view of the huge success of convolution neural networks (CNN) for image\nclassification and object recognition, there have been attempts to generalize\nthe method to general graph-structured data. One major direction is based on\nspectral graph theory and graph signal processing. In this paper, we study the\nproblem from a completely different perspective, by introducing parallel flow\ndecomposition of graphs. The essential idea is to decompose a graph into\nfamilies of non-intersecting one dimensional (1D) paths, after which, we may\napply a 1D CNN along each family of paths. We demonstrate that the our method,\nwhich we call GraphFlow, is able to transfer CNN architectures to general\ngraphs. To show the effectiveness of our approach, we test our method on the\nclassical MNIST dataset, synthetic datasets on network information propagation\nand a news article classification dataset.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 10:06:15 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 08:06:09 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 09:50:59 GMT"}, {"version": "v4", "created": "Fri, 6 Mar 2020 09:50:15 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Ji", "Feng", ""], ["Yang", "Jielong", ""], ["Zhang", "Qiang", ""], ["Tay", "Wee Peng", ""]]}, {"id": "1902.09184", "submitter": "Jan-Aike Bolte", "authors": "Jan-Aike Bolte, Andreas B\\\"ar, Daniel Lipinski, Tim Fingscheidt", "title": "Towards Corner Case Detection for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The progress in autonomous driving is also due to the increased availability\nof vast amounts of training data for the underlying machine learning\napproaches. Machine learning systems are generally known to lack robustness,\ne.g., if the training data did rarely or not at all cover critical situations.\nThe challenging task of corner case detection in video, which is also somehow\nrelated to unusual event or anomaly detection, aims at detecting these unusual\nsituations, which could become critical, and to communicate this to the\nautonomous driving system (online use case). Such a system, however, could be\nalso used in offline mode to screen vast amounts of data and select only the\nrelevant situations for storing and (re)training machine learning algorithms.\nSo far, the approaches for corner case detection have been limited to videos\nrecorded from a fixed camera, mostly for security surveillance. In this paper,\nwe provide a formal definition of a corner case and propose a system framework\nfor both the online and the offline use case that can handle video signals from\nfront cameras of a naturally moving vehicle and can output a corner case score.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 10:34:36 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 08:18:36 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Bolte", "Jan-Aike", ""], ["B\u00e4r", "Andreas", ""], ["Lipinski", "Daniel", ""], ["Fingscheidt", "Tim", ""]]}, {"id": "1902.09212", "submitter": "Bin Xiao", "authors": "Ke Sun and Bin Xiao and Dong Liu and Jingdong Wang", "title": "Deep High-Resolution Representation Learning for Human Pose Estimation", "comments": "accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is an official pytorch implementation of Deep High-Resolution\nRepresentation Learning for Human Pose Estimation. In this work, we are\ninterested in the human pose estimation problem with a focus on learning\nreliable high-resolution representations. Most existing methods recover\nhigh-resolution representations from low-resolution representations produced by\na high-to-low resolution network. Instead, our proposed network maintains\nhigh-resolution representations through the whole process. We start from a\nhigh-resolution subnetwork as the first stage, gradually add high-to-low\nresolution subnetworks one by one to form more stages, and connect the\nmutli-resolution subnetworks in parallel. We conduct repeated multi-scale\nfusions such that each of the high-to-low resolution representations receives\ninformation from other parallel representations over and over, leading to rich\nhigh-resolution representations. As a result, the predicted keypoint heatmap is\npotentially more accurate and spatially more precise. We empirically\ndemonstrate the effectiveness of our network through the superior pose\nestimation results over two benchmark datasets: the COCO keypoint detection\ndataset and the MPII Human Pose dataset. The code and models have been publicly\navailable at\n\\url{https://github.com/leoxiaobin/deep-high-resolution-net.pytorch}.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 11:55:28 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Sun", "Ke", ""], ["Xiao", "Bin", ""], ["Liu", "Dong", ""], ["Wang", "Jingdong", ""]]}, {"id": "1902.09286", "submitter": "Jan Philip G\\\"opfert", "authors": "Jan Philip G\\\"opfert and Andr\\'e Artelt and Heiko Wersing and Barbara\n  Hammer", "title": "Adversarial attacks hidden in plain sight", "comments": null, "journal-ref": "Advances in Intelligent Data Analysis XVIII (2020) Pages 235-247", "doi": "10.1007/978-3-030-44584-3_19", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have been used to achieve a string of successes\nduring recent years, but their lack of interpretability remains a serious\nissue. Adversarial examples are designed to deliberately fool neural networks\ninto making any desired incorrect classification, potentially with very high\ncertainty. Several defensive approaches increase robustness against adversarial\nattacks, demanding attacks of greater magnitude, which lead to visible\nartifacts. By considering human visual perception, we compose a technique that\nallows to hide such adversarial attacks in regions of high complexity, such\nthat they are imperceptible even to an astute observer. We carry out a user\nstudy on classifying adversarially modified images to validate the perceptual\nquality of our approach and find significant evidence for its concealment with\nregards to human visual perception.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 14:27:05 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 20:48:37 GMT"}, {"version": "v3", "created": "Sun, 26 Apr 2020 13:45:21 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["G\u00f6pfert", "Jan Philip", ""], ["Artelt", "Andr\u00e9", ""], ["Wersing", "Heiko", ""], ["Hammer", "Barbara", ""]]}, {"id": "1902.09305", "submitter": "Xiong Zhang", "authors": "Xiong Zhang and Qiang Li and Hong Mo and Wenbo Zhang and Wen Zheng", "title": "End-to-end Hand Mesh Recovery from a Monocular RGB Image", "comments": "11 pages;", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a HAnd Mesh Recovery (HAMR) framework to tackle the\nproblem of reconstructing the full 3D mesh of a human hand from a single RGB\nimage. In contrast to existing research on 2D or 3D hand pose estimation from\nRGB or/and depth image data, HAMR can provide a more expressive and useful mesh\nrepresentation for monocular hand image understanding. In particular, the mesh\nrepresentation is achieved by parameterizing a generic 3D hand model with shape\nand relative 3D joint angles. By utilizing this mesh representation, we can\neasily compute the 3D joint locations via linear interpolations between the\nvertexes of the mesh, while obtain the 2D joint locations with a projection of\nthe 3D joints.To this end, a differentiable re-projection loss can be defined\nin terms of the derived representations and the ground-truth labels, thus\nmaking our framework end-to-end trainable.Qualitative experiments show that our\nframework is capable of recovering appealing 3D hand mesh even in the presence\nof severe occlusions.Quantitatively, our approach also outperforms the\nstate-of-the-art methods for both 2D and 3D hand pose estimation from a\nmonocular RGB image on several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 14:47:11 GMT"}, {"version": "v2", "created": "Sat, 9 Mar 2019 16:25:23 GMT"}, {"version": "v3", "created": "Sat, 7 Sep 2019 12:46:32 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Zhang", "Xiong", ""], ["Li", "Qiang", ""], ["Mo", "Hong", ""], ["Zhang", "Wenbo", ""], ["Zheng", "Wen", ""]]}, {"id": "1902.09324", "submitter": "Stefan Schneider", "authors": "Stefan Schneider, Graham W. Taylor, Stefan Linquist, Stefan C. Kremer", "title": "Similarity Learning Networks for Animal Individual Re-Identification --\n  Beyond the Capabilities of a Human Observer", "comments": "9 pages, 4 figures, 3 table. WACV 2020 - Deep Learning for Animal\n  Re-ID Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become the standard methodology to approach computer vision\ntasks when large amounts of labeled data are available. One area where\ntraditional deep learning approaches fail to perform is one-shot learning tasks\nwhere a model must correctly classify a new category after seeing only one\nexample. One such domain is animal re-identification, an application of\ncomputer vision which can be used globally as a method to automate species\npopulation estimates from camera trap images. Our work demonstrates both the\napplication of similarity comparison networks to animal re-identification, as\nwell as the capabilities of deep convolutional neural networks to generalize\nacross domains. Few studies have considered animal re-identification methods\nacross species. Here, we compare two similarity comparison methodologies:\nSiamese and Triplet-Loss, based on the AlexNet, VGG-19, DenseNet201,\nMobileNetV2, and InceptionV3 architectures considering mean average precision\n(mAP)@1 and mAP@5. We consider five data sets corresponding to five different\nspecies: humans, chimpanzees, humpback whales, fruit flies, and Siberian\ntigers, each with their own unique set of challenges. We demonstrate that\nTriplet Loss outperformed its Siamese counterpart for all species. Without any\nspecies-specific modifications, our results demonstrate that similarity\ncomparison networks can reach a performance level beyond that of humans for the\ntask of animal re-identification. The ability for researchers to re-identify an\nanimal individual upon re-encounter is fundamental for addressing a broad range\nof questions in the study of population dynamics and community/behavioural\necology. Our expectation is that similarity comparison networks are the\nbeginning of a major trend that could stand to revolutionize animal\nre-identification from camera trap data.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 19:52:23 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 01:45:59 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 16:05:44 GMT"}, {"version": "v4", "created": "Wed, 1 Jul 2020 15:53:15 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Schneider", "Stefan", ""], ["Taylor", "Graham W.", ""], ["Linquist", "Stefan", ""], ["Kremer", "Stefan C.", ""]]}, {"id": "1902.09326", "submitter": "Tianhao Yang", "authors": "Tianhao Yang, Zheng-Jun Zha, Hanwang Zhang", "title": "Making History Matter: History-Advantage Sequence Training for Visual\n  Dialog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the multi-round response generation in visual dialog, where a\nresponse is generated according to a visually grounded conversational history.\nGiven a triplet: an image, Q&A history, and current question, all the\nprevailing methods follow a codec (i.e., encoder-decoder) fashion in a\nsupervised learning paradigm: a multimodal encoder encodes the triplet into a\nfeature vector, which is then fed into the decoder for the current answer\ngeneration, supervised by the ground-truth. However, this conventional\nsupervised learning does NOT take into account the impact of imperfect history,\nviolating the conversational nature of visual dialog and thus making the codec\nmore inclined to learn history bias but not contextual reasoning. To this end,\ninspired by the actor-critic policy gradient in reinforcement learning, we\npropose a novel training paradigm called History Advantage Sequence Training\n(HAST). Specifically, we intentionally impose wrong answers in the history,\nobtaining an adverse critic, and see how the historic error impacts the codec's\nfuture behavior by History Advantage-a quantity obtained by subtracting the\nadverse critic from the gold reward of ground-truth history. Moreover, to make\nthe codec more sensitive to the history, we propose a novel attention network\ncalled History-Aware Co-Attention Network (HACAN) which can be effectively\ntrained by using HAST. Experimental results on three benchmarks: VisDial\nv0.9&v1.0 and GuessWhat?!, show that the proposed HAST strategy consistently\noutperforms the state-of-the-art supervised counterparts.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 14:58:35 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 15:16:57 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2019 15:09:16 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Yang", "Tianhao", ""], ["Zha", "Zheng-Jun", ""], ["Zhang", "Hanwang", ""]]}, {"id": "1902.09368", "submitter": "Gi-Cheon Kang", "authors": "Gi-Cheon Kang, Jaeseo Lim, Byoung-Tak Zhang", "title": "Dual Attention Networks for Visual Reference Resolution in Visual Dialog", "comments": "EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual dialog (VisDial) is a task which requires an AI agent to answer a\nseries of questions grounded in an image. Unlike in visual question answering\n(VQA), the series of questions should be able to capture a temporal context\nfrom a dialog history and exploit visually-grounded information. A problem\ncalled visual reference resolution involves these challenges, requiring the\nagent to resolve ambiguous references in a given question and find the\nreferences in a given image. In this paper, we propose Dual Attention Networks\n(DAN) for visual reference resolution. DAN consists of two kinds of attention\nnetworks, REFER and FIND. Specifically, REFER module learns latent\nrelationships between a given question and a dialog history by employing a\nself-attention mechanism. FIND module takes image features and reference-aware\nrepresentations (i.e., the output of REFER module) as input, and performs\nvisual grounding via bottom-up attention mechanism. We qualitatively and\nquantitatively evaluate our model on VisDial v1.0 and v0.9 datasets, showing\nthat DAN outperforms the previous state-of-the-art model by a significant\nmargin.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 15:32:56 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 06:31:31 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 02:24:23 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Kang", "Gi-Cheon", ""], ["Lim", "Jaeseo", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1902.09383", "submitter": "Amy Zhao", "authors": "Amy Zhao, Guha Balakrishnan, Fr\\'edo Durand, John V. Guttag, Adrian V.\n  Dalca", "title": "Data augmentation using learned transformations for one-shot medical\n  image segmentation", "comments": "9 pages, CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is an important task in many medical applications. Methods\nbased on convolutional neural networks attain state-of-the-art accuracy;\nhowever, they typically rely on supervised training with large labeled\ndatasets. Labeling medical images requires significant expertise and time, and\ntypical hand-tuned approaches for data augmentation fail to capture the complex\nvariations in such images.\n  We present an automated data augmentation method for synthesizing labeled\nmedical images. We demonstrate our method on the task of segmenting magnetic\nresonance imaging (MRI) brain scans. Our method requires only a single\nsegmented scan, and leverages other unlabeled scans in a semi-supervised\napproach. We learn a model of transformations from the images, and use the\nmodel along with the labeled example to synthesize additional labeled examples.\nEach transformation is comprised of a spatial deformation field and an\nintensity change, enabling the synthesis of complex effects such as variations\nin anatomy and image acquisition procedures. We show that training a supervised\nsegmenter with these new examples provides significant improvements over\nstate-of-the-art methods for one-shot biomedical image segmentation. Our code\nis available at https://github.com/xamyzhao/brainstorm.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 15:49:47 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 21:49:38 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zhao", "Amy", ""], ["Balakrishnan", "Guha", ""], ["Durand", "Fr\u00e9do", ""], ["Guttag", "John V.", ""], ["Dalca", "Adrian V.", ""]]}, {"id": "1902.09385", "submitter": "Satadal Saha", "authors": "Satadal Saha", "title": "A Review on Automatic License Plate Recognition System", "comments": "In Proceedings of Students' Article Competition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic License Plate Recognition (ALPR) is a challenging problem to the\nresearch community due to its potential applicability in the diverse\ngeographical condition over the globe with varying license plate parameters.\nAny ALPR system includes three main modules, viz. localization of the license\nplate, segmentation of the characters therein and recognition of the segmented\ncharacters. In real life applications where the images are captured over days\nand nights in an outdoor environment with varying lighting and weather\nconditions, varying pollution level and wind turbulences, localization,\nsegmentation and recognition become challenging tasks. The tasks become more\ncomplex if the license plate is not in conformity with the standards laid by\ncorresponding Motor Vehicles Department in terms of various features, e.g. area\nand aspect ratio of the license plate, background color, foreground color,\nshape, number of lines, font face/ size of characters, spacing between\ncharacters etc. Besides, license plates are often dirty or broken or having\nscratches or bent or tilted at its position. All these add to the challenges in\ndeveloping an effective ALPR system.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 15:51:14 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Saha", "Satadal", ""]]}, {"id": "1902.09487", "submitter": "R\\'emi Cad\\`ene", "authors": "Remi Cadene and Hedi Ben-younes and Matthieu Cord and Nicolas Thome", "title": "MUREL: Multimodal Relational Reasoning for Visual Question Answering", "comments": "CVPR2019 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal attentional networks are currently state-of-the-art models for\nVisual Question Answering (VQA) tasks involving real images. Although attention\nallows to focus on the visual content relevant to the question, this simple\nmechanism is arguably insufficient to model complex reasoning features required\nfor VQA or other high-level tasks.\n  In this paper, we propose MuRel, a multimodal relational network which is\nlearned end-to-end to reason over real images. Our first contribution is the\nintroduction of the MuRel cell, an atomic reasoning primitive representing\ninteractions between question and image regions by a rich vectorial\nrepresentation, and modeling region relations with pairwise combinations.\nSecondly, we incorporate the cell into a full MuRel network, which\nprogressively refines visual and question interactions, and can be leveraged to\ndefine visualization schemes finer than mere attention maps.\n  We validate the relevance of our approach with various ablation studies, and\nshow its superiority to attention-based methods on three datasets: VQA 2.0,\nVQA-CP v2 and TDIUC. Our final MuRel network is competitive to or outperforms\nstate-of-the-art results in this challenging context.\n  Our code is available: https://github.com/Cadene/murel.bootstrap.pytorch\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 18:04:05 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Cadene", "Remi", ""], ["Ben-younes", "Hedi", ""], ["Cord", "Matthieu", ""], ["Thome", "Nicolas", ""]]}, {"id": "1902.09506", "submitter": "Drew A. Hudson", "authors": "Drew A. Hudson and Christopher D. Manning", "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional\n  Question Answering", "comments": "Published as a conference paper at CVPR 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce GQA, a new dataset for real-world visual reasoning and\ncompositional question answering, seeking to address key shortcomings of\nprevious VQA datasets. We have developed a strong and robust question engine\nthat leverages scene graph structures to create 22M diverse reasoning\nquestions, all come with functional programs that represent their semantics. We\nuse the programs to gain tight control over the answer distribution and present\na new tunable smoothing technique to mitigate question biases. Accompanying the\ndataset is a suite of new metrics that evaluate essential qualities such as\nconsistency, grounding and plausibility. An extensive analysis is performed for\nbaselines as well as state-of-the-art models, providing fine-grained results\nfor different question types and topologies. Whereas a blind LSTM obtains mere\n42.1%, and strong VQA models achieve 54.1%, human performance tops at 89.3%,\noffering ample opportunity for new research to explore. We strongly hope GQA\nwill provide an enabling resource for the next generation of models with\nenhanced robustness, improved consistency, and deeper semantic understanding\nfor images and language.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 18:37:49 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 09:10:11 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 22:24:55 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Hudson", "Drew A.", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1902.09513", "submitter": "Paul Voigtlaender", "authors": "Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig Adam, Bastian\n  Leibe, Liang-Chieh Chen", "title": "FEELVOS: Fast End-to-End Embedding Learning for Video Object\n  Segmentation", "comments": "CVPR 2019 camera-ready version", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n  2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the recent successful methods for video object segmentation (VOS) are\noverly complicated, heavily rely on fine-tuning on the first frame, and/or are\nslow, and are hence of limited practical use. In this work, we propose FEELVOS\nas a simple and fast method which does not rely on fine-tuning. In order to\nsegment a video, for each frame FEELVOS uses a semantic pixel-wise embedding\ntogether with a global and a local matching mechanism to transfer information\nfrom the first frame and from the previous frame of the video to the current\nframe. In contrast to previous work, our embedding is only used as an internal\nguidance of a convolutional network. Our novel dynamic segmentation head allows\nus to train the network, including the embedding, end-to-end for the multiple\nobject segmentation task with a cross entropy loss. We achieve a new state of\nthe art in video object segmentation without fine-tuning with a J&F measure of\n71.5% on the DAVIS 2017 validation set. We make our code and models available\nat https://github.com/tensorflow/models/tree/master/research/feelvos.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 18:50:40 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 13:50:21 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Voigtlaender", "Paul", ""], ["Chai", "Yuning", ""], ["Schroff", "Florian", ""], ["Adam", "Hartwig", ""], ["Leibe", "Bastian", ""], ["Chen", "Liang-Chieh", ""]]}, {"id": "1902.09516", "submitter": "Jos\\'e M. F\\'acil", "authors": "Jose M. Facil, Daniel Olid, Luis Montesano and Javier Civera", "title": "Condition-Invariant Multi-View Place Recognition", "comments": "Project website: http://webdiis.unizar.es/~jmfacil/cimvpr/ In\n  submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual place recognition is particularly challenging when places suffer\nchanges in its appearance. Such changes are indeed common, e.g., due to\nweather, night/day or seasons. In this paper we leverage on recent research\nusing deep networks, and explore how they can be improved by exploiting the\ntemporal sequence information. Specifically, we propose 3 different\nalternatives (Descriptor Grouping, Fusion and Recurrent Descriptors) for deep\nnetworks to use several frames of a sequence. We show that our approaches\nproduce more compact and best performing descriptors than single- and\nmulti-view baselines in the literature in two public databases.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 18:56:55 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Facil", "Jose M.", ""], ["Olid", "Daniel", ""], ["Montesano", "Luis", ""], ["Civera", "Javier", ""]]}, {"id": "1902.09596", "submitter": "Pierre-Henri Conze", "authors": "Pierre-Henri Conze, Florian Tilquin, Mathieu Lamard, Fabrice Heitz,\n  Gwenol\\'e Quellec", "title": "Unsupervised learning-based long-term superpixel tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding correspondences between structural entities decomposing images is of\nhigh interest for computer vision applications. In particular, we analyze how\nto accurately track superpixels - visual primitives generated by aggregating\nadjacent pixels sharing similar characteristics - over extended time periods\nrelying on unsupervised learning and temporal integration. A two-step video\nprocessing pipeline dedicated to long-term superpixel tracking is proposed.\nFirst, unsupervised learning-based superpixel matching provides correspondences\nbetween consecutive and distant frames using new context-rich features extended\nfrom greyscale to multi-channel and forward-backward consistency contraints.\nResulting elementary matches are then combined along multi-step paths running\nthrough the whole sequence with various inter-frame distances. This produces a\nlarge set of candidate long-term superpixel pairings upon which majority voting\nis performed. Video object tracking experiments demonstrate the accuracy of our\nelementary estimator against state-of-the-art methods and proves the ability of\nmulti-step integration to provide accurate long-term superpixel matches\ncompared to usual direct and sequential integration.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 20:11:12 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Conze", "Pierre-Henri", ""], ["Tilquin", "Florian", ""], ["Lamard", "Mathieu", ""], ["Heitz", "Fabrice", ""], ["Quellec", "Gwenol\u00e9", ""]]}, {"id": "1902.09600", "submitter": "Rayson Laroca", "authors": "Rayson Laroca, Victor Barroso, Matheus A. Diniz, Gabriel R.\n  Gon\\c{c}alves, William Robson Schwartz, David Menotti", "title": "Convolutional Neural Networks for Automatic Meter Reading", "comments": null, "journal-ref": "Journal of Electronic Imaging 28(1), 013023 (5 February 2019)", "doi": "10.1117/1.JEI.28.1.013023", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle Automatic Meter Reading (AMR) by leveraging the high\ncapability of Convolutional Neural Networks (CNNs). We design a two-stage\napproach that employs the Fast-YOLO object detector for counter detection and\nevaluates three different CNN-based approaches for counter recognition. In the\nAMR literature, most datasets are not available to the research community since\nthe images belong to a service company. In this sense, we introduce a new\npublic dataset, called UFPR-AMR dataset, with 2,000 fully and manually\nannotated images. This dataset is, to the best of our knowledge, three times\nlarger than the largest public dataset found in the literature and contains a\nwell-defined evaluation protocol to assist the development and evaluation of\nAMR methods. Furthermore, we propose the use of a data augmentation technique\nto generate a balanced training set with many more examples to train the CNN\nmodels for counter recognition. In the proposed dataset, impressive results\nwere obtained and a detailed speed/accuracy trade-off evaluation of each model\nwas performed. In a public dataset, state-of-the-art results were achieved\nusing less than 200 images for training.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 20:32:42 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Laroca", "Rayson", ""], ["Barroso", "Victor", ""], ["Diniz", "Matheus A.", ""], ["Gon\u00e7alves", "Gabriel R.", ""], ["Schwartz", "William Robson", ""], ["Menotti", "David", ""]]}, {"id": "1902.09630", "submitter": "Nathan Tsoi", "authors": "Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian\n  Reid, Silvio Savarese", "title": "Generalized Intersection over Union: A Metric and A Loss for Bounding\n  Box Regression", "comments": "accepted in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Intersection over Union (IoU) is the most popular evaluation metric used in\nthe object detection benchmarks. However, there is a gap between optimizing the\ncommonly used distance losses for regressing the parameters of a bounding box\nand maximizing this metric value. The optimal objective for a metric is the\nmetric itself. In the case of axis-aligned 2D bounding boxes, it can be shown\nthat $IoU$ can be directly used as a regression loss. However, $IoU$ has a\nplateau making it infeasible to optimize in the case of non-overlapping\nbounding boxes. In this paper, we address the weaknesses of $IoU$ by\nintroducing a generalized version as both a new loss and a new metric. By\nincorporating this generalized $IoU$ ($GIoU$) as a loss into the state-of-the\nart object detection frameworks, we show a consistent improvement on their\nperformance using both the standard, $IoU$ based, and new, $GIoU$ based,\nperformance measures on popular object detection benchmarks such as PASCAL VOC\nand MS COCO.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 21:47:33 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 03:18:03 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Rezatofighi", "Hamid", ""], ["Tsoi", "Nathan", ""], ["Gwak", "JunYoung", ""], ["Sadeghian", "Amir", ""], ["Reid", "Ian", ""], ["Savarese", "Silvio", ""]]}, {"id": "1902.09631", "submitter": "Matt Amodio", "authors": "Matthew Amodio, Smita Krishnaswamy", "title": "TraVeLGAN: Image-to-image Translation by Transformation Vector Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Interest in image-to-image translation has grown substantially in recent\nyears with the success of unsupervised models based on the cycle-consistency\nassumption. The achievements of these models have been limited to a particular\nsubset of domains where this assumption yields good results, namely homogeneous\ndomains that are characterized by style or texture differences. We tackle the\nchallenging problem of image-to-image translation where the domains are defined\nby high-level shapes and contexts, as well as including significant clutter and\nheterogeneity. For this purpose, we introduce a novel GAN based on preserving\nintra-domain vector transformations in a latent space learned by a siamese\nnetwork. The traditional GAN system introduced a discriminator network to guide\nthe generator into generating images in the target domain. To this two-network\nsystem we add a third: a siamese network that guides the generator so that each\noriginal image shares semantics with its generated version. With this new\nthree-network system, we no longer need to constrain the generators with the\nubiquitous cycle-consistency restraint. As a result, the generators can learn\nmappings between more complex domains that differ from each other by large\ndifferences - not just style or texture.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 21:48:32 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Amodio", "Matthew", ""], ["Krishnaswamy", "Smita", ""]]}, {"id": "1902.09641", "submitter": "Chen Sun", "authors": "Chen Sun and Per Karlsson and Jiajun Wu and Joshua B Tenenbaum and\n  Kevin Murphy", "title": "Stochastic Prediction of Multi-Agent Interactions from Partial\n  Observations", "comments": "ICLR 2019 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that learns to integrate temporal information, from a\nlearned dynamics model, with ambiguous visual information, from a learned\nvision model, in the context of interacting agents. Our method is based on a\ngraph-structured variational recurrent neural network (Graph-VRNN), which is\ntrained end-to-end to infer the current state of the (partially observed)\nworld, as well as to forecast future states. We show that our method\noutperforms various baselines on two sports datasets, one based on real\nbasketball trajectories, and one generated by a soccer game engine.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 22:17:34 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Sun", "Chen", ""], ["Karlsson", "Per", ""], ["Wu", "Jiajun", ""], ["Tenenbaum", "Joshua B", ""], ["Murphy", "Kevin", ""]]}, {"id": "1902.09658", "submitter": "Yi Li Dr.", "authors": "Yi Li", "title": "Detecting Lesion Bounding Ellipses With Gaussian Proposal Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lesions characterized by computed tomography (CT) scans, are arguably often\nelliptical objects. However, current lesion detection systems are predominantly\nadopted from the popular Region Proposal Networks (RPNs) that only propose\nbounding boxes without fully leveraging the elliptical geometry of lesions. In\nthis paper, we present Gaussian Proposal Networks (GPNs), a novel extension to\nRPNs, to detect lesion bounding ellipses. Instead of directly regressing the\nrotation angle of the ellipse as the common practice, GPN represents bounding\nellipses as 2D Gaussian distributions on the image plain and minimizes the\nKullback-Leibler (KL) divergence between the proposed Gaussian and the ground\ntruth Gaussian for object localization. We show the KL divergence loss\napproximately incarnates the regression loss in the RPN framework when the\nrotation angle is 0. Experiments on the DeepLesion dataset show that GPN\nsignificantly outperforms RPN for lesion bounding ellipse detection thanks to\nlower localization error. GPN is open sourced at\nhttps://github.com/baidu-research/GPN\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 23:17:41 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Li", "Yi", ""]]}, {"id": "1902.09680", "submitter": "Zihao Wang", "authors": "Zihao W. Wang, Weixin Jiang, Kuan He, Boxin Shi, Aggelos Katsaggelos,\n  Oliver Cossairt", "title": "Event-driven Video Frame Synthesis", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Temporal Video Frame Synthesis (TVFS) aims at synthesizing novel frames at\ntimestamps different from existing frames, which has wide applications in video\ncodec, editing and analysis. In this paper, we propose a high framerate TVFS\nframework which takes hybrid input data from a low-speed frame-based sensor and\na high-speed event-based sensor. Compared to frame-based sensors, event-based\nsensors report brightness changes at very high speed, which may well provide\nuseful spatio-temoral information for high framerate TVFS. In our framework, we\nfirst introduce a differentiable forward model to approximate the physical\nsensing process, fusing the two different modes of data as well as unifying a\nvariety of TVFS tasks, i.e., interpolation, prediction and motion deblur. We\nleverage autodifferentiation which propagates the gradients of a loss defined\non the measured data back to the latent high framerate video. We show results\nwith better performance compared to state-of-the-art. Second, we develop a deep\nlearning-based strategy to enhance the results from the first step, which we\nrefer as a residual \"denoising\" process. Our trained \"denoiser\" is beyond\nGaussian denoising and shows properties such as contrast enhancement and motion\nawareness. We show that our framework is capable of handling challenging scenes\nincluding both fast motion and strong occlusions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 00:54:09 GMT"}, {"version": "v2", "created": "Sat, 6 Jul 2019 02:11:05 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Wang", "Zihao W.", ""], ["Jiang", "Weixin", ""], ["He", "Kuan", ""], ["Shi", "Boxin", ""], ["Katsaggelos", "Aggelos", ""], ["Cossairt", "Oliver", ""]]}, {"id": "1902.09690", "submitter": "Bin Song", "authors": "Xu Kang, Bin Song, Jie Guo, Xiaojiang Du and Mohsen Guizani", "title": "Self-Selective Correlation Ship Tracking Method for Smart Ocean System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, with the development of the marine industry, navigation\nenvironment becomes more complicated. Some artificial intelligence\ntechnologies, such as computer vision, can recognize, track and count the\nsailing ships to ensure the maritime security and facilitates the management\nfor Smart Ocean System. Aiming at the scaling problem and boundary effect\nproblem of traditional correlation filtering methods, we propose a\nself-selective correlation filtering method based on box regression (BRCF). The\nproposed method mainly include: 1) A self-selective model with negative samples\nmining method which effectively reduces the boundary effect in strengthening\nthe classification ability of classifier at the same time; 2) A bounding box\nregression method combined with a key points matching method for the scale\nprediction, leading to a fast and efficient calculation. The experimental\nresults show that the proposed method can effectively deal with the problem of\nship size changes and background interference. The success rates and precisions\nwere higher than Discriminative Scale Space Tracking (DSST) by over 8\npercentage points on the marine traffic dataset of our laboratory. In terms of\nprocessing speed, the proposed method is higher than DSST by nearly 22 Frames\nPer Second (FPS).\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 01:19:35 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Kang", "Xu", ""], ["Song", "Bin", ""], ["Guo", "Jie", ""], ["Du", "Xiaojiang", ""], ["Guizani", "Mohsen", ""]]}, {"id": "1902.09692", "submitter": "Shahriar Esmaeili", "authors": "Hassan Ataeian, Shahriar Esmaeili, Saeideh Roshanfekr, Neda Maleki\n  Khas, Ali Amiri, and Hossein Safari", "title": "QLMC-HD: Quasi Large Margin Classifier based on Hyperdisk", "comments": "12 pages, 1 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the area of data classification, the different classifiers have been\ndeveloped by their own strengths and weaknesses. Among these classifiers, we\npropose a method that is based on the maximum margin between two classes. One\nof the main challenges in this area is dealt with noisy data. In this paper,\nour aim is to optimize the method of large margin classifiers based on\nhyperdisk (LMC-HD) and combine it into a quasisupport vector data description\n(QSVDD) method. In the proposed method, the bounding hypersphere is calculated\nbased on the QSVDD method. So our convex class model is more robust compared\nwith the support vector machine (SVM) and less tight than LMC-HD. Large margin\nclassifiers aim to maximize the margin and minimizing the risk. Since our\nproposed method ignores the effect of outliers and noises, so this method has\nthe widest margin compared with other large margin classifiers. In the end, we\ncompare our proposed method with other popular large margin classifiers by the\nexperiments on a set of standard data which indicates our results are more\nefficient than the others\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 01:23:14 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 00:25:37 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 21:34:47 GMT"}, {"version": "v4", "created": "Sat, 29 Aug 2020 23:15:53 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Ataeian", "Hassan", ""], ["Esmaeili", "Shahriar", ""], ["Roshanfekr", "Saeideh", ""], ["Khas", "Neda Maleki", ""], ["Amiri", "Ali", ""], ["Safari", "Hossein", ""]]}, {"id": "1902.09705", "submitter": "Giovanni Saponaro", "authors": "Giovanni Saponaro, Lorenzo Jamone, Alexandre Bernardino, Giampiero\n  Salvi", "title": "Beyond the Self: Using Grounded Affordances to Interpret and Describe\n  Others' Actions", "comments": "code available at https://github.com/gsaponaro/tcds-gestures, IEEE\n  Transactions on Cognitive and Developmental Systems", "journal-ref": "IEEE Transactions on Cognitive and Developmental Systems, vol. 12,\n  no. 2, pp. 209-221, June 2020", "doi": "10.1109/TCDS.2018.2882140", "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a developmental approach that allows a robot to interpret and\ndescribe the actions of human agents by reusing previous experience. The robot\nfirst learns the association between words and object affordances by\nmanipulating the objects in its environment. It then uses this information to\nlearn a mapping between its own actions and those performed by a human in a\nshared environment. It finally fuses the information from these two models to\ninterpret and describe human actions in light of its own experience. In our\nexperiments, we show that the model can be used flexibly to do inference on\ndifferent aspects of the scene. We can predict the effects of an action on the\nbasis of object properties. We can revise the belief that a certain action\noccurred, given the observed effects of the human action. In an early action\nrecognition fashion, we can anticipate the effects when the action has only\nbeen partially observed. By estimating the probability of words given the\nevidence and feeding them into a pre-defined grammar, we can generate relevant\ndescriptions of the scene. We believe that this is a step towards providing\nrobots with the fundamental skills to engage in social collaboration with\nhumans.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 02:14:10 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Saponaro", "Giovanni", ""], ["Jamone", "Lorenzo", ""], ["Bernardino", "Alexandre", ""], ["Salvi", "Giampiero", ""]]}, {"id": "1902.09707", "submitter": "Qunliang Xing", "authors": "Qunliang Xing, Zhenyu Guan, Mai Xu, Ren Yang, Tie Liu, Zulin Wang", "title": "MFQE 2.0: A New Approach for Multi-frame Quality Enhancement on\n  Compressed Video", "comments": "Accepted to TPAMI in September, 2019. v6 updates: correct units in\n  Fig. 11; correct author info; delete bio photos. arXiv admin note: text\n  overlap with arXiv:1803.04680", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2944806", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years have witnessed great success in applying deep learning to\nenhance the quality of compressed image/video. The existing approaches mainly\nfocus on enhancing the quality of a single frame, not considering the\nsimilarity between consecutive frames. Since heavy fluctuation exists across\ncompressed video frames as investigated in this paper, frame similarity can be\nutilized for quality enhancement of low-quality frames given their neighboring\nhigh-quality frames. This task is Multi-Frame Quality Enhancement (MFQE).\nAccordingly, this paper proposes an MFQE approach for compressed video, as the\nfirst attempt in this direction. In our approach, we firstly develop a\nBidirectional Long Short-Term Memory (BiLSTM) based detector to locate Peak\nQuality Frames (PQFs) in compressed video. Then, a novel Multi-Frame\nConvolutional Neural Network (MF-CNN) is designed to enhance the quality of\ncompressed video, in which the non-PQF and its nearest two PQFs are the input.\nIn MF-CNN, motion between the non-PQF and PQFs is compensated by a motion\ncompensation subnet. Subsequently, a quality enhancement subnet fuses the\nnon-PQF and compensated PQFs, and then reduces the compression artifacts of the\nnon-PQF. Also, PQF quality is enhanced in the same way. Finally, experiments\nvalidate the effectiveness and generalization ability of our MFQE approach in\nadvancing the state-of-the-art quality enhancement of compressed video. The\ncode is available at https://github.com/RyanXingQL/MFQEv2.0.git.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 02:35:55 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 15:52:25 GMT"}, {"version": "v3", "created": "Fri, 4 Oct 2019 05:20:30 GMT"}, {"version": "v4", "created": "Mon, 25 Nov 2019 08:53:17 GMT"}, {"version": "v5", "created": "Mon, 6 Jul 2020 10:03:35 GMT"}, {"version": "v6", "created": "Sat, 3 Oct 2020 13:17:46 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Xing", "Qunliang", ""], ["Guan", "Zhenyu", ""], ["Xu", "Mai", ""], ["Yang", "Ren", ""], ["Liu", "Tie", ""], ["Wang", "Zulin", ""]]}, {"id": "1902.09720", "submitter": "Thibaut Durand", "authors": "Thibaut Durand, Nazanin Mehrasa, Greg Mori", "title": "Learning a Deep ConvNet for Multi-label Classification with Partial\n  Labels", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep ConvNets have shown great performance for single-label image\nclassification (e.g. ImageNet), but it is necessary to move beyond the\nsingle-label classification task because pictures of everyday life are\ninherently multi-label. Multi-label classification is a more difficult task\nthan single-label classification because both the input images and output label\nspaces are more complex. Furthermore, collecting clean multi-label annotations\nis more difficult to scale-up than single-label annotations. To reduce the\nannotation cost, we propose to train a model with partial labels i.e. only some\nlabels are known per image. We first empirically compare different labeling\nstrategies to show the potential for using partial labels on multi-label\ndatasets. Then to learn with partial labels, we introduce a new classification\nloss that exploits the proportion of known labels per example. Our approach\nallows the use of the same training settings as when learning with all the\nannotations. We further explore several curriculum learning based strategies to\npredict missing labels. Experiments are performed on three large-scale\nmulti-label datasets: MS COCO, NUS-WIDE and Open Images.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 03:52:16 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Durand", "Thibaut", ""], ["Mehrasa", "Nazanin", ""], ["Mori", "Greg", ""]]}, {"id": "1902.09727", "submitter": "Rui Zhang", "authors": "Rui Zhang, Tomas Pfister, Jia Li", "title": "Harmonic Unpaired Image-to-image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent direction of unpaired image-to-image translation is on one hand\nvery exciting as it alleviates the big burden in obtaining label-intensive\npixel-to-pixel supervision, but it is on the other hand not fully satisfactory\ndue to the presence of artifacts and degenerated transformations. In this\npaper, we take a manifold view of the problem by introducing a smoothness term\nover the sample graph to attain harmonic functions to enforce consistent\nmappings during the translation. We develop HarmonicGAN to learn bi-directional\ntranslations between the source and the target domains. With the help of\nsimilarity-consistency, the inherent self-consistency property of samples can\nbe maintained. Distance metrics defined on two types of features including\nhistogram and CNN are exploited. Under an identical problem setting as\nCycleGAN, without additional manual inputs and only at a small training-time\ncost, HarmonicGAN demonstrates a significant qualitative and quantitative\nimprovement over the state of the art, as well as improved interpretability. We\nshow experimental results in a number of applications including medical\nimaging, object transfiguration, and semantic labeling. We outperform the\ncompeting methods in all tasks, and for a medical imaging task in particular\nour method turns CycleGAN from a failure to a success, halving the mean-squared\nerror, and generating images that radiologists prefer over competing methods in\n95% of cases.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 04:49:56 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Zhang", "Rui", ""], ["Pfister", "Tomas", ""], ["Li", "Jia", ""]]}, {"id": "1902.09738", "submitter": "Peiliang Li", "authors": "Peiliang Li, Xiaozhi Chen, and Shaojie Shen", "title": "Stereo R-CNN based 3D Object Detection for Autonomous Driving", "comments": "Accepted by cvpr2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a 3D object detection method for autonomous driving by fully\nexploiting the sparse and dense, semantic and geometry information in stereo\nimagery. Our method, called Stereo R-CNN, extends Faster R-CNN for stereo\ninputs to simultaneously detect and associate object in left and right images.\nWe add extra branches after stereo Region Proposal Network (RPN) to predict\nsparse keypoints, viewpoints, and object dimensions, which are combined with 2D\nleft-right boxes to calculate a coarse 3D object bounding box. We then recover\nthe accurate 3D bounding box by a region-based photometric alignment using left\nand right RoIs. Our method does not require depth input and 3D position\nsupervision, however, outperforms all existing fully supervised image-based\nmethods. Experiments on the challenging KITTI dataset show that our method\noutperforms the state-of-the-art stereo-based method by around 30% AP on both\n3D detection and 3D localization tasks. Code has been released at\nhttps://github.com/HKUST-Aerial-Robotics/Stereo-RCNN.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 05:19:12 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 10:20:11 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Li", "Peiliang", ""], ["Chen", "Xiaozhi", ""], ["Shen", "Shaojie", ""]]}, {"id": "1902.09774", "submitter": "Dalu Guo Mr.", "authors": "Dalu Guo, Chang Xu, Dacheng Tao", "title": "Image-Question-Answer Synergistic Network for Visual Dialog", "comments": "Accepted by cvpr2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The image, question (combined with the history for de-referencing), and the\ncorresponding answer are three vital components of visual dialog. Classical\nvisual dialog systems integrate the image, question, and history to search for\nor generate the best matched answer, and so, this approach significantly\nignores the role of the answer. In this paper, we devise a novel\nimage-question-answer synergistic network to value the role of the answer for\nprecise visual dialog. We extend the traditional one-stage solution to a\ntwo-stage solution. In the first stage, candidate answers are coarsely scored\naccording to their relevance to the image and question pair. Afterward, in the\nsecond stage, answers with high probability of being correct are re-ranked by\nsynergizing with image and question. On the Visual Dialog v1.0 dataset, the\nproposed synergistic network boosts the discriminative visual dialog model to\nachieve a new state-of-the-art of 57.88\\% normalized discounted cumulative\ngain. A generative visual dialog model equipped with the proposed technique\nalso shows promising improvements.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 07:30:43 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Guo", "Dalu", ""], ["Xu", "Chang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1902.09777", "submitter": "Jia Zheng", "authors": "Zehao Yu and Jia Zheng and Dongze Lian and Zihan Zhou and Shenghua Gao", "title": "Single-Image Piece-wise Planar 3D Reconstruction via Associative\n  Embedding", "comments": "Minor Revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-image piece-wise planar 3D reconstruction aims to simultaneously\nsegment plane instances and recover 3D plane parameters from an image. Most\nrecent approaches leverage convolutional neural networks (CNNs) and achieve\npromising results. However, these methods are limited to detecting a fixed\nnumber of planes with certain learned order. To tackle this problem, we propose\na novel two-stage method based on associative embedding, inspired by its recent\nsuccess in instance segmentation. In the first stage, we train a CNN to map\neach pixel to an embedding space where pixels from the same plane instance have\nsimilar embeddings. Then, the plane instances are obtained by grouping the\nembedding vectors in planar regions via an efficient mean shift clustering\nalgorithm. In the second stage, we estimate the parameter for each plane\ninstance by considering both pixel-level and instance-level consistencies. With\nthe proposed method, we are able to detect an arbitrary number of planes.\nExtensive experiments on public datasets validate the effectiveness and\nefficiency of our method. Furthermore, our method runs at 30 fps at the testing\ntime, thus could facilitate many real-time applications such as visual SLAM and\nhuman-robot interaction. Code is available at\nhttps://github.com/svip-lab/PlanarReconstruction.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 07:39:11 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 06:38:54 GMT"}, {"version": "v3", "created": "Wed, 24 Apr 2019 11:47:13 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Yu", "Zehao", ""], ["Zheng", "Jia", ""], ["Lian", "Dongze", ""], ["Zhou", "Zihan", ""], ["Gao", "Shenghua", ""]]}, {"id": "1902.09782", "submitter": "Qingyan Duan", "authors": "Qingyan Duan and Lei Zhang", "title": "BoostGAN for Occlusive Profile Face Frontalization and Recognition", "comments": "9 pages, 7 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many facts affecting human face recognition, such as pose,\nocclusion, illumination, age, etc. First and foremost are large pose and\nocclusion problems, which can even result in more than 10% performance\ndegradation. Pose-invariant feature representation and face frontalization with\ngenerative adversarial networks (GAN) have been widely used to solve the pose\nproblem. However, the synthesis and recognition of occlusive but profile faces\nis still an uninvestigated problem. To address this issue, in this paper, we\naim to contribute an effective solution on how to recognize occlusive but\nprofile faces, even with facial keypoint region (e.g. eyes, nose, etc.)\ncorrupted. Specifically, we propose a boosting Generative Adversarial Network\n(BoostGAN) for de-occlusion, frontalization, and recognition of faces. Upon the\nassumption that facial occlusion is partial and incomplete, multiple patch\noccluded images are fed as inputs for knowledge boosting, such as identity and\ntexture information. A new aggregation structure composed of a deep GAN for\ncoarse face synthesis and a shallow boosting net for fine face generation is\nfurther designed. Exhaustive experiments demonstrate that the proposed approach\nnot only presents clear perceptual photo-realistic results but also shows\nstate-of-the-art recognition performance for occlusive but profile faces.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 07:59:47 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Duan", "Qingyan", ""], ["Zhang", "Lei", ""]]}, {"id": "1902.09795", "submitter": "Cairong Zhang", "authors": "Guijin Wang, Cairong Zhang, Xinghao Chen, Xiangyang Ji, Jing-Hao Xue,\n  Hang Wang", "title": "Bi-stream Pose Guided Region Ensemble Network for Fingertip Localization\n  from Stereo Images", "comments": "Cairong Zhang and Xinghao Chen are equally contributed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In human-computer interaction, it is important to accurately estimate the\nhand pose especially fingertips. However, traditional approaches for fingertip\nlocalization mainly rely on depth images and thus suffer considerably from the\nnoise and missing values. Instead of depth images, stereo images can also\nprovide 3D information of hands and promote 3D hand pose estimation. There are\nnevertheless limitations on the dataset size, global viewpoints, hand\narticulations and hand shapes in the publicly available stereo-based hand pose\ndatasets. To mitigate these limitations and promote further research on hand\npose estimation from stereo images, we propose a new large-scale binocular hand\npose dataset called THU-Bi-Hand, offering a new perspective for fingertip\nlocalization. In the THU-Bi-Hand dataset, there are 447k pairs of stereo images\nof different hand shapes from 10 subjects with accurate 3D location annotations\nof the wrist and five fingertips. Captured with minimal restriction on the\nrange of hand motion, the dataset covers large global viewpoint space and hand\narticulation space. To better present the performance of fingertip localization\non THU-Bi-Hand, we propose a novel scheme termed Bi-stream Pose Guided Region\nEnsemble Network (Bi-Pose-REN). It extracts more representative feature regions\naround joint points in the feature maps under the guidance of the previously\nestimated pose. The feature regions are integrated hierarchically according to\nthe topology of hand joints to regress the refined hand pose. Bi-Pose-REN and\nseveral existing methods are evaluated on THU-Bi-Hand so that benchmarks are\nprovided for further research. Experimental results show that our new method\nhas achieved the best performance on THU-Bi-Hand.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 08:28:53 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Wang", "Guijin", ""], ["Zhang", "Cairong", ""], ["Chen", "Xinghao", ""], ["Ji", "Xiangyang", ""], ["Xue", "Jing-Hao", ""], ["Wang", "Hang", ""]]}, {"id": "1902.09809", "submitter": "Zhendong Zhang", "authors": "Zhendong Zhang and Cheolkon Jung", "title": "Recurrent Convolution for Compact and Cost-Adjustable Neural Networks:\n  An Empirical Study", "comments": "8 pages; preprint; work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recurrent convolution (RC) shares the same convolutional kernels and unrolls\nthem multiple steps, which is originally proposed to model time-space signals.\nWe argue that RC can be viewed as a model compression strategy for deep\nconvolutional neural networks. RC reduces the redundancy across layers.\nHowever, the performance of an RC network is not satisfactory if we directly\nunroll the same kernels multiple steps. We propose a simple yet effective\nvariant which improves the RC networks: the batch normalization layers of an RC\nmodule are learned independently (not shared) for different unrolling steps.\nMoreover, we verify that RC can perform cost-adjustable inference which is\nachieved by varying its unrolling steps. We learn double independent BN layers\nfor cost-adjustable RC networks, i.e. independent w.r.t both the unrolling\nsteps of current cell and upstream cell. We provide insights on why the\nproposed method works successfully. Experiments on both image classification\nand image denoise demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 09:09:24 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Zhang", "Zhendong", ""], ["Jung", "Cheolkon", ""]]}, {"id": "1902.09811", "submitter": "Leonid Karlinsky", "authors": "Amit Alfassy, Leonid Karlinsky, Amit Aides, Joseph Shtok, Sivan\n  Harary, Rogerio Feris, Raja Giryes, Alex M. Bronstein", "title": "LaSO: Label-Set Operations networks for multi-label few-shot learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Example synthesis is one of the leading methods to tackle the problem of\nfew-shot learning, where only a small number of samples per class are\navailable. However, current synthesis approaches only address the scenario of a\nsingle category label per image. In this work, we propose a novel technique for\nsynthesizing samples with multiple labels for the (yet unhandled) multi-label\nfew-shot classification scenario. We propose to combine pairs of given examples\nin feature space, so that the resulting synthesized feature vectors will\ncorrespond to examples whose label sets are obtained through certain set\noperations on the label sets of the corresponding input pairs. Thus, our method\nis capable of producing a sample containing the intersection, union or\nset-difference of labels present in two input samples. As we show, these set\noperations generalize to labels unseen during training. This enables performing\naugmentation on examples of novel categories, thus, facilitating multi-label\nfew-shot classifier learning. We conduct numerous experiments showing promising\nresults for the label-set manipulation capabilities of the proposed approach,\nboth directly (using the classification and retrieval metrics), and in the\ncontext of performing data augmentation for multi-label few-shot learning. We\npropose a benchmark for this new and challenging task and show that our method\ncompares favorably to all the common baselines.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 09:12:09 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Alfassy", "Amit", ""], ["Karlinsky", "Leonid", ""], ["Aides", "Amit", ""], ["Shtok", "Joseph", ""], ["Harary", "Sivan", ""], ["Feris", "Rogerio", ""], ["Giryes", "Raja", ""], ["Bronstein", "Alex M.", ""]]}, {"id": "1902.09817", "submitter": "Ziyao Li", "authors": "Ziyao Li, Liang Zhang, Guojie Song", "title": "GCN-LASE: Towards Adequately Incorporating Link Attributes in Graph\n  Convolutional Networks", "comments": "IJCAI2019 Accepted Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) have proved to be a most powerful\narchitecture in aggregating local neighborhood information for individual graph\nnodes. Low-rank proximities and node features are successfully leveraged in\nexisting GCNs, however, attributes that graph links may carry are commonly\nignored, as almost all of these models simplify graph links into binary or\nscalar values describing node connectedness. In our paper instead, links are\nreverted to hypostatic relationships between entities with descriptional\nattributes. We propose GCN-LASE (GCN with Link Attributes and Sampling\nEstimation), a novel GCN model taking both node and link attributes as inputs.\nTo adequately captures the interactions between link and node attributes, their\ntensor product is used as neighbor features, based on which we define several\ngraph kernels and further develop according architectures for LASE. Besides, to\naccelerate the training process, the sum of features in entire neighborhoods\nare estimated through Monte Carlo method, with novel sampling strategies\ndesigned for LASE to minimize the estimation variance. Our experiments show\nthat LASE outperforms strong baselines over various graph datasets, and further\nexperiments corroborate the informativeness of link attributes and our model's\nability of adequately leveraging them.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 09:21:27 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 15:27:56 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Li", "Ziyao", ""], ["Zhang", "Liang", ""], ["Song", "Guojie", ""]]}, {"id": "1902.09818", "submitter": "Heming Zhang", "authors": "Heming Zhang, Shalini Ghosh, Larry Heck, Stephen Walsh, Junting Zhang,\n  Jie Zhang, C.-C. Jay Kuo", "title": "Generative Visual Dialogue System via Adaptive Reasoning and Weighted\n  Likelihood Estimation", "comments": "IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key challenge of generative Visual Dialogue (VD) systems is to respond to\nhuman queries with informative answers in natural and contiguous conversation\nflow. Traditional Maximum Likelihood Estimation (MLE)-based methods only learn\nfrom positive responses but ignore the negative responses, and consequently\ntend to yield safe or generic responses. To address this issue, we propose a\nnovel training scheme in conjunction with weighted likelihood estimation (WLE)\nmethod. Furthermore, an adaptive multi-modal reasoning module is designed, to\naccommodate various dialogue scenarios automatically and select relevant\ninformation accordingly. The experimental results on the VisDial benchmark\ndemonstrate the superiority of our proposed algorithm over other\nstate-of-the-art approaches, with an improvement of 5.81% on recall@10.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 09:23:28 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 01:13:44 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Zhang", "Heming", ""], ["Ghosh", "Shalini", ""], ["Heck", "Larry", ""], ["Walsh", "Stephen", ""], ["Zhang", "Junting", ""], ["Zhang", "Jie", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1902.09839", "submitter": "Stefan Milz", "authors": "Maximilian P\\\"opperl, Raghavendra Gulagundi, Senthil Yogamani, Stefan\n  Milz", "title": "Capsule Neural Network based Height Classification using Low-Cost\n  Automotive Ultrasonic Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High performance ultrasonic sensor hardware is mainly used in medical\napplications. Although, the development in automotive scenarios is towards\nautonomous driving, the ultrasonic sensor hardware still stays low-cost and\nlow-performance, respectively. To overcome the strict hardware limitations, we\npropose to use capsule neural networks. By the high classification capability\nof this network architecture, we can achieve outstanding results for performing\na detailed height analysis of detected objects. We apply a novel resorting and\nreshaping method to feed the neural network with ultrasonic data. This\nincreases classification performance and computation speed. We tested the\napproach under different environmental conditions to verify that the proposed\nmethod is working independent of external parameters that is needed for\nautonomous driving.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 10:15:32 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["P\u00f6pperl", "Maximilian", ""], ["Gulagundi", "Raghavendra", ""], ["Yogamani", "Senthil", ""], ["Milz", "Stefan", ""]]}, {"id": "1902.09842", "submitter": "Stefan Milz", "authors": "Maximilian P\\\"opperl, Raghavendra Gulagundi, Senthil Yogamani, Stefan\n  Milz", "title": "Realistic Ultrasonic Environment Simulation Using Conditional Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, realistic data augmentation using neural networks especially\ngenerative neural networks (GAN) has achieved outstanding results. The\ncommunities main research focus is visual image processing. However, automotive\ncars and robots are equipped with a large suite of sensors to achieve a high\nredundancy. In addition to others, ultrasonic sensors are often used due to\ntheir low-costs and reliable near field distance measuring capabilities. Hence,\nPattern recognition needs to be applied to ultrasonic signals as well. Machine\nLearning requires extensive data sets and those measurements are\ntime-consuming, expensive and not flexible to hardware and environmental\nchanges. On the other hand, there exists no method to simulate those signals\ndeterministically. We present a novel approach for synthetic ultrasonic signal\nsimulation using conditional GANs (cGANs). For the best of our knowledge, we\npresent the first realistic data augmentation for automotive ultrasonics. The\nperformance of cGANs allows us to bring the realistic environment simulation to\na new level. By using setup and environmental parameters as condition, the\nproposed approach is flexible to external influences. Due to the low complexity\nand time effort for data generation, we outperform other simulation algorithms,\nsuch as finite element method. We verify the outstanding accuracy and realism\nof our method by applying a detailed statistical analysis and comparing the\ngenerated data to an extensive amount of measured signals.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 10:22:25 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["P\u00f6pperl", "Maximilian", ""], ["Gulagundi", "Raghavendra", ""], ["Yogamani", "Senthil", ""], ["Milz", "Stefan", ""]]}, {"id": "1902.09852", "submitter": "Xinlong Wang", "authors": "Xinlong Wang, Shu Liu, Xiaoyong Shen, Chunhua Shen, Jiaya Jia", "title": "Associatively Segmenting Instances and Semantics in Point Clouds", "comments": "Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A 3D point cloud describes the real scene precisely and intuitively.To date\nhow to segment diversified elements in such an informative 3D scene is rarely\ndiscussed. In this paper, we first introduce a simple and flexible framework to\nsegment instances and semantics in point clouds simultaneously. Then, we\npropose two approaches which make the two tasks take advantage of each other,\nleading to a win-win situation. Specifically, we make instance segmentation\nbenefit from semantic segmentation through learning semantic-aware point-level\ninstance embedding. Meanwhile, semantic features of the points belonging to the\nsame instance are fused together to make more accurate per-point semantic\npredictions. Our method largely outperforms the state-of-the-art method in 3D\ninstance segmentation along with a significant improvement in 3D semantic\nsegmentation. Code has been made available at:\nhttps://github.com/WXinlong/ASIS.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 10:38:26 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 07:04:18 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Wang", "Xinlong", ""], ["Liu", "Shu", ""], ["Shen", "Xiaoyong", ""], ["Shen", "Chunhua", ""], ["Jia", "Jiaya", ""]]}, {"id": "1902.09856", "submitter": "Changhee Han", "authors": "Changhee Han, Kohei Murao, Tomoyuki Noguchi, Yusuke Kawata, Fumiya\n  Uchiyama, Leonardo Rundo, Hideki Nakayama, Shin'ichi Satoh", "title": "Learning More with Less: Conditional PGGAN-based Data Augmentation for\n  Brain Metastases Detection Using Highly-Rough Annotation on MR Images", "comments": "9 pages, 7 figures, accepted to CIKM 2019 (acceptance rate: 19%)", "journal-ref": null, "doi": "10.1145/3357384.3357890", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate Computer-Assisted Diagnosis, associated with proper data wrangling,\ncan alleviate the risk of overlooking the diagnosis in a clinical environment.\nTowards this, as a Data Augmentation (DA) technique, Generative Adversarial\nNetworks (GANs) can synthesize additional training data to handle the\nsmall/fragmented medical imaging datasets collected from various scanners;\nthose images are realistic but completely different from the original ones,\nfilling the data lack in the real image distribution. However, we cannot easily\nuse them to locate disease areas, considering expert physicians' expensive\nannotation cost. Therefore, this paper proposes Conditional Progressive Growing\nof GANs (CPGGANs), incorporating highly-rough bounding box conditions\nincrementally into PGGANs to place brain metastases at desired positions/sizes\non 256 X 256 Magnetic Resonance (MR) images, for Convolutional Neural\nNetwork-based tumor detection; this first GAN-based medical DA using automatic\nbounding box annotation improves the training robustness. The results show that\nCPGGAN-based DA can boost 10% sensitivity in diagnosis with clinically\nacceptable additional False Positives. Surprisingly, further tumor realism,\nachieved with additional normal brain MR images for CPGGAN training, does not\ncontribute to detection performance, while even three physicians cannot\naccurately distinguish them from the real ones in Visual Turing Test.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 10:46:53 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 15:33:12 GMT"}, {"version": "v3", "created": "Mon, 27 May 2019 15:05:36 GMT"}, {"version": "v4", "created": "Wed, 29 May 2019 12:44:45 GMT"}, {"version": "v5", "created": "Thu, 22 Aug 2019 11:01:29 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Han", "Changhee", ""], ["Murao", "Kohei", ""], ["Noguchi", "Tomoyuki", ""], ["Kawata", "Yusuke", ""], ["Uchiyama", "Fumiya", ""], ["Rundo", "Leonardo", ""], ["Nakayama", "Hideki", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "1902.09863", "submitter": "Benjamin Berkels", "authors": "Niklas Mevenkamp, Benjamin Berkels", "title": "Variational Multi-Phase Segmentation using High-Dimensional Local\n  Features", "comments": null, "journal-ref": "2016 IEEE Winter Conference on Applications of Computer Vision\n  (WACV)", "doi": "10.1109/WACV.2016.7477729", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for multi-phase segmentation of images based on\nhigh-dimensional local feature vectors. While the method was developed for the\nsegmentation of extremely noisy crystal images based on localized Fourier\ntransforms, the resulting framework is not tied to specific feature\ndescriptors. For instance, using local spectral histograms as features, it\nallows for robust texture segmentation. The segmentation itself is based on the\nmulti-phase Mumford-Shah model. Initializing the high-dimensional mean features\ndirectly is computationally too demanding and ill-posed in practice. This is\nresolved by projecting the features onto a low-dimensional space using\nprinciple component analysis. The resulting objective functional is minimized\nusing a convexification and the Chambolle-Pock algorithm. Numerical results are\npresented, illustrating that the algorithm is very competitive in texture\nsegmentation with state-of-the-art performance on the Prague benchmark and\nprovides new possibilities in crystal segmentation, being robust to extreme\nnoise and requiring no prior knowledge of the crystal structure.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 11:19:20 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Mevenkamp", "Niklas", ""], ["Berkels", "Benjamin", ""]]}, {"id": "1902.09868", "submitter": "Bastian Wandt", "authors": "Bastian Wandt and Bodo Rosenhahn", "title": "RepNet: Weakly Supervised Training of an Adversarial Reprojection\n  Network for 3D Human Pose Estimation", "comments": "accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of 3D human pose estimation from single\nimages. While for a long time human skeletons were parameterized and fitted to\nthe observation by satisfying a reprojection error, nowadays researchers\ndirectly use neural networks to infer the 3D pose from the observations.\nHowever, most of these approaches ignore the fact that a reprojection\nconstraint has to be satisfied and are sensitive to overfitting. We tackle the\noverfitting problem by ignoring 2D to 3D correspondences. This efficiently\navoids a simple memorization of the training data and allows for a weakly\nsupervised training. One part of the proposed reprojection network (RepNet)\nlearns a mapping from a distribution of 2D poses to a distribution of 3D poses\nusing an adversarial training approach. Another part of the network estimates\nthe camera. This allows for the definition of a network layer that performs the\nreprojection of the estimated 3D pose back to 2D which results in a\nreprojection loss function. Our experiments show that RepNet generalizes well\nto unknown data and outperforms state-of-the-art methods when applied to unseen\ndata. Moreover, our implementation runs in real-time on a standard desktop PC.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 11:23:54 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 14:20:04 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Wandt", "Bastian", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "1902.09878", "submitter": "Cuiyin Liu", "authors": "Nanyu Li and Cuiyin Liu", "title": "MC-ISTA-Net: Adaptive Measurement and Initialization and Channel\n  Attention Optimization inspired Neural Network for Compressive Sensing", "comments": "We request withdraw this paper for the reasons stated as following.\n  The paper is not published in any journal. Some errors and insufficient\n  experiments are found in the recently work. The rectified work and relevant\n  supplementary experiments will be done for this paper in the future. After\n  these, we will submit the new version of this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The optimization inspired network can bridge convex optimization and neural\nnetworks in Compressive Sensing (CS) reconstruction of natural image, like\nISTA-Net+, which mapping optimization algorithm: iterative\nshrinkage-thresholding algorithm (ISTA) into network. However, measurement\nmatrix and input initialization are still hand-crafted, and multi-channel\nfeature map contain information at different frequencies, which is treated\nequally across channels, hindering the ability of CS reconstruction in\noptimization-inspired networks. In order to solve the above problems, we\nproposed MC-ISTA-Net\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 12:02:39 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 23:02:03 GMT"}, {"version": "v3", "created": "Sun, 11 Aug 2019 23:30:00 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Li", "Nanyu", ""], ["Liu", "Cuiyin", ""]]}, {"id": "1902.09887", "submitter": "Qianyi Wu", "authors": "Zi-Hang Jiang, Qianyi Wu, Keyu Chen and Juyong Zhang", "title": "Disentangled Representation Learning for 3D Face Shape", "comments": "15 pages, 8 figures. CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel strategy to design disentangled 3D face\nshape representation. Specifically, a given 3D face shape is decomposed into\nidentity part and expression part, which are both encoded and decoded in a\nnonlinear way. To solve this problem, we propose an attribute decomposition\nframework for 3D face mesh. To better represent face shapes which are usually\nnonlinear deformed between each other, the face shapes are represented by a\nvertex based deformation representation rather than Euclidean coordinates. The\nexperimental results demonstrate that our method has better performance than\nexisting methods on decomposing the identity and expression parts. Moreover,\nmore natural expression transfer results can be achieved with our method than\nexisting methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 12:22:18 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 06:38:24 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Jiang", "Zi-Hang", ""], ["Wu", "Qianyi", ""], ["Chen", "Keyu", ""], ["Zhang", "Juyong", ""]]}, {"id": "1902.09904", "submitter": "Yechong Huang", "authors": "Yechong Huang, Jiahang Xu, Yuncheng Zhou, Tong Tong, Xiahai Zhuang,\n  the Alzheimer's Disease Neuroimaging Initiative", "title": "Diagnosis of Alzheimer's Disease via Multi-modality 3D Convolutional\n  Neural Network", "comments": "21 pages, 5 figures, 9 tables", "journal-ref": "https://www.frontiersin.org/articles/10.3389/fnins.2019.00509/full", "doi": "10.3389/fnins.2019.00509", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's Disease (AD) is one of the most concerned neurodegenerative\ndiseases. In the last decade, studies on AD diagnosis attached great\nsignificance to artificial intelligence (AI)-based diagnostic algorithms. Among\nthe diverse modality imaging data, T1-weighted MRI and 18F-FDGPET are widely\nresearched for this task. In this paper, we propose a novel convolutional\nneural network (CNN) to fuse the multi-modality information including T1-MRI\nand FDG-PDT images around the hippocampal area for the diagnosis of AD.\nDifferent from the traditional machine learning algorithms, this method does\nnot require manually extracted features, and utilizes the stateof-art 3D\nimage-processing CNNs to learn features for the diagnosis and prognosis of AD.\nTo validate the performance of the proposed network, we trained the classifier\nwith paired T1-MRI and FDG-PET images using the ADNI datasets, including 731\nNormal (NL) subjects, 647 AD subjects, 441 stable MCI (sMCI) subjects and 326\nprogressive MCI (pMCI) subjects. We obtained the maximal accuracies of 90.10%\nfor NL/AD task, 87.46% for NL/pMCI task, and 76.90% for sMCI/pMCI task. The\nproposed framework yields comparative results against state-of-the-art\napproaches. Moreover, the experimental results have demonstrated that (1)\nsegmentation is not a prerequisite by using CNN, (2) the hippocampal area\nprovides enough information to give a reference to AD diagnosis. Keywords:\nAlzheimer's Disease, Multi-modality, Image Classification, CNN, Deep Learning,\nHippocampal\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 13:01:44 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Huang", "Yechong", ""], ["Xu", "Jiahang", ""], ["Zhou", "Yuncheng", ""], ["Tong", "Tong", ""], ["Zhuang", "Xiahai", ""], ["Initiative", "the Alzheimer's Disease Neuroimaging", ""]]}, {"id": "1902.09907", "submitter": "Haofei Xu", "authors": "Haofei Xu, Jianmin Zheng, Jianfei Cai and Juyong Zhang", "title": "Region Deformer Networks for Unsupervised Depth Estimation from\n  Unconstrained Monocular Videos", "comments": "IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While learning based depth estimation from images/videos has achieved\nsubstantial progress, there still exist intrinsic limitations. Supervised\nmethods are limited by a small amount of ground truth or labeled data and\nunsupervised methods for monocular videos are mostly based on the static scene\nassumption, not performing well on real world scenarios with the presence of\ndynamic objects. In this paper, we propose a new learning based method\nconsisting of DepthNet, PoseNet and Region Deformer Networks (RDN) to estimate\ndepth from unconstrained monocular videos without ground truth supervision. The\ncore contribution lies in RDN for proper handling of rigid and non-rigid\nmotions of various objects such as rigidly moving cars and deformable humans.\nIn particular, a deformation based motion representation is proposed to model\nindividual object motion on 2D images. This representation enables our method\nto be applicable to diverse unconstrained monocular videos. Our method can not\nonly achieve the state-of-the-art results on standard benchmarks KITTI and\nCityscapes, but also show promising results on a crowded pedestrian tracking\ndataset, which demonstrates the effectiveness of the deformation based motion\nrepresentation. Code and trained models are available at\nhttps://github.com/haofeixu/rdn4depth.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 13:03:15 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 12:24:00 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Xu", "Haofei", ""], ["Zheng", "Jianmin", ""], ["Cai", "Jianfei", ""], ["Zhang", "Juyong", ""]]}, {"id": "1902.09928", "submitter": "Ke Yang", "authors": "Ke Yang, Peng Qiao, Dongsheng Li, Yong Dou", "title": "IF-TTN: Information Fused Temporal Transformation Network for Video\n  Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective spatiotemporal feature representation is crucial to the video-based\naction recognition task. Focusing on discriminate spatiotemporal feature\nlearning, we propose Information Fused Temporal Transformation Network (IF-TTN)\nfor action recognition on top of popular Temporal Segment Network (TSN)\nframework. In the network, Information Fusion Module (IFM) is designed to fuse\nthe appearance and motion features at multiple ConvNet levels for each video\nsnippet, forming a short-term video descriptor. With fused features as inputs,\nTemporal Transformation Networks (TTN) are employed to model middle-term\ntemporal transformation between the neighboring snippets following a sequential\norder. As TSN itself depicts long-term temporal structure by segmental\nconsensus, the proposed network comprehensively considers multiple granularity\ntemporal features. Our IF-TTN achieves the state-of-the-art results on two most\npopular action recognition datasets: UCF101 and HMDB51. Empirical investigation\nreveals that our architecture is robust to the input motion map quality.\nReplacing optical flow with the motion vectors from compressed video stream,\nthe performance is still comparable to the flow-based methods while the testing\nspeed is 10x faster.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 13:44:08 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 16:35:39 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Yang", "Ke", ""], ["Qiao", "Peng", ""], ["Li", "Dongsheng", ""], ["Dou", "Yong", ""]]}, {"id": "1902.09941", "submitter": "Jian Zhang", "authors": "Jian Zhang, Runsheng Zhang, Yaping Huang and Qi Zou", "title": "Unsupervised Part Mining for Fine-grained Image Classification", "comments": "10 pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained image classification remains challenging due to the large\nintra-class variance and small inter-class variance. Since the subtle visual\ndifferences are only in local regions of discriminative parts among\nsubcategories, part localization is a key issue for fine-grained image\nclassification. Most existing approaches localize object or parts in an image\nwith object or part annotations, which are expensive and labor-consuming. To\ntackle this issue, we propose a fully unsupervised part mining (UPM) approach\nto localize the discriminative parts without even image-level annotations,\nwhich largely improves the fine-grained classification performance. We first\nutilize pattern mining techniques to discover frequent patterns, i.e.,\nco-occurrence highlighted regions, in the feature maps extracted from a\npre-trained convolutional neural network (CNN) model. Inspired by the fact that\nthese relevant meaningful patterns typically hold appearance and spatial\nconsistency, we then cluster the mined regions to obtain the cluster centers\nand the discriminative parts surrounding the cluster centers are generated.\nImportantly, any annotations and sophisticated training procedures are not used\nin our proposed part localization approach. Finally, a multi-stream\nclassification network is built for aggregating the original, object-level and\npart-level features simultaneously. Compared with other state-of-the-art\napproaches, our UPM approach achieves the competitive performance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 14:04:58 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Zhang", "Jian", ""], ["Zhang", "Runsheng", ""], ["Huang", "Yaping", ""], ["Zou", "Qi", ""]]}, {"id": "1902.09967", "submitter": "Stefan Hinterstoisser", "authors": "Stefan Hinterstoisser, Olivier Pauly, Hauke Heibel, Martina Marek,\n  Martin Bokeloh", "title": "An Annotation Saved is an Annotation Earned: Using Fully Synthetic\n  Training for Object Instance Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods typically require vast amounts of training data to\nreach their full potential. While some publicly available datasets exists,\ndomain specific data always needs to be collected and manually labeled, an\nexpensive, time consuming and error prone process. Training with synthetic data\nis therefore very lucrative, as dataset creation and labeling comes for free.\nWe propose a novel method for creating purely synthetic training data for\nobject detection. We leverage a large dataset of 3D background models and\ndensely render them using full domain randomization. This yields background\nimages with realistic shapes and texture on top of which we render the objects\nof interest. During training, the data generation process follows a curriculum\nstrategy guaranteeing that all foreground models are presented to the network\nequally under all possible poses and conditions with increasing complexity. As\na result, we entirely control the underlying statistics and we create optimal\ntraining samples at every stage of training. Using a set of 64 retail objects,\nwe demonstrate that our simple approach enables the training of detectors that\noutperform models trained with real data on a challenging evaluation dataset.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 14:36:35 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Hinterstoisser", "Stefan", ""], ["Pauly", "Olivier", ""], ["Heibel", "Hauke", ""], ["Marek", "Martina", ""], ["Bokeloh", "Martin", ""]]}, {"id": "1902.09968", "submitter": "Runsheng Zhang", "authors": "Runsheng Zhang, Yaping Huang, Mengyang Pu, Jian Zhang, Qingji Guan, Qi\n  Zou, Haibin Ling", "title": "Object Discovery From a Single Unlabeled Image by Mining Frequent\n  Itemset With Multi-scale Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TThe goal of our work is to discover dominant objects in a very general\nsetting where only a single unlabeled image is given. This is far more\nchallenge than typical co-localization or weakly-supervised localization tasks.\nTo tackle this problem, we propose a simple but effective pattern mining-based\nmethod, called Object Location Mining (OLM), which exploits the advantages of\ndata mining and feature representation of pre-trained convolutional neural\nnetworks (CNNs). Specifically, we first convert the feature maps from a\npre-trained CNN model into a set of transactions, and then discovers frequent\npatterns from transaction database through pattern mining techniques. We\nobserve that those discovered patterns, i.e., co-occurrence highlighted\nregions, typically hold appearance and spatial consistency. Motivated by this\nobservation, we can easily discover and localize possible objects by merging\nrelevant meaningful patterns. Extensive experiments on a variety of benchmarks\ndemonstrate that OLM achieves competitive localization performance compared\nwith the state-of-the-art methods. We also evaluate our approach compared with\nunsupervised saliency detection methods and achieves competitive results on\nseven benchmark datasets. Moreover, we conduct experiments on fine-grained\nclassification to show that our proposed method can locate the entire object\nand parts accurately, which can benefit to improving the classification results\nsignificantly.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 14:37:01 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 11:19:00 GMT"}, {"version": "v3", "created": "Sat, 8 Aug 2020 05:05:12 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Zhang", "Runsheng", ""], ["Huang", "Yaping", ""], ["Pu", "Mengyang", ""], ["Zhang", "Jian", ""], ["Guan", "Qingji", ""], ["Zou", "Qi", ""], ["Ling", "Haibin", ""]]}, {"id": "1902.09969", "submitter": "Ashutosh Mishra", "authors": "Ashutosh Mishra, Marcus Liwicki", "title": "Using Deep Object Features for Image Descriptions", "comments": "arXiv admin note: text overlap with arXiv:1411.2539, arXiv:1609.06647\n  by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent advances in leveraging multiple modalities in machine\ntranslation, we introduce an encoder-decoder pipeline that uses (1) specific\nobjects within an image and their object labels, (2) a language model for\ndecoding joint embedding of object features and the object labels. Our pipeline\nmerges prior detected objects from the image and their object labels and then\nlearns the sequences of captions describing the particular image. The decoder\nmodel learns to extract descriptions for the image from scratch by decoding the\njoint representation of the object visual features and their object classes\nconditioned by the encoder component. The idea of the model is to concentrate\nonly on the specific objects of the image and their labels for generating\ndescriptions of the image rather than visual feature of the entire image. The\nmodel needs to be calibrated more by adjusting the parameters and settings to\nresult in better accuracy and performance.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 18:40:25 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Mishra", "Ashutosh", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1902.10016", "submitter": "Hina Afridi", "authors": "Michalis Voutouris, Giovanni Sachi, Hina Afridi", "title": "Anomalous Situation Detection in Complex Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate a robust method to identify anomalies in complex\nscenes. This task is performed by evaluating the collective behavior by\nextracting the local binary patterns (LBP) and Laplacian of Gaussian (LoG)\nfeatures. We fuse both features together which are exploited to train an MLP\nneural network during the training stage, and the anomaly is identified on the\ntest samples. Considering the challenge of tracking individuals in dense\ncrowded scenes due to multiple occlusions and clutter, in this paper we extract\nLBP and LoG features and use them as an approximate representation of the\nanomalous situation. These features well match the appearance of anomaly and\ntheir consistency, and accuracy is higher both in regular and irregular areas\ncompared to other descriptors. In this paper, these features are exploited as\ninput prior to train the neural network. The MLP neural network is subsequently\nexplored to consider these features that can detect the anomalous situation.\nThe experimental tests are conducted on a set of benchmark video sequences\ncommonly used for anomaly situation detection.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 15:56:27 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Voutouris", "Michalis", ""], ["Sachi", "Giovanni", ""], ["Afridi", "Hina", ""]]}, {"id": "1902.10024", "submitter": "William McNally", "authors": "William McNally, Alexander Wong, John McPhee", "title": "STAR-Net: Action Recognition using Spatio-Temporal Activation\n  Reprojection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While depth cameras and inertial sensors have been frequently leveraged for\nhuman action recognition, these sensing modalities are impractical in many\nscenarios where cost or environmental constraints prohibit their use. As such,\nthere has been recent interest on human action recognition using low-cost,\nreadily-available RGB cameras via deep convolutional neural networks. However,\nmany of the deep convolutional neural networks proposed for action recognition\nthus far have relied heavily on learning global appearance cues directly from\nimaging data, resulting in highly complex network architectures that are\ncomputationally expensive and difficult to train. Motivated to reduce network\ncomplexity and achieve higher performance, we introduce the concept of\nspatio-temporal activation reprojection (STAR). More specifically, we reproject\nthe spatio-temporal activations generated by human pose estimation layers in\nspace and time using a stack of 3D convolutions. Experimental results on\nUTD-MHAD and J-HMDB demonstrate that an end-to-end architecture based on the\nproposed STAR framework (which we nickname STAR-Net) is proficient in\nsingle-environment and small-scale applications. On UTD-MHAD, STAR-Net\noutperforms several methods using richer data modalities such as depth and\ninertial sensors.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 16:06:29 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["McNally", "William", ""], ["Wong", "Alexander", ""], ["McPhee", "John", ""]]}, {"id": "1902.10030", "submitter": "Hussam Qassim Mr.", "authors": "Hussein A. Al-Barazanchi, Hussam Qassim, David Feinzimer, and Abhishek\n  Verma", "title": "Residual-CNDS for Grand Challenge Scene Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing depth of convolutional neural networks (CNNs) is a highly\npromising method of increasing the accuracy of the (CNNs). Increased CNN depth\nwill also result in increased layer count (parameters), leading to a slow\nbackpropagation convergence prone to overfitting. We trained our model\n(Residual-CNDS) to classify very large-scale scene datasets MIT Places 205, and\nMIT Places 365-Standard. The outcome result from the two datasets proved our\nproposed model (Residual-CNDS) effectively handled the slow convergence,\noverfitting, and degradation. CNNs that include deep supervision (CNDS) add\nsupplementary branches to the deep convolutional neural network in specified\nlayers by calculating vanishing, effectively addressing delayed convergence and\noverfitting. Nevertheless, (CNDS) does not resolve degradation; hence, we add\nresidual learning to the (CNDS) in certain layers after studying the best place\nin which to add it. With this approach we overcome degradation in the very deep\nnetwork. We have built two models (Residual-CNDS 8), and (Residual-CNDS 10).\nMoreover, we tested our models on two large-scale datasets, and we compared our\nresults with other recently introduced cutting-edge networks in the domain of\ntop-1 and top-5 classification accuracy. As a result, both of models have shown\ngood improvement, which supports the assertion that the addition of residual\nconnections enhances network CNDS accuracy without adding any computation\ncomplexity.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 23:00:11 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Al-Barazanchi", "Hussein A.", ""], ["Qassim", "Hussam", ""], ["Feinzimer", "David", ""], ["Verma", "Abhishek", ""]]}, {"id": "1902.10031", "submitter": "Nikola Milo\\v{s}evi\\'c Dr", "authors": "Nikola Milosevic, Cassie Gregson, Robert Hernandez, Goran Nenadic", "title": "A framework for information extraction from tables in biomedical\n  literature", "comments": "24 pages", "journal-ref": "2019, International Journal on Document Analysis and Recognition\n  (IJDAR)", "doi": "10.1007/s10032-019-00317-0", "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The scientific literature is growing exponentially, and professionals are no\nmore able to cope with the current amount of publications. Text mining provided\nin the past methods to retrieve and extract information from text; however,\nmost of these approaches ignored tables and figures. The research done in\nmining table data still does not have an integrated approach for mining that\nwould consider all complexities and challenges of a table. Our research is\nexamining the methods for extracting numerical (number of patients, age, gender\ndistribution) and textual (adverse reactions) information from tables in the\nclinical literature. We present a requirement analysis template and an integral\nmethodology for information extraction from tables in clinical domain that\ncontains 7 steps: (1) table detection, (2) functional processing, (3)\nstructural processing, (4) semantic tagging, (5) pragmatic processing, (6) cell\nselection and (7) syntactic processing and extraction. Our approach performed\nwith the F-measure ranged between 82 and 92%, depending on the variable, task\nand its complexity.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 16:22:15 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Milosevic", "Nikola", ""], ["Gregson", "Cassie", ""], ["Hernandez", "Robert", ""], ["Nenadic", "Goran", ""]]}, {"id": "1902.10053", "submitter": "Bin Kong", "authors": "Bin Kong, Xin Wang, Junjie Bai, Yi Lu, Feng Gao, Kunlin Cao, Qi Song,\n  Shaoting Zhang, Siwei Lyu, Youbing Yin", "title": "Attention-driven Tree-structured Convolutional LSTM for High Dimensional\n  Data Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the sequential information of image sequences has been a vital step\nof various vision tasks and convolutional long short-term memory (ConvLSTM) has\ndemonstrated its superb performance in such spatiotemporal problems.\nNevertheless, the hierarchical data structures in a significant amount of tasks\n(e.g., human body parts and vessel/airway tree in biomedical images) cannot be\nproperly modeled by sequential models. Thus, ConvLSTM is not suitable for\ntree-structured image data analysis. In order to address these limitations, we\npresent tree-structured ConvLSTM models for tree-structured image analysis\ntasks which can be trained end-to-end. To demonstrate the effectiveness of the\nproposed tree-structured ConvLSTM model, we present a tree-structured\nsegmentation framework which consists of a tree-structured ConvLSTM and an\nattention fully convolutional network (FCN) model. The proposed framework is\nextensively validated on four large-scale coronary artery datasets. The results\ndemonstrate the effectiveness and efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 20:34:17 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Kong", "Bin", ""], ["Wang", "Xin", ""], ["Bai", "Junjie", ""], ["Lu", "Yi", ""], ["Gao", "Feng", ""], ["Cao", "Kunlin", ""], ["Song", "Qi", ""], ["Zhang", "Shaoting", ""], ["Lyu", "Siwei", ""], ["Yin", "Youbing", ""]]}, {"id": "1902.10054", "submitter": "Ding Ma", "authors": "Ding Ma and Xiangqian Wu", "title": "TCDCaps: Visual Tracking via Cascaded Dense Capsules", "comments": "The description of DCaps have some mistakes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The critical challenge in tracking-by-detection framework is how to avoid\ndrift problem during online learning, where the robust features for a variety\nof appearance changes are difficult to be learned and a reasonable intersection\nover union (IoU) threshold that defines the true/false positives is hard to\nset. This paper presents the TCDCaps method to address the problems above via a\ncascaded dense capsule architecture. To get robust features, we extend original\ncapsules with dense-connected routing, which are referred as DCaps. Depending\non the preservation of part-whole relationships in the Capsule Networks, our\ndense-connected capsules can capture a variety of appearance variations. In\naddition, to handle the issue of IoU threshold, a cascaded DCaps model (CDCaps)\nis proposed to improve the quality of candidates, it consists of sequential\nDCaps trained with increasing IoU thresholds so as to sequentially improve the\nquality of candidates. Extensive experiments on 3 popular benchmarks\ndemonstrate the robustness of the proposed TCDCaps.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 17:02:05 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 06:58:30 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Ma", "Ding", ""], ["Wu", "Xiangqian", ""]]}, {"id": "1902.10058", "submitter": "Peng Yin", "authors": "Peng Yin, Lingyun Xu, Xueqian Li, Chen Yin, Yingli Li, Rangaprasad\n  Arun Srivatsan, Lu Li, Jianmin Ji, Yuqing He", "title": "A Multi-Domain Feature Learning Method for Visual Place Recognition", "comments": "6 pages, 5 figures, ICRA 2019 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Place Recognition (VPR) is an important component in both computer\nvision and robotics applications, thanks to its ability to determine whether a\nplace has been visited and where specifically. A major challenge in VPR is to\nhandle changes of environmental conditions including weather, season and\nillumination. Most VPR methods try to improve the place recognition performance\nby ignoring the environmental factors, leading to decreased accuracy decreases\nwhen environmental conditions change significantly, such as day versus night.\nTo this end, we propose an end-to-end conditional visual place recognition\nmethod. Specifically, we introduce the multi-domain feature learning method\n(MDFL) to capture multiple attribute-descriptions for a given place, and then\nuse a feature detaching module to separate the environmental condition-related\nfeatures from those that are not. The only label required within this feature\nlearning pipeline is the environmental condition. Evaluation of the proposed\nmethod is conducted on the multi-season \\textit{NORDLAND} dataset, and the\nmulti-weather \\textit{GTAV} dataset. Experimental results show that our method\nimproves the feature robustness against variant environmental conditions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 17:09:49 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Yin", "Peng", ""], ["Xu", "Lingyun", ""], ["Li", "Xueqian", ""], ["Yin", "Chen", ""], ["Li", "Yingli", ""], ["Srivatsan", "Rangaprasad Arun", ""], ["Li", "Lu", ""], ["Ji", "Jianmin", ""], ["He", "Yuqing", ""]]}, {"id": "1902.10059", "submitter": "Peng Yin", "authors": "Peng Yin, Rangaprasad Arun Srivatsan, Yin Chen, Xueqian Li, Hongda\n  Zhang, Lingyun Xu, Lu Li, Zhenzhong Jia, Jianmin Ji, Yuqing He", "title": "MRS-VPR: a multi-resolution sampling based global visual place\n  recognition method", "comments": "6 pages, 5 figures, ICRA 2019, accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Place recognition and loop closure detection are challenging for long-term\nvisual navigation tasks. SeqSLAM is considered to be one of the most successful\napproaches to achieving long-term localization under varying environmental\nconditions and changing viewpoints. It depends on a brute-force, time-consuming\nsequential matching method. We propose MRS-VPR, a multi-resolution,\nsampling-based place recognition method, which can significantly improve the\nmatching efficiency and accuracy in sequential matching. The novelty of this\nmethod lies in the coarse-to-fine searching pipeline and a particle\nfilter-based global sampling scheme, that can balance the matching efficiency\nand accuracy in the long-term navigation task. Moreover, our model works much\nbetter than SeqSLAM when the testing sequence has a much smaller scale than the\nreference sequence. Our experiments demonstrate that the proposed method is\nefficient in locating short temporary trajectories within long-term reference\nones without losing accuracy compared to SeqSLAM.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 17:10:16 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Yin", "Peng", ""], ["Srivatsan", "Rangaprasad Arun", ""], ["Chen", "Yin", ""], ["Li", "Xueqian", ""], ["Zhang", "Hongda", ""], ["Xu", "Lingyun", ""], ["Li", "Lu", ""], ["Jia", "Zhenzhong", ""], ["Ji", "Jianmin", ""], ["He", "Yuqing", ""]]}, {"id": "1902.10099", "submitter": "Ren\\'e Schuster", "authors": "Ren\\'e Schuster, Oliver Wasenm\\\"uller, Christian Unger, Georg Kuschk,\n  Didier Stricker", "title": "SceneFlowFields++: Multi-frame Matching, Visibility Prediction, and\n  Robust Interpolation for Scene Flow Estimation", "comments": "arXiv admin note: text overlap with arXiv:1710.10096", "journal-ref": null, "doi": "10.1007/s11263-019-01258-1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art scene flow algorithms pursue the conflicting targets of\naccuracy, run time, and robustness. With the successful concept of pixel-wise\nmatching and sparse-to-dense interpolation, we push the limits of scene flow\nestimation. Avoiding strong assumptions on the domain or the problem yields a\nmore robust algorithm. This algorithm is fast because we avoid explicit\nregularization during matching, which allows an efficient computation. Using\nimage information from multiple time steps and explicit visibility prediction\nbased on previous results, we achieve competitive performances on different\ndata sets. Our contributions and results are evaluated in comparative\nexperiments. Overall, we present an accurate scene flow algorithm that is\nfaster and more generic than any individual benchmark leader.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 18:29:33 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 09:27:22 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Schuster", "Ren\u00e9", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Unger", "Christian", ""], ["Kuschk", "Georg", ""], ["Stricker", "Didier", ""]]}, {"id": "1902.10127", "submitter": "Maryam Gholizadeh-Ansari", "authors": "Maryam Gholizadeh-Ansari, Javad Alirezaie and Paul Babyn", "title": "Deep Learning for Low-Dose CT Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-dose CT denoising is a challenging task that has been studied by many\nresearchers. Some studies have used deep neural networks to improve the quality\nof low-dose CT images and achieved fruitful results. In this paper, we propose\na deep neural network that uses dilated convolutions with different dilation\nrates instead of standard convolution helping to capture more contextual\ninformation in fewer layers. Also, we have employed residual learning by\ncreating shortcut connections to transmit image information from the early\nlayers to later ones. To further improve the performance of the network, we\nhave introduced a non-trainable edge detection layer that extracts edges in\nhorizontal, vertical, and diagonal directions. Finally, we demonstrate that\noptimizing the network by a combination of mean-square error loss and\nperceptual loss preserves many structural details in the CT image. This\nobjective function does not suffer from over smoothing and blurring effects\ncaused by per-pixel loss and grid-like artifacts resulting from perceptual\nloss. The experiments show that each modification to the network improves the\noutcome while only minimally changing the complexity of the network.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 21:14:45 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Gholizadeh-Ansari", "Maryam", ""], ["Alirezaie", "Javad", ""], ["Babyn", "Paul", ""]]}, {"id": "1902.10178", "submitter": "Wojciech Samek", "authors": "Sebastian Lapuschkin, Stephan W\\\"aldchen, Alexander Binder, Gr\\'egoire\n  Montavon, Wojciech Samek, Klaus-Robert M\\\"uller", "title": "Unmasking Clever Hans Predictors and Assessing What Machines Really\n  Learn", "comments": "Accepted for publication in Nature Communications", "journal-ref": null, "doi": "10.1038/s41467-019-08987-4", "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current learning machines have successfully solved hard application problems,\nreaching high accuracy and displaying seemingly \"intelligent\" behavior. Here we\napply recent techniques for explaining decisions of state-of-the-art learning\nmachines and analyze various tasks from computer vision and arcade games. This\nshowcases a spectrum of problem-solving behaviors ranging from naive and\nshort-sighted, to well-informed and strategic. We observe that standard\nperformance evaluation metrics can be oblivious to distinguishing these diverse\nproblem solving behaviors. Furthermore, we propose our semi-automated Spectral\nRelevance Analysis that provides a practically effective way of characterizing\nand validating the behavior of nonlinear learning machines. This helps to\nassess whether a learned model indeed delivers reliably for the problem that it\nwas conceived for. Furthermore, our work intends to add a voice of caution to\nthe ongoing excitement about machine intelligence and pledges to evaluate and\njudge some of these recent successes in a more nuanced manner.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 19:25:11 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Lapuschkin", "Sebastian", ""], ["W\u00e4ldchen", "Stephan", ""], ["Binder", "Alexander", ""], ["Montavon", "Gr\u00e9goire", ""], ["Samek", "Wojciech", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1902.10190", "submitter": "Atul Ingle", "authors": "Atul Ingle and Andreas Velten and Mohit Gupta", "title": "High Flux Passive Imaging with Single-Photon Sensors", "comments": "28 pages, 15 figures, addressed reviewers's comments, fixed some\n  errors and typos, official peer reviewed version to appear in IEEE CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-photon avalanche diodes (SPADs) are an emerging technology with a\nunique capability of capturing individual photons with high timing precision.\nSPADs are being used in several active imaging systems (e.g., fluorescence\nlifetime microscopy and LiDAR), albeit mostly limited to low photon flux\nsettings. We propose passive free-running SPAD (PF-SPAD) imaging, an imaging\nmodality that uses SPADs for capturing 2D intensity images with unprecedented\ndynamic range under ambient lighting, without any active light source. Our key\nobservation is that the precise inter-photon timing measured by a SPAD can be\nused for estimating scene brightness under ambient lighting conditions, even\nfor very bright scenes. We develop a theoretical model for PF-SPAD imaging, and\nderive a scene brightness estimator based on the average time of darkness\nbetween successive photons detected by a PF-SPAD pixel. Our key insight is that\ndue to the stochastic nature of photon arrivals, this estimator does not suffer\nfrom a hard saturation limit. Coupled with high sensitivity at low flux, this\nenables a PF-SPAD pixel to measure a wide range of scene brightness, from very\nlow to very high, thereby achieving extreme dynamic range. We demonstrate an\nimprovement of over 2 orders of magnitude over conventional sensors by imaging\nscenes spanning a dynamic range of 1,000,000:1.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 20:05:42 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 22:20:02 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Ingle", "Atul", ""], ["Velten", "Andreas", ""], ["Gupta", "Mohit", ""]]}, {"id": "1902.10194", "submitter": "Georgi Tinchev", "authors": "Georgi Tinchev, Adrian Penate-Sanchez and Maurice Fallon", "title": "Learning to See the Wood for the Trees: Deep Laser Localization in Urban\n  and Natural Environments on a CPU", "comments": "Accepted for publication at RA-L/ICRA 2019. More info:\n  https://ori.ox.ac.uk/esm-localization", "journal-ref": null, "doi": "10.1109/LRA.2019.2895264", "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localization in challenging, natural environments such as forests or\nwoodlands is an important capability for many applications from guiding a robot\nnavigating along a forest trail to monitoring vegetation growth with handheld\nsensors. In this work we explore laser-based localization in both urban and\nnatural environments, which is suitable for online applications. We propose a\ndeep learning approach capable of learning meaningful descriptors directly from\n3D point clouds by comparing triplets (anchor, positive and negative examples).\nThe approach learns a feature space representation for a set of segmented point\nclouds that are matched between a current and previous observations. Our\nlearning method is tailored towards loop closure detection resulting in a small\nmodel which can be deployed using only a CPU. The proposed learning method\nwould allow the full pipeline to run on robots with limited computational\npayload such as drones, quadrupeds or UGVs.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 20:13:14 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Tinchev", "Georgi", ""], ["Penate-Sanchez", "Adrian", ""], ["Fallon", "Maurice", ""]]}, {"id": "1902.10200", "submitter": "Roei Herzig", "authors": "Moshiko Raboh, Roei Herzig, Gal Chechik, Jonathan Berant and Amir\n  Globerson", "title": "Differentiable Scene Graphs", "comments": "Winter Conference on Applications of Computer Vision (WACV), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning about complex visual scenes involves perception of entities and\ntheir relations. Scene graphs provide a natural representation for reasoning\ntasks, by assigning labels to both entities (nodes) and relations (edges).\nUnfortunately, reasoning systems based on SGs are typically trained in a\ntwo-step procedure: First, training a model to predict SGs from images; Then, a\nseparate model is created to reason based on predicted SGs. In many domains, it\nis preferable to train systems jointly in an end-to-end manner, but SGs are not\ncommonly used as intermediate components in visual reasoning systems because\nbeing discrete and sparse, scene-graph representations are non-differentiable\nand difficult to optimize. Here we propose Differentiable Scene Graphs (DSGs),\nan image representation that is amenable to differentiable end-to-end\noptimization, and requires supervision only from the downstream tasks. DSGs\nprovide a dense representation for all regions and pairs of regions, and do not\nspend modelling capacity on areas of the images that do not contain objects or\nrelations of interest. We evaluate our model on the challenging task of\nidentifying referring relationships (RR) in three benchmark datasets, Visual\nGenome, VRD and CLEVR. We describe a multi-task objective, and train in an\nend-to-end manner supervised by the downstream RR task. Using DSGs as an\nintermediate representation leads to new state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 20:22:33 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 21:25:01 GMT"}, {"version": "v3", "created": "Sun, 28 Jul 2019 06:21:43 GMT"}, {"version": "v4", "created": "Sun, 26 Jan 2020 10:33:19 GMT"}, {"version": "v5", "created": "Sat, 14 Mar 2020 16:25:32 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Raboh", "Moshiko", ""], ["Herzig", "Roei", ""], ["Chechik", "Gal", ""], ["Berant", "Jonathan", ""], ["Globerson", "Amir", ""]]}, {"id": "1902.10205", "submitter": "Mohammad Golbabaee", "authors": "Mohammad Golbabaee, Carolin M. Pirkl, Marion I. Menzel, Guido\n  Buonincontri and Pedro A. G\\'omez", "title": "Deep MR Fingerprinting with total-variation and low-rank subspace priors", "comments": null, "journal-ref": "International Society for Magnetic Resonance in Medicine,\n  Montreal, Canada, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) has recently emerged to address the heavy storage and\ncomputation requirements of the baseline dictionary-matching (DM) for Magnetic\nResonance Fingerprinting (MRF) reconstruction. Fed with non-iterated\nback-projected images, the network is unable to fully resolve\nspatially-correlated corruptions caused from the undersampling artefacts. We\npropose an accelerated iterative reconstruction to minimize these artefacts\nbefore feeding into the network. This is done through a convex regularization\nthat jointly promotes spatio-temporal regularities of the MRF time-series.\nExcept for training, the rest of the parameter estimation pipeline is\ndictionary-free. We validate the proposed approach on synthetic and in-vivo\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 20:33:02 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Golbabaee", "Mohammad", ""], ["Pirkl", "Carolin M.", ""], ["Menzel", "Marion I.", ""], ["Buonincontri", "Guido", ""], ["G\u00f3mez", "Pedro A.", ""]]}, {"id": "1902.10226", "submitter": "Sirisha Rambhatla", "authors": "Sirisha Rambhatla, Nikos D. Sidiropoulos, and Jarvis Haupt", "title": "TensorMap: Lidar-Based Topological Mapping and Localization via Tensor\n  Decompositions", "comments": "5 pages; Index Terms - Topological maps, Lidar, Localization of\n  Autonomous Vehicles, Orthogonal Tucker Decomposition, and Scan-matching", "journal-ref": "2018 IEEE Global Conference on Signal and Information Processing\n  (GlobalSIP)", "doi": "10.1109/GlobalSIP.2018.8646665", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a technique to develop (and localize in) topological maps from\nlight detection and ranging (Lidar) data. Localizing an autonomous vehicle with\nrespect to a reference map in real-time is crucial for its safe operation.\nOwing to the rich information provided by Lidar sensors, these are emerging as\na promising choice for this task. However, since a Lidar outputs a large amount\nof data every fraction of a second, it is progressively harder to process the\ninformation in real-time. Consequently, current systems have migrated towards\nfaster alternatives at the expense of accuracy. To overcome this inherent\ntrade-off between latency and accuracy, we propose a technique to develop\ntopological maps from Lidar data using the orthogonal Tucker3 tensor\ndecomposition. Our experimental evaluations demonstrate that in addition to\nachieving a high compression ratio as compared to full data, the proposed\ntechnique, $\\textit{TensorMap}$, also accurately detects the position of the\nvehicle in a graph-based representation of a map. We also analyze the\nrobustness of the proposed technique to Gaussian and translational noise, thus\ninitiating explorations into potential applications of tensor decompositions in\nLidar data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 21:46:20 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Rambhatla", "Sirisha", ""], ["Sidiropoulos", "Nikos D.", ""], ["Haupt", "Jarvis", ""]]}, {"id": "1902.10238", "submitter": "Sirisha Rambhatla", "authors": "Sirisha Rambhatla, Xingguo Li, Jineng Ren and Jarvis Haupt", "title": "A Dictionary-Based Generalization of Robust PCA Part II: Applications to\n  Hyperspectral Demixing", "comments": "13 pages; Index Terms - Hyperspectral imaging, Robust-PCA, Dictionary\n  Sparse, Matrix Demixing, Target Localization, and Remote Sensing", "journal-ref": null, "doi": "10.1109/TSP.2020.2977458", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of localizing targets of interest in a hyperspectral\n(HS) image based on their spectral signature(s), by posing the problem as two\ndistinct convex demixing task(s). With applications ranging from remote sensing\nto surveillance, this task of target detection leverages the fact that each\nmaterial/object possesses its own characteristic spectral response, depending\nupon its composition. However, since $\\textit{signatures}$ of different\nmaterials are often correlated, matched filtering-based approaches may not be\napply here. To this end, we model a HS image as a superposition of a low-rank\ncomponent and a dictionary sparse component, wherein the dictionary consists of\nthe $\\textit{a priori}$ known characteristic spectral responses of the target\nwe wish to localize, and develop techniques for two different sparsity\nstructures, resulting from different model assumptions. We also present the\ncorresponding recovery guarantees, leveraging our recent theoretical results\nfrom a companion paper. Finally, we analyze the performance of the proposed\napproach via experimental evaluations on real HS datasets for a classification\ntask, and compare its performance with related techniques.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 21:42:17 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Rambhatla", "Sirisha", ""], ["Li", "Xingguo", ""], ["Ren", "Jineng", ""], ["Haupt", "Jarvis", ""]]}, {"id": "1902.10272", "submitter": "Ali Cheraghian", "authors": "Ali Cheraghian, Shafin Rahman, Lars Petersson", "title": "Zero-shot Learning of 3D Point Cloud Objects", "comments": null, "journal-ref": "International Conference on Machine Vision Applications (MVA)\n  (2019)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning architectures can recognize instances of 3D point cloud\nobjects of previously seen classes quite well. At the same time, current 3D\ndepth camera technology allows generating/segmenting a large amount of 3D point\ncloud objects from an arbitrary scene, for which there is no previously seen\ntraining data. A challenge for a 3D point cloud recognition system is, then, to\nclassify objects from new, unseen, classes. This issue can be resolved by\nadopting a zero-shot learning (ZSL) approach for 3D data, similar to the 2D\nimage version of the same problem. ZSL attempts to classify unseen objects by\ncomparing semantic information (attribute/word vector) of seen and unseen\nclasses. Here, we adapt several recent 3D point cloud recognition systems to\nthe ZSL setting with some changes to their architectures. To the best of our\nknowledge, this is the first attempt to classify unseen 3D point cloud objects\nin the ZSL setting. A standard protocol (which includes the choice of datasets\nand the seen/unseen split) to evaluate such systems is also proposed. Baseline\nperformances are reported using the new protocol on the investigated models.\nThis investigation throws a new challenge to the 3D point cloud recognition\ncommunity that may instigate numerous future works.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 00:15:31 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Cheraghian", "Ali", ""], ["Rahman", "Shafin", ""], ["Petersson", "Lars", ""]]}, {"id": "1902.10274", "submitter": "Dr. Suryansh Kumar", "authors": "Suryansh Kumar", "title": "Non-Rigid Structure from Motion: Prior-Free Factorization Method\n  Revisited", "comments": "Accepted for publication in IEEE, WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple prior free factorization algorithm \\cite{dai2014simple} is quite\noften cited work in the field of Non-Rigid Structure from Motion (NRSfM). The\nbenefit of this work lies in its simplicity of implementation, strong\ntheoretical justification to the motion and structure estimation, and its\ninvincible originality. Despite this, the prevailing view is, that it performs\nexceedingly inferior to other methods on several benchmark datasets\n\\cite{jensen2018benchmark,akhter2009nonrigid}. However, our subtle\ninvestigation provides some empirical statistics which made us think against\nsuch views. The statistical results we obtained supersedes Dai {\\it{et\nal.}}\\cite{dai2014simple} originally reported results on the benchmark datasets\nby a significant margin under some elementary changes in their core algorithmic\nidea \\cite{dai2014simple}. Now, these results not only exposes some unrevealed\nareas for research in NRSfM but also give rise to new mathematical challenges\nfor NRSfM researchers. We argue that by \\textbf{properly} utilizing the\nwell-established assumptions about a non-rigidly deforming shape i.e, it\ndeforms smoothly over frames \\cite{rabaud2008re} and it spans a low-rank space,\nthe simple prior-free idea can provide results which is comparable to the best\navailable algorithms. In this paper, we explore some of the hidden intricacies\nmissed by Dai {\\it{et. al.}} work \\cite{dai2014simple} and how some elementary\nmeasures and modifications can enhance its performance, as high as approx. 18\\%\non the benchmark dataset. The improved performance is justified and empirically\nverified by extensive experiments on several datasets. We believe our work has\nboth practical and theoretical importance for the development of better NRSfM\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 00:20:37 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 07:15:23 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2019 04:51:27 GMT"}, {"version": "v4", "created": "Sat, 23 Mar 2019 10:31:45 GMT"}, {"version": "v5", "created": "Sat, 21 Dec 2019 11:19:56 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Kumar", "Suryansh", ""]]}, {"id": "1902.10311", "submitter": "Ying Tai", "authors": "Yao Liu and Ying Tai and Jilin Li and Shouhong Ding and Chengjie Wang\n  and Feiyue Huang and Dongyang Li and Wenshuai Qi and Rongrong Ji", "title": "Aurora Guard: Real-Time Face Anti-Spoofing via Light Reflection", "comments": "Technical report that introduces our face anti-spoofing system:\n  Aurora Guard. Simple demo video can be found at:\n  https://drive.google.com/file/d/1wLXwGvy2zsh5xkDhRCPNzQXGUihZNq1g/view?usp=sharing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a light reflection based face anti-spoofing method\nnamed Aurora Guard (AG), which is fast, simple yet effective that has already\nbeen deployed in real-world systems serving for millions of users.\nSpecifically, our method first extracts the normal cues via light reflection\nanalysis, and then uses an end-to-end trainable multi-task Convolutional Neural\nNetwork (CNN) to not only recover subjects' depth maps to assist liveness\nclassification, but also provide the light CAPTCHA checking mechanism in the\nregression branch to further improve the system reliability. Moreover, we\nfurther collect a large-scale dataset containing $12,000$ live and spoofing\nsamples, which covers abundant imaging qualities and Presentation Attack\nInstruments (PAI). Extensive experiments on both public and our datasets\ndemonstrate the superiority of our proposed method over the state of the arts.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 02:29:17 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Liu", "Yao", ""], ["Tai", "Ying", ""], ["Li", "Jilin", ""], ["Ding", "Shouhong", ""], ["Wang", "Chengjie", ""], ["Huang", "Feiyue", ""], ["Li", "Dongyang", ""], ["Qi", "Wenshuai", ""], ["Ji", "Rongrong", ""]]}, {"id": "1902.10322", "submitter": "Nayyer Aafaq Mr.", "authors": "Nayyer Aafaq, Naveed Akhtar, Wei Liu, Syed Zulqarnain Gilani and Ajmal\n  Mian", "title": "Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding\n  for Video Captioning", "comments": "Accepted in CVPR-2019 (Camera Ready)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic generation of video captions is a fundamental challenge in computer\nvision. Recent techniques typically employ a combination of Convolutional\nNeural Networks (CNNs) and Recursive Neural Networks (RNNs) for video\ncaptioning. These methods mainly focus on tailoring sequence learning through\nRNNs for better caption generation, whereas off-the-shelf visual features are\nborrowed from CNNs. We argue that careful designing of visual features for this\ntask is equally important, and present a visual feature encoding technique to\ngenerate semantically rich captions using Gated Recurrent Units (GRUs). Our\nmethod embeds rich temporal dynamics in visual features by hierarchically\napplying Short Fourier Transform to CNN features of the whole video. It\nadditionally derives high level semantics from an object detector to enrich the\nrepresentation with spatial dynamics of the detected objects. The final\nrepresentation is projected to a compact space and fed to a language model. By\nlearning a relatively simple language model comprising two GRU layers, we\nestablish new state-of-the-art on MSVD and MSR-VTT datasets for METEOR and\nROUGE_L metrics.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 03:52:08 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 05:20:00 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Aafaq", "Nayyer", ""], ["Akhtar", "Naveed", ""], ["Liu", "Wei", ""], ["Gilani", "Syed Zulqarnain", ""], ["Mian", "Ajmal", ""]]}, {"id": "1902.10363", "submitter": "Benjamin Meyer", "authors": "Benjamin J. Meyer and Tom Drummond", "title": "The Importance of Metric Learning for Robotic Vision: Open Set\n  Recognition and Active Learning", "comments": "Accepted to the 2019 IEEE International Conference on Robotics and\n  Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep neural network recognition systems are designed for a\nstatic and closed world. It is usually assumed that the distribution at test\ntime will be the same as the distribution during training. As a result,\nclassifiers are forced to categorise observations into one out of a set of\npredefined semantic classes. Robotic problems are dynamic and open world; a\nrobot will likely observe objects that are from outside of the training set\ndistribution. Classifier outputs in robotic applications can lead to real-world\nrobotic action and as such, a practical recognition system should not silently\nfail by confidently misclassifying novel observations. We show how a deep\nmetric learning classification system can be applied to such open set\nrecognition problems, allowing the classifier to label novel observations as\nunknown. Further to detecting novel examples, we propose an open set active\nlearning approach that allows a robot to efficiently query a user about unknown\nobservations. Our approach enables a robot to improve its understanding of the\ntrue distribution of data in the environment, from a small number of label\nqueries. Experimental results show that our approach significantly outperforms\ncomparable methods in both the open set recognition and active learning\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 07:18:24 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Meyer", "Benjamin J.", ""], ["Drummond", "Tom", ""]]}, {"id": "1902.10364", "submitter": "Yiming Hu", "authors": "Yiming Hu, Siyang Sun, Jianquan Li, Jiagang Zhu, Xingang Wang, Qingyi\n  Gu", "title": "Multi-loss-aware Channel Pruning of Deep Networks", "comments": "4 pages, 2 figures", "journal-ref": "IEEE ICIP 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Channel pruning, which seeks to reduce the model size by removing redundant\nchannels, is a popular solution for deep networks compression. Existing channel\npruning methods usually conduct layer-wise channel selection by directly\nminimizing the reconstruction error of feature maps between the baseline model\nand the pruned one. However, they ignore the feature and semantic distributions\nwithin feature maps and real contribution of channels to the overall\nperformance. In this paper, we propose a new channel pruning method by\nexplicitly using both intermediate outputs of the baseline model and the\nclassification loss of the pruned model to supervise layer-wise channel\nselection. Particularly, we introduce an additional loss to encode the\ndifferences in the feature and semantic distributions within feature maps\nbetween the baseline model and the pruned one. By considering the\nreconstruction error, the additional loss and the classification loss at the\nsame time, our approach can significantly improve the performance of the pruned\nmodel. Comprehensive experiments on benchmark datasets demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 07:22:24 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Hu", "Yiming", ""], ["Sun", "Siyang", ""], ["Li", "Jianquan", ""], ["Zhu", "Jiagang", ""], ["Wang", "Xingang", ""], ["Gu", "Qingyi", ""]]}, {"id": "1902.10370", "submitter": "Yiming Hu", "authors": "Yiming Hu, Jianquan Li, Xianlei Long, Shenhua Hu, Jiagang Zhu, Xingang\n  Wang, Qingyi Gu", "title": "Cluster Regularized Quantization for Deep Networks Compression", "comments": "4 pages, 1 figure", "journal-ref": "IEEE ICIP 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved great success in a wide range of\ncomputer vision areas, but the applications to mobile devices is limited due to\ntheir high storage and computational cost. Much efforts have been devoted to\ncompress DNNs. In this paper, we propose a simple yet effective method for deep\nnetworks compression, named Cluster Regularized Quantization (CRQ), which can\nreduce the presentation precision of a full-precision model to ternary values\nwithout significant accuracy drop. In particular, the proposed method aims at\nreducing the quantization error by introducing a cluster regularization term,\nwhich is imposed on the full-precision weights to enable them naturally\nconcentrate around the target values. Through explicitly regularizing the\nweights during the re-training stage, the full-precision model can achieve the\nsmooth transition to the low-bit one. Comprehensive experiments on benchmark\ndatasets demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 07:40:19 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Hu", "Yiming", ""], ["Li", "Jianquan", ""], ["Long", "Xianlei", ""], ["Hu", "Shenhua", ""], ["Zhu", "Jiagang", ""], ["Wang", "Xingang", ""], ["Gu", "Qingyi", ""]]}, {"id": "1902.10404", "submitter": "{\\L}ukasz Maziarka", "authors": "Sylwester Klocek, {\\L}ukasz Maziarka, Maciej Wo{\\l}czyk, Jacek Tabor,\n  Jakub Nowak, Marek \\'Smieja", "title": "Hypernetwork functional image representation", "comments": null, "journal-ref": "Artificial Neural Networks and Machine Learning -- ICANN 2019:\n  Workshop and Special Sessions", "doi": "10.1007/978-3-030-30493-5_48", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the human way of memorizing images we introduce their functional\nrepresentation, where an image is represented by a neural network. For this\npurpose, we construct a hypernetwork which takes an image and returns weights\nto the target network, which maps point from the plane (representing positions\nof the pixel) into its corresponding color in the image. Since the obtained\nrepresentation is continuous, one can easily inspect the image at various\nresolutions and perform on it arbitrary continuous operations. Moreover, by\ninspecting interpolations we show that such representation has some properties\ncharacteristic to generative models. To evaluate the proposed mechanism\nexperimentally, we apply it to image super-resolution problem. Despite using a\nsingle model for various scaling factors, we obtained results comparable to\nexisting super-resolution methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 09:12:29 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 21:44:12 GMT"}, {"version": "v3", "created": "Mon, 3 Jun 2019 13:11:41 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Klocek", "Sylwester", ""], ["Maziarka", "\u0141ukasz", ""], ["Wo\u0142czyk", "Maciej", ""], ["Tabor", "Jacek", ""], ["Nowak", "Jakub", ""], ["\u015amieja", "Marek", ""]]}, {"id": "1902.10414", "submitter": "Leon Bungert", "authors": "Leon Bungert, Martin Burger, Daniel Tenbrinck", "title": "Computing Nonlinear Eigenfunctions via Gradient Flow Extinction", "comments": "12 pages, 5 figure, accepted for publication in SSVM conference\n  proceedings 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we investigate the computation of nonlinear eigenfunctions via\nthe extinction profiles of gradient flows. We analyze a scheme that recursively\nsubtracts such eigenfunctions from given data and show that this procedure\nyields a decomposition of the data into eigenfunctions in some cases as the\n1-dimensional total variation, for instance. We discuss results of numerical\nexperiments in which we use extinction profiles and the gradient flow for the\ntask of spectral graph clustering as used, e.g., in machine learning\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 09:50:38 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Bungert", "Leon", ""], ["Burger", "Martin", ""], ["Tenbrinck", "Daniel", ""]]}, {"id": "1902.10416", "submitter": "Pierre Stock", "authors": "Pierre Stock, Benjamin Graham, R\\'emi Gribonval and Herv\\'e J\\'egou", "title": "Equi-normalization of Neural Networks", "comments": "ICLR 2019 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern neural networks are over-parametrized. In particular, each rectified\nlinear hidden unit can be modified by a multiplicative factor by adjusting\ninput and output weights, without changing the rest of the network. Inspired by\nthe Sinkhorn-Knopp algorithm, we introduce a fast iterative method for\nminimizing the L2 norm of the weights, equivalently the weight decay\nregularizer. It provably converges to a unique solution. Interleaving our\nalgorithm with SGD during training improves the test accuracy. For small\nbatches, our approach offers an alternative to batch-and group-normalization on\nCIFAR-10 and ImageNet with a ResNet-18.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 09:52:43 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Stock", "Pierre", ""], ["Graham", "Benjamin", ""], ["Gribonval", "R\u00e9mi", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1902.10421", "submitter": "Jungbeom Lee", "authors": "Jungbeom Lee, Eunji Kim, Sungmin Lee, Jangho Lee, Sungroh Yoon", "title": "FickleNet: Weakly and Semi-supervised Semantic Image Segmentation using\n  Stochastic Inference", "comments": "To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main obstacle to weakly supervised semantic image segmentation is the\ndifficulty of obtaining pixel-level information from coarse image-level\nannotations. Most methods based on image-level annotations use localization\nmaps obtained from the classifier, but these only focus on the small\ndiscriminative parts of objects and do not capture precise boundaries.\nFickleNet explores diverse combinations of locations on feature maps created by\ngeneric deep neural networks. It selects hidden units randomly and then uses\nthem to obtain activation scores for image classification. FickleNet implicitly\nlearns the coherence of each location in the feature maps, resulting in a\nlocalization map which identifies both discriminative and other parts of\nobjects. The ensemble effects are obtained from a single network by selecting\nrandom hidden unit pairs, which means that a variety of localization maps are\ngenerated from a single image. Our approach does not require any additional\ntraining steps and only adds a simple layer to a standard convolutional neural\nnetwork; nevertheless it outperforms recent comparable techniques on the Pascal\nVOC 2012 benchmark in both weakly and semi-supervised settings.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 09:59:21 GMT"}, {"version": "v2", "created": "Sat, 2 Mar 2019 02:55:16 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Lee", "Jungbeom", ""], ["Kim", "Eunji", ""], ["Lee", "Sungmin", ""], ["Lee", "Jangho", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1902.10424", "submitter": "Gabriel Eilertsen", "authors": "Gabriel Eilertsen, Rafa{\\l} K. Mantiuk, Jonas Unger", "title": "Single-frame Regularization for Temporally Stable CNNs", "comments": "Project web: http://hdrv.org/hdrcnn/cvpr2019/", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2019", "doi": "10.1109/CVPR.2019.01143", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) can model complicated non-linear\nrelations between images. However, they are notoriously sensitive to small\nchanges in the input. Most CNNs trained to describe image-to-image mappings\ngenerate temporally unstable results when applied to video sequences, leading\nto flickering artifacts and other inconsistencies over time. In order to use\nCNNs for video material, previous methods have relied on estimating dense\nframe-to-frame motion information (optical flow) in the training and/or the\ninference phase, or by exploring recurrent learning structures. We take a\ndifferent approach to the problem, posing temporal stability as a\nregularization of the cost function. The regularization is formulated to\naccount for different types of motion that can occur between frames, so that\ntemporally stable CNNs can be trained without the need for video material or\nexpensive motion estimation. The training can be performed as a fine-tuning\noperation, without architectural modifications of the CNN. Our evaluation shows\nthat the training strategy leads to large improvements in temporal smoothness.\nMoreover, for small datasets the regularization can help in boosting the\ngeneralization performance to a much larger extent than what is possible with\nna\\\"ive augmentation strategies.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 10:01:49 GMT"}, {"version": "v2", "created": "Sat, 8 Jun 2019 05:39:31 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Eilertsen", "Gabriel", ""], ["Mantiuk", "Rafa\u0142 K.", ""], ["Unger", "Jonas", ""]]}, {"id": "1902.10425", "submitter": "Xiong Zhang", "authors": "Hongmin Xu, Qiang Li, Wenbo Zhang, Wen Zheng", "title": "StyleRemix: An Interpretable Representation for Neural Image Style\n  Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Style Transfer (MST) intents to capture the high-level visual\nvocabulary of different styles and expresses these vocabularies in a joint\nmodel to transfer each specific style. Recently, Style Embedding Learning (SEL)\nbased methods represent each style with an explicit set of parameters to\nperform MST task. However, most existing SEL methods either learn explicit\nstyle representation with numerous independent parameters or learn a relatively\nblack-box style representation, which makes them difficult to control the\nstylized results. In this paper, we outline a novel MST model, StyleRemix, to\ncompactly and explicitly integrate multiple styles into one network. By\ndecomposing diverse styles into the same basis, StyleRemix represents a\nspecific style in a continuous vector space with 1-dimensional coefficients.\nWith the interpretable style representation, StyleRemix not only enables the\nstyle visualization task but also allows several ways of remixing styles in the\nsmooth style embedding space.~Extensive experiments demonstrate the\neffectiveness of StyleRemix on various MST tasks compared to state-of-the-art\nSEL approaches.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 10:02:14 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 04:47:07 GMT"}, {"version": "v3", "created": "Mon, 25 Mar 2019 02:40:12 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Xu", "Hongmin", ""], ["Li", "Qiang", ""], ["Zhang", "Wenbo", ""], ["Zheng", "Wen", ""]]}, {"id": "1902.10441", "submitter": "Federico Pernici", "authors": "Federico Pernici, Matteo Bruni, Claudio Baecchi and Alberto Del Bimbo", "title": "Fix Your Features: Stationary and Maximally Discriminative Embeddings\n  using Regular Polytope (Fixed Classifier) Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are widely used as a model for classification in a large\nvariety of tasks. Typically, a learnable transformation (i.e. the classifier)\nis placed at the end of such models returning a value for each class used for\nclassification. This transformation plays an important role in determining how\nthe generated features change during the learning process.\n  In this work we argue that this transformation not only can be fixed (i.e.\nset as non trainable) with no loss of accuracy, but it can also be used to\nlearn stationary and maximally discriminative embeddings.\n  We show that the stationarity of the embedding and its maximal discriminative\nrepresentation can be theoretically justified by setting the weights of the\nfixed classifier to values taken from the coordinate vertices of three regular\npolytopes available in $\\mathbb{R}^d$, namely: the $d$-Simplex, the $d$-Cube\nand the $d$-Orthoplex. These regular polytopes have the maximal amount of\nsymmetry that can be exploited to generate stationary features angularly\ncentered around their corresponding fixed weights.\n  Our approach improves and broadens the concept of a fixed classifier,\nrecently proposed in \\cite{hoffer2018fix}, to a larger class of fixed\nclassifier models. Experimental results confirm both the theoretical analysis\nand the generalization capability of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 10:33:46 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 09:45:22 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Pernici", "Federico", ""], ["Bruni", "Matteo", ""], ["Baecchi", "Claudio", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1902.10460", "submitter": "Jiasong Wu", "authors": "Jinpeng Xia, Jiasong Wu, Youyong Kong, Pinzheng Zhang, Lotfi Senhadji,\n  Huazhong Shu", "title": "Modulated binary cliquenet", "comments": "5 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Convolutional Neural Networks (CNNs) achieve effectiveness in\nvarious computer vision tasks, the significant requirement of storage of such\nnetworks hinders the deployment on computationally limited devices. In this\npaper, we propose a new compact and portable deep learning network named\nModulated Binary Cliquenet (MBCliqueNet) aiming to improve the portability of\nCNNs based on binarized filters while achieving comparable performance with the\nfull-precision CNNs like Resnet. In MBCliqueNet, we introduce a novel modulated\noperation to approximate the unbinarized filters and gives an initialization\nmethod to speed up its convergence. We reduce the extra parameters caused by\nmodulated operation with parameters sharing. As a result, the proposed\nMBCliqueNet can reduce the required storage space of convolutional filters by a\nfactor of at least 32, in contrast to the full-precision model, and achieve\nbetter performance than other state-of-the-art binarized models. More\nimportantly, our model compares even better with some full-precision models\nlike Resnet on the dataset we used.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 11:14:01 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Xia", "Jinpeng", ""], ["Wu", "Jiasong", ""], ["Kong", "Youyong", ""], ["Zhang", "Pinzheng", ""], ["Senhadji", "Lotfi", ""], ["Shu", "Huazhong", ""]]}, {"id": "1902.10466", "submitter": "Yanlin Qian", "authors": "Yanlin Qian, Song Yan, Joni-Kristian K\\\"am\\\"ar\\\"ainen, Jiri Matas", "title": "Flash Lightens Gray Pixels", "comments": "5 pages including refs, 4 figures, submitted to International\n  Conference on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the real world, a scene is usually cast by multiple illuminants and herein\nwe address the problem of spatial illumination estimation. Our solution is\nbased on detecting gray pixels with the help of flash photography. We show that\nflash photography significantly improves the performance of gray pixel\ndetection without illuminant prior, training data or calibration of the flash.\nWe also introduce a novel flash photography dataset generated from the MIT\nintrinsic dataset.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 11:30:13 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Qian", "Yanlin", ""], ["Yan", "Song", ""], ["K\u00e4m\u00e4r\u00e4inen", "Joni-Kristian", ""], ["Matas", "Jiri", ""]]}, {"id": "1902.10467", "submitter": "Mohamed El Amine Seddik", "authors": "Mohamed El Amine Seddik, Mohamed Tamaazousti, John Lin", "title": "Generative Collaborative Networks for Single Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common issue of deep neural networks-based methods for the problem of\nSingle Image Super-Resolution (SISR), is the recovery of finer texture details\nwhen super-resolving at large upscaling factors. This issue is particularly\nrelated to the choice of the objective loss function. In particular, recent\nworks proposed the use of a VGG loss which consists in minimizing the error\nbetween the generated high resolution images and ground-truth in the feature\nspace of a Convolutional Neural Network (VGG19), pre-trained on the very\n\"large\" ImageNet dataset. When considering the problem of super-resolving\nimages with a distribution \"far\" from the ImageNet images distribution\n(\\textit{e.g.,} satellite images), their proposed \\textit{fixed} VGG loss is no\nlonger relevant. In this paper, we present a general framework named\n\\textit{Generative Collaborative Networks} (GCN), where the idea consists in\noptimizing the \\textit{generator} (the mapping of interest) in the feature\nspace of a \\textit{features extractor} network. The two networks (generator and\nextractor) are \\textit{collaborative} in the sense that the latter \"helps\" the\nformer, by constructing discriminative and relevant features (not necessarily\n\\textit{fixed} and possibly learned \\textit{mutually} with the generator). We\nevaluate the GCN framework in the context of SISR, and we show that it results\nin a method that is adapted to super-resolution domains that are \"far\" from the\nImageNet domain.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 11:34:10 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 13:07:24 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Seddik", "Mohamed El Amine", ""], ["Tamaazousti", "Mohamed", ""], ["Lin", "John", ""]]}, {"id": "1902.10471", "submitter": "Jiasong Wu", "authors": "Jiasong Wu, Fuzhi Wu, Qihan Yang, Youyong Kong, Xilin Liu, Yan Zhang,\n  Lotfi Senhadji, Huazhong Shu", "title": "Fractional spectral graph wavelets and their applications", "comments": "27 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges in the area of signal processing on graphs is to\ndesign transforms and dictionaries methods to identify and exploit structure in\nsignals on weighted graphs. In this paper, we first generalize graph Fourier\ntransform (GFT) to graph fractional Fourier transform (GFRFT), which is then\nused to define a novel transform named spectral graph fractional wavelet\ntransform (SGFRWT), which is a generalized and extended version of spectral\ngraph wavelet transform (SGWT). A fast algorithm for SGFRWT is also derived and\nimplemented based on Fourier series approximation. The potential applications\nof SGFRWT are also presented.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 11:47:41 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Wu", "Jiasong", ""], ["Wu", "Fuzhi", ""], ["Yang", "Qihan", ""], ["Kong", "Youyong", ""], ["Liu", "Xilin", ""], ["Zhang", "Yan", ""], ["Senhadji", "Lotfi", ""], ["Shu", "Huazhong", ""]]}, {"id": "1902.10528", "submitter": "Shuzhao Li", "authors": "Shuzhao Li, Huimin Yu, Wei Huang, Jing Zhang", "title": "Attributes-aided Part Detection and Refinement for Person\n  Re-identification", "comments": "10 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person attributes are often exploited as mid-level human semantic information\nto help promote the performance of person re-identification task. In this\npaper, unlike most existing methods simply taking attribute learning as a\nclassification problem, we perform it in a different way with the motivation\nthat attributes are related to specific local regions, which refers to the\nperceptual ability of attributes. We utilize the process of attribute detection\nto generate corresponding attribute-part detectors, whose invariance to many\ninfluences like poses and camera views can be guaranteed. With detected local\npart regions, our model extracts local features to handle the body part\nmisalignment problem, which is another major challenge for person\nre-identification. The local descriptors are further refined by fused attribute\ninformation to eliminate interferences caused by detection deviation. Extensive\nexperiments on two popular benchmarks with attribute annotations demonstrate\nthe effectiveness of our model and competitive performance compared with\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 13:51:12 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Li", "Shuzhao", ""], ["Yu", "Huimin", ""], ["Huang", "Wei", ""], ["Zhang", "Jing", ""]]}, {"id": "1902.10556", "submitter": "Yao Yao None", "authors": "Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang and Long Quan", "title": "Recurrent MVSNet for High-resolution Multi-view Stereo Depth Inference", "comments": "Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has recently demonstrated its excellent performance for\nmulti-view stereo (MVS). However, one major limitation of current learned MVS\napproaches is the scalability: the memory-consuming cost volume regularization\nmakes the learned MVS hard to be applied to high-resolution scenes. In this\npaper, we introduce a scalable multi-view stereo framework based on the\nrecurrent neural network. Instead of regularizing the entire 3D cost volume in\none go, the proposed Recurrent Multi-view Stereo Network (R-MVSNet)\nsequentially regularizes the 2D cost maps along the depth direction via the\ngated recurrent unit (GRU). This reduces dramatically the memory consumption\nand makes high-resolution reconstruction feasible. We first show the\nstate-of-the-art performance achieved by the proposed R-MVSNet on the recent\nMVS benchmarks. Then, we further demonstrate the scalability of the proposed\nmethod on several large-scale scenarios, where previous learned approaches\noften fail due to the memory constraint. Code is available at\nhttps://github.com/YoYo000/MVSNet.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 14:34:50 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Yao", "Yao", ""], ["Luo", "Zixin", ""], ["Li", "Shiwei", ""], ["Shen", "Tianwei", ""], ["Fang", "Tian", ""], ["Quan", "Long", ""]]}, {"id": "1902.10562", "submitter": "Younggun Cho", "authors": "Younggun Cho, Giseop Kim and Ayoung Kim", "title": "DeepLO: Geometry-Aware Deep LiDAR Odometry", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, learning-based ego-motion estimation approaches have drawn strong\ninterest from studies mostly focusing on visual perception. These\ngroundbreaking works focus on unsupervised learning for odometry estimation but\nmostly for visual sensors. Compared to images, a learning-based approach using\nLight Detection and Ranging (LiDAR) has been reported in a few studies where,\nmost often, a supervised learning framework is proposed. In this paper, we\npropose a novel approach to geometry-aware deep LiDAR odometry trainable via\nboth supervised and unsupervised frameworks. We incorporate the Iterated\nClosest Point (ICP) algorithm into a deep-learning framework and show the\nreliability of the proposed pipeline. We provide two loss functions that allow\nswitching between supervised and unsupervised learning depending on the\nground-truth validity in the training phase. An evaluation using the KITTI and\nOxford RobotCar dataset demonstrates the prominent performance and efficiency\nof the proposed method when achieving pose accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 14:48:42 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Cho", "Younggun", ""], ["Kim", "Giseop", ""], ["Kim", "Ayoung", ""]]}, {"id": "1902.10586", "submitter": "Jinyong Jeong", "authors": "Jinyong Jeong, Lucas Y. Cho, Ayoung Kim", "title": "Road is Enough! Extrinsic Calibration of Non-overlapping Stereo Camera\n  and LiDAR using Road Information", "comments": "8 pages, 11 figures, calibration paper for complex urban dataset\n  (http://irap.kaist.ac.kr/dataset)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework for the targetless extrinsic calibration of\nstereo cameras and Light Detection and Ranging (LiDAR) sensors with a\nnon-overlapping Field of View (FOV). In order to solve the extrinsic\ncalibrations problem under such challenging configuration, the proposed\nsolution exploits road markings as static and robust features among the various\ndynamic objects that are present in urban environment. First, this study\nutilizes road markings that are commonly captured by the two sensor modalities\nto select informative images for estimating the extrinsic parameters. In order\nto accomplish stable optimization, multiple cost functions are defined,\nincluding Normalized Information Distance (NID), edge alignment and, plane\nfitting cost. Therefore a smooth cost curve is formed for global optimization\nto prevent convergence to the local optimal point. We further evaluate each\ncost function by examining parameter sensitivity near the optimal point.\nAnother key characteristic of extrinsic calibration, repeatability, is analyzed\nby conducting the proposed method multiple times with varying randomly\nperturbed initial points.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 15:27:00 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 01:40:52 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Jeong", "Jinyong", ""], ["Cho", "Lucas Y.", ""], ["Kim", "Ayoung", ""]]}, {"id": "1902.10640", "submitter": "Shweta Bhardwaj", "authors": "Shweta Bhardwaj, Mukundhan Srinivasan, Mitesh M. Khapra", "title": "Efficient Video Classification Using Fewer Frames", "comments": "To Appear in Proceedings of IEEE International Conference on Computer\n  Vision and Pattern Recognition (CVPR'2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently,there has been a lot of interest in building compact models for\nvideo classification which have a small memory footprint (<1 GB). While these\nmodels are compact, they typically operate by repeated application of a small\nweight matrix to all the frames in a video. E.g. recurrent neural network based\nmethods compute a hidden state for every frame of the video using a recurrent\nweight matrix. Similarly, cluster-and-aggregate based methods such as NetVLAD,\nhave a learnable clustering matrix which is used to assign soft-clusters to\nevery frame in the video. Since these models look at every frame in the video,\nthe number of floating point operations (FLOPs) is still large even though the\nmemory footprint is small. We focus on building compute-efficient video\nclassification models which process fewer frames and hence have less number of\nFLOPs. Similar to memory efficient models, we use the idea of distillation\nalbeit in a different setting. Specifically, in our case, a compute-heavy\nteacher which looks at all the frames in the video is used to train a\ncompute-efficient student which looks at only a small fraction of frames in the\nvideo. This is in contrast to a typical memory efficient Teacher-Student\nsetting, wherein both the teacher and the student look at all the frames in the\nvideo but the student has fewer parameters. Our work thus complements the\nresearch on memory efficient video classification. We do an extensive\nevaluation with three types of models for video classification,viz.(i)\nrecurrent models (ii) cluster-and-aggregate models and (iii) memory-efficient\ncluster-and-aggregate models and show that in each of these cases, a see-it-all\nteacher can be used to train a compute efficient see-very-little student. We\nshow that the proposed student network can reduce the inference time by 30% and\nthe number of FLOPs by approximately 90% with a negligible drop in the\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 17:13:17 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Bhardwaj", "Shweta", ""], ["Srinivasan", "Mukundhan", ""], ["Khapra", "Mitesh M.", ""]]}, {"id": "1902.10657", "submitter": "Michael Burke Dr", "authors": "Michael Burke, Svetlin Penkov, Subramanian Ramamoorthy", "title": "From explanation to synthesis: Compositional program induction for\n  learning from demonstration", "comments": null, "journal-ref": "Proceedings of Robotics: Science and Systems (2019)", "doi": "10.15607/RSS.2019.XV.015", "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid systems are a compact and natural mechanism with which to address\nproblems in robotics. This work introduces an approach to learning hybrid\nsystems from demonstrations, with an emphasis on extracting models that are\nexplicitly verifiable and easily interpreted by robot operators. We fit a\nsequence of controllers using sequential importance sampling under a generative\nswitching proportional controller task model. Here, we parameterise controllers\nusing a proportional gain and a visually verifiable joint angle goal. Inference\nunder this model is challenging, but we address this by introducing an\nattribution prior extracted from a neural end-to-end visuomotor control model.\nGiven the sequence of controllers comprising a task, we simplify the trace\nusing grammar parsing strategies, taking advantage of the sequence\ncompositionality, before grounding the controllers by training perception\nnetworks to predict goals given images. Using this approach, we are\nsuccessfully able to induce a program for a visuomotor reaching task involving\nloops and conditionals from a single demonstration and a neural end-to-end\nmodel. In addition, we are able to discover the program used for a tower\nbuilding task. We argue that computer program-like control systems are more\ninterpretable than alternative end-to-end learning approaches, and that hybrid\nsystems inherently allow for better generalisation across task configurations.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 17:43:30 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 10:04:56 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Burke", "Michael", ""], ["Penkov", "Svetlin", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "1902.10658", "submitter": "Baihan Lin", "authors": "Baihan Lin", "title": "Constraining Implicit Space with Minimum Description Length: An\n  Unsupervised Attention Mechanism across Neural Network Layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the adaptation phenomenon of neuronal firing, we propose the\nregularity normalization (RN) as an unsupervised attention mechanism (UAM)\nwhich computes the statistical regularity in the implicit space of neural\nnetworks under the Minimum Description Length (MDL) principle. Treating the\nneural network optimization process as a partially observable model selection\nproblem, UAM constrains the implicit space by a normalization factor, the\nuniversal code length. We compute this universal code incrementally across\nneural network layers and demonstrated the flexibility to include data priors\nsuch as top-down attention and other oracle information. Empirically, our\napproach outperforms existing normalization methods in tackling limited,\nimbalanced and non-stationary input distribution in image classification,\nclassic control, procedurally-generated reinforcement learning, generative\nmodeling, handwriting generation and question answering tasks with various\nneural network architectures. Lastly, UAM tracks dependency and critical\nlearning stages across layers and recurrent time steps of deep networks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 17:44:50 GMT"}, {"version": "v10", "created": "Tue, 26 May 2020 01:07:00 GMT"}, {"version": "v11", "created": "Fri, 5 Jun 2020 21:49:26 GMT"}, {"version": "v12", "created": "Thu, 10 Sep 2020 08:58:16 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 04:30:58 GMT"}, {"version": "v3", "created": "Mon, 11 Mar 2019 13:06:31 GMT"}, {"version": "v4", "created": "Sat, 30 Mar 2019 21:55:07 GMT"}, {"version": "v5", "created": "Wed, 1 May 2019 18:42:40 GMT"}, {"version": "v6", "created": "Thu, 25 Jul 2019 21:47:10 GMT"}, {"version": "v7", "created": "Wed, 31 Jul 2019 15:01:28 GMT"}, {"version": "v8", "created": "Mon, 25 Nov 2019 17:03:43 GMT"}, {"version": "v9", "created": "Fri, 21 Feb 2020 20:25:26 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Lin", "Baihan", ""]]}, {"id": "1902.10707", "submitter": "Pawe{\\l} Korus", "authors": "Pawel Korus, Nasir Memon", "title": "Neural Imaging Pipelines - the Scourge or Hope of Forensics?", "comments": "Manuscript + supplement; currently under review; compressed figures\n  to minimize file size. arXiv admin note: text overlap with arXiv:1812.01516", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forensic analysis of digital photographs relies on intrinsic statistical\ntraces introduced at the time of their acquisition or subsequent editing. Such\ntraces are often removed by post-processing (e.g., down-sampling and\nre-compression applied upon distribution in the Web) which inhibits reliable\nprovenance analysis. Increasing adoption of computational methods within\ndigital cameras further complicates the process and renders explicit\nmathematical modeling infeasible. While this trend challenges forensic analysis\neven in near-acquisition conditions, it also creates new opportunities. This\npaper explores end-to-end optimization of the entire image acquisition and\ndistribution workflow to facilitate reliable forensic analysis at the end of\nthe distribution channel, where state-of-the-art forensic techniques fail. We\ndemonstrate that a neural network can be trained to replace the entire photo\ndevelopment pipeline, and jointly optimized for high-fidelity photo rendering\nand reliable provenance analysis. Such optimized neural imaging pipeline\nallowed us to increase image manipulation detection accuracy from approx. 45%\nto over 90%. The network learns to introduce carefully crafted artifacts, akin\nto digital watermarks, which facilitate subsequent manipulation detection.\nAnalysis of performance trade-offs indicates that most of the gains can be\nobtained with only minor distortion. The findings encourage further research\ntowards building more reliable imaging pipelines with explicit\nprovenance-guaranteeing properties.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 17:08:09 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Korus", "Pawel", ""], ["Memon", "Nasir", ""]]}, {"id": "1902.10733", "submitter": "Panagiotis Agrafiotis", "authors": "Panagiotis Agrafiotis, Dimitrios Skarlatos, Andreas Georgopoulos and\n  Konstantinos Karantzalos", "title": "Shallow Water Bathymetry Mapping from UAV Imagery based on Machine\n  Learning", "comments": "8 pages, 9 figures", "journal-ref": "Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., XLII-2/W10,\n  9-16, 2019", "doi": "10.5194/isprs-archives-XLII-2-W10-9-2019", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The determination of accurate bathymetric information is a key element for\nnear offshore activities, hydrological studies such as coastal engineering\napplications, sedimentary processes, hydrographic surveying as well as\narchaeological mapping and biological research. UAV imagery processed with\nStructure from Motion (SfM) and Multi View Stereo (MVS) techniques can provide\na low-cost alternative to established shallow seabed mapping techniques\noffering as well the important visual information. Nevertheless, water\nrefraction poses significant challenges on depth determination. Till now, this\nproblem has been addressed through customized image-based refraction correction\nalgorithms or by modifying the collinearity equation. In this paper, in order\nto overcome the water refraction errors, we employ machine learning tools that\nare able to learn the systematic underestimation of the estimated depths. In\nthe proposed approach, based on known depth observations from bathymetric LiDAR\nsurveys, an SVR model was developed able to estimate more accurately the real\ndepths of point clouds derived from SfM-MVS procedures. Experimental results\nover two test sites along with the performed quantitative validation indicated\nthe high potential of the developed approach.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 19:09:13 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 09:17:04 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 15:13:29 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Agrafiotis", "Panagiotis", ""], ["Skarlatos", "Dimitrios", ""], ["Georgopoulos", "Andreas", ""], ["Karantzalos", "Konstantinos", ""]]}, {"id": "1902.10739", "submitter": "John Leuner", "authors": "John Leuner", "title": "A Replication Study: Machine Learning Models Are Capable of Predicting\n  Sexual Orientation From Facial Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research used machine learning methods to predict a person's sexual\norientation from their photograph (Wang and Kosinski, 2017). To verify this\nresult, two of these models are replicated, one based on a deep neural network\n(DNN) and one on facial morphology (FM). Using a new dataset of 20,910\nphotographs from dating websites, the ability to predict sexual orientation is\nconfirmed (DNN accuracy male 68%, female 77%, FM male 62%, female 72%). To\ninvestigate whether facial features such as brightness or predominant colours\nare predictive of sexual orientation, a new model based on highly blurred\nfacial images was created. This model was also able to predict sexual\norientation (male 63%, female 72%). The tested models are invariant to\nintentional changes to a subject's makeup, eyewear, facial hair and head pose\n(angle that the photograph is taken at). It is shown that the head pose is not\ncorrelated with sexual orientation. While demonstrating that dating profile\nimages carry rich information about sexual orientation these results leave open\nthe question of how much is determined by facial morphology and how much by\ndifferences in grooming, presentation and lifestyle. The advent of new\ntechnology that is able to detect sexual orientation in this way may have\nserious implications for the privacy and safety of gay men and women.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 19:24:41 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Leuner", "John", ""]]}, {"id": "1902.10740", "submitter": "Wenbo Li", "authors": "Wenbo Li, Pengchuan Zhang, Lei Zhang, Qiuyuan Huang, Xiaodong He,\n  Siwei Lyu, Jianfeng Gao", "title": "Object-driven Text-to-Image Synthesis via Adversarial Training", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Object-driven Attentive Generative Adversarial\nNewtorks (Obj-GANs) that allow object-centered text-to-image synthesis for\ncomplex scenes. Following the two-step (layout-image) generation process, a\nnovel object-driven attentive image generator is proposed to synthesize salient\nobjects by paying attention to the most relevant words in the text description\nand the pre-generated semantic layout. In addition, a new Fast R-CNN based\nobject-wise discriminator is proposed to provide rich object-wise\ndiscrimination signals on whether the synthesized object matches the text\ndescription and the pre-generated layout. The proposed Obj-GAN significantly\noutperforms the previous state of the art in various metrics on the large-scale\nCOCO benchmark, increasing the Inception score by 27% and decreasing the FID\nscore by 11%. A thorough comparison between the traditional grid attention and\nthe new object-driven attention is provided through analyzing their mechanisms\nand visualizing their attention layers, showing insights of how the proposed\nmodel generates complex scenes in high quality.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 19:25:52 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Li", "Wenbo", ""], ["Zhang", "Pengchuan", ""], ["Zhang", "Lei", ""], ["Huang", "Qiuyuan", ""], ["He", "Xiaodong", ""], ["Lyu", "Siwei", ""], ["Gao", "Jianfeng", ""]]}, {"id": "1902.10744", "submitter": "Noranart Vesdapunt", "authors": "Bindita Chaudhuri, Noranart Vesdapunt, Baoyuan Wang", "title": "Joint Face Detection and Facial Motion Retargeting for Multiple Faces", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial motion retargeting is an important problem in both computer graphics\nand vision, which involves capturing the performance of a human face and\ntransferring it to another 3D character. Learning 3D morphable model (3DMM)\nparameters from 2D face images using convolutional neural networks is common in\n2D face alignment, 3D face reconstruction etc. However, existing methods either\nrequire an additional face detection step before retargeting or use a cascade\nof separate networks to perform detection followed by retargeting in a\nsequence. In this paper, we present a single end-to-end network to jointly\npredict the bounding box locations and 3DMM parameters for multiple faces.\nFirst, we design a novel multitask learning framework that learns a\ndisentangled representation of 3DMM parameters for a single face. Then, we\nleverage the trained single face model to generate ground truth 3DMM parameters\nfor multiple faces to train another network that performs joint face detection\nand motion retargeting for images with multiple faces. Experimental results\nshow that our joint detection and retargeting network has high face detection\naccuracy and is robust to extreme expressions and poses while being faster than\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 19:29:42 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Chaudhuri", "Bindita", ""], ["Vesdapunt", "Noranart", ""], ["Wang", "Baoyuan", ""]]}, {"id": "1902.10747", "submitter": "Mikael Brudfors", "authors": "Mikael Brudfors, Ya\\\"el Balbastre, John Ashburner", "title": "Nonlinear Markov Random Fields Learned via Backpropagation", "comments": "Accepted for the international conference on Information Processing\n  in Medical Imaging (IPMI) 2019, camera ready version", "journal-ref": null, "doi": "10.1007/978-3-030-20351-1_63", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although convolutional neural networks (CNNs) currently dominate competitions\non image segmentation, for neuroimaging analysis tasks, more classical\ngenerative approaches based on mixture models are still used in practice to\nparcellate brains. To bridge the gap between the two, in this paper we propose\na marriage between a probabilistic generative model, which has been shown to be\nrobust to variability among magnetic resonance (MR) images acquired via\ndifferent imaging protocols, and a CNN. The link is in the prior distribution\nover the unknown tissue classes, which are classically modelled using a Markov\nrandom field. In this work we model the interactions among neighbouring pixels\nby a type of recurrent CNN, which can encode more complex spatial interactions.\nWe validate our proposed model on publicly available MR data, from different\ncentres, and show that it generalises across imaging protocols. This result\ndemonstrates a successful and principled inclusion of a CNN in a generative\nmodel, which in turn could be adapted by any probabilistic generative approach\nfor image segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 19:34:22 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 12:34:18 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Brudfors", "Mikael", ""], ["Balbastre", "Ya\u00ebl", ""], ["Ashburner", "John", ""]]}, {"id": "1902.10785", "submitter": "Ruizhi Liao", "authors": "Ruizhi Liao, Jonathan Rubin, Grace Lam, Seth Berkowitz, Sandeep Dalal,\n  William Wells, Steven Horng, Polina Golland", "title": "Semi-supervised Learning for Quantification of Pulmonary Edema in Chest\n  X-Ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and demonstrate machine learning algorithms to assess the severity\nof pulmonary edema in chest x-ray images of congestive heart failure patients.\nAccurate assessment of pulmonary edema in heart failure is critical when making\ntreatment and disposition decisions. Our work is grounded in a large-scale\nclinical dataset of over 300,000 x-ray images with associated radiology\nreports. While edema severity labels can be extracted unambiguously from a\nsmall fraction of the radiology reports, accurate annotation is challenging in\nmost cases. To take advantage of the unlabeled images, we develop a Bayesian\nmodel that includes a variational auto-encoder for learning a latent\nrepresentation from the entire image set trained jointly with a regressor that\nemploys this representation for predicting pulmonary edema severity. Our\nexperimental results suggest that modeling the distribution of images jointly\nwith the limited labels improves the accuracy of pulmonary edema scoring\ncompared to a strictly supervised approach. To the best of our knowledge, this\nis the first attempt to employ machine learning algorithms to automatically and\nquantitatively assess the severity of pulmonary edema in chest x-ray images.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 21:03:40 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 14:27:07 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 01:47:21 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Liao", "Ruizhi", ""], ["Rubin", "Jonathan", ""], ["Lam", "Grace", ""], ["Berkowitz", "Seth", ""], ["Dalal", "Sandeep", ""], ["Wells", "William", ""], ["Horng", "Steven", ""], ["Golland", "Polina", ""]]}, {"id": "1902.10796", "submitter": "Ashwini Tonge", "authors": "Ashwini Tonge and Cornelia Caragea", "title": "Dynamic Deep Multi-modal Fusion for Image Privacy Prediction", "comments": "Accepted by The Web Conference (WWW) 2019", "journal-ref": null, "doi": "10.1145/3308558.3313691", "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With millions of images that are shared online on social networking sites,\neffective methods for image privacy prediction are highly needed. In this\npaper, we propose an approach for fusing object, scene context, and image tags\nmodalities derived from convolutional neural networks for accurately predicting\nthe privacy of images shared online. Specifically, our approach identifies the\nset of most competent modalities on the fly, according to each new target image\nwhose privacy has to be predicted. The approach considers three stages to\npredict the privacy of a target image, wherein we first identify the\nneighborhood images that are visually similar and/or have similar sensitive\ncontent as the target image. Then, we estimate the competence of the modalities\nbased on the neighborhood images. Finally, we fuse the decisions of the most\ncompetent modalities and predict the privacy label for the target image.\nExperimental results show that our approach predicts the sensitive (or private)\ncontent more accurately than the models trained on individual modalities\n(object, scene, and tags) and prior privacy prediction works. Also, our\napproach outperforms strong baselines, that train meta-classifiers to obtain an\noptimal combination of modalities.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 21:42:08 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 15:54:24 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Tonge", "Ashwini", ""], ["Caragea", "Cornelia", ""]]}, {"id": "1902.10811", "submitter": "Ludwig Schmidt", "authors": "Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar", "title": "Do ImageNet Classifiers Generalize to ImageNet?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build new test sets for the CIFAR-10 and ImageNet datasets. Both\nbenchmarks have been the focus of intense research for almost a decade, raising\nthe danger of overfitting to excessively re-used test sets. By closely\nfollowing the original dataset creation processes, we test to what extent\ncurrent classification models generalize to new data. We evaluate a broad range\nof models and find accuracy drops of 3% - 15% on CIFAR-10 and 11% - 14% on\nImageNet. However, accuracy gains on the original test sets translate to larger\ngains on the new test sets. Our results suggest that the accuracy drops are not\ncaused by adaptivity, but by the models' inability to generalize to slightly\n\"harder\" images than those found in the original test sets.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 20:35:44 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 17:42:33 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Recht", "Benjamin", ""], ["Roelofs", "Rebecca", ""], ["Schmidt", "Ludwig", ""], ["Shankar", "Vaishaal", ""]]}, {"id": "1902.10814", "submitter": "Chun-Ta Lu", "authors": "Da-Cheng Juan, Chun-Ta Lu, Zhen Li, Futang Peng, Aleksei Timofeev,\n  Yi-Ting Chen, Yaxi Gao, Tom Duerig, Andrew Tomkins, Sujith Ravi", "title": "Graph-RISE: Graph-Regularized Image Semantic Embedding", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning image representations to capture fine-grained semantics has been a\nchallenging and important task enabling many applications such as image search\nand clustering. In this paper, we present Graph-Regularized Image Semantic\nEmbedding (Graph-RISE), a large-scale neural graph learning framework that\nallows us to train embeddings to discriminate an unprecedented O(40M)\nultra-fine-grained semantic labels. Graph-RISE outperforms state-of-the-art\nimage embedding algorithms on several evaluation tasks, including image\nclassification and triplet ranking. We provide case studies to demonstrate\nthat, qualitatively, image retrieval based on Graph-RISE effectively captures\nsemantics and, compared to the state-of-the-art, differentiates nuances at\nlevels that are closer to human-perception.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 04:55:28 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Juan", "Da-Cheng", ""], ["Lu", "Chun-Ta", ""], ["Li", "Zhen", ""], ["Peng", "Futang", ""], ["Timofeev", "Aleksei", ""], ["Chen", "Yi-Ting", ""], ["Gao", "Yaxi", ""], ["Duerig", "Tom", ""], ["Tomkins", "Andrew", ""], ["Ravi", "Sujith", ""]]}, {"id": "1902.10815", "submitter": "Cheng Ouyang", "authors": "Cheng Ouyang, Jo Schlemper, Carlo Biffi, Gavin Seegoolam, Jose\n  Caballero, Anthony N. Price, Joseph V. Hajnal, Daniel Rueckert", "title": "Generalising Deep Learning MRI Reconstruction across Different Domains", "comments": "Accepted for ISBI2019 as a 1-page abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We look into robustness of deep learning based MRI reconstruction when tested\non unseen contrasts and organs. We then propose to generalise the network by\ntraining with large publicly-available natural image datasets with synthesised\nphase information to achieve high cross-domain reconstruction performance which\nis competitive with domain-specific training. To explain its generalisation\nmechanism, we have also analysed patch sets for different training datasets.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 12:08:33 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Ouyang", "Cheng", ""], ["Schlemper", "Jo", ""], ["Biffi", "Carlo", ""], ["Seegoolam", "Gavin", ""], ["Caballero", "Jose", ""], ["Price", "Anthony N.", ""], ["Hajnal", "Joseph V.", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1902.10816", "submitter": "Ali Lenjani", "authors": "Chul Min Yeum, Ali Lenjani, Shirley J. Dyke, Ilias Bilionis", "title": "Automated Detection of Pre-Disaster Building Images from Google Street\n  View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After a disaster, teams of structural engineers collect vast amounts of\nimages from damaged buildings to obtain lessons and gain knowledge from the\nevent. Images of damaged buildings and components provide valuable evidence to\nunderstand the consequences on our structures. However, in many cases, images\nof damaged buildings are often captured without sufficient spatial context.\nAlso, they may be hard to recognize in cases with severe damage. Incorporating\npast images showing a pre-disaster condition of such buildings is helpful to\naccurately evaluate possible circumstances related to a building's failure. One\nof the best resources to observe the pre-disaster condition of the buildings is\nGoogle Street View. A sequence of 360 panorama images which are captured along\nstreets enables all-around views at each location on the street. Once a user\nknows the GPS information near the building, all external views of the building\ncan be made available. In this study, we develop an automated technique to\nextract past building images from 360 panorama images serviced by Google Street\nView. Users only need to provide a geo-tagged image, collected near the target\nbuilding, and the rest of the process is fully automated. High-quality and\nundistorted building images are extracted from past panoramas. Since the\npanoramas are collected from various locations near the building along the\nstreet, the user can identify its pre-disaster conditions from the full set of\nexternal views.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 01:14:23 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Yeum", "Chul Min", ""], ["Lenjani", "Ali", ""], ["Dyke", "Shirley J.", ""], ["Bilionis", "Ilias", ""]]}, {"id": "1902.10840", "submitter": "Chen Kong", "authors": "Chen Kong and Simon Lucey", "title": "Deep Interpretable Non-Rigid Structure from Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All current non-rigid structure from motion (NRSfM) algorithms are limited\nwith respect to: (i) the number of images, and (ii) the type of shape\nvariability they can handle. This has hampered the practical utility of NRSfM\nfor many applications within vision. In this paper we propose a novel deep\nneural network to recover camera poses and 3D points solely from an ensemble of\n2D image coordinates. The proposed neural network is mathematically\ninterpretable as a multi-layer block sparse dictionary learning problem, and\ncan handle problems of unprecedented scale and shape complexity. Extensive\nexperiments demonstrate the impressive performance of our approach where we\nexhibit superior precision and robustness against all available\nstate-of-the-art works. The considerable model capacity of our approach affords\nremarkable generalization to unseen data. We propose a quality measure (based\non the network weights) which circumvents the need for 3D ground-truth to\nascertain the confidence we have in the reconstruction. Once the network's\nweights are estimated (for a non-rigid object) we show how our approach can\neffectively recover 3D shape from a single image -- outperforming comparable\nmethods that rely on direct 3D supervision.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 00:04:24 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Kong", "Chen", ""], ["Lucey", "Simon", ""]]}, {"id": "1902.10847", "submitter": "Olga Moskvyak", "authors": "Olga Moskvyak, Frederic Maire, Asia O. Armstrong, Feras Dayoub, Mahsa\n  Baktashmotlagh", "title": "Robust Re-identification of Manta Rays from Natural Markings by Learning\n  Pose Invariant Embeddings", "comments": "12 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual identification of individual animals that bear unique natural body\nmarkings is an important task in wildlife conservation. The photo databases of\nanimal markings grow larger and each new observation has to be matched against\nthousands of images. Existing photo-identification solutions have constraints\non image quality and appearance of the pattern of interest in the image. These\nconstraints limit the use of photos from citizen scientists. We present a novel\nsystem for visual re-identification based on unique natural markings that is\nrobust to occlusions, viewpoint and illumination changes. We adapt methods\ndeveloped for face re-identification and implement a deep convolutional neural\nnetwork (CNN) to learn embeddings for images of natural markings. The distance\nbetween the learned embedding points provides a dissimilarity measure between\nthe corresponding input images. The network is optimized using the triplet loss\nfunction and the online semi-hard triplet mining strategy. The proposed\nre-identification method is generic and not species specific. We evaluate the\nproposed system on image databases of manta ray belly patterns and humpback\nwhale flukes. To be of practical value and adopted by marine biologists, a\nre-identification system needs to have a top-10 accuracy of at least 95%. The\nproposed system achieves this performance standard.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 00:37:32 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Moskvyak", "Olga", ""], ["Maire", "Frederic", ""], ["Armstrong", "Asia O.", ""], ["Dayoub", "Feras", ""], ["Baktashmotlagh", "Mahsa", ""]]}, {"id": "1902.10848", "submitter": "Sara Mousavi", "authors": "Sara Mousavi, Ramin Nabati, Megan Kleeschulte, Audris Mockus", "title": "Machine-assisted annotation of forensic imagery", "comments": "Submitted to ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image collections, if critical aspects of image content are exposed, can spur\nresearch and practical applications in many domains. Supervised machine\nlearning may be the only feasible way to annotate very large collections, but\nleading approaches rely on large samples of completely and accurately annotated\nimages. In the case of a large forensic collection, we are aiming to annotate,\nneither the complete annotation nor the large training samples can be feasibly\nproduced. We, therefore, investigate ways to assist manual annotation efforts\ndone by forensic experts. We present a method that can propose both images and\nareas within an image likely to contain desired classes. Evaluation of the\nmethod with human annotators showed highly accurate classification that was\nstrongly helped by transfer learning. The segmentation precision (mAP) was\nimproved by adding a separate class capturing background, but that did not\naffect the recall (mAR). Further work is needed to both increase the accuracy\nof segmentation and enhances prediction with additional covariates affecting\ndecomposition. We hope this effort to be of help in other domains that require\nweak segmentation and have limited availability of qualified annotators.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 00:39:53 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Mousavi", "Sara", ""], ["Nabati", "Ramin", ""], ["Kleeschulte", "Megan", ""], ["Mockus", "Audris", ""]]}, {"id": "1902.10858", "submitter": "Renlong Hang", "authors": "Renlong Hang, Qingshan Liu, Danfeng Hong, and Pedram Ghamisi", "title": "Cascaded Recurrent Neural Networks for Hyperspectral Image\n  Classification", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2019.2899129", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By considering the spectral signature as a sequence, recurrent neural\nnetworks (RNNs) have been successfully used to learn discriminative features\nfrom hyperspectral images (HSIs) recently. However, most of these models only\ninput the whole spectral bands into RNNs directly, which may not fully explore\nthe specific properties of HSIs. In this paper, we propose a cascaded RNN model\nusing gated recurrent units (GRUs) to explore the redundant and complementary\ninformation of HSIs. It mainly consists of two RNN layers. The first RNN layer\nis used to eliminate redundant information between adjacent spectral bands,\nwhile the second RNN layer aims to learn the complementary information from\nnon-adjacent spectral bands. To improve the discriminative ability of the\nlearned features, we design two strategies for the proposed model. Besides,\nconsidering the rich spatial information contained in HSIs, we further extend\nthe proposed model to its spectral-spatial counterpart by incorporating some\nconvolutional layers. To test the effectiveness of our proposed models, we\nconduct experiments on two widely used HSIs. The experimental results show that\nour proposed models can achieve better results than the compared models.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 01:29:59 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Hang", "Renlong", ""], ["Liu", "Qingshan", ""], ["Hong", "Danfeng", ""], ["Ghamisi", "Pedram", ""]]}, {"id": "1902.10859", "submitter": "Xiaojie Guo", "authors": "Xiaojie Guo, Siyuan Li, Jinke Yu, Jiawan Zhang, Jiayi Ma, Lin Ma, Wei\n  Liu, and Haibin Ling", "title": "PFLD: A Practical Facial Landmark Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being accurate, efficient, and compact is essential to a facial landmark\ndetector for practical use. To simultaneously consider the three concerns, this\npaper investigates a neat model with promising detection accuracy under wild\nenvironments e.g., unconstrained pose, expression, lighting, and occlusion\nconditions) and super real-time speed on a mobile device. More concretely, we\ncustomize an end-to-end single stage network associated with acceleration\ntechniques. During the training phase, for each sample, rotation information is\nestimated for geometrically regularizing landmark localization, which is then\nNOT involved in the testing phase. A novel loss is designed to, besides\nconsidering the geometrical regularization, mitigate the issue of data\nimbalance by adjusting weights of samples to different states, such as large\npose, extreme lighting, and occlusion, in the training set. Extensive\nexperiments are conducted to demonstrate the efficacy of our design and reveal\nits superior performance over state-of-the-art alternatives on widely-adopted\nchallenging benchmarks, i.e., 300W (including iBUG, LFPW, AFW, HELEN, and\nXM2VTS) and AFLW. Our model can be merely 2.1Mb of size and reach over 140 fps\nper face on a mobile phone (Qualcomm ARM 845 processor) with high precision,\nmaking it attractive for large-scale or real-time applications. We have made\nour practical system based on PFLD 0.25X model publicly available at\n\\url{http://sites.google.com/view/xjguo/fld} for encouraging comparisons and\nimprovements from the community.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 01:45:55 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 05:37:17 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Guo", "Xiaojie", ""], ["Li", "Siyuan", ""], ["Yu", "Jinke", ""], ["Zhang", "Jiawan", ""], ["Ma", "Jiayi", ""], ["Ma", "Lin", ""], ["Liu", "Wei", ""], ["Ling", "Haibin", ""]]}, {"id": "1902.10885", "submitter": "Hemalatha M", "authors": "Anubha Pearline.S and Hemalatha.M", "title": "Face Recognition Under Varying Blur, Illumination and Expression in an\n  Unconstrained Environment", "comments": null, "journal-ref": "Special Issue International Journal of Computer Science and\n  Information Security (IJCSIS) 2016", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition system is one of the esteemed research areas in pattern\nrecognition and computer vision as long as its major challenges. A few\nchallenges in recognizing faces are blur, illumination, and varied expressions.\nBlur is natural while taking photographs using cameras, mobile phones, etc.\nBlur can be uniform and non-uniform. Usually non-uniform blur happens in images\ntaken using handheld image devices. Distinguishing or handling a blurred image\nin a face recognition system is generally tough. Under varying lighting\nconditions, it is challenging to identify the person correctly. Diversified\nfacial expressions such as happiness, sad, surprise, fear, anger changes or\ndeforms the faces from normal images. Identifying faces with facial expressions\nis also a challenging task, due to the deformation caused by the facial\nexpressions. To solve these issues, a pre-processing step was carried out after\nwhich Blur and Illumination-Robust Face recognition (BIRFR) algorithm was\nperformed. The test image and training images with facial expression are\ntransformed to neutral face using Facial expression removal (FER) peration.\nEvery training image is transformed based on the optimal Transformation Spread\nFunction (TSF), and illumination coefficients. Local Binary Pattern (LBP)\nfeatures extracted from test image and transformed training image is used for\nclassification.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 04:18:28 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["S", "Anubha Pearline.", ""], ["M", "Hemalatha.", ""]]}, {"id": "1902.10887", "submitter": "Jingfeng Zhang", "authors": "Jingfeng Zhang, Bo Han, Laura Wynter, Kian Hsiang Low, and Mohan\n  Kankanhalli", "title": "Towards Robust ResNet: A Small Step but A Giant Leap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple yet principled approach to boosting the\nrobustness of the residual network (ResNet) that is motivated by the dynamical\nsystem perspective. Namely, a deep neural network can be interpreted using a\npartial differential equation, which naturally inspires us to characterize\nResNet by an explicit Euler method. Our analytical studies reveal that the step\nfactor h in the Euler method is able to control the robustness of ResNet in\nboth its training and generalization. Specifically, we prove that a small step\nfactor h can benefit the training robustness for back-propagation; from the\nview of forward-propagation, a small h can aid in the robustness of the model\ngeneralization. A comprehensive empirical evaluation on both vision CIFAR-10\nand text AG-NEWS datasets confirms that a small h aids both the training and\ngeneralization robustness.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 04:24:46 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 01:17:30 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2019 03:02:31 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Zhang", "Jingfeng", ""], ["Han", "Bo", ""], ["Wynter", "Laura", ""], ["Low", "Kian Hsiang", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "1902.10892", "submitter": "Young-Sik Shin", "authors": "Young-Sik Shin and Ayoung Kim", "title": "Sparse Depth Enhanced Direct Thermal-infrared SLAM Beyond the Visible\n  Spectrum", "comments": "8 pages, 7 figures, Submitted to 2019 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2019) and IEEE Robotics\n  and Automation Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a thermal-infrared simultaneous localization and\nmapping (SLAM) system enhanced by sparse depth measurements from Light\nDetection and Ranging (LiDAR). Thermal-infrared cameras are relatively robust\nagainst fog, smoke, and dynamic lighting conditions compared to RGB cameras\noperating under the visible spectrum. Due to the advantages of thermal-infrared\ncameras, exploiting them for motion estimation and mapping is highly appealing.\nHowever, operating a thermal-infrared camera directly in existing vision-based\nmethods is difficult because of the modality difference. This paper proposes a\nmethod to use sparse depth measurement for 6-DOF motion estimation by directly\ntracking under 14- bit raw measurement of the thermal camera. In addition, we\nperform a refinement to improve the local accuracy and include a loop closure\nto maintain global consistency. The experimental results demonstrate that the\nsystem is not only robust under various lighting conditions such as day and\nnight, but also overcomes the scale problem of monocular cameras. The video is\navailable at https://youtu.be/oO7lT3uAzLc.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 05:05:05 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Shin", "Young-Sik", ""], ["Kim", "Ayoung", ""]]}, {"id": "1902.10895", "submitter": "Jordan Malof", "authors": "Jordan M. Malof, Boning Li, Bohao Huang, Kyle Bradbury, and Artem\n  Stretslov", "title": "Mapping solar array location, size, and capacity using deep learning and\n  overhead imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effective integration of distributed solar photovoltaic (PV) arrays into\nexisting power grids will require access to high quality data; the location,\npower capacity, and energy generation of individual solar PV installations.\nUnfortunately, existing methods for obtaining this data are limited in their\nspatial resolution and completeness. We propose a general framework for\naccurately and cheaply mapping individual PV arrays, and their capacities, over\nlarge geographic areas. At the core of this approach is a deep learning\nalgorithm called SolarMapper - which we make publicly available - that can\nautomatically map PV arrays in high resolution overhead imagery. We estimate\nthe performance of SolarMapper on a large dataset of overhead imagery across\nthree US cities in California. We also describe a procedure for deploying\nSolarMapper to new geographic regions, so that it can be utilized by others. We\ndemonstrate the effectiveness of the proposed deployment procedure by using it\nto map solar arrays across the entire US state of Connecticut (CT). Using these\nresults, we demonstrate that we achieve highly accurate estimates of total\ninstalled PV capacity within each of CT's 168 municipal regions.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 05:10:08 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Malof", "Jordan M.", ""], ["Li", "Boning", ""], ["Huang", "Bohao", ""], ["Bradbury", "Kyle", ""], ["Stretslov", "Artem", ""]]}, {"id": "1902.10899", "submitter": "Jiancheng Yang", "authors": "Jiancheng Yang, Qiang Zhang, Rongyao Fang, Bingbing Ni, Jinxian Liu,\n  Qi Tian", "title": "Adversarial Attack and Defense on Point Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emergence of the utility of 3D point cloud data in safety-critical vision\ntasks (e.g., ADAS) urges researchers to pay more attention to the robustness of\n3D representations and deep networks. To this end, we develop an attack and\ndefense scheme, dedicated to 3D point cloud data, for preventing 3D point\nclouds from manipulated as well as pursuing noise-tolerable 3D representation.\nA set of novel 3D point cloud attack operations are proposed via pointwise\ngradient perturbation and adversarial point attachment / detachment. We then\ndevelop a flexible perturbation-measurement scheme for 3D point cloud data to\ndetect potential attack data or noisy sensing data. Notably, the proposed\ndefense methods are even effective to detect the adversarial point clouds\ngenerated by a proof-of-concept attack directly targeting the defense.\nTransferability of adversarial attacks between several point cloud networks is\naddressed, and we propose an momentum-enhanced pointwise gradient to improve\nthe attack transferability. We further analyze the transferability from\nadversarial point clouds to grid CNNs and the inverse. Extensive experimental\nresults on common point cloud benchmarks demonstrate the validity of the\nproposed 3D attack and defense framework.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 05:27:40 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 16:31:38 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2020 16:13:03 GMT"}, {"version": "v4", "created": "Mon, 31 May 2021 16:03:23 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Yang", "Jiancheng", ""], ["Zhang", "Qiang", ""], ["Fang", "Rongyao", ""], ["Ni", "Bingbing", ""], ["Liu", "Jinxian", ""], ["Tian", "Qi", ""]]}, {"id": "1902.10903", "submitter": "Jianzhong He", "authors": "Jianzhong He, Shiliang Zhang, Ming Yang, Yanhu Shan, Tiejun Huang", "title": "Bi-Directional Cascade Network for Perceptual Edge Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting multi-scale representations is critical to improve edge detection\nfor objects at different scales. To extract edges at dramatically different\nscales, we propose a Bi-Directional Cascade Network (BDCN) structure, where an\nindividual layer is supervised by labeled edges at its specific scale, rather\nthan directly applying the same supervision to all CNN outputs. Furthermore, to\nenrich multi-scale representations learned by BDCN, we introduce a Scale\nEnhancement Module (SEM) which utilizes dilated convolution to generate\nmulti-scale features, instead of using deeper CNNs or explicitly fusing\nmulti-scale edge maps. These new approaches encourage the learning of\nmulti-scale representations in different layers and detect edges that are well\ndelineated by their scales. Learning scale dedicated layers also results in\ncompact network with a fraction of parameters. We evaluate our method on three\ndatasets, i.e., BSDS500, NYUDv2, and Multicue, and achieve ODS Fmeasure of\n0.828, 1.3% higher than current state-of-the art on BSDS500. The code has been\navailable at https://github.com/pkuCactus/BDCN.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 05:35:15 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["He", "Jianzhong", ""], ["Zhang", "Shiliang", ""], ["Yang", "Ming", ""], ["Shan", "Yanhu", ""], ["Huang", "Tiejun", ""]]}, {"id": "1902.10904", "submitter": "Changhee Won", "authors": "Changhee Won, Jongbin Ryu and Jongwoo Lim", "title": "SweepNet: Wide-baseline Omnidirectional Depth Estimation", "comments": "Accepted by ICRA 2019", "journal-ref": null, "doi": "10.1109/ICRA.2019.8793823", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omnidirectional depth sensing has its advantage over the conventional stereo\nsystems since it enables us to recognize the objects of interest in all\ndirections without any blind regions. In this paper, we propose a novel\nwide-baseline omnidirectional stereo algorithm which computes the dense depth\nestimate from the fisheye images using a deep convolutional neural network. The\ncapture system consists of multiple cameras mounted on a wide-baseline rig with\nultrawide field of view (FOV) lenses, and we present the calibration algorithm\nfor the extrinsic parameters based on the bundle adjustment. Instead of\nestimating depth maps from multiple sets of rectified images and stitching\nthem, our approach directly generates one dense omnidirectional depth map with\nfull 360-degree coverage at the rig global coordinate system. To this end, the\nproposed neural network is designed to output the cost volume from the warped\nimages in the sphere sweeping method, and the final depth map is estimated by\ntaking the minimum cost indices of the aggregated cost volume by SGM. For\ntraining the deep neural network and testing the entire system, realistic\nsynthetic urban datasets are rendered using Blender. The experiments using the\nsynthetic and real-world datasets show that our algorithm outperforms the\nconventional depth estimation methods and generate highly accurate depth maps.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 05:36:19 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 17:07:50 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Won", "Changhee", ""], ["Ryu", "Jongbin", ""], ["Lim", "Jongwoo", ""]]}, {"id": "1902.10905", "submitter": "Dahuin Jung", "authors": "Dahuin Jung, Ho Bae, Hyun-Soo Choi, Sungroh Yoon", "title": "PixelSteganalysis: Pixel-wise Hidden Information Removal with Low Visual\n  Degradation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is difficult to detect and remove secret images that are hidden in natural\nimages using deep-learning algorithms. Our technique is the first work to\neffectively disable covert communications and transactions that use\ndeep-learning steganography. We address the problem by exploiting sophisticated\npixel distributions and edge areas of images using a deep neural network. Based\non the given information, we adaptively remove secret information at the pixel\nlevel. We also introduce a new quantitative metric called destruction rate\nsince the decoding method of deep-learning steganography is approximate\n(lossy), which is different from conventional steganography. We evaluate our\ntechnique using three public benchmarks in comparison with conventional\nsteganalysis methods and show that the decoding rate improves by 10 ~ 20%.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 05:36:29 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Jung", "Dahuin", ""], ["Bae", "Ho", ""], ["Choi", "Hyun-Soo", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1902.10938", "submitter": "Yongqing Huo", "authors": "Yongqing Huo and Xiaofeng Zhu", "title": "High dynamic range image forensics using cnn", "comments": "6 pages,4 figures,3 tables,conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dynamic range (HDR) imaging has recently drawn much attention in\nmultimedia community. In this paper, we proposed a HDR image forensics method\nbased on convolutional neural network (CNN).To our best knowledge, this is the\nfirst time to apply deep learning method on HDR image forensics. The proposed\nalgorithm uses CNN to distinguish HDR images generated by multiple low dynamic\nrange (LDR) images from that expanded by single LDR image using inverse tone\nmapping (iTM). To do this, we learn the change of statistical characteristics\nextracted by the proposed CNN architectures and classify two kinds of HDR\nimages. Comparision results with some traditional statistical characteristics\nshows efficiency of the proposed method in HDR image source identification.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 07:53:42 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Huo", "Yongqing", ""], ["Zhu", "Xiaofeng", ""]]}, {"id": "1902.10943", "submitter": "Yongqing Huo", "authors": "Wei Gao, Yongqing Huo, Yan Qiao", "title": "A security steganography scheme based on hdr image", "comments": "5 pages,6 figures,1 table,conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely recognized that the image format is crucial to steganography for\nthat each individual format has its unique properities. Nowadays, the most\nfamous approach of digital image steganography is to combine a well-defined\ndistortion function with efficient practical codes such as STC. And numerous\nresearches are concentrated on spatial domain and jpeg domain. However, whether\nin spatial domain or jpeg domain, high payload (e.g., 0.5 bit per pixel) is not\nsecure enough. In this paper, we propose a novel adaptive steganography scheme\nbased on 32-bit HDR (High dynamic range) format and Norm IEEE 754. Experiments\nshow that the steganographic method can achieve satisfactory security under\npayload from 0.3bpp to 0.5bpp.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 08:05:33 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Gao", "Wei", ""], ["Huo", "Yongqing", ""], ["Qiao", "Yan", ""]]}, {"id": "1902.10946", "submitter": "Bolei Xu", "authors": "Bolei Xu, Jingxin Liu, Xianxu Hou, Bozhi Liu, Jon Garibaldi, Ian O.\n  Ellis, Andy Green, Linlin Shen, Guoping Qiu", "title": "Look, Investigate, and Classify: A Deep Hybrid Attention Method for\n  Breast Cancer Classification", "comments": "Accepted to ISBI'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One issue with computer based histopathology image analysis is that the size\nof the raw image is usually very large. Taking the raw image as input to the\ndeep learning model would be computationally expensive while resizing the raw\nimage to low resolution would incur information loss. In this paper, we present\na novel deep hybrid attention approach to breast cancer classification. It\nfirst adaptively selects a sequence of coarse regions from the raw image by a\nhard visual attention algorithm, and then for each such region it is able to\ninvestigate the abnormal parts based on a soft-attention mechanism. A recurrent\nnetwork is then built to make decisions to classify the image region and also\nto predict the location of the image region to be investigated at the next time\nstep. As the region selection process is non-differentiable, we optimize the\nwhole network through a reinforcement approach to learn an optimal policy to\nclassify the regions. Based on this novel Look, Investigate and Classify\napproach, we only need to process a fraction of the pixels in the raw image\nresulting in significant saving in computational resources without sacrificing\nperformances. Our approach is evaluated on a public breast cancer\nhistopathology database, where it demonstrates superior performance to the\nstate-of-the-art deep learning approaches, achieving around 96\\% classification\naccuracy while only 15% of raw pixels are used.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 08:24:24 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Xu", "Bolei", ""], ["Liu", "Jingxin", ""], ["Hou", "Xianxu", ""], ["Liu", "Bozhi", ""], ["Garibaldi", "Jon", ""], ["Ellis", "Ian O.", ""], ["Green", "Andy", ""], ["Shen", "Linlin", ""], ["Qiu", "Guoping", ""]]}, {"id": "1902.10949", "submitter": "Shunfeng Zhou", "authors": "Yingcheng Su, Shunfeng Zhou, Yichao Wu, Tian Su, Ding Liang, Jiaheng\n  Liu, Dixin Zheng, Yingxu Wang, Junjie Yan, Xiaolin Hu", "title": "Dynamic Multi-path Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deeper and larger neural networks have achieved better performance,\nthe complex network structure and increasing computational cost cannot meet the\ndemands of many resource-constrained applications. Existing methods usually\nchoose to execute or skip an entire specific layer, which can only alter the\ndepth of the network. In this paper, we propose a novel method called Dynamic\nMulti-path Neural Network (DMNN), which provides more path selection choices in\nterms of network width and depth during inference. The inference path of the\nnetwork is determined by a controller, which takes into account both previous\nstate and object category information. The proposed method can be easily\nincorporated into most modern network architectures. Experimental results on\nImageNet and CIFAR-100 demonstrate the superiority of our method on both\nefficiency and overall classification accuracy. To be specific, DMNN-101\nsignificantly outperforms ResNet-101 with an encouraging 45.1% FLOPs reduction,\nand DMNN-50 performs comparably to ResNet-101 while saving 42.1% parameters.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 08:48:18 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2019 14:36:09 GMT"}, {"version": "v3", "created": "Sun, 7 Apr 2019 07:23:06 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Su", "Yingcheng", ""], ["Zhou", "Shunfeng", ""], ["Wu", "Yichao", ""], ["Su", "Tian", ""], ["Liang", "Ding", ""], ["Liu", "Jiaheng", ""], ["Zheng", "Dixin", ""], ["Wang", "Yingxu", ""], ["Yan", "Junjie", ""], ["Hu", "Xiaolin", ""]]}, {"id": "1902.10953", "submitter": "St\\'ephane Lathuili\\`ere", "authors": "Benoit Mass\\'e, St\\'ephane Lathuili\\`ere, Pablo Mesejo and Radu Horaud", "title": "Extended Gaze Following: Detecting Objects in Videos Beyond the Camera\n  Field of View", "comments": "FG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problems of detecting objects of interest in a\nvideo and of estimating their locations, solely from the gaze directions of\npeople present in the video. Objects can be indistinctly located inside or\noutside the camera field of view. We refer to this problem as extended gaze\nfollowing. The contributions of the paper are the followings. First, we propose\na novel spatial representation of the gaze directions adopting a top-view\nperspective. Second, we develop several convolutional encoder/decoder networks\nto predict object locations and compare them with heuristics and with classical\nlearning-based approaches. Third, in order to train the proposed models, we\ngenerate a very large number of synthetic scenarios employing a probabilistic\nformulation. Finally, our methodology is empirically validated using a publicly\navailable dataset.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 08:59:59 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Mass\u00e9", "Benoit", ""], ["Lathuili\u00e8re", "St\u00e9phane", ""], ["Mesejo", "Pablo", ""], ["Horaud", "Radu", ""]]}, {"id": "1902.10990", "submitter": "Yeonwoo Jeong", "authors": "Yeonwoo Jeong, Yoonsung Kim, Hyun Oh Song", "title": "End-to-End Efficient Representation Learning via Cascading Combinatorial\n  Optimization", "comments": "Accepted and to appear at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop hierarchically quantized efficient embedding representations for\nsimilarity-based search and show that this representation provides not only the\nstate of the art performance on the search accuracy but also provides several\norders of speed up during inference. The idea is to hierarchically quantize the\nrepresentation so that the quantization granularity is greatly increased while\nmaintaining the accuracy and keeping the computational complexity low. We also\nshow that the problem of finding the optimal sparse compound hash code\nrespecting the hierarchical structure can be optimized in polynomial time via\nminimum cost flow in an equivalent flow network. This allows us to train the\nmethod end-to-end in a mini-batch stochastic gradient descent setting. Our\nexperiments on Cifar100 and ImageNet datasets show the state of the art search\naccuracy while providing several orders of magnitude search speedup\nrespectively over exhaustive linear search over the dataset.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 10:14:31 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 07:34:14 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Jeong", "Yeonwoo", ""], ["Kim", "Yoonsung", ""], ["Song", "Hyun Oh", ""]]}, {"id": "1902.10993", "submitter": "Nevrez Imamoglu", "authors": "Nevrez Imamoglu, Guanqun Ding, Yuming Fang, Asako Kanezaki, Toru\n  Kouyama, Ryosuke Nakamura", "title": "Salient object detection on hyperspectral images using features learned\n  from unsupervised segmentation task", "comments": "5 pages, 3 figures, accepted to appear in IEEE ICASSP 2019 (accepted\n  version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various saliency detection algorithms from color images have been proposed to\nmimic eye fixation or attentive object detection response of human observers\nfor the same scenes. However, developments on hyperspectral imaging systems\nenable us to obtain redundant spectral information of the observed scenes from\nthe reflected light source from objects. A few studies using low-level features\non hyperspectral images demonstrated that salient object detection can be\nachieved. In this work, we proposed a salient object detection model on\nhyperspectral images by applying manifold ranking (MR) on self-supervised\nConvolutional Neural Network (CNN) features (high-level features) from\nunsupervised image segmentation task. Self-supervision of CNN continues until\nclustering loss or saliency maps converges to a defined error between each\niteration. Finally, saliency estimations is done as the saliency map at last\niteration when the self-supervision procedure terminates with convergence.\nExperimental evaluations demonstrated that proposed saliency detection\nalgorithm on hyperspectral images is outperforming state-of-the-arts\nhyperspectral saliency models including the original MR based saliency model.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 10:23:00 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Imamoglu", "Nevrez", ""], ["Ding", "Guanqun", ""], ["Fang", "Yuming", ""], ["Kanezaki", "Asako", ""], ["Kouyama", "Toru", ""], ["Nakamura", "Ryosuke", ""]]}, {"id": "1902.11000", "submitter": "Carlo Biffi", "authors": "Carlo Biffi, Juan J. Cerrolaza, Giacomo Tarroni, Antonio de Marvao,\n  Stuart A. Cook, Declan P. O'Regan, Daniel Rueckert", "title": "3D High-Resolution Cardiac Segmentation Reconstruction from 2D Views\n  using Conditional Variational Autoencoders", "comments": "Accepted in IEEE International Symposium on Biomedical Imaging (ISBI\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of heart structures imaged by cardiac MR is key for the\nquantitative analysis of pathology. High-resolution 3D MR sequences enable\nwhole-heart structural imaging but are time-consuming, expensive to acquire and\nthey often require long breath holds that are not suitable for patients.\nConsequently, multiplanar breath-hold 2D cine sequences are standard practice\nbut are disadvantaged by lack of whole-heart coverage and low through-plane\nresolution. To address this, we propose a conditional variational autoencoder\narchitecture able to learn a generative model of 3D high-resolution left\nventricular (LV) segmentations which is conditioned on three 2D LV\nsegmentations of one short-axis and two long-axis images. By only employing\nthese three 2D segmentations, our model can efficiently reconstruct the 3D\nhigh-resolution LV segmentation of a subject. When evaluated on 400 unseen\nhealthy volunteers, our model yielded an average Dice score of $87.92 \\pm 0.15$\nand outperformed competing architectures.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 10:39:51 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Biffi", "Carlo", ""], ["Cerrolaza", "Juan J.", ""], ["Tarroni", "Giacomo", ""], ["de Marvao", "Antonio", ""], ["Cook", "Stuart A.", ""], ["O'Regan", "Declan P.", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1902.11007", "submitter": "Yule Li", "authors": "Yule Li", "title": "MassFace: an efficient implementation using triplet loss for face\n  recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an efficient implementation using triplet loss for\nface recognition. We conduct the practical experiment to analyze the factors\nthat influence the training of triplet loss. All models are trained on\nCASIA-Webface dataset and tested on LFW. We analyze the experiment results and\ngive some insights to help others balance the factors when they apply triplet\nloss to their own problem especially for face recognition task. Code has been\nreleased in https://github.com/yule-li/MassFace.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 10:49:39 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Li", "Yule", ""]]}, {"id": "1902.11020", "submitter": "Sergey Zakharov", "authors": "Sergey Zakharov, Ivan Shugurov, Slobodan Ilic", "title": "DPOD: 6D Pose Object Detector and Refiner", "comments": "ICCV 2019. 8 pages + supplementary material + references. The first\n  two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel deep learning method for 3D object detection\nand 6D pose estimation from RGB images. Our method, named DPOD (Dense Pose\nObject Detector), estimates dense multi-class 2D-3D correspondence maps between\nan input image and available 3D models. Given the correspondences, a 6DoF pose\nis computed via PnP and RANSAC. An additional RGB pose refinement of the\ninitial pose estimates is performed using a custom deep learning-based\nrefinement scheme. Our results and comparison to a vast number of related works\ndemonstrate that a large number of correspondences is beneficial for obtaining\nhigh-quality 6D poses both before and after refinement. Unlike other methods\nthat mainly use real data for training and do not train on synthetic\nrenderings, we perform evaluation on both synthetic and real training data\ndemonstrating superior results before and after refinement when compared to all\nrecent detectors. While being precise, the presented approach is still\nreal-time capable.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 11:15:02 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 17:45:36 GMT"}, {"version": "v3", "created": "Tue, 20 Aug 2019 17:40:55 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Zakharov", "Sergey", ""], ["Shugurov", "Ivan", ""], ["Ilic", "Slobodan", ""]]}, {"id": "1902.11026", "submitter": "Haoye Dong", "authors": "Haoye Dong, Xiaodan Liang, Bochao Wang, Hanjiang Lai, Jia Zhu, Jian\n  Yin", "title": "Towards Multi-pose Guided Virtual Try-on Network", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual try-on system under arbitrary human poses has huge application\npotential, yet raises quite a lot of challenges, e.g. self-occlusions, heavy\nmisalignment among diverse poses, and diverse clothes textures. Existing\nmethods aim at fitting new clothes into a person can only transfer clothes on\nthe fixed human pose, but still show unsatisfactory performances which often\nfail to preserve the identity, lose the texture details, and decrease the\ndiversity of poses. In this paper, we make the first attempt towards multi-pose\nguided virtual try-on system, which enables transfer clothes on a person image\nunder diverse poses. Given an input person image, a desired clothes image, and\na desired pose, the proposed Multi-pose Guided Virtual Try-on Network (MG-VTON)\ncan generate a new person image after fitting the desired clothes into the\ninput image and manipulating human poses. Our MG-VTON is constructed in three\nstages: 1) a desired human parsing map of the target image is synthesized to\nmatch both the desired pose and the desired clothes shape; 2) a deep Warping\nGenerative Adversarial Network (Warp-GAN) warps the desired clothes appearance\ninto the synthesized human parsing map and alleviates the misalignment problem\nbetween the input human pose and desired human pose; 3) a refinement render\nutilizing multi-pose composition masks recovers the texture details of clothes\nand removes some artifacts. Extensive experiments on well-known datasets and\nour newly collected largest virtual try-on benchmark demonstrate that our\nMG-VTON significantly outperforms all state-of-the-art methods both\nqualitatively and quantitatively with promising multi-pose virtual try-on\nperformances.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 11:34:52 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Dong", "Haoye", ""], ["Liang", "Xiaodan", ""], ["Wang", "Bochao", ""], ["Lai", "Hanjiang", ""], ["Zhu", "Jia", ""], ["Yin", "Jian", ""]]}, {"id": "1902.11032", "submitter": "Simon Vary", "authors": "Giancarlo A. Antonucci, Simon Vary, David Humphreys, Robert A. Lamb,\n  Jonathan Piper, Jared Tanner", "title": "Multispectral snapshot demosaicing via non-convex matrix completion", "comments": "5 pages, 2 figures, 1 table", "journal-ref": null, "doi": "10.1109/DSW.2019.8755561", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Snapshot mosaic multispectral imagery acquires an undersampled data cube by\nacquiring a single spectral measurement per spatial pixel. Sensors which\nacquire $p$ frequencies, therefore, suffer from severe $1/p$ undersampling of\nthe full data cube. We show that the missing entries can be accurately imputed\nusing non-convex techniques from sparse approximation and matrix completion\ninitialised with traditional demosaicing algorithms. In particular, we observe\nthe peak signal-to-noise ratio can typically be improved by 2 to 5 dB over\ncurrent state-of-the-art methods when simulating a $p=16$ mosaic sensor\nmeasuring both high and low altitude urban and rural scenes as well as\nground-based scenes.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 11:56:53 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 12:14:46 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Antonucci", "Giancarlo A.", ""], ["Vary", "Simon", ""], ["Humphreys", "David", ""], ["Lamb", "Robert A.", ""], ["Piper", "Jonathan", ""], ["Tanner", "Jared", ""]]}, {"id": "1902.11036", "submitter": "Moti Freiman", "authors": "Moti Freiman, Ravindra Manjeshwar, and Liran Goshen", "title": "Unsupervised Abnormality Detection through Mixed Structure\n  Regularization (MSR) in Deep Sparse Autoencoders", "comments": "Accepted for publication in the journal: \"Medical Physics\" (2019)", "journal-ref": null, "doi": "10.1002/mp.13464", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep sparse auto-encoders with mixed structure regularization (MSR) in\naddition to explicit sparsity regularization term and stochastic corruption of\nthe input data with Gaussian noise have the potential to improve unsupervised\nabnormality detection. Unsupervised abnormality detection based on identifying\noutliers using deep sparse auto-encoders is a very appealing approach for\nmedical computer aided detection systems as it requires only healthy data for\ntraining rather than expert annotated abnormality. In the task of detecting\ncoronary artery disease from Coronary Computed Tomography Angiography (CCTA),\nour results suggests that the MSR has the potential to improve overall\nperformance by 20-30% compared to deep sparse and denoising auto-encoders.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 12:01:48 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Freiman", "Moti", ""], ["Manjeshwar", "Ravindra", ""], ["Goshen", "Liran", ""]]}, {"id": "1902.11046", "submitter": "Jiexiong Tang", "authors": "Jiexiong Tang, Ludvig Ericson, John Folkesson and Patric Jensfelt", "title": "GCNv2: Efficient Correspondence Prediction for Real-Time SLAM", "comments": "Project page: https://github.com/jiexiong2016/GCNv2_SLAM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a deep learning-based network, GCNv2, for\ngeneration of keypoints and descriptors. GCNv2 is built on our previous method,\nGCN, a network trained for 3D projective geometry. GCNv2 is designed with a\nbinary descriptor vector as the ORB feature so that it can easily replace ORB\nin systems such as ORB-SLAM2. GCNv2 significantly improves the computational\nefficiency over GCN that was only able to run on desktop hardware. We show how\na modified version of ORB-SLAM2 using GCNv2 features runs on a Jetson TX2, an\nembedded low-power platform. Experimental results show that GCNv2 retains\ncomparable accuracy as GCN and that it is robust enough to use for control of a\nflying drone.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 12:23:53 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 14:42:47 GMT"}, {"version": "v3", "created": "Tue, 18 Jun 2019 12:37:50 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Tang", "Jiexiong", ""], ["Ericson", "Ludvig", ""], ["Folkesson", "John", ""], ["Jensfelt", "Patric", ""]]}, {"id": "1902.11050", "submitter": "Abraham Smith", "authors": "Abraham George Smith, Jens Petersen, Raghavendra Selvan and Camilla\n  Ru{\\o} Rasmussen", "title": "Segmentation of Roots in Soil with U-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plant root research can provide a way to attain stress-tolerant crops that\nproduce greater yield in a diverse array of conditions. Phenotyping roots in\nsoil is often challenging due to the roots being difficult to access and the\nuse of time consuming manual methods. Rhizotrons allow visual inspection of\nroot growth through transparent surfaces. Agronomists currently manually label\nphotographs of roots obtained from rhizotrons using a line-intersect method to\nobtain root length density and rooting depth measurements which are essential\nfor their experiments. We investigate the effectiveness of an automated image\nsegmentation method based on the U-Net Convolutional Neural Network (CNN)\narchitecture to enable such measurements. We design a data-set of 50 annotated\nChicory (Cichorium intybus L.) root images which we use to train, validate and\ntest the system and compare against a baseline built using the Frangi\nvesselness filter. We obtain metrics using manual annotations and\nline-intersect counts. Our results on the held out data show our proposed\nautomated segmentation system to be a viable solution for detecting and\nquantifying roots. We evaluate our system using 867 images for which we have\nobtained line-intersect counts, attaining a Spearman rank correlation of 0.9748\nand an $r^2$ of 0.9217. We also achieve an $F_1$ of 0.7 when comparing the\nautomated segmentation to the manual annotations, with our automated\nsegmentation system producing segmentations with higher quality than the manual\nannotations for large portions of the image.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 12:34:31 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 15:08:51 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Smith", "Abraham George", ""], ["Petersen", "Jens", ""], ["Selvan", "Raghavendra", ""], ["Rasmussen", "Camilla Ru\u00f8", ""]]}, {"id": "1902.11065", "submitter": "Marta Gomez-Barrero", "authors": "Ruben Tolosana and Marta Gomez-Barrero and Christoph Busch and Javier\n  Ortega-Garcia", "title": "Biometric Presentation Attack Detection: Beyond the Visible Spectrum", "comments": null, "journal-ref": null, "doi": "10.1109/TIFS.2019.2934867", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased need for unattended authentication in multiple scenarios has\nmotivated a wide deployment of biometric systems in the last few years. This\nhas in turn led to the disclosure of security concerns specifically related to\nbiometric systems. Among them, Presentation Attacks (PAs, i.e., attempts to log\ninto the system with a fake biometric characteristic or presentation attack\ninstrument) pose a severe threat to the security of the system: any person\ncould eventually fabricate or order a gummy finger or face mask to impersonate\nsomeone else. The biometrics community has thus made a considerable effort to\nthe development of automatic Presentation Attack Detection (PAD) mechanisms,\nfor instance through the international LivDet competitions.\n  In this context, we present a novel fingerprint PAD scheme based on $i)$ a\nnew capture device able to acquire images within the short wave infrared (SWIR)\nspectrum, and $ii)$ an in-depth analysis of several state-of-the-art techniques\nbased on both handcrafted and deep learning features. The approach is evaluated\non a database comprising over 4700 samples, stemming from 562 different\nsubjects and 35 different presentation attack instrument (PAI) species. The\nresults show the soundness of the proposed approach with a detection equal\nerror rate (D-EER) as low as 1.36\\% even in a realistic scenario where five\ndifferent PAI species are considered only for testing purposes (i.e., unknown\nattacks).\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 13:12:11 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Tolosana", "Ruben", ""], ["Gomez-Barrero", "Marta", ""], ["Busch", "Christoph", ""], ["Ortega-Garcia", "Javier", ""]]}, {"id": "1902.11084", "submitter": "Mat\\v{e}j \\v{S}m\\'id", "authors": "Matej Smid and Jiri Matas", "title": "Rolling Shutter Camera Synchronization with Sub-millisecond Accuracy", "comments": "8 pages, 10 figures, published at VISAPP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple method for synchronization of video streams with a precision better\nthan one millisecond is proposed. The method is applicable to any number of\nrolling shutter cameras and when a few photographic flashes or other abrupt\nlighting changes are present in the video. The approach exploits the rolling\nshutter sensor property that every sensor row starts its exposure with a small\ndelay after the onset of the previous row. The cameras may have different frame\nrates and resolutions, and need not have overlapping fields of view. The method\nwas validated on five minutes of four streams from an ice hockey match. The\nfound transformation maps events visible in all cameras to a reference time\nwith a standard deviation of the temporal error in the range of 0.3 to 0.5\nmilliseconds. The quality of the synchronization is demonstrated on temporally\nand spatially overlapping images of a fast moving puck observed in two cameras.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 13:57:58 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Smid", "Matej", ""], ["Matas", "Jiri", ""]]}, {"id": "1902.11089", "submitter": "Jian-Qing Zheng", "authors": "Jian-Qing Zheng, Xiao-Yun Zhou and Guang-Zhong Yang", "title": "Real-time 3D Shape Instantiation for Partially-deployed Stent Segment\n  from a Single 2D Fluoroscopic Image in Robot-assisted Fenestrated\n  Endovascular Aortic Repair", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": "10.1109/LRA.2019.2950499", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In robot-assisted Fenestrated Endovascular Aortic Repair (FEVAR), accurate\nalignment of stent graft fenestrations or scallops with aortic branches is\nessential for establishing complete blood flow perfusion. Current navigation is\nlargely based on 2D fluoroscopic images, which lacks 3D anatomical information,\nthus causing longer operation time as well as high risks of radiation exposure.\nPreviously, 3D shape instantiation frameworks for real-time 3D shape\nreconstruction of fully-deployed or fully-compressed stent graft from a single\n2D fluoroscopic image have been proposed for 3D navigation in robot-assisted\nFEVAR. However, these methods could not instantiate partially-deployed stent\nsegments, as the 3D marker references are unknown. In this paper, an adapted\nGraph Convolutional Network (GCN) is proposed to predict 3D marker references\nfrom 3D fully-deployed markers. As original GCN is for classification, in this\npaper, the coarsening layers are removed and the softmax function at the\nnetwork end is replaced with linear mapping for the regression task. The\nderived 3D and the 2D marker references are used to instantiate\npartially-deployed stent segment shape with the existing 3D shape instantiation\nframework. Validations were performed on three commonly used stent grafts and\nfive patient-specific 3D printed aortic aneurysm phantoms. Comparable\nperformances with average mesh distance errors of 1$\\sim$3mm and average\nangular errors around 7degree were achieved.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 14:06:36 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Zheng", "Jian-Qing", ""], ["Zhou", "Xiao-Yun", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "1902.11097", "submitter": "Jamie Morgenstern", "authors": "Benjamin Wilson and Judy Hoffman and Jamie Morgenstern", "title": "Predictive Inequity in Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate whether state-of-the-art object detection\nsystems have equitable predictive performance on pedestrians with different\nskin tones. This work is motivated by many recent examples of ML and vision\nsystems displaying higher error rates for certain demographic groups than\nothers. We annotate an existing large scale dataset which contains pedestrians,\nBDD100K, with Fitzpatrick skin tones in ranges [1-3] or [4-6]. We then provide\nan in-depth comparative analysis of performance between these two skin tone\ngroupings, finding that neither time of day nor occlusion explain this\nbehavior, suggesting this disparity is not merely the result of pedestrians in\nthe 4-6 range appearing in more difficult scenes for detection. We investigate\nto what extent time of day, occlusion, and reweighting the supervised loss\nduring training affect this predictive bias.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 21:11:16 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Wilson", "Benjamin", ""], ["Hoffman", "Judy", ""], ["Morgenstern", "Jamie", ""]]}, {"id": "1902.11100", "submitter": "Sergey Belim", "authors": "S.V. Belim, D.E. Vilkhovskiy", "title": "Usage of analytic hierarchy process for steganographic inserts detection\n  in images", "comments": null, "journal-ref": "2016 Dynamics of Systems, Mechanisms and Machines (Dynamics),\n  Omsk, Russia, pp. 1-5", "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.GR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the method of steganography detection, which is formed\nby replacing the least significant bit (LSB). Detection is performed by\ndividing the image into layers and making an analysis of zero-layer of adjacent\nbits for every bit. First-layer and second-layer are analyzed too. Hierarchies\nanalysis method is used for making decision if current bit is changed.\nWeighting coefficients as part of the analytic hierarchy process are formed on\nthe values of bits. Then a matrix of corrupted pixels is generated.\nVisualization of matrix with corrupted pixels allows to determine size,\nlocation and presence of the embedded message. Computer experiment was\nperformed. Message was embedded in a bounded rectangular area of the image.\nThis method demonstrated efficiency even at low filling container, less than\n10\\%. Widespread statistical methods are unable to detect this steganographic\ninsert. The location and size of the embedded message can be determined with an\nerror which is not exceeding to five pixels.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 12:34:21 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Belim", "S. V.", ""], ["Vilkhovskiy", "D. E.", ""]]}, {"id": "1902.11106", "submitter": "Serkan Kiranyaz", "authors": "Serkan Kiranyaz, Turker Ince, Alexandros Iosifidis and Moncef Gabbouj", "title": "Operational Neural Networks", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feed-forward, fully-connected Artificial Neural Networks (ANNs) or the\nso-called Multi-Layer Perceptrons (MLPs) are well-known universal\napproximators. However, their learning performance varies significantly\ndepending on the function or the solution space that they attempt to\napproximate. This is mainly because of their homogenous configuration based\nsolely on the linear neuron model. Therefore, while they learn very well those\nproblems with a monotonous, relatively simple and linearly separable solution\nspace, they may entirely fail to do so when the solution space is highly\nnonlinear and complex. Sharing the same linear neuron model with two additional\nconstraints (local connections and weight sharing), this is also true for the\nconventional Convolutional Neural Networks (CNNs) and, it is, therefore, not\nsurprising that in many challenging problems only the deep CNNs with a massive\ncomplexity and depth can achieve the required diversity and the learning\nperformance. In order to address this drawback and also to accomplish a more\ngeneralized model over the convolutional neurons, this study proposes a novel\nnetwork model, called Operational Neural Networks (ONNs), which can be\nheterogeneous and encapsulate neurons with any set of operators to boost\ndiversity and to learn highly complex and multi-modal functions or spaces with\nminimal network complexity and training data. Finally, a novel training method\nis formulated to back-propagate the error through the operational layers of\nONNs. Experimental results over highly challenging problems demonstrate the\nsuperior learning capabilities of ONNs even with few neurons and hidden layers.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 20:13:51 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 11:19:56 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Kiranyaz", "Serkan", ""], ["Ince", "Turker", ""], ["Iosifidis", "Alexandros", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "1902.11107", "submitter": "Zhanyu Ma", "authors": "Zhanyu Ma, Dongliang Chang, Xiaoxu Li", "title": "Channel Max Pooling Layer for Fine-Grained Vehicle Classification", "comments": "Report of ongoing work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks have recently shown excellent performance on\nFine-Grained Vehicle Classification. Based on these existing works, we consider\nthat the back-probation algorithm does not focus on extracting less\ndiscriminative feature as much as possible, but focus on that the loss function\nequals zero. Intuitively, if we can learn less discriminative features, and\nthese features still could fit the training data well, the generalization\nability of neural network could be improved. Therefore, we propose a new layer\nwhich is placed between fully connected layers and convolutional layers, called\nas Chanel Max Pooling. The proposed layer groups the features map first and\nthen compress each group into a new feature map by computing maximum of pixels\nwith same positions in the group of feature maps. Meanwhile, the proposed layer\nhas an advantage that it could help neural network reduce massive parameters.\nExperimental results on two fine-grained vehicle datasets, the Stanford\nCars-196 dataset and the Comp Cars dataset, demonstrate that the proposed layer\ncould improve classification accuracies of deep neural networks on fine-grained\nvehicle classification in the situation that a massive of parameters are\nreduced. Moreover, it has a competitive performance with the-state-of-art\nperformance on the two datasets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 14:47:04 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2020 12:01:36 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Ma", "Zhanyu", ""], ["Chang", "Dongliang", ""], ["Li", "Xiaoxu", ""]]}, {"id": "1902.11108", "submitter": "Rahul Bhalley", "authors": "Rahul Bhalley and Jianlin Su", "title": "Artist Style Transfer Via Quadratic Potential", "comments": "8 pages, 3 figures, uses nips_2018.sty, renamed the network to\n  CycleGAN-QP for maintaining consistency with work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we address the problem of artist style transfer where the\npainting style of a given artist is applied on a real world photograph. We\ntrain our neural networks in adversarial setting via recently introduced\nquadratic potential divergence for stable learning process. To further improve\nthe quality of generated artist stylized images we also integrate some of the\nrecently introduced deep learning techniques in our method. To our best\nknowledge this is the first attempt towards artist style transfer via quadratic\npotential divergence. We provide some stylized image samples in the\nsupplementary material. The source code for experimentation was written in\nPyTorch and is available online in my GitHub repository.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 12:09:13 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 05:09:17 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Bhalley", "Rahul", ""], ["Su", "Jianlin", ""]]}, {"id": "1902.11109", "submitter": "Xuan Liang", "authors": "Xuan Liang, Yida Xu", "title": "Actions Generation from Captions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence transduction models have been widely explored in many natural\nlanguage processing tasks. However, the target sequence usually consists of\ndiscrete tokens which represent word indices in a given vocabulary. We barely\nsee the case where target sequence is composed of continuous vectors, where\neach vector is an element of a time series taken successively in a temporal\ndomain. In this work, we introduce a new data set, named Action Generation Data\nSet (AGDS) which is specifically designed to carry out the task of\ncaption-to-action generation. This data set contains caption-action pairs. The\ncaption is comprised of a sequence of words describing the interactive movement\nbetween two people, and the action is a captured sequence of poses representing\nthe movement. This data set is introduced to study the ability of generating\ncontinuous sequences through sequence transduction models. We also propose a\nmodel to innovatively combine Multi-Head Attention (MHA) and Generative\nAdversarial Network (GAN) together. In our model, we have one generator to\ngenerate actions from captions and three discriminators where each of them is\ndesigned to carry out a unique functionality: caption-action consistency\ndiscriminator, pose discriminator and pose transition discriminator. This novel\ndesign allowed us to achieve plausible generation performance which is\ndemonstrated in the experiments.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 04:37:49 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Liang", "Xuan", ""], ["Xu", "Yida", ""]]}, {"id": "1902.11110", "submitter": "Swetava Ganguli", "authors": "Anthony Perez, Swetava Ganguli, Stefano Ermon, George Azzari, Marshall\n  Burke, David Lobell", "title": "Semi-Supervised Multitask Learning on Multispectral Satellite Images\n  Using Wasserstein Generative Adversarial Networks (GANs) for Predicting\n  Poverty", "comments": "This project was recognized as the best two-person project during the\n  Spring 2017 offering of CS 231N Convolutional Neural Networks for Visual\n  Recognition. Second revised version corrects typographical errors and adds a\n  few additional references", "journal-ref": null, "doi": null, "report-no": "Final report of research project conducted by the authors as part of\n  the Sustainability and Artificial Intelligence Laboratory (SAIL) at Stanford\n  University and as part of the Spring 2017 offering of CS 231N", "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Obtaining reliable data describing local poverty metrics at a granularity\nthat is informative to policy-makers requires expensive and logistically\ndifficult surveys, particularly in the developing world. Not surprisingly, the\npoverty stricken regions are also the ones which have a high probability of\nbeing a war zone, have poor infrastructure and sometimes have governments that\ndo not cooperate with internationally funded development efforts. We train a\nCNN on free and publicly available daytime satellite images of the African\ncontinent from Landsat 7 to build a model for predicting local economic\nlivelihoods. Only 5% of the satellite images can be associated with labels\n(which are obtained from DHS Surveys) and thus a semi-supervised approach using\na GAN (similar to the approach of Salimans, et al. (2016)), albeit with a more\nstable-to-train flavor of GANs called the Wasserstein GAN regularized with\ngradient penalty(Gulrajani, et al. (2017)) is used. The method of multitask\nlearning is employed to regularize the network and also create an end-to-end\nmodel for the prediction of multiple poverty metrics.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 21:52:17 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 19:27:01 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Perez", "Anthony", ""], ["Ganguli", "Swetava", ""], ["Ermon", "Stefano", ""], ["Azzari", "George", ""], ["Burke", "Marshall", ""], ["Lobell", "David", ""]]}, {"id": "1902.11111", "submitter": "Sirisha Rambhatla", "authors": "Sirisha Rambhatla, Xingguo Li, and Jarvis Haupt", "title": "Target-based Hyperspectral Demixing via Generalized Robust PCA", "comments": "5 Pages; Index Terms - Hyperspectral imaging, Robust-PCA, Dictionary\n  Sparse, Matrix Demixing, Target Localization, and Remote Sensing. arXiv admin\n  note: substantial text overlap with arXiv:1902.10238", "journal-ref": "2017 51st Asilomar Conference on Signals, Systems, and Computers", "doi": "10.1109/ACSSC.2017.8335372", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localizing targets of interest in a given hyperspectral (HS) image has\napplications ranging from remote sensing to surveillance. This task of target\ndetection leverages the fact that each material/object possesses its own\ncharacteristic spectral response, depending upon its composition. As\n$\\textit{signatures}$ of different materials are often correlated, matched\nfiltering based approaches may not be appropriate in this case. In this work,\nwe present a technique to localize targets of interest based on their spectral\nsignatures. We also present the corresponding recovery guarantees, leveraging\nour recent theoretical results. To this end, we model a HS image as a\nsuperposition of a low-rank component and a dictionary sparse component,\nwherein the dictionary consists of the $\\textit{a priori}$ known characteristic\nspectral responses of the target we wish to localize. Finally, we analyze the\nperformance of the proposed approach via experimental validation on real HS\ndata for a classification task, and compare it with related techniques.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 21:43:51 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Rambhatla", "Sirisha", ""], ["Li", "Xingguo", ""], ["Haupt", "Jarvis", ""]]}, {"id": "1902.11114", "submitter": "Md Moniruzzaman", "authors": "Md Moniruzzaman, S. M. Shamsul Islam, Paul Lavery, Mohammed Bennamoun,\n  C. Peng Lam", "title": "Imaging and Classification Techniques for Seagrass Mapping and\n  Monitoring: A Comprehensive Survey", "comments": "36 pages, 14 figures, 8tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring underwater habitats is a vital part of observing the condition of\nthe environment. The detection and mapping of underwater vegetation, especially\nseagrass has drawn the attention of the research community as early as the\nnineteen eighties. Initially, this monitoring relied on in situ observation by\nexperts. Later, advances in remote-sensing technology, satellite-monitoring\ntechniques and, digital photo- and video-based techniques opened a window to\nquicker, cheaper, and, potentially, more accurate seagrass-monitoring methods.\nSo far, for seagrass detection and mapping, digital images from airborne\ncameras, spectral images from satellites, acoustic image data using underwater\nsonar technology, and digital underwater photo and video images have been used\nto map the seagrass meadows or monitor their condition. In this article, we\nhave reviewed the recent approaches to seagrass detection and mapping to\nunderstand the gaps of the present approaches and determine further research\nscope to monitor the ocean health more easily. We have identified four classes\nof approach to seagrass mapping and assessment: still image-, video data-,\nacoustic image-, and spectral image data-based techniques. We have critically\nanalysed the surveyed approaches and found the research gaps including the need\nfor quick, cheap and effective imaging techniques robust to depth, turbidity,\nlocation and weather conditions, fully automated seagrass detectors that can\nwork in real-time, accurate techniques for estimating the seagrass density, and\nthe availability of high computation facilities for processing large scale\ndata. For addressing these gaps, future research should focus on developing\ncheaper image and video data collection techniques, deep learning based\nautomatic annotation and classification, and real-time percentage-cover\ncalculation.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 12:29:15 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 08:25:50 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Moniruzzaman", "Md", ""], ["Islam", "S. M. Shamsul", ""], ["Lavery", "Paul", ""], ["Bennamoun", "Mohammed", ""], ["Lam", "C. Peng", ""]]}, {"id": "1902.11119", "submitter": "Behnam Dezfouli", "authors": "Salma Abdel Magid, Francesco Petrini, and Behnam Dezfouli", "title": "Image Classification on IoT Edge Devices: Profiling and Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": "SIOTLAB-TECHREP-OCT2018", "categories": "cs.CV cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of powerful, low-cost IoT systems, processing data closer to\nwhere the data originates, known as edge computing, has become an increasingly\nviable option. In addition to lowering the cost of networking infrastructures,\nedge computing reduces edge-cloud delay, which is essential for\nmission-critical applications. In this paper, we show the feasibility and study\nthe performance of image classification using IoT devices. Specifically, we\nexplore the relationships between various factors of image classification\nalgorithms that may affect energy consumption such as dataset size, image\nresolution, algorithm type, algorithm phase, and device hardware. Our\nexperiments show a strong, positive linear relationship between three predictor\nvariables, namely model complexity, image resolution, and dataset size, with\nrespect to energy consumption. In addition, in order to provide a means of\npredicting the energy consumption of an edge device performing image\nclassification, we investigate the usage of three machine learning algorithms\nusing the data generated from our experiments. The performance as well as the\ntrade offs for using linear regression, Gaussian process, and random forests\nare discussed and validated. Our results indicate that the random forest model\noutperforms the two former algorithms, with an R-squared value of 0.95 and 0.79\nfor two different validation datasets.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 06:41:29 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 05:28:16 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Magid", "Salma Abdel", ""], ["Petrini", "Francesco", ""], ["Dezfouli", "Behnam", ""]]}, {"id": "1902.11121", "submitter": "Weiliang Zhang", "authors": "Yunxuan Zhang and Weiliang Zhang and Qinyan Zhang and Jijiang Yang and\n  Xiuyu Chen and Shihua Zhao", "title": "CMR motion artifact correction using generative adversarial nets", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular Magnetic Resonance (CMR) plays an important role in the\ndiagnoses and treatment of cardiovascular diseases while motion artifacts which\nare formed during the scanning process of CMR seriously affects doctors to find\nthe exact focus. The current correction methods mainly focus on the K-space\nwhich is a grid of raw data obtained from the MR signal directly and then\ntransfer to CMR image by inverse Fourier transform. They are neither effective\nnor efficient and can not be utilized in clinic. In this paper, we propose a\nnovel approach for CMR motion artifact correction using deep learning.\nSpecially, we use deep residual network (ResNet) as net framework and train our\nmodel in adversarial manner. Our approach is motivated by the connection\nbetween image motion blur and CMR motion artifact, so we can transfer methods\nfrom motion-deblur where deep learning has made great progress to CMR\nmotion-correction successfully. To evaluate motion artifact correction methods,\nwe propose a novel algorithm on how edge detection results are improved by\ndeblurred algorithm. Boosted by deep learning and adversarial training\nalgorithm, our model is trainable in an end-to-end manner, can be tested in\nreal-time and achieves the state-of-art results for CMR correction.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 12:39:23 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Zhang", "Yunxuan", ""], ["Zhang", "Weiliang", ""], ["Zhang", "Qinyan", ""], ["Yang", "Jijiang", ""], ["Chen", "Xiuyu", ""], ["Zhao", "Shihua", ""]]}, {"id": "1902.11122", "submitter": "Paschalis Bizopoulos", "authors": "Paschalis Bizopoulos and Dimitrios Koutsouris", "title": "Deep Learning in Cardiology", "comments": "27 pages, 2 figures, 10 tables", "journal-ref": "IEEE Reviews in Biomedical Engineering 12 (2019): 168-193", "doi": "10.1109/RBME.2018.2885714", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The medical field is creating large amount of data that physicians are unable\nto decipher and use efficiently. Moreover, rule-based expert systems are\ninefficient in solving complicated medical tasks or for creating insights using\nbig data. Deep learning has emerged as a more accurate and effective technology\nin a wide range of medical problems such as diagnosis, prediction and\nintervention. Deep learning is a representation learning method that consists\nof layers that transform the data non-linearly, thus, revealing hierarchical\nrelationships and structures. In this review we survey deep learning\napplication papers that use structured data, signal and imaging modalities from\ncardiology. We discuss the advantages and limitations of applying deep learning\nin cardiology that also apply in medicine in general, while proposing certain\ndirections as the most viable for clinical use.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 10:09:11 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 21:22:09 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 16:43:32 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Bizopoulos", "Paschalis", ""], ["Koutsouris", "Dimitrios", ""]]}, {"id": "1902.11123", "submitter": "Mennatullah Siam M.S.", "authors": "Mennatullah Siam, Boris Oreshkin, Martin Jagersand", "title": "Adaptive Masked Proxies for Few-Shot Segmentation", "comments": "Accepted to ICCV'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has thrived by training on large-scale datasets. However, in\nrobotics applications sample efficiency is critical. We propose a novel\nadaptive masked proxies method that constructs the final segmentation layer\nweights from few labelled samples. It utilizes multi-resolution average pooling\non base embeddings masked with the label to act as a positive proxy for the new\nclass, while fusing it with the previously learned class signatures. Our method\nis evaluated on PASCAL-$5^i$ dataset and outperforms the state-of-the-art in\nthe few-shot semantic segmentation. Unlike previous methods, our approach does\nnot require a second branch to estimate parameters or prototypes, which enables\nit to be used with 2-stream motion and appearance based segmentation networks.\nWe further propose a novel setup for evaluating continual learning of object\nsegmentation which we name incremental PASCAL (iPASCAL) where our method\noutperforms the baseline method. Our code is publicly available at\nhttps://github.com/MSiam/AdaptiveMaskedProxies.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 22:28:02 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 22:31:32 GMT"}, {"version": "v3", "created": "Mon, 15 Jul 2019 15:16:40 GMT"}, {"version": "v4", "created": "Mon, 19 Aug 2019 04:04:44 GMT"}, {"version": "v5", "created": "Mon, 14 Oct 2019 19:56:53 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Siam", "Mennatullah", ""], ["Oreshkin", "Boris", ""], ["Jagersand", "Martin", ""]]}, {"id": "1902.11124", "submitter": "Mingpan Guo", "authors": "Mingpan Guo, Stefan Matthes, Jiaojiao Ye, Hao Shen", "title": "A Generative Map for Image-based Camera Localization", "comments": "typo fixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image-based camera localization systems, information about the environment\nis usually stored in some representation, which can be referred to as a map.\nConventionally, most maps are built upon hand-crafted features. Recently,\nneural networks have attracted attention as a data-driven map representation,\nand have shown promising results in visual localization. However, these neural\nnetwork maps are generally hard to interpret by human. A readable map is not\nonly accessible to humans, but also provides a way to be verified when the\nground truth pose is unavailable. To tackle this problem, we propose Generative\nMap, a new framework for learning human-readable neural network maps, by\ncombining a generative model with the Kalman filter, which also allows it to\nincorporate additional sensor information such as stereo visual odometry. For\nevaluation, we use real world images from the 7-Scenes and Oxford RobotCar\ndatasets. We demonstrate that our Generative Map can be queried with a pose of\ninterest from the test sequence to predict an image, which closely resembles\nthe true scene. For localization, we show that Generative Map achieves\ncomparable performance with current regression models. Moreover, our framework\nis trained completely from scratch, unlike regression models which rely on\nlarge ImageNet pretrained networks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 13:18:36 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 13:13:46 GMT"}, {"version": "v3", "created": "Mon, 8 Apr 2019 11:08:38 GMT"}, {"version": "v4", "created": "Tue, 16 Apr 2019 15:59:29 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Guo", "Mingpan", ""], ["Matthes", "Stefan", ""], ["Ye", "Jiaojiao", ""], ["Shen", "Hao", ""]]}, {"id": "1902.11128", "submitter": "Chuteng Zhou", "authors": "Paul N. Whatmough, Chuteng Zhou, Patrick Hansen, Shreyas Kolala\n  Venkataramanaiah, Jae-sun Seo, Matthew Mattina", "title": "FixyNN: Efficient Hardware for Mobile Computer Vision via Transfer\n  Learning", "comments": "10 pages, 8 figures, paper accepted at SysML2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational demands of computer vision tasks based on state-of-the-art\nConvolutional Neural Network (CNN) image classification far exceed the energy\nbudgets of mobile devices. This paper proposes FixyNN, which consists of a\nfixed-weight feature extractor that generates ubiquitous CNN features, and a\nconventional programmable CNN accelerator which processes a dataset-specific\nCNN. Image classification models for FixyNN are trained end-to-end via transfer\nlearning, with the common feature extractor representing the transfered part,\nand the programmable part being learnt on the target dataset. Experimental\nresults demonstrate FixyNN hardware can achieve very high energy efficiencies\nup to 26.6 TOPS/W ($4.81 \\times$ better than iso-area programmable\naccelerator). Over a suite of six datasets we trained models via transfer\nlearning with an accuracy loss of $<1\\%$ resulting in up to 11.2 TOPS/W -\nnearly $2 \\times$ more efficient than a conventional programmable CNN\naccelerator of the same area.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 02:42:33 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Whatmough", "Paul N.", ""], ["Zhou", "Chuteng", ""], ["Hansen", "Patrick", ""], ["Venkataramanaiah", "Shreyas Kolala", ""], ["Seo", "Jae-sun", ""], ["Mattina", "Matthew", ""]]}, {"id": "1902.11131", "submitter": "Md. Abu Bakr Siddique", "authors": "Shadman Sakib, Md. Abu Bakr Siddique", "title": "Unsupervised Segmentation Algorithms' Implementation in ITK for Tissue\n  Classification via Human Head MRI Scans", "comments": "4 Pages, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Tissue classification is one of the significant tasks in the field of\nbiomedical image analysis. Magnetic Resonance Imaging (MRI) is of great\nimportance in tissue classification especially in the areas of brain tissue\nclassification which is able to recognize anatomical areas of interest such as\nsurgical planning, monitoring therapy, clinical drug trials, image\nregistration, stereotactic neurosurgery, radiotherapy etc. The task of this\npaper is to implement different unsupervised classification algorithms in ITK\nand perform tissue classification (white matter, gray matter, cerebrospinal\nfluid (CSF) and background of the human brain). For this purpose, 5 grayscale\nhead MRI scans are provided. In order of classifying brain tissues, three\nalgorithms are used. These are: Otsu thresholding, Bayesian classification and\nBayesian classification with Gaussian smoothing. The obtained classification\nresults are analyzed in the results and discussion section.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 12:48:43 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 11:12:16 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 02:40:10 GMT"}, {"version": "v4", "created": "Sat, 25 Jan 2020 12:11:55 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Sakib", "Shadman", ""], ["Siddique", "Md. Abu Bakr", ""]]}, {"id": "1902.11132", "submitter": "M. Salman Asif", "authors": "Rakib Hyder and M. Salman Asif", "title": "Generative Models for Low-Rank Video Representation and Reconstruction", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2020.2977256", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding compact representation of videos is an essential component in almost\nevery problem related to video processing or understanding. In this paper, we\npropose a generative model to learn compact latent codes that can efficiently\nrepresent and reconstruct a video sequence from its missing or under-sampled\nmeasurements. We use a generative network that is trained to map a compact code\ninto an image. We first demonstrate that if a video sequence belongs to the\nrange of the pretrained generative network, then we can recover it by\nestimating the underlying compact latent codes. Then we demonstrate that even\nif the video sequence does not belong to the range of a pretrained network, we\ncan still recover the true video sequence by jointly updating the latent codes\nand the weights of the generative network. To avoid overfitting in our model,\nwe regularize the recovery problem by imposing low-rank and similarity\nconstraints on the latent codes of the neighboring frames in the video\nsequence. We use our methods to recover a variety of videos from compressive\nmeasurements at different compression rates. We also demonstrate that we can\ngenerate missing frames in a video sequence by interpolating the latent codes\nof the observed frames in the low-dimensional space.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 05:48:23 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Hyder", "Rakib", ""], ["Asif", "M. Salman", ""]]}, {"id": "1902.11133", "submitter": "Debayan Ganguly", "authors": "Swagato Chatterjee, Rwik Kumar Dutta, Debayan Ganguly, Kingshuk\n  Chatterjee and Sudipta Roy", "title": "Bengali Handwritten Character Classification using Transfer Learning on\n  Deep Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a solution which uses state-of-the-art techniques\nin Deep Learning to tackle the problem of Bengali Handwritten Character\nRecognition ( HCR ). Our method uses lesser iterations to train than most other\ncomparable methods. We employ Transfer Learning on ResNet 50, a\nstate-of-the-art deep Convolutional Neural Network Model, pretrained on\nImageNet dataset. We also use other techniques like a modified version of One\nCycle Policy, varying the input image sizes etc. to ensure that our training\noccurs fast. We use the BanglaLekha-Isolated Dataset for evaluation of our\ntechnique which consists of 84 classes (50 Basic, 10 Numerals and 24 Compound\nCharacters). We are able to achieve 96.12% accuracy in just 47 epochs on\nBanglaLekha-Isolated dataset. When comparing our method with that of other\nresearchers, considering number of classes and without using Ensemble Learning,\nthe proposed solution achieves state of the art result for Handwritten Bengali\nCharacter Recognition. Code and weight files are available at\nhttps://github.com/swagato-c/bangla-hwcr-present.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 13:52:53 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Chatterjee", "Swagato", ""], ["Dutta", "Rwik Kumar", ""], ["Ganguly", "Debayan", ""], ["Chatterjee", "Kingshuk", ""], ["Roy", "Sudipta", ""]]}, {"id": "1902.11134", "submitter": "Zhenyu Duan", "authors": "Zhenyu Duan, Martin Renqiang Min, Li Erran Li, Mingbo Cai, Yi Xu,\n  Bingbing Ni", "title": "Disentangled Deep Autoencoding Regularization for Robust Image\n  Classification", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of achieving revolutionary successes in machine learning, deep\nconvolutional neural networks have been recently found to be vulnerable to\nadversarial attacks and difficult to generalize to novel test images with\nreasonably large geometric transformations. Inspired by a recent neuroscience\ndiscovery revealing that primate brain employs disentangled shape and\nappearance representations for object recognition, we propose a general\ndisentangled deep autoencoding regularization framework that can be easily\napplied to any deep embedding based classification model for improving the\nrobustness of deep neural networks. Our framework effectively learns\ndisentangled appearance code and geometric code for robust image\nclassification, which is the first disentangling based method defending against\nadversarial attacks and complementary to standard defense methods. Extensive\nexperiments on several benchmark datasets show that, our proposed\nregularization framework leveraging disentangled embedding significantly\noutperforms traditional unregularized convolutional neural networks for image\nclassification on robustness against adversarial attacks and generalization to\nnovel test data.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 04:49:57 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Duan", "Zhenyu", ""], ["Min", "Martin Renqiang", ""], ["Li", "Li Erran", ""], ["Cai", "Mingbo", ""], ["Xu", "Yi", ""], ["Ni", "Bingbing", ""]]}, {"id": "1902.11153", "submitter": "Xinsheng Xuan", "authors": "Xinsheng Xuan, Bo Peng, Wei Wang and Jing Dong", "title": "On the generalization of GAN image forensics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently the GAN generated face images are more and more realistic with\nhigh-quality, even hard for human eyes to detect. On the other hand, the\nforensics community keeps on developing methods to detect these generated fake\nimages and try to guarantee the credibility of visual contents. Although\nresearchers have developed some methods to detect generated images, few of them\nexplore the important problem of generalization ability of forensics model. As\nnew types of GANs are emerging fast, the generalization ability of forensics\nmodels to detect new types of GAN images is absolutely an essential research\ntopic. In this paper, we explore this problem and propose to use preprocessed\nimages to train a forensic CNN model. By applying similar image level\npreprocessing to both real and fake training images, the forensics model is\nforced to learn more intrinsic features to classify the generated and real face\nimages. Our experimental results also prove the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 07:05:38 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 08:19:47 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Xuan", "Xinsheng", ""], ["Peng", "Bo", ""], ["Wang", "Wei", ""], ["Dong", "Jing", ""]]}, {"id": "1902.11154", "submitter": "Hochang Seok", "authors": "Hochang Seok, Jongwoo Lim", "title": "ROVO: Robust Omnidirectional Visual Odometry for Wide-baseline Wide-FOV\n  Camera Systems", "comments": "7 pages, 9 figures, ICRA 2019 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a robust visual odometry system for a wide-baseline\ncamera rig with wide field-of-view (FOV) fisheye lenses, which provides full\nomnidirectional stereo observations of the environment. For more robust and\naccurate ego-motion estimation we adds three components to the standard VO\npipeline, 1) the hybrid projection model for improved feature matching, 2)\nmulti-view P3P RANSAC algorithm for pose estimation, and 3) online update of\nrig extrinsic parameters. The hybrid projection model combines the perspective\nand cylindrical projection to maximize the overlap between views and minimize\nthe image distortion that degrades feature matching performance. The multi-view\nP3P RANSAC algorithm extends the conventional P3P RANSAC to multi-view images\nso that all feature matches in all views are considered in the inlier counting\nfor robust pose estimation. Finally the online extrinsic calibration is\nseamlessly integrated in the backend optimization framework so that the changes\nin camera poses due to shocks or vibrations can be corrected automatically. The\nproposed system is extensively evaluated with synthetic datasets with\nground-truth and real sequences of highly dynamic environment, and its superior\nperformance is demonstrated.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 15:29:27 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 15:45:42 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Seok", "Hochang", ""], ["Lim", "Jongwoo", ""]]}, {"id": "1902.11179", "submitter": "Zuheng Ming", "authors": "Zuheng Ming, Junshi Xia, Muhammad Muzzamil Luqman, Jean-Christophe\n  Burie, Kaixing Zhao", "title": "FaceLiveNet+: A Holistic Networks For Face Authentication Based On\n  Dynamic Multi-task Convolutional Neural Networks", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a holistic multi-task Convolutional Neural Networks\n(CNNs) with the dynamic weights of the tasks,namely FaceLiveNet+, for face\nauthentication. FaceLiveNet+ can employ face verification and facial expression\nrecognition as a solution of liveness control simultaneously. Comparing to the\nsingle-task learning, the proposed multi-task learning can better capture the\nfeature representation for all of the tasks. The experimental results show the\nsuperiority of the multi-task learning to the single-task learning for both the\nface verification task and facial expression recognition task. Rather using a\nconventional multi-task learning with fixed weights for the tasks, this work\nproposes a so called dynamic-weight-unit to automatically learn the weights of\nthe tasks. The experiments have shown the effectiveness of the dynamic weights\nfor training the networks. Finally, the holistic evaluation for face\nauthentication based on the proposed protocol has shown the feasibility to\napply the FaceLiveNet+ for face authentication.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 15:58:08 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Ming", "Zuheng", ""], ["Xia", "Junshi", ""], ["Luqman", "Muhammad Muzzamil", ""], ["Burie", "Jean-Christophe", ""], ["Zhao", "Kaixing", ""]]}, {"id": "1902.11203", "submitter": "Jinjin Gu", "authors": "Haonan Qiu, Chuan Wang, Hang Zhu, Xiangyu Zhu, Jinjin Gu, Xiaoguang\n  Han", "title": "Two-phase Hair Image Synthesis by Self-Enhancing Generative Model", "comments": null, "journal-ref": null, "doi": "10.1111/cgf.13847", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating plausible hair image given limited guidance, such as sparse\nsketches or low-resolution image, has been made possible with the rise of\nGenerative Adversarial Networks (GANs). Traditional image-to-image translation\nnetworks can generate recognizable results, but finer textures are usually lost\nand blur artifacts commonly exist. In this paper, we propose a two-phase\ngenerative model for high-quality hair image synthesis. The two-phase pipeline\nfirst generates a coarse image by an existing image translation model, then\napplies a re-generating network with self-enhancing capability to the coarse\nimage. The self-enhancing capability is achieved by a proposed structure\nextraction layer, which extracts the texture and orientation map from a hair\nimage. Extensive experiments on two tasks, Sketch2Hair and Hair\nSuper-Resolution, demonstrate that our approach is able to synthesize plausible\nhair image with finer details, and outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 16:41:23 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Qiu", "Haonan", ""], ["Wang", "Chuan", ""], ["Zhu", "Hang", ""], ["Zhu", "Xiangyu", ""], ["Gu", "Jinjin", ""], ["Han", "Xiaoguang", ""]]}, {"id": "1902.11208", "submitter": "Gideon Maillette De Buy Wenniger", "authors": "Gideon Maillette de Buy Wenniger, Lambert Schomaker, Andy Way", "title": "No Padding Please: Efficient Neural Handwriting Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/ICDAR.2019.00064", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural handwriting recognition (NHR) is the recognition of handwritten text\nwith deep learning models, such as multi-dimensional long short-term memory\n(MDLSTM) recurrent neural networks. Models with MDLSTM layers have achieved\nstate-of-the art results on handwritten text recognition tasks. While\nmulti-directional MDLSTM-layers have an unbeaten ability to capture the\ncomplete context in all directions, this strength limits the possibilities for\nparallelization, and therefore comes at a high computational cost. In this work\nwe develop methods to create efficient MDLSTM-based models for NHR,\nparticularly a method aimed at eliminating computation waste that results from\npadding. This proposed method, called example-packing, replaces wasteful\nstacking of padded examples with efficient tiling in a 2-dimensional grid. For\nword-based NHR this yields a speed improvement of factor 6.6 over an already\nefficient baseline of minimal padding for each batch separately. For line-based\nNHR the savings are more modest, but still significant. In addition to\nexample-packing, we propose: 1) a technique to optimize parallelization for\ndynamic graph definition frameworks including PyTorch, using convolutions with\ngrouping, 2) a method for parallelization across GPUs for variable-length\nexample batches. All our techniques are thoroughly tested on our own PyTorch\nre-implementation of MDLSTM-based NHR models. A thorough evaluation on the IAM\ndataset shows that our models are performing similar to earlier implementations\nof state-of-the-art models. Our efficient NHR model and some of the reusable\ntechniques discussed with it offer ways to realize relatively efficient models\nfor the omnipresent scenario of variable-length inputs in deep learning.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 16:46:43 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Wenniger", "Gideon Maillette de Buy", ""], ["Schomaker", "Lambert", ""], ["Way", "Andy", ""]]}, {"id": "1902.11237", "submitter": "Kassem Kallas", "authors": "Mauro Barni, Kassem Kallas, Benedetta Tondi", "title": "A new Backdoor Attack in CNNs by training set corruption without label\n  poisoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backdoor attacks against CNNs represent a new threat against deep learning\nsystems, due to the possibility of corrupting the training set so to induce an\nincorrect behaviour at test time. To avoid that the trainer recognises the\npresence of the corrupted samples, the corruption of the training set must be\nas stealthy as possible. Previous works have focused on the stealthiness of the\nperturbation injected into the training samples, however they all assume that\nthe labels of the corrupted samples are also poisoned. This greatly reduces the\nstealthiness of the attack, since samples whose content does not agree with the\nlabel can be identified by visual inspection of the training set or by running\na pre-classification step. In this paper we present a new backdoor attack\nwithout label poisoning Since the attack works by corrupting only samples of\nthe target class, it has the additional advantage that it does not need to\nidentify beforehand the class of the samples to be attacked at test time.\nResults obtained on the MNIST digits recognition task and the traffic signs\nclassification task show that backdoor attacks without label poisoning are\nindeed possible, thus raising a new alarm regarding the use of deep learning in\nsecurity-critical applications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 16:27:41 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Barni", "Mauro", ""], ["Kallas", "Kassem", ""], ["Tondi", "Benedetta", ""]]}, {"id": "1902.11266", "submitter": "M. Saquib Sarfraz", "authors": "M. Saquib Sarfraz, Vivek Sharma, Rainer Stiefelhagen", "title": "Efficient Parameter-free Clustering Using First Neighbor Relations", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new clustering method in the form of a single clustering\nequation that is able to directly discover groupings in the data. The main\nproposition is that the first neighbor of each sample is all one needs to\ndiscover large chains and finding the groups in the data. In contrast to most\nexisting clustering algorithms our method does not require any\nhyper-parameters, distance thresholds and/or the need to specify the number of\nclusters. The proposed algorithm belongs to the family of hierarchical\nagglomerative methods. The technique has a very low computational overhead, is\neasily scalable and applicable to large practical problems. Evaluation on well\nknown datasets from different domains ranging between 1077 and 8.1 million\nsamples shows substantial performance gains when compared to the existing\nclustering techniques.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 18:12:57 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Sarfraz", "M. Saquib", ""], ["Sharma", "Vivek", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1902.11268", "submitter": "Siyu Liao", "authors": "Siyu Liao, Zhe Li, Liang Zhao, Qinru Qiu, Yanzhi Wang, Bo Yuan", "title": "CircConv: A Structured Convolution with Low Complexity", "comments": null, "journal-ref": "Published in AAAI 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs), especially deep convolutional neural networks\n(CNNs), have emerged as the powerful technique in various machine learning\napplications. However, the large model sizes of DNNs yield high demands on\ncomputation resource and weight storage, thereby limiting the practical\ndeployment of DNNs. To overcome these limitations, this paper proposes to\nimpose the circulant structure to the construction of convolutional layers, and\nhence leads to circulant convolutional layers (CircConvs) and circulant CNNs.\nThe circulant structure and models can be either trained from scratch or\nre-trained from a pre-trained non-circulant model, thereby making it very\nflexible for different training environments. Through extensive experiments,\nsuch strong structure-imposing approach is proved to be able to substantially\nreduce the number of parameters of convolutional layers and enable significant\nsaving of computational cost by using fast multiplication of the circulant\ntensor.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 18:18:51 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Liao", "Siyu", ""], ["Li", "Zhe", ""], ["Zhao", "Liang", ""], ["Qiu", "Qinru", ""], ["Wang", "Yanzhi", ""], ["Yuan", "Bo", ""]]}, {"id": "1902.11274", "submitter": "Gencer Sumbul", "authors": "Gencer Sumbul and Beg\\\"um Demir", "title": "A Novel Multi-Attention Driven System For Multi-Label Remote Sensing\n  Image Classification", "comments": "Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS) 2019", "journal-ref": null, "doi": "10.1109/IGARSS.2019.8898188", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel multi-attention driven system that jointly\nexploits Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN)\nin the context of multi-label remote sensing (RS) image classification. The\nproposed system consists of four main modules. The first module aims to extract\npreliminary local descriptors of RS image bands that can be associated to\ndifferent spatial resolutions. To this end, we introduce a K-Branch CNN, in\nwhich each branch extracts descriptors of image bands that have the same\nspatial resolution. The second module aims to model spatial relationship among\nlocal descriptors. This is achieved by a bidirectional RNN architecture, in\nwhich Long Short-Term Memory nodes enrich local descriptors by considering\nspatial relationships of local areas (image patches). The third module aims to\ndefine multiple attention scores for local descriptors. This is achieved by a\nnovel patch-based multi-attention mechanism that takes into account the joint\noccurrence of multiple land-cover classes and provides the attention-based\nlocal descriptors. The last module exploits these descriptors for multi-label\nRS image classification. Experimental results obtained on the BigEarthNet that\nis a large-scale Sentinel-2 benchmark archive show the effectiveness of the\nproposed method compared to a state of the art method.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 18:25:19 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 16:38:45 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 08:39:36 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Sumbul", "Gencer", ""], ["Demir", "Beg\u00fcm", ""]]}]