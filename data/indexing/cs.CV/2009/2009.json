[{"id": "2009.00029", "submitter": "Filippo Maria Castelli", "authors": "Filippo Maria Castelli, Matteo Roffilli, Giacomo Mazzamuto, Irene\n  Costantini, Ludovico Silvestri and Francesco Saverio Pavone", "title": "Semantic Segmentation of Neuronal Bodies in Fluorescence Microscopy\n  Using a 2D+3D CNN Training Strategy with Sparsely Annotated Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Semantic segmentation of neuronal structures in 3D high-resolution\nfluorescence microscopy imaging of the human brain cortex can take advantage of\nbidimensional CNNs, which yield good results in neuron localization but lead to\ninaccurate surface reconstruction. 3D CNNs, on the other hand, would require\nmanually annotated volumetric data on a large scale and hence considerable\nhuman effort. Semi-supervised alternative strategies which make use only of\nsparse annotations suffer from longer training times and achieved models tend\nto have increased capacity compared to 2D CNNs, needing more ground truth data\nto attain similar results. To overcome these issues we propose a two-phase\nstrategy for training native 3D CNN models on sparse 2D annotations where\nmissing labels are inferred by a 2D CNN model and combined with manual\nannotations in a weighted manner during loss calculation.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 18:01:02 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 00:37:53 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Castelli", "Filippo Maria", ""], ["Roffilli", "Matteo", ""], ["Mazzamuto", "Giacomo", ""], ["Costantini", "Irene", ""], ["Silvestri", "Ludovico", ""], ["Pavone", "Francesco Saverio", ""]]}, {"id": "2009.00071", "submitter": "Zhiyuan Mao", "authors": "Zhiyuan Mao, Nicholas Chimitt, Stanley Chan", "title": "Image Reconstruction of Static and Dynamic Scenes through Anisoplanatic\n  Turbulence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground based long-range passive imaging systems often suffer from degraded\nimage quality due to a turbulent atmosphere. While methods exist for removing\nsuch turbulent distortions, many are limited to static sequences which cannot\nbe extended to dynamic scenes. In addition, the physics of the turbulence is\noften not integrated into the image reconstruction algorithms, making the\nphysics foundations of the methods weak. In this paper, we present a unified\nmethod for atmospheric turbulence mitigation in both static and dynamic\nsequences. We are able to achieve better results compared to existing methods\nby utilizing (i) a novel space-time non-local averaging method to construct a\nreliable reference frame, (ii) a geometric consistency and a sharpness metric\nto generate the lucky frame, (iii) a physics-constrained prior model of the\npoint spread function for blind deconvolution. Experimental results based on\nsynthetic and real long-range turbulence sequences validate the performance of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 19:20:46 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Mao", "Zhiyuan", ""], ["Chimitt", "Nicholas", ""], ["Chan", "Stanley", ""]]}, {"id": "2009.00088", "submitter": "Ronald Clark", "authors": "Shuyu Lin and Ronald Clark", "title": "LaDDer: Latent Data Distribution Modelling with a Generative Prior", "comments": "Accepted to BMVC 2020. Code and demos are available at:\n  https://github.com/lin-shuyu/ladder-latent-data-distribution-modelling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that the performance of a learnt generative model is\nclosely related to the model's ability to accurately represent the inferred\n\\textbf{latent data distribution}, i.e. its topology and structural properties.\nWe propose LaDDer to achieve accurate modelling of the latent data distribution\nin a variational autoencoder framework and to facilitate better representation\nlearning. The central idea of LaDDer is a meta-embedding concept, which uses\nmultiple VAE models to learn an embedding of the embeddings, forming a ladder\nof encodings. We use a non-parametric mixture as the hyper prior for the\ninnermost VAE and learn all the parameters in a unified variational framework.\nFrom extensive experiments, we show that our LaDDer model is able to accurately\nestimate complex latent distribution and results in improvement in the\nrepresentation quality. We also propose a novel latent space interpolation\nmethod that utilises the derived data distribution.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 20:10:01 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Lin", "Shuyu", ""], ["Clark", "Ronald", ""]]}, {"id": "2009.00092", "submitter": "Muhammad Usman Ghani", "authors": "Muhammad Usman Ghani and W. Clem Karl", "title": "Data and Image Prior Integration for Image Reconstruction Using\n  Consensus Equilibrium", "comments": "Submitted to IEEE Transactions on Computational Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image domain prior models have been shown to improve the quality of\nreconstructed images, especially when data are limited. Pre-processing of raw\ndata, through the implicit or explicit inclusion of data domain priors have\nseparately also shown utility in improving reconstructions. In this work, a\nprincipled approach is presented allowing the unified integration of both data\nand image domain priors for improved image reconstruction. The consensus\nequilibrium framework is extended to integrate physical sensor models, data\nmodels, and image models. In order to achieve this integration, the\nconventional image variables used in consensus equilibrium are augmented with\nvariables representing data domain quantities. The overall result produces\ncombined estimates of both the data and the reconstructed image that is\nconsistent with the physical models and prior models being utilized. The prior\nmodels used in both domains in this work are created using deep neural\nnetworks. The superior quality allowed by incorporating both data and image\ndomain prior models is demonstrated for two applications: limited-angle CT and\naccelerated MRI. The prior data model in both these applications is focused on\nrecovering missing data. Experimental results are presented for a 90 degree\nlimited-angle tomography problem from a real checked-bagged CT dataset and a 4x\naccelerated MRI problem on a simulated dataset. The new framework is very\nflexible and can be easily applied to other computational imaging problems with\nimperfect data.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 20:48:07 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Ghani", "Muhammad Usman", ""], ["Karl", "W. Clem", ""]]}, {"id": "2009.00093", "submitter": "Zheda Mai", "authors": "Dongsub Shim, Zheda Mai, Jihwan Jeong, Scott Sanner, Hyunwoo Kim,\n  Jongseong Jang", "title": "Online Class-Incremental Continual Learning with Adversarial Shapley\n  Value", "comments": "Proceedings of the 35th AAAI Conference on Artificial Intelligence\n  (AAAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As image-based deep learning becomes pervasive on every device, from cell\nphones to smart watches, there is a growing need to develop methods that\ncontinually learn from data while minimizing memory footprint and power\nconsumption. While memory replay techniques have shown exceptional promise for\nthis task of continual learning, the best method for selecting which buffered\nimages to replay is still an open question. In this paper, we specifically\nfocus on the online class-incremental setting where a model needs to learn new\nclasses continually from an online data stream. To this end, we contribute a\nnovel Adversarial Shapley value scoring method that scores memory data samples\naccording to their ability to preserve latent decision boundaries for\npreviously observed classes (to maintain learning stability and avoid\nforgetting) while interfering with latent decision boundaries of current\nclasses being learned (to encourage plasticity and optimal learning of new\nclass boundaries). Overall, we observe that our proposed ASER method provides\ncompetitive or improved performance compared to state-of-the-art replay-based\ncontinual learning methods on a variety of datasets.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 20:52:27 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 07:27:00 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 03:05:46 GMT"}, {"version": "v4", "created": "Mon, 22 Mar 2021 20:03:10 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Shim", "Dongsub", ""], ["Mai", "Zheda", ""], ["Jeong", "Jihwan", ""], ["Sanner", "Scott", ""], ["Kim", "Hyunwoo", ""], ["Jang", "Jongseong", ""]]}, {"id": "2009.00097", "submitter": "Linjun Zhou", "authors": "Linjun Zhou, Peng Cui, Yinan Jiang, Shiqiang Yang", "title": "Adversarial Eigen Attack on Black-Box Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black-box adversarial attack has attracted a lot of research interests for\nits practical use in AI safety. Compared with the white-box attack, a black-box\nsetting is more difficult for less available information related to the\nattacked model and the additional constraint on the query budget. A general way\nto improve the attack efficiency is to draw support from a pre-trained\ntransferable white-box model. In this paper, we propose a novel setting of\ntransferable black-box attack: attackers may use external information from a\npre-trained model with available network parameters, however, different from\nprevious studies, no additional training data is permitted to further change or\ntune the pre-trained model. To this end, we further propose a new algorithm,\nEigenBA to tackle this problem. Our method aims to explore more gradient\ninformation of the black-box model, and promote the attack efficiency, while\nkeeping the perturbation to the original attacked image small, by leveraging\nthe Jacobian matrix of the pre-trained white-box model. We show the optimal\nperturbations are closely related to the right singular vectors of the Jacobian\nmatrix. Further experiments on ImageNet and CIFAR-10 show that even the\nunlearnable pre-trained white-box model could also significantly boost the\nefficiency of the black-box attack and our proposed method could further\nimprove the attack efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 07:37:43 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Zhou", "Linjun", ""], ["Cui", "Peng", ""], ["Jiang", "Yinan", ""], ["Yang", "Shiqiang", ""]]}, {"id": "2009.00100", "submitter": "Young-Min Song", "authors": "Young-min Song, Young-chul Yoon, Kwangjin Yoon, Moongu Jeon,\n  Seong-Whan Lee, Witold Pedrycz", "title": "Online Multi-Object Tracking and Segmentation with GMPHD Filter and\n  Mask-based Affinity Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a highly practical fully online multi-object\ntracking and segmentation (MOTS) method that uses instance segmentation results\nas an input. The proposed method is based on the Gaussian mixture probability\nhypothesis density (GMPHD) filter, a hierarchical data association (HDA), and a\nmask-based affinity fusion (MAF) model to achieve high-performance online\ntracking. The HDA consists of two associations: segment-to-track and\ntrack-to-track associations. One affinity, for position and motion, is computed\nby using the GMPHD filter, and the other affinity, for appearance is computed\nby using the responses from a single object tracker such as a kernalized\ncorrelation filter. These two affinities are simply fused by using a\nscore-level fusion method such as min-max normalization referred to as MAF. In\naddition, to reduce the number of false positive segments, we adopt mask\nIoU-based merging (mask merging). The proposed MOTS framework with the key\nmodules: HDA, MAF, and mask merging, is easily extensible to simultaneously\ntrack multiple types of objects with CPU only execution in parallel processing.\nIn addition, the developed framework only requires simple parameter tuning\nunlike many existing MOTS methods that need intensive hyperparameter\noptimization. In the experiments on the two popular MOTS datasets, the key\nmodules show some improvements. For instance, ID-switch decreases by more than\nhalf compared to a baseline method in the training sets. In conclusion, our\ntracker achieves state-of-the-art MOTS performance in the test sets.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 21:06:22 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 10:55:36 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Song", "Young-min", ""], ["Yoon", "Young-chul", ""], ["Yoon", "Kwangjin", ""], ["Jeon", "Moongu", ""], ["Lee", "Seong-Whan", ""], ["Pedrycz", "Witold", ""]]}, {"id": "2009.00104", "submitter": "William Falcon", "authors": "William Falcon, Kyunghyun Cho", "title": "A Framework For Contrastive Self-Supervised Learning And Designing A New\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive self-supervised learning (CSL) is an approach to learn useful\nrepresentations by solving a pretext task that selects and compares anchor,\nnegative and positive (APN) features from an unlabeled dataset. We present a\nconceptual framework that characterizes CSL approaches in five aspects (1) data\naugmentation pipeline, (2) encoder selection, (3) representation extraction,\n(4) similarity measure, and (5) loss function. We analyze three leading CSL\napproaches--AMDIM, CPC, and SimCLR--, and show that despite different\nmotivations, they are special cases under this framework. We show the utility\nof our framework by designing Yet Another DIM (YADIM) which achieves\ncompetitive results on CIFAR-10, STL-10 and ImageNet, and is more robust to the\nchoice of encoder and the representation extraction strategy. To support\nongoing CSL research, we release the PyTorch implementation of this conceptual\nframework along with standardized implementations of AMDIM, CPC (V2), SimCLR,\nBYOL, Moco (V2) and YADIM.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 21:11:48 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Falcon", "William", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "2009.00145", "submitter": "Zihao Zhu", "authors": "Jing Yu, Zihao Zhu, Yujing Wang, Weifeng Zhang, Yue Hu, Jianlong Tan", "title": "Cross-modal Knowledge Reasoning for Knowledge-based Visual Question\n  Answering", "comments": "Accepted at Pattern Recognition. arXiv admin note: substantial text\n  overlap with arXiv:2006.09073", "journal-ref": null, "doi": "10.1016/j.patcog.2020.107563", "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge-based Visual Question Answering (KVQA) requires external knowledge\nbeyond the visible content to answer questions about an image. This ability is\nchallenging but indispensable to achieve general VQA. One limitation of\nexisting KVQA solutions is that they jointly embed all kinds of information\nwithout fine-grained selection, which introduces unexpected noises for\nreasoning the correct answer. How to capture the question-oriented and\ninformation-complementary evidence remains a key challenge to solve the\nproblem. Inspired by the human cognition theory, in this paper, we depict an\nimage by multiple knowledge graphs from the visual, semantic and factual views.\nThereinto, the visual graph and semantic graph are regarded as\nimage-conditioned instantiation of the factual graph. On top of these new\nrepresentations, we re-formulate Knowledge-based Visual Question Answering as a\nrecurrent reasoning process for obtaining complementary evidence from\nmultimodal information. To this end, we decompose the model into a series of\nmemory-based reasoning steps, each performed by a G raph-based R ead, U pdate,\nand C ontrol ( GRUC ) module that conducts parallel reasoning over both visual\nand semantic information. By stacking the modules multiple times, our model\nperforms transitive reasoning and obtains question-oriented concept\nrepresentations under the constrain of different modalities. Finally, we\nperform graph neural networks to infer the global-optimal answer by jointly\nconsidering all the concepts. We achieve a new state-of-the-art performance on\nthree popular benchmark datasets, including FVQA, Visual7W-KB and OK-VQA, and\ndemonstrate the effectiveness and interpretability of our model with extensive\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 23:25:01 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Yu", "Jing", ""], ["Zhu", "Zihao", ""], ["Wang", "Yujing", ""], ["Zhang", "Weifeng", ""], ["Hu", "Yue", ""], ["Tan", "Jianlong", ""]]}, {"id": "2009.00149", "submitter": "Timo Bolkart", "authors": "Partha Ghosh, Pravir Singh Gupta, Roy Uziel, Anurag Ranjan, Michael\n  Black, Timo Bolkart", "title": "GIF: Generative Interpretable Faces", "comments": "International Conference on 3D Vision (3DV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo-realistic visualization and animation of expressive human faces have\nbeen a long standing challenge. 3D face modeling methods provide parametric\ncontrol but generates unrealistic images, on the other hand, generative 2D\nmodels like GANs (Generative Adversarial Networks) output photo-realistic face\nimages, but lack explicit control. Recent methods gain partial control, either\nby attempting to disentangle different factors in an unsupervised manner, or by\nadding control post hoc to a pre-trained model. Unconditional GANs, however,\nmay entangle factors that are hard to undo later. We condition our generative\nmodel on pre-defined control parameters to encourage disentanglement in the\ngeneration process. Specifically, we condition StyleGAN2 on FLAME, a generative\n3D face model. While conditioning on FLAME parameters yields unsatisfactory\nresults, we find that conditioning on rendered FLAME geometry and photometric\ndetails works well. This gives us a generative 2D face model named GIF\n(Generative Interpretable Faces) that offers FLAME's parametric control. Here,\ninterpretable refers to the semantic meaning of different parameters. Given\nFLAME parameters for shape, pose, expressions, parameters for appearance,\nlighting, and an additional style vector, GIF outputs photo-realistic face\nimages. We perform an AMT based perceptual study to quantitatively and\nqualitatively evaluate how well GIF follows its conditioning. The code, data,\nand trained model are publicly available for research purposes at\nhttp://gif.is.tue.mpg.de.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 23:40:26 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 13:37:01 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Ghosh", "Partha", ""], ["Gupta", "Pravir Singh", ""], ["Uziel", "Roy", ""], ["Ranjan", "Anurag", ""], ["Black", "Michael", ""], ["Bolkart", "Timo", ""]]}, {"id": "2009.00155", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Xiangyu Yue, Shanghang Zhang, Bo Li, Han Zhao, Bichen\n  Wu, Ravi Krishna, Joseph E. Gonzalez, Alberto L. Sangiovanni-Vincentelli,\n  Sanjit A. Seshia, Kurt Keutzer", "title": "A Review of Single-Source Deep Unsupervised Visual Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale labeled training datasets have enabled deep neural networks to\nexcel across a wide range of benchmark vision tasks. However, in many\napplications, it is prohibitively expensive and time-consuming to obtain large\nquantities of labeled data. To cope with limited labeled training data, many\nhave attempted to directly apply models trained on a large-scale labeled source\ndomain to another sparsely labeled or unlabeled target domain. Unfortunately,\ndirect transfer across domains often performs poorly due to the presence of\ndomain shift or dataset bias. Domain adaptation is a machine learning paradigm\nthat aims to learn a model from a source domain that can perform well on a\ndifferent (but related) target domain. In this paper, we review the latest\nsingle-source deep unsupervised domain adaptation methods focused on visual\ntasks and discuss new perspectives for future research. We begin with the\ndefinitions of different domain adaptation strategies and the descriptions of\nexisting benchmark datasets. We then summarize and compare different categories\nof single-source unsupervised domain adaptation methods, including\ndiscrepancy-based methods, adversarial discriminative methods, adversarial\ngenerative methods, and self-supervision-based methods. Finally, we discuss\nfuture research directions with challenges and possible solutions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 00:06:50 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 06:35:19 GMT"}, {"version": "v3", "created": "Sat, 19 Sep 2020 00:46:27 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhao", "Sicheng", ""], ["Yue", "Xiangyu", ""], ["Zhang", "Shanghang", ""], ["Li", "Bo", ""], ["Zhao", "Han", ""], ["Wu", "Bichen", ""], ["Krishna", "Ravi", ""], ["Gonzalez", "Joseph E.", ""], ["Sangiovanni-Vincentelli", "Alberto L.", ""], ["Seshia", "Sanjit A.", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2009.00164", "submitter": "Yecheng Lyu", "authors": "Ce Zheng, Yecheng Lyu, Ming Li, Ziming Zhang", "title": "LodoNet: A Deep Neural Network with 2D Keypoint Matchingfor 3D LiDAR\n  Odometry Estimation", "comments": "In 28th ACM International Conference on Multimedia, 9 pages", "journal-ref": null, "doi": "10.1145/3394171.3413771", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based LiDAR odometry (LO) estimation attracts increasing\nresearch interests in the field of autonomous driving and robotics. Existing\nworks feed consecutive LiDAR frames into neural networks as point clouds and\nmatch pairs in the learned feature space. In contrast, motivated by the success\nof image based feature extractors, we propose to transfer the LiDAR frames to\nimage space and reformulate the problem as image feature extraction. With the\nhelp of scale-invariant feature transform (SIFT) for feature extraction, we are\nable to generate matched keypoint pairs (MKPs) that can be precisely returned\nto the 3D space. A convolutional neural network pipeline is designed for LiDAR\nodometry estimation by extracted MKPs. The proposed scheme, namely LodoNet, is\nthen evaluated in the KITTI odometry estimation benchmark, achieving on par\nwith or even better results than the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 01:09:41 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Zheng", "Ce", ""], ["Lyu", "Yecheng", ""], ["Li", "Ming", ""], ["Zhang", "Ziming", ""]]}, {"id": "2009.00173", "submitter": "Asif Patankar", "authors": "Asif Ashraf Patankar and Hyeonjoon Moon", "title": "Automatic Radish Wilt Detection Using Image Processing Based Techniques\n  and Machine Learning Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image processing, computer vision, and pattern recognition have been playing\na vital role in diverse agricultural applications, such as species detection,\nrecognition, classification, identification, plant growth stages, plant disease\ndetection, and many more. On the other hand, there is a growing need to capture\nhigh resolution images using unmanned aerial vehicles (UAV) and to develop\nbetter algorithms in order to find highly accurate and to the point results. In\nthis paper, we propose a segmentation and extraction-based technique to detect\nfusarium wilt in radish crops. Recent wilt detection algorithms are either\nbased on image processing techniques or conventional machine learning\nalgorithms. However, our methodology is based on a hybrid algorithm, which\ncombines image processing and machine learning. First, the crop image is\ndivided into three segments, which include viz., healthy vegetation, ground and\npacking material. Based on the HSV decision tree algorithm, all the three\nsegments are segregated from the image. Second, the extracted segments are\nsummed together into an empty canvas of the same resolution as the image and\none new image is produced. Third, this new image is compared with the original\nimage, and a final noisy image, which contains traces of wilt is extracted.\nFinally, a k-means algorithm is applied to eliminate the noise and to extract\nthe accurate wilt from it. Moreover, the extracted wilt is mapped on the\noriginal image using the contouring method. The proposed combination of\nalgorithms detects the wilt appropriately, which surpasses the traditional\npractice of separately using the image processing techniques or machine\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 01:37:01 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Patankar", "Asif Ashraf", ""], ["Moon", "Hyeonjoon", ""]]}, {"id": "2009.00185", "submitter": "Meida Chen", "authors": "Kyle McCullough, Andrew Feng, Meida Chen, Ryan McAlinden", "title": "Utilizing Satellite Imagery Datasets and Machine Learning Data Models to\n  Evaluate Infrastructure Change in Undeveloped Regions", "comments": null, "journal-ref": "Interservice/Industry Training, Simulation, and Education\n  Conference (I/ITSEC) 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the globalized economic world, it has become important to understand the\npurpose behind infrastructural and construction initiatives occurring within\ndeveloping regions of the earth. This is critical when the financing for such\nprojects must be coming from external sources, as is occurring throughout major\nportions of the African continent. When it comes to imagery analysis to\nresearch these regions, ground and aerial coverage is either non-existent or\nnot commonly acquired. However, imagery from a large number of commercial,\nprivate, and government satellites have produced enormous datasets with global\ncoverage, compiling geospatial resources that can be mined and processed using\nmachine learning algorithms and neural networks. The downside is that a\nmajority of these geospatial data resources are in a state of technical stasis,\nas it is difficult to quickly parse and determine a plan for request and\nprocessing when acquiring satellite image data. A goal of this research is to\nallow automated monitoring for largescale infrastructure projects, such as\nrailways, to determine reliable metrics that define and predict the direction\nconstruction initiatives could take, allowing for a directed monitoring via\nnarrowed and targeted satellite imagery requests. By utilizing photogrammetric\ntechniques on available satellite data to create 3D Meshes and Digital Surface\nModels (DSM) we hope to effectively predict transport routes. In understanding\nthe potential directions that largescale transport infrastructure will take\nthrough predictive modeling, it becomes much easier to track, understand, and\nmonitor progress, especially in areas with limited imagery coverage.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 02:11:14 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["McCullough", "Kyle", ""], ["Feng", "Andrew", ""], ["Chen", "Meida", ""], ["McAlinden", "Ryan", ""]]}, {"id": "2009.00189", "submitter": "Hua Qi", "authors": "Likun Liu, Hua Qi", "title": "Object Detection-Based Variable Quantization Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a preprocessing method for conventional image and\nvideo encoders that can make these existing encoders content-aware. By going\nthrough our process, a higher quality parameter could be set on a traditional\nencoder without increasing the output size. A still frame or an image will\nfirstly go through an object detector. Either the properties of the detection\nresult will decide the parameters of the following procedures, or the system\nwill be bypassed if no object is detected in the given frame. The processing\nmethod utilizes an adaptive quantization process to determine the portion of\ndata to be dropped. This method is primarily based on the JPEG compression\ntheory and is optimum for JPEG-based encoders such as JPEG encoders and the\nMotion JPEG encoders. However, other DCT-based encoders like MPEG part 2,\nH.264, etc. can also benefit from this method. In the experiments, we compare\nthe MS-SSIM under the same bitrate as well as similar MS-SSIM but enhanced\nbitrate. As this method is based on human perception, even with similar\nMS-SSIM, the overall watching experience will be better than the direct encoded\nones.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 02:40:56 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Liu", "Likun", ""], ["Qi", "Hua", ""]]}, {"id": "2009.00191", "submitter": "Debvrat Varshney", "authors": "Debvrat Varshney, Maryam Rahnemoonfar, Masoud Yari, and John Paden", "title": "Deep Ice Layer Tracking and Thickness Estimation using Fully\n  Convolutional Networks", "comments": null, "journal-ref": null, "doi": "10.1109/BigData50022.2020.9378070", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global warming is rapidly reducing glaciers and ice sheets across the world.\nReal time assessment of this reduction is required so as to monitor its global\nclimatic impact. In this paper, we introduce a novel way of estimating the\nthickness of each internal ice layer using Snow Radar images and Fully\nConvolutional Networks. The estimated thickness can be used to understand snow\naccumulation each year. To understand the depth and structure of each internal\nice layer, we perform multi-class semantic segmentation on radar images, which\nhasn't been performed before. As the radar images lack good training labels, we\ncarry out a pre-processing technique to get a clean set of labels. After\ndetecting each ice layer uniquely, we calculate its thickness and compare it\nwith the processed ground truth. This is the first time that each ice layer is\ndetected separately and its thickness calculated through automated techniques.\nThrough this procedure we were able to estimate the ice-layer thicknesses\nwithin a Mean Absolute Error of approximately 3.6 pixels. Such a Deep Learning\nbased method can be used with ever-increasing datasets to make accurate\nassessments for cryospheric studies.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 02:43:59 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 16:20:21 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 08:30:02 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Varshney", "Debvrat", ""], ["Rahnemoonfar", "Maryam", ""], ["Yari", "Masoud", ""], ["Paden", "John", ""]]}, {"id": "2009.00206", "submitter": "Zhidong Liang", "authors": "Zhidong Liang, Ming Zhang, Zehan Zhang, Xian Zhao, Shiliang Pu", "title": "RangeRCNN: Towards Fast and Accurate 3D Object Detection with Range\n  Image Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RangeRCNN, a novel and effective 3D object detection framework\nbased on the range image representation. Most existing methods are voxel-based\nor point-based. Though several optimizations have been introduced to ease the\nsparsity issue and speed up the running time, the two representations are still\ncomputationally inefficient. Compared to them, the range image representation\nis dense and compact which can exploit powerful 2D convolution. Even so, the\nrange image is not preferred in 3D object detection due to scale variation and\nocclusion. In this paper, we utilize the dilated residual block (DRB) to better\nadapt different object scales and obtain a more flexible receptive field.\nConsidering scale variation and occlusion, we propose the RV-PV-BEV (range\nview-point view-bird's eye view) module to transfer features from RV to BEV.\nThe anchor is defined in BEV which avoids scale variation and occlusion.\nNeither RV nor BEV can provide enough information for height estimation;\ntherefore, we propose a two-stage RCNN for better 3D detection performance. The\naforementioned point view not only serves as a bridge from RV to BEV but also\nprovides pointwise features for RCNN. Experiments show that RangeRCNN achieves\nstate-of-the-art performance on the KITTI dataset and the Waymo Open dataset,\nand provides more possibilities for real-time 3D object detection. We further\nintroduce and discuss the data augmentation strategy for the range image based\nmethod, which will be very valuable for future research on range image.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 03:28:13 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 06:53:11 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Liang", "Zhidong", ""], ["Zhang", "Ming", ""], ["Zhang", "Zehan", ""], ["Zhao", "Xian", ""], ["Pu", "Shiliang", ""]]}, {"id": "2009.00210", "submitter": "Yang Liu", "authors": "Yang Liu, Keze Wang, Guanbin Li, Liang Lin", "title": "Semantics-aware Adaptive Knowledge Distillation for Sensor-to-Vision\n  Action Recognition", "comments": "This paper focuses on the sensor-to-vision heterogenous action\n  recognition problem. Code is available at\n  https://github.com/YangLiu9208/SAKDN", "journal-ref": null, "doi": "10.1109/TIP.2021.3086590", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing vision-based action recognition is susceptible to occlusion and\nappearance variations, while wearable sensors can alleviate these challenges by\ncapturing human motion with one-dimensional time-series signal. For the same\naction, the knowledge learned from vision sensors and wearable sensors, may be\nrelated and complementary. However, there exists significantly large modality\ndifference between action data captured by wearable-sensor and vision-sensor in\ndata dimension, data distribution and inherent information content. In this\npaper, we propose a novel framework, named Semantics-aware Adaptive Knowledge\nDistillation Networks (SAKDN), to enhance action recognition in vision-sensor\nmodality (videos) by adaptively transferring and distilling the knowledge from\nmultiple wearable sensors. The SAKDN uses multiple wearable-sensors as teacher\nmodalities and uses RGB videos as student modality. To preserve local temporal\nrelationship and facilitate employing visual deep learning model, we transform\none-dimensional time-series signals of wearable sensors to two-dimensional\nimages by designing a gramian angular field based virtual image generation\nmodel. Then, we build a novel Similarity-Preserving Adaptive Multi-modal Fusion\nModule to adaptively fuse intermediate representation knowledge from different\nteacher networks. Finally, to fully exploit and transfer the knowledge of\nmultiple well-trained teacher networks to the student network, we propose a\nnovel Graph-guided Semantically Discriminative Mapping loss, which utilizes\ngraph-guided ablation analysis to produce a good visual explanation\nhighlighting the important regions across modalities and concurrently\npreserving the interrelations of original data. Experimental results on\nBerkeley-MHAD, UTD-MHAD and MMAct datasets well demonstrate the effectiveness\nof our proposed SAKDN.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 03:38:31 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 09:33:11 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2021 08:16:38 GMT"}, {"version": "v4", "created": "Wed, 24 Mar 2021 12:31:34 GMT"}, {"version": "v5", "created": "Thu, 27 May 2021 07:16:45 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Liu", "Yang", ""], ["Wang", "Keze", ""], ["Li", "Guanbin", ""], ["Lin", "Liang", ""]]}, {"id": "2009.00215", "submitter": "Orhun Utku Aydin", "authors": "Orhun Utku Aydin, Abdel Aziz Taha, Adam Hilbert, Ahmed A. Khalil,\n  Ivana Galinovic, Jochen B. Fiebach, Dietmar Frey and Vince Istvan Madai", "title": "On The Usage Of Average Hausdorff Distance For Segmentation Performance\n  Assessment: Hidden Bias When Used For Ranking", "comments": "Added Disclosures, changed typo in Discussion", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Average Hausdorff Distance (AVD) is a widely used performance measure to\ncalculate the distance between two point sets. In medical image segmentation,\nAVD is used to compare ground truth images with segmentation results allowing\ntheir ranking. We identified, however, a ranking bias of AVD making it less\nsuitable for segmentation ranking. To mitigate this bias, we present a modified\ncalculation of AVD that we have coined balanced AVD (bAVD). To simulate\nsegmentations for ranking, we manually created non-overlapping segmentation\nerrors common in cerebral vessel segmentation as our use-case. Adding the\ncreated errors consecutively and randomly to the ground truth, we created sets\nof simulated segmentations with increasing number of errors. Each set of\nsimulated segmentations was ranked using AVD and bAVD. We calculated the\nKendall-rank-correlation-coefficient between the segmentation ranking and the\nnumber of errors in each simulated segmentation. The rankings produced by bAVD\nhad a significantly higher average correlation (0.969) than those of AVD\n(0.847). In 200 total rankings, bAVD misranked 52 and AVD misranked 179\nsegmentations. Our proposed evaluation measure, bAVD, alleviates AVDs ranking\nbias making it more suitable for rankings and quality assessment of\nsegmentations.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 03:58:16 GMT"}, {"version": "v2", "created": "Sun, 13 Sep 2020 13:37:14 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Aydin", "Orhun Utku", ""], ["Taha", "Abdel Aziz", ""], ["Hilbert", "Adam", ""], ["Khalil", "Ahmed A.", ""], ["Galinovic", "Ivana", ""], ["Fiebach", "Jochen B.", ""], ["Frey", "Dietmar", ""], ["Madai", "Vince Istvan", ""]]}, {"id": "2009.00221", "submitter": "Cedric Le Gentil", "authors": "Cedric Le Gentil, Mallikarjuna Vayugundla, Riccardo Giubilato,\n  Wolfgang St\\\"urzl, Teresa Vidal-Calleja, Rudolph Triebel", "title": "Gaussian Process Gradient Maps for Loop-Closure Detection in\n  Unstructured Planetary Environments", "comments": "This work is accepted for presentation at the 2020 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS). Please\n  find IEEE's copyright statement at the bottom of the first page. Cedric Le\n  Gentil and Mallikarjuna Vayugundla share the first authorship of this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to recognize previously mapped locations is an essential feature\nfor autonomous systems. Unstructured planetary-like environments pose a major\nchallenge to these systems due to the similarity of the terrain. As a result,\nthe ambiguity of the visual appearance makes state-of-the-art visual place\nrecognition approaches less effective than in urban or man-made environments.\nThis paper presents a method to solve the loop closure problem using only\nspatial information. The key idea is to use a novel continuous and\nprobabilistic representations of terrain elevation maps. Given 3D point clouds\nof the environment, the proposed approach exploits Gaussian Process (GP)\nregression with linear operators to generate continuous gradient maps of the\nterrain elevation information. Traditional image registration techniques are\nthen used to search for potential matches. Loop closures are verified by\nleveraging both the spatial characteristic of the elevation maps (SE(2)\nregistration) and the probabilistic nature of the GP representation. A\nsubmap-based localization and mapping framework is used to demonstrate the\nvalidity of the proposed approach. The performance of this pipeline is\nevaluated and benchmarked using real data from a rover that is equipped with a\nstereo camera and navigates in challenging, unstructured planetary-like\nenvironments in Morocco and on Mt. Etna.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 04:41:40 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Gentil", "Cedric Le", ""], ["Vayugundla", "Mallikarjuna", ""], ["Giubilato", "Riccardo", ""], ["St\u00fcrzl", "Wolfgang", ""], ["Vidal-Calleja", "Teresa", ""], ["Triebel", "Rudolph", ""]]}, {"id": "2009.00225", "submitter": "Baosheng Yu", "authors": "Baosheng Yu, Dacheng Tao", "title": "Heatmap Regression via Randomized Rounding", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heatmap regression has become the mainstream methodology for deep\nlearning-based semantic landmark localization, including in facial landmark\nlocalization and human pose estimation. Though heatmap regression is robust to\nlarge variations in pose, illumination, and occlusion in unconstrained\nsettings, it usually suffers from a sub-pixel localization problem.\nSpecifically, considering that the activation point indices in heatmaps are\nalways integers, quantization error thus appears when using heatmaps as the\nrepresentation of numerical coordinates. Previous methods to overcome the\nsub-pixel localization problem usually rely on high-resolution heatmaps. As a\nresult, there is always a trade-off between achieving localization accuracy and\ncomputational cost, where the computational complexity of heatmap regression\ndepends on the heatmap resolution in a quadratic manner. In this paper, we\nformally analyze the quantization error of vanilla heatmap regression and\npropose a simple yet effective quantization system to address the sub-pixel\nlocalization problem. The proposed quantization system induced by the\nrandomized rounding operation 1) encodes the fractional part of numerical\ncoordinates into the ground truth heatmap using a probabilistic approach during\ntraining; and 2) decodes the predicted numerical coordinates from a set of\nactivation points during testing. We prove that the proposed quantization\nsystem for heatmap regression is unbiased and lossless. Experimental results on\nfour popular facial landmark localization datasets (WFLW, 300W, COFW, and AFLW)\ndemonstrate the effectiveness of the proposed method for efficient and accurate\nsemantic landmark localization. Code is available at\nhttp://github.com/baoshengyu/H3R.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 04:54:22 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Yu", "Baosheng", ""], ["Tao", "Dacheng", ""]]}, {"id": "2009.00242", "submitter": "Usman Ali", "authors": "Usman Ali, Bayram Bayramli, Hongtao Lu", "title": "Temporal Continuity Based Unsupervised Learning for Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-id) aims to match the same person from images\ntaken across multiple cameras. Most existing person re-id methods generally\nrequire a large amount of identity labeled data to act as discriminative\nguideline for representation learning. Difficulty in manually collecting\nidentity labeled data leads to poor adaptability in practical scenarios. To\novercome this problem, we propose an unsupervised center-based clustering\napproach capable of progressively learning and exploiting the underlying re-id\ndiscriminative information from temporal continuity within a camera. We call\nour framework Temporal Continuity based Unsupervised Learning (TCUL).\nSpecifically, TCUL simultaneously does center based clustering of unlabeled\n(target) dataset and fine-tunes a convolutional neural network (CNN)\npre-trained on irrelevant labeled (source) dataset to enhance discriminative\ncapability of the CNN for the target dataset. Furthermore, it exploits\ntemporally continuous nature of images within-camera jointly with spatial\nsimilarity of feature maps across-cameras to generate reliable pseudo-labels\nfor training a re-identification model. As the training progresses, number of\nreliable samples keep on growing adaptively which in turn boosts representation\nability of the CNN. Extensive experiments on three large-scale person re-id\nbenchmark datasets are conducted to compare our framework with state-of-the-art\ntechniques, which demonstrate superiority of TCUL over existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 05:29:30 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Ali", "Usman", ""], ["Bayramli", "Bayram", ""], ["Lu", "Hongtao", ""]]}, {"id": "2009.00258", "submitter": "Fabio Poiesi", "authors": "Fabio Poiesi and Davide Boscaini", "title": "Distinctive 3D local deep descriptors", "comments": "IEEE International Conference on Pattern Recognition 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple but yet effective method for learning distinctive 3D\nlocal deep descriptors (DIPs) that can be used to register point clouds without\nrequiring an initial alignment. Point cloud patches are extracted,\ncanonicalised with respect to their estimated local reference frame and encoded\ninto rotation-invariant compact descriptors by a PointNet-based deep neural\nnetwork. DIPs can effectively generalise across different sensor modalities\nbecause they are learnt end-to-end from locally and randomly sampled points.\nBecause DIPs encode only local geometric information, they are robust to\nclutter, occlusions and missing regions. We evaluate and compare DIPs against\nalternative hand-crafted and deep descriptors on several indoor and outdoor\ndatasets consisting of point clouds reconstructed using different sensors.\nResults show that DIPs (i) achieve comparable results to the state-of-the-art\non RGB-D indoor scenes (3DMatch dataset), (ii) outperform state-of-the-art by a\nlarge margin on laser-scanner outdoor scenes (ETH dataset), and (iii)\ngeneralise to indoor scenes reconstructed with the Visual-SLAM system of\nAndroid ARCore. Source code: https://github.com/fabiopoiesi/dip.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 06:25:06 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 14:01:52 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Poiesi", "Fabio", ""], ["Boscaini", "Davide", ""]]}, {"id": "2009.00268", "submitter": "Marco Mobilio", "authors": "Anna Ferrari, Daniela Micucci, Marco Mobilio, Paolo Napoletano", "title": "Personalization in Human Activity Recognition", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years there has been a growing interest in techniques able to\nautomatically recognize activities performed by people. This field is known as\nHuman Activity recognition (HAR). HAR can be crucial in monitoring the\nwellbeing of the people, with special regard to the elder population and those\npeople affected by degenerative conditions. One of the main challenges concerns\nthe diversity of the population and how the same activities can be performed in\ndifferent ways due to physical characteristics and life-style. In this paper we\nexplore the possibility of exploiting physical characteristics and signal\nsimilarity to achieve better results with respect to deep learning classifiers\nthat do not rely on this information.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 06:59:17 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Ferrari", "Anna", ""], ["Micucci", "Daniela", ""], ["Mobilio", "Marco", ""], ["Napoletano", "Paolo", ""]]}, {"id": "2009.00294", "submitter": "Leyuan Wang", "authors": "Leyuan Wang, Kunbo Zhang, Min Ren, Yunlong Wang, Zhenan Sun", "title": "Recognition Oriented Iris Image Quality Assessment in the Feature Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large portion of iris images captured in real world scenarios are poor\nquality due to the uncontrolled environment and the non-cooperative subject. To\nensure that the recognition algorithm is not affected by low-quality images,\ntraditional hand-crafted factors based methods discard most images, which will\ncause system timeout and disrupt user experience. In this paper, we propose a\nrecognition-oriented quality metric and assessment method for iris image to\ndeal with the problem. The method regards the iris image embeddings Distance in\nFeature Space (DFS) as the quality metric and the prediction is based on deep\nneural networks with the attention mechanism. The quality metric proposed in\nthis paper can significantly improve the performance of the recognition\nalgorithm while reducing the number of images discarded for recognition, which\nis advantageous over hand-crafted factors based iris quality assessment\nmethods. The relationship between Image Rejection Rate (IRR) and Equal Error\nRate (EER) is proposed to evaluate the performance of the quality assessment\nalgorithm under the same image quality distribution and the same recognition\nalgorithm. Compared with hand-crafted factors based methods, the proposed\nmethod is a trial to bridge the gap between the image quality assessment and\nbiometric recognition. The code is available at\nhttps://github.com/Debatrix/DFSNet.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 08:58:18 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 06:47:49 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Wang", "Leyuan", ""], ["Zhang", "Kunbo", ""], ["Ren", "Min", ""], ["Wang", "Yunlong", ""], ["Sun", "Zhenan", ""]]}, {"id": "2009.00299", "submitter": "Necati Cihan Camgoz", "authors": "Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, Richard Bowden", "title": "Multi-channel Transformers for Multi-articulatory Sign Language\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign languages use multiple asynchronous information channels (articulators),\nnot just the hands but also the face and body, which computational approaches\noften ignore. In this paper we tackle the multi-articulatory sign language\ntranslation task and propose a novel multi-channel transformer architecture.\nThe proposed architecture allows both the inter and intra contextual\nrelationships between different sign articulators to be modelled within the\ntransformer network itself, while also maintaining channel specific\ninformation. We evaluate our approach on the RWTH-PHOENIX-Weather-2014T dataset\nand report competitive translation performance. Importantly, we overcome the\nreliance on gloss annotations which underpin other state-of-the-art approaches,\nthereby removing future need for expensive curated datasets.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 09:10:55 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Camgoz", "Necati Cihan", ""], ["Koller", "Oscar", ""], ["Hadfield", "Simon", ""], ["Bowden", "Richard", ""]]}, {"id": "2009.00300", "submitter": "Radu Tudor Ionescu", "authors": "Cezara Benegui and Radu Tudor Ionescu", "title": "To augment or not to augment? Data augmentation in user identification\n  based on motion sensors", "comments": "Extended version (12 pages, 2 figures) of our paper (9 pages)\n  accepted at ICONIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, commonly-used authentication systems for mobile device users, e.g.\npassword checking, face recognition or fingerprint scanning, are susceptible to\nvarious kinds of attacks. In order to prevent some of the possible attacks,\nthese explicit authentication systems can be enhanced by considering a\ntwo-factor authentication scheme, in which the second factor is an implicit\nauthentication system based on analyzing motion sensor data captured by\naccelerometers or gyroscopes. In order to avoid any additional burdens to the\nuser, the registration process of the implicit authentication system must be\nperformed quickly, i.e. the number of data samples collected from the user is\ntypically small. In the context of designing a machine learning model for\nimplicit user authentication based on motion signals, data augmentation can\nplay an important role. In this paper, we study several data augmentation\ntechniques in the quest of finding useful augmentation methods for motion\nsensor data. We propose a set of four research questions related to data\naugmentation in the context of few-shot user identification based on motion\nsensor signals. We conduct experiments on a benchmark data set, using two deep\nlearning architectures, convolutional neural networks and Long Short-Term\nMemory networks, showing which and when data augmentation methods bring\naccuracy improvements. Interestingly, we find that data augmentation is not\nvery helpful, most likely because the signal patterns useful to discriminate\nusers are too sensitive to the transformations brought by certain data\naugmentation techniques. This result is somewhat contradictory to the common\nbelief that data augmentation is expected to increase the accuracy of machine\nlearning models.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 09:11:12 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Benegui", "Cezara", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "2009.00303", "submitter": "Yuchao Gu", "authors": "Yu-Chao Gu, Le Zhang, Yun Liu, Shao-Ping Lu, Ming-Ming Cheng", "title": "Generalized Zero-Shot Learning via VAE-Conditioned Generative Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized zero-shot learning (GZSL) aims to recognize both seen and unseen\nclasses by transferring knowledge from semantic descriptions to visual\nrepresentations. Recent generative methods formulate GZSL as a missing data\nproblem, which mainly adopts GANs or VAEs to generate visual features for\nunseen classes. However, GANs often suffer from instability, and VAEs can only\noptimize the lower bound on the log-likelihood of observed data. To overcome\nthe above limitations, we resort to generative flows, a family of generative\nmodels with the advantage of accurate likelihood estimation. More specifically,\nwe propose a conditional version of generative flows for GZSL, i.e.,\nVAE-Conditioned Generative Flow (VAE-cFlow). By using VAE, the semantic\ndescriptions are firstly encoded into tractable latent distributions,\nconditioned on that the generative flow optimizes the exact log-likelihood of\nthe observed visual features. We ensure the conditional latent distribution to\nbe both semantic meaningful and inter-class discriminative by i) adopting the\nVAE reconstruction objective, ii) releasing the zero-mean constraint in VAE\nposterior regularization, and iii) adding a classification regularization on\nthe latent variables. Our method achieves state-of-the-art GZSL results on five\nwell-known benchmark datasets, especially for the significant improvement in\nthe large-scale setting. Code is released at\nhttps://github.com/guyuchao/VAE-cFlow-ZSL.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 09:12:31 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Gu", "Yu-Chao", ""], ["Zhang", "Le", ""], ["Liu", "Yun", ""], ["Lu", "Shao-Ping", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "2009.00312", "submitter": "Jingchen Sun", "authors": "Jingchen Sun, Jiming Chen, Tao Chen, Jiayuan Fan, Shibo He", "title": "PIDNet: An Efficient Network for Dynamic Pedestrian Intrusion Detection", "comments": "Proceedings of the 28th ACM International Conference on Multimedia\n  (MM '20), October 12--16, 2020, Seattle, WA, USA", "journal-ref": null, "doi": "10.1145/3394171.3413837", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based dynamic pedestrian intrusion detection (PID), judging whether\npedestrians intrude an area-of-interest (AoI) by a moving camera, is an\nimportant task in mobile surveillance. The dynamically changing AoIs and a\nnumber of pedestrians in video frames increase the difficulty and computational\ncomplexity of determining whether pedestrians intrude the AoI, which makes\nprevious algorithms incapable of this task. In this paper, we propose a novel\nand efficient multi-task deep neural network, PIDNet, to solve this problem.\nPIDNet is mainly designed by considering two factors: accurately segmenting the\ndynamically changing AoIs from a video frame captured by the moving camera and\nquickly detecting pedestrians from the generated AoI-contained areas. Three\nefficient network designs are proposed and incorporated into PIDNet to reduce\nthe computational complexity: 1) a special PID task backbone for feature\nsharing, 2) a feature cropping module for feature cropping, and 3) a lighter\ndetection branch network for feature compression. In addition, considering\nthere are no public datasets and benchmarks in this field, we establish a\nbenchmark dataset to evaluate the proposed network and give the corresponding\nevaluation metrics for the first time. Experimental results show that PIDNet\ncan achieve 67.1% PID accuracy and 9.6 fps inference speed on the proposed\ndataset, which serves as a good baseline for the future vision-based dynamic\nPID study.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 09:34:43 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Sun", "Jingchen", ""], ["Chen", "Jiming", ""], ["Chen", "Tao", ""], ["Fan", "Jiayuan", ""], ["He", "Shibo", ""]]}, {"id": "2009.00320", "submitter": "Lei Ding", "authors": "Bing Liu, Anzhu Yu, Pengqiang Zhang, Lei Ding, Wenyue Guo, Kuiliang\n  Gao, Xibing Zuo", "title": "Active Deep Densely Connected Convolutional Network for Hyperspectral\n  Image Classification", "comments": null, "journal-ref": null, "doi": "10.1080/01431161.2021.1931542", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based methods have seen a massive rise in popularity for\nhyperspectral image classification over the past few years. However, the\nsuccess of deep learning is attributed greatly to numerous labeled samples. It\nis still very challenging to use only a few labeled samples to train deep\nlearning models to reach a high classification accuracy. An active\ndeep-learning framework trained by an end-to-end manner is, therefore, proposed\nby this paper in order to minimize the hyperspectral image classification\ncosts. First, a deep densely connected convolutional network is considered for\nhyperspectral image classification. Different from the traditional active\nlearning methods, an additional network is added to the designed deep densely\nconnected convolutional network to predict the loss of input samples. Then, the\nadditional network could be used to suggest unlabeled samples that the deep\ndensely connected convolutional network is more likely to produce a wrong\nlabel. Note that the additional network uses the intermediate features of the\ndeep densely connected convolutional network as input. Therefore, the proposed\nmethod is an end-to-end framework. Subsequently, a few of the selected samples\nare labelled manually and added to the training samples. The deep densely\nconnected convolutional network is therefore trained using the new training\nset. Finally, the steps above are repeated to train the whole framework\niteratively. Extensive experiments illustrates that the method proposed could\nreach a high accuracy in classification after selecting just a few samples.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 09:53:38 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 11:46:16 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Liu", "Bing", ""], ["Yu", "Anzhu", ""], ["Zhang", "Pengqiang", ""], ["Ding", "Lei", ""], ["Guo", "Wenyue", ""], ["Gao", "Kuiliang", ""], ["Zuo", "Xibing", ""]]}, {"id": "2009.00325", "submitter": "Mayu Otani", "authors": "Mayu Otani, Yuta Nakashima, Esa Rahtu, Janne Heikkil\\\"a", "title": "Uncovering Hidden Challenges in Query-Based Video Moment Retrieval", "comments": "British Machine Vision Conference (BMVC), 2020. (v2) added references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The query-based moment retrieval is a problem of localising a specific clip\nfrom an untrimmed video according a query sentence. This is a challenging task\nthat requires interpretation of both the natural language query and the video\ncontent. Like in many other areas in computer vision and machine learning, the\nprogress in query-based moment retrieval is heavily driven by the benchmark\ndatasets and, therefore, their quality has significant impact on the field. In\nthis paper, we present a series of experiments assessing how well the benchmark\nresults reflect the true progress in solving the moment retrieval task. Our\nresults indicate substantial biases in the popular datasets and unexpected\nbehaviour of the state-of-the-art models. Moreover, we present new sanity check\nexperiments and approaches for visualising the results. Finally, we suggest\npossible directions to improve the temporal sentence grounding in the future.\nOur code for this paper is available at\nhttps://mayu-ot.github.io/hidden-challenges-MR .\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 10:07:23 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 10:15:13 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Otani", "Mayu", ""], ["Nakashima", "Yuta", ""], ["Rahtu", "Esa", ""], ["Heikkil\u00e4", "Janne", ""]]}, {"id": "2009.00330", "submitter": "Alvaro Hernandez", "authors": "A. Hern\\'andez, S. Woo, H. Corrales, I. Parra, E. Kim, D. F. Llorca\n  and M. A. Sotelo", "title": "3D-DEEP: 3-Dimensional Deep-learning based on elevation patterns forroad\n  scene interpretation", "comments": "\"This work has been accepted for publication at IEEE Intelligent\n  Vehicle Symposium 2020\"", "journal-ref": null, "doi": "10.1109/IV47402.2020.9304601", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road detection and segmentation is a crucial task in computer vision for safe\nautonomous driving. With this in mind, a new net architecture (3D-DEEP) and its\nend-to-end training methodology for CNN-based semantic segmentation are\ndescribed along this paper for. The method relies on disparity filtered and\nLiDAR projected images for three-dimensional information and image feature\nextraction through fully convolutional networks architectures. The developed\nmodels were trained and validated over Cityscapes dataset using just fine\nannotation examples with 19 different training classes, and over KITTI road\ndataset. 72.32% mean intersection over union(mIoU) has been obtained for the 19\nCityscapes training classes using the validation images. On the other hand,\nover KITTIdataset the model has achieved an F1 error value of 97.85%\ninvalidation and 96.02% using the test images.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 10:18:08 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 12:05:54 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Hern\u00e1ndez", "A.", ""], ["Woo", "S.", ""], ["Corrales", "H.", ""], ["Parra", "I.", ""], ["Kim", "E.", ""], ["Llorca", "D. F.", ""], ["Sotelo", "M. A.", ""]]}, {"id": "2009.00348", "submitter": "Adrian Llopart", "authors": "Adrian Llopart", "title": "LiftFormer: 3D Human Pose Estimation using attention models", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the 3D position of human joints has become a widely researched\ntopic in the last years. Special emphasis has gone into defining novel methods\nthat extrapolate 2-dimensional data (keypoints) into 3D, namely predicting the\nroot-relative coordinates of joints associated to human skeletons. The latest\nresearch trends have proven that the Transformer Encoder blocks aggregate\ntemporal information significantly better than previous approaches. Thus, we\npropose the usage of these models to obtain more accurate 3D predictions by\nleveraging temporal information using attention mechanisms on ordered sequences\nhuman poses in videos.\n  Our method consistently outperforms the previous best results from the\nliterature when using both 2D keypoint predictors by 0.3 mm (44.8 MPJPE, 0.7%\nimprovement) and ground truth inputs by 2mm (MPJPE: 31.9, 8.4% improvement) on\nHuman3.6M. It also achieves state-of-the-art performance on the HumanEva-I\ndataset with 10.5 P-MPJPE (22.2% reduction). The number of parameters in our\nmodel is easily tunable and is smaller (9.5M) than current methodologies\n(16.95M and 11.25M) whilst still having better performance. Thus, our 3D\nlifting model's accuracy exceeds that of other end-to-end or SMPL approaches\nand is comparable to many multi-view methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 11:05:45 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Llopart", "Adrian", ""]]}, {"id": "2009.00382", "submitter": "Norimichi Ukita", "authors": "Tomoki Yoshida and Kazutoshi Akita and Muhammad Haris and Norimichi\n  Ukita", "title": "Image Super-Resolution using Explicit Perceptual Loss", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an explicit way to optimize the super-resolution network\nfor generating visually pleasing images. The previous approaches use several\nloss functions which is hard to interpret and has the implicit relationships to\nimprove the perceptual score. We show how to exploit the machine learning based\nmodel which is directly trained to provide the perceptual score on generated\nimages. It is believed that these models can be used to optimizes the\nsuper-resolution network which is easier to interpret. We further analyze the\ncharacteristic of the existing loss and our proposed explicit perceptual loss\nfor better interpretation. The experimental results show the explicit approach\nhas a higher perceptual score than other approaches. Finally, we demonstrate\nthe relation of explicit perceptual loss and visually pleasing images using\nsubjective evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 12:22:39 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Yoshida", "Tomoki", ""], ["Akita", "Kazutoshi", ""], ["Haris", "Muhammad", ""], ["Ukita", "Norimichi", ""]]}, {"id": "2009.00402", "submitter": "Liqi Yan", "authors": "Liqi Yan and Dongfang Liu and Yaoxian Song and Changbin Yu", "title": "Multimodal Aggregation Approach for Memory Vision-Voice Indoor\n  Navigation with Meta-Learning", "comments": "8 pages, 6 figures, 2 tables, accepted at 2020 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision and voice are two vital keys for agents' interaction and learning. In\nthis paper, we present a novel indoor navigation model called Memory\nVision-Voice Indoor Navigation (MVV-IN), which receives voice commands and\nanalyzes multimodal information of visual observation in order to enhance\nrobots' environment understanding. We make use of single RGB images taken by a\nfirst-view monocular camera. We also apply a self-attention mechanism to keep\nthe agent focusing on key areas. Memory is important for the agent to avoid\nrepeating certain tasks unnecessarily and in order for it to adapt adequately\nto new scenes, therefore, we make use of meta-learning. We have experimented\nwith various functional features extracted from visual observation. Comparative\nexperiments prove that our methods outperform state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 13:12:27 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Yan", "Liqi", ""], ["Liu", "Dongfang", ""], ["Song", "Yaoxian", ""], ["Yu", "Changbin", ""]]}, {"id": "2009.00453", "submitter": "Gabriel Spadon", "authors": "Bruno Brandoli, Gabriel Spadon, Travis Esau, Patrick Hennessy, Andre\n  C. P. L. Carvalho, Jose F. Rodrigues-Jr, and Sihem Amer-Yahia", "title": "DropLeaf: a precision farming smartphone application for measuring\n  pesticide spraying methods", "comments": "Submitted to Computers and Electronics in Agriculture. arXiv admin\n  note: text overlap with arXiv:1711.07828", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pesticide application has been heavily used in the cultivation of major\ncrops, contributing to the increase of crop production over the past decades.\nHowever, their appropriate use and calibration of machines rely upon evaluation\nmethodologies that can precisely estimate how well the pesticides' spraying\ncovered the crops. A few strategies have been proposed in former works, yet\ntheir elevated costs and low portability do not permit their wide adoption.\nThis work introduces and experimentally assesses a novel tool that functions\nover a smartphone-based mobile application, named DropLeaf - Spraying Meter.\nTests performed using DropLeaf demonstrated that, notwithstanding its\nversatility, it can estimate the pesticide spraying with high precision. Our\nmethodology is based on image analysis, and the assessment of spraying\ndeposition measures is performed successfully over real and synthetic\nwater-sensitive papers. The proposed tool can be extensively used by farmers\nand agronomists furnished with regular smartphones, improving the utilization\nof pesticides with well-being, ecological, and monetary advantages. DropLeaf\ncan be easily used for spray drift assessment of different methods, including\nemerging UAV (Unmanned Aerial Vehicle) sprayers.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 15:51:06 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Brandoli", "Bruno", ""], ["Spadon", "Gabriel", ""], ["Esau", "Travis", ""], ["Hennessy", "Patrick", ""], ["Carvalho", "Andre C. P. L.", ""], ["Rodrigues-Jr", "Jose F.", ""], ["Amer-Yahia", "Sihem", ""]]}, {"id": "2009.00463", "submitter": "Min H. Kim", "authors": "Seung-Hwan Baek, Hayato Ikoma, Daniel S. Jeon, Yuqi Li, Wolfgang\n  Heidrich, Gordon Wetzstein, Min H. Kim", "title": "Single-shot Hyperspectral-Depth Imaging with Learned Diffractive Optics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging depth and spectrum have been extensively studied in isolation from\neach other for decades. Recently, hyperspectral-depth (HS-D) imaging emerges to\ncapture both information simultaneously by combining two different imaging\nsystems; one for depth, the other for spectrum. While being accurate, this\ncombinational approach induces increased form factor, cost, capture time, and\nalignment/registration problems. In this work, departing from the combinational\nprinciple, we propose a compact single-shot monocular HS-D imaging method. Our\nmethod uses a diffractive optical element (DOE), the point spread function of\nwhich changes with respect to both depth and spectrum. This enables us to\nreconstruct spectrum and depth from a single captured image. To this end, we\ndevelop a differentiable simulator and a neural-network-based reconstruction\nthat are jointly optimized via automatic differentiation. To facilitate\nlearning the DOE, we present a first HS-D dataset by building a benchtop HS-D\nimager that acquires high-quality ground truth. We evaluate our method with\nsynthetic and real experiments by building an experimental prototype and\nachieve state-of-the-art HS-D imaging results.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 14:19:35 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 13:18:38 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Baek", "Seung-Hwan", ""], ["Ikoma", "Hayato", ""], ["Jeon", "Daniel S.", ""], ["Li", "Yuqi", ""], ["Heidrich", "Wolfgang", ""], ["Wetzstein", "Gordon", ""], ["Kim", "Min H.", ""]]}, {"id": "2009.00508", "submitter": "Kai Dierkes", "authors": "Marc Tonsen, Chris Kay Baumann, Kai Dierkes", "title": "A High-Level Description and Performance Evaluation of Pupil Invisible", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Head-mounted eye trackers promise convenient access to reliable gaze data in\nunconstrained environments. Due to several limitations, however, often they can\nonly partially deliver on this promise.\n  Among those are the following: (i) the necessity of performing a device setup\nand calibration prior to every use of the eye tracker, (ii) a lack of\nrobustness of gaze-estimation results against perturbations, such as outdoor\nlighting conditions and unavoidable slippage of the eye tracker on the head of\nthe subject, and (iii) behavioral distortion resulting from social awkwardness,\ndue to the unnatural appearance of current head-mounted eye trackers.\n  Recently, Pupil Labs released Pupil Invisible glasses, a head-mounted eye\ntracker engineered to tackle these limitations. Here, we present an extensive\nevaluation of its gaze-estimation capabilities. To this end, we designed a\ndata-collection protocol and evaluation scheme geared towards providing a\nfaithful portrayal of the real-world usage of Pupil Invisible glasses.\n  In particular, we develop a geometric framework for gauging gaze-estimation\naccuracy that goes beyond reporting mean angular accuracy. We demonstrate that\nPupil Invisible glasses, without the need of a calibration, provide gaze\nestimates which are robust to perturbations, including outdoor lighting\nconditions and slippage of the headset.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 15:10:10 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Tonsen", "Marc", ""], ["Baumann", "Chris Kay", ""], ["Dierkes", "Kai", ""]]}, {"id": "2009.00520", "submitter": "Weikai Li", "authors": "Weikai Li and Songcan Chen", "title": "Unsupervised Domain Adaptation with Progressive Adaptation of Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised Domain Adaptation (UDA) aims to classify unlabeled target domain\nby transferring knowledge from labeled source domain with domain shift. Most of\nthe existing UDA methods try to mitigate the adverse impact induced by the\nshift via reducing domain discrepancy. However, such approaches easily suffer a\nnotorious mode collapse issue due to the lack of labels in target domain.\nNaturally, one of the effective ways to mitigate this issue is to reliably\nestimate the pseudo labels for target domain, which itself is hard. To overcome\nthis, we propose a novel UDA method named Progressive Adaptation of Subspaces\napproach (PAS) in which we utilize such an intuition that appears much\nreasonable to gradually obtain reliable pseudo labels. Speci fically, we\nprogressively and steadily refine the shared subspaces as bridge of knowledge\ntransfer by adaptively anchoring/selecting and leveraging those target samples\nwith reliable pseudo labels. Subsequently, the refined subspaces can in turn\nprovide more reliable pseudo-labels of the target domain, making the mode\ncollapse highly mitigated. Our thorough evaluation demonstrates that PAS is not\nonly effective for common UDA, but also outperforms the state-of-the arts for\nmore challenging Partial Domain Adaptation (PDA) situation, where the source\nlabel set subsumes the target one.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 15:40:50 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Li", "Weikai", ""], ["Chen", "Songcan", ""]]}, {"id": "2009.00540", "submitter": "Prasanna Date", "authors": "Prasanna Date, Christopher D. Carothers, John E. Mitchell, James A.\n  Hendler, Malik Magdon-Ismail", "title": "Training Deep Neural Networks with Constrained Learning Parameters", "comments": null, "journal-ref": null, "doi": "10.1109/ICRC2020.2020.00018", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's deep learning models are primarily trained on CPUs and GPUs. Although\nthese models tend to have low error, they consume high power and utilize large\namount of memory owing to double precision floating point learning parameters.\nBeyond the Moore's law, a significant portion of deep learning tasks would run\non edge computing systems, which will form an indispensable part of the entire\ncomputation fabric. Subsequently, training deep learning models for such\nsystems will have to be tailored and adopted to generate models that have the\nfollowing desirable characteristics: low error, low memory, and low power. We\nbelieve that deep neural networks (DNNs), where learning parameters are\nconstrained to have a set of finite discrete values, running on neuromorphic\ncomputing systems would be instrumental for intelligent edge computing systems\nhaving these desirable characteristics. To this extent, we propose the\nCombinatorial Neural Network Training Algorithm (CoNNTrA), that leverages a\ncoordinate gradient descent-based approach for training deep learning models\nwith finite discrete learning parameters. Next, we elaborate on the theoretical\nunderpinnings and evaluate the computational complexity of CoNNTrA. As a proof\nof concept, we use CoNNTrA to train deep learning models with ternary learning\nparameters on the MNIST, Iris and ImageNet data sets and compare their\nperformance to the same models trained using Backpropagation. We use following\nperformance metrics for the comparison: (i) Training error; (ii) Validation\nerror; (iii) Memory usage; and (iv) Training time. Our results indicate that\nCoNNTrA models use 32x less memory and have errors at par with the\nBackpropagation models.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 16:20:11 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Date", "Prasanna", ""], ["Carothers", "Christopher D.", ""], ["Mitchell", "John E.", ""], ["Hendler", "James A.", ""], ["Magdon-Ismail", "Malik", ""]]}, {"id": "2009.00564", "submitter": "Alexander Mathis", "authors": "Alexander Mathis and Steffen Schneider and Jessy Lauer and Mackenzie\n  W. Mathis", "title": "A Primer on Motion Capture with Deep Learning: Principles, Pitfalls and\n  Perspectives", "comments": "Review, 21 pages, 8 figures and 5 boxes", "journal-ref": "Neuron Volume 108, Issue 1, 14 October 2020, Pages 44-65", "doi": "10.1016/j.neuron.2020.09.017", "report-no": null, "categories": "cs.CV cs.LG q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting behavioral measurements non-invasively from video is stymied by\nthe fact that it is a hard computational problem. Recent advances in deep\nlearning have tremendously advanced predicting posture from videos directly,\nwhich quickly impacted neuroscience and biology more broadly. In this primer we\nreview the budding field of motion capture with deep learning. In particular,\nwe will discuss the principles of those novel algorithms, highlight their\npotential as well as pitfalls for experimentalists, and provide a glimpse into\nthe future.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 16:51:33 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 20:29:12 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Mathis", "Alexander", ""], ["Schneider", "Steffen", ""], ["Lauer", "Jessy", ""], ["Mathis", "Mackenzie W.", ""]]}, {"id": "2009.00576", "submitter": "Alberto Badias", "authors": "Alberto Badias, Iciar Alfaro, David Gonzalez, Francisco Chinesta and\n  Elias Cueto", "title": "MORPH-DSLAM: Model Order Reduction for PHysics-based Deformable SLAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a new methodology to estimate the 3D displacement field of\ndeformable objects from video sequences using standard monocular cameras. We\nsolve in real time the complete (possibly visco-)hyperelasticity problem to\nproperly describe the strain and stress fields that are consistent with the\ndisplacements captured by the images, constrained by real physics. We do not\nimpose any ad-hoc prior or energy minimization in the external surface, since\nthe real and complete mechanics problem is solved. This means that we can also\nestimate the internal state of the objects, even in occluded areas, just by\nobserving the external surface and the knowledge of material properties and\ngeometry. Solving this problem in real time using a realistic constitutive law,\nusually non-linear, is out of reach for current systems. To overcome this\ndifficulty, we solve off-line a parametrized problem that considers each source\nof variability in the problem as a new parameter and, consequently, as a new\ndimension in the formulation. Model Order Reduction methods allow us to reduce\nthe dimensionality of the problem, and therefore, its computational cost, while\npreserving the visualization of the solution in the high-dimensionality space.\nThis allows an accurate estimation of the object deformations, improving also\nthe robustness in the 3D points estimation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 17:06:41 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Badias", "Alberto", ""], ["Alfaro", "Iciar", ""], ["Gonzalez", "David", ""], ["Chinesta", "Francisco", ""], ["Cueto", "Elias", ""]]}, {"id": "2009.00577", "submitter": "Yusheng Xiang", "authors": "Jun Li, Wanrong Hong, Yusheng Xiang", "title": "A Short Review on Data Modelling for Vector Fields", "comments": "18 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning methods based on statistical principles have proven highly\nsuccessful in dealing with a wide variety of data analysis and analytics tasks.\nTraditional data models are mostly concerned with independent identically\ndistributed data. The recent success of end-to-end modelling scheme using deep\nneural networks equipped with effective structures such as convolutional layers\nor skip connections allows the extension to more sophisticated and structured\npractical data, such as natural language, images, videos, etc. On the\napplication side, vector fields are an extremely useful type of data in\nempirical sciences, as well as signal processing, e.g. non-parametric\ntransformations of 3D point clouds using 3D vector fields, the modelling of the\nfluid flow in earth science, and the modelling of physical fields.\n  This review article is dedicated to recent computational tools of vector\nfields, including vector data representations, predictive model of spatial\ndata, as well as applications in computer vision, signal processing, and\nempirical sciences.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 17:07:29 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Li", "Jun", ""], ["Hong", "Wanrong", ""], ["Xiang", "Yusheng", ""]]}, {"id": "2009.00581", "submitter": "Matthew Evanusa", "authors": "Matthew Evanusa and Cornelia Fermuller and Yiannis Aloimonos", "title": "A Deep 2-Dimensional Dynamical Spiking Neuronal Network for Temporal\n  Encoding trained with STDP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The brain is known to be a highly complex, asynchronous dynamical system that\nis highly tailored to encode temporal information. However, recent deep\nlearning approaches to not take advantage of this temporal coding. Spiking\nNeural Networks (SNNs) can be trained using biologically-realistic learning\nmechanisms, and can have neuronal activation rules that are biologically\nrelevant. This type of network is also structured fundamentally around\naccepting temporal information through a time-decaying voltage update, a kind\nof input that current rate-encoding networks have difficulty with. Here we show\nthat a large, deep layered SNN with dynamical, chaotic activity mimicking the\nmammalian cortex with biologically-inspired learning rules, such as STDP, is\ncapable of encoding information from temporal data. We argue that the\nrandomness inherent in the network weights allow the neurons to form groups\nthat encode the temporal data being inputted after self-organizing with STDP.\nWe aim to show that precise timing of input stimulus is critical in forming\nsynchronous neural groups in a layered network. We analyze the network in terms\nof network entropy as a metric of information transfer. We hope to tackle two\nproblems at once: the creation of artificial temporal neural systems for\nartificial intelligence, as well as solving coding mechanisms in the brain.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 17:12:18 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Evanusa", "Matthew", ""], ["Fermuller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "2009.00584", "submitter": "Bram Ruijsink", "authors": "Bram Ruijsink, Esther Puyol-Anton, Ye Li, Wenja Bai, Eric Kerfoot,\n  Reza Razavi, and Andrew P. King", "title": "Quality-aware semi-supervised learning for CMR segmentation", "comments": "MICCAI STACOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges in developing deep learning algorithms for medical\nimage segmentation is the scarcity of annotated training data. To overcome this\nlimitation, data augmentation and semi-supervised learning (SSL) methods have\nbeen developed. However, these methods have limited effectiveness as they\neither exploit the existing data set only (data augmentation) or risk negative\nimpact by adding poor training examples (SSL). Segmentations are rarely the\nfinal product of medical image analysis - they are typically used in downstream\ntasks to infer higher-order patterns to evaluate diseases. Clinicians take into\naccount a wealth of prior knowledge on biophysics and physiology when\nevaluating image analysis results. We have used these clinical assessments in\nprevious works to create robust quality-control (QC) classifiers for automated\ncardiac magnetic resonance (CMR) analysis. In this paper, we propose a novel\nscheme that uses QC of the downstream task to identify high quality outputs of\nCMR segmentation networks, that are subsequently utilised for further network\ntraining. In essence, this provides quality-aware augmentation of training data\nin a variant of SSL for segmentation networks (semiQCSeg). We evaluate our\napproach in two CMR segmentation tasks (aortic and short axis cardiac volume\nsegmentation) using UK Biobank data and two commonly used network architectures\n(U-net and a Fully Convolutional Network) and compare against supervised and\nSSL strategies. We show that semiQCSeg improves training of the segmentation\nnetworks. It decreases the need for labelled data, while outperforming the\nother methods in terms of Dice and clinical metrics. SemiQCSeg can be an\nefficient approach for training segmentation networks for medical image data\nwhen labelled datasets are scarce.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 17:18:22 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Ruijsink", "Bram", ""], ["Puyol-Anton", "Esther", ""], ["Li", "Ye", ""], ["Bai", "Wenja", ""], ["Kerfoot", "Eric", ""], ["Razavi", "Reza", ""], ["King", "Andrew P.", ""]]}, {"id": "2009.00603", "submitter": "Weidi Xie", "authors": "Weidi Xie, Jeffrey Byrne, Andrew Zisserman", "title": "Inducing Predictive Uncertainty Estimation for Face Recognition", "comments": "To Appear at the British Machine Vision Conference (BMVC), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowing when an output can be trusted is critical for reliably using face\nrecognition systems. While there has been enormous effort in recent research on\nimproving face verification performance, understanding when a model's\npredictions should or should not be trusted has received far less attention.\nOur goal is to assign a confidence score for a face image that reflects its\nquality in terms of recognizable information. To this end, we propose a method\nfor generating image quality training data automatically from 'mated-pairs' of\nface images, and use the generated data to train a lightweight Predictive\nConfidence Network, termed as PCNet, for estimating the confidence score of a\nface image. We systematically evaluate the usefulness of PCNet with its error\nversus reject performance, and demonstrate that it can be universally paired\nwith and improve the robustness of any verification model. We describe three\nuse cases on the public IJB-C face verification benchmark: (i) to improve 1:1\nimage-based verification error rates by rejecting low-quality face images; (ii)\nto improve quality score based fusion performance on the 1:1 set-based\nverification benchmark; and (iii) its use as a quality measure for selecting\nhigh quality (unblurred, good lighting, more frontal) faces from a collection,\ne.g. for automatic enrolment or display.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 17:52:00 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Xie", "Weidi", ""], ["Byrne", "Jeffrey", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2009.00612", "submitter": "Junaid Malik", "authors": "Junaid Malik, Serkan Kiranyaz, Moncef Gabbouj", "title": "Operational vs Convolutional Neural Networks for Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) have recently become a favored technique\nfor image denoising due to its adaptive learning ability, especially with a\ndeep configuration. However, their efficacy is inherently limited owing to\ntheir homogenous network formation with the unique use of linear convolution.\nIn this study, we propose a heterogeneous network model which allows greater\nflexibility for embedding additional non-linearity at the core of the data\ntransformation. To this end, we propose the idea of an operational neuron or\nOperational Neural Networks (ONN), which enables a flexible non-linear and\nheterogeneous configuration employing both inter and intra-layer neuronal\ndiversity. Furthermore, we propose a robust operator search strategy inspired\nby the Hebbian theory, called the Synaptic Plasticity Monitoring (SPM) which\ncan make data-driven choices for non-linearities in any architecture. An\nextensive set of comparative evaluations of ONNs and CNNs over two severe image\ndenoising problems yield conclusive evidence that ONNs enriched by non-linear\noperators can achieve a superior denoising performance against CNNs with both\nequivalent and well-known deep configurations.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 12:15:28 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Malik", "Junaid", ""], ["Kiranyaz", "Serkan", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2009.00633", "submitter": "Paul Rosin", "authors": "Paul L. Rosin, Yu-Kun Lai, David Mould, Ran Yi, Itamar Berger, Lars\n  Doyle, Seungyong Lee, Chuan Li, Yong-Jin Liu, Amir Semmo, Ariel Shamir,\n  Minjung Son, Holger Winnemoller", "title": "NPRportrait 1.0: A Three-Level Benchmark for Non-Photorealistic\n  Rendering of Portraits", "comments": "17 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent upsurge of activity in image-based non-photorealistic\nrendering (NPR), and in particular portrait image stylisation, due to the\nadvent of neural style transfer, the state of performance evaluation in this\nfield is limited, especially compared to the norms in the computer vision and\nmachine learning communities. Unfortunately, the task of evaluating image\nstylisation is thus far not well defined, since it involves subjective,\nperceptual and aesthetic aspects. To make progress towards a solution, this\npaper proposes a new structured, three level, benchmark dataset for the\nevaluation of stylised portrait images. Rigorous criteria were used for its\nconstruction, and its consistency was validated by user studies. Moreover, a\nnew methodology has been developed for evaluating portrait stylisation\nalgorithms, which makes use of the different benchmark levels as well as\nannotations provided by user studies regarding the characteristics of the\nfaces. We perform evaluation for a wide variety of image stylisation methods\n(both portrait-specific and general purpose, and also both traditional NPR\napproaches and neural style transfer) using the new benchmark dataset.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 18:04:19 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Rosin", "Paul L.", ""], ["Lai", "Yu-Kun", ""], ["Mould", "David", ""], ["Yi", "Ran", ""], ["Berger", "Itamar", ""], ["Doyle", "Lars", ""], ["Lee", "Seungyong", ""], ["Li", "Chuan", ""], ["Liu", "Yong-Jin", ""], ["Semmo", "Amir", ""], ["Shamir", "Ariel", ""], ["Son", "Minjung", ""], ["Winnemoller", "Holger", ""]]}, {"id": "2009.00638", "submitter": "Yogesh Rawat", "authors": "Yogesh S Rawat, Shruti Vyas", "title": "View-invariant action recognition", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-03243-2_878-1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition is an important problem in computer vision. It has a\nwide range of applications in surveillance, human-computer interaction,\naugmented reality, video indexing, and retrieval. The varying pattern of\nspatio-temporal appearance generated by human action is key for identifying the\nperformed action. We have seen a lot of research exploring this dynamics of\nspatio-temporal appearance for learning a visual representation of human\nactions. However, most of the research in action recognition is focused on some\ncommon viewpoints, and these approaches do not perform well when there is a\nchange in viewpoint. Human actions are performed in a 3-dimensional environment\nand are projected to a 2-dimensional space when captured as a video from a\ngiven viewpoint. Therefore, an action will have a different spatio-temporal\nappearance from different viewpoints. The research in view-invariant action\nrecognition addresses this problem and focuses on recognizing human actions\nfrom unseen viewpoints.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 18:08:46 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Rawat", "Yogesh S", ""], ["Vyas", "Shruti", ""]]}, {"id": "2009.00668", "submitter": "Daiqing Li", "authors": "Daiqing Li, Amlan Kar, Nishant Ravikumar, Alejandro F Frangi, Sanja\n  Fidler", "title": "Fed-Sim: Federated Simulation for Medical Imaging", "comments": "MICCAI 2020 (Early Accept)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labelling data is expensive and time consuming especially for domains such as\nmedical imaging that contain volumetric imaging data and require expert\nknowledge. Exploiting a larger pool of labeled data available across multiple\ncenters, such as in federated learning, has also seen limited success since\ncurrent deep learning approaches do not generalize well to images acquired with\nscanners from different manufacturers. We aim to address these problems in a\ncommon, learning-based image simulation framework which we refer to as\nFederated Simulation. We introduce a physics-driven generative approach that\nconsists of two learnable neural modules: 1) a module that synthesizes 3D\ncardiac shapes along with their materials, and 2) a CT simulator that renders\nthese into realistic 3D CT Volumes, with annotations. Since the model of\ngeometry and material is disentangled from the imaging sensor, it can\neffectively be trained across multiple medical centers. We show that our data\nsynthesis framework improves the downstream segmentation performance on several\ndatasets. Project Page: https://nv-tlabs.github.io/fed-sim/ .\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 19:17:46 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Li", "Daiqing", ""], ["Kar", "Amlan", ""], ["Ravikumar", "Nishant", ""], ["Frangi", "Alejandro F", ""], ["Fidler", "Sanja", ""]]}, {"id": "2009.00675", "submitter": "Seyedehnafiseh Mirniaharikandehei", "authors": "Seyedehnafiseh Mirniaharikandehei (1), Morteza Heidari (1), Gopichandh\n  Danala (1), Sivaramakrishnan Lakshmivarahan (2), Bin Zheng (1) ((1) School of\n  Electrical and Computer Engineering, University of Oklahoma, Norman, OK, USA,\n  (2) School of Computer Sciences, University of Oklahoma, Norman, OK, USA)", "title": "Applying a random projection algorithm to optimize machine learning\n  model for predicting peritoneal metastasis in gastric cancer patients using\n  CT images", "comments": "24 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and Objective: Non-invasively predicting the risk of cancer\nmetastasis before surgery plays an essential role in determining optimal\ntreatment methods for cancer patients (including who can benefit from\nneoadjuvant chemotherapy). Although developing radiomics based machine learning\n(ML) models has attracted broad research interest for this purpose, it often\nfaces a challenge of how to build a highly performed and robust ML model using\nsmall and imbalanced image datasets. Methods: In this study, we explore a new\napproach to build an optimal ML model. A retrospective dataset involving\nabdominal computed tomography (CT) images acquired from 159 patients diagnosed\nwith gastric cancer is assembled. Among them, 121 cases have peritoneal\nmetastasis (PM), while 38 cases do not have PM. A computer-aided detection\n(CAD) scheme is first applied to segment primary gastric tumor volumes and\ninitially computes 315 image features. Then, two Gradient Boosting Machine\n(GBM) models embedded with two different feature dimensionality reduction\nmethods, namely, the principal component analysis (PCA) and a random projection\nalgorithm (RPA) and a synthetic minority oversampling technique, are built to\npredict the risk of the patients having PM. All GBM models are trained and\ntested using a leave-one-case-out cross-validation method. Results: Results\nshow that the GBM embedded with RPA yielded a significantly higher prediction\naccuracy (71.2%) than using PCA (65.2%) (p<0.05). Conclusions: The study\ndemonstrated that CT images of the primary gastric tumors contain\ndiscriminatory information to predict the risk of PM, and RPA is a promising\nmethod to generate optimal feature vector, improving the performance of ML\nmodels of medical images.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 19:53:09 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Mirniaharikandehei", "Seyedehnafiseh", ""], ["Heidari", "Morteza", ""], ["Danala", "Gopichandh", ""], ["Lakshmivarahan", "Sivaramakrishnan", ""], ["Zheng", "Bin", ""]]}, {"id": "2009.00678", "submitter": "Brian Davis", "authors": "Brian Davis, Chris Tensmeyer, Brian Price, Curtis Wigington, Bryan\n  Morse, Rajiv Jain", "title": "Text and Style Conditioned GAN for Generation of Offline Handwriting\n  Lines", "comments": "Includes Supplementary Material. Accepted at BMVC 2020. 32 pages, 30\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a GAN for generating images of handwritten lines\nconditioned on arbitrary text and latent style vectors. Unlike prior work,\nwhich produce stroke points or single-word images, this model generates entire\nlines of offline handwriting. The model produces variable-sized images by using\nstyle vectors to determine character widths. A generator network is trained\nwith GAN and autoencoder techniques to learn style, and uses a pre-trained\nhandwriting recognition network to induce legibility. A study using human\nevaluators demonstrates that the model produces images that appear to be\nwritten by a human. After training, the encoder network can extract a style\nvector from an image, allowing images in a similar style to be generated, but\nwith arbitrary text.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 20:19:42 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Davis", "Brian", ""], ["Tensmeyer", "Chris", ""], ["Price", "Brian", ""], ["Wigington", "Curtis", ""], ["Morse", "Bryan", ""], ["Jain", "Rajiv", ""]]}, {"id": "2009.00681", "submitter": "Yutong Ban", "authors": "Yutong Ban, Guy Rosman, Thomas Ward, Daniel Hashimoto, Taisei Kondo,\n  Hidekazu Iwaki, Ozanan Meireles, Daniela Rus", "title": "Aggregating Long-Term Context for Learning Laparoscopic and\n  Robot-Assisted Surgical Workflows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing surgical workflow is crucial for surgical assistance robots to\nunderstand surgeries. With the understanding of the complete surgical workflow,\nthe robots are able to assist the surgeons in intra-operative events, such as\nby giving a warning when the surgeon is entering specific keys or high-risk\nphases. Deep learning techniques have recently been widely applied to\nrecognizing surgical workflows. Many of the existing temporal neural network\nmodels are limited in their capability to handle long-term dependencies in the\ndata, instead, relying upon the strong performance of the underlying per-frame\nvisual models. We propose a new temporal network structure that leverages\ntask-specific network representation to collect long-term sufficient statistics\nthat are propagated by a sufficient statistics model (SSM). We implement our\napproach within an LSTM backbone for the task of surgical phase recognition and\nexplore several choices for propagated statistics. We demonstrate superior\nresults over existing and novel state-of-the-art segmentation techniques on two\nlaparoscopic cholecystectomy datasets: the publicly available Cholec80 dataset\nand MGH100, a novel dataset with more challenging and clinically meaningful\nsegment labels.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 20:29:14 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 16:05:26 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 18:58:43 GMT"}, {"version": "v4", "created": "Mon, 10 May 2021 20:02:18 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Ban", "Yutong", ""], ["Rosman", "Guy", ""], ["Ward", "Thomas", ""], ["Hashimoto", "Daniel", ""], ["Kondo", "Taisei", ""], ["Iwaki", "Hidekazu", ""], ["Meireles", "Ozanan", ""], ["Rus", "Daniela", ""]]}, {"id": "2009.00702", "submitter": "SeyedHamed RahmaniKhezri", "authors": "Suhong Kim, Hamed RahmaniKhezri, Seyed Mohammad Nourbakhsh and Mohamed\n  Hefeeda", "title": "Unsupervised Single-Image Reflection Separation Using Perceptual Deep\n  Image Priors", "comments": "11 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reflections often degrade the quality of the image by obstructing the\nbackground scene. This is not desirable for everyday users, and it negatively\nimpacts the performance of multimedia applications that process images with\nreflections. Most current methods for removing reflections utilize\nsupervised-learning models. However, these models require an extensive number\nof image pairs to perform well, especially on natural images with reflection,\nwhich is difficult to achieve in practice. In this paper, we propose a novel\nunsupervised framework for single-image reflection separation. Instead of\nlearning from a large dataset, we optimize the parameters of two cross-coupled\ndeep convolutional networks on a target image to generate two exclusive\nbackground and reflection layers. In particular, we design a new architecture\nof the network to embed semantic features extracted from a pre-trained deep\nclassification network, which gives more meaningful separation similar to human\nperception. Quantitative and qualitative results on commonly used datasets in\nthe literature show that our method's performance is at least on par with the\nstate-of-the-art supervised methods and, occasionally, better without requiring\nlarge training datasets. Our results also show that our method significantly\noutperforms the closest unsupervised method in the literature for removing\nreflections from single images.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 21:08:30 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Kim", "Suhong", ""], ["RahmaniKhezri", "Hamed", ""], ["Nourbakhsh", "Seyed Mohammad", ""], ["Hefeeda", "Mohamed", ""]]}, {"id": "2009.00726", "submitter": "Xuefeng Hu", "authors": "Xuefeng Hu, Zhihan Zhang, Zhenye Jiang, Syomantak Chaudhuri, Zhenheng\n  Yang, Ram Nevatia", "title": "SPAN: Spatial Pyramid Attention Network forImage Manipulation\n  Localization", "comments": "Accepted at ECCV 2020\n  (https://link.springer.com/chapter/10.1007%2F978-3-030-58589-1_19) Code\n  Available at https://github.com/ZhiHanZ/IRIS0-SPAN/", "journal-ref": null, "doi": "10.1007/978-3-030-58589-1_19 10.1007/978-3-030-58589-1_19\n  10.1007/978-3-030-58589-1_19 10.1007/978-3-030-58589-1_19", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework, Spatial Pyramid Attention Network (SPAN) for\ndetection and localization of multiple types of image manipulations. The\nproposed architecture efficiently and effectively models the relationship\nbetween image patches at multiple scales by constructing a pyramid of local\nself-attention blocks. The design includes a novel position projection to\nencode the spatial positions of the patches. SPAN is trained on a generic,\nsynthetic dataset but can also be fine tuned for specific datasets; The\nproposed method shows significant gains in performance on standard datasets\nover previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 21:59:35 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 01:43:21 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Hu", "Xuefeng", ""], ["Zhang", "Zhihan", ""], ["Jiang", "Zhenye", ""], ["Chaudhuri", "Syomantak", ""], ["Yang", "Zhenheng", ""], ["Nevatia", "Ram", ""]]}, {"id": "2009.00735", "submitter": "Markus K\\\"uhbach", "authors": "Markus K\\\"uhbach and Matthew Kasemer and Baptiste Gault and Andrew\n  Breen", "title": "On Open and Strong-Scaling Tools for Atom Probe Crystallography:\n  High-Throughput Methods for Indexing Crystal Structure and Orientation", "comments": "36 pages, 19 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cond-mat.mtrl-sci cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric crystal structure indexing and orientation mapping are key data\nprocessing steps for virtually any quantitative study of spatial correlations\nbetween the local chemistry and the microstructure of a material. For electron\nand X-ray diffraction methods it is possible to develop indexing tools which\ncompare measured and analytically computed patterns to decode the structure and\nrelative orientation within local regions of interest. Consequently, a number\nof numerically efficient and automated software tools exist to solve the above\ncharacterisation tasks.\n  For atom probe tomography (APT) experiments, however, the strategy of making\ncomparisons between measured and analytically computed patterns is less robust\nbecause many APT datasets may contain substantial noise. Given that general\nenough predictive models for such noise remain elusive, crystallography tools\nfor APT face several limitations: Their robustness to noise, and therefore,\ntheir capability to identify and distinguish different crystal structures and\norientation is limited. In addition, the tools are sequential and demand\nsubstantial manual interaction. In combination, this makes robust uncertainty\nquantifying with automated high-throughput studies of the latent\ncrystallographic information a difficult task with APT data.\n  To improve the situation, we review the existent methods and discuss how they\nlink to those in the diffraction communities. With this we modify some of the\nAPT methods to yield more robust descriptors of the atomic arrangement. We\nreport how this enables the development of an open-source software tool for\nstrong-scaling and automated identifying of crystal structure and mapping\ncrystal orientation in nanocrystalline APT datasets with multiple phases.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 22:50:03 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["K\u00fchbach", "Markus", ""], ["Kasemer", "Matthew", ""], ["Gault", "Baptiste", ""], ["Breen", "Andrew", ""]]}, {"id": "2009.00743", "submitter": "Mannat Kaur", "authors": "Shubhra Aich, Jean Marie Uwabeza Vianney, Md Amirul Islam, Mannat\n  Kaur, and Bingbing Liu", "title": "Bidirectional Attention Network for Monocular Depth Estimation", "comments": "Camera-ready for IEEE International Conference on Robotics and\n  Automation (ICRA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Bidirectional Attention Network (BANet), an\nend-to-end framework for monocular depth estimation (MDE) that addresses the\nlimitation of effectively integrating local and global information in\nconvolutional neural networks. The structure of this mechanism derives from a\nstrong conceptual foundation of neural machine translation, and presents a\nlight-weight mechanism for adaptive control of computation similar to the\ndynamic nature of recurrent neural networks. We introduce bidirectional\nattention modules that utilize the feed-forward feature maps and incorporate\nthe global context to filter out ambiguity. Extensive experiments reveal the\nhigh degree of capability of this bidirectional attention model over\nfeed-forward baselines and other state-of-the-art methods for monocular depth\nestimation on two challenging datasets -- KITTI and DIODE. We show that our\nproposed approach either outperforms or performs at least on a par with the\nstate-of-the-art monocular depth estimation methods with less memory and\ncomputational complexity.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 23:14:05 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 18:43:01 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Aich", "Shubhra", ""], ["Vianney", "Jean Marie Uwabeza", ""], ["Islam", "Md Amirul", ""], ["Kaur", "Mannat", ""], ["Liu", "Bingbing", ""]]}, {"id": "2009.00749", "submitter": "Priyanka Das", "authors": "Priyanka Das, Joseph McGrath, Zhaoyuan Fang, Aidan Boyd, Ganghee Jang,\n  Amir Mohammadi, Sandip Purnapatra, David Yambay, S\\'ebastien Marcel, Mateusz\n  Trokielewicz, Piotr Maciejewicz, Kevin Bowyer, Adam Czajka, Stephanie\n  Schuckers, Juan Tapia, Sebastian Gonzalez, Meiling Fang, Naser Damer, Fadi\n  Boutros, Arjan Kuijper, Renu Sharma, Cunjian Chen, Arun Ross", "title": "Iris Liveness Detection Competition (LivDet-Iris) -- The 2020 Edition", "comments": "9 pages, 3 figures, 3 tables, Accepted for presentation at\n  International Joint Conference on Biometrics (IJCB 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Launched in 2013, LivDet-Iris is an international competition series open to\nacademia and industry with the aim to assess and report advances in iris\nPresentation Attack Detection (PAD). This paper presents results from the\nfourth competition of the series: LivDet-Iris 2020. This year's competition\nintroduced several novel elements: (a) incorporated new types of attacks\n(samples displayed on a screen, cadaver eyes and prosthetic eyes), (b)\ninitiated LivDet-Iris as an on-going effort, with a testing protocol available\nnow to everyone via the Biometrics Evaluation and Testing\n(BEAT)(https://www.idiap.ch/software/beat/) open-source platform to facilitate\nreproducibility and benchmarking of new algorithms continuously, and (c)\nperformance comparison of the submitted entries with three baseline methods\n(offered by the University of Notre Dame and Michigan State University), and\nthree open-source iris PAD methods available in the public domain. The best\nperforming entry to the competition reported a weighted average APCER of\n59.10\\% and a BPCER of 0.46\\% over all five attack types. This paper serves as\nthe latest evaluation of iris PAD on a large spectrum of presentation attack\ninstruments.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 23:43:19 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Das", "Priyanka", ""], ["McGrath", "Joseph", ""], ["Fang", "Zhaoyuan", ""], ["Boyd", "Aidan", ""], ["Jang", "Ganghee", ""], ["Mohammadi", "Amir", ""], ["Purnapatra", "Sandip", ""], ["Yambay", "David", ""], ["Marcel", "S\u00e9bastien", ""], ["Trokielewicz", "Mateusz", ""], ["Maciejewicz", "Piotr", ""], ["Bowyer", "Kevin", ""], ["Czajka", "Adam", ""], ["Schuckers", "Stephanie", ""], ["Tapia", "Juan", ""], ["Gonzalez", "Sebastian", ""], ["Fang", "Meiling", ""], ["Damer", "Naser", ""], ["Boutros", "Fadi", ""], ["Kuijper", "Arjan", ""], ["Sharma", "Renu", ""], ["Chen", "Cunjian", ""], ["Ross", "Arun", ""]]}, {"id": "2009.00764", "submitter": "Peixuan Li", "authors": "Peixuan Li", "title": "Monocular 3D Detection with Geometric Constraints Embedding and\n  Semi-supervised Training", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel single-shot and keypoints-based framework\nfor monocular 3D objects detection using only RGB images, called KM3D-Net. We\ndesign a fully convolutional model to predict object keypoints, dimension, and\norientation, and then combine these estimations with perspective geometry\nconstraints to compute position attribute. Further, we reformulate the\ngeometric constraints as a differentiable version and embed it into the network\nto reduce running time while maintaining the consistency of model outputs in an\nend-to-end fashion. Benefiting from this simple structure, we then propose an\neffective semi-supervised training strategy for the setting where labeled\ntraining data is scarce. In this strategy, we enforce a consensus prediction of\ntwo shared-weights KM3D-Net for the same unlabeled image under different input\naugmentation conditions and network regularization. In particular, we unify the\ncoordinate-dependent augmentations as the affine transformation for the\ndifferential recovering position of objects and propose a keypoints-dropout\nmodule for the network regularization. Our model only requires RGB images\nwithout synthetic data, instance segmentation, CAD model, or depth generator.\nNevertheless, extensive experiments on the popular KITTI 3D detection dataset\nindicate that the KM3D-Net surpasses all previous state-of-the-art methods in\nboth efficiency and accuracy by a large margin. And also, to the best of our\nknowledge, this is the first time that semi-supervised learning is applied in\nmonocular 3D objects detection. We even surpass most of the previous fully\nsupervised methods with only 13\\% labeled data on KITTI.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 00:51:51 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Li", "Peixuan", ""]]}, {"id": "2009.00771", "submitter": "Xuerui Zhang", "authors": "Zhang Xuerui, Yuan Xia", "title": "LSMVOS: Long-Short-Term Similarity Matching for Video Object", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective Semi-supervised video object segmentation refers to segmenting the\nobject in subsequent frames given the object label in the first frame. Existing\nalgorithms are mostly based on the objectives of matching and propagation\nstrategies, which often make use of the previous frame with masking or optical\nflow. This paper explores a new propagation method, uses short-term matching\nmodules to extract the information of the previous frame and apply it in\npropagation, and proposes the network of Long-Short-Term similarity matching\nfor video object segmentation (LSMOVS) Method: By conducting pixel-level\nmatching and correlation between long-term matching module and short-term\nmatching module with the first frame and previous frame, global similarity map\nand local similarity map are obtained, as well as feature pattern of current\nframe and masking of previous frame. After two refine networks, final results\nare obtained through segmentation network. Results: According to the\nexperiments on the two data sets DAVIS 2016 and 2017, the method of this paper\nachieves favorable average of region similarity and contour accuracy without\nonline fine tuning, which achieves 86.5% and 77.4% in terms of single target\nand multiple targets. Besides, the count of segmented frames per second reached\n21. Conclusion: The short-term matching module proposed in this paper is more\nconducive to extracting the information of the previous frame than only the\nmask. By combining the long-term matching module with the short-term matching\nmodule, the whole network can achieve efficient video object segmentation\nwithout online fine tuning\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 01:32:05 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Xuerui", "Zhang", ""], ["Xia", "Yuan", ""]]}, {"id": "2009.00782", "submitter": "Alan Sun", "authors": "Alan Sun", "title": "A perception centred self-driving system without HD Maps", "comments": null, "journal-ref": null, "doi": "10.14569/IJACSA.2020.0111081", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building a fully autonomous self-driving system has been discussed for more\nthan 20 years yet remains unsolved. Previous systems have limited ability to\nscale. Their localization subsystem needs labor-intensive map recording for\nrunning in a new area, and the accuracy decreases after the changes occur in\nthe environment. In this paper, a new localization method is proposed to solve\nthe scalability problems, with a new method for detecting and making sense of\ndiverse traffic lines. Like the way human drives, a self-driving system should\nnot rely on an exact position to travel in most scenarios. As a result, without\nHD Maps, GPS or IMU, the proposed localization subsystem relies only on\ndetecting driving-related features around (like lane lines, stop lines, and\nmerging lane lines). For spotting and reasoning all these features, a new line\ndetector is proposed and tested against multiple datasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 02:06:29 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 02:13:27 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Sun", "Alan", ""]]}, {"id": "2009.00784", "submitter": "Su Pang", "authors": "Su Pang, Daniel Morris, Hayder Radha", "title": "CLOCs: Camera-LiDAR Object Candidates Fusion for 3D Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been significant advances in neural networks for both 3D object\ndetection using LiDAR and 2D object detection using video. However, it has been\nsurprisingly difficult to train networks to effectively use both modalities in\na way that demonstrates gain over single-modality networks. In this paper, we\npropose a novel Camera-LiDAR Object Candidates (CLOCs) fusion network. CLOCs\nfusion provides a low-complexity multi-modal fusion framework that\nsignificantly improves the performance of single-modality detectors. CLOCs\noperates on the combined output candidates before Non-Maximum Suppression (NMS)\nof any 2D and any 3D detector, and is trained to leverage their geometric and\nsemantic consistencies to produce more accurate final 3D and 2D detection\nresults. Our experimental evaluation on the challenging KITTI object detection\nbenchmark, including 3D and bird's eye view metrics, shows significant\nimprovements, especially at long distance, over the state-of-the-art fusion\nbased methods. At time of submission, CLOCs ranks the highest among all the\nfusion-based methods in the official KITTI leaderboard. We will release our\ncode upon acceptance.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 02:07:00 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Pang", "Su", ""], ["Morris", "Daniel", ""], ["Radha", "Hayder", ""]]}, {"id": "2009.00802", "submitter": "Andrew Lohn", "authors": "Andrew J. Lohn", "title": "Estimating the Brittleness of AI: Safety Integrity Levels and the Need\n  for Testing Out-Of-Distribution Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.CY cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Test, Evaluation, Verification, and Validation (TEVV) for Artificial\nIntelligence (AI) is a challenge that threatens to limit the economic and\nsocietal rewards that AI researchers have devoted themselves to producing. A\ncentral task of TEVV for AI is estimating brittleness, where brittleness\nimplies that the system functions well within some bounds and poorly outside of\nthose bounds. This paper argues that neither of those criteria are certain of\nDeep Neural Networks. First, highly touted AI successes (eg. image\nclassification and speech recognition) are orders of magnitude more\nfailure-prone than are typically certified in critical systems even within\ndesign bounds (perfectly in-distribution sampling). Second, performance falls\noff only gradually as inputs become further Out-Of-Distribution (OOD). Enhanced\nemphasis is needed on designing systems that are resilient despite\nfailure-prone AI components as well as on evaluating and improving OOD\nperformance in order to get AI to where it can clear the challenging hurdles of\nTEVV and certification.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 03:33:40 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Lohn", "Andrew J.", ""]]}, {"id": "2009.00814", "submitter": "Rui Shao", "authors": "Rui Shao and Pramuditha Perera and Pong C. Yuen and Vishal M. Patel", "title": "Open-set Adversarial Defense", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-set recognition and adversarial defense study two key aspects of deep\nlearning that are vital for real-world deployment. The objective of open-set\nrecognition is to identify samples from open-set classes during testing, while\nadversarial defense aims to defend the network against images with\nimperceptible adversarial perturbations. In this paper, we show that open-set\nrecognition systems are vulnerable to adversarial attacks. Furthermore, we show\nthat adversarial defense mechanisms trained on known classes do not generalize\nwell to open-set samples. Motivated by this observation, we emphasize the need\nof an Open-Set Adversarial Defense (OSAD) mechanism. This paper proposes an\nOpen-Set Defense Network (OSDN) as a solution to the OSAD problem. The proposed\nnetwork uses an encoder with feature-denoising layers coupled with a classifier\nto learn a noise-free latent feature representation. Two techniques are\nemployed to obtain an informative latent feature space with the objective of\nimproving open-set performance. First, a decoder is used to ensure that clean\nimages can be reconstructed from the obtained latent features. Then,\nself-supervision is used to ensure that the latent features are informative\nenough to carry out an auxiliary task. We introduce a testing protocol to\nevaluate OSAD performance and show the effectiveness of the proposed method in\nmultiple object classification datasets. The implementation code of the\nproposed method is available at: https://github.com/rshaojimmy/ECCV2020-OSAD.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 04:35:33 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Shao", "Rui", ""], ["Perera", "Pramuditha", ""], ["Yuen", "Pong C.", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2009.00817", "submitter": "Charles Lehman", "authors": "Charles Lehman, Dogancan Temel, and Ghassan AlRegib", "title": "On the Structures of Representation for the Robustness of Semantic\n  Segmentation to Input Corruption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is a scene understanding task at the heart of\nsafety-critical applications where robustness to corrupted inputs is essential.\nImplicit Background Estimation (IBE) has demonstrated to be a promising\ntechnique to improve the robustness to out-of-distribution inputs for semantic\nsegmentation models for little to no cost. In this paper, we provide analysis\ncomparing the structures learned as a result of optimization objectives that\nuse Softmax, IBE, and Sigmoid in order to improve understanding their\nrelationship to robustness. As a result of this analysis, we propose combining\nSigmoid with IBE (SCrIBE) to improve robustness. Finally, we demonstrate that\nSCrIBE exhibits superior segmentation performance aggregated across all\ncorruptions and severity levels with a mIOU of 42.1 compared to both IBE 40.3\nand the Softmax Baseline 37.5.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 04:49:27 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Lehman", "Charles", ""], ["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "2009.00831", "submitter": "Ruiki Kobayashi", "authors": "Ruiki Kobayashi, Shogo Muramatsu", "title": "Convolutional Nonlinear Dictionary with Cascaded Structure Filter Banks", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes a convolutional nonlinear dictionary (CNLD) for image\nrestoration using cascaded filter banks. Generally, convolutional neural\nnetworks (CNN) demonstrate their practicality in image restoration\napplications; however, existing CNNs are constructed without considering the\nrelationship among atomic images (convolution kernels). As a result, there\nremains room for discussing the role of design spaces. To provide a framework\nfor constructing an effective and structured convolutional network, this study\nproposes the CNLD. The backpropagation learning procedure is derived from\ncertain image restoration experiments, and thereby the significance of CNLD is\nverified. It is demonstrated that the number of parameters is reduced while\npreserving the restoration performance.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 05:40:01 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Kobayashi", "Ruiki", ""], ["Muramatsu", "Shogo", ""]]}, {"id": "2009.00833", "submitter": "Jia Li", "authors": "Kui Fu, Jia Li, Lin Ma, Kai Mu, Yonghong Tian", "title": "Intrinsic Relationship Reasoning for Small Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The small objects in images and videos are usually not independent\nindividuals. Instead, they more or less present some semantic and spatial\nlayout relationships with each other. Modeling and inferring such intrinsic\nrelationships can thereby be beneficial for small object detection. In this\npaper, we propose a novel context reasoning approach for small object detection\nwhich models and infers the intrinsic semantic and spatial layout relationships\nbetween objects. Specifically, we first construct a semantic module to model\nthe sparse semantic relationships based on the initial regional features, and a\nspatial layout module to model the sparse spatial layout relationships based on\ntheir position and shape information, respectively. Both of them are then fed\ninto a context reasoning module for integrating the contextual information with\nrespect to the objects and their relationships, which is further fused with the\noriginal regional visual features for classification and regression.\nExperimental results reveal that the proposed approach can effectively boost\nthe small object detection performance.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 06:03:05 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Fu", "Kui", ""], ["Li", "Jia", ""], ["Ma", "Lin", ""], ["Mu", "Kai", ""], ["Tian", "Yonghong", ""]]}, {"id": "2009.00842", "submitter": "Pranjay Shyam", "authors": "Pranjay Shyam, Antyanta Bangunharcana and Kyung-Soo Kim", "title": "Retaining Image Feature Matching Performance Under Low Light Conditions", "comments": "Accepted in ICCAS 2020 - 20th International Conference on Control,\n  Robotics, and Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Poor image quality in low light images may result in a reduced number of\nfeature matching between images. In this paper, we investigate the performance\nof feature extraction algorithms in low light environments. To find an optimal\nsetting to retain feature matching performance in low light images, we look\ninto the effect of changing feature acceptance threshold for feature detector\nand adding pre-processing in the form of Low Light Image Enhancement (LLIE)\nprior to feature detection. We observe that even in low light images, feature\nmatching using traditional hand-crafted feature detectors still performs\nreasonably well by lowering the threshold parameter. We also show that applying\nLow Light Image Enhancement (LLIE) algorithms can improve feature matching even\nmore when paired with the right feature extraction algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 06:44:45 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Shyam", "Pranjay", ""], ["Bangunharcana", "Antyanta", ""], ["Kim", "Kyung-Soo", ""]]}, {"id": "2009.00855", "submitter": "Andres Ussa Caycedo", "authors": "Bharath Ramesh, Shihao Zhang, Hong Yang, Andres Ussa, Matthew Ong,\n  Garrick Orchard and Cheng Xiang", "title": "e-TLD: Event-based Framework for Dynamic Object Tracking", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a long-term object tracking framework with a moving event\ncamera under general tracking conditions. A first of its kind for these\nrevolutionary cameras, the tracking framework uses a discriminative\nrepresentation for the object with online learning, and detects and re-tracks\nthe object when it comes back into the field-of-view. One of the key novelties\nis the use of an event-based local sliding window technique that tracks\nreliably in scenes with cluttered and textured background. In addition,\nBayesian bootstrapping is used to assist real-time processing and boost the\ndiscriminative power of the object representation. On the other hand, when the\nobject re-enters the field-of-view of the camera, a data-driven, global sliding\nwindow detector locates the object for subsequent tracking. Extensive\nexperiments demonstrate the ability of the proposed framework to track and\ndetect arbitrary objects of various shapes and sizes, including dynamic objects\nsuch as a human. This is a significant improvement compared to earlier works\nthat simply track objects as long as they are visible under simpler background\nsettings. Using the ground truth locations for five different objects under\nthree motion settings, namely translation, rotation and 6-DOF, quantitative\nmeasurement is reported for the event-based tracking framework with critical\ninsights on various performance issues. Finally, real-time implementation in\nC++ highlights tracking ability under scale, rotation, view-point and occlusion\nscenarios in a lab setting.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 07:08:56 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Ramesh", "Bharath", ""], ["Zhang", "Shihao", ""], ["Yang", "Hong", ""], ["Ussa", "Andres", ""], ["Ong", "Matthew", ""], ["Orchard", "Garrick", ""], ["Xiang", "Cheng", ""]]}, {"id": "2009.00857", "submitter": "Cao Haichao", "authors": "Haichao Cao", "title": "Breast mass detection in digital mammography based on anchor-free\n  architecture", "comments": "26 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and Objective: Accurate detection of breast masses in mammography\nimages is critical to diagnose early breast cancer, which can greatly improve\nthe patients survival rate. However, it is still a big challenge due to the\nheterogeneity of breast masses and the complexity of their surrounding\nenvironment.Methods: To address these problems, we propose a one-stage object\ndetection architecture, called Breast Mass Detection Network (BMassDNet), based\non anchor-free and feature pyramid which makes the detection of breast masses\nof different sizes well adapted. We introduce a truncation normalization method\nand combine it with adaptive histogram equalization to enhance the contrast\nbetween the breast mass and the surrounding environment. Meanwhile, to solve\nthe overfitting problem caused by small data size, we propose a natural\ndeformation data augmentation method and mend the train data dynamic updating\nmethod based on the data complexity to effectively utilize the limited data.\nFinally, we use transfer learning to assist the training process and to improve\nthe robustness of the model ulteriorly.Results: On the INbreast dataset, each\nimage has an average of 0.495 false positives whilst the recall rate is 0.930;\nOn the DDSM dataset, when each image has 0.599 false positives, the recall rate\nreaches 0.943.Conclusions: The experimental results on datasets INbreast and\nDDSM show that the proposed BMassDNet can obtain competitive detection\nperformance over the current top ranked methods.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 07:11:16 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Cao", "Haichao", ""]]}, {"id": "2009.00859", "submitter": "Ishani Mondal", "authors": "Ishani Mondal and Debasis Ganguly", "title": "ALEX: Active Learning based Enhancement of a Model's Explainability", "comments": "CIKM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An active learning (AL) algorithm seeks to construct an effective classifier\nwith a minimal number of labeled examples in a bootstrapping manner. While\nstandard AL heuristics, such as selecting those points for annotation for which\na classification model yields least confident predictions, there has been no\nempirical investigation to see if these heuristics lead to models that are more\ninterpretable to humans. In the era of data-driven learning, this is an\nimportant research direction to pursue. This paper describes our\nwork-in-progress towards developing an AL selection function that in addition\nto model effectiveness also seeks to improve on the interpretability of a model\nduring the bootstrapping steps. Concretely speaking, our proposed selection\nfunction trains an `explainer' model in addition to the classifier model, and\nfavours those instances where a different part of the data is used, on an\naverage, to explain the predicted class. Initial experiments exhibited\nencouraging trends in showing that such a heuristic can lead to developing more\neffective and more explainable end-to-end data-driven classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 07:15:39 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Mondal", "Ishani", ""], ["Ganguly", "Debasis", ""]]}, {"id": "2009.00872", "submitter": "Moritz Knolle", "authors": "Moritz Knolle (1 and 2), Georgios Kaissis (1 and 2 and 3 and 4),\n  Friederike Jungmann (1), Sebastian Ziegelmayer (1), Daniel Sasse (1), Marcus\n  Makowski (1), Daniel Rueckert (2 and 4), Rickmer Braren (1) ((1) Department\n  of diagnostic and interventional Radiology, Technical University of Munich,\n  Munich, Germany, (2) Institute for Artificial Intelligence and Data Science\n  in Medicine and Healthcare, Technical University of Munich, Munich, Germany,\n  (3) OpenMined Research, (4) Department of Computing, Imperial College London,\n  London, United Kingdom)", "title": "Efficient, high-performance pancreatic segmentation using multi-scale\n  feature extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For artificial intelligence-based image analysis methods to reach clinical\napplicability, the development of high-performance algorithms is crucial. For\nexample, existent segmentation algorithms based on natural images are neither\nefficient in their parameter use nor optimized for medical imaging. Here we\npresent MoNet, a highly optimized neural-network-based pancreatic segmentation\nalgorithm focused on achieving high performance by efficient multi-scale image\nfeature utilization.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 07:47:39 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 14:24:18 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Knolle", "Moritz", "", "1 and 2"], ["Kaissis", "Georgios", "", "1 and 2 and 3 and 4"], ["Jungmann", "Friederike", "", "2 and 4"], ["Ziegelmayer", "Sebastian", "", "2 and 4"], ["Sasse", "Daniel", "", "2 and 4"], ["Makowski", "Marcus", "", "2 and 4"], ["Rueckert", "Daniel", "", "2 and 4"], ["Braren", "Rickmer", ""]]}, {"id": "2009.00878", "submitter": "Ibrahim Batuhan Akkaya", "authors": "Ibrahim Batuhan Akkaya and Ugur Halici", "title": "GAIT: Gradient Adjusted Unsupervised Image-to-Image Translation", "comments": "Accepted by ICIP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation (IIT) has made much progress recently with the\ndevelopment of adversarial learning. In most of the recent work, an adversarial\nloss is utilized to match the distributions of the translated and target image\nsets. However, this may create artifacts if two domains have different marginal\ndistributions, for example, in uniform areas. In this work, we propose an\nunsupervised IIT method that preserves the uniform regions after the\ntranslation. The gradient adjustment loss, which is the L2 norm between the\nSobel response of the target image and the adjusted Sobel response of the\nsource images, is utilized. The proposed method is validated on the\njellyfish-to-Haeckel dataset, which is prepared to demonstrate the mentioned\nproblem, which contains images with different background distributions. We\ndemonstrate that our method obtained a performance gain compared to the\nbaseline method qualitatively and quantitatively, showing the effectiveness of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 08:04:00 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Akkaya", "Ibrahim Batuhan", ""], ["Halici", "Ugur", ""]]}, {"id": "2009.00893", "submitter": "Chen Shen", "authors": "Shaotian Yan, Chen Shen, Zhongming Jin, Jianqiang Huang, Rongxin\n  Jiang, Yaowu Chen, Xian-Sheng Hua", "title": "PCPL: Predicate-Correlation Perception Learning for Unbiased Scene Graph\n  Generation", "comments": "To be appeared on ACMMM 2020", "journal-ref": null, "doi": "10.1145/3394171.3413722", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, scene graph generation(SGG) task is largely limited in realistic\nscenarios, mainly due to the extremely long-tailed bias of predicate annotation\ndistribution. Thus, tackling the class imbalance trouble of SGG is critical and\nchallenging. In this paper, we first discover that when predicate labels have\nstrong correlation with each other, prevalent re-balancing strategies(e.g.,\nre-sampling and re-weighting) will give rise to either over-fitting the tail\ndata(e.g., bench sitting on sidewalk rather than on), or still suffering the\nadverse effect from the original uneven distribution(e.g., aggregating varied\nparked on/standing on/sitting on into on). We argue the principal reason is\nthat re-balancing strategies are sensitive to the frequencies of predicates yet\nblind to their relatedness, which may play a more important role to promote the\nlearning of predicate features. Therefore, we propose a novel\nPredicate-Correlation Perception Learning(PCPL for short) scheme to adaptively\nseek out appropriate loss weights by directly perceiving and utilizing the\ncorrelation among predicate classes. Moreover, our PCPL framework is further\nequipped with a graph encoder module to better extract context features.\nExtensive experiments on the benchmark VG150 dataset show that the proposed\nPCPL performs markedly better on tail classes while well-preserving the\nperformance on head ones, which significantly outperforms previous\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 08:30:09 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Yan", "Shaotian", ""], ["Shen", "Chen", ""], ["Jin", "Zhongming", ""], ["Huang", "Jianqiang", ""], ["Jiang", "Rongxin", ""], ["Chen", "Yaowu", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "2009.00902", "submitter": "Minjing Dong", "authors": "Minjing Dong, Yanxi Li, Yunhe Wang and Chang Xu", "title": "Adversarially Robust Neural Architectures", "comments": "9 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network (DNN) are vulnerable to adversarial attack. Existing\nmethods are devoted to developing various robust training strategies or\nregularizations to update the weights of the neural network. But beyond the\nweights, the overall structure and information flow in the network are\nexplicitly determined by the neural architecture, which remains unexplored.\nThis paper thus aims to improve the adversarial robustness of the network from\nthe architecture perspective with NAS framework. We explore the relationship\namong adversarial robustness, Lipschitz constant, and architecture parameters\nand show that an appropriate constraint on architecture parameters could reduce\nthe Lipschitz constant to further improve the robustness. For NAS framework,\nall the architecture parameters are equally treated when the discrete\narchitecture is sampled from supernet. However, the importance of architecture\nparameters could vary from operation to operation or connection to connection,\nwhich is not explored and might reduce the confidence of robust architecture\nsampling. Thus, we propose to sample architecture parameters from trainable\nmultivariate log-normal distributions, with which the Lipschitz constant of\nentire network can be approximated using a univariate log-normal distribution\nwith mean and variance related to architecture parameters. Compared with\nadversarially trained neural architectures searched by various NAS algorithms\nas well as efficient human-designed models, our algorithm empirically achieves\nthe best performance among all the models under various attacks on different\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 08:52:15 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Dong", "Minjing", ""], ["Li", "Yanxi", ""], ["Wang", "Yunhe", ""], ["Xu", "Chang", ""]]}, {"id": "2009.00905", "submitter": "Sanghun Park", "authors": "Sanghun Park, Kwanggyoon Seo, Junyong Noh", "title": "Neural Crossbreed: Neural Based Image Metamorphosis", "comments": "16 pages", "journal-ref": "ACM Transactions on Graphics (Proceeding of SIGGRAPH Asia), 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose Neural Crossbreed, a feed-forward neural network that can learn a\nsemantic change of input images in a latent space to create the morphing\neffect. Because the network learns a semantic change, a sequence of meaningful\nintermediate images can be generated without requiring the user to specify\nexplicit correspondences. In addition, the semantic change learning makes it\npossible to perform the morphing between the images that contain objects with\nsignificantly different poses or camera views. Furthermore, just as in\nconventional morphing techniques, our morphing network can handle shape and\nappearance transitions separately by disentangling the content and the style\ntransfer for rich usability. We prepare a training dataset for morphing using a\npre-trained BigGAN, which generates an intermediate image by interpolating two\nlatent vectors at an intended morphing value. This is the first attempt to\naddress image morphing using a pre-trained generative model in order to learn\nsemantic transformation. The experiments show that Neural Crossbreed produces\nhigh quality morphed images, overcoming various limitations associated with\nconventional approaches. In addition, Neural Crossbreed can be further extended\nfor diverse applications such as multi-image morphing, appearance transfer, and\nvideo frame interpolation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 08:56:47 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Park", "Sanghun", ""], ["Seo", "Kwanggyoon", ""], ["Noh", "Junyong", ""]]}, {"id": "2009.00908", "submitter": "Lufan Chang", "authors": "Lufan Chang, Wenjing Zhuang, Richeng Wu, Sai Feng, Hao Liu, Jing Yu,\n  Jia Ding, Ziteng Wang, Jiaqi Zhang", "title": "DARWIN: A Highly Flexible Platform for Imaging Research in Radiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To conduct a radiomics or deep learning research experiment, the radiologists\nor physicians need to grasp the needed programming skills, which, however,\ncould be frustrating and costly when they have limited coding experience. In\nthis paper, we present DARWIN, a flexible research platform with a graphical\nuser interface for medical imaging research. Our platform is consists of a\nradiomics module and a deep learning module. The radiomics module can extract\nmore than 1000 dimension features(first-, second-, and higher-order) and\nprovided many draggable supervised and unsupervised machine learning models.\nOur deep learning module integrates state of the art architectures of\nclassification, detection, and segmentation tasks. It allows users to manually\nselect hyperparameters, or choose an algorithm to automatically search for the\nbest ones. DARWIN also offers the possibility for users to define a custom\npipeline for their experiment. These flexibilities enable radiologists to carry\nout various experiments easily.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 09:19:40 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Chang", "Lufan", ""], ["Zhuang", "Wenjing", ""], ["Wu", "Richeng", ""], ["Feng", "Sai", ""], ["Liu", "Hao", ""], ["Yu", "Jing", ""], ["Ding", "Jia", ""], ["Wang", "Ziteng", ""], ["Zhang", "Jiaqi", ""]]}, {"id": "2009.00909", "submitter": "Wen Zhang", "authors": "Wen Zhang, Lingfei Deng, Lei Zhang, Dongrui Wu", "title": "A Survey on Negative Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning (TL) tries to utilize data or knowledge from one or more\nsource domains to facilitate the learning in a target domain. It is\nparticularly useful when the target domain has few or no labeled data, due to\nannotation expense, privacy concerns, etc. Unfortunately, the effectiveness of\nTL is not always guaranteed. Negative transfer (NT), i.e., the source domain\ndata/knowledge cause reduced learning performance in the target domain, has\nbeen a long-standing and challenging problem in TL. Various approaches to\nhandle NT have been proposed in the literature. However, this filed lacks a\nsystematic survey on the formalization of NT, their factors and the algorithms\nthat handle NT. This paper proposes to fill this gap. First, the definition of\nnegative transfer is considered and a taxonomy of the factors are discussed.\nThen, near fifty representative approaches for handling NT are categorized and\nreviewed, from four perspectives: secure transfer, domain similarity\nestimation, distant transfer and negative transfer mitigation. NT in related\nfields, e.g., multi-task learning, lifelong learning, and adversarial attacks\nare also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 09:20:20 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 03:05:51 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 13:01:08 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Zhang", "Wen", ""], ["Deng", "Lingfei", ""], ["Zhang", "Lei", ""], ["Wu", "Dongrui", ""]]}, {"id": "2009.00919", "submitter": "Matthias De Lange", "authors": "Matthias De Lange, Tinne Tuytelaars", "title": "Continual Prototype Evolution: Learning Online from Non-Stationary Data\n  Streams", "comments": "10 pages, code publicly available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attaining prototypical features to represent class distributions is well\nestablished in representation learning. However, learning prototypes online\nfrom streaming data proves a challenging endeavor as they rapidly become\noutdated, caused by an ever-changing parameter space during the learning\nprocess. Additionally, continual learning does not assume the data stream to be\nstationary, typically resulting in catastrophic forgetting of previous\nknowledge. As a first, we introduce a system addressing both problems, where\nprototypes evolve continually in a shared latent space, enabling learning and\nprediction at any point in time. In contrast to the major body of work in\ncontinual learning, data streams are processed in an online fashion, without\nadditional task-information, and an efficient memory scheme provides robustness\nto imbalanced data streams. Besides nearest neighbor based prediction, learning\nis facilitated by a novel objective function, encouraging cluster density about\nthe class prototype and increased inter-class variance. Furthermore, the latent\nspace quality is elevated by pseudo-prototypes in each batch, constituted by\nreplay of exemplars from memory. As an additional contribution, we generalize\nthe existing paradigms in continual learning to incorporate data incremental\nlearning from data streams by formalizing a two-agent learner-evaluator\nframework. We obtain state-of-the-art performance by a significant margin on\neight benchmarks, including three highly imbalanced data streams.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 09:39:26 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 14:21:18 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2021 09:22:40 GMT"}, {"version": "v4", "created": "Tue, 6 Apr 2021 10:40:42 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["De Lange", "Matthias", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "2009.00922", "submitter": "Anna Hilsmann", "authors": "Anna Hilsmann, Philipp Fechteler, Wieland Morgenstern, Wolfgang Paier,\n  Ingo Feldmann, Oliver Schreer, Peter Eisert", "title": "Going beyond Free Viewpoint: Creating Animatable Volumetric Video of\n  Human Performances", "comments": null, "journal-ref": "ET Computer Vision, Special Issue on Computer Vision for the\n  Creative Industries (2020)", "doi": "10.1049/iet-cvi.2019.0786", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an end-to-end pipeline for the creation of\nhigh-quality animatable volumetric video content of human performances. Going\nbeyond the application of free-viewpoint volumetric video, we allow\nre-animation and alteration of an actor's performance through (i) the\nenrichment of the captured data with semantics and animation properties and\n(ii) applying hybrid geometry- and video-based animation methods that allow a\ndirect animation of the high-quality data itself instead of creating an\nanimatable model that resembles the captured data. Semantic enrichment and\ngeometric animation ability are achieved by establishing temporal consistency\nin the 3D data, followed by an automatic rigging of each frame using a\nparametric shape-adaptive full human body model. Our hybrid geometry- and\nvideo-based animation approaches combine the flexibility of classical CG\nanimation with the realism of real captured data. For pose editing, we exploit\nthe captured data as much as possible and kinematically deform the captured\nframes to fit a desired pose. Further, we treat the face differently from the\nbody in a hybrid geometry- and video-based animation approach where coarse\nmovements and poses are modeled in the geometry only, while very fine and\nsubtle details in the face, often lacking in purely geometric methods, are\ncaptured in video-based textures. These are processed to be interactively\ncombined to form new facial expressions. On top of that, we learn the\nappearance of regions that are challenging to synthesize, such as the teeth or\nthe eyes, and fill in missing regions realistically in an autoencoder-based\napproach. This paper covers the full pipeline from capturing and producing\nhigh-quality video content, over the enrichment with semantics and deformation\nproperties for re-animation and processing of the data for the final hybrid\nanimation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 09:46:12 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Hilsmann", "Anna", ""], ["Fechteler", "Philipp", ""], ["Morgenstern", "Wieland", ""], ["Paier", "Wolfgang", ""], ["Feldmann", "Ingo", ""], ["Schreer", "Oliver", ""], ["Eisert", "Peter", ""]]}, {"id": "2009.00926", "submitter": "Paul Smyth", "authors": "Thomas Beznik, Paul Smyth, Ga\\\"el de Lannoy and John A. Lee", "title": "Deep Learning to Detect Bacterial Colonies for the Production of\n  Vaccines", "comments": "6 pages, 2 figures, accepted at ESANN 2020 (European Symposium on\n  Artificial Neural Networks, Computational Intelligence and Machine Learning)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  During the development of vaccines, bacterial colony forming units (CFUs) are\ncounted in order to quantify the yield in the fermentation process. This manual\ntask is time-consuming and error-prone. In this work we test multiple\nsegmentation algorithms based on the U-Net CNN architecture and show that these\noffer robust, automated CFU counting. We show that the multiclass\ngeneralisation with a bespoke loss function allows distinguishing virulent and\navirulent colonies with acceptable accuracy. While many possibilities are left\nto explore, our results show the potential of deep learning for separating and\nclassifying bacterial colonies.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 10:10:43 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Beznik", "Thomas", ""], ["Smyth", "Paul", ""], ["de Lannoy", "Ga\u00ebl", ""], ["Lee", "John A.", ""]]}, {"id": "2009.00935", "submitter": "Jianwen Lou", "authors": "Jianwen Lou, Xiaoxu Cai, Junyu Dong and Hui Yu", "title": "Real-time 3D Facial Tracking via Cascaded Compositional Learning", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3065819", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to learn a cascade of globally-optimized modular boosted ferns\n(GoMBF) to solve multi-modal facial motion regression for real-time 3D facial\ntracking from a monocular RGB camera. GoMBF is a deep composition of multiple\nregression models with each is a boosted ferns initially trained to predict\npartial motion parameters of the same modality, and then concatenated together\nvia a global optimization step to form a singular strong boosted ferns that can\neffectively handle the whole regression target. It can explicitly cope with the\nmodality variety in output variables, while manifesting increased fitting power\nand a faster learning speed comparing against the conventional boosted ferns.\nBy further cascading a sequence of GoMBFs (GoMBF-Cascade) to regress facial\nmotion parameters, we achieve competitive tracking performance on a variety of\nin-the-wild videos comparing to the state-of-the-art methods, which require\nmuch more training data or have higher computational complexity. It provides a\nrobust and highly elegant solution to real-time 3D facial tracking using a\nsmall set of training data and hence makes it more practical in real-world\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 10:27:36 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Lou", "Jianwen", ""], ["Cai", "Xiaoxu", ""], ["Dong", "Junyu", ""], ["Yu", "Hui", ""]]}, {"id": "2009.00938", "submitter": "Jianwen Lou", "authors": "Xiaoxu Cai, Hui Yu, Jianwen Lou, Xuguang Zhang, Gongfa Li, Junyu Dong", "title": "3D Facial Geometry Recovery from a Depth View with Attention Guided\n  Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present to recover the complete 3D facial geometry from a single depth\nview by proposing an Attention Guided Generative Adversarial Networks (AGGAN).\nIn contrast to existing work which normally requires two or more depth views to\nrecover a full 3D facial geometry, the proposed AGGAN is able to generate a\ndense 3D voxel grid of the face from a single unconstrained depth view.\nSpecifically, AGGAN encodes the 3D facial geometry within a voxel space and\nutilizes an attention-guided GAN to model the illposed 2.5D depth-3D mapping.\nMultiple loss functions, which enforce the 3D facial geometry consistency,\ntogether with a prior distribution of facial surface points in voxel space are\nincorporated to guide the training process. Both qualitative and quantitative\ncomparisons show that AGGAN recovers a more complete and smoother 3D facial\nshape, with the capability to handle a much wider range of view angles and\nresist to noise in the depth view than conventional methods\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 10:35:26 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Cai", "Xiaoxu", ""], ["Yu", "Hui", ""], ["Lou", "Jianwen", ""], ["Zhang", "Xuguang", ""], ["Li", "Gongfa", ""], ["Dong", "Junyu", ""]]}, {"id": "2009.00944", "submitter": "Hao Wang", "authors": "Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao", "title": "Structure-Aware Generation Network for Recipe Generation from Images", "comments": "Published at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sharing food has become very popular with the development of social media.\nFor many real-world applications, people are keen to know the underlying\nrecipes of a food item. In this paper, we are interested in automatically\ngenerating cooking instructions for food. We investigate an open research task\nof generating cooking instructions based on only food images and ingredients,\nwhich is similar to the image captioning task. However, compared with image\ncaptioning datasets, the target recipes are long-length paragraphs and do not\nhave annotations on structure information. To address the above limitations, we\npropose a novel framework of Structure-aware Generation Network (SGN) to tackle\nthe food recipe generation task. Our approach brings together several novel\nideas in a systematic framework: (1) exploiting an unsupervised learning\napproach to obtain the sentence-level tree structure labels before training;\n(2) generating trees of target recipes from images with the supervision of tree\nstructure labels learned from (1); and (3) integrating the inferred tree\nstructures with the recipe generation procedure. Our proposed model can produce\nhigh-quality and coherent recipes, and achieve the state-of-the-art performance\non the benchmark Recipe1M dataset.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 10:54:25 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Wang", "Hao", ""], ["Lin", "Guosheng", ""], ["Hoi", "Steven C. H.", ""], ["Miao", "Chunyan", ""]]}, {"id": "2009.00952", "submitter": "Kun Zhan", "authors": "Kun Zhan, Chaoxi Niu", "title": "Mutual Teaching for Graph Convolutional Networks", "comments": "GCN, 8 pages, 1 figures", "journal-ref": "Future Generation Computer Systems, 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional networks produce good predictions of unlabeled samples\ndue to its transductive label propagation. Since samples have different\npredicted confidences, we take high-confidence predictions as pseudo labels to\nexpand the label set so that more samples are selected for updating models. We\npropose a new training method named as mutual teaching, i.e., we train dual\nmodels and let them teach each other during each batch. First, each network\nfeeds forward all samples and selects samples with high-confidence predictions.\nSecond, each model is updated by samples selected by its peer network. We view\nthe high-confidence predictions as useful knowledge, and the useful knowledge\nof one network teaches the peer network with model updating in each batch. In\nmutual teaching, the pseudo-label set of a network is from its peer network.\nSince we use the new strategy of network training, performance improves\nsignificantly. Extensive experimental results demonstrate that our method\nachieves superior performance over state-of-the-art methods under very low\nlabel rates.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 11:10:55 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Zhan", "Kun", ""], ["Niu", "Chaoxi", ""]]}, {"id": "2009.00953", "submitter": "Zeyu Cao", "authors": "Zeyu Cao, Xiaorun Li, Liaoying Zhao", "title": "Unsupervised Feature Learning by Autoencoder and Prototypical\n  Contrastive Learning for Hyperspectral Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning methods for feature extraction are becoming more and\nmore popular. We combine the popular contrastive learning method (prototypical\ncontrastive learning) and the classic representation learning method\n(autoencoder) to design an unsupervised feature learning network for\nhyperspectral classification. Experiments have proved that our two proposed\nautoencoder networks have good feature learning capabilities by themselves, and\nthe contrastive learning network we designed can better combine the features of\nthe two to learn more representative features. As a result, our method\nsurpasses other comparison methods in the hyperspectral classification\nexperiments, including some supervised methods. Moreover, our method maintains\na fast feature extraction speed than baseline methods. In addition, our method\nreduces the requirements for huge computing resources, separates feature\nextraction and contrastive learning, and allows more researchers to conduct\nresearch and experiments on unsupervised contrastive learning.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 11:17:48 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Cao", "Zeyu", ""], ["Li", "Xiaorun", ""], ["Zhao", "Liaoying", ""]]}, {"id": "2009.00960", "submitter": "Chen Ma", "authors": "Chen Ma, Li Chen, Jun-Hai Yong", "title": "Simulating Unknown Target Models for Query-Efficient Black-box Attacks", "comments": "Accepted at CVPR 2021. Code and models are available at\n  https://github.com/machanic/SimulatorAttack", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many adversarial attacks have been proposed to investigate the security\nissues of deep neural networks. In the black-box setting, current model\nstealing attacks train a substitute model to counterfeit the functionality of\nthe target model. However, the training requires querying the target model.\nConsequently, the query complexity remains high, and such attacks can be\ndefended easily. This study aims to train a generalized substitute model called\n\"Simulator\", which can mimic the functionality of any unknown target model. To\nthis end, we build the training data with the form of multiple tasks by\ncollecting query sequences generated during the attacks of various existing\nnetworks. The learning process uses a mean square error-based\nknowledge-distillation loss in the meta-learning to minimize the difference\nbetween the Simulator and the sampled networks. The meta-gradients of this loss\nare then computed and accumulated from multiple tasks to update the Simulator\nand subsequently improve generalization. When attacking a target model that is\nunseen in training, the trained Simulator can accurately simulate its\nfunctionality using its limited feedback. As a result, a large fraction of\nqueries can be transferred to the Simulator, thereby reducing query complexity.\nResults of the comprehensive experiments conducted using the CIFAR-10,\nCIFAR-100, and TinyImageNet datasets demonstrate that the proposed approach\nreduces query complexity by several orders of magnitude compared to the\nbaseline method. The implementation source code is released at\nhttps://github.com/machanic/SimulatorAttack.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 11:30:40 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 14:01:46 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Ma", "Chen", ""], ["Chen", "Li", ""], ["Yong", "Jun-Hai", ""]]}, {"id": "2009.00982", "submitter": "Shadrokh Samavi", "authors": "Sajjad Abbasi, Mohsen Hajabdollahi, Pejman Khadivi, Nader Karimi,\n  Roshanak Roshandel, Shahram Shirani, Shadrokh Samavi", "title": "Classification of Diabetic Retinopathy Using Unlabeled Data and\n  Knowledge Distillation", "comments": "21 pages, 6 figures, 7 tables. arXiv admin note: substantial text\n  overlap with arXiv:2002.03321", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation allows transferring knowledge from a pre-trained model\nto another. However, it suffers from limitations, and constraints related to\nthe two models need to be architecturally similar. Knowledge distillation\naddresses some of the shortcomings associated with transfer learning by\ngeneralizing a complex model to a lighter model. However, some parts of the\nknowledge may not be distilled by knowledge distillation sufficiently. In this\npaper, a novel knowledge distillation approach using transfer learning is\nproposed. The proposed method transfers the entire knowledge of a model to a\nnew smaller one. To accomplish this, unlabeled data are used in an unsupervised\nmanner to transfer the maximum amount of knowledge to the new slimmer model.\nThe proposed method can be beneficial in medical image analysis, where labeled\ndata are typically scarce. The proposed approach is evaluated in the context of\nclassification of images for diagnosing Diabetic Retinopathy on two publicly\navailable datasets, including Messidor and EyePACS. Simulation results\ndemonstrate that the approach is effective in transferring knowledge from a\ncomplex model to a lighter one. Furthermore, experimental results illustrate\nthat the performance of different small models is improved significantly using\nunlabeled data and knowledge distillation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 07:18:39 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Abbasi", "Sajjad", ""], ["Hajabdollahi", "Mohsen", ""], ["Khadivi", "Pejman", ""], ["Karimi", "Nader", ""], ["Roshandel", "Roshanak", ""], ["Shirani", "Shahram", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "2009.00984", "submitter": "Lorenzo Bertoni", "authors": "Lorenzo Bertoni, Sven Kreiss, Alexandre Alahi", "title": "Perceiving Humans: from Monocular 3D Localization to Social Distancing", "comments": "IEEE Transactions on Intelligent Transportation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceiving humans in the context of Intelligent Transportation Systems (ITS)\noften relies on multiple cameras or expensive LiDAR sensors. In this work, we\npresent a new cost-effective vision-based method that perceives humans'\nlocations in 3D and their body orientation from a single image. We address the\nchallenges related to the ill-posed monocular 3D tasks by proposing a neural\nnetwork architecture that predicts confidence intervals in contrast to point\nestimates. Our neural network estimates human 3D body locations and their\norientation with a measure of uncertainty. Our proposed solution (i) is\nprivacy-safe, (ii) works with any fixed or moving cameras, and (iii) does not\nrely on ground plane estimation. We demonstrate the performance of our method\nwith respect to three applications: locating humans in 3D, detecting social\ninteractions, and verifying the compliance of recent safety measures due to the\nCOVID-19 outbreak. We show that it is possible to rethink the concept of\n\"social distancing\" as a form of social interaction in contrast to a simple\nlocation-based rule. We publicly share the source code towards an open science\nmission.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 10:12:30 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 10:30:19 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Bertoni", "Lorenzo", ""], ["Kreiss", "Sven", ""], ["Alahi", "Alexandre", ""]]}, {"id": "2009.01005", "submitter": "Akash Gupta", "authors": "Akash Gupta, Abhishek Aich, Amit K. Roy-Chowdhury", "title": "ALANET: Adaptive Latent Attention Network forJoint Video Deblurring and\n  Interpolation", "comments": "Accepted to ACM-MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing works address the problem of generating high frame-rate sharp videos\nby separately learning the frame deblurring and frame interpolation modules.\nMost of these approaches have a strong prior assumption that all the input\nframes are blurry whereas in a real-world setting, the quality of frames\nvaries. Moreover, such approaches are trained to perform either of the two\ntasks - deblurring or interpolation - in isolation, while many practical\nsituations call for both. Different from these works, we address a more\nrealistic problem of high frame-rate sharp video synthesis with no prior\nassumption that input is always blurry. We introduce a novel architecture,\nAdaptive Latent Attention Network (ALANET), which synthesizes sharp high\nframe-rate videos with no prior knowledge of input frames being blurry or not,\nthereby performing the task of both deblurring and interpolation. We\nhypothesize that information from the latent representation of the consecutive\nframes can be utilized to generate optimized representations for both frame\ndeblurring and frame interpolation. Specifically, we employ combination of\nself-attention and cross-attention module between consecutive frames in the\nlatent space to generate optimized representation for each frame. The optimized\nrepresentation learnt using these attention modules help the model to generate\nand interpolate sharp frames. Extensive experiments on standard datasets\ndemonstrate that our method performs favorably against various state-of-the-art\napproaches, even though we tackle a much more difficult problem.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 21:11:53 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Gupta", "Akash", ""], ["Aich", "Abhishek", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "2009.01027", "submitter": "Xiangxiang Chu", "authors": "Xiangxiang Chu, Xiaoxing Wang, Bo Zhang, Shun Lu, Xiaolin Wei, Junchi\n  Yan", "title": "DARTS-: Robustly Stepping out of Performance Collapse Without Indicators", "comments": "Accepted to ICLR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the fast development of differentiable architecture search (DARTS),\nit suffers from long-standing performance instability, which extremely limits\nits application. Existing robustifying methods draw clues from the resulting\ndeteriorated behavior instead of finding out its causing factor. Various\nindicators such as Hessian eigenvalues are proposed as a signal to stop\nsearching before the performance collapses. However, these indicator-based\nmethods tend to easily reject good architectures if the thresholds are\ninappropriately set, let alone the searching is intrinsically noisy. In this\npaper, we undertake a more subtle and direct approach to resolve the collapse.\nWe first demonstrate that skip connections have a clear advantage over other\ncandidate operations, where it can easily recover from a disadvantageous state\nand become dominant. We conjecture that this privilege is causing degenerated\nperformance. Therefore, we propose to factor out this benefit with an auxiliary\nskip connection, ensuring a fairer competition for all operations. We call this\napproach DARTS-. Extensive experiments on various datasets verify that it can\nsubstantially improve robustness. Our code is available at\nhttps://github.com/Meituan-AutoML/DARTS- .\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 12:54:13 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 07:58:11 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Chu", "Xiangxiang", ""], ["Wang", "Xiaoxing", ""], ["Zhang", "Bo", ""], ["Lu", "Shun", ""], ["Wei", "Xiaolin", ""], ["Yan", "Junchi", ""]]}, {"id": "2009.01030", "submitter": "Haiwei Wu", "authors": "Haiwei Wu and Jiantao Zhou", "title": "Privacy Leakage of SIFT Features via Deep Generative Model based Image\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many practical applications, e.g., content based image retrieval and object\nrecognition, heavily rely on the local features extracted from the query image.\nAs these local features are usually exposed to untrustworthy parties, the\nprivacy leakage problem of image local features has received increasing\nattention in recent years. In this work, we thoroughly evaluate the privacy\nleakage of Scale Invariant Feature Transform (SIFT), which is one of the most\nwidely-used image local features. We first consider the case that the adversary\ncan fully access the SIFT features, i.e., both the SIFT descriptors and the\ncoordinates are available. We propose a novel end-to-end, coarse-to-fine deep\ngenerative model for reconstructing the latent image from its SIFT features.\nThe designed deep generative model consists of two networks, where the first\none attempts to learn the structural information of the latent image by\ntransforming from SIFT features to Local Binary Pattern (LBP) features, while\nthe second one aims to reconstruct the pixel values guided by the learned LBP.\nCompared with the state-of-the-art algorithms, the proposed deep generative\nmodel produces much improved reconstructed results over three public datasets.\nFurthermore, we address more challenging cases that only partial SIFT features\n(either SIFT descriptors or coordinates) are accessible to the adversary. It is\nshown that, if the adversary can only have access to the SIFT descriptors while\nnot their coordinates, then the modest success of reconstructing the latent\nimage can be achieved for highly-structured images (e.g., faces) and would fail\nin general settings. In addition, the latent image can be reconstructed with\nreasonably good quality solely from the SIFT coordinates. Our results would\nsuggest that the privacy leakage problem can be largely avoided if the SIFT\ncoordinates can be well protected.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 12:59:12 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Wu", "Haiwei", ""], ["Zhou", "Jiantao", ""]]}, {"id": "2009.01031", "submitter": "Haiwei Wu", "authors": "Haiwei Wu and Jiantao Zhou and Yuanman Li", "title": "Deep Generative Model for Image Inpainting with Local Binary Pattern\n  Learning and Spatial Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) has demonstrated its powerful capabilities in the field of\nimage inpainting. The DL-based image inpainting approaches can produce visually\nplausible results, but often generate various unpleasant artifacts, especially\nin the boundary and highly textured regions. To tackle this challenge, in this\nwork, we propose a new end-to-end, two-stage (coarse-to-fine) generative model\nthrough combining a local binary pattern (LBP) learning network with an actual\ninpainting network. Specifically, the first LBP learning network using U-Net\narchitecture is designed to accurately predict the structural information of\nthe missing region, which subsequently guides the second image inpainting\nnetwork for better filling the missing pixels. Furthermore, an improved spatial\nattention mechanism is integrated in the image inpainting network, by\nconsidering the consistency not only between the known region with the\ngenerated one, but also within the generated region itself. Extensive\nexperiments on public datasets including CelebA-HQ, Places and Paris StreetView\ndemonstrate that our model generates better inpainting results than the\nstate-of-the-art competing algorithms, both quantitatively and qualitatively.\nThe source code and trained models will be made available at\nhttps://github.com/HighwayWu/ImageInpainting.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 12:59:28 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Wu", "Haiwei", ""], ["Zhou", "Jiantao", ""], ["Li", "Yuanman", ""]]}, {"id": "2009.01035", "submitter": "Ruibing Hou", "authors": "Ruibing Hou and Bingpeng Ma and Hong Chang and Xinqian Gu and Shiguang\n  Shan and Xilin Chen", "title": "IAUnet: Global Context-Aware Feature Learning for Person\n  Re-Identification", "comments": "14 pages, 9 figures. Accepted by IEEE Transactions on Neural Networks\n  and Learning Systems (TNNLS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (reID) by CNNs based networks has achieved favorable\nperformance in recent years. However, most of existing CNNs based methods do\nnot take full advantage of spatial-temporal context modeling. In fact, the\nglobal spatial-temporal context can greatly clarify local distractions to\nenhance the target feature representation. To comprehensively leverage the\nspatial-temporal context information, in this work, we present a novel block,\nInteraction-Aggregation-Update (IAU), for high-performance person reID.\nFirstly, Spatial-Temporal IAU (STIAU) module is introduced. STIAU jointly\nincorporates two types of contextual interactions into a CNN framework for\ntarget feature learning. Here the spatial interactions learn to compute the\ncontextual dependencies between different body parts of a single frame. While\nthe temporal interactions are used to capture the contextual dependencies\nbetween the same body parts across all frames. Furthermore, a Channel IAU\n(CIAU) module is designed to model the semantic contextual interactions between\nchannel features to enhance the feature representation, especially for\nsmall-scale visual cues and body parts. Therefore, the IAU block enables the\nfeature to incorporate the globally spatial, temporal, and channel context. It\nis lightweight, end-to-end trainable, and can be easily plugged into existing\nCNNs to form IAUnet. The experiments show that IAUnet performs favorably\nagainst state-of-the-art on both image and video reID tasks and achieves\ncompelling results on a general object categorization task. The source code is\navailable at https://github.com/blue-blue272/ImgReID-IAnet.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 13:07:10 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Hou", "Ruibing", ""], ["Ma", "Bingpeng", ""], ["Chang", "Hong", ""], ["Gu", "Xinqian", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "2009.01039", "submitter": "Alessio Sarullo", "authors": "Alessio Sarullo, Tingting Mu", "title": "Zero-Shot Human-Object Interaction Recognition via Affordance Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for Zero-Shot Human-Object Interaction Recognition\nin the challenging setting that involves interactions with unseen actions (as\nopposed to just unseen combinations of seen actions and objects). Our approach\nmakes use of knowledge external to the image content in the form of a graph\nthat models affordance relations between actions and objects, i.e., whether an\naction can be performed on the given object or not. We propose a loss function\nwith the aim of distilling the knowledge contained in the graph into the model,\nwhile also using the graph to regularise learnt representations by imposing a\nlocal structure on the latent space. We evaluate our approach on several\ndatasets (including the popular HICO and HICO-DET) and show that it outperforms\nthe current state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 13:14:44 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Sarullo", "Alessio", ""], ["Mu", "Tingting", ""]]}, {"id": "2009.01053", "submitter": "James-Andrew Sarmiento", "authors": "James-Andrew Sarmiento", "title": "Exploiting Latent Codes: Interactive Fashion Product Generation, Similar\n  Image Retrieval, and Cross-Category Recommendation using Variational\n  Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rise of deep learning applications in the fashion industry has fueled\nadvances in curating large-scale datasets to build applications for product\ndesign, image retrieval, and recommender systems. In this paper, the author\nproposes using Variational Autoencoder (VAE) to build an interactive fashion\nproduct application framework that allows the users to generate products with\nattributes according to their liking, retrieve similar styles for the same\nproduct category, and receive content-based recommendations from other\ncategories. Fashion product images dataset containing eyewear, footwear, and\nbags are appropriate to illustrate that this pipeline is applicable in the\nbooming industry of e-commerce enabling direct user interaction in specifying\ndesired products paired with new methods for data matching, and recommendation\nsystems by using VAE and exploiting its generated latent codes.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 13:27:30 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Sarmiento", "James-Andrew", ""]]}, {"id": "2009.01062", "submitter": "Akram Hussain Engr.", "authors": "Akram Hussain, Yuan Luo", "title": "Decentralized Source Localization without Sensor Parameters in Wireless\n  Sensor Networks", "comments": "16 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the source (event) localization problem in decentralized\nwireless sensor networks (WSNs) under the fault model without knowing the\nsensor parameters. Event localizations have many applications such as\nlocalizing intruders, Wifi hotspots and users, and faults in power systems.\nPrevious studies assume the true knowledge (or good estimates) of sensor\nparameters (e.g., fault model probability or Region of Influence (ROI) of the\nsource) for source localization. However, we propose two methods to estimate\nthe source location in this paper under the fault model: hitting set approach\nand feature selection method, which only utilize the noisy data set at the\nfusion center for estimation of the source location without knowing the sensor\nparameters. The proposed methods have been shown to localize the source\neffectively. We also study the lower bound on the sample complexity requirement\nfor hitting set method. These methods have also been extended for multiple\nsources localizations. In addition, we modify the proposed feature selection\napproach to use maximum likelihood. Finally, extensive simulations are carried\nout for different settings (i.e., the number of sensor nodes and sample\ncomplexity) to validate our proposed methods in comparison to centroid, maximum\nlikelihood, FTML, SNAP estimators.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 13:34:55 GMT"}, {"version": "v2", "created": "Sun, 13 Sep 2020 13:32:52 GMT"}, {"version": "v3", "created": "Sat, 31 Oct 2020 13:18:23 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Hussain", "Akram", ""], ["Luo", "Yuan", ""]]}, {"id": "2009.01067", "submitter": "Jingyi Hou", "authors": "Jingyi Hou, Yunde Jia, Xinxiao wu, Yayun Qi", "title": "Video Captioning Using Weak Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video captioning has shown impressive progress in recent years. One key\nreason of the performance improvements made by existing methods lie in massive\npaired video-sentence data, but collecting such strong annotation, i.e.,\nhigh-quality sentences, is time-consuming and laborious. It is the fact that\nthere now exist an amazing number of videos with weak annotation that only\ncontains semantic concepts such as actions and objects. In this paper, we\ninvestigate using weak annotation instead of strong annotation to train a video\ncaptioning model. To this end, we propose a progressive visual reasoning method\nthat progressively generates fine sentences from weak annotations by inferring\nmore semantic concepts and their dependency relationships for video captioning.\nTo model concept relationships, we use dependency trees that are spanned by\nexploiting external knowledge from large sentence corpora. Through traversing\nthe dependency trees, the sentences are generated to train the captioning\nmodel. Accordingly, we develop an iterative refinement algorithm that refines\nsentences via spanning dependency trees and fine-tunes the captioning model\nusing the refined sentences in an alternative training manner. Experimental\nresults demonstrate that our method using weak annotation is very competitive\nto the state-of-the-art methods using strong annotation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 13:45:01 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Hou", "Jingyi", ""], ["Jia", "Yunde", ""], ["wu", "Xinxiao", ""], ["Qi", "Yayun", ""]]}, {"id": "2009.01081", "submitter": "Tewodros Ayalew", "authors": "Tewodros Ayalew, Jordan Ubbens, Ian Stavness", "title": "Unsupervised Domain Adaptation For Plant Organ Counting", "comments": "To be published in Computer Vision Problems in Plant Phenotyping\n  (CVPPP) in conjunction with ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning is often used to count objects in images, but for\ncounting small, densely located objects, the required image annotations are\nburdensome to collect. Counting plant organs for image-based plant phenotyping\nfalls within this category. Object counting in plant images is further\nchallenged by having plant image datasets with significant domain shift due to\ndifferent experimental conditions, e.g. applying an annotated dataset of indoor\nplant images for use on outdoor images, or on a different plant species. In\nthis paper, we propose a domain-adversarial learning approach for domain\nadaptation of density map estimation for the purposes of object counting. The\napproach does not assume perfectly aligned distributions between the source and\ntarget datasets, which makes it more broadly applicable within general object\ncounting and plant organ counting tasks. Evaluation on two diverse object\ncounting tasks (wheat spikelets, leaves) demonstrates consistent performance on\nthe target datasets across different classes of domain shift: from\nindoor-to-outdoor images and from species-to-species adaptation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 13:57:09 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Ayalew", "Tewodros", ""], ["Ubbens", "Jordan", ""], ["Stavness", "Ian", ""]]}, {"id": "2009.01103", "submitter": "Torsten Schlett", "authors": "Torsten Schlett, Christian Rathgeb, Olaf Henniger, Javier Galbally,\n  Julian Fierrez, Christoph Busch", "title": "Face Image Quality Assessment: A Literature Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of face analysis and recognition systems depends on the\nquality of the acquired face data, which is influenced by numerous factors.\nAutomatically assessing the quality of face data in terms of biometric utility\ncan thus be useful to detect low-quality data and make decisions accordingly.\nThis survey provides an overview of the face image quality assessment\nliterature, which predominantly focuses on single visible wavelength face image\ninput. A trend towards deep learning based methods is observed, including\nnotable conceptual differences among the recent approaches, such as the\nintegration of quality assessment into face recognition models. Besides image\nselection, face image quality assessment can also be used in a variety of other\napplication scenarios, which are discussed herein. Open issues and challenges\nare pointed out, i.a. highlighting the importance of comparability for\nalgorithm evaluations, and the challenge for future work to create deep\nlearning approaches that are interpretable in addition to providing accurate\nutility predictions.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 14:26:12 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 12:06:53 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Schlett", "Torsten", ""], ["Rathgeb", "Christian", ""], ["Henniger", "Olaf", ""], ["Galbally", "Javier", ""], ["Fierrez", "Julian", ""], ["Busch", "Christoph", ""]]}, {"id": "2009.01110", "submitter": "Danilo Vasconcellos  Vargas", "authors": "Danilo Vasconcellos Vargas, Bingli Liao, Takahiro Kanzaki", "title": "Perceptual Deep Neural Networks: Adversarial Robustness through Input\n  Recreation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples have shown that albeit highly accurate, models learned\nby machines, differently from humans, have many weaknesses. However, humans'\nperception is also fundamentally different from machines, because we do not see\nthe signals which arrive at the retina but a rather complex recreation of them.\nIn this paper, we explore how machines could recreate the input as well as\ninvestigate the benefits of such an augmented perception. In this regard, we\npropose Perceptual Deep Neural Networks ($\\varphi$DNN) which also recreate\ntheir own input before further processing. The concept is formalized\nmathematically and two variations of it are developed (one based on inpainting\nthe whole image and the other based on a noisy resized super resolution\nrecreation). Experiments reveal that $\\varphi$DNNs and their adversarial\ntraining variations can increase the robustness substantially, surpassing both\nstate-of-the-art defenses and pre-processing types of defenses in 100% of the\ntests. $\\varphi$DNNs are shown to scale well to bigger image sizes, keeping a\nsimilar high accuracy throughout; while the state-of-the-art worsen up to 35%.\nMoreover, the recreation process intentionally corrupts the input image.\nInterestingly, we show by ablation tests that corrupting the input is, although\ncounter-intuitive, beneficial. Thus, $\\varphi$DNNs reveal that input recreation\nhas strong benefits for artificial neural networks similar to biological ones,\nshedding light into the importance of purposely corrupting the input as well as\npioneering an area of perception models based on GANs and autoencoders for\nrobust recognition in artificial intelligence.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 14:36:36 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 09:39:09 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 08:58:13 GMT"}, {"version": "v4", "created": "Mon, 30 Nov 2020 10:36:03 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Vargas", "Danilo Vasconcellos", ""], ["Liao", "Bingli", ""], ["Kanzaki", "Takahiro", ""]]}, {"id": "2009.01129", "submitter": "Wang Zhou", "authors": "Wang Zhou, Shiyu Chang, Norma Sosa, Hendrik Hamann, David Cox", "title": "Lifelong Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in object detection have benefited significantly from rapid\ndevelopments in deep neural networks. However, neural networks suffer from the\nwell-known issue of catastrophic forgetting, which makes continual or lifelong\nlearning problematic. In this paper, we leverage the fact that new training\nclasses arrive in a sequential manner and incrementally refine the model so\nthat it additionally detects new object classes in the absence of previous\ntraining data. Specifically, we consider the representative object detector,\nFaster R-CNN, for both accurate and efficient prediction. To prevent abrupt\nperformance degradation due to catastrophic forgetting, we propose to apply\nknowledge distillation on both the region proposal network and the region\nclassification network, to retain the detection of previously trained classes.\nA pseudo-positive-aware sampling strategy is also introduced for distillation\nsample selection. We evaluate the proposed method on PASCAL VOC 2007 and MS\nCOCO benchmarks and show competitive mAP and 6x inference speed improvement,\nwhich makes the approach more suitable for real-time applications. Our\nimplementation will be publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 15:08:51 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Zhou", "Wang", ""], ["Chang", "Shiyu", ""], ["Sosa", "Norma", ""], ["Hamann", "Hendrik", ""], ["Cox", "David", ""]]}, {"id": "2009.01142", "submitter": "Yazan Abu Farha", "authors": "Yazan Abu Farha, Qiuhong Ke, Bernt Schiele, Juergen Gall", "title": "Long-Term Anticipation of Activities with Cycle Consistency", "comments": "GCPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the success of deep learning methods in analyzing activities in videos,\nmore attention has recently been focused towards anticipating future\nactivities. However, most of the work on anticipation either analyzes a\npartially observed activity or predicts the next action class. Recently, new\napproaches have been proposed to extend the prediction horizon up to several\nminutes in the future and that anticipate a sequence of future activities\nincluding their durations. While these works decouple the semantic\ninterpretation of the observed sequence from the anticipation task, we propose\na framework for anticipating future activities directly from the features of\nthe observed frames and train it in an end-to-end fashion. Furthermore, we\nintroduce a cycle consistency loss over time by predicting the past activities\ngiven the predicted future. Our framework achieves state-of-the-art results on\ntwo datasets: the Breakfast dataset and 50Salads.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 15:41:32 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Farha", "Yazan Abu", ""], ["Ke", "Qiuhong", ""], ["Schiele", "Bernt", ""], ["Gall", "Juergen", ""]]}, {"id": "2009.01152", "submitter": "Hamed Ayoobi", "authors": "H. Ayoobi, H. Kasaei, M. Cao, R. Verbrugge, B. Verheij", "title": "Local-HDP: Interactive Open-Ended 3D Object Categorization in Real-Time\n  Robotic Scenarios", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a non-parametric hierarchical Bayesian approach for open-ended\n3D object categorization, named the Local Hierarchical Dirichlet Process\n(Local-HDP). This method allows an agent to learn independent topics for each\ncategory incrementally and to adapt to the environment in time. Hierarchical\nBayesian approaches like Latent Dirichlet Allocation (LDA) can transform\nlow-level features to high-level conceptual topics for 3D object\ncategorization. However, the efficiency and accuracy of LDA-based approaches\ndepend on the number of topics that is chosen manually. Moreover, fixing the\nnumber of topics for all categories can lead to overfitting or underfitting of\nthe model. In contrast, the proposed Local-HDP can autonomously determine the\nnumber of topics for each category. Furthermore, the online variational\ninference method has been adapted for fast posterior approximation in the\nLocal-HDP model. Experiments show that the proposed Local-HDP method\noutperforms other state-of-the-art approaches in terms of accuracy,\nscalability, and memory efficiency by a large margin. Moreover, two robotic\nexperiments have been conducted to show the applicability of the proposed\napproach in real-time applications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 15:55:49 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 17:18:12 GMT"}, {"version": "v3", "created": "Sun, 11 Apr 2021 17:58:49 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ayoobi", "H.", ""], ["Kasaei", "H.", ""], ["Cao", "M.", ""], ["Verbrugge", "R.", ""], ["Verheij", "B.", ""]]}, {"id": "2009.01166", "submitter": "Luigi Musto", "authors": "Luigi Musto and Andrea Zinelli", "title": "Semantically Adaptive Image-to-image Translation for Domain Adaptation\n  of Semantic Segmentation", "comments": "Paper will appear on BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain shift is a very challenging problem for semantic segmentation. Any\nmodel can be easily trained on synthetic data, where images and labels are\nartificially generated, but it will perform poorly when deployed on real\nenvironments. In this paper, we address the problem of domain adaptation for\nsemantic segmentation of street scenes. Many state-of-the-art approaches focus\non translating the source image while imposing that the result should be\nsemantically consistent with the input. However, we advocate that the image\nsemantics can also be exploited to guide the translation algorithm. To this\nend, we rethink the generative model to enforce this assumption and strengthen\nthe connection between pixel-level and feature-level domain alignment. We\nconduct extensive experiments by training common semantic segmentation models\nwith our method and show that the results we obtain on the synthetic-to-real\nbenchmarks surpass the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 16:16:50 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Musto", "Luigi", ""], ["Zinelli", "Andrea", ""]]}, {"id": "2009.01174", "submitter": "Sean I. Young", "authors": "Sean I. Young, Wang Zhe, David Taubman, and Bernd Girod", "title": "Transform Quantization for CNN Compression", "comments": "To appear in IEEE Trans Pattern Anal Mach Intell", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT cs.LG eess.IV math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we compress convolutional neural network (CNN) weights\npost-training via transform quantization. Previous CNN quantization techniques\ntend to ignore the joint statistics of weights and activations, producing\nsub-optimal CNN performance at a given quantization bit-rate, or consider their\njoint statistics during training only and do not facilitate efficient\ncompression of already trained CNN models. We optimally transform (decorrelate)\nand quantize the weights post-training using a rate-distortion framework to\nimprove compression at any given quantization bit-rate. Transform quantization\nunifies quantization and dimensionality reduction (decorrelation) techniques in\na single framework to facilitate low bit-rate compression of CNNs and efficient\ninference in the transform domain. We first introduce a theory of rate and\ndistortion for CNN quantization, and pose optimum quantization as a\nrate-distortion optimization problem. We then show that this problem can be\nsolved using optimal bit-depth allocation following decorrelation by the\noptimal End-to-end Learned Transform (ELT) we derive in this paper. Experiments\ndemonstrate that transform quantization advances the state of the art in CNN\ncompression in both retrained and non-retrained quantization scenarios. In\nparticular, we find that transform quantization with retraining is able to\ncompress CNN models such as AlexNet, ResNet and DenseNet to very low bit-rates\n(1-2 bits).\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 16:33:42 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 17:52:53 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 04:02:55 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Young", "Sean I.", ""], ["Zhe", "Wang", ""], ["Taubman", "David", ""], ["Girod", "Bernd", ""]]}, {"id": "2009.01181", "submitter": "Sagar Kora Venu", "authors": "Sagar Kora Venu", "title": "Evaluation of Deep Convolutional Generative Adversarial Networks for\n  data augmentation of chest X-ray images", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": "10.3390/fi13010008", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Medical image datasets are usually imbalanced, due to the high costs of\nobtaining the data and time-consuming annotations. Training deep neural network\nmodels on such datasets to accurately classify the medical condition does not\nyield desired results and often over-fits the data on majority class samples.\nIn order to address this issue, data augmentation is often performed on\ntraining data by position augmentation techniques such as scaling, cropping,\nflipping, padding, rotation, translation, affine transformation, and color\naugmentation techniques such as brightness, contrast, saturation, and hue to\nincrease the dataset sizes. These augmentation techniques are not guaranteed to\nbe advantageous in domains with limited data, especially medical image data,\nand could lead to further overfitting. In this work, we performed data\naugmentation on the Chest X-rays dataset through generative modeling (deep\nconvolutional generative adversarial network) which creates artificial\ninstances retaining similar characteristics to the original data and evaluation\nof the model resulted in Fr\\'echet Distance of Inception (FID) score of 1.289.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 16:43:55 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Venu", "Sagar Kora", ""]]}, {"id": "2009.01192", "submitter": "Faezeh Nejati Hatamian", "authors": "Faezeh Nejati Hatamian, AmirAbbas Davari, Andreas Maier", "title": "The Effect of Various Strengths of Noises and Data Augmentations on\n  Classification of Short Single-Lead ECG Signals Using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the multiple imperfections during the signal acquisition,\nElectrocardiogram (ECG) datasets are typically contaminated with numerous types\nof noise, like salt and pepper and baseline drift. These datasets may contain\ndifferent recordings with various types of noise [1] and thus, denoising may\nnot be the easiest task. Furthermore, usually, the number of labeled\nbio-signals is very limited for a proper classification task.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 15:26:17 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Hatamian", "Faezeh Nejati", ""], ["Davari", "AmirAbbas", ""], ["Maier", "Andreas", ""]]}, {"id": "2009.01193", "submitter": "Tashnim Chowdhury", "authors": "Maryam Rahnemoonfar, Tashnim Chowdhury, Robin Murphy, Odair Fernandes", "title": "Comprehensive Semantic Segmentation on High Resolution UAV Imagery for\n  Natural Disaster Damage Assessment", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a large-scale hurricane Michael dataset for visual\nperception in disaster scenarios, and analyze state-of-the-art deep neural\nnetwork models for semantic segmentation. The dataset consists of around 2000\nhigh-resolution aerial images, with annotated ground-truth data for semantic\nsegmentation. We discuss the challenges of the dataset and train the\nstate-of-the-art methods on this dataset to evaluate how well these methods can\nrecognize the disaster situations. Finally, we discuss challenges for future\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 17:07:28 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 22:03:20 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Rahnemoonfar", "Maryam", ""], ["Chowdhury", "Tashnim", ""], ["Murphy", "Robin", ""], ["Fernandes", "Odair", ""]]}, {"id": "2009.01215", "submitter": "Michael Lyons", "authors": "Michael J. Lyons", "title": "Excavating \"Excavating AI\": The Elephant in the Gallery", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": "10.5281/zenodo.4037538", "report-no": null, "categories": "cs.CY cs.AI cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two art exhibitions, \"Training Humans\" and \"Making Faces,\" and the\naccompanying essay \"Excavating AI: The politics of images in machine learning\ntraining sets\" by Kate Crawford and Trevor Paglen, are making substantial\nimpact on discourse taking place in the social and mass media networks, and\nsome scholarly circles. Critical scrutiny reveals, however, a\nself-contradictory stance regarding informed consent for the use of facial\nimages, as well as serious flaws in their critique of ML training sets. Our\nanalysis underlines the non-negotiability of informed consent when using human\ndata in artistic and other contexts, and clarifies issues relating to the\ndescription of ML training sets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 17:42:06 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 17:07:10 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2020 01:27:53 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Lyons", "Michael J.", ""]]}, {"id": "2009.01225", "submitter": "Liliane Momeni", "authors": "Liliane Momeni and Triantafyllos Afouras and Themos Stafylakis and\n  Samuel Albanie and Andrew Zisserman", "title": "Seeing wake words: Audio-visual Keyword Spotting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of this work is to automatically determine whether and when a word\nof interest is spoken by a talking face, with or without the audio. We propose\na zero-shot method suitable for in the wild videos. Our key contributions are:\n(1) a novel convolutional architecture, KWS-Net, that uses a similarity map\nintermediate representation to separate the task into (i) sequence matching,\nand (ii) pattern detection, to decide whether the word is there and when; (2)\nwe demonstrate that if audio is available, visual keyword spotting improves the\nperformance both for a clean and noisy audio signal. Finally, (3) we show that\nour method generalises to other languages, specifically French and German, and\nachieves a comparable performance to English with less language specific data,\nby fine-tuning the network pre-trained on English. The method exceeds the\nperformance of the previous state-of-the-art visual keyword spotting\narchitecture when trained and tested on the same benchmark, and also that of a\nstate-of-the-art lip reading method.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 17:57:38 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Momeni", "Liliane", ""], ["Afouras", "Triantafyllos", ""], ["Stafylakis", "Themos", ""], ["Albanie", "Samuel", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2009.01228", "submitter": "John Christian", "authors": "John A. Christian, Harm Derksen, and Ryan Watkins", "title": "Lunar Crater Identification in Digital Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is often necessary to identify a pattern of observed craters in a single\nimage of the lunar surface and without any prior knowledge of the camera's\nlocation. This so-called \"lost-in-space\" crater identification problem is\ncommon in both crater-based terrain relative navigation (TRN) and in automatic\nregistration of scientific imagery. Past work on crater identification has\nlargely been based on heuristic schemes, with poor performance outside of a\nnarrowly defined operating regime (e.g., nadir pointing images, small search\nareas). This work provides the first mathematically rigorous treatment of the\ngeneral crater identification problem. It is shown when it is (and when it is\nnot) possible to recognize a pattern of elliptical crater rims in an image\nformed by perspective projection. For the cases when it is possible to\nrecognize a pattern, descriptors are developed using invariant theory that\nprovably capture all of the viewpoint invariant information. These descriptors\nmay be pre-computed for known crater patterns and placed in a searchable index\nfor fast recognition. New techniques are also developed for computing pose from\ncrater rim observations and for evaluating crater rim correspondences. These\ntechniques are demonstrated on both synthetic and real images.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 17:59:51 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 16:25:05 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Christian", "John A.", ""], ["Derksen", "Harm", ""], ["Watkins", "Ryan", ""]]}, {"id": "2009.01270", "submitter": "Amogh Gudi", "authors": "Amogh Gudi, Xin Li, Jan van Gemert", "title": "Efficiency in Real-time Webcam Gaze Tracking", "comments": "Awarded Best Paper at European Conference on Computer Vision (ECCV)\n  Workshop on Eye Gaze in AR, VR, and in the Wild (OpenEyes) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiency and ease of use are essential for practical applications of camera\nbased eye/gaze-tracking. Gaze tracking involves estimating where a person is\nlooking on a screen based on face images from a computer-facing camera. In this\npaper we investigate two complementary forms of efficiency in gaze tracking: 1.\nThe computational efficiency of the system which is dominated by the inference\nspeed of a CNN predicting gaze-vectors; 2. The usability efficiency which is\ndetermined by the tediousness of the mandatory calibration of the gaze-vector\nto a computer screen. To do so, we evaluate the computational speed/accuracy\ntrade-off for the CNN and the calibration effort/accuracy trade-off for screen\ncalibration. For the CNN, we evaluate the full face, two-eyes, and single eye\ninput. For screen calibration, we measure the number of calibration points\nneeded and evaluate three types of calibration: 1. pure geometry, 2. pure\nmachine learning, and 3. hybrid geometric regression. Results suggest that a\nsingle eye input and geometric regression calibration achieve the best\ntrade-off.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 18:07:41 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Gudi", "Amogh", ""], ["Li", "Xin", ""], ["van Gemert", "Jan", ""]]}, {"id": "2009.01280", "submitter": "Min Zhang", "authors": "Min Zhang, Pranav Kadam, Shan Liu, C. -C. Jay Kuo", "title": "Unsupervised Feedforward Feature (UFF) Learning for Point Cloud\n  Classification and Segmentation", "comments": "7 pages, 2 figures, the final version is accepted by VCIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to supervised backpropagation-based feature learning in deep\nneural networks (DNNs), an unsupervised feedforward feature (UFF) learning\nscheme for joint classification and segmentation of 3D point clouds is proposed\nin this work. The UFF method exploits statistical correlations of points in a\npoint cloud set to learn shape and point features in a one-pass feedforward\nmanner through a cascaded encoder-decoder architecture. It learns global shape\nfeatures through the encoder and local point features through the concatenated\nencoder-decoder architecture. The extracted features of an input point cloud\nare fed to classifiers for shape classification and part segmentation.\nExperiments are conducted to evaluate the performance of the UFF method. For\nshape classification, the UFF is superior to existing unsupervised methods and\non par with state-of-the-art DNNs. For part segmentation, the UFF outperforms\nsemi-supervised methods and performs slightly worse than DNNs.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 18:25:25 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Zhang", "Min", ""], ["Kadam", "Pranav", ""], ["Liu", "Shan", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2009.01293", "submitter": "Pranav Kadam", "authors": "Pranav Kadam, Min Zhang, Shan Liu, C.-C. Jay Kuo", "title": "Unsupervised Point Cloud Registration via Salient Points Analysis (SPA)", "comments": "7 pages, 5 figures, final version is accepted by IEEE International\n  Conference on Visual Communications and Image Processing (VCIP) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An unsupervised point cloud registration method, called salient points\nanalysis (SPA), is proposed in this work. The proposed SPA method can register\ntwo point clouds effectively using only a small subset of salient points. It\nfirst applies the PointHop++ method to point clouds, finds corresponding\nsalient points in two point clouds based on the local surface characteristics\nof points and performs registration by matching the corresponding salient\npoints. The SPA method offers several advantages over the recent deep learning\nbased solutions for registration. Deep learning methods such as PointNetLK and\nDCP train end-to-end networks and rely on full supervision (namely, ground\ntruth transformation matrix and class label). In contrast, the SPA is\ncompletely unsupervised. Furthermore, SPA's training time and model size are\nmuch less. The effectiveness of the SPA method is demonstrated by experiments\non seen and unseen classes and noisy point clouds from the ModelNet-40 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 18:40:37 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Kadam", "Pranav", ""], ["Zhang", "Min", ""], ["Liu", "Shan", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2009.01315", "submitter": "Zixiang Zhao", "authors": "Zixiang Zhao, Jiangshe Zhang, Shuang Xu, Kai Sun, Chunxia Zhang,\n  Junmin Liu", "title": "When Image Decomposition Meets Deep Learning: A Novel Infrared and\n  Visible Image Fusion Method", "comments": "arXiv admin note: substantial text overlap with arXiv:2003.09210", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrared and visible image fusion, as a hot topic in image processing and\nimage enhancement, aims to produce fused images retaining the detail texture\ninformation in visible images and the thermal radiation information in infrared\nimages. A critical step for this issue is to decompose features in different\nscales and to merge them separately. In this paper, we propose a novel\ndual-stream auto-encoder (AE) based fusion network. The core idea is that the\nencoder decomposes an image into base and detail feature maps with low- and\nhigh-frequency information, respectively, and that the decoder is responsible\nfor the original image reconstruction. To this end, a well-designed loss\nfunction is established to make the base/detail feature maps\nsimilar/dissimilar. In the test phase, base and detail feature maps are\nrespectively merged via an additional fusion layer, which contains a saliency\nweighted-based spatial attention module and a channel attention module to\nadaptively preserve more information from source images and to highlight the\nobjects. Then the fused image is recovered by the decoder. Qualitative and\nquantitative results demonstrate that our method can generate fusion images\ncontaining highlighted targets and abundant detail texture information with\nstrong reproducibility and meanwhile is superior to the state-of-the-art (SOTA)\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 19:32:28 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 12:24:44 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Zhao", "Zixiang", ""], ["Zhang", "Jiangshe", ""], ["Xu", "Shuang", ""], ["Sun", "Kai", ""], ["Zhang", "Chunxia", ""], ["Liu", "Junmin", ""]]}, {"id": "2009.01328", "submitter": "Shuyue Guan", "authors": "Shuyue Guan, Murray Loew", "title": "An Internal Cluster Validity Index Using a Distance-based Separability\n  Measure", "comments": "8 pages, 4 figures. Accepted by IEEE ICTAI 2020 (Long Paper & Oral\n  Presentation)", "journal-ref": "IEEE 32nd International Conference on Tools with Artificial\n  Intelligence (ICTAI), 2020, pp. 827-834", "doi": "10.1109/ICTAI50040.2020.00131", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To evaluate clustering results is a significant part of cluster analysis.\nThere are no true class labels for clustering in typical unsupervised learning.\nThus, a number of internal evaluations, which use predicted labels and data,\nhave been created. They are also named internal cluster validity indices\n(CVIs). Without true labels, to design an effective CVI is not simple because\nit is similar to create a clustering method. And, to have more CVIs is crucial\nbecause there is no universal CVI that can be used to measure all datasets, and\nno specific method for selecting a proper CVI for clusters without true labels.\nTherefore, to apply more CVIs to evaluate clustering results is necessary. In\nthis paper, we propose a novel CVI - called Distance-based Separability Index\n(DSI), based on a data separability measure. We applied the DSI and eight other\ninternal CVIs including early studies from Dunn (1974) to most recent studies\nCVDD (2019) as comparison. We used an external CVI as ground truth for\nclustering results of five clustering algorithms on 12 real and 97 synthetic\ndatasets. Results show DSI is an effective, unique, and competitive CVI to\nother compared CVIs. In addition, we summarized the general process to evaluate\nCVIs and created a new method - rank difference - to compare the results of\nCVIs.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 20:20:29 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 21:22:03 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Guan", "Shuyue", ""], ["Loew", "Murray", ""]]}, {"id": "2009.01369", "submitter": "Ayman Mukhaimar Mr", "authors": "Ayman Mukhaimar, Ruwan Tennakoon, Chow Yin Lai, Reza Hoseinnezhad,\n  Alireza Bab-Hadiashar", "title": "Robust Object Classification Approach using Spherical Harmonics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a robust spherical harmonics approach for the\nclassification of point cloud-based objects. Spherical harmonics have been used\nfor classification over the years, with several frameworks existing in the\nliterature. These approaches use variety of spherical harmonics based\ndescriptors to classify objects. We first investigated these frameworks\nrobustness against data augmentation, such as outliers and noise, as it has not\nbeen studied before. Then we propose a spherical convolution neural network\nframework for robust object classification. The proposed framework uses the\nvoxel grid of concentric spheres to learn features over the unit ball. Our\nproposed model learn features that are less sensitive to data augmentation due\nto the selected sampling strategy and the designed convolution operation. We\ntested our proposed model against several types of data augmentation, such as\nnoise and outliers. Our results show that the proposed model outperforms the\nstate of art networks in terms of robustness to data augmentation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 22:29:06 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Mukhaimar", "Ayman", ""], ["Tennakoon", "Ruwan", ""], ["Lai", "Chow Yin", ""], ["Hoseinnezhad", "Reza", ""], ["Bab-Hadiashar", "Alireza", ""]]}, {"id": "2009.01371", "submitter": "Zhihong Pan", "authors": "Zhihong Pan, Baopu Li, Teng Xi, Yanwen Fan, Gang Zhang, Jingtuo Liu,\n  Junyu Han, Errui Ding", "title": "Real Image Super Resolution Via Heterogeneous Model Ensemble using\n  GP-NAS", "comments": "This is a manuscript related to our algorithm that won the ECCV AIM\n  2020 Real Image Super-Resolution Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  With advancement in deep neural network (DNN), recent state-of-the-art (SOTA)\nimage superresolution (SR) methods have achieved impressive performance using\ndeep residual network with dense skip connections. While these models perform\nwell on benchmark dataset where low-resolution (LR) images are constructed from\nhigh-resolution (HR) references with known blur kernel, real image SR is more\nchallenging when both images in the LR-HR pair are collected from real cameras.\nBased on existing dense residual networks, a Gaussian process based neural\narchitecture search (GP-NAS) scheme is utilized to find candidate network\narchitectures using a large search space by varying the number of dense\nresidual blocks, the block size and the number of features. A suite of\nheterogeneous models with diverse network structure and hyperparameter are\nselected for model-ensemble to achieve outstanding performance in real image\nSR. The proposed method won the first place in all three tracks of the AIM 2020\nReal Image Super-Resolution Challenge.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 22:33:23 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 18:48:30 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Pan", "Zhihong", ""], ["Li", "Baopu", ""], ["Xi", "Teng", ""], ["Fan", "Yanwen", ""], ["Zhang", "Gang", ""], ["Liu", "Jingtuo", ""], ["Han", "Junyu", ""], ["Ding", "Errui", ""]]}, {"id": "2009.01376", "submitter": "Xuejing Lei", "authors": "Xuejing Lei, Ganning Zhao, C.-C. Jay Kuo", "title": "NITES: A Non-Parametric Interpretable Texture Synthesis Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non-parametric interpretable texture synthesis method, called the NITES\nmethod, is proposed in this work. Although automatic synthesis of visually\npleasant texture can be achieved by deep neural networks nowadays, the\nassociated generation models are mathematically intractable and their training\ndemands higher computational cost. NITES offers a new texture synthesis\nsolution to address these shortcomings. NITES is mathematically transparent and\nefficient in training and inference. The input is a single exemplary texture\nimage. The NITES method crops out patches from the input and analyzes the\nstatistical properties of these texture patches to obtain their joint\nspatial-spectral representations. Then, the probabilistic distributions of\nsamples in the joint spatial-spectral spaces are characterized. Finally,\nnumerous texture images that are visually similar to the exemplary texture\nimage can be generated automatically. Experimental results are provided to show\nthe superior quality of generated texture images and efficiency of the proposed\nNITES method in terms of both training and inference time.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 22:52:44 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Lei", "Xuejing", ""], ["Zhao", "Ganning", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2009.01377", "submitter": "Felix Oliver Sumari Huayta", "authors": "Felix O. Sumari, Luigy Machaca, Jose Huaman, Esteban W. G. Clua, Joris\n  Gu\\'erin", "title": "Towards Practical Implementations of Person Re-Identification from Full\n  Video Frames", "comments": "7 pages, 9 figures, This paper is under consideration at Pattern\n  Recognition Letters", "journal-ref": null, "doi": "10.1016/j.patrec.2020.08.023", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the major adoption of automation for cities security, person\nre-identification (Re-ID) has been extensively studied recently. In this paper,\nwe argue that the current way of studying person re-identification, i.e. by\ntrying to re-identify a person within already detected and pre-cropped images\nof people, is not sufficient to implement practical security applications,\nwhere the inputs to the system are the full frames of the video streams. To\nsupport this claim, we introduce the Full Frame Person Re-ID setting (FF-PRID)\nand define specific metrics to evaluate FF-PRID implementations. To improve\nrobustness, we also formalize the hybrid human-machine collaboration framework,\nwhich is inherent to any Re-ID security applications. To demonstrate the\nimportance of considering the FF-PRID setting, we build an experiment showing\nthat combining a good people detection network with a good Re-ID model does not\nnecessarily produce good results for the final application. This underlines a\nfailure of the current formulation in assessing the quality of a Re-ID model\nand justifies the use of different metrics. We hope that this work will\nmotivate the research community to consider the full problem in order to\ndevelop algorithms that are better suited to real-world scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 22:53:46 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Sumari", "Felix O.", ""], ["Machaca", "Luigy", ""], ["Huaman", "Jose", ""], ["Clua", "Esteban W. G.", ""], ["Gu\u00e9rin", "Joris", ""]]}, {"id": "2009.01385", "submitter": "Zohreh Azizi", "authors": "Zohreh Azizi, Xuejing Lei, and C.-C Jay Kuo", "title": "Noise-Aware Texture-Preserving Low-Light Enhancement", "comments": "Accepted by IEEE VCIP 2020. The final version will appear in IEEE\n  VCIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple and effective low-light image enhancement method based on a\nnoise-aware texture-preserving retinex model is proposed in this work. The new\nmethod, called NATLE, attempts to strike a balance between noise removal and\nnatural texture preservation through a low-complexity solution. Its cost\nfunction includes an estimated piece-wise smooth illumination map and a\nnoise-free texture-preserving reflectance map. Afterwards, illumination is\nadjusted to form the enhanced image together with the reflectance map.\nExtensive experiments are conducted on common low-light image enhancement\ndatasets to demonstrate the superior performance of NATLE.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 23:30:03 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Azizi", "Zohreh", ""], ["Lei", "Xuejing", ""], ["Kuo", "C. -C Jay", ""]]}, {"id": "2009.01427", "submitter": "Yuan Fang", "authors": "Yuan Fang, Chunyan Xu, Zhen Cui, Yuan Zong, and Jian Yang", "title": "Spatial Transformer Point Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds are unstructured and unordered in the embedded 3D space. In\norder to produce consistent responses under different permutation layouts, most\nexisting methods aggregate local spatial points through maximum or summation\noperation. But such an aggregation essentially belongs to the isotropic\nfiltering on all operated points therein, which tends to lose the information\nof geometric structures. In this paper, we propose a spatial transformer point\nconvolution (STPC) method to achieve anisotropic convolution filtering on point\nclouds. To capture and represent implicit geometric structures, we specifically\nintroduce spatial direction dictionary to learn those latent geometric\ncomponents. To better encode unordered neighbor points, we design sparse\ndeformer to transform them into the canonical ordered dictionary space by using\ndirection dictionary learning. In the transformed space, the standard\nimage-like convolution can be leveraged to generate anisotropic filtering,\nwhich is more robust to express those finer variances of local regions.\nDictionary learning and encoding processes are encapsulated into a network\nmodule and jointly learnt in an end-to-end manner. Extensive experiments on\nseveral public datasets (including S3DIS, Semantic3D, SemanticKITTI)\ndemonstrate the effectiveness of our proposed method in point clouds semantic\nsegmentation task.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 03:12:25 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Fang", "Yuan", ""], ["Xu", "Chunyan", ""], ["Cui", "Zhen", ""], ["Zong", "Yuan", ""], ["Yang", "Jian", ""]]}, {"id": "2009.01438", "submitter": "Lei Zhang", "authors": "Lei Zhang and Zhenwei He and Yi Yang and Liang Wang and Xinbo Gao", "title": "Tasks Integrated Networks: Joint Detection and Retrieval for Image\n  Search", "comments": "To appear in IEEE TPAMI, 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional object retrieval task aims to learn a discriminative feature\nrepresentation with intra-similarity and inter-dissimilarity, which supposes\nthat the objects in an image are manually or automatically pre-cropped exactly.\nHowever, in many real-world searching scenarios (e.g., video surveillance), the\nobjects (e.g., persons, vehicles, etc.) are seldom accurately detected or\nannotated. Therefore, object-level retrieval becomes intractable without\nbounding-box annotation, which leads to a new but challenging topic, i.e.\nimage-level search. In this paper, to address the image search issue, we first\nintroduce an end-to-end Integrated Net (I-Net), which has three merits: 1) A\nSiamese architecture and an on-line pairing strategy for similar and dissimilar\nobjects in the given images are designed. 2) A novel on-line pairing (OLP) loss\nis introduced with a dynamic feature dictionary, which alleviates the\nmulti-task training stagnation problem, by automatically generating a number of\nnegative pairs to restrict the positives. 3) A hard example priority (HEP)\nbased softmax loss is proposed to improve the robustness of classification task\nby selecting hard categories. With the philosophy of divide and conquer, we\nfurther propose an improved I-Net, called DC-I-Net, which makes two new\ncontributions: 1) two modules are tailored to handle different tasks separately\nin the integrated framework, such that the task specification is guaranteed. 2)\nA class-center guided HEP loss (C2HEP) by exploiting the stored class centers\nis proposed, such that the intra-similarity and inter-dissimilarity can be\ncaptured for ultimate retrieval. Extensive experiments on famous image-level\nsearch oriented benchmark datasets demonstrate that the proposed DC-I-Net\noutperforms the state-of-the-art tasks-integrated and tasks-separated image\nsearch models.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 03:57:50 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Zhang", "Lei", ""], ["He", "Zhenwei", ""], ["Yang", "Yi", ""], ["Wang", "Liang", ""], ["Gao", "Xinbo", ""]]}, {"id": "2009.01439", "submitter": "Priyanka Mandikal", "authors": "Priyanka Mandikal, Kristen Grauman", "title": "Learning Dexterous Grasping with Object-Centric Visual Affordances", "comments": "Accepted at ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dexterous robotic hands are appealing for their agility and human-like\nmorphology, yet their high degree of freedom makes learning to manipulate\nchallenging. We introduce an approach for learning dexterous grasping. Our key\nidea is to embed an object-centric visual affordance model within a deep\nreinforcement learning loop to learn grasping policies that favor the same\nobject regions favored by people. Unlike traditional approaches that learn from\nhuman demonstration trajectories (e.g., hand joint sequences captured with a\nglove), the proposed prior is object-centric and image-based, allowing the\nagent to anticipate useful affordance regions for objects unseen during policy\nlearning. We demonstrate our idea with a 30-DoF five-fingered robotic hand\nsimulator on 40 objects from two datasets, where it successfully and\nefficiently learns policies for stable functional grasps. Our affordance-guided\npolicies are significantly more effective, generalize better to novel objects,\ntrain 3 X faster than the baselines, and are more robust to noisy sensor\nreadings and actuation. Our work offers a step towards manipulation agents that\nlearn by watching how people use objects, without requiring state and action\ninformation about the human body. Project website:\nhttp://vision.cs.utexas.edu/projects/graff-dexterous-affordance-grasp\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 04:00:40 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 22:28:15 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Mandikal", "Priyanka", ""], ["Grauman", "Kristen", ""]]}, {"id": "2009.01449", "submitter": "Long Chen", "authors": "Long Chen, Wenbo Ma, Jun Xiao, Hanwang Zhang, Shih-Fu Chang", "title": "Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression\n  Grounding", "comments": "Camera ready version at AAAI 2021, Codes are available at:\n  https://github.com/ChopinSharp/ref-nms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevailing framework for solving referring expression grounding is based\non a two-stage process: 1) detecting proposals with an object detector and 2)\ngrounding the referent to one of the proposals. Existing two-stage solutions\nmostly focus on the grounding step, which aims to align the expressions with\nthe proposals. In this paper, we argue that these methods overlook an obvious\nmismatch between the roles of proposals in the two stages: they generate\nproposals solely based on the detection confidence (i.e., expression-agnostic),\nhoping that the proposals contain all right instances in the expression (i.e.,\nexpression-aware). Due to this mismatch, current two-stage methods suffer from\na severe performance drop between detected and ground-truth proposals. To this\nend, we propose Ref-NMS, which is the first method to yield expression-aware\nproposals at the first stage. Ref-NMS regards all nouns in the expression as\ncritical objects, and introduces a lightweight module to predict a score for\naligning each box with a critical object. These scores can guide the NMS\noperation to filter out the boxes irrelevant to the expression, increasing the\nrecall of critical objects, resulting in a significantly improved grounding\nperformance. Since Ref- NMS is agnostic to the grounding step, it can be easily\nintegrated into any state-of-the-art two-stage method. Extensive ablation\nstudies on several backbones, benchmarks, and tasks consistently demonstrate\nthe superiority of Ref-NMS. Codes are available at:\nhttps://github.com/ChopinSharp/ref-nms.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 05:04:12 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 08:19:22 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 01:25:59 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Chen", "Long", ""], ["Ma", "Wenbo", ""], ["Xiao", "Jun", ""], ["Zhang", "Hanwang", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2009.01466", "submitter": "Da He", "authors": "Da He, Xiaoyu Shang, Jiajia Luo", "title": "Adherent Mist and Raindrop Removal from a Single Image Using Attentive\n  Convolutional Network", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Temperature difference-induced mist adhered to the glass, such as windshield,\ncamera lens, is often inhomogeneous and obscure, easily obstructing the vision\nand severely degrading the image. Together with adherent raindrops, they bring\nconsiderable challenges to various vision systems but without enough attention.\nRecent methods for other similar problems typically use hand-crafted priors to\ngenerate spatial attention maps. In this work, we newly present a problem of\nimage degradation caused by adherent mist and raindrops. An attentive\nconvolutional network is adopted to visually remove the adherent mist and\nraindrop from a single image. A baseline architecture with general channel-wise\nattention, spatial attention, and multilevel feature fusion is used.\nConsidering the variations and regional characteristics of adherent mist and\nraindrops, we apply interpolation-based pyramid-attention blocks to perceive\nspatial information at different scales. Experiments show that the proposed\nmethod can improve severely degraded images' visibility, both qualitatively and\nquantitatively. More applied experiments show that this underrated practical\nproblem is critical to high-level vision scenes. Our method also achieves\nstate-of-the-art performance on conventional dehazing and pure de-raindrop\nproblems, in addition to our task of handling adherent mist and raindrops.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 06:17:53 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 12:38:39 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["He", "Da", ""], ["Shang", "Xiaoyu", ""], ["Luo", "Jiajia", ""]]}, {"id": "2009.01468", "submitter": "Nicholas Wilkins", "authors": "Nicholas Wilkins, Beck Cordes Galbraith, Ifeoma Nwogu", "title": "Modeling Global Body Configurations in American Sign Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  American Sign Language (ASL) is the fourth most commonly used language in the\nUnited States and is the language most commonly used by Deaf people in the\nUnited States and the English-speaking regions of Canada. Unfortunately, until\nrecently, ASL received little research. This is due, in part, to its delayed\nrecognition as a language until William C. Stokoe's publication in 1960.\nLimited data has been a long-standing obstacle to ASL research and\ncomputational modeling. The lack of large-scale datasets has prohibited many\nmodern machine-learning techniques, such as Neural Machine Translation, from\nbeing applied to ASL. In addition, the modality required to capture sign\nlanguage (i.e. video) is complex in natural settings (as one must deal with\nbackground noise, motion blur, and the curse of dimensionality). Finally, when\ncompared with spoken languages, such as English, there has been limited\nresearch conducted into the linguistics of ASL.\n  We realize a simplified version of Liddell and Johnson's Movement-Hold (MH)\nModel using a Probabilistic Graphical Model (PGM). We trained our model on\nASLing, a dataset collected from three fluent ASL signers. We evaluate our PGM\nagainst other models to determine its ability to model ASL. Finally, we\ninterpret various aspects of the PGM and draw conclusions about ASL phonetics.\nThe main contributions of this paper are\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 06:20:10 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Wilkins", "Nicholas", ""], ["Galbraith", "Beck Cordes", ""], ["Nwogu", "Ifeoma", ""]]}, {"id": "2009.01469", "submitter": "Bin Chen", "authors": "Ruizhen Hu, Juzhan Xu, Bin Chen, Minglun Gong, Hao Zhang, Hui Huang", "title": "TAP-Net: Transport-and-Pack using Reinforcement Learning", "comments": null, "journal-ref": "ACM Transactions on Graphics 2020", "doi": "10.1145/3414685.3417796", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the transport-and-pack(TAP) problem, a frequently encountered\ninstance of real-world packing, and develop a neural optimization solution\nbased on reinforcement learning. Given an initial spatial configuration of\nboxes, we seek an efficient method to iteratively transport and pack the boxes\ncompactly into a target container. Due to obstruction and accessibility\nconstraints, our problem has to add a new search dimension, i.e., finding an\noptimal transport sequence, to the already immense search space for packing\nalone. Using a learning-based approach, a trained network can learn and encode\nsolution patterns to guide the solution of new problem instances instead of\nexecuting an expensive online search. In our work, we represent the transport\nconstraints using a precedence graph and train a neural network, coined\nTAP-Net, using reinforcement learning to reward efficient and stable packing.\nThe network is built on an encoder-decoder architecture, where the encoder\nemploys convolution layers to encode the box geometry and precedence graph and\nthe decoder is a recurrent neural network (RNN) which inputs the current\nencoder output, as well as the current box packing state of the target\ncontainer, and outputs the next box to pack, as well as its orientation. We\ntrain our network on randomly generated initial box configurations, without\nsupervision, via policy gradients to learn optimal TAP policies to maximize\npacking efficiency and stability. We demonstrate the performance of TAP-Net on\na variety of examples, evaluating the network through ablation studies and\ncomparisons to baselines and alternative network designs. We also show that our\nnetwork generalizes well to larger problem instances, when trained on\nsmall-sized inputs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 06:20:17 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Hu", "Ruizhen", ""], ["Xu", "Juzhan", ""], ["Chen", "Bin", ""], ["Gong", "Minglun", ""], ["Zhang", "Hao", ""], ["Huang", "Hui", ""]]}, {"id": "2009.01485", "submitter": "Pinkesh Badjatiya", "authors": "Surgan Jandial, Ayush Chopra, Pinkesh Badjatiya, Pranit Chawla,\n  Mausoom Sarkar, Balaji Krishnamurthy", "title": "TRACE: Transform Aggregate and Compose Visiolinguistic Representations\n  for Image Search with Text Feedback", "comments": "Surgan Jandial, Ayush Chopra and Pinkesh Badjatiya contributed\n  equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to efficiently search for images over an indexed database is the\ncornerstone for several user experiences. Incorporating user feedback, through\nmulti-modal inputs provide flexible and interaction to serve fine-grained\nspecificity in requirements. We specifically focus on text feedback, through\ndescriptive natural language queries. Given a reference image and textual user\nfeedback, our goal is to retrieve images that satisfy constraints specified by\nboth of these input modalities. The task is challenging as it requires\nunderstanding the textual semantics from the text feedback and then applying\nthese changes to the visual representation. To address these challenges, we\npropose a novel architecture TRACE which contains a hierarchical feature\naggregation module to learn the composite visio-linguistic representations.\nTRACE achieves the SOTA performance on 3 benchmark datasets: FashionIQ, Shoes,\nand Birds-to-Words, with an average improvement of at least ~5.7%, ~3%, and ~5%\nrespectively in R@K metric. Our extensive experiments and ablation studies show\nthat TRACE consistently outperforms the existing techniques by significant\nmargins both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 06:55:23 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Jandial", "Surgan", ""], ["Chopra", "Ayush", ""], ["Badjatiya", "Pinkesh", ""], ["Chawla", "Pranit", ""], ["Sarkar", "Mausoom", ""], ["Krishnamurthy", "Balaji", ""]]}, {"id": "2009.01512", "submitter": "Julien Tierny", "authors": "Harish Doraiswamy and Julien Tierny and Paulo J. S. Silva and Luis\n  Gustavo Nonato and Claudio Silva", "title": "TopoMap: A 0-dimensional Homology Preserving Projection of\n  High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional Projection is a fundamental tool for high-dimensional data\nanalytics and visualization. With very few exceptions, projection techniques\nare designed to map data from a high-dimensional space to a visual space so as\nto preserve some dissimilarity (similarity) measure, such as the Euclidean\ndistance for example. In fact, although adopting distinct mathematical\nformulations designed to favor different aspects of the data, most\nmultidimensional projection methods strive to preserve dissimilarity measures\nthat encapsulate geometric properties such as distances or the proximity\nrelation between data objects. However, geometric relations are not the only\ninteresting property to be preserved in a projection. For instance, the\nanalysis of particular structures such as clusters and outliers could be more\nreliably performed if the mapping process gives some guarantee as to\ntopological invariants such as connected components and loops. This paper\nintroduces TopoMap, a novel projection technique which provides topological\nguarantees during the mapping process. In particular, the proposed method\nperforms the mapping from a high-dimensional space to a visual space, while\npreserving the 0-dimensional persistence diagram of the Rips filtration of the\nhigh-dimensional data, ensuring that the filtrations generate the same\nconnected components when applied to the original as well as projected data.\nThe presented case studies show that the topological guarantee provided by\nTopoMap not only brings confidence to the visual analytic process but also can\nbe used to assist in the assessment of other projection methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 08:30:02 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Doraiswamy", "Harish", ""], ["Tierny", "Julien", ""], ["Silva", "Paulo J. S.", ""], ["Nonato", "Luis Gustavo", ""], ["Silva", "Claudio", ""]]}, {"id": "2009.01523", "submitter": "Yikuan Li", "authors": "Yikuan Li, Hanyin Wang and Yuan Luo", "title": "A Comparison of Pre-trained Vision-and-Language Models for Multimodal\n  Representation Learning across Medical Images and Reports", "comments": "10 pages, 3 figures, submitted to BIBM2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Joint image-text embedding extracted from medical images and associated\ncontextual reports is the bedrock for most biomedical vision-and-language (V+L)\ntasks, including medical visual question answering, clinical image-text\nretrieval, clinical report auto-generation. In this study, we adopt four\npre-trained V+L models: LXMERT, VisualBERT, UNIER and PixelBERT to learn\nmultimodal representation from MIMIC-CXR radiographs and associated reports.\nThe extrinsic evaluation on OpenI dataset shows that in comparison to the\npioneering CNN-RNN model, the joint embedding learned by pre-trained V+L models\ndemonstrate performance improvement in the thoracic findings classification\ntask. We conduct an ablation study to analyze the contribution of certain model\ncomponents and validate the advantage of joint embedding over text-only\nembedding. We also visualize attention maps to illustrate the attention\nmechanism of V+L models.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 09:00:47 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Li", "Yikuan", ""], ["Wang", "Hanyin", ""], ["Luo", "Yuan", ""]]}, {"id": "2009.01540", "submitter": "Anil Baslamisli", "authors": "Anil S. Baslamisli and Yang Liu and Sezer Karaoglu and Theo Gevers", "title": "Physics-based Shading Reconstruction for Intrinsic Image Decomposition", "comments": "Submitted to Computer Vision and Image Understanding (CVIU)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of photometric invariance and deep learning to compute\nintrinsic images (albedo and shading). We propose albedo and shading gradient\ndescriptors which are derived from physics-based models. Using the descriptors,\nalbedo transitions are masked out and an initial sparse shading map is\ncalculated directly from the corresponding RGB image gradients in a\nlearning-free unsupervised manner. Then, an optimization method is proposed to\nreconstruct the full dense shading map. Finally, we integrate the generated\nshading map into a novel deep learning framework to refine it and also to\npredict corresponding albedo image to achieve intrinsic image decomposition. By\ndoing so, we are the first to directly address the texture and intensity\nambiguity problems of the shading estimations. Large scale experiments show\nthat our approach steered by physics-based invariant descriptors achieve\nsuperior results on MIT Intrinsics, NIR-RGB Intrinsics, Multi-Illuminant\nIntrinsic Images, Spectral Intrinsic Images, As Realistic As Possible, and\ncompetitive results on Intrinsic Images in the Wild datasets while achieving\nstate-of-the-art shading estimations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 09:30:17 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Baslamisli", "Anil S.", ""], ["Liu", "Yang", ""], ["Karaoglu", "Sezer", ""], ["Gevers", "Theo", ""]]}, {"id": "2009.01548", "submitter": "Sharath M. Shankaranarayana Mr", "authors": "Sharath M Shankaranarayana", "title": "Fundus Image Analysis for Age Related Macular Degeneration: ADAM-2020\n  Challenge Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age related macular degeneration (AMD) is one of the major causes for\nblindness in the elderly population. In this report, we propose deep learning\nbased methods for retinal analysis using color fundus images for computer aided\ndiagnosis of AMD. We leverage the recent state of the art deep networks for\nbuilding a single fundus image based AMD classification pipeline. We also\npropose methods for the other directly relevant and auxiliary tasks such as\nlesions detection and segmentation, fovea detection and optic disc\nsegmentation. We propose the use of generative adversarial networks (GANs) for\nthe tasks of segmentation and detection. We also propose a novel method of\nfovea detection using GANs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 09:46:32 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Shankaranarayana", "Sharath M", ""]]}, {"id": "2009.01559", "submitter": "Jingru Tan", "authors": "Jingru Tan, Gang Zhang, Hanming Deng, Changbao Wang, Lewei Lu,\n  Quanquan Li, Jifeng Dai", "title": "1st Place Solution of LVIS Challenge 2020: A Good Box is not a Guarantee\n  of a Good Mask", "comments": "Winner of LVIS challenge 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces the solutions of the team lvisTraveler for LVIS\nChallenge 2020. In this work, two characteristics of LVIS dataset are mainly\nconsidered: the long-tailed distribution and high quality instance segmentation\nmask. We adopt a two-stage training pipeline. In the first stage, we\nincorporate EQL and self-training to learn generalized representation. In the\nsecond stage, we utilize Balanced GroupSoftmax to promote the classifier, and\npropose a novel proposal assignment strategy and a new balanced mask loss for\nmask head to get more precise mask predictions. Finally, we achieve 41.5 and\n41.2 AP on LVIS v1.0 val and test-dev splits respectively, outperforming the\nbaseline based on X101-FPN-MaskRCNN by a large margin.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 10:09:44 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Tan", "Jingru", ""], ["Zhang", "Gang", ""], ["Deng", "Hanming", ""], ["Wang", "Changbao", ""], ["Lu", "Lewei", ""], ["Li", "Quanquan", ""], ["Dai", "Jifeng", ""]]}, {"id": "2009.01565", "submitter": "Boseong Jeon", "authors": "Boseong Felipe Jeon, Dongseok Shim and H. Jin Kim", "title": "Detection-Aware Trajectory Generation for a Drone Cinematographer", "comments": "8 pages, IROS 2020 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates an efficient trajectory generation for chasing a\ndynamic target, which incorporates the detectability objective. The proposed\nmethod actively guides the motion of a cinematographer drone so that the color\nof a target is well-distinguished against the colors of the background in the\nview of the drone. For the objective, we define a measure of color\ndetectability given a chasing path. After computing a discrete path optimized\nfor the metric, we generate a dynamically feasible trajectory. The whole\npipeline can be updated on-the-fly to respond to the motion of the target. For\nthe efficient discrete path generation, we construct a directed acyclic graph\n(DAG) for which a topological sorting can be determined analytically without\nthe depth-first search. The smooth path is obtained in quadratic programming\n(QP) framework. We validate the enhanced performance of state-of-the-art object\ndetection and tracking algorithms when the camera drone executes the trajectory\nobtained from the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 10:27:56 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Jeon", "Boseong Felipe", ""], ["Shim", "Dongseok", ""], ["Kim", "H. Jin", ""]]}, {"id": "2009.01573", "submitter": "Vasco Lopes Ferrinho", "authors": "Vasco Lopes, Lu\\'is A. Alexandre", "title": "Auto-Classifier: A Robust Defect Detector Based on an AutoML Head", "comments": "12 pages, 2 figures. Published in ICONIP2020, proceedings published\n  in the Springer's series of Lecture Notes in Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant approach for surface defect detection is the use of hand-crafted\nfeature-based methods. However, this falls short when conditions vary that\naffect extracted images. So, in this paper, we sought to determine how well\nseveral state-of-the-art Convolutional Neural Networks perform in the task of\nsurface defect detection. Moreover, we propose two methods: CNN-Fusion, that\nfuses the prediction of all the networks into a final one, and Auto-Classifier,\nwhich is a novel proposal that improves a Convolutional Neural Network by\nmodifying its classification component using AutoML. We carried out experiments\nto evaluate the proposed methods in the task of surface defect detection using\ndifferent datasets from DAGM2007. We show that the use of Convolutional Neural\nNetworks achieves better results than traditional methods, and also, that\nAuto-Classifier out-performs all other methods, by achieving 100% accuracy and\n100% AUC results throughout all the datasets.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 10:39:02 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Lopes", "Vasco", ""], ["Alexandre", "Lu\u00eds A.", ""]]}, {"id": "2009.01579", "submitter": "Adrian Lopez Rodriguez", "authors": "Adrian Lopez-Rodriguez, Krystian Mikolajczyk", "title": "DESC: Domain Adaptation for Depth Estimation via Semantic Consistency", "comments": "BMVC20 (Oral). Code: https://github.com/alopezgit/DESC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate real depth annotations are difficult to acquire, needing the use of\nspecial devices such as a LiDAR sensor. Self-supervised methods try to overcome\nthis problem by processing video or stereo sequences, which may not always be\navailable. Instead, in this paper, we propose a domain adaptation approach to\ntrain a monocular depth estimation model using a fully-annotated source dataset\nand a non-annotated target dataset. We bridge the domain gap by leveraging\nsemantic predictions and low-level edge features to provide guidance for the\ntarget domain. We enforce consistency between the main model and a second model\ntrained with semantic segmentation and edge maps, and introduce priors in the\nform of instance heights. Our approach is evaluated on standard domain\nadaptation benchmarks for monocular depth estimation and show consistent\nimprovement upon the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 10:54:05 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Lopez-Rodriguez", "Adrian", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "2009.01588", "submitter": "Duy Thanh Nguyen", "authors": "Duy Thanh Nguyen, Hyun Kim, and Hyuk-Jae Lee", "title": "Layer-specific Optimization for Mixed Data Flow with Mixed Precision in\n  FPGA Design for CNN-based Object Detectors", "comments": "Accepted for publication in IEEE Transaction on Circuit and System\n  for Video Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2020.3020569", "report-no": null, "categories": "cs.CV cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) require both intensive computation and\nfrequent memory access, which lead to a low processing speed and large power\ndissipation. Although the characteristics of the different layers in a CNN are\nfrequently quite different, previous hardware designs have employed common\noptimization schemes for them. This paper proposes a layer-specific design that\nemploys different organizations that are optimized for the different layers.\nThe proposed design employs two layer-specific optimizations: layer-specific\nmixed data flow and layer-specific mixed precision. The mixed data flow aims to\nminimize the off-chip access while demanding a minimal on-chip memory (BRAM)\nresource of an FPGA device. The mixed precision quantization is to achieve both\na lossless accuracy and an aggressive model compression, thereby further\nreducing the off-chip access. A Bayesian optimization approach is used to\nselect the best sparsity for each layer, achieving the best trade-off between\nthe accuracy and compression. This mixing scheme allows the entire network\nmodel to be stored in BRAMs of the FPGA to aggressively reduce the off-chip\naccess, and thereby achieves a significant performance enhancement. The model\nsize is reduced by 22.66-28.93 times compared to that in a full-precision\nnetwork with a negligible degradation of accuracy on VOC, COCO, and ImageNet\ndatasets. Furthermore, the combination of mixed dataflow and mixed precision\nsignificantly outperforms the previous works in terms of both throughput,\noff-chip access, and on-chip memory requirement.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 11:27:40 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Nguyen", "Duy Thanh", ""], ["Kim", "Hyun", ""], ["Lee", "Hyuk-Jae", ""]]}, {"id": "2009.01592", "submitter": "Marvin Lerousseau", "authors": "Marvin Lerousseau, Eric Deutsh, Nikos Paragios", "title": "Multimodal brain tumor classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cancer is a complex disease that provides various types of information\ndepending on the scale of observation. While most tumor diagnostics are\nperformed by observing histopathological slides, radiology images should yield\nadditional knowledge towards the efficacy of cancer diagnostics. This work\ninvestigates a deep learning method combining whole slide images and magnetic\nresonance images to classify tumors. In particular, our solution comprises a\npowerful, generic and modular architecture for whole slide image\nclassification. Experiments are prospectively conducted on the 2020\nComputational Precision Medicine challenge, in a 3-classes unbalanced\nclassification task. We report cross-validation (resp. validation)\nbalanced-accuracy, kappa and f1 of 0.913, 0.897 and 0.951 (resp. 0.91, 0.90 and\n0.94). For research purposes, including reproducibility and direct performance\ncomparisons, our finale submitted models are usable off-the-shelf in a Docker\nimage available at\nhttps://hub.docker.com/repository/docker/marvinler/cpm_2020_marvinler.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 11:41:50 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 16:05:22 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Lerousseau", "Marvin", ""], ["Deutsh", "Eric", ""], ["Paragios", "Nikos", ""]]}, {"id": "2009.01599", "submitter": "Qinghui Liu", "authors": "Qinghui Liu, Michael Kampffmeyer, Robert Jenssen, Arnt-B{\\o}rre\n  Salberg", "title": "SCG-Net: Self-Constructing Graph Neural Networks for Semantic\n  Segmentation", "comments": "11 pages, 5 figs. code will be open soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing global contextual representations by exploiting long-range\npixel-pixel dependencies has shown to improve semantic segmentation\nperformance. However, how to do this efficiently is an open question as current\napproaches of utilising attention schemes or very deep models to increase the\nmodels field of view, result in complex models with large memory consumption.\nInspired by recent work on graph neural networks, we propose the\nSelf-Constructing Graph (SCG) module that learns a long-range dependency graph\ndirectly from the image and uses it to propagate contextual information\nefficiently to improve semantic segmentation. The module is optimised via a\nnovel adaptive diagonal enhancement method and a variational lower bound that\nconsists of a customized graph reconstruction term and a Kullback-Leibler\ndivergence regularization term. When incorporated into a neural network\n(SCG-Net), semantic segmentation is performed in an end-to-end manner and\ncompetitive performance (mean F1-scores of 92.0% and 89.8% respectively) on the\npublicly available ISPRS Potsdam and Vaihingen datasets is achieved, with much\nfewer parameters, and at a lower computational cost compared to related pure\nconvolutional neural network (CNN) based models.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 12:13:09 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2021 14:59:24 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Liu", "Qinghui", ""], ["Kampffmeyer", "Michael", ""], ["Jenssen", "Robert", ""], ["Salberg", "Arnt-B\u00f8rre", ""]]}, {"id": "2009.01601", "submitter": "Peyman Tahghighi", "authors": "Peyman Tahghighi, Reza A.Zoroofi, Sare Safi and Alireza Ramezani", "title": "Heightmap Reconstruction of Macula on Color Fundus Images Using\n  Conditional Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  For screening, 3D shape of the eye retina often provides structural\ninformation and can assist ophthalmologists to diagnose diseases. However,\nfundus images which are one the most common screening modalities for retina\ndiagnosis lack this information due to their 2D nature. Hence, in this work, we\ntry to infer about this 3D information or more specifically its heights. Recent\napproaches have used shading information for reconstructing the heights but\ntheir output is not accurate since the utilized information is not sufficient.\nAdditionally, other methods were dependent on the availability of more than one\nimage of the eye which is not available in practice. In this paper, motivated\nby the success of Conditional Generative Adversarial Networks(cGANs) and deeply\nsupervised networks, we propose a novel architecture for the generator which\nenhances the details in a sequence of steps. Comparisons on our dataset\nillustrate that the proposed method outperforms all of the state-of-the-art\nmethods in image translation and medical image translation on this particular\ntask. Additionally, clinical studies also indicate that the proposed method can\nprovide additional information for ophthalmologists for diagnosis.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 12:13:51 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2020 16:59:45 GMT"}, {"version": "v3", "created": "Sat, 31 Oct 2020 08:55:14 GMT"}, {"version": "v4", "created": "Mon, 28 Dec 2020 07:27:35 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Tahghighi", "Peyman", ""], ["Zoroofi", "Reza A.", ""], ["Safi", "Sare", ""], ["Ramezani", "Alireza", ""]]}, {"id": "2009.01616", "submitter": "Zixuan Xiao", "authors": "Zixuan Xiao, Ping Zhong, Yuan Quan, Xuping Yin, Wei Xue", "title": "Few-shot Object Detection with Feature Attention Highlight Module in\n  Remote Sensing Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there are many applications of object detection in remote\nsensing field, which demands a great number of labeled data. However, in many\ncases, data is extremely rare. In this paper, we proposed a few-shot object\ndetector which is designed for detecting novel objects based on only a few\nexamples. Through fully leveraging labeled base classes, our model that is\ncomposed of a feature-extractor, a feature attention highlight module as well\nas a two-stage detection backend can quickly adapt to novel classes. The\npre-trained feature extractor whose parameters are shared produces general\nfeatures. While the feature attention highlight module is designed to be\nlight-weighted and simple in order to fit the few-shot cases. Although it is\nsimple, the information provided by it in a serial way is helpful to make the\ngeneral features to be specific for few-shot objects. Then the object-specific\nfeatures are delivered to the two-stage detection backend for the detection\nresults. The experiments demonstrate the effectiveness of the proposed method\nfor few-shot cases.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 12:38:49 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Xiao", "Zixuan", ""], ["Zhong", "Ping", ""], ["Quan", "Yuan", ""], ["Yin", "Xuping", ""], ["Xue", "Wei", ""]]}, {"id": "2009.01617", "submitter": "Menua Gevorgyan", "authors": "Menua Gevorgyan", "title": "Modification method for single-stage object detectors that allows to\n  exploit the temporal behaviour of a scene to improve detection accuracy", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple modification method for single-stage generic object detection neural\nnetworks, such as YOLO and SSD, is proposed, which allows for improving the\ndetection accuracy on video data by exploiting the temporal behavior of the\nscene in the detection pipeline. It is shown that, using this method, the\ndetection accuracy of the base network can be considerably improved, especially\nfor occluded and hidden objects. It is shown that a modified network is more\nprone to detect hidden objects with more confidence than an unmodified one. A\nweakly supervised training method is proposed, which allows for training a\nmodified network without requiring any additional annotated data.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 12:38:55 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Gevorgyan", "Menua", ""]]}, {"id": "2009.01636", "submitter": "Marin Or\\v{s}i\\'c", "authors": "Petra Bevandi\\'c, Marin Or\\v{s}i\\'c, Ivan Grubi\\v{s}i\\'c, Josip\n  \\v{S}ari\\'c, and Sini\\v{s}a \\v{S}egvi\\'c", "title": "Multi-domain Semantic Segmentation on Datasets with Overlapping Classes", "comments": "12 pages, 3 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep supervised models have an unprecedented capacity to apsorb large\nquantities of training data. Hence, training on all available datasets appears\nas a feasible approach towards accurate semantic segmentation models with\ngraceful degradation in unusual scenes. Unfortunately, different datasets often\nuse incompatible labels. For instance, the Cityscapes road class subsumes all\npixels on driving surfaces, while Vistas defines separate classes for road\nmarkings, zebra crossings etc. Such inconsistencies pose a major obstacle\ntowards successful multi-domain learning. We address this challenge by\nproposing a principled technique for learning with incompatible labeling\npolicies. Different than recent related work, our technique allows seamless\ntraining on datasets with overlapping classes. Consequently, it can learn\nvisual concepts which are not represented as a separate class in any of the\nindividual datasets. We evaluate our method on a collection of seven semantic\nsegmentation datasets across four different domains. The results exceed the\nstate of the art in multi-domain semantic segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 08:37:14 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 12:06:23 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 07:30:29 GMT"}, {"version": "v4", "created": "Wed, 20 Jan 2021 11:29:47 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Bevandi\u0107", "Petra", ""], ["Or\u0161i\u0107", "Marin", ""], ["Grubi\u0161i\u0107", "Ivan", ""], ["\u0160ari\u0107", "Josip", ""], ["\u0160egvi\u0107", "Sini\u0161a", ""]]}, {"id": "2009.01660", "submitter": "Pradeep Singh Dr", "authors": "Akanksha Baghel, Meemansa Rathod, Pradeep Singh", "title": "Software Effort Estimation using parameter tuned Models", "comments": "Nine Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software estimation is one of the most important activities in the software\nproject. The software effort estimation is required in the early stages of\nsoftware life cycle. Project Failure is the major problem undergoing nowadays\nas seen by software project managers. The imprecision of the estimation is the\nreason for this problem. Assize of software size grows, it also makes a system\ncomplex, thus difficult to accurately predict the cost of software development\nprocess. The greatest pitfall of the software industry was the fast-changing\nnature of software development which has made it difficult to develop\nparametric models that yield high accuracy for software development in all\ndomains. We need the development of useful models that accurately predict the\ncost of developing a software product. This study presents the novel analysis\nof various regression models with hyperparameter tuning to get the effective\nmodel. Nine different regression techniques are considered for model\ndevelopment\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 15:18:59 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Baghel", "Akanksha", ""], ["Rathod", "Meemansa", ""], ["Singh", "Pradeep", ""]]}, {"id": "2009.01689", "submitter": "Sukhendu Das PhD", "authors": "Jasmeen Kaur, Sukhendu Das", "title": "Future Frame Prediction of a Video Sequence", "comments": "Acknowledgement: the contributions, support, and help of Sonam Gupta,\n  PhD Scholar, VPLAB, Deptt. of CS&E, IIT Madras", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting future frames of a video sequence has been a problem of high\ninterest in the field of Computer Vision as it caters to a multitude of\napplications. The ability to predict, anticipate and reason about future events\nis the essence of intelligence and one of the main goals of decision-making\nsystems such as human-machine interaction, robot navigation and autonomous\ndriving. However, the challenge lies in the ambiguous nature of the problem as\nthere may be multiple future sequences possible for the same input video shot.\nA naively designed model averages multiple possible futures into a single\nblurry prediction.\n  Recently, two distinct approaches have attempted to address this problem as:\n(a) use of latent variable models that represent underlying stochasticity and\n(b) adversarially trained models that aim to produce sharper images. A latent\nvariable model often struggles to produce realistic results, while an\nadversarially trained model underutilizes latent variables and thus fails to\nproduce diverse predictions. These methods have revealed complementary\nstrengths and weaknesses. Combining the two approaches produces predictions\nthat appear more realistic and better cover the range of plausible futures.\nThis forms the basis and objective of study in this project work.\n  In this paper, we proposed a novel multi-scale architecture combining both\napproaches. We validate our proposed model through a series of experiments and\nempirical evaluations on Moving MNIST, UCF101, and Penn Action datasets. Our\nmethod outperforms the results obtained using the baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 15:31:02 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Kaur", "Jasmeen", ""], ["Das", "Sukhendu", ""]]}, {"id": "2009.01717", "submitter": "Rick Groenendijk", "authors": "Rick Groenendijk, Sezer Karaoglu, Theo Gevers, Thomas Mensink", "title": "Multi-Loss Weighting with Coefficient of Variations", "comments": "Paper was accepted at the IEEE Winter Conference on Applications of\n  Computer Vision 2021 (WACV2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many interesting tasks in machine learning and computer vision are learned by\noptimising an objective function defined as a weighted linear combination of\nmultiple losses. The final performance is sensitive to choosing the correct\n(relative) weights for these losses. Finding a good set of weights is often\ndone by adopting them into the set of hyper-parameters, which are set using an\nextensive grid search. This is computationally expensive. In this paper, we\npropose a weighting scheme based on the coefficient of variations and set the\nweights based on properties observed while training the model. The proposed\nmethod incorporates a measure of uncertainty to balance the losses, and as a\nresult the loss weights evolve during training without requiring another\n(learning based) optimisation. In contrast to many loss weighting methods in\nliterature, we focus on single-task multi-loss problems, such as monocular\ndepth estimation and semantic segmentation, and show that multi-task approaches\nfor loss weighting do not work on those single-tasks. The validity of the\napproach is shown empirically for depth estimation and semantic segmentation on\nmultiple datasets.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 14:51:19 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 10:41:03 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Groenendijk", "Rick", ""], ["Karaoglu", "Sezer", ""], ["Gevers", "Theo", ""], ["Mensink", "Thomas", ""]]}, {"id": "2009.01729", "submitter": "Haoyu Zhang", "authors": "Haoyu Zhang, Sushma Venkatesh, Raghavendra Ramachandra, Kiran Raja,\n  Naser Damer, Christoph Busch", "title": "MIPGAN -- Generating Strong and High Quality Morphing Attacks Using\n  Identity Prior Driven GAN", "comments": "Revised version. Submitted to IEEE T-BIOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face morphing attacks target to circumvent Face Recognition Systems (FRS) by\nemploying face images derived from multiple data subjects (e.g., accomplices\nand malicious actors). Morphed images can be verified against contributing data\nsubjects with a reasonable success rate, given they have a high degree of\nfacial resemblance. The success of morphing attacks is directly dependent on\nthe quality of the generated morph images. We present a new approach for\ngenerating strong attacks extending our earlier framework for generating face\nmorphs. We present a new approach using an Identity Prior Driven Generative\nAdversarial Network, which we refer to as MIPGAN (Morphing through Identity\nPrior driven GAN). The proposed MIPGAN is derived from the StyleGAN with a\nnewly formulated loss function exploiting perceptual quality and identity\nfactor to generate a high quality morphed facial image with minimal artefacts\nand with high resolution. We demonstrate the proposed approach's applicability\nto generate strong morphing attacks by evaluating its vulnerability against\nboth commercial and deep learning based Face Recognition System (FRS) and\ndemonstrate the success rate of attacks. Extensive experiments are carried out\nto assess the FRS's vulnerability against the proposed morphed face generation\ntechnique on three types of data such as digital images, re-digitized (printed\nand scanned) images, and compressed images after re-digitization from newly\ngenerated MIPGAN Face Morph Dataset. The obtained results demonstrate that the\nproposed approach of morph generation poses a high threat to FRS.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 15:08:38 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 08:47:41 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 11:02:03 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Zhang", "Haoyu", ""], ["Venkatesh", "Sushma", ""], ["Ramachandra", "Raghavendra", ""], ["Raja", "Kiran", ""], ["Damer", "Naser", ""], ["Busch", "Christoph", ""]]}, {"id": "2009.01766", "submitter": "Wu Weijia", "authors": "Weijia Wu and Ning Lu and Enze Xie", "title": "Synthetic-to-Real Unsupervised Domain Adaptation for Scene Text\n  Detection in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning-based scene text detection can achieve preferable performance,\npowered with sufficient labeled training data. However, manual labeling is time\nconsuming and laborious. At the extreme, the corresponding annotated data are\nunavailable. Exploiting synthetic data is a very promising solution except for\ndomain distribution mismatches between synthetic datasets and real datasets. To\naddress the severe domain distribution mismatch, we propose a synthetic-to-real\ndomain adaptation method for scene text detection, which transfers knowledge\nfrom synthetic data (source domain) to real data (target domain). In this\npaper, a text self-training (TST) method and adversarial text instance\nalignment (ATA) for domain adaptive scene text detection are introduced. ATA\nhelps the network learn domain-invariant features by training a domain\nclassifier in an adversarial manner. TST diminishes the adverse effects of\nfalse positives~(FPs) and false negatives~(FNs) from inaccurate pseudo-labels.\nTwo components have positive effects on improving the performance of scene text\ndetectors when adapting from synthetic-to-real scenes. We evaluate the proposed\nmethod by transferring from SynthText, VISD to ICDAR2015, ICDAR2013. The\nresults demonstrate the effectiveness of the proposed method with up to 10%\nimprovement, which has important exploration significance for domain adaptive\nscene text detection. Code is available at\nhttps://github.com/weijiawu/SyntoReal_STD\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 16:16:34 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Wu", "Weijia", ""], ["Lu", "Ning", ""], ["Xie", "Enze", ""]]}, {"id": "2009.01782", "submitter": "Bo Zhou", "authors": "Bo Zhou, S. Kevin Zhou, James S. Duncan, Chi Liu", "title": "Limited View Tomographic Reconstruction Using a Deep Recurrent Framework\n  with Residual Dense Spatial-Channel Attention Network and Sinogram\n  Consistency", "comments": "Submitted to the IEEE for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Limited view tomographic reconstruction aims to reconstruct a tomographic\nimage from a limited number of sinogram or projection views arising from sparse\nview or limited angle acquisitions that reduce radiation dose or shorten\nscanning time. However, such a reconstruction suffers from high noise and\nsevere artifacts due to the incompleteness of sinogram. To derive quality\nreconstruction, previous state-of-the-art methods use UNet-like neural\narchitectures to directly predict the full view reconstruction from limited\nview data; but these methods leave the deep network architecture issue largely\nintact and cannot guarantee the consistency between the sinogram of the\nreconstructed image and the acquired sinogram, leading to a non-ideal\nreconstruction. In this work, we propose a novel recurrent reconstruction\nframework that stacks the same block multiple times. The recurrent block\nconsists of a custom-designed residual dense spatial-channel attention network.\nFurther, we develop a sinogram consistency layer interleaved in our recurrent\nframework in order to ensure that the sampled sinogram is consistent with the\nsinogram of the intermediate outputs of the recurrent blocks. We evaluate our\nmethods on two datasets. Our experimental results on AAPM Low Dose CT Grand\nChallenge datasets demonstrate that our algorithm achieves a consistent and\nsignificant improvement over the existing state-of-the-art neural methods on\nboth limited angle reconstruction (over 5dB better in terms of PSNR) and sparse\nview reconstruction (about 4dB better in term of PSNR). In addition, our\nexperimental results on Deep Lesion datasets demonstrate that our method is\nable to generate high-quality reconstruction for 8 major lesion types.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 16:39:48 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Zhou", "Bo", ""], ["Zhou", "S. Kevin", ""], ["Duncan", "James S.", ""], ["Liu", "Chi", ""]]}, {"id": "2009.01786", "submitter": "Stefan Schonsheck", "authors": "Stefan C Schonsheck", "title": "Computational Analysis of Deformable Manifolds: from Geometric Modelling\n  to Deep Learning", "comments": "PhD Thesis, Versions of several chapters have previously appeard or\n  been submitted under different titles", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leo Tolstoy opened his monumental novel Anna Karenina with the now famous\nwords: Happy families are all alike; every unhappy family is unhappy in its own\nway A similar notion also applies to mathematical spaces: Every flat space is\nalike; every unflat space is unflat in its own way. However, rather than being\na source of unhappiness, we will show that the diversity of non-flat spaces\nprovides a rich area of study. The genesis of the so-called big data era and\nthe proliferation of social and scientific databases of increasing size has led\nto a need for algorithms that can efficiently process, analyze and, even\ngenerate high dimensional data. However, the curse of dimensionality leads to\nthe fact that many classical approaches do not scale well with respect to the\nsize of these problems. One technique to avoid some of these ill-effects is to\nexploit the geometric structure of coherent data. In this thesis, we will\nexplore geometric methods for shape processing and data analysis. More\nspecifically, we will study techniques for representing manifolds and signals\nsupported on them through a variety of mathematical tools including, but not\nlimited to, computational differential geometry, variational PDE modeling, and\ndeep learning. First, we will explore non-isometric shape matching through\nvariational modeling. Next, we will use ideas from parallel transport on\nmanifolds to generalize convolution and convolutional neural networks to\ndeformable manifolds. Finally, we conclude by proposing a novel auto-regressive\nmodel for capturing the intrinsic geometry and topology of data. Throughout\nthis work, we will use the idea of computing correspondences as a though-line\nto both motivate our work and analyze our results.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 16:50:48 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Schonsheck", "Stefan C", ""]]}, {"id": "2009.01816", "submitter": "Dimitris Perdios", "authors": "Dimitris Perdios, Manuel Vonlanthen, Florian Martinez, Marcel Arditi,\n  Jean-Philippe Thiran", "title": "CNN-Based Ultrasound Image Reconstruction for Ultrafast Displacement\n  Tracking", "comments": "11 pages, 4 figures. Animation and slideshow of Figure 4 are provided\n  as ancillary files. This version has been accepted for publication in the\n  IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2020.3046700", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to its capability of acquiring full-view frames at multiple kilohertz,\nultrafast ultrasound imaging unlocked the analysis of rapidly changing physical\nphenomena in the human body, with pioneering applications such as\nultrasensitive flow imaging in the cardiovascular system or shear-wave\nelastography. The accuracy achievable with these motion estimation techniques\nis strongly contingent upon two contradictory requirements: a high quality of\nconsecutive frames and a high frame rate. Indeed, the image quality can usually\nbe improved by increasing the number of steered ultrafast acquisitions, but at\nthe expense of a reduced frame rate and possible motion artifacts. To achieve\naccurate motion estimation at uncompromised frame rates and immune to motion\nartifacts, the proposed approach relies on single ultrafast acquisitions to\nreconstruct high-quality frames and on only two consecutive frames to obtain\n2-D displacement estimates. To this end, we deployed a convolutional neural\nnetwork-based image reconstruction method combined with a speckle tracking\nalgorithm based on cross-correlation. Numerical and in vivo experiments,\nconducted in the context of plane-wave imaging, demonstrate that the proposed\napproach is capable of estimating displacements in regions where the presence\nof side lobe and grating lobe artifacts prevents any displacement estimation\nwith a state-of-the-art technique that relies on conventional delay-and-sum\nbeamforming. The proposed approach may therefore unlock the full potential of\nultrafast ultrasound, in applications such as ultrasensitive cardiovascular\nmotion and flow analysis or shear-wave elastography.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 17:31:44 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 17:56:40 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Perdios", "Dimitris", ""], ["Vonlanthen", "Manuel", ""], ["Martinez", "Florian", ""], ["Arditi", "Marcel", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "2009.01835", "submitter": "Chen Gao", "authors": "Chen Gao, Ayush Saraf, Jia-Bin Huang, Johannes Kopf", "title": "Flow-edge Guided Video Completion", "comments": "ECCV 2020. Project: http://chengao.vision/FGVC/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new flow-based video completion algorithm. Previous flow\ncompletion methods are often unable to retain the sharpness of motion\nboundaries. Our method first extracts and completes motion edges, and then uses\nthem to guide piecewise-smooth flow completion with sharp edges. Existing\nmethods propagate colors among local flow connections between adjacent frames.\nHowever, not all missing regions in a video can be reached in this way because\nthe motion boundaries form impenetrable barriers. Our method alleviates this\nproblem by introducing non-local flow connections to temporally distant frames,\nenabling propagating video content over motion boundaries. We validate our\napproach on the DAVIS dataset. Both visual and quantitative results show that\nour method compares favorably against the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 17:59:42 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Gao", "Chen", ""], ["Saraf", "Ayush", ""], ["Huang", "Jia-Bin", ""], ["Kopf", "Johannes", ""]]}, {"id": "2009.01865", "submitter": "Michael Lomnitz", "authors": "Michael Lomnitz, Zigfried Hampel-Arias, Nina Lopatina, Felipe A. Mejia", "title": "A general approach to bridge the reality-gap", "comments": "8 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Employing machine learning models in the real world requires collecting large\namounts of data, which is both time consuming and costly to collect. A common\napproach to circumvent this is to leverage existing, similar data-sets with\nlarge amounts of labelled data. However, models trained on these canonical\ndistributions do not readily transfer to real-world ones. Domain adaptation and\ntransfer learning are often used to breach this \"reality gap\", though both\nrequire a substantial amount of real-world data. In this paper we discuss a\nmore general approach: we propose learning a general transformation to bring\narbitrary images towards a canonical distribution where we can naively apply\nthe trained machine learning models. This transformation is trained in an\nunsupervised regime, leveraging data augmentation to generate off-canonical\nexamples of images and training a Deep Learning model to recover their original\ncounterpart. We quantify the performance of this transformation using\npre-trained ImageNet classifiers, demonstrating that this procedure can recover\nhalf of the loss in performance on the distorted data-set. We then validate the\neffectiveness of this approach on a series of pre-trained ImageNet models on a\nreal world data set collected by printing and photographing images in different\nlighting conditions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 18:19:28 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Lomnitz", "Michael", ""], ["Hampel-Arias", "Zigfried", ""], ["Lopatina", "Nina", ""], ["Mejia", "Felipe A.", ""]]}, {"id": "2009.01867", "submitter": "Sheng Lin", "authors": "Sheng Lin, Chenghong Wang, Hongjia Li, Jieren Deng, Yanzhi Wang,\n  Caiwen Ding", "title": "ESMFL: Efficient and Secure Models for Federated Learning", "comments": "7 pages, 3 figures, accepted by NeurIPS Workshop 2020, SpicyFL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, Deep Neural Networks are widely applied to various domains.\nHowever, massive data collection required for deep neural network reveals the\npotential privacy issues and also consumes large mounts of communication\nbandwidth. To address these problems, we propose a privacy-preserving method\nfor the federated learning distributed system, operated on Intel Software Guard\nExtensions, a set of instructions that increase the security of application\ncode and data. Meanwhile, the encrypted models make the transmission overhead\nlarger. Hence, we reduce the commutation cost by sparsification and it can\nachieve reasonable accuracy with different model architectures.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 18:27:32 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 19:45:00 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Lin", "Sheng", ""], ["Wang", "Chenghong", ""], ["Li", "Hongjia", ""], ["Deng", "Jieren", ""], ["Wang", "Yanzhi", ""], ["Ding", "Caiwen", ""]]}, {"id": "2009.01871", "submitter": "Holger Roth", "authors": "Holger R. Roth, Ken Chang, Praveer Singh, Nir Neumark, Wenqi Li,\n  Vikash Gupta, Sharut Gupta, Liangqiong Qu, Alvin Ihsani, Bernardo C. Bizzo,\n  Yuhong Wen, Varun Buch, Meesam Shah, Felipe Kitamura, Matheus Mendon\\c{c}a,\n  Vitor Lavor, Ahmed Harouni, Colin Compas, Jesse Tetreault, Prerna Dogra, Yan\n  Cheng, Selnur Erdal, Richard White, Behrooz Hashemian, Thomas Schultz, Miao\n  Zhang, Adam McCarthy, B. Min Yun, Elshaimaa Sharaf, Katharina V. Hoebel, Jay\n  B. Patel, Bryan Chen, Sean Ko, Evan Leibovitz, Etta D. Pisano, Laura Coombs,\n  Daguang Xu, Keith J. Dreyer, Ittai Dayan, Ram C. Naidu, Mona Flores, Daniel\n  Rubin, Jayashree Kalpathy-Cramer", "title": "Federated Learning for Breast Density Classification: A Real-World\n  Implementation", "comments": "Accepted at the 1st MICCAI Workshop on \"Distributed And Collaborative\n  Learning\"; add citation to Fig. 1 & 2 and update Fig. 5; fix typo in\n  affiliations", "journal-ref": "In: Albarqouni S. et al. (eds) Domain Adaptation and\n  Representation Transfer, and Distributed and Collaborative Learning. DART\n  2020, DCL 2020. Lecture Notes in Computer Science, vol 12444. Springer, Cham", "doi": "10.1007/978-3-030-60548-3_18", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building robust deep learning-based models requires large quantities of\ndiverse training data. In this study, we investigate the use of federated\nlearning (FL) to build medical imaging classification models in a real-world\ncollaborative setting. Seven clinical institutions from across the world joined\nthis FL effort to train a model for breast density classification based on\nBreast Imaging, Reporting & Data System (BI-RADS). We show that despite\nsubstantial differences among the datasets from all sites (mammography system,\nclass distribution, and data set size) and without centralizing data, we can\nsuccessfully train AI models in federation. The results show that models\ntrained using FL perform 6.3% on average better than their counterparts trained\non an institute's local data alone. Furthermore, we show a 45.8% relative\nimprovement in the models' generalizability when evaluated on the other\nparticipating sites' testing data.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 18:34:59 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 19:15:02 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 13:46:55 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Roth", "Holger R.", ""], ["Chang", "Ken", ""], ["Singh", "Praveer", ""], ["Neumark", "Nir", ""], ["Li", "Wenqi", ""], ["Gupta", "Vikash", ""], ["Gupta", "Sharut", ""], ["Qu", "Liangqiong", ""], ["Ihsani", "Alvin", ""], ["Bizzo", "Bernardo C.", ""], ["Wen", "Yuhong", ""], ["Buch", "Varun", ""], ["Shah", "Meesam", ""], ["Kitamura", "Felipe", ""], ["Mendon\u00e7a", "Matheus", ""], ["Lavor", "Vitor", ""], ["Harouni", "Ahmed", ""], ["Compas", "Colin", ""], ["Tetreault", "Jesse", ""], ["Dogra", "Prerna", ""], ["Cheng", "Yan", ""], ["Erdal", "Selnur", ""], ["White", "Richard", ""], ["Hashemian", "Behrooz", ""], ["Schultz", "Thomas", ""], ["Zhang", "Miao", ""], ["McCarthy", "Adam", ""], ["Yun", "B. Min", ""], ["Sharaf", "Elshaimaa", ""], ["Hoebel", "Katharina V.", ""], ["Patel", "Jay B.", ""], ["Chen", "Bryan", ""], ["Ko", "Sean", ""], ["Leibovitz", "Evan", ""], ["Pisano", "Etta D.", ""], ["Coombs", "Laura", ""], ["Xu", "Daguang", ""], ["Dreyer", "Keith J.", ""], ["Dayan", "Ittai", ""], ["Naidu", "Ram C.", ""], ["Flores", "Mona", ""], ["Rubin", "Daniel", ""], ["Kalpathy-Cramer", "Jayashree", ""]]}, {"id": "2009.01875", "submitter": "Chen Fu", "authors": "Chen Fu, Chiyu Dong, Christoph Mertz and John M. Dolan", "title": "Depth Completion via Inductive Fusion of Planar LIDAR and Monocular\n  Camera", "comments": "Accepted at IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern high-definition LIDAR is expensive for commercial autonomous driving\nvehicles and small indoor robots. An affordable solution to this problem is\nfusion of planar LIDAR with RGB images to provide a similar level of perception\ncapability. Even though state-of-the-art methods provide approaches to predict\ndepth information from limited sensor input, they are usually a simple\nconcatenation of sparse LIDAR features and dense RGB features through an\nend-to-end fusion architecture. In this paper, we introduce an inductive\nlate-fusion block which better fuses different sensor modalities inspired by a\nprobability model. The proposed demonstration and aggregation network\npropagates the mixed context and depth features to the prediction network and\nserves as a prior knowledge of the depth completion. This late-fusion block\nuses the dense context features to guide the depth prediction based on\ndemonstrations by sparse depth features. In addition to evaluating the proposed\nmethod on benchmark depth completion datasets including NYUDepthV2 and KITTI,\nwe also test the proposed method on a simulated planar LIDAR dataset. Our\nmethod shows promising results compared to previous approaches on both the\nbenchmark datasets and simulated dataset with various 3D densities.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 18:39:57 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Fu", "Chen", ""], ["Dong", "Chiyu", ""], ["Mertz", "Christoph", ""], ["Dolan", "John M.", ""]]}, {"id": "2009.01907", "submitter": "Adrian Galdran", "authors": "Adrian Galdran, Andr\\'e Anjos, Jos\\'e Dolz, Hadi Chakor, Herv\\'e\n  Lombaert, Ismail Ben Ayed", "title": "The Little W-Net That Could: State-of-the-Art Retinal Vessel\n  Segmentation with Minimalistic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of the retinal vasculature from eye fundus images represents\none of the most fundamental tasks in retinal image analysis. Over recent years,\nincreasingly complex approaches based on sophisticated Convolutional Neural\nNetwork architectures have been slowly pushing performance on well-established\nbenchmark datasets. In this paper, we take a step back and analyze the real\nneed of such complexity. Specifically, we demonstrate that a minimalistic\nversion of a standard U-Net with several orders of magnitude less parameters,\ncarefully trained and rigorously evaluated, closely approximates the\nperformance of current best techniques. In addition, we propose a simple\nextension, dubbed W-Net, which reaches outstanding performance on several\npopular datasets, still using orders of magnitude less learnable weights than\nany previously published approach. Furthermore, we provide the most\ncomprehensive cross-dataset performance analysis to date, involving up to 10\ndifferent databases. Our analysis demonstrates that the retinal vessel\nsegmentation problem is far from solved when considering test images that\ndiffer substantially from the training data, and that this task represents an\nideal scenario for the exploration of domain adaptation techniques. In this\ncontext, we experiment with a simple self-labeling strategy that allows us to\nmoderately enhance cross-dataset performance, indicating that there is still\nmuch room for improvement in this area. Finally, we also test our approach on\nthe Artery/Vein segmentation problem, where we again achieve results\nwell-aligned with the state-of-the-art, at a fraction of the model complexity\nin recent literature. All the code to reproduce the results in this paper is\nreleased.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 19:59:51 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Galdran", "Adrian", ""], ["Anjos", "Andr\u00e9", ""], ["Dolz", "Jos\u00e9", ""], ["Chakor", "Hadi", ""], ["Lombaert", "Herv\u00e9", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "2009.01924", "submitter": "Nina Montana Brown Miss", "authors": "N. Montana Brown, Y. Fu, S. U. Saeed, A. Casamitjana, Z. M. C. Baum,\n  R. Delaunay, Q. Yang, A. Grimwood, Z. Min, E. Bonmati, T. Vercauteren, M. J.\n  Clarkson, and Y. Hu", "title": "Introduction to Medical Image Registration with DeepReg, Between Old and\n  New", "comments": "Submitted to MICCAI Educational Challenge 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This document outlines a tutorial to get started with medical image\nregistration using the open-source package DeepReg. The basic concepts of\nmedical image registration are discussed, linking classical methods to newer\nmethods using deep learning. Two iterative, classical algorithms using\noptimisation and one learning-based algorithm using deep learning are coded\nstep-by-step using DeepReg utilities, all with real, open-accessible, medical\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 19:44:23 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 06:45:40 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Brown", "N. Montana", ""], ["Fu", "Y.", ""], ["Saeed", "S. U.", ""], ["Casamitjana", "A.", ""], ["Baum", "Z. M. C.", ""], ["Delaunay", "R.", ""], ["Yang", "Q.", ""], ["Grimwood", "A.", ""], ["Min", "Z.", ""], ["Bonmati", "E.", ""], ["Vercauteren", "T.", ""], ["Clarkson", "M. J.", ""], ["Hu", "Y.", ""]]}, {"id": "2009.01956", "submitter": "Priyadarshini Panda", "authors": "Varigonda Pavan Teja, and Priyadarshini Panda", "title": "Compression-aware Continual Learning using Singular Value Decomposition", "comments": "13 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a compression based continual task learning method that can\ndynamically grow a neural network. Inspired from the recent model compression\ntechniques, we employ compression-aware training and perform low-rank weight\napproximations using singular value decomposition (SVD) to achieve network\ncompaction. By encouraging the network to learn low-rank weight filters, our\nmethod achieves compressed representations with minimal performance degradation\nwithout the need for costly fine-tuning. Specifically, we decompose the weight\nfilters using SVD and train the network on incremental tasks in its factorized\nform. Such a factorization allows us to directly impose sparsity-inducing\nregularizers over the singular values and allows us to use fewer number of\nparameters for each task. We further introduce a novel shared representational\nspace based learning between tasks. This promotes the incoming tasks to only\nlearn residual task-specific information on top of the previously learnt weight\nfilters and greatly helps in learning under fixed capacity constraints. Our\nmethod significantly outperforms prior continual learning approaches on three\nbenchmark datasets, demonstrating accuracy improvements of 10.3%, 12.3%, 15.6%\non 20-split CIFAR-100, miniImageNet and a 5-sequence dataset, respectively,\nover state-of-the-art. Further, our method yields compressed models that have\n~3.64x, 2.88x, 5.91x fewer number of parameters respectively, on the above\nmentioned datasets in comparison to baseline individual task models. Our source\ncode is available at https://github.com/pavanteja295/CACL.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 23:29:50 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 22:29:21 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Teja", "Varigonda Pavan", ""], ["Panda", "Priyadarshini", ""]]}, {"id": "2009.01972", "submitter": "Seyed Mehdi Iranmanesh", "authors": "Seyed Mehdi Iranmanesh, Ali Dabouei, Nasser M. Nasrabadi", "title": "Attribute Adaptive Margin Softmax Loss using Privileged Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework to exploit privileged information for\nrecognition which is provided only during the training phase. Here, we focus on\nrecognition task where images are provided as the main view and soft biometric\ntraits (attributes) are provided as the privileged data (only available during\ntraining phase). We demonstrate that more discriminative feature space can be\nlearned by enforcing a deep network to adjust adaptive margins between classes\nutilizing attributes. This tight constraint also effectively reduces the class\nimbalance inherent in the local data neighborhood, thus carving more balanced\nclass boundaries locally and using feature space more efficiently. Extensive\nexperiments are performed on five different datasets and the results show the\nsuperiority of our method compared to the state-of-the-art models in both tasks\nof face recognition and person re-identification.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 01:04:12 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Iranmanesh", "Seyed Mehdi", ""], ["Dabouei", "Ali", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "2009.01987", "submitter": "Mohammad Fasha", "authors": "Mohammad Fasha, Bassam Hammo, Nadim Obeid, Jabir Widian", "title": "A Hybrid Deep Learning Model for Arabic Text Recognition", "comments": "11 pages", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications, Vol. 11, No. 8, 2020", "doi": "10.14569/issn.2156-5570", "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arabic text recognition is a challenging task because of the cursive nature\nof Arabic writing system, its joint writing scheme, the large number of\nligatures and many other challenges. Deep Learning DL models achieved\nsignificant progress in numerous domains including computer vision and sequence\nmodelling. This paper presents a model that can recognize Arabic text that was\nprinted using multiple font types including fonts that mimic Arabic handwritten\nscripts. The proposed model employs a hybrid DL network that can recognize\nArabic printed text without the need for character segmentation. The model was\ntested on a custom dataset comprised of over two million word samples that were\ngenerated using 18 different Arabic font types. The objective of the testing\nprocess was to assess the model capability in recognizing a diverse set of\nArabic fonts representing a varied cursive styles. The model achieved good\nresults in recognizing characters and words and it also achieved promising\nresults in recognizing characters when it was tested on unseen data. The\nprepared model, the custom datasets and the toolkit for generating similar\ndatasets are made publicly available, these tools can be used to prepare models\nfor recognizing other font types as well as to further extend and enhance the\nperformance of the proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 02:49:17 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Fasha", "Mohammad", ""], ["Hammo", "Bassam", ""], ["Obeid", "Nadim", ""], ["Widian", "Jabir", ""]]}, {"id": "2009.01998", "submitter": "Diogo Luvizon", "authors": "Diogo Luvizon and Hedi Tabia and David Picard", "title": "SSP-Net: Scalable Sequential Pyramid Networks for Real-Time 3D Human\n  Pose Regression", "comments": "Under review at PR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a highly scalable convolutional neural network,\nend-to-end trainable, for real-time 3D human pose regression from still RGB\nimages. We call this approach the Scalable Sequential Pyramid Networks\n(SSP-Net) as it is trained with refined supervision at multiple scales in a\nsequential manner. Our network requires a single training procedure and is\ncapable of producing its best predictions at 120 frames per second (FPS), or\nacceptable predictions at more than 200 FPS when cut at test time. We show that\nthe proposed regression approach is invariant to the size of feature maps,\nallowing our method to perform multi-resolution intermediate supervisions and\nreaching results comparable to the state-of-the-art with very low resolution\nfeature maps. We demonstrate the accuracy and the effectiveness of our method\nby providing extensive experiments on two of the most important publicly\navailable datasets for 3D pose estimation, Human3.6M and MPI-INF-3DHP.\nAdditionally, we provide relevant insights about our decisions on the network\narchitecture and show its flexibility to meet the best precision-speed\ncompromise.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 03:43:24 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Luvizon", "Diogo", ""], ["Tabia", "Hedi", ""], ["Picard", "David", ""]]}, {"id": "2009.02007", "submitter": "Jiyang Yu", "authors": "Jiyang Yu, Ravi Ramamoorthi, Keli Cheng, Michel Sarkis, Ning Bi", "title": "Real-Time Selfie Video Stabilization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel real-time selfie video stabilization method. Our method is\ncompletely automatic and runs at 26 fps. We use a 1D linear convolutional\nnetwork to directly infer the rigid moving least squares warping which\nimplicitly balances between the global rigidity and local flexibility. Our\nnetwork structure is specifically designed to stabilize the background and\nforeground at the same time, while providing optional control of stabilization\nfocus (relative importance of foreground vs. background) to the users. To train\nour network, we collect a selfie video dataset with 1005 videos, which is\nsignificantly larger than previous selfie video datasets. We also propose a\ngrid approximation method to the rigid moving least squares warping that\nenables the real-time frame warping. Our method is fully automatic and produces\nvisually and quantitatively better results than previous real-time general\nvideo stabilization methods. Compared to previous offline selfie video methods,\nour approach produces comparable quality with a speed improvement of orders of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 04:41:05 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 22:04:42 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Yu", "Jiyang", ""], ["Ramamoorthi", "Ravi", ""], ["Cheng", "Keli", ""], ["Sarkis", "Michel", ""], ["Bi", "Ning", ""]]}, {"id": "2009.02009", "submitter": "Jaeseong Lee", "authors": "Jaeseong Lee, Duseok Kang and Soonhoi Ha", "title": "S3NAS: Fast NPU-aware Neural Architecture Search Methodology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the application area of convolutional neural networks (CNN) is growing in\nembedded devices, it becomes popular to use a hardware CNN accelerator, called\nneural processing unit (NPU), to achieve higher performance per watt than CPUs\nor GPUs. Recently, automated neural architecture search (NAS) emerges as the\ndefault technique to find a state-of-the-art CNN architecture with higher\naccuracy than manually-designed architectures for image classification. In this\npaper, we present a fast NPU-aware NAS methodology, called S3NAS, to find a CNN\narchitecture with higher accuracy than the existing ones under a given latency\nconstraint. It consists of three steps: supernet design, Single-Path NAS for\nfast architecture exploration, and scaling. To widen the search space of the\nsupernet structure that consists of stages, we allow stages to have a different\nnumber of blocks and blocks to have parallel layers of different kernel sizes.\nFor a fast neural architecture search, we apply a modified Single-Path NAS\ntechnique to the proposed supernet structure. In this step, we assume a shorter\nlatency constraint than the required to reduce the search space and the search\ntime. The last step is to scale up the network maximally within the latency\nconstraint. For accurate latency estimation, an analytical latency estimator is\ndevised, based on a cycle-level NPU simulator that runs an entire CNN\nconsidering the memory access overhead accurately. With the proposed\nmethodology, we are able to find a network in 3 hours using TPUv3, which shows\n82.72% top-1 accuracy on ImageNet with 11.66 ms latency. Code are released at\nhttps://github.com/cap-lab/S3NAS\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 04:45:50 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Lee", "Jaeseong", ""], ["Kang", "Duseok", ""], ["Ha", "Soonhoi", ""]]}, {"id": "2009.02018", "submitter": "DongGyu Joo", "authors": "Doyeon Kim, Donggyu Joo, Junmo Kim", "title": "TiVGAN: Text to Image to Video Generation with Step-by-Step Evolutionary\n  Generator", "comments": "IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in technology have led to the development of methods that can create\ndesired visual multimedia. In particular, image generation using deep learning\nhas been extensively studied across diverse fields. In comparison, video\ngeneration, especially on conditional inputs, remains a challenging and less\nexplored area. To narrow this gap, we aim to train our model to produce a video\ncorresponding to a given text description. We propose a novel training\nframework, Text-to-Image-to-Video Generative Adversarial Network (TiVGAN),\nwhich evolves frame-by-frame and finally produces a full-length video. In the\nfirst phase, we focus on creating a high-quality single video frame while\nlearning the relationship between the text and an image. As the steps proceed,\nour model is trained gradually on more number of consecutive frames.This\nstep-by-step learning process helps stabilize the training and enables the\ncreation of high-resolution video based on conditional text descriptions.\nQualitative and quantitative experimental results on various datasets\ndemonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 06:33:08 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 00:25:23 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kim", "Doyeon", ""], ["Joo", "Donggyu", ""], ["Kim", "Junmo", ""]]}, {"id": "2009.02062", "submitter": "Foivos Diakogiannis", "authors": "Foivos I. Diakogiannis, Fran\\c{c}ois Waldner, Peter Caccetta", "title": "Looking for change? Roll the Dice and demand Attention", "comments": "28 pages, under review in ISPRS P&RS, 1st revision. Figures of low\n  quality due to compression for arxiv. Reduced abstract in arxiv due to\n  character limitations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection, i.e. identification per pixel of changes for some classes\nof interest from a set of bi-temporal co-registered images, is a fundamental\ntask in the field of remote sensing. It remains challenging due to unrelated\nforms of change that appear at different times in input images. Here, we\npropose a reliable deep learning framework for the task of semantic change\ndetection in very high-resolution aerial images. Our framework consists of a\nnew loss function, new attention modules, new feature extraction building\nblocks, and a new backbone architecture that is tailored for the task of\nsemantic change detection. Specifically, we define a new form of set\nsimilarity, that is based on an iterative evaluation of a variant of the Dice\ncoefficient. We use this similarity metric to define a new loss function as\nwell as a new spatial and channel convolution Attention layer (the FracTAL).\nThe new attention layer, designed specifically for vision tasks, is memory\nefficient, thus suitable for use in all levels of deep convolutional networks.\nBased on these, we introduce two new efficient self-contained feature\nextraction convolution units. We validate the performance of these feature\nextraction building blocks on the CIFAR10 reference data and compare the\nresults with standard ResNet modules. Further, we introduce a new\nencoder/decoder scheme, a network macro-topology, that is tailored for the task\nof change detection. Our network moves away from any notion of subtraction of\nfeature layers for identifying change. We validate our approach by showing\nexcellent performance and achieving state of the art score (F1 and Intersection\nover Union-hereafter IoU) on two building change detection datasets, namely,\nthe LEVIRCD (F1: 0.918, IoU: 0.848) and the WHU (F1: 0.938, IoU: 0.882)\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 08:30:25 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 08:15:48 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Diakogiannis", "Foivos I.", ""], ["Waldner", "Fran\u00e7ois", ""], ["Caccetta", "Peter", ""]]}, {"id": "2009.02119", "submitter": "Youngwoo Yoon", "authors": "Youngwoo Yoon, Bok Cha, Joo-Haeng Lee, Minsu Jang, Jaeyeon Lee,\n  Jaehong Kim, Geehyuk Lee", "title": "Speech Gesture Generation from the Trimodal Context of Text, Audio, and\n  Speaker Identity", "comments": "16 pages; ACM Transactions on Graphics (SIGGRAPH Asia 2020)", "journal-ref": null, "doi": "10.1145/3414685.3417838", "report-no": null, "categories": "cs.GR cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For human-like agents, including virtual avatars and social robots, making\nproper gestures while speaking is crucial in human--agent interaction.\nCo-speech gestures enhance interaction experiences and make the agents look\nalive. However, it is difficult to generate human-like gestures due to the lack\nof understanding of how people gesture. Data-driven approaches attempt to learn\ngesticulation skills from human demonstrations, but the ambiguous and\nindividual nature of gestures hinders learning. In this paper, we present an\nautomatic gesture generation model that uses the multimodal context of speech\ntext, audio, and speaker identity to reliably generate gestures. By\nincorporating a multimodal context and an adversarial training scheme, the\nproposed model outputs gestures that are human-like and that match with speech\ncontent and rhythm. We also introduce a new quantitative evaluation metric for\ngesture generation models. Experiments with the introduced metric and\nsubjective human evaluation showed that the proposed gesture generation model\nis better than existing end-to-end generation models. We further confirm that\nour model is able to work with synthesized audio in a scenario where contexts\nare constrained, and show that different gesture styles can be generated for\nthe same speech by specifying different speaker identities in the style\nembedding space that is learned from videos of various speakers. All the code\nand data is available at\nhttps://github.com/ai4r/Gesture-Generation-from-Trimodal-Context.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 11:42:45 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Yoon", "Youngwoo", ""], ["Cha", "Bok", ""], ["Lee", "Joo-Haeng", ""], ["Jang", "Minsu", ""], ["Lee", "Jaeyeon", ""], ["Kim", "Jaehong", ""], ["Lee", "Geehyuk", ""]]}, {"id": "2009.02130", "submitter": "Li Rui", "authors": "Rui Li, Shunyi Zheng, Chenxi Duan, Ce Zhang, Jianlin Su, P.M. Atkinson", "title": "Multi-Attention-Network for Semantic Segmentation of Fine Resolution\n  Remote Sensing Images", "comments": "arXiv admin note: substantial text overlap with arXiv:2007.14902", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of remote sensing images plays an important role in a\nwide range of applications including land resource management, biosphere\nmonitoring and urban planning. Although the accuracy of semantic segmentation\nin remote sensing images has been increased significantly by deep convolutional\nneural networks, several limitations exist in standard models. First, for\nencoder-decoder architectures such as U-Net, the utilization of multi-scale\nfeatures causes the underuse of information, where low-level features and\nhigh-level features are concatenated directly without any refinement. Second,\nlong-range dependencies of feature maps are insufficiently explored, resulting\nin sub-optimal feature representations associated with each semantic class.\nThird, even though the dot-product attention mechanism has been introduced and\nutilized in semantic segmentation to model long-range dependencies, the large\ntime and space demands of attention impede the actual usage of attention in\napplication scenarios with large-scale input. This paper proposed a\nMulti-Attention-Network (MANet) to address these issues by extracting\ncontextual dependencies through multiple efficient attention modules. A novel\nattention mechanism of kernel attention with linear complexity is proposed to\nalleviate the large computational demand in attention. Based on kernel\nattention and channel attention, we integrate local feature maps extracted by\nResNeXt-101 with their corresponding global dependencies and reweight\ninterdependent channel maps adaptively. Numerical experiments on three\nlarge-scale fine resolution remote sensing images captured by different\nsatellite sensors demonstrate the superior performance of the proposed MANet,\noutperforming the DeepLab V3+, PSPNet, FastFCN, DANet, OCRNet, and other\nbenchmark approaches.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 09:08:02 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 09:32:06 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 12:56:22 GMT"}, {"version": "v4", "created": "Mon, 23 Nov 2020 12:56:55 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Li", "Rui", ""], ["Zheng", "Shunyi", ""], ["Duan", "Chenxi", ""], ["Zhang", "Ce", ""], ["Su", "Jianlin", ""], ["Atkinson", "P. M.", ""]]}, {"id": "2009.02174", "submitter": "Lyes Khacef", "authors": "Lyes Khacef, Laurent Rodriguez, Benoit Miramond", "title": "Improving Self-Organizing Maps with Unsupervised Feature Extraction", "comments": "Accepted for publication in the International Conference on Neural\n  Information Processing (ICONIP) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Self-Organizing Map (SOM) is a brain-inspired neural model that is very\npromising for unsupervised learning, especially in embedded applications.\nHowever, it is unable to learn efficient prototypes when dealing with complex\ndatasets. We propose in this work to improve the SOM performance by using\nextracted features instead of raw data. We conduct a comparative study on the\nSOM classification accuracy with unsupervised feature extraction using two\ndifferent approaches: a machine learning approach with Sparse Convolutional\nAuto-Encoders using gradient-based learning, and a neuroscience approach with\nSpiking Neural Networks using Spike Timing Dependant Plasticity learning. The\nSOM is trained on the extracted features, then very few labeled samples are\nused to label the neurons with their corresponding class. We investigate the\nimpact of the feature maps, the SOM size and the labeled subset size on the\nclassification accuracy using the different feature extraction methods. We\nimprove the SOM classification by +6.09\\% and reach state-of-the-art\nperformance on unsupervised image classification.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 13:19:24 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Khacef", "Lyes", ""], ["Rodriguez", "Laurent", ""], ["Miramond", "Benoit", ""]]}, {"id": "2009.02185", "submitter": "Tomer Barak", "authors": "Tomer Barak, Yehonatan Avidan and Yonatan Loewenstein", "title": "Naive Artificial Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the cognitive sciences, it is common to distinguish between crystal\nintelligence, the ability to utilize knowledge acquired through past learning\nor experience and fluid intelligence, the ability to solve novel problems\nwithout relying on prior knowledge. Using this cognitive distinction between\nthe two types of intelligence, extensively-trained deep networks that can play\nchess or Go exhibit crystal but not fluid intelligence. In humans, fluid\nintelligence is typically studied and quantified using intelligence tests.\nPrevious studies have shown that deep networks can solve some forms of\nintelligence tests, but only after extensive training. Here we present a\ncomputational model that solves intelligence tests without any prior training.\nThis ability is based on continual inductive reasoning, and is implemented by\ndeep unsupervised latent-prediction networks. Our work demonstrates the\npotential fluid intelligence of deep networks. Finally, we propose that the\ncomputational principles underlying our approach can be used to model fluid\nintelligence in the cognitive sciences.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 13:40:10 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Barak", "Tomer", ""], ["Avidan", "Yehonatan", ""], ["Loewenstein", "Yonatan", ""]]}, {"id": "2009.02189", "submitter": "Yechan Kim", "authors": "Yechan Kim, Younkwan Lee, and Moongu Jeon", "title": "Imbalanced Image Classification with Complement Cross Entropy", "comments": "8 pages, Revision on Pattern Recognition Letters (PRL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning models have achieved great success in computer vision\napplications, relying on large-scale class-balanced datasets. However,\nimbalanced class distributions still limit the wide applicability of these\nmodels due to degradation in performance. To solve this problem, in this paper,\nwe concentrate on the study of cross entropy which mostly ignores output scores\non incorrect classes. This work discovers that neutralizing predicted\nprobabilities on incorrect classes improves the prediction accuracy for\nimbalanced image classification. This paper proposes a simple but effective\nloss named complement cross entropy based on this finding. The proposed loss\nmakes the ground truth class overwhelm the other classes in terms of softmax\nprobability, by neutralizing probabilities of incorrect classes, without\nadditional training procedures. Along with it, this loss facilitates the models\nto learn key information especially from samples on minority classes. It\nensures more accurate and robust classification results on imbalanced\ndistributions. Extensive experiments on imbalanced datasets demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 13:46:24 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 03:32:48 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 10:57:58 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Kim", "Yechan", ""], ["Lee", "Younkwan", ""], ["Jeon", "Moongu", ""]]}, {"id": "2009.02191", "submitter": "Jae Hyun Park", "authors": "Jae Hyun Park, Ji Sub Choi, Jong Hwan Ko", "title": "Dual Precision Deep Neural Network", "comments": "5 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-line Precision scalability of the deep neural networks(DNNs) is a critical\nfeature to support accuracy and complexity trade-off during the DNN inference.\nIn this paper, we propose dual-precision DNN that includes two different\nprecision modes in a single model, thereby supporting an on-line precision\nswitch without re-training. The proposed two-phase training process optimizes\nboth low- and high-precision modes.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 02:56:51 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Park", "Jae Hyun", ""], ["Choi", "Ji Sub", ""], ["Ko", "Jong Hwan", ""]]}, {"id": "2009.02216", "submitter": "Noa Fish", "authors": "Noa Fish, Lilach Perry, Amit Bermano, Daniel Cohen-Or", "title": "SketchPatch: Sketch Stylization via Seamless Patch-level Synthesis", "comments": "SIGGRAPH Asia 2020", "journal-ref": null, "doi": "10.1145/3414685.3417816", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paradigm of image-to-image translation is leveraged for the benefit of\nsketch stylization via transfer of geometric textural details. Lacking the\nnecessary volumes of data for standard training of translation systems, we\nadvocate for operation at the patch level, where a handful of stylized sketches\nprovide ample mining potential for patches featuring basic geometric\nprimitives. Operating at the patch level necessitates special consideration of\nfull sketch translation, as individual translation of patches with no regard to\nneighbors is likely to produce visible seams and artifacts at patch borders.\nAligned pairs of styled and plain primitives are combined to form input hybrids\ncontaining styled elements around the border and plain elements within, and\ngiven as input to a seamless translation (ST) generator, whose output patches\nare expected to reconstruct the fully styled patch. An adversarial addition\npromotes generalization and robustness to diverse geometries at inference time,\nforming a simple and effective system for arbitrary sketch stylization, as\ndemonstrated upon a variety of styles and sketches.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 14:20:46 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Fish", "Noa", ""], ["Perry", "Lilach", ""], ["Bermano", "Amit", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2009.02256", "submitter": "Wei Xu", "authors": "Xinyi Huang, Suphanut Jamonnak, Ye Zhao, Boyu Wang, Minh Hoai, Kevin\n  Yager, Wei Xu", "title": "Interactive Visual Study of Multiple Attributes Learning Model of X-Ray\n  Scattering Images", "comments": "IEEE SciVis Conference 2020", "journal-ref": "IEEE Transactions on Visualization & Computer Graphics 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing interactive visualization tools for deep learning are mostly applied\nto the training, debugging, and refinement of neural network models working on\nnatural images. However, visual analytics tools are lacking for the specific\napplication of x-ray image classification with multiple structural attributes.\nIn this paper, we present an interactive system for domain scientists to\nvisually study the multiple attributes learning models applied to x-ray\nscattering images. It allows domain scientists to interactively explore this\nimportant type of scientific images in embedded spaces that are defined on the\nmodel prediction output, the actual labels, and the discovered feature space of\nneural networks. Users are allowed to flexibly select instance images, their\nclusters, and compare them regarding the specified visual representation of\nattributes. The exploration is guided by the manifestation of model performance\nrelated to mutual relationships among attributes, which often affect the\nlearning accuracy and effectiveness. The system thus supports domain scientists\nto improve the training dataset and model, find questionable attributes labels,\nand identify outlier images or spurious data clusters. Case studies and\nscientists feedback demonstrate its functionalities and usefulness.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 00:38:45 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Huang", "Xinyi", ""], ["Jamonnak", "Suphanut", ""], ["Zhao", "Ye", ""], ["Wang", "Boyu", ""], ["Hoai", "Minh", ""], ["Yager", "Kevin", ""], ["Xu", "Wei", ""]]}, {"id": "2009.02264", "submitter": "Miguel Boland", "authors": "Miguel Boland, Edward A.K. Cohen, Seth Flaxman, Mark A.A. Neil", "title": "Improving axial resolution in SIM using deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured Illumination Microscopy is a widespread methodology to image live\nand fixed biological structures smaller than the diffraction limits of\nconventional optical microscopy. Using recent advances in image up-scaling\nthrough deep learning models, we demonstrate a method to reconstruct 3D SIM\nimage stacks with twice the axial resolution attainable through conventional\nSIM reconstructions. We further evaluate our method for robustness to noise &\ngeneralisability to varying observed specimens, and discuss potential adaptions\nof the method to further improvements in resolution.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 15:48:48 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 22:22:47 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 09:56:10 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Boland", "Miguel", ""], ["Cohen", "Edward A. K.", ""], ["Flaxman", "Seth", ""], ["Neil", "Mark A. A.", ""]]}, {"id": "2009.02276", "submitter": "Jonas Geiping", "authors": "Jonas Geiping, Liam Fowl, W. Ronny Huang, Wojciech Czaja, Gavin\n  Taylor, Michael Moeller, Tom Goldstein", "title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching", "comments": "First two authors contributed equally. Last two authors contributed\n  equally. 21 pages, 11 figures. Published at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Poisoning attacks modify training data to maliciously control a model\ntrained on such data. In this work, we focus on targeted poisoning attacks\nwhich cause a reclassification of an unmodified test image and as such breach\nmodel integrity. We consider a particularly malicious poisoning attack that is\nboth \"from scratch\" and \"clean label\", meaning we analyze an attack that\nsuccessfully works against new, randomly initialized models, and is nearly\nimperceptible to humans, all while perturbing only a small fraction of the\ntraining data. Previous poisoning attacks against deep neural networks in this\nsetting have been limited in scope and success, working only in simplified\nsettings or being prohibitively expensive for large datasets. The central\nmechanism of the new attack is matching the gradient direction of malicious\nexamples. We analyze why this works, supplement with practical considerations.\nand show its threat to real-world practitioners, finding that it is the first\npoisoning method to cause targeted misclassification in modern deep networks\ntrained from scratch on a full-sized, poisoned ImageNet dataset. Finally we\ndemonstrate the limitations of existing defensive strategies against such an\nattack, concluding that data poisoning is a credible threat, even for\nlarge-scale deep learning systems.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 16:17:54 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 15:58:21 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Geiping", "Jonas", ""], ["Fowl", "Liam", ""], ["Huang", "W. Ronny", ""], ["Czaja", "Wojciech", ""], ["Taylor", "Gavin", ""], ["Moeller", "Michael", ""], ["Goldstein", "Tom", ""]]}, {"id": "2009.02286", "submitter": "Hadi Mansourifar", "authors": "Hadi Mansourifar, Weidong Shi", "title": "Vulnerability of Face Recognition Systems Against Composite Face\n  Reconstruction Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rounding confidence score is considered trivial but a simple and effective\ncountermeasure to stop gradient descent based image reconstruction attacks.\nHowever, its capability in the face of more sophisticated reconstruction\nattacks is an uninvestigated research area. In this paper, we prove that, the\nface reconstruction attacks based on composite faces can reveal the\ninefficiency of rounding policy as countermeasure. We assume that, the attacker\ntakes advantage of face composite parts which helps the attacker to get access\nto the most important features of the face or decompose it to the independent\nsegments. Afterwards, decomposed segments are exploited as search parameters to\ncreate a search path to reconstruct optimal face. Face composition parts enable\nthe attacker to violate the privacy of face recognition models even with a\nblind search. However, we assume that, the attacker may take advantage of\nrandom search to reconstruct the target face faster. The algorithm is started\nwith random composition of face parts as initial face and confidence score is\nconsidered as fitness value. Our experiments show that, since the rounding\npolicy as countermeasure can't stop the random search process, current face\nrecognition systems are extremely vulnerable against such sophisticated\nattacks. To address this problem, we successfully test Face Detection Score\nFiltering (FDSF) as a countermeasure to protect the privacy of training data\nagainst proposed attack.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 03:37:51 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Mansourifar", "Hadi", ""], ["Shi", "Weidong", ""]]}, {"id": "2009.02326", "submitter": "Mojan Javaheripi", "authors": "Mojan Javaheripi, Mohammad Samragh, Gregory Fields, Tara Javidi,\n  Farinaz Koushanfar", "title": "CLEANN: Accelerated Trojan Shield for Embedded Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3400302.3415671", "report-no": null, "categories": "cs.LG cs.AR cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose CLEANN, the first end-to-end framework that enables online\nmitigation of Trojans for embedded Deep Neural Network (DNN) applications. A\nTrojan attack works by injecting a backdoor in the DNN while training; during\ninference, the Trojan can be activated by the specific backdoor trigger. What\ndifferentiates CLEANN from the prior work is its lightweight methodology which\nrecovers the ground-truth class of Trojan samples without the need for labeled\ndata, model retraining, or prior assumptions on the trigger or the attack. We\nleverage dictionary learning and sparse approximation to characterize the\nstatistical behavior of benign data and identify Trojan triggers. CLEANN is\ndevised based on algorithm/hardware co-design and is equipped with specialized\nhardware to enable efficient real-time execution on resource-constrained\nembedded platforms. Proof of concept evaluations on CLEANN for the\nstate-of-the-art Neural Trojan attacks on visual benchmarks demonstrate its\ncompetitive advantage in terms of attack resiliency and execution overhead.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 05:29:38 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Javaheripi", "Mojan", ""], ["Samragh", "Mohammad", ""], ["Fields", "Gregory", ""], ["Javidi", "Tara", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "2009.02345", "submitter": "Raphael Royer-Rivard", "authors": "Rapha\\\"el Royer-Rivard, Fantin Girard, Nagib Dahdah and Farida Cheriet", "title": "End-to-End Deep Learning Model for Cardiac Cycle Synchronization from\n  Multi-View Angiographic Sequences", "comments": null, "journal-ref": null, "doi": "10.1109/EMBC44109.2020.9175453", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic reconstructions (3D+T) of coronary arteries could give important\nperfusion details to clinicians. Temporal matching of the different views,\nwhich may not be acquired simultaneously, is a prerequisite for an accurate\nstereo-matching of the coronary segments. In this paper, we show how a neural\nnetwork can be trained from angiographic sequences to synchronize different\nviews during the cardiac cycle using raw x-ray angiography videos exclusively.\nFirst, we train a neural network model with angiographic sequences to extract\nfeatures describing the progression of the cardiac cycle. Then, we compute the\ndistance between the feature vectors of every frame from the first view with\nthose from the second view to generate distance maps that display stripe\npatterns. Using pathfinding, we extract the best temporally coherent\nassociations between each frame of both videos. Finally, we compare the\nsynchronized frames of an evaluation set with the ECG signals to show an\nalignment with 96.04% accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 18:11:50 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Royer-Rivard", "Rapha\u00ebl", ""], ["Girard", "Fantin", ""], ["Dahdah", "Nagib", ""], ["Cheriet", "Farida", ""]]}, {"id": "2009.02383", "submitter": "Bonifaz Stuhr", "authors": "Bonifaz Stuhr, J\\\"urgen Brauer", "title": "Don't miss the Mismatch: Investigating the Objective Function Mismatch\n  for Unsupervised Representation Learning", "comments": "21 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding general evaluation metrics for unsupervised representation learning\ntechniques is a challenging open research question, which recently has become\nmore and more necessary due to the increasing interest in unsupervised methods.\nEven though these methods promise beneficial representation characteristics,\nmost approaches currently suffer from the objective function mismatch. This\nmismatch states that the performance on a desired target task can decrease when\nthe unsupervised pretext task is learned too long - especially when both tasks\nare ill-posed. In this work, we build upon the widely used linear evaluation\nprotocol and define new general evaluation metrics to quantitatively capture\nthe objective function mismatch and the more generic metrics mismatch. We\ndiscuss the usability and stability of our protocols on a variety of pretext\nand target tasks and study mismatches in a wide range of experiments. Thereby\nwe disclose dependencies of the objective function mismatch across several\npretext and target tasks with respect to the pretext model's representation\nsize, target model complexity, pretext and target augmentations as well as\npretext and target task types.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 20:21:17 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Stuhr", "Bonifaz", ""], ["Brauer", "J\u00fcrgen", ""]]}, {"id": "2009.02386", "submitter": "Ze Wang", "authors": "Ze Wang, Xiuyuan Cheng, Guillermo Sapiro, Qiang Qiu", "title": "ACDC: Weight Sharing in Atom-Coefficient Decomposed Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are known to be significantly\nover-parametrized, and difficult to interpret, train and adapt. In this paper,\nwe introduce a structural regularization across convolutional kernels in a CNN.\nIn our approach, each convolution kernel is first decomposed as 2D dictionary\natoms linearly combined by coefficients. The widely observed correlation and\nredundancy in a CNN hint a common low-rank structure among the decomposed\ncoefficients, which is here further supported by our empirical observations. We\nthen explicitly regularize CNN kernels by enforcing decomposed coefficients to\nbe shared across sub-structures, while leaving each sub-structure only its own\ndictionary atoms, a few hundreds of parameters typically, which leads to\ndramatic model reductions. We explore models with sharing across different\nsub-structures to cover a wide range of trade-offs between parameter reduction\nand expressiveness. Our proposed regularized network structures open the door\nto better interpreting, training and adapting deep models. We validate the\nflexibility and compatibility of our method by image classification experiments\non multiple datasets and underlying network structures, and show that CNNs now\nmaintain performance with dramatic reduction in parameters and computations,\ne.g., only 5\\% parameters are used in a ResNet-18 to achieve comparable\nperformance. Further experiments on few-shot classification show that faster\nand more robust task adaptation is obtained in comparison with models with\nstandard convolutions.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 20:41:47 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Wang", "Ze", ""], ["Cheng", "Xiuyuan", ""], ["Sapiro", "Guillermo", ""], ["Qiu", "Qiang", ""]]}, {"id": "2009.02396", "submitter": "Bharti Munjal", "authors": "Bharti Munjal, Sikandar Amin, Fabio Galasso", "title": "Class Interference Regularization", "comments": "Accepted at BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive losses yield state-of-the-art performance for person\nre-identification, face verification and few shot learning. They have recently\noutperformed the cross-entropy loss on classification at the ImageNet scale and\noutperformed all self-supervision prior results by a large margin (SimCLR).\nSimple and effective regularization techniques such as label smoothing and\nself-distillation do not apply anymore, because they act on multinomial label\ndistributions, adopted in cross-entropy losses, and not on tuple comparative\nterms, which characterize the contrastive losses.\n  Here we propose a novel, simple and effective regularization technique, the\nClass Interference Regularization (CIR), which applies to cross-entropy losses\nbut is especially effective on contrastive losses. CIR perturbs the output\nfeatures by randomly moving them towards the average embeddings of the negative\nclasses. To the best of our knowledge, CIR is the first regularization\ntechnique to act on the output features.\n  In experimental evaluation, the combination of CIR and a plain Siamese-net\nwith triplet loss yields best few-shot learning performance on the challenging\ntieredImageNet. CIR also improves the state-of-the-art technique in person\nre-identification on the Market-1501 dataset, based on triplet loss, and the\nstate-of-the-art technique in person search on the CUHK-SYSU dataset, based on\na cross-entropy loss. Finally, on the task of classification CIR performs on\npar with the popular label smoothing, as demonstrated for CIFAR-10 and -100.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 21:03:32 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Munjal", "Bharti", ""], ["Amin", "Sikandar", ""], ["Galasso", "Fabio", ""]]}, {"id": "2009.02397", "submitter": "Javad Rahimipour Anaraki", "authors": "Javad Rahimipour Anaraki, Silvia Orlandi, Tom Chau", "title": "A Deep Learning Approach to Tongue Detection for Pediatric Population", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Children with severe disabilities and complex communication needs face\nlimitations in the usage of access technology (AT) devices. Conventional ATs\n(e.g., mechanical switches) can be insufficient for nonverbal children and\nthose with limited voluntary motion control. Automatic techniques for the\ndetection of tongue gestures represent a promising pathway. Previous studies\nhave shown the robustness of tongue detection algorithms on adult participants,\nbut further research is needed to use these methods with children. In this\nstudy, a network architecture for tongue-out gesture recognition was\nimplemented and evaluated on videos recorded in a naturalistic setting when\nchildren were playing a video-game. A cascade object detector algorithm was\nused to detect the participants' faces, and an automated classification scheme\nfor tongue gesture detection was developed using a convolutional neural network\n(CNN). In evaluation experiments conducted, the network was trained using\nadults and children's images. The network classification accuracy was evaluated\nusing leave-one-subject-out cross-validation. Preliminary classification\nresults obtained from the analysis of videos of five typically developing\nchildren showed an accuracy of up to 99% in predicting tongue-out gestures.\nMoreover, we demonstrated that using only children data for training the\nclassifier yielded better performance than adult's one supporting the need for\npediatric tongue gesture datasets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 21:04:57 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 22:41:39 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 19:43:29 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Anaraki", "Javad Rahimipour", ""], ["Orlandi", "Silvia", ""], ["Chau", "Tom", ""]]}, {"id": "2009.02406", "submitter": "Xinli Yu T", "authors": "Xinli Yu, Mohsen Malmir, Cynthia He, Yue Liu, Rex Wu", "title": "Video Moment Retrieval via Natural Language Queries", "comments": "needs internal approval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method for video moment retrieval (VMR)\nthat achieves state of the arts (SOTA) performance on R@1 metrics and\nsurpassing the SOTA on the high IoU metric (R@1, IoU=0.7).\n  First, we propose to use a multi-head self-attention mechanism, and further a\ncross-attention scheme to capture video/query interaction and long-range query\ndependencies from video context. The attention-based methods can develop\nframe-to-query interaction and query-to-frame interaction at arbitrary\npositions and the multi-head setting ensures the sufficient understanding of\ncomplicated dependencies. Our model has a simple architecture, which enables\nfaster training and inference while maintaining .\n  Second, We also propose to use multiple task training objective consists of\nmoment segmentation task, start/end distribution prediction and start/end\nlocation regression task. We have verified that start/end prediction are noisy\ndue to annotator disagreement and joint training with moment segmentation task\ncan provide richer information since frames inside the target clip are also\nutilized as positive training examples.\n  Third, we propose to use an early fusion approach, which achieves better\nperformance at the cost of inference time. However, the inference time will not\nbe a problem for our model since our model has a simple architecture which\nenables efficient training and inference.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 22:06:34 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 14:49:04 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Yu", "Xinli", ""], ["Malmir", "Mohsen", ""], ["He", "Cynthia", ""], ["Liu", "Yue", ""], ["Wu", "Rex", ""]]}, {"id": "2009.02418", "submitter": "Tom Grimes", "authors": "Tom Grimes, Eric Church, William Pitts, Lynn Wood", "title": "Explanation of Unintended Radiated Emission Classification via LIME", "comments": "7 pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unintended radiated emissions arise during the use of electronic devices.\nIdentifying and mitigating the effects of these emissions is a key element of\nmodern power engineering and associated control systems. Signal processing of\nthe electrical system can identify the sources of these emissions. A dataset\nknown as Flaming Moes includes captured unintended radiated emissions from\nconsumer electronics. This dataset was analyzed to construct next-generation\nmethods for device identification. To this end, a neural network based on\napplying the ResNet-18 image classification architecture to the short time\nFourier transforms of short segments of voltage signatures was constructed.\nUsing this classifier, the 18 device classes and background class were\nidentified with close to 100 percent accuracy. By applying LIME to this\nclassifier and aggregating the results over many classifications for the same\ndevice, it was possible to determine the frequency bands used by the classifier\nto make decisions. Using ensembles of classifiers trained on very similar\ndatasets from the same parent data distribution, it was possible to recover\nrobust sets of features of device output useful for identification. The\nadditional understanding provided by the application of LIME enhances the\ntrainability, trustability, and transferability of URE analysis networks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 23:14:50 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 16:37:29 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Grimes", "Tom", ""], ["Church", "Eric", ""], ["Pitts", "William", ""], ["Wood", "Lynn", ""]]}, {"id": "2009.02429", "submitter": "Alvin Chan", "authors": "Alvin Chan, Martin D. Levine, Mehrsan Javan", "title": "Player Identification in Hockey Broadcast Videos", "comments": null, "journal-ref": "Volume 165, 1 March 2021, 113891", "doi": "10.1016/j.eswa.2020.113891", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep recurrent convolutional neural network (CNN) approach to\nsolve the problem of hockey player identification in NHL broadcast videos.\nPlayer identification is a difficult computer vision problem mainly because of\nthe players' similar appearance, occlusion, and blurry facial and physical\nfeatures. However, we can observe players' jersey numbers over time by\nprocessing variable length image sequences of players (aka 'tracklets'). We\npropose an end-to-end trainable ResNet+LSTM network, with a residual network\n(ResNet) base and a long short-term memory (LSTM) layer, to discover\nspatio-temporal features of jersey numbers over time and learn long-term\ndependencies. For this work, we created a new hockey player tracklet dataset\nthat contains sequences of hockey player bounding boxes. Additionally, we\nemploy a secondary 1-dimensional convolutional neural network classifier as a\nlate score-level fusion method to classify the output of the ResNet+LSTM\nnetwork. This achieves an overall player identification accuracy score over 87%\non the test split of our new dataset.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 01:30:15 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 01:18:30 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Chan", "Alvin", ""], ["Levine", "Martin D.", ""], ["Javan", "Mehrsan", ""]]}, {"id": "2009.02437", "submitter": "Louise Gillian Bautista", "authors": "Louise Gillian C. Bautista and Prospero C. Naval Jr", "title": "GazeMAE: General Representations of Eye Movements using a Micro-Macro\n  Autoencoder", "comments": "Accepted to 25th International Conference on Pattern Recognition\n  (ICPR2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye movements are intricate and dynamic events that contain a wealth of\ninformation about the subject and the stimuli. We propose an abstract\nrepresentation of eye movements that preserve the important nuances in gaze\nbehavior while being stimuli-agnostic. We consider eye movements as raw\nposition and velocity signals and train separate deep temporal convolutional\nautoencoders. The autoencoders learn micro-scale and macro-scale\nrepresentations that correspond to the fast and slow features of eye movements.\nWe evaluate the joint representations with a linear classifier fitted on\nvarious classification tasks. Our work accurately discriminates between gender\nand age groups, and outperforms previous works on biometrics and stimuli\nclasification. Further experiments highlight the validity and generalizability\nof this method, bringing eye tracking research closer to real-world\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 02:13:42 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 06:02:31 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Bautista", "Louise Gillian C.", ""], ["Naval", "Prospero C.", "Jr"]]}, {"id": "2009.02455", "submitter": "Ashwin Raju", "authors": "Ashwin Raju, Zhanghexuan Ji, Chi Tung Cheng, Jinzheng Cai, Junzhou\n  Huang, Jing Xiao, Le Lu, ChienHung Liao, Adam P. Harrison", "title": "User-Guided Domain Adaptation for Rapid Annotation from User\n  Interactions: A Study on Pathological Liver Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mask-based annotation of medical images, especially for 3D data, is a\nbottleneck in developing reliable machine learning models. Using minimal-labor\nuser interactions (UIs) to guide the annotation is promising, but challenges\nremain on best harmonizing the mask prediction with the UIs. To address this,\nwe propose the user-guided domain adaptation (UGDA) framework, which uses\nprediction-based adversarial domain adaptation (PADA) to model the combined\ndistribution of UIs and mask predictions. The UIs are then used as anchors to\nguide and align the mask prediction. Importantly, UGDA can both learn from\nunlabelled data and also model the high-level semantic meaning behind different\nUIs. We test UGDA on annotating pathological livers using a clinically\ncomprehensive dataset of 927 patient studies. Using only extreme-point UIs, we\nachieve a mean (worst-case) performance of 96.1%(94.9%), compared to 93.0%\n(87.0%) for deep extreme points (DEXTR). Furthermore, we also show UGDA can\nretain this state-of-the-art performance even when only seeing a fraction of\navailable UIs, demonstrating an ability for robust and reliable UI-guided\nsegmentation with extremely minimal labor demands.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 04:24:58 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Raju", "Ashwin", ""], ["Ji", "Zhanghexuan", ""], ["Cheng", "Chi Tung", ""], ["Cai", "Jinzheng", ""], ["Huang", "Junzhou", ""], ["Xiao", "Jing", ""], ["Lu", "Le", ""], ["Liao", "ChienHung", ""], ["Harrison", "Adam P.", ""]]}, {"id": "2009.02470", "submitter": "Wei-An Lin", "authors": "Wei-An Lin, Chun Pong Lau, Alexander Levine, Rama Chellappa, Soheil\n  Feizi", "title": "Dual Manifold Adversarial Robustness: Defense against Lp and non-Lp\n  Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training is a popular defense strategy against attack threat\nmodels with bounded Lp norms. However, it often degrades the model performance\non normal images and the defense does not generalize well to novel attacks.\nGiven the success of deep generative models such as GANs and VAEs in\ncharacterizing the underlying manifold of images, we investigate whether or not\nthe aforementioned problems can be remedied by exploiting the underlying\nmanifold information. To this end, we construct an \"On-Manifold ImageNet\"\n(OM-ImageNet) dataset by projecting the ImageNet samples onto the manifold\nlearned by StyleGSN. For this dataset, the underlying manifold information is\nexact. Using OM-ImageNet, we first show that adversarial training in the latent\nspace of images improves both standard accuracy and robustness to on-manifold\nattacks. However, since no out-of-manifold perturbations are realized, the\ndefense can be broken by Lp adversarial attacks. We further propose Dual\nManifold Adversarial Training (DMAT) where adversarial perturbations in both\nlatent and image spaces are used in robustifying the model. Our DMAT improves\nperformance on normal images, and achieves comparable robustness to the\nstandard adversarial training against Lp attacks. In addition, we observe that\nmodels defended by DMAT achieve improved robustness against novel attacks which\nmanipulate images by global color shifts or various types of image filtering.\nInterestingly, similar improvements are also achieved when the defended models\nare tested on out-of-manifold natural images. These results demonstrate the\npotential benefits of using manifold information in enhancing robustness of\ndeep learning models against various types of novel adversarial attacks.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 06:00:28 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Lin", "Wei-An", ""], ["Lau", "Chun Pong", ""], ["Levine", "Alexander", ""], ["Chellappa", "Rama", ""], ["Feizi", "Soheil", ""]]}, {"id": "2009.02491", "submitter": "Ying Zhao", "authors": "Fangfang Zhou, Yong Zhao, Wenjiang Chen, Yijing Tan, Yaqi Xu, Yi Chen,\n  Chao Liu, Ying Zhao", "title": "Reverse-engineering Bar Charts Using Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reverse-engineering bar charts extracts textual and numeric information from\nthe visual representations of bar charts to support application scenarios that\nrequire the underlying information. In this paper, we propose a neural\nnetwork-based method for reverse-engineering bar charts. We adopt a neural\nnetwork-based object detection model to simultaneously localize and classify\ntextual information. This approach improves the efficiency of textual\ninformation extraction. We design an encoder-decoder framework that integrates\nconvolutional and recurrent neural networks to extract numeric information. We\nfurther introduce an attention mechanism into the framework to achieve high\naccuracy and robustness. Synthetic and real-world datasets are used to evaluate\nthe effectiveness of the method. To the best of our knowledge, this work takes\nthe lead in constructing a complete neural network-based method of\nreverse-engineering bar charts.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 08:14:35 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Zhou", "Fangfang", ""], ["Zhao", "Yong", ""], ["Chen", "Wenjiang", ""], ["Tan", "Yijing", ""], ["Xu", "Yaqi", ""], ["Chen", "Yi", ""], ["Liu", "Chao", ""], ["Zhao", "Ying", ""]]}, {"id": "2009.02516", "submitter": "Erico Tjoa", "authors": "Erico Tjoa, Guan Cuntai", "title": "Generalization on the Enhancement of Layerwise Relevance\n  Interpretability of Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The practical application of deep neural networks are still limited by their\nlack of transparency. One of the efforts to provide explanation for decisions\nmade by artificial intelligence (AI) is the use of saliency or heat maps\nhighlighting relevant regions that contribute significantly to its prediction.\nA layer-wise amplitude filtering method was previously introduced to improve\nthe quality of heatmaps, performing error corrections by noise-spike\nsuppression. In this study, we generalize the layerwise error correction by\nconsidering any identifiable error and assuming there exists a groundtruth\ninterpretable information. The forms of errors propagated through layerwise\nrelevance methods are studied and we propose a filtering technique for\ninterpretability signal rectification taylored to the trend of signal amplitude\nof the particular neural network used. Finally, we put forth arguments for the\nuse of groundtruth interpretable information.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 11:26:53 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 10:01:52 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Tjoa", "Erico", ""], ["Cuntai", "Guan", ""]]}, {"id": "2009.02523", "submitter": "Lili Huang", "authors": "Bo Jiang, Panpan Zhang, Lili Huang", "title": "Visual Object Tracking by Segmentation with Graph Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation-based tracking has been actively studied in computer vision and\nmultimedia. Superpixel based object segmentation and tracking methods are\nusually developed for this task. However, they independently perform feature\nrepresentation and learning of superpixels which may lead to sub-optimal\nresults. In this paper, we propose to utilize graph convolutional network (GCN)\nmodel for superpixel based object tracking. The proposed model provides a\ngeneral end-to-end framework which integrates i) label linear prediction, and\nii) structure-aware feature information of each superpixel together to obtain\nobject segmentation and further improves the performance of tracking. The main\nbenefits of the proposed GCN method have two main aspects. First, it provides\nan effective end-to-end way to exploit both spatial and temporal consistency\nconstraint for target object segmentation. Second, it utilizes a mixed graph\nconvolution module to learn a context-aware and discriminative feature for\nsuperpixel representation and labeling. An effective algorithm has been\ndeveloped to optimize the proposed model. Extensive experiments on five\ndatasets demonstrate that our method obtains better performance against\nexisting alternative methods.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 12:43:21 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 01:04:13 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Jiang", "Bo", ""], ["Zhang", "Panpan", ""], ["Huang", "Lili", ""]]}, {"id": "2009.02564", "submitter": "Haochuan Jiang", "authors": "Haochuan Jiang, Agisilaos Chartsias, Xinheng Zhang, Giorgos\n  Papanastasiou, Scott Semple, Mark Dweck, David Semple, Rohan Dharmakumar,\n  Sotirios A. Tsaftaris", "title": "Semi-supervised Pathology Segmentation with Disentangled Representations", "comments": "12 Pages, 4 figures", "journal-ref": "MICCAI-2020 DART workshop", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated pathology segmentation remains a valuable diagnostic tool in\nclinical practice. However, collecting training data is challenging.\nSemi-supervised approaches by combining labelled and unlabelled data can offer\na solution to data scarcity. An approach to semi-supervised learning relies on\nreconstruction objectives (as self-supervision objectives) that learns in a\njoint fashion suitable representations for the task. Here, we propose\nAnatomy-Pathology Disentanglement Network (APD-Net), a pathology segmentation\nmodel that attempts to learn jointly for the first time: disentanglement of\nanatomy, modality, and pathology. The model is trained in a semi-supervised\nfashion with new reconstruction losses directly aiming to improve pathology\nsegmentation with limited annotations. In addition, a joint optimization\nstrategy is proposed to fully take advantage of the available annotations. We\nevaluate our methods with two private cardiac infarction segmentation datasets\nwith LGE-MRI scans. APD-Net can perform pathology segmentation with few\nannotations, maintain performance with different amounts of supervision, and\noutperform related deep learning methods.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 17:07:59 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Jiang", "Haochuan", ""], ["Chartsias", "Agisilaos", ""], ["Zhang", "Xinheng", ""], ["Papanastasiou", "Giorgos", ""], ["Semple", "Scott", ""], ["Dweck", "Mark", ""], ["Semple", "David", ""], ["Dharmakumar", "Rohan", ""], ["Tsaftaris", "Sotirios A.", ""]]}, {"id": "2009.02565", "submitter": "Borneel Phukan", "authors": "Borneel Bikash Phukan, Amiya Ranjan Panda", "title": "An Efficient Technique for Image Captioning using Deep Neural Network", "comments": "5 pages, 10 figures, Under review by an internationally recognized\n  Scopus indexed journal 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the huge expansion of internet and trillions of gigabytes of data\ngenerated every single day, the needs for the development of various tools has\nbecome mandatory in order to maintain system adaptability to rapid changes. One\nof these tools is known as Image Captioning. Every entity in internet must be\nproperly identified and managed and therefore in the case of image data,\nautomatic captioning for identification is required. Similarly, content\ngeneration for missing labels, image classification and artificial languages\nall requires the process of Image Captioning. This paper discusses an efficient\nand unique way to perform automatic image captioning on individual image and\ndiscusses strategies to improve its performances and functionalities.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 17:11:44 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Phukan", "Borneel Bikash", ""], ["Panda", "Amiya Ranjan", ""]]}, {"id": "2009.02568", "submitter": "Allen Lee", "authors": "Anelise Newman, Camilo Fosco, Vincent Casser, Allen Lee, Barry\n  McNamara, and Aude Oliva", "title": "Multimodal Memorability: Modeling Effects of Semantics and Decay on\n  Video Memorability", "comments": "European Conference on Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key capability of an intelligent system is deciding when events from past\nexperience must be remembered and when they can be forgotten. Towards this\ngoal, we develop a predictive model of human visual event memory and how those\nmemories decay over time. We introduce Memento10k, a new, dynamic video\nmemorability dataset containing human annotations at different viewing delays.\nBased on our findings we propose a new mathematical formulation of memorability\ndecay, resulting in a model that is able to produce the first quantitative\nestimation of how a video decays in memory over time. In contrast with previous\nwork, our model can predict the probability that a video will be remembered at\nan arbitrary delay. Importantly, our approach combines visual and semantic\ninformation (in the form of textual captions) to fully represent the meaning of\nevents. Our experiments on two video memorability benchmarks, including\nMemento10k, show that our model significantly improves upon the best prior\napproach (by 12% on average).\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 17:24:02 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Newman", "Anelise", ""], ["Fosco", "Camilo", ""], ["Casser", "Vincent", ""], ["Lee", "Allen", ""], ["McNamara", "Barry", ""], ["Oliva", "Aude", ""]]}, {"id": "2009.02577", "submitter": "Ke Yan", "authors": "Ke Yan, Jinzheng Cai, Youjing Zheng, Adam P. Harrison, Dakai Jin,\n  Youbao Tang, Yuxing Tang, Lingyun Huang, Jing Xiao, Le Lu", "title": "Learning from Multiple Datasets with Heterogeneous and Partial Labels\n  for Universal Lesion Detection in CT", "comments": "IEEE Trans on Medical Imaging. Annotation and evaluation code in\n  https://github.com/viggin/DeepLesion_manual_test_set", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale datasets with high-quality labels are desired for training\naccurate deep learning models. However, due to the annotation cost, datasets in\nmedical imaging are often either partially-labeled or small. For example,\nDeepLesion is such a large-scale CT image dataset with lesions of various\ntypes, but it also has many unlabeled lesions (missing annotations). When\ntraining a lesion detector on a partially-labeled dataset, the missing\nannotations will generate incorrect negative signals and degrade the\nperformance. Besides DeepLesion, there are several small single-type datasets,\nsuch as LUNA for lung nodules and LiTS for liver tumors. These datasets have\nheterogeneous label scopes, i.e., different lesion types are labeled in\ndifferent datasets with other types ignored. In this work, we aim to develop a\nuniversal lesion detection algorithm to detect a variety of lesions. The\nproblem of heterogeneous and partial labels is tackled. First, we build a\nsimple yet effective lesion detection framework named Lesion ENSemble (LENS).\nLENS can efficiently learn from multiple heterogeneous lesion datasets in a\nmulti-task fashion and leverage their synergy by proposal fusion. Next, we\npropose strategies to mine missing annotations from partially-labeled datasets\nby exploiting clinical prior knowledge and cross-dataset knowledge transfer.\nFinally, we train our framework on four public lesion datasets and evaluate it\non 800 manually-labeled sub-volumes in DeepLesion. Our method brings a relative\nimprovement of 49% compared to the current state-of-the-art approach in the\nmetric of average sensitivity. We have publicly released our manual 3D\nannotations of DeepLesion in\nhttps://github.com/viggin/DeepLesion_manual_test_set.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 17:55:21 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 23:23:50 GMT"}, {"version": "v3", "created": "Sun, 3 Jan 2021 18:55:59 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Yan", "Ke", ""], ["Cai", "Jinzheng", ""], ["Zheng", "Youjing", ""], ["Harrison", "Adam P.", ""], ["Jin", "Dakai", ""], ["Tang", "Youbao", ""], ["Tang", "Yuxing", ""], ["Huang", "Lingyun", ""], ["Xiao", "Jing", ""], ["Lu", "Le", ""]]}, {"id": "2009.02582", "submitter": "Shachar Ben Dayan", "authors": "Shachar Ben Dayan, David Mendlovic and Raja Giryes", "title": "Deep Sparse Light Field Refocusing", "comments": "Published at BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field photography enables to record 4D images, containing angular\ninformation alongside spatial information of the scene. One of the important\napplications of light field imaging is post-capture refocusing. Current methods\nrequire for this purpose a dense field of angle views; those can be acquired\nwith a micro-lens system or with a compressive system. Both techniques have\nmajor drawbacks to consider, including bulky structures and angular-spatial\nresolution trade-off. We present a novel implementation of digital refocusing\nbased on sparse angular information using neural networks. This allows\nrecording high spatial resolution in favor of the angular resolution, thus,\nenabling to design compact and simple devices with improved hardware as well as\nbetter performance of compressive systems. We use a novel convolutional neural\nnetwork whose relatively small structure enables fast reconstruction with low\nmemory consumption. Moreover, it allows handling without re-training various\nrefocusing ranges and noise levels. Results show major improvement compared to\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 18:34:55 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Dayan", "Shachar Ben", ""], ["Mendlovic", "David", ""], ["Giryes", "Raja", ""]]}, {"id": "2009.02650", "submitter": "Chaoxing Huang", "authors": "Chaoxing Huang, Xuanying Zhu, Tom Gedeon", "title": "A Genetic Feature Selection Based Two-stream Neural Network for Anger\n  Veracity Recognition", "comments": "This paper has been accepted by the 27th International Conference on\n  Neural Information Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People can manipulate emotion expressions when interacting with others. For\nexample, acted anger can be expressed when stimuli is not genuinely angry with\nan aim to manipulate the observer. In this paper, we aim to examine if the\nveracity of anger can be recognized with observers' pupillary data with\ncomputational approaches. We use Genetic-based Feature Selection (GFS) methods\nto select time-series pupillary features of of observers who observe acted and\ngenuine anger of the video stimuli. We then use the selected features to train\na simple fully connected neural work and a two-stream neural network. Our\nresults show that the two-stream architecture is able to achieve a promising\nrecognition result with an accuracy of 93.58% when the pupillary responses from\nboth eyes are available. It also shows that genetic algorithm based feature\nselection method can effectively improve the classification accuracy by 3.07%.\nWe hope our work could help daily research such as human machine interaction\nand psychology studies that require emotion recognition .\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 05:52:41 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 12:43:56 GMT"}, {"version": "v3", "created": "Sat, 12 Sep 2020 03:03:43 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Huang", "Chaoxing", ""], ["Zhu", "Xuanying", ""], ["Gedeon", "Tom", ""]]}, {"id": "2009.02653", "submitter": "Jiang Lu", "authors": "Jiang Lu, Pinghua Gong, Jieping Ye, and Changshui Zhang", "title": "Learning from Very Few Samples: A Survey", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few sample learning (FSL) is significant and challenging in the field of\nmachine learning. The capability of learning and generalizing from very few\nsamples successfully is a noticeable demarcation separating artificial\nintelligence and human intelligence since humans can readily establish their\ncognition to novelty from just a single or a handful of examples whereas\nmachine learning algorithms typically entail hundreds or thousands of\nsupervised samples to guarantee generalization ability. Despite the long\nhistory dated back to the early 2000s and the widespread attention in recent\nyears with booming deep learning technologies, little surveys or reviews for\nFSL are available until now. In this context, we extensively review 300+ papers\nof FSL spanning from the 2000s to 2019 and provide a timely and comprehensive\nsurvey for FSL. In this survey, we review the evolution history as well as the\ncurrent progress on FSL, categorize FSL approaches into the generative model\nbased and discriminative model based kinds in principle, and emphasize\nparticularly on the meta learning based FSL approaches. We also summarize\nseveral recently emerging extensional topics of FSL and review the latest\nadvances on these topics. Furthermore, we highlight the important FSL\napplications covering many research hotspots in computer vision, natural\nlanguage processing, audio and speech, reinforcement learning and robotic, data\nanalysis, etc. Finally, we conclude the survey with a discussion on promising\ntrends in the hope of providing guidance and insights to follow-up researches.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 06:13:09 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 14:21:57 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Lu", "Jiang", ""], ["Gong", "Pinghua", ""], ["Ye", "Jieping", ""], ["Zhang", "Changshui", ""]]}, {"id": "2009.02672", "submitter": "Ke Wang", "authors": "Ke Wang, Sai Ma, Junlan Chen, Fan Ren", "title": "Approaches, Challenges, and Applications for Deep Visual Odometry:\n  Toward to Complicated and Emerging Areas", "comments": null, "journal-ref": "IEEE Transactions on Cognitive and Developmental Systems. 2020", "doi": "10.1109/TCDS.2020.3038898", "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual odometry (VO) is a prevalent way to deal with the relative\nlocalization problem, which is becoming increasingly mature and accurate, but\nit tends to be fragile under challenging environments. Comparing with classical\ngeometry-based methods, deep learning-based methods can automatically learn\neffective and robust representations, such as depth, optical flow, feature,\nego-motion, etc., from data without explicit computation. Nevertheless, there\nstill lacks a thorough review of the recent advances of deep learning-based VO\n(Deep VO). Therefore, this paper aims to gain a deep insight on how deep\nlearning can profit and optimize the VO systems. We first screen out a number\nof qualifications including accuracy, efficiency, scalability, dynamicity,\npracticability, and extensibility, and employ them as the criteria. Then, using\nthe offered criteria as the uniform measurements, we detailedly evaluate and\ndiscuss how deep learning improves the performance of VO from the aspects of\ndepth estimation, feature extraction and matching, pose estimation. We also\nsummarize the complicated and emerging areas of Deep VO, such as mobile robots,\nmedical robots, augmented reality and virtual reality, etc. Through the\nliterature decomposition, analysis, and comparison, we finally put forward a\nnumber of open issues and raise some future research directions in this field.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 08:25:23 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Wang", "Ke", ""], ["Ma", "Sai", ""], ["Chen", "Junlan", ""], ["Ren", "Fan", ""]]}, {"id": "2009.02704", "submitter": "Zhen Yuan", "authors": "Zhen Yuan, Esther Puyol-Anton, Haran Jogeesvaran, Catriona Reid, Baba\n  Inusa, Andrew P. King", "title": "Deep Learning for Automatic Spleen Length Measurement in Sickle Cell\n  Disease Patients", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sickle Cell Disease (SCD) is one of the most common genetic diseases in the\nworld. Splenomegaly (abnormal enlargement of the spleen) is frequent among\nchildren with SCD. If left untreated, splenomegaly can be life-threatening. The\ncurrent workflow to measure spleen size includes palpation, possibly followed\nby manual length measurement in 2D ultrasound imaging. However, this manual\nmeasurement is dependent on operator expertise and is subject to intra- and\ninter-observer variability. We investigate the use of deep learning to perform\nautomatic estimation of spleen length from ultrasound images. We investigate\ntwo types of approach, one segmentation-based and one based on direct length\nestimation, and compare the results against measurements made by human experts.\nOur best model (segmentation-based) achieved a percentage length error of\n7.42%, which is approaching the level of inter-observer variability\n(5.47%-6.34%). To the best of our knowledge, this is the first attempt to\nmeasure spleen size in a fully automated way from ultrasound images.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 10:47:49 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Yuan", "Zhen", ""], ["Puyol-Anton", "Esther", ""], ["Jogeesvaran", "Haran", ""], ["Reid", "Catriona", ""], ["Inusa", "Baba", ""], ["King", "Andrew P.", ""]]}, {"id": "2009.02711", "submitter": "Tsaipei Wang", "authors": "Sheng-Ho Chiang, Tsaipei Wang, Yi-Fu Chen", "title": "Efficient Pedestrian Detection in Top-View Fisheye Images Using\n  Compositions of Perspective View Patches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection in images is a topic that has been studied extensively,\nbut existing detectors designed for perspective images do not perform as\nsuccessfully on images taken with top-view fisheye cameras, mainly due to the\norientation variation of people in such images. In our proposed approach,\nseveral perspective views are generated from a fisheye image and then\nconcatenated to form a composite image. As pedestrians in this composite image\nare more likely to be upright, existing detectors designed and trained for\nperspective images can be applied directly without additional training. We also\ndescribe a new method of mapping detection bounding boxes from the perspective\nviews to the fisheye frame. The detection performance on several public\ndatasets compare favorably with state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 11:19:00 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 04:48:30 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Chiang", "Sheng-Ho", ""], ["Wang", "Tsaipei", ""], ["Chen", "Yi-Fu", ""]]}, {"id": "2009.02715", "submitter": "Hugo Bertiche", "authors": "Hugo Bertiche, Meysam Madadi, Emilio Tylson and Sergio Escalera", "title": "DeePSD: Automatic Deep Skinning And Pose Space Deformation For 3D\n  Garment Animation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel solution to the garment animation problem through deep\nlearning. Our contribution allows animating any template outfit with arbitrary\ntopology and geometric complexity. Recent works develop models for garment\nedition, resizing and animation at the same time by leveraging the support body\nmodel (encoding garments as body homotopies). This leads to complex engineering\nsolutions that suffer from scalability, applicability and compatibility. By\nlimiting our scope to garment animation only, we are able to propose a simple\nmodel that can animate any outfit, independently of its topology, vertex order\nor connectivity. Our proposed architecture maps outfits to animated 3D models\ninto the standard format for 3D animation (blend weights and blend shapes\nmatrices), automatically providing of compatibility with any graphics engine.\nWe also propose a methodology to complement supervised learning with an\nunsupervised physically based learning that implicitly solves collisions and\nenhances cloth quality.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 11:52:17 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 09:19:24 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Bertiche", "Hugo", ""], ["Madadi", "Meysam", ""], ["Tylson", "Emilio", ""], ["Escalera", "Sergio", ""]]}, {"id": "2009.02738", "submitter": "Chuanxi Chen", "authors": "Dengpan Ye, Chuanxi Chen, Changrui Liu, Hao Wang, Shunzhi Jiang", "title": "Detection Defense Against Adversarial Attacks with Saliency Map", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well established that neural networks are vulnerable to adversarial\nexamples, which are almost imperceptible on human vision and can cause the deep\nmodels misbehave. Such phenomenon may lead to severely inestimable consequences\nin the safety and security critical applications. Existing defenses are trend\nto harden the robustness of models against adversarial attacks, e.g.,\nadversarial training technology. However, these are usually intractable to\nimplement due to the high cost of re-training and the cumbersome operations of\naltering the model architecture or parameters. In this paper, we discuss the\nsaliency map method from the view of enhancing model interpretability, it is\nsimilar to introducing the mechanism of the attention to the model, so as to\ncomprehend the progress of object identification by the deep networks. We then\npropose a novel method combined with additional noises and utilize the\ninconsistency strategy to detect adversarial examples. Our experimental results\nof some representative adversarial attacks on common datasets including\nImageNet and popular models show that our method can detect all the attacks\nwith high detection success rate effectively. We compare it with the existing\nstate-of-the-art technique, and the experiments indicate that our method is\nmore general.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 13:57:17 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Ye", "Dengpan", ""], ["Chen", "Chuanxi", ""], ["Liu", "Changrui", ""], ["Wang", "Hao", ""], ["Jiang", "Shunzhi", ""]]}, {"id": "2009.02755", "submitter": "Boris Lorbeer", "authors": "Boris Lorbeer, Max Botler", "title": "Anomaly Detection With Partitioning Overfitting Autoencoder Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose POTATOES (Partitioning OverfiTting AuTOencoder\nEnSemble), a new method for unsupervised outlier detection (UOD). More\nprecisely, given any autoencoder for UOD, this technique can be used to improve\nits accuracy while at the same time removing the burden of tuning its\nregularization. The idea is to not regularize at all, but to rather randomly\npartition the data into sufficiently many equally sized parts, overfit each\npart with its own autoencoder, and to use the maximum over all autoencoder\nreconstruction errors as the anomaly score. We apply our model to various\nrealistic datasets and show that if the set of inliers is dense enough, our\nmethod indeed improves the UOD performance of a given autoencoder\nsignificantly. For reproducibility, the code is made available on github so the\nreader can recreate the results in this paper as well as apply the method to\nother autoencoders and datasets.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 15:35:53 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 14:58:15 GMT"}, {"version": "v3", "created": "Sat, 26 Sep 2020 07:53:58 GMT"}, {"version": "v4", "created": "Thu, 1 Oct 2020 05:55:18 GMT"}, {"version": "v5", "created": "Wed, 28 Apr 2021 14:23:07 GMT"}, {"version": "v6", "created": "Sun, 4 Jul 2021 05:14:50 GMT"}, {"version": "v7", "created": "Fri, 9 Jul 2021 04:40:04 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Lorbeer", "Boris", ""], ["Botler", "Max", ""]]}, {"id": "2009.02759", "submitter": "Yongxiang Huang", "authors": "Yongxiang Huang and Albert C. S. Chung", "title": "Edge-variational Graph Convolutional Networks for Uncertainty-aware\n  Disease Prediction", "comments": "Accepted to MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a rising need for computational models that can complementarily\nleverage data of different modalities while investigating associations between\nsubjects for population-based disease analysis. Despite the success of\nconvolutional neural networks in representation learning for imaging data, it\nis still a very challenging task. In this paper, we propose a generalizable\nframework that can automatically integrate imaging data with non-imaging data\nin populations for uncertainty-aware disease prediction. At its core is a\nlearnable adaptive population graph with variational edges, which we\nmathematically prove that it is optimizable in conjunction with graph\nconvolutional neural networks. To estimate the predictive uncertainty related\nto the graph topology, we propose the novel concept of Monte-Carlo edge\ndropout. Experimental results on four databases show that our method can\nconsistently and significantly improve the diagnostic accuracy for Autism\nspectrum disorder, Alzheimer's disease, and ocular diseases, indicating its\ngeneralizability in leveraging multimodal data for computer-aided diagnosis.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 15:53:17 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Huang", "Yongxiang", ""], ["Chung", "Albert C. S.", ""]]}, {"id": "2009.02773", "submitter": "Zinan Lin", "authors": "Zinan Lin, Vyas Sekar, Giulia Fanti", "title": "Why Spectral Normalization Stabilizes GANs: Analysis and Improvements", "comments": "54 pages, 74 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral normalization (SN) is a widely-used technique for improving the\nstability and sample quality of Generative Adversarial Networks (GANs).\nHowever, there is currently limited understanding of why SN is effective. In\nthis work, we show that SN controls two important failure modes of GAN\ntraining: exploding and vanishing gradients. Our proofs illustrate a (perhaps\nunintentional) connection with the successful LeCun initialization. This\nconnection helps to explain why the most popular implementation of SN for GANs\nrequires no hyper-parameter tuning, whereas stricter implementations of SN have\npoor empirical performance out-of-the-box. Unlike LeCun initialization which\nonly controls gradient vanishing at the beginning of training, SN preserves\nthis property throughout training. Building on this theoretical understanding,\nwe propose a new spectral normalization technique: Bidirectional Scaled\nSpectral Normalization (BSSN), which incorporates insights from later\nimprovements to LeCun initialization: Xavier initialization and Kaiming\ninitialization. Theoretically, we show that BSSN gives better gradient control\nthan SN. Empirically, we demonstrate that it outperforms SN in sample quality\nand training stability on several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 16:51:42 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 00:29:30 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Lin", "Zinan", ""], ["Sekar", "Vyas", ""], ["Fanti", "Giulia", ""]]}, {"id": "2009.02796", "submitter": "Peirong Liu", "authors": "Peirong Liu, Yueh Z. Lee, Stephen R. Aylward, and Marc Niethammer", "title": "Perfusion Imaging: A Data Assimilation Approach", "comments": "Submitted to IEEE-TMI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perfusion imaging (PI) is clinically used to assess strokes and brain tumors.\nCommonly used PI approaches based on magnetic resonance imaging (MRI) or\ncomputed tomography (CT) measure the effect of a contrast agent moving through\nblood vessels and into tissue. Contrast-agent free approaches, for example,\nbased on intravoxel incoherent motion, also exist, but are so far not routinely\nused clinically. These methods rely on estimating on the arterial input\nfunction (AIF) to approximately model tissue perfusion, neglecting spatial\ndependencies, and reliably estimating the AIF is also non-trivial, leading to\ndifficulties with standardizing perfusion measures. In this work we therefore\npropose a data-assimilation approach (PIANO) which estimates the velocity and\ndiffusion fields of an advection-diffusion model that best explains the\ncontrast dynamics. PIANO accounts for spatial dependencies and neither requires\nestimating the AIF nor relies on a particular contrast agent bolus shape.\nSpecifically, we propose a convenient parameterization of the estimation\nproblem, a numerical estimation approach, and extensively evaluate PIANO. We\ndemonstrate that PIANO can successfully resolve velocity and diffusion field\nambiguities and results in sensitive measures for the assessment of stroke,\ncomparing favorably to conventional measures of perfusion.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 18:44:44 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Liu", "Peirong", ""], ["Lee", "Yueh Z.", ""], ["Aylward", "Stephen R.", ""], ["Niethammer", "Marc", ""]]}, {"id": "2009.02797", "submitter": "Peirong Liu", "authors": "Peirong Liu, Zhengwang Wu, Gang Li, Pew-Thian Yap and Dinggang Shen", "title": "Deep Modeling of Growth Trajectories for Longitudinal Prediction of\n  Missing Infant Cortical Surfaces", "comments": "Accepted as oral presentation at IPMI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Charting cortical growth trajectories is of paramount importance for\nunderstanding brain development. However, such analysis necessitates the\ncollection of longitudinal data, which can be challenging due to subject\ndropouts and failed scans. In this paper, we will introduce a method for\nlongitudinal prediction of cortical surfaces using a spatial graph\nconvolutional neural network (GCNN), which extends conventional CNNs from\nEuclidean to curved manifolds. The proposed method is designed to model the\ncortical growth trajectories and jointly predict inner and outer cortical\nsurfaces at multiple time points. Adopting a binary flag in loss calculation to\ndeal with missing data, we fully utilize all available cortical surfaces for\ntraining our deep learning model, without requiring a complete collection of\nlongitudinal data. Predicting the surfaces directly allows cortical attributes\nsuch as cortical thickness, curvature, and convexity to be computed for\nsubsequent analysis. We will demonstrate with experimental results that our\nmethod is capable of capturing the nonlinearity of spatiotemporal cortical\ngrowth patterns and can predict cortical surfaces with improved accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 18:46:04 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 03:34:36 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Liu", "Peirong", ""], ["Wu", "Zhengwang", ""], ["Li", "Gang", ""], ["Yap", "Pew-Thian", ""], ["Shen", "Dinggang", ""]]}, {"id": "2009.02805", "submitter": "Ayat Abedalla", "authors": "Ayat Abedalla, Malak Abdullah, Mahmoud Al-Ayyoub, Elhadj Benkhelifa", "title": "The 2ST-UNet for Pneumothorax Segmentation in Chest X-Rays using\n  ResNet34 as a Backbone for U-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pneumothorax, also called a collapsed lung, refers to the presence of the air\nin the pleural space between the lung and chest wall. It can be small (no need\nfor treatment), or large and causes death if it is not identified and treated\non time. It is easily seen and identified by experts using a chest X-ray.\nAlthough this method is mostly error-free, it is time-consuming and needs\nexpert radiologists. Recently, Computer Vision has been providing great\nassistance in detecting and segmenting pneumothorax. In this paper, we propose\na 2-Stage Training system (2ST-UNet) to segment images with pneumothorax. This\nsystem is built based on U-Net with Residual Networks (ResNet-34) backbone that\nis pre-trained on the ImageNet dataset. We start with training the network at a\nlower resolution before we load the trained model weights to retrain the\nnetwork with a higher resolution. Moreover, we utilize different techniques\nincluding Stochastic Weight Averaging (SWA), data augmentation, and Test-Time\nAugmentation (TTA). We use the chest X-ray dataset that is provided by the 2019\nSIIM-ACR Pneumothorax Segmentation Challenge, which contains 12,047 training\nimages and 3,205 testing images. Our experiments show that 2-Stage Training\nleads to better and faster network convergence. Our method achieves 0.8356 mean\nDice Similarity Coefficient (DSC) placing it among the top 9% of models with a\nrank of 124 out of 1,475.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 19:39:05 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Abedalla", "Ayat", ""], ["Abdullah", "Malak", ""], ["Al-Ayyoub", "Mahmoud", ""], ["Benkhelifa", "Elhadj", ""]]}, {"id": "2009.02819", "submitter": "Artem Sevastopolsky", "authors": "Maria Kolos, Artem Sevastopolsky, Victor Lempitsky", "title": "TRANSPR: Transparency Ray-Accumulating Neural 3D Scene Point Renderer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and evaluate a neural point-based graphics method that can model\nsemi-transparent scene parts. Similarly to its predecessor pipeline, ours uses\npoint clouds to model proxy geometry, and augments each point with a neural\ndescriptor. Additionally, a learnable transparency value is introduced in our\napproach for each point.\n  Our neural rendering procedure consists of two steps. Firstly, the point\ncloud is rasterized using ray grouping into a multi-channel image. This is\nfollowed by the neural rendering step that \"translates\" the rasterized image\ninto an RGB output using a learnable convolutional network. New scenes can be\nmodeled using gradient-based optimization of neural descriptors and of the\nrendering network.\n  We show that novel views of semi-transparent point cloud scenes can be\ngenerated after training with our approach. Our experiments demonstrate the\nbenefit of introducing semi-transparency into the neural point-based modeling\nfor a range of scenes with semi-transparent parts.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 21:19:18 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Kolos", "Maria", ""], ["Sevastopolsky", "Artem", ""], ["Lempitsky", "Victor", ""]]}, {"id": "2009.02827", "submitter": "Po Yang Dr", "authors": "Po Yang, Jun Qi, Xulong Wang, Yun Yang", "title": "MFL_COVID19: Quantifying Country-based Factors affecting Case Fatality\n  Rate in Early Phase of COVID-19 Epidemic via Regularised Multi-task Feature\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent outbreak of COVID-19 has led a rapid global spread around the world.\nMany countries have implemented timely intensive suppression to minimize the\ninfections, but resulted in high case fatality rate (CFR) due to critical\ndemand of health resources. Other country-based factors such as sociocultural\nissues, ageing population etc., has also influenced practical effectiveness of\ntaking interventions to improve morality in early phase. To better understand\nthe relationship of these factors across different countries with COVID-19 CFR\nis of primary importance to prepare for potentially second wave of COVID-19\ninfections. In the paper, we propose a novel regularized multi-task learning\nbased factor analysis approach for quantifying country-based factors affecting\nCFR in early phase of COVID-19 epidemic. We formulate the prediction of CFR\nprogression as a ML regression problem with observed CFR and other\ncountries-based factors. In this formulation, all CFR related factors were\ncategorized into 6 sectors with 27 indicators. We proposed a hybrid feature\nselection method combining filter, wrapper and tree-based models to calibrate\ninitial factors for a preliminary feature interaction. Then we adopted two\ntypical single task model (Ridge and Lasso regression) and one state-of-the-art\nMTFL method (fused sparse group lasso) in our formulation. The fused sparse\ngroup Lasso (FSGL) method allows the simultaneous selection of a common set of\ncountry-based factors for multiple time points of COVID-19 epidemic and also\nenables incorporating temporal smoothness of each factor over the whole early\nphase period. Finally, we proposed one novel temporal voting feature selection\nscheme to balance the weight instability of multiple factors in our MTFL model.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 22:34:14 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Yang", "Po", ""], ["Qi", "Jun", ""], ["Wang", "Xulong", ""], ["Yang", "Yun", ""]]}, {"id": "2009.02831", "submitter": "Chenyu You", "authors": "Chenyu You, Junlin Yang, Julius Chapiro, James S. Duncan", "title": "Unsupervised Wasserstein Distance Guided Domain Adaptation for 3D\n  Multi-Domain Liver Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have shown exceptional learning capability and\ngeneralizability in the source domain when massive labeled data is provided.\nHowever, the well-trained models often fail in the target domain due to the\ndomain shift. Unsupervised domain adaptation aims to improve network\nperformance when applying robust models trained on medical images from source\ndomains to a new target domain. In this work, we present an approach based on\nthe Wasserstein distance guided disentangled representation to achieve 3D\nmulti-domain liver segmentation. Concretely, we embed images onto a shared\ncontent space capturing shared feature-level information across domains and\ndomain-specific appearance spaces. The existing mutual information-based\nrepresentation learning approaches often fail to capture complete\nrepresentations in multi-domain medical imaging tasks. To mitigate these\nissues, we utilize Wasserstein distance to learn more complete representation,\nand introduces a content discriminator to further facilitate the representation\ndisentanglement. Experiments demonstrate that our method outperforms the\nstate-of-the-art on the multi-modality liver segmentation task.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 23:48:27 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["You", "Chenyu", ""], ["Yang", "Junlin", ""], ["Chapiro", "Julius", ""], ["Duncan", "James S.", ""]]}, {"id": "2009.02857", "submitter": "Dongho Choi", "authors": "Dongho Choi", "title": "3D Room Layout Estimation Beyond the Manhattan World Assumption", "comments": "3rd Place @ ECCV 2020 Holistic Scene Structures for 3D Vision\n  Workshop Challenges Track 1; 6 pages with 3 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting 3D room layout from single image is a challenging task with many\napplications. In this paper, we propose a new training and post-processing\nmethod for 3D room layout estimation, built on a recent state-of-the-art 3D\nroom layout estimation model. Experimental results show our method outperforms\nstate-of-the-art approaches by a large margin in predicting visible room\nlayout. Our method has obtained the 3rd place in 2020 Holistic Scene Structures\nfor 3D Vision Workshop.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 02:14:29 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Choi", "Dongho", ""]]}, {"id": "2009.02862", "submitter": "Xinge Zhu", "authors": "Hang Yang, Shan Jiang, Xinge Zhu, Mingyang Huang, Zhiqiang Shen,\n  Chunxiao Liu, Jianping Shi", "title": "Channel-wise Alignment for Adaptive Object Detection", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generic object detection has been immensely promoted by the development of\ndeep convolutional neural networks in the past decade. However, in the domain\nshift circumstance, the changes in weather, illumination, etc., often cause\ndomain gap, and thus performance drops substantially when detecting objects\nfrom one domain to another. Existing methods on this task usually draw\nattention on the high-level alignment based on the whole image or object of\ninterest, which naturally, cannot fully utilize the fine-grained channel\ninformation. In this paper, we realize adaptation from a thoroughly different\nperspective, i.e., channel-wise alignment. Motivated by the finding that each\nchannel focuses on a specific pattern (e.g., on special semantic regions, such\nas car), we aim to align the distribution of source and target domain on the\nchannel level, which is finer for integration between discrepant domains. Our\nmethod mainly consists of self channel-wise and cross channel-wise alignment.\nThese two parts explore the inner-relation and cross-relation of attention\nregions implicitly from the view of channels. Further more, we also propose a\nRPN domain classifier module to obtain a domain-invariant RPN network.\nExtensive experiments show that the proposed method performs notably better\nthan existing methods with about 5% improvement under various domain-shift\nsettings. Experiments on different task (e.g. instance segmentation) also\ndemonstrate its good scalability.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 02:42:18 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Yang", "Hang", ""], ["Jiang", "Shan", ""], ["Zhu", "Xinge", ""], ["Huang", "Mingyang", ""], ["Shen", "Zhiqiang", ""], ["Liu", "Chunxiao", ""], ["Shi", "Jianping", ""]]}, {"id": "2009.02869", "submitter": "Zezhou Sun", "authors": "Zezhou Sun, Banghe Wu, Cheng-Zhong Xu, Sanjay E. Sarma, Jian Yang, and\n  Hui Kong", "title": "Frontier Detection and Reachability Analysis for Efficient 2D Graph-SLAM\n  Based Active Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an integrated approach to active exploration by exploiting the\nCartographer method as the base SLAM module for submap creation and performing\nefficient frontier detection in the geometrically co-aligned submaps induced by\ngraph optimization. We also carry out analysis on the reachability of frontiers\nand their clusters to ensure that the detected frontier can be reached by\nrobot. Our method is tested on a mobile robot in real indoor scene to\ndemonstrate the effectiveness and efficiency of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 03:13:47 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Sun", "Zezhou", ""], ["Wu", "Banghe", ""], ["Xu", "Cheng-Zhong", ""], ["Sarma", "Sanjay E.", ""], ["Yang", "Jian", ""], ["Kong", "Hui", ""]]}, {"id": "2009.02878", "submitter": "Shireen Elhabian", "authors": "Anupama Goparaju, Alexandre Bone, Nan Hu, Heath B. Henninger, Andrew\n  E. Anderson, Stanley Durrleman, Matthijs Jacxsens, Alan Morris, Ibolya Csecs,\n  Nassir Marrouche, Shireen Y. Elhabian", "title": "Benchmarking off-the-shelf statistical shape modeling tools in clinical\n  applications", "comments": "22 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical shape modeling (SSM) is widely used in biology and medicine as a\nnew generation of morphometric approaches for the quantitative analysis of\nanatomical shapes. Technological advancements of in vivo imaging have led to\nthe development of open-source computational tools that automate the modeling\nof anatomical shapes and their population-level variability. However, little\nwork has been done on the evaluation and validation of such tools in clinical\napplications that rely on morphometric quantifications (e.g., implant design\nand lesion screening). Here, we systematically assess the outcome of widely\nused, state-of-the-art SSM tools, namely ShapeWorks, Deformetrica, and\nSPHARM-PDM. We use both quantitative and qualitative metrics to evaluate shape\nmodels from different tools. We propose validation frameworks for anatomical\nlandmark/measurement inference and lesion screening. We also present a lesion\nscreening method to objectively characterize subtle abnormal shape changes with\nrespect to learned population-level statistics of controls. Results demonstrate\nthat SSM tools display different levels of consistencies, where ShapeWorks and\nDeformetrica models are more consistent compared to models from SPHARM-PDM due\nto the groupwise approach of estimating surface correspondences. Furthermore,\nShapeWorks and Deformetrica shape models are found to capture clinically\nrelevant population-level variability compared to SPHARM-PDM models.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 03:51:35 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Goparaju", "Anupama", ""], ["Bone", "Alexandre", ""], ["Hu", "Nan", ""], ["Henninger", "Heath B.", ""], ["Anderson", "Andrew E.", ""], ["Durrleman", "Stanley", ""], ["Jacxsens", "Matthijs", ""], ["Morris", "Alan", ""], ["Csecs", "Ibolya", ""], ["Marrouche", "Nassir", ""], ["Elhabian", "Shireen Y.", ""]]}, {"id": "2009.02899", "submitter": "Erico Tjoa", "authors": "Erico Tjoa, Cuntai Guan", "title": "Quantifying Explainability of Saliency Methods in Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way to achieve eXplainable artificial intelligence (XAI) is through the\nuse of post-hoc analysis methods. In particular, methods that generate heatmaps\nhave been used to explain black-box models, such as deep neural network. In\nsome cases, heatmaps are appealing due to the intuitive and visual ways to\nunderstand them. However, quantitative analysis that demonstrates the actual\npotential of heatmaps have been lacking, and comparison between different\nmethods are not standardized as well. In this paper, we introduce a synthetic\ndataset that can be generated adhoc along with the ground-truth heatmaps for\nbetter quantitative assessment. Each sample data is an image of a cell with\neasily distinguishable features, facilitating a more transparent assessment of\ndifferent XAI methods. Comparison and recommendations are made, shortcomings\nare clarified along with suggestions for future research directions to handle\nthe finer details of select post-hoc analysis methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 05:55:24 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 13:27:57 GMT"}, {"version": "v3", "created": "Fri, 29 Jan 2021 05:45:01 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Tjoa", "Erico", ""], ["Guan", "Cuntai", ""]]}, {"id": "2009.02918", "submitter": "Zhaoyu Su", "authors": "Zhaoyu Su, Pin Siang Tan, Junkang Chow, Jimmy Wu, Yehur Cheong,\n  Yu-Hsing Wang", "title": "DV-ConvNet: Fully Convolutional Deep Learning on Point Clouds with\n  Dynamic Voxelization and 3D Group Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D point cloud interpretation is a challenging task due to the randomness and\nsparsity of the component points. Many of the recently proposed methods like\nPointNet and PointCNN have been focusing on learning shape descriptions from\npoint coordinates as point-wise input features, which usually involves\ncomplicated network architectures. In this work, we draw attention back to the\nstandard 3D convolutions towards an efficient 3D point cloud interpretation.\nInstead of converting the entire point cloud into voxel representations like\nthe other volumetric methods, we voxelize the sub-portions of the point cloud\nonly at necessary locations within each convolution layer on-the-fly, using our\ndynamic voxelization operation with self-adaptive voxelization resolution. In\naddition, we incorporate 3D group convolution into our dense convolution kernel\nimplementation to further exploit the rotation invariant features of point\ncloud. Benefiting from its simple fully-convolutional architecture, our network\nis able to run and converge at a considerably fast speed, while yields on-par\nor even better performance compared with the state-of-the-art methods on\nseveral benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 07:45:05 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 10:10:04 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Su", "Zhaoyu", ""], ["Tan", "Pin Siang", ""], ["Chow", "Junkang", ""], ["Wu", "Jimmy", ""], ["Cheong", "Yehur", ""], ["Wang", "Yu-Hsing", ""]]}, {"id": "2009.02967", "submitter": "Tiago Azevedo", "authors": "Tiago Azevedo, Ren\\'e de Jong, Matthew Mattina, Partha Maji", "title": "Stochastic-YOLO: Efficient Probabilistic Object Detection under Dataset\n  Shifts", "comments": "To appear in the Workshop on Machine Learning for Autonomous Driving\n  (ML4AD) at NeurIPS 2020. 9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image classification tasks, the evaluation of models' robustness to\nincreased dataset shifts with a probabilistic framework is very well studied.\nHowever, object detection (OD) tasks pose other challenges for uncertainty\nestimation and evaluation. For example, one needs to evaluate both the quality\nof the label uncertainty (i.e., what?) and spatial uncertainty (i.e., where?)\nfor a given bounding box, but that evaluation cannot be performed with more\ntraditional average precision metrics (e.g., mAP). In this paper, we adapt the\nwell-established YOLOv3 architecture to generate uncertainty estimations by\nintroducing stochasticity in the form of Monte Carlo Dropout (MC-Drop), and\nevaluate it across different levels of dataset shift. We call this novel\narchitecture Stochastic-YOLO, and provide an efficient implementation to\neffectively reduce the burden of the MC-Drop sampling mechanism at inference\ntime. Finally, we provide some sensitivity analyses, while arguing that\nStochastic-YOLO is a sound approach that improves different components of\nuncertainty estimations, in particular spatial uncertainties.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 09:28:17 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 12:57:02 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Azevedo", "Tiago", ""], ["de Jong", "Ren\u00e9", ""], ["Mattina", "Matthew", ""], ["Maji", "Partha", ""]]}, {"id": "2009.02978", "submitter": "Nan Meng", "authors": "Nan Meng, Kai Li, Jianzhuang Liu, Edmund Y. Lam", "title": "Light Field View Synthesis via Aperture Disparity and Warping Confidence\n  Map", "comments": "14 pages, 14 figures", "journal-ref": "IEEE Transactions on Image Processing 30 (2021): 3908-3921", "doi": "10.1109/TIP.2021.3066293", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a learning-based approach to synthesize the view from an\narbitrary camera position given a sparse set of images. A key challenge for\nthis novel view synthesis arises from the reconstruction process, when the\nviews from different input images may not be consistent due to obstruction in\nthe light path. We overcome this by jointly modeling the epipolar property and\nocclusion in designing a convolutional neural network. We start by defining and\ncomputing the aperture disparity map, which approximates the parallax and\nmeasures the pixel-wise shift between two views. While this relates to\nfree-space rendering and can fail near the object boundaries, we further\ndevelop a warping confidence map to address pixel occlusion in these\nchallenging regions. The proposed method is evaluated on diverse real-world and\nsynthetic light field scenes, and it shows better performance over several\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 09:46:01 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 05:00:06 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Meng", "Nan", ""], ["Li", "Kai", ""], ["Liu", "Jianzhuang", ""], ["Lam", "Edmund Y.", ""]]}, {"id": "2009.03008", "submitter": "Tomer Weiss", "authors": "Tomer Weiss, Sanketh Vedula, Ortal Senouf, Oleg Michailovich, and\n  AlexBronstein", "title": "Towards learned optimal q-space sampling in diffusion MRI", "comments": "CDMRI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fiber tractography is an important tool of computational neuroscience that\nenables reconstructing the spatial connectivity and organization of white\nmatter of the brain. Fiber tractography takes advantage of diffusion Magnetic\nResonance Imaging (dMRI) which allows measuring the apparent diffusivity of\ncerebral water along different spatial directions. Unfortunately, collecting\nsuch data comes at the price of reduced spatial resolution and substantially\nelevated acquisition times, which limits the clinical applicability of dMRI.\nThis problem has been thus far addressed using two principal strategies. Most\nof the efforts have been extended towards improving the quality of signal\nestimation for any, yet fixed sampling scheme (defined through the choice of\ndiffusion-encoding gradients). On the other hand, optimization over the\nsampling scheme has also proven to be effective. Inspired by the previous\nresults, the present work consolidates the above strategies into a unified\nestimation framework, in which the optimization is carried out with respect to\nboth estimation model and sampling design {\\it concurrently}. The proposed\nsolution offers substantial improvements in the quality of signal estimation as\nwell as the accuracy of ensuing analysis by means of fiber tractography. While\nproving the optimality of the learned estimation models would probably need\nmore extensive evaluation, we nevertheless claim that the learned sampling\nschemes can be of immediate use, offering a way to improve the dMRI analysis\nwithout the necessity of deploying the neural network used for their\nestimation. We present a comprehensive comparative analysis based on the Human\nConnectome Project data. Code and learned sampling designs aviliable at\nhttps://github.com/tomer196/Learned_dMRI.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 10:46:12 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Weiss", "Tomer", ""], ["Vedula", "Sanketh", ""], ["Senouf", "Ortal", ""], ["Michailovich", "Oleg", ""], ["AlexBronstein", "", ""]]}, {"id": "2009.03016", "submitter": "Luis Carlos Garcia-Peraza-Herrera", "authors": "Luis C. Garc\\'ia-Peraza-Herrera, Wenqi Li, Caspar Gruijthuijsen, Alain\n  Devreker, George Attilakos, Jan Deprest, Emmanuel Vander Poorten, Danail\n  Stoyanov, Tom Vercauteren, S\\'ebastien Ourselin", "title": "Real-Time Segmentation of Non-Rigid Surgical Tools based on Deep\n  Learning and Tracking", "comments": "Accepted in CARE Workshop, held in conjunction with MICCAI 2016", "journal-ref": null, "doi": "10.1007/978-3-319-54057-3_8", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time tool segmentation is an essential component in computer-assisted\nsurgical systems. We propose a novel real-time automatic method based on Fully\nConvolutional Networks (FCN) and optical flow tracking. Our method exploits the\nability of deep neural networks to produce accurate segmentations of highly\ndeformable parts along with the high speed of optical flow. Furthermore, the\npre-trained FCN can be fine-tuned on a small amount of medical images without\nthe need to hand-craft features. We validated our method using existing and new\nbenchmark datasets, covering both ex vivo and in vivo real clinical cases where\ndifferent surgical instruments are employed. Two versions of the method are\npresented, non-real-time and real-time. The former, using only deep learning,\nachieves a balanced accuracy of 89.6% on a real clinical dataset, outperforming\nthe (non-real-time) state of the art by 3.8% points. The latter, a combination\nof deep learning with optical flow tracking, yields an average balanced\naccuracy of 78.2% across all the validated datasets.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 11:06:14 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Garc\u00eda-Peraza-Herrera", "Luis C.", ""], ["Li", "Wenqi", ""], ["Gruijthuijsen", "Caspar", ""], ["Devreker", "Alain", ""], ["Attilakos", "George", ""], ["Deprest", "Jan", ""], ["Poorten", "Emmanuel Vander", ""], ["Stoyanov", "Danail", ""], ["Vercauteren", "Tom", ""], ["Ourselin", "S\u00e9bastien", ""]]}, {"id": "2009.03046", "submitter": "Jakub Nemcek", "authors": "Jakub Nemcek, Roman Jakubicek, Jiri Chmelik", "title": "Localization and classification of intracranialhemorrhages in CT data", "comments": "Submitted on EMBEC 2020, paper has not been reviewed yet, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intracranial hemorrhages (ICHs) are life-threatening brain injures with a\nrelatively high incidence. In this paper, the automatic algorithm for the\ndetection and classification of ICHs, including localization, is present. The\nset of binary convolutional neural network-based classifiers with a designed\ncascade-parallel architecture is used. This automatic system may lead to a\ndistinct decrease in the diagnostic process's duration in acute cases. An\naverage Jaccard coefficient of 53.7 % is achieved on the data from the publicly\navailable head CT dataset CQ500.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 12:07:27 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Nemcek", "Jakub", ""], ["Jakubicek", "Roman", ""], ["Chmelik", "Jiri", ""]]}, {"id": "2009.03051", "submitter": "Kashif Ahmad Dr", "authors": "Syed Zohaib Hassan, Kashif Ahmad, Steven Hicks, Paal Halvorsen, Ala\n  Al-Fuqaha, Nicola Conci, Michael Riegler", "title": "Visual Sentiment Analysis from Disaster Images in Social Media", "comments": "10 pages, 6 figures, 6 tables. arXiv admin note: substantial text\n  overlap with arXiv:2002.03773", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of social networks and users' tendency towards\nsharing their feelings, expressions, and opinions in text, visual, and audio\ncontent, have opened new opportunities and challenges in sentiment analysis.\nWhile sentiment analysis of text streams has been widely explored in\nliterature, sentiment analysis from images and videos is relatively new. This\narticle focuses on visual sentiment analysis in a societal important domain,\nnamely disaster analysis in social media. To this aim, we propose a deep visual\nsentiment analyzer for disaster related images, covering different aspects of\nvisual sentiment analysis starting from data collection, annotation, model\nselection, implementation, and evaluations. For data annotation, and analyzing\npeoples' sentiments towards natural disasters and associated images in social\nmedia, a crowd-sourcing study has been conducted with a large number of\nparticipants worldwide. The crowd-sourcing study resulted in a large-scale\nbenchmark dataset with four different sets of annotations, each aiming a\nseparate task. The presented analysis and the associated dataset will provide a\nbaseline/benchmark for future research in the domain. We believe the proposed\nsystem can contribute toward more livable communities by helping different\nstakeholders, such as news broadcasters, humanitarian organizations, as well as\nthe general public.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 11:29:52 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Hassan", "Syed Zohaib", ""], ["Ahmad", "Kashif", ""], ["Hicks", "Steven", ""], ["Halvorsen", "Paal", ""], ["Al-Fuqaha", "Ala", ""], ["Conci", "Nicola", ""], ["Riegler", "Michael", ""]]}, {"id": "2009.03063", "submitter": "Lingbin Kong", "authors": "Xi Gu, Lingbin Kong, Zhicheng Wang, Jie Li, Zhaohui Yu, Gang Wei", "title": "A Light-Weight Object Detection Framework with FPA Module for Optical\n  Remote Sensing Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of remote sensing technology, the acquisition of remote\nsensing images is easier and easier, which provides sufficient data resources\nfor the task of detecting remote sensing objects. However, how to detect\nobjects quickly and accurately from many complex optical remote sensing images\nis a challenging hot issue. In this paper, we propose an efficient anchor free\nobject detector, CenterFPANet. To pursue speed, we use a lightweight backbone\nand introduce the asymmetric revolution block. To improve the accuracy, we\ndesigned the FPA module, which links the feature maps of different levels, and\nintroduces the attention mechanism to dynamically adjust the weights of each\nlevel of feature maps, which solves the problem of detection difficulty caused\nby large size range of remote sensing objects. This strategy can improve the\naccuracy of remote sensing image object detection without reducing the\ndetection speed. On the DOTA dataset, CenterFPANet mAP is 64.00%, and FPS is\n22.2, which is close to the accuracy of the anchor-based methods currently used\nand much faster than them. Compared with Faster RCNN, mAP is 6.76% lower but\n60.87% faster. All in all, CenterFPANet achieves a balance between speed and\naccuracy in large-scale optical remote sensing object detection.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 12:41:17 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Gu", "Xi", ""], ["Kong", "Lingbin", ""], ["Wang", "Zhicheng", ""], ["Li", "Jie", ""], ["Yu", "Zhaohui", ""], ["Wei", "Gang", ""]]}, {"id": "2009.03075", "submitter": "Jing Zhang", "authors": "Jing Zhang, Deng-Ping Fan, Yuchao Dai, Saeed Anwar, Fatemeh Saleh,\n  Sadegh Aliakbarian, Nick Barnes", "title": "Uncertainty Inspired RGB-D Saliency Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the first stochastic framework to employ uncertainty for RGB-D\nsaliency detection by learning from the data labeling process. Existing RGB-D\nsaliency detection models treat this task as a point estimation problem by\npredicting a single saliency map following a deterministic learning pipeline.\nWe argue that, however, the deterministic solution is relatively ill-posed.\nInspired by the saliency data labeling process, we propose a generative\narchitecture to achieve probabilistic RGB-D saliency detection which utilizes a\nlatent variable to model the labeling variations. Our framework includes two\nmain models: 1) a generator model, which maps the input image and latent\nvariable to stochastic saliency prediction, and 2) an inference model, which\ngradually updates the latent variable by sampling it from the true or\napproximate posterior distribution. The generator model is an encoder-decoder\nsaliency network. To infer the latent variable, we introduce two different\nsolutions: i) a Conditional Variational Auto-encoder with an extra encoder to\napproximate the posterior distribution of the latent variable; and ii) an\nAlternating Back-Propagation technique, which directly samples the latent\nvariable from the true posterior distribution. Qualitative and quantitative\nresults on six challenging RGB-D benchmark datasets show our approach's\nsuperior performance in learning the distribution of saliency maps. The source\ncode is publicly available via our project page:\nhttps://github.com/JingZhang617/UCNet.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 13:01:45 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Zhang", "Jing", ""], ["Fan", "Deng-Ping", ""], ["Dai", "Yuchao", ""], ["Anwar", "Saeed", ""], ["Saleh", "Fatemeh", ""], ["Aliakbarian", "Sadegh", ""], ["Barnes", "Nick", ""]]}, {"id": "2009.03098", "submitter": "Min Cao", "authors": "Min Cao, Chen Chen, Hao Dou, Xiyuan Hu, Silong Peng and Arjan Kuijper", "title": "Progressive Bilateral-Context Driven Model for Post-Processing Person\n  Re-Identification", "comments": null, "journal-ref": "Transactions on Multimedia 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing person re-identification methods compute pairwise similarity by\nextracting robust visual features and learning the discriminative metric. Owing\nto visual ambiguities, these content-based methods that determine the pairwise\nrelationship only based on the similarity between them, inevitably produce a\nsuboptimal ranking list. Instead, the pairwise similarity can be estimated more\naccurately along the geodesic path of the underlying data manifold by exploring\nthe rich contextual information of the sample. In this paper, we propose a\nlightweight post-processing person re-identification method in which the\npairwise measure is determined by the relationship between the sample and the\ncounterpart's context in an unsupervised way. We translate the point-to-point\ncomparison into the bilateral point-to-set comparison. The sample's context is\ncomposed of its neighbor samples with two different definition ways: the first\norder context and the second order context, which are used to compute the\npairwise similarity in sequence, resulting in a progressive post-processing\nmodel. The experiments on four large-scale person re-identification benchmark\ndatasets indicate that (1) the proposed method can consistently achieve higher\naccuracies by serving as a post-processing procedure after the content-based\nperson re-identification methods, showing its state-of-the-art results, (2) the\nproposed lightweight method only needs about 6 milliseconds for optimizing the\nranking results of one sample, showing its high-efficiency. Code is available\nat: https://github.com/123ci/PBCmodel.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 13:35:09 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Cao", "Min", ""], ["Chen", "Chen", ""], ["Dou", "Hao", ""], ["Hu", "Xiyuan", ""], ["Peng", "Silong", ""], ["Kuijper", "Arjan", ""]]}, {"id": "2009.03108", "submitter": "Chao Zhu", "authors": "Chao Zhu, Teng Miao, Tongyu Xu, Tao Yang, Na Li", "title": "Stem-leaf segmentation and phenotypic trait extraction of maize shoots\n  from three-dimensional point cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, there are many approaches to acquire three-dimensional (3D) point\nclouds of maize plants. However, automatic stem-leaf segmentation of maize\nshoots from three-dimensional (3D) point clouds remains challenging, especially\nfor new emerging leaves that are very close and wrapped together during the\nseedling stage. To address this issue, we propose an automatic segmentation\nmethod consisting of three main steps: skeleton extraction, coarse segmentation\nbased on the skeleton, fine segmentation based on stem-leaf classification. The\nsegmentation method was tested on 30 maize seedlings and compared with manually\nobtained ground truth. The mean precision, mean recall, mean micro F1 score and\nmean over accuracy of our segmentation algorithm were 0.964, 0.966, 0.963 and\n0.969. Using the segmentation results, two applications were also developed in\nthis paper, including phenotypic trait extraction and skeleton optimization.\nSix phenotypic parameters can be accurately and automatically measured,\nincluding plant height, crown diameter, stem height and diameter, leaf width\nand length. Furthermore, the values of R2 for the six phenotypic traits were\nall above 0.94. The results indicated that the proposed algorithm could\nautomatically and precisely segment not only the fully expanded leaves, but\nalso the new leaves wrapped together and close together. The proposed approach\nmay play an important role in further maize research and applications, such as\ngenotype-to-phenotype study, geometric reconstruction and dynamic growth\nanimation. We released the source code and test data at the web site\nhttps://github.com/syau-miao/seg4maize.git\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 13:58:09 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Zhu", "Chao", ""], ["Miao", "Teng", ""], ["Xu", "Tongyu", ""], ["Yang", "Tao", ""], ["Li", "Na", ""]]}, {"id": "2009.03118", "submitter": "Iman Marivani", "authors": "Iman Marivani, Evaggelia Tsiligianni, Bruno Cornelis, Nikos\n  Deligiannis", "title": "Interpretable Deep Multimodal Image Super-Resolution", "comments": "in Proceedings of iTWIST'20, Paper-ID: 41, Nantes, France, December,\n  2-4, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal image super-resolution (SR) is the reconstruction of a high\nresolution image given a low-resolution observation with the aid of another\nimage modality. While existing deep multimodal models do not incorporate domain\nknowledge about image SR, we present a multimodal deep network design that\nintegrates coupled sparse priors and allows the effective fusion of information\nfrom another modality into the reconstruction process. Our method is inspired\nby a novel iterative algorithm for coupled convolutional sparse coding,\nresulting in an interpretable network by design. We apply our model to the\nsuper-resolution of near-infrared image guided by RGB images. Experimental\nresults show that our model outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 14:08:35 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Marivani", "Iman", ""], ["Tsiligianni", "Evaggelia", ""], ["Cornelis", "Bruno", ""], ["Deligiannis", "Nikos", ""]]}, {"id": "2009.03137", "submitter": "Qingyong Hu", "authors": "Qingyong Hu, Bo Yang, Sheikh Khalid, Wen Xiao, Niki Trigoni, Andrew\n  Markham", "title": "Towards Semantic Segmentation of Urban-Scale 3D Point Clouds: A Dataset,\n  Benchmarks and Challenges", "comments": "CVPR 2021, Code: https://github.com/QingyongHu/SensatUrban", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An essential prerequisite for unleashing the potential of supervised deep\nlearning algorithms in the area of 3D scene understanding is the availability\nof large-scale and richly annotated datasets. However, publicly available\ndatasets are either in relative small spatial scales or have limited semantic\nannotations due to the expensive cost of data acquisition and data annotation,\nwhich severely limits the development of fine-grained semantic understanding in\nthe context of 3D point clouds. In this paper, we present an urban-scale\nphotogrammetric point cloud dataset with nearly three billion richly annotated\npoints, which is three times the number of labeled points than the existing\nlargest photogrammetric point cloud dataset. Our dataset consists of large\nareas from three UK cities, covering about 7.6 km^2 of the city landscape. In\nthe dataset, each 3D point is labeled as one of 13 semantic classes. We\nextensively evaluate the performance of state-of-the-art algorithms on our\ndataset and provide a comprehensive analysis of the results. In particular, we\nidentify several key challenges towards urban-scale point cloud understanding.\nThe dataset is available at https://github.com/QingyongHu/SensatUrban.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 14:47:07 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 06:36:14 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 04:19:34 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Hu", "Qingyong", ""], ["Yang", "Bo", ""], ["Khalid", "Sheikh", ""], ["Xiao", "Wen", ""], ["Trigoni", "Niki", ""], ["Markham", "Andrew", ""]]}, {"id": "2009.03155", "submitter": "Pavel Korshunov", "authors": "Pavel Korshunov and S\\'ebastien Marcel", "title": "Deepfake detection: humans vs. machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deepfake videos, where a person's face is automatically swapped with a face\nof someone else, are becoming easier to generate with more realistic results.\nIn response to the threat such manipulations can pose to our trust in video\nevidence, several large datasets of deepfake videos and many methods to detect\nthem were proposed recently. However, it is still unclear how realistic\ndeepfake videos are for an average person and whether the algorithms are\nsignificantly better than humans at detecting them. In this paper, we present a\nsubjective study conducted in a crowdsourcing-like scenario, which\nsystematically evaluates how hard it is for humans to see if the video is\ndeepfake or not. For the evaluation, we used 120 different videos (60 deepfakes\nand 60 originals) manually pre-selected from the Facebook deepfake database,\nwhich was provided in the Kaggle's Deepfake Detection Challenge 2020. For each\nvideo, a simple question: \"Is face of the person in the video real of fake?\"\nwas answered on average by 19 na\\\"ive subjects. The results of the subjective\nevaluation were compared with the performance of two different state of the art\ndeepfake detection methods, based on Xception and EfficientNets (B4 variant)\nneural networks, which were pre-trained on two other large public databases:\nthe Google's subset from FaceForensics++ and the recent Celeb-DF dataset. The\nevaluation demonstrates that while the human perception is very different from\nthe perception of a machine, both successfully but in different ways are fooled\nby deepfakes. Specifically, algorithms struggle to detect those deepfake\nvideos, which human subjects found to be very easy to spot.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 15:20:37 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Korshunov", "Pavel", ""], ["Marcel", "S\u00e9bastien", ""]]}, {"id": "2009.03162", "submitter": "Mayank Golhar", "authors": "Mayank Golhar, Taylor L. Bobrow, MirMilad Pourmousavi Khoshknab,\n  Simran Jit, Saowanee Ngamruengphong, Nicholas J. Durr", "title": "Improving colonoscopy lesion classification using semi-supervised deep\n  learning", "comments": "10 pages, 5 figures", "journal-ref": "IEEE Access, vol. 9, pp. 631-640, 2021", "doi": "10.1109/ACCESS.2020.3047544", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While data-driven approaches excel at many image analysis tasks, the\nperformance of these approaches is often limited by a shortage of annotated\ndata available for training. Recent work in semi-supervised learning has shown\nthat meaningful representations of images can be obtained from training with\nlarge quantities of unlabeled data, and that these representations can improve\nthe performance of supervised tasks. Here, we demonstrate that an unsupervised\njigsaw learning task, in combination with supervised training, results in up to\na 9.8% improvement in correctly classifying lesions in colonoscopy images when\ncompared to a fully-supervised baseline. We additionally benchmark improvements\nin domain adaptation and out-of-distribution detection, and demonstrate that\nsemi-supervised learning outperforms supervised learning in both cases. In\ncolonoscopy applications, these metrics are important given the skill required\nfor endoscopic assessment of lesions, the wide variety of endoscopy systems in\nuse, and the homogeneity that is typical of labeled datasets.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 15:25:35 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Golhar", "Mayank", ""], ["Bobrow", "Taylor L.", ""], ["Khoshknab", "MirMilad Pourmousavi", ""], ["Jit", "Simran", ""], ["Ngamruengphong", "Saowanee", ""], ["Durr", "Nicholas J.", ""]]}, {"id": "2009.03173", "submitter": "Zhenyue Qin", "authors": "Yang Liu and Zhenyue Qin and Saeed Anwar and Sabrina Caldwell and Tom\n  Gedeon", "title": "Are Deep Neural Architectures Losing Information? Invertibility Is\n  Indispensable", "comments": "ICONIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ever since the advent of AlexNet, designing novel deep neural architectures\nfor different tasks has consistently been a productive research direction.\nDespite the exceptional performance of various architectures in practice, we\nstudy a theoretical question: what is the condition for deep neural\narchitectures to preserve all the information of the input data? Identifying\nthe information lossless condition for deep neural architectures is important,\nbecause tasks such as image restoration require keep the detailed information\nof the input data as much as possible. Using the definition of mutual\ninformation, we show that: a deep neural architecture can preserve maximum\ndetails about the given data if and only if the architecture is invertible. We\nverify the advantages of our Invertible Restoring Autoencoder (IRAE) network by\ncomparing it with competitive models on three perturbed image restoration\ntasks: image denoising, jpeg image decompression and image inpainting.\nExperimental results show that IRAE consistently outperforms non-invertible\nones. Our model even contains far fewer parameters. Thus, it may be worthwhile\nto try replacing standard components of deep neural architectures, such as\nresidual blocks and ReLU, with their invertible counterparts. We believe our\nwork provides a unique perspective and direction for future deep learning\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 15:39:24 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 00:15:58 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Liu", "Yang", ""], ["Qin", "Zhenyue", ""], ["Anwar", "Saeed", ""], ["Caldwell", "Sabrina", ""], ["Gedeon", "Tom", ""]]}, {"id": "2009.03184", "submitter": "Yanwei Fu", "authors": "Yanwei Fu, Feng Li, Wenxuan Wang, Haicheng Tang, Xuelin Qian, Mengwei\n  Gu, Xiangyang Xue", "title": "A New Screening Method for COVID-19 based on Ocular Feature Recognition\n  by Machine Learning Tools", "comments": "technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Coronavirus disease 2019 (COVID-19) has affected several million people.\nWith the outbreak of the epidemic, many researchers are devoting themselves to\nthe COVID-19 screening system. The standard practices for rapid risk screening\nof COVID-19 are the CT imaging or RT-PCR (real-time polymerase chain reaction).\nHowever, these methods demand professional efforts of the acquisition of CT\nimages and saliva samples, a certain amount of waiting time, and most\nimportantly prohibitive examination fee in some countries. Recently, some\nliteratures have shown that the COVID-19 patients usually accompanied by ocular\nmanifestations consistent with the conjunctivitis, including conjunctival\nhyperemia, chemosis, epiphora, or increased secretions. After more than four\nmonths study, we found that the confirmed cases of COVID-19 present the\nconsistent ocular pathological symbols; and we propose a new screening method\nof analyzing the eye-region images, captured by common CCD and CMOS cameras,\ncould reliably make a rapid risk screening of COVID-19 with very high accuracy.\nWe believe a system implementing such an algorithm should assist the triage\nmanagement or the clinical diagnosis. To further evaluate our algorithm and\napproved by the Ethics Committee of Shanghai public health clinic center of\nFudan University, we conduct a study of analyzing the eye-region images of 303\npatients (104 COVID-19, 131 pulmonary, and 68 ocular patients), as well as 136\nhealthy people. Remarkably, our results of COVID-19 patients in testing set\nconsistently present similar ocular pathological symbols; and very high testing\nresults have been achieved in terms of sensitivity and specificity. We hope\nthis study can be inspiring and helpful for encouraging more researches in this\ntopic.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 00:50:27 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Fu", "Yanwei", ""], ["Li", "Feng", ""], ["Wang", "Wenxuan", ""], ["Tang", "Haicheng", ""], ["Qian", "Xuelin", ""], ["Gu", "Mengwei", ""], ["Xue", "Xiangyang", ""]]}, {"id": "2009.03224", "submitter": "Thyagharajan K K", "authors": "K. K. Thyagharajan, G. Kalaiarasi", "title": "A Review on Near Duplicate Detection of Images using Computer Vision\n  Techniques", "comments": "37 Pages, 7 figures, \"For online first version, see\n  https://link.springer.com/article/10.1007/s11831-020-09400-w\"", "journal-ref": null, "doi": "10.1007/s11831-020-09400-w", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, digital content is widespread and simply redistributable, either\nlawfully or unlawfully. For example, after images are posted on the internet,\nother web users can modify them and then repost their versions, thereby\ngenerating near-duplicate images. The presence of near-duplicates affects the\nperformance of the search engines critically. Computer vision is concerned with\nthe automatic extraction, analysis and understanding of useful information from\ndigital images. The main application of computer vision is image understanding.\nThere are several tasks in image understanding such as feature extraction,\nobject detection, object recognition, image cleaning, image transformation,\netc. There is no proper survey in literature related to near duplicate\ndetection of images. In this paper, we review the state-of-the-art computer\nvision-based approaches and feature extraction methods for the detection of\nnear duplicate images. We also discuss the main challenges in this field and\nhow other researchers addressed those challenges. This review provides research\ndirections to the fellow researchers who are interested to work in this field.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 16:41:46 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Thyagharajan", "K. K.", ""], ["Kalaiarasi", "G.", ""]]}, {"id": "2009.03231", "submitter": "Samyak Datta", "authors": "Samyak Datta, Oleksandr Maksymets, Judy Hoffman, Stefan Lee, Dhruv\n  Batra, Devi Parikh", "title": "Integrating Egocentric Localization for More Realistic Point-Goal\n  Navigation Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has presented embodied agents that can navigate to point-goal\ntargets in novel indoor environments with near-perfect accuracy. However, these\nagents are equipped with idealized sensors for localization and take\ndeterministic actions. This setting is practically sterile by comparison to the\ndirty reality of noisy sensors and actuations in the real world -- wheels can\nslip, motion sensors have error, actuations can rebound. In this work, we take\na step towards this noisy reality, developing point-goal navigation agents that\nrely on visual estimates of egomotion under noisy action dynamics. We find\nthese agents outperform naive adaptions of current point-goal agents to this\nsetting as well as those incorporating classic localization baselines. Further,\nour model conceptually divides learning agent dynamics or odometry (where am\nI?) from task-specific navigation policy (where do I want to go?). This enables\na seamless adaption to changing dynamics (a different robot or floor type) by\nsimply re-calibrating the visual odometry model -- circumventing the expense of\nre-training of the navigation policy. Our agent was the runner-up in the\nPointNav track of CVPR 2020 Habitat Challenge.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 16:52:47 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Datta", "Samyak", ""], ["Maksymets", "Oleksandr", ""], ["Hoffman", "Judy", ""], ["Lee", "Stefan", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "2009.03259", "submitter": "Yumeng Xue", "authors": "Rongzheng Bian, Yumeng Xue, Liang Zhou, Jian Zhang, Baoquan Chen,\n  Daniel Weiskopf, Yunhai Wang", "title": "Implicit Multidimensional Projection of Local Subspaces", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2020.3030368", "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a visualization method to understand the effect of\nmultidimensional projection on local subspaces, using implicit function\ndifferentiation. Here, we understand the local subspace as the multidimensional\nlocal neighborhood of data points. Existing methods focus on the projection of\nmultidimensional data points, and the neighborhood information is ignored. Our\nmethod is able to analyze the shape and directional information of the local\nsubspace to gain more insights into the global structure of the data through\nthe perception of local structures. Local subspaces are fitted by\nmultidimensional ellipses that are spanned by basis vectors. An accurate and\nefficient vector transformation method is proposed based on analytical\ndifferentiation of multidimensional projections formulated as implicit\nfunctions. The results are visualized as glyphs and analyzed using a full set\nof specifically-designed interactions supported in our efficient web-based\nvisualization tool. The usefulness of our method is demonstrated using various\nmulti- and high-dimensional benchmark datasets. Our implicit differentiation\nvector transformation is evaluated through numerical comparisons; the overall\nmethod is evaluated through exploration examples and use cases.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 17:27:27 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Bian", "Rongzheng", ""], ["Xue", "Yumeng", ""], ["Zhou", "Liang", ""], ["Zhang", "Jian", ""], ["Chen", "Baoquan", ""], ["Weiskopf", "Daniel", ""], ["Wang", "Yunhai", ""]]}, {"id": "2009.03281", "submitter": "Mohamed Hefeeda", "authors": "Amgad Ahmed, Suhong Kim, Mohamed Elgharib, Mohamed Hefeeda", "title": "User-assisted Video Reflection Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reflections in videos are obstructions that often occur when videos are taken\nbehind reflective surfaces like glass. These reflections reduce the quality of\nsuch videos, lead to information loss and degrade the accuracy of many computer\nvision algorithms. A video containing reflections is a combination of\nbackground and reflection layers. Thus, reflection removal is equivalent to\ndecomposing the video into two layers. This, however, is a challenging and\nill-posed problem as there is an infinite number of valid decompositions. To\naddress this problem, we propose a user-assisted method for video reflection\nremoval. We rely on both spatial and temporal information and utilize sparse\nuser hints to help improve separation. The key idea of the proposed method is\nto use motion cues to separate the background layer from the reflection layer\nwith minimal user assistance. We show that user-assistance significantly\nimproves the layer separation results. We implement and evaluate the proposed\nmethod through quantitative and qualitative results on real and synthetic\nvideos. Our experiments show that the proposed method successfully removes\nreflection from video sequences, does not introduce visual distortions, and\nsignificantly outperforms the state-of-the-art reflection removal methods in\nthe literature.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 17:42:40 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Ahmed", "Amgad", ""], ["Kim", "Suhong", ""], ["Elgharib", "Mohamed", ""], ["Hefeeda", "Mohamed", ""]]}, {"id": "2009.03285", "submitter": "Thyagharajan K K", "authors": "L. Aneesh Euprazia, K.K.Thyagharajan", "title": "A novel action recognition system for smart monitoring of elderly people\n  using Action Pattern Image and Series CNN with transfer learning", "comments": "30 pages, 19 figures, \"Submitted to Pattern Recognition Journal,\n  Elsevier\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Falling of elderly people who are staying alone at home leads to health\nrisks. If they are not attended immediately even it may lead to fatal danger to\ntheir life. In this paper a novel computer vision-based system for smart\nmonitoring of elderly people using Series Convolutional Neural Network (SCNN)\nwith transfer learning is proposed. When CNN is trained by the frames of the\nvideos directly, it learns from all pixels including the background pixels.\nGenerally, the background in a video does not contribute anything in\nidentifying the action and actually it will mislead the action classification.\nSo, we propose a novel action recognition system and our contributions are 1)\nto generate more general action patterns which are not affected by illumination\nand background variations of the video sequences and eliminate the obligation\nof image augmentation in CNN training 2) to design SCNN architecture and\nenhance the feature extraction process to learn large amount of data, 3) to\npresent the patterns learnt by the neurons in the layers and analyze how these\nneurons capture the action when the input pattern is passing through these\nneurons, and 4) to extend the capability of the trained SCNN for recognizing\nfall actions using transfer learning.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 17:51:27 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Euprazia", "L. Aneesh", ""], ["Thyagharajan", "K. K.", ""]]}, {"id": "2009.03298", "submitter": "Kamal Gupta", "authors": "Kamal Gupta and Susmija Jabbireddy and Ketul Shah and Abhinav\n  Shrivastava and Matthias Zwicker", "title": "Improved Modeling of 3D Shapes with Multi-view Depth Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple yet effective general-purpose framework for modeling 3D\nshapes by leveraging recent advances in 2D image generation using CNNs. Using\njust a single depth image of the object, we can output a dense multi-view depth\nmap representation of 3D objects. Our simple encoder-decoder framework,\ncomprised of a novel identity encoder and class-conditional viewpoint\ngenerator, generates 3D consistent depth maps. Our experimental results\ndemonstrate the two-fold advantage of our approach. First, we can directly\nborrow architectures that work well in the 2D image domain to 3D. Second, we\ncan effectively generate high-resolution 3D shapes with low computational\nmemory. Our quantitative evaluations show that our method is superior to\nexisting depth map methods for reconstructing and synthesizing 3D objects and\nis competitive with other representations, such as point clouds, voxel grids,\nand implicit functions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 17:58:27 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Gupta", "Kamal", ""], ["Jabbireddy", "Susmija", ""], ["Shah", "Ketul", ""], ["Shrivastava", "Abhinav", ""], ["Zwicker", "Matthias", ""]]}, {"id": "2009.03301", "submitter": "Maria Elli", "authors": "Jack Weast", "title": "Sensors, Safety Models and A System-Level Approach to Safe and Scalable\n  Automated Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When considering the accuracy of sensors in an automated vehicle (AV), it is\nnot sufficient to evaluate the performance of any given sensor in isolation.\nRather, the performance of any individual sensor must be considered in the\ncontext of the overall system design. Techniques like redundancy and different\nsensing modalities can reduce the chances of a sensing failure. Additionally,\nthe use of safety models is essential to understanding whether any particular\nsensing failure is relevant. Only when the entire system design is taken into\naccount can one properly understand the meaning of safety-relevant sensing\nfailures in an AV. In this paper, we will consider what should actually\nconstitute a sensing failure, how safety models play an important role in\nmitigating potential failures, how a system-level approach to safety will\ndeliver a safe and scalable AV, and what an acceptable sensing failure rate\nshould be considering the full picture of an AV's architecture.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 20:14:59 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Weast", "Jack", ""]]}, {"id": "2009.03303", "submitter": "L\\'eo Lebrat", "authors": "Rodrigo Santa Cruz, L\\'eo Lebrat, Pierrick Bourgeat, Vincent Dor\\'e,\n  Jason Dowling, Jurgen Fripp, Clinton Fookes, Olivier Salvado", "title": "Going deeper with brain morphometry using neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain morphometry from magnetic resonance imaging (MRI) is a consolidated\nbiomarker for many neurodegenerative diseases. Recent advances in this domain\nindicate that deep convolutional neural networks can infer morphometric\nmeasurements within a few seconds. Nevertheless, the accuracy of the devised\nmodel for insightful bio-markers (mean curvature and thickness) remains\nunsatisfactory. In this paper, we propose a more accurate and efficient neural\nnetwork model for brain morphometry named HerstonNet. More specifically, we\ndevelop a 3D ResNet-based neural network to learn rich features directly from\nMRI, design a multi-scale regression scheme by predicting morphometric measures\nat feature maps of different resolutions, and leverage a robust optimization\nmethod to avoid poor quality minima and reduce the prediction variance. As a\nresult, HerstonNet improves the existing approach by 24.30% in terms of\nintraclass correlation coefficient (agreement measure) to FreeSurfer\nsilver-standards while maintaining a competitive run-time.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 07:57:13 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Cruz", "Rodrigo Santa", ""], ["Lebrat", "L\u00e9o", ""], ["Bourgeat", "Pierrick", ""], ["Dor\u00e9", "Vincent", ""], ["Dowling", "Jason", ""], ["Fripp", "Jurgen", ""], ["Fookes", "Clinton", ""], ["Salvado", "Olivier", ""]]}, {"id": "2009.03364", "submitter": "Micha{\\l} Byra", "authors": "Michal Byra, Grzegorz Styczynski, Cezary Szmigielski, Piotr\n  Kalinowski, Lukasz Michalowski, Rafal Paluszkiewicz, Bogna\n  Ziarkiewicz-Wroblewska, Krzysztof Zieniewicz, Andrzej Nowicki", "title": "Adversarial attacks on deep learning models for fatty liver disease\n  classification by modification of ultrasound image reconstruction method", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have achieved remarkable success in\nmedical image analysis tasks. In ultrasound (US) imaging, CNNs have been\napplied to object classification, image reconstruction and tissue\ncharacterization. However, CNNs can be vulnerable to adversarial attacks, even\nsmall perturbations applied to input data may significantly affect model\nperformance and result in wrong output. In this work, we devise a novel\nadversarial attack, specific to ultrasound (US) imaging. US images are\nreconstructed based on radio-frequency signals. Since the appearance of US\nimages depends on the applied image reconstruction method, we explore the\npossibility of fooling deep learning model by perturbing US B-mode image\nreconstruction method. We apply zeroth order optimization to find small\nperturbations of image reconstruction parameters, related to attenuation\ncompensation and amplitude compression, which can result in wrong output. We\nillustrate our approach using a deep learning model developed for fatty liver\ndisease diagnosis, where the proposed adversarial attack achieved success rate\nof 48%.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 18:35:35 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Byra", "Michal", ""], ["Styczynski", "Grzegorz", ""], ["Szmigielski", "Cezary", ""], ["Kalinowski", "Piotr", ""], ["Michalowski", "Lukasz", ""], ["Paluszkiewicz", "Rafal", ""], ["Ziarkiewicz-Wroblewska", "Bogna", ""], ["Zieniewicz", "Krzysztof", ""], ["Nowicki", "Andrzej", ""]]}, {"id": "2009.03456", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Yezhen Wang, Bo Li, Bichen Wu, Yang Gao, Pengfei Xu,\n  Trevor Darrell, Kurt Keutzer", "title": "ePointDA: An End-to-End Simulation-to-Real Domain Adaptation Framework\n  for LiDAR Point Cloud Segmentation", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its robust and precise distance measurements, LiDAR plays an important\nrole in scene understanding for autonomous driving. Training deep neural\nnetworks (DNNs) on LiDAR data requires large-scale point-wise annotations,\nwhich are time-consuming and expensive to obtain. Instead, simulation-to-real\ndomain adaptation (SRDA) trains a DNN using unlimited synthetic data with\nautomatically generated labels and transfers the learned model to real\nscenarios. Existing SRDA methods for LiDAR point cloud segmentation mainly\nemploy a multi-stage pipeline and focus on feature-level alignment. They\nrequire prior knowledge of real-world statistics and ignore the pixel-level\ndropout noise gap and the spatial feature gap between different domains. In\nthis paper, we propose a novel end-to-end framework, named ePointDA, to address\nthe above issues. Specifically, ePointDA consists of three modules:\nself-supervised dropout noise rendering, statistics-invariant and\nspatially-adaptive feature alignment, and transferable segmentation learning.\nThe joint optimization enables ePointDA to bridge the domain shift at the\npixel-level by explicitly rendering dropout noise for synthetic LiDAR and at\nthe feature-level by spatially aligning the features between different domains,\nwithout requiring the real-world statistics. Extensive experiments adapting\nfrom synthetic GTA-LiDAR to real KITTI and SemanticKITTI demonstrate the\nsuperiority of ePointDA for LiDAR point cloud segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 23:46:08 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 16:52:41 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Zhao", "Sicheng", ""], ["Wang", "Yezhen", ""], ["Li", "Bo", ""], ["Wu", "Bichen", ""], ["Gao", "Yang", ""], ["Xu", "Pengfei", ""], ["Darrell", "Trevor", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2009.03458", "submitter": "Sanjay Seshan", "authors": "Sanjay Seshan", "title": "Horus: Using Sensor Fusion to Combine Infrastructure and On-board\n  Sensing to Improve Autonomous Vehicle Safety", "comments": "Presented at Intel ISEF 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies predict that demand for autonomous vehicles will increase tenfold\nbetween 2019 and 2026. However, recent high-profile accidents have\nsignificantly impacted consumer confidence in this technology. The cause for\nmany of these accidents can be traced back to the inability of these vehicles\nto correctly sense the impending danger. In response, manufacturers have been\nimproving the already extensive on-vehicle sensor packages to ensure that the\nsystem always has access to the data necessary to ensure safe navigation.\nHowever, these sensor packages only provide a view from the vehicle's\nperspective and, as a result, autonomous vehicles still require frequent human\nintervention to ensure safety.\n  To address this issue, I developed a system, called Horus, that combines\non-vehicle and infrastructure-based sensors to provide a more complete view of\nthe environment, including areas not visible from the vehicle. I built a\nsmall-scale experimental testbed as a proof of concept. My measurements of the\nimpact of sensor failures showed that even short outages (1 second) at slow\nspeeds (25 km/hr scaled velocity) prevents vehicles that rely on on-vehicle\nsensors from navigating properly. My experiments also showed that Horus\ndramatically improves driving safety and that the sensor fusion algorithm\nselected plays a significant role in the quality of the navigation. With just a\npair of infrastructure sensors, Horus could tolerate sensors that fail 40% of\nthe time and still navigate safely. These results are a promising first step\ntowards safer autonomous vehicles.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 23:52:57 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Seshan", "Sanjay", ""]]}, {"id": "2009.03465", "submitter": "Heng Fan", "authors": "Heng Fan, Hexin Bai, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia\n  Yu, Harshit, Mingzhen Huang, Juehuan Liu, Yong Xu, Chunyuan Liao, Lin Yuan,\n  Haibin Ling", "title": "LaSOT: A High-quality Large-scale Single Object Tracking Benchmark", "comments": "Tech Report. Update project website", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite great recent advances in visual tracking, its further development,\nincluding both algorithm design and evaluation, is limited due to lack of\ndedicated large-scale benchmarks. To address this problem, we present LaSOT, a\nhigh-quality Large-scale Single Object Tracking benchmark. LaSOT contains a\ndiverse selection of 85 object classes, and offers 1,550 totaling more than\n3.87 million frames. Each video frame is carefully and manually annotated with\na bounding box. This makes LaSOT, to our knowledge, the largest densely\nannotated tracking benchmark. Our goal in releasing LaSOT is to provide a\ndedicated high quality platform for both training and evaluation of trackers.\nThe average video length of LaSOT is around 2,500 frames, where each video\ncontains various challenge factors that exist in real world video footage,such\nas the targets disappearing and re-appearing. These longer video lengths allow\nfor the assessment of long-term trackers. To take advantage of the close\nconnection between visual appearance and natural language, we provide language\nspecification for each video in LaSOT. We believe such additions will allow for\nfuture research to use linguistic features to improve tracking. Two protocols,\nfull-overlap and one-shot, are designated for flexible assessment of trackers.\nWe extensively evaluate 48 baseline trackers on LaSOT with in-depth analysis,\nand results reveal that there still exists significant room for improvement.\nThe complete benchmark, tracking results as well as analysis are available at\nhttp://vision.cs.stonybrook.edu/~lasot/.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 00:31:56 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 11:59:36 GMT"}, {"version": "v3", "created": "Sat, 12 Sep 2020 03:53:45 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Fan", "Heng", ""], ["Bai", "Hexin", ""], ["Lin", "Liting", ""], ["Yang", "Fan", ""], ["Chu", "Peng", ""], ["Deng", "Ge", ""], ["Yu", "Sijia", ""], ["Harshit", "", ""], ["Huang", "Mingzhen", ""], ["Liu", "Juehuan", ""], ["Xu", "Yong", ""], ["Liao", "Chunyuan", ""], ["Yuan", "Lin", ""], ["Ling", "Haibin", ""]]}, {"id": "2009.03477", "submitter": "Yuanhao Gong", "authors": "Yuanhao Gong", "title": "A Residual Solver and Its Unfolding Neural Network for Total Variation\n  Regularized Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to solve the Total Variation regularized models by\nfinding the residual between the input and the unknown optimal solution. After\nanalyzing a previous method, we developed a new iterative algorithm, named as\nResidual Solver, which implicitly solves the model in gradient domain. We\ntheoretically prove the uniqueness of the gradient field in our algorithm. We\nfurther numerically confirm that the residual solver can reach the same global\noptimal solutions as the classical method on 500 natural images. Moreover, we\nunfold our iterative algorithm into a convolution neural network (named as\nResidual Solver Network). This network is unsupervised and can be considered as\nan \"enhanced version\" of our iterative algorithm. Finally, both the proposed\nalgorithm and neural network are successfully applied on several problems to\ndemonstrate their effectiveness and efficiency, including image smoothing,\ndenoising, and biomedical image reconstruction. The proposed network is general\nand can be applied to solve other total variation regularized models.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 01:44:34 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Gong", "Yuanhao", ""]]}, {"id": "2009.03508", "submitter": "Shengjie Liu", "authors": "Shengjie Liu, Qian Shi, and Liangpei Zhang", "title": "Few-Shot Hyperspectral Image Classification With Unknown Classes Using\n  Multitask Deep Learning", "comments": "Accepted by IEEE TGRS", "journal-ref": null, "doi": "10.1109/TGRS.2020.3018879", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current hyperspectral image classification assumes that a predefined\nclassification system is closed and complete, and there are no unknown or novel\nclasses in the unseen data. However, this assumption may be too strict for the\nreal world. Often, novel classes are overlooked when the classification system\nis constructed. The closed nature forces a model to assign a label given a new\nsample and may lead to overestimation of known land covers (e.g., crop area).\nTo tackle this issue, we propose a multitask deep learning method that\nsimultaneously conducts classification and reconstruction in the open world\n(named MDL4OW) where unknown classes may exist. The reconstructed data are\ncompared with the original data; those failing to be reconstructed are\nconsidered unknown, based on the assumption that they are not well represented\nin the latent features due to the lack of labels. A threshold needs to be\ndefined to separate the unknown and known classes; we propose two strategies\nbased on the extreme value theory for few-shot and many-shot scenarios. The\nproposed method was tested on real-world hyperspectral images; state-of-the-art\nresults were achieved, e.g., improving the overall accuracy by 4.94% for the\nSalinas data. By considering the existence of unknown classes in the open\nworld, our method achieved more accurate hyperspectral image classification,\nespecially under the few-shot context.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 03:53:10 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Liu", "Shengjie", ""], ["Shi", "Qian", ""], ["Zhang", "Liangpei", ""]]}, {"id": "2009.03558", "submitter": "Zhiyu Xue", "authors": "Zhiyu Xue, Lixin Duan, Wen Li, Lin Chen and Jiebo Luo", "title": "Region Comparison Network for Interpretable Few-shot Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning has been successfully applied to many real-world computer\nvision tasks, training robust classifiers usually requires a large amount of\nwell-labeled data. However, the annotation is often expensive and\ntime-consuming. Few-shot image classification has thus been proposed to\neffectively use only a limited number of labeled examples to train models for\nnew classes. Recent works based on transferable metric learning methods have\nachieved promising classification performance through learning the similarity\nbetween the features of samples from the query and support sets. However, rare\nof them explicitly considers the model interpretability, which can actually be\nrevealed during the training phase.\n  For that, in this work, we propose a metric learning based method named\nRegion Comparison Network (RCN), which is able to reveal how few-shot learning\nworks as in a neural network as well as to find out specific regions that are\nrelated to each other in images coming from the query and support sets.\nMoreover, we also present a visualization strategy named Region Activation\nMapping (RAM) to intuitively explain what our method has learned by visualizing\nintermediate variables in our network. We also present a new way to generalize\nthe interpretability from the level of tasks to categories, which can also be\nviewed as a method to find the prototypical parts for supporting the final\ndecision of our RCN. Extensive experiments on four benchmark datasets clearly\nshow the effectiveness of our method over existing baselines.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 07:29:05 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Xue", "Zhiyu", ""], ["Duan", "Lixin", ""], ["Li", "Wen", ""], ["Chen", "Lin", ""], ["Luo", "Jiebo", ""]]}, {"id": "2009.03632", "submitter": "Chris Dongjoo Kim", "authors": "Chris Dongjoo Kim, Jinseo Jeong, and Gunhee Kim", "title": "Imbalanced Continual Learning with Partitioning Reservoir Sampling", "comments": "Published to ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning from a sequential stream of data is a crucial challenge\nfor machine learning research. Most studies have been conducted on this topic\nunder the single-label classification setting along with an assumption of\nbalanced label distribution. This work expands this research horizon towards\nmulti-label classification. In doing so, we identify unanticipated adversity\ninnately existent in many multi-label datasets, the long-tailed distribution.\nWe jointly address the two independently solved problems, Catastropic\nForgetting and the long-tailed label distribution by first empirically showing\na new challenge of destructive forgetting of the minority concepts on the tail.\nThen, we curate two benchmark datasets, COCOseq and NUS-WIDEseq, that allow the\nstudy of both intra- and inter-task imbalances. Lastly, we propose a new\nsampling strategy for replay-based approach named Partitioning Reservoir\nSampling (PRS), which allows the model to maintain a balanced knowledge of both\nhead and tail classes. We publicly release the dataset and the code in our\nproject page.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 10:28:18 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Kim", "Chris Dongjoo", ""], ["Jeong", "Jinseo", ""], ["Kim", "Gunhee", ""]]}, {"id": "2009.03651", "submitter": "Sasho Nedelkoski", "authors": "Sasho Nedelkoski, Mihail Bogojeski, Odej Kao", "title": "Learning more expressive joint distributions in multimodal variational\n  methods", "comments": "12 pages, Accepted and presented at LOD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data often are formed of multiple modalities, which jointly describe the\nobserved phenomena. Modeling the joint distribution of multimodal data requires\nlarger expressive power to capture high-level concepts and provide better data\nrepresentations. However, multimodal generative models based on variational\ninference are limited due to the lack of flexibility of the approximate\nposterior, which is obtained by searching within a known parametric family of\ndistributions. We introduce a method that improves the representational\ncapacity of multimodal variational methods using normalizing flows. It\napproximates the joint posterior with a simple parametric distribution and\nsubsequently transforms into a more complex one. Through several experiments,\nwe demonstrate that the model improves on state-of-the-art multimodal methods\nbased on variational inference on various computer vision tasks such as\ncolorization, edge and mask detection, and weakly supervised learning. We also\nshow that learning more powerful approximate joint distributions improves the\nquality of the generated samples. The code of our model is publicly available\nat https://github.com/SashoNedelkoski/BPFDMVM.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 11:45:27 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Nedelkoski", "Sasho", ""], ["Bogojeski", "Mihail", ""], ["Kao", "Odej", ""]]}, {"id": "2009.03665", "submitter": "Lyes Khacef", "authors": "Lyes Khacef, Vincent Gripon, Benoit Miramond", "title": "GPU-based Self-Organizing Maps for Post-Labeled Few-Shot Unsupervised\n  Learning", "comments": "Accepted for publication in the International Conference on Neural\n  Information Processing (ICONIP) 2020. arXiv admin note: text overlap with\n  arXiv:2009.02174", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot classification is a challenge in machine learning where the goal is\nto train a classifier using a very limited number of labeled examples. This\nscenario is likely to occur frequently in real life, for example when data\nacquisition or labeling is expensive. In this work, we consider the problem of\npost-labeled few-shot unsupervised learning, a classification task where\nrepresentations are learned in an unsupervised fashion, to be later labeled\nusing very few annotated examples. We argue that this problem is very likely to\noccur on the edge, when the embedded device directly acquires the data, and the\nexpert needed to perform labeling cannot be prompted often. To address this\nproblem, we consider an algorithm consisting of the concatenation of transfer\nlearning with clustering using Self-Organizing Maps (SOMs). We introduce a\nTensorFlow-based implementation to speed-up the process in multi-core CPUs and\nGPUs. Finally, we demonstrate the effectiveness of the method using standard\noff-the-shelf few-shot classification benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 13:22:28 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Khacef", "Lyes", ""], ["Gripon", "Vincent", ""], ["Miramond", "Benoit", ""]]}, {"id": "2009.03671", "submitter": "Haocong Rao", "authors": "Haocong Rao, Siqi Wang, Xiping Hu, Mingkui Tan, Yi Guo, Jun Cheng,\n  Xinwang Liu, and Bin Hu", "title": "A Self-Supervised Gait Encoding Approach with Locality-Awareness for 3D\n  Skeleton Based Person Re-Identification", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI). Journal version of\n  https://www.ijcai.org/proceedings/2020/0125 (IJCAI 2020). Codes are available\n  at https://github.com/Kali-Hac/Locality-Awareness-SGE. arXiv admin note: text\n  overlap with arXiv:2008.09435", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3092833", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (Re-ID) via gait features within 3D skeleton\nsequences is a newly-emerging topic with several advantages. Existing solutions\neither rely on hand-crafted descriptors or supervised gait representation\nlearning. This paper proposes a self-supervised gait encoding approach that can\nleverage unlabeled skeleton data to learn gait representations for person\nRe-ID. Specifically, we first create self-supervision by learning to\nreconstruct unlabeled skeleton sequences reversely, which involves richer\nhigh-level semantics to obtain better gait representations. Other pretext tasks\nare also explored to further improve self-supervised learning. Second, inspired\nby the fact that motion's continuity endows adjacent skeletons in one skeleton\nsequence and temporally consecutive skeleton sequences with higher correlations\n(referred as locality in 3D skeleton data), we propose a locality-aware\nattention mechanism and a locality-aware contrastive learning scheme, which aim\nto preserve locality-awareness on intra-sequence level and inter-sequence level\nrespectively during self-supervised learning. Last, with context vectors\nlearned by our locality-aware attention mechanism and contrastive learning\nscheme, a novel feature named Constrastive Attention-based Gait Encodings\n(CAGEs) is designed to represent gait effectively. Empirical evaluations show\nthat our approach significantly outperforms skeleton-based counterparts by\n15-40% Rank-1 accuracy, and it even achieves superior performance to numerous\nmulti-modal methods with extra RGB or depth information. Our codes are\navailable at https://github.com/Kali-Hac/Locality-Awareness-SGE.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 16:06:04 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 13:38:43 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 02:37:09 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Rao", "Haocong", ""], ["Wang", "Siqi", ""], ["Hu", "Xiping", ""], ["Tan", "Mingkui", ""], ["Guo", "Yi", ""], ["Cheng", "Jun", ""], ["Liu", "Xinwang", ""], ["Hu", "Bin", ""]]}, {"id": "2009.03683", "submitter": "Maxime Tremblay", "authors": "Maxime Tremblay, Shirsendu Sukanta Halder, Raoul de Charette,\n  Jean-Fran\\c{c}ois Lalonde", "title": "Rain rendering for evaluating and improving robustness to bad weather", "comments": "19 pages, 19 figures, IJCV 2020 preprint. arXiv admin note: text\n  overlap with arXiv:1908.10335", "journal-ref": null, "doi": "10.1007/s11263-020-01366-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain fills the atmosphere with water particles, which breaks the common\nassumption that light travels unaltered from the scene to the camera. While it\nis well-known that rain affects computer vision algorithms, quantifying its\nimpact is difficult. In this context, we present a rain rendering pipeline that\nenables the systematic evaluation of common computer vision algorithms to\ncontrolled amounts of rain. We present three different ways to add synthetic\nrain to existing images datasets: completely physic-based; completely\ndata-driven; and a combination of both. The physic-based rain augmentation\ncombines a physical particle simulator and accurate rain photometric modeling.\nWe validate our rendering methods with a user study, demonstrating our rain is\njudged as much as 73% more realistic than the state-of-theart. Using our\ngenerated rain-augmented KITTI, Cityscapes, and nuScenes datasets, we conduct a\nthorough evaluation of object detection, semantic segmentation, and depth\nestimation algorithms and show that their performance decreases in degraded\nweather, on the order of 15% for object detection, 60% for semantic\nsegmentation, and 6-fold increase in depth estimation error. Finetuning on our\naugmented synthetic data results in improvements of 21% on object detection,\n37% on semantic segmentation, and 8% on depth estimation.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 21:08:41 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Tremblay", "Maxime", ""], ["Halder", "Shirsendu Sukanta", ""], ["de Charette", "Raoul", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""]]}, {"id": "2009.03693", "submitter": "Rao Muhammad Umer", "authors": "Rao Muhammad Umer, Christian Micheloni", "title": "Deep Cyclic Generative Adversarial Residual Convolutional Networks for\n  Real Image Super-Resolution", "comments": "In proceedings of European Conference on Computer Vision (ECCV)\n  Workshops. arXiv admin note: substantial text overlap with arXiv:2005.00953", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recent deep learning based single image super-resolution (SISR) methods\nmostly train their models in a clean data domain where the low-resolution (LR)\nand the high-resolution (HR) images come from noise-free settings (same domain)\ndue to the bicubic down-sampling assumption. However, such degradation process\nis not available in real-world settings. We consider a deep cyclic network\nstructure to maintain the domain consistency between the LR and HR data\ndistributions, which is inspired by the recent success of CycleGAN in the\nimage-to-image translation applications. We propose the Super-Resolution\nResidual Cyclic Generative Adversarial Network (SRResCycGAN) by training with a\ngenerative adversarial network (GAN) framework for the LR to HR domain\ntranslation in an end-to-end manner. We demonstrate our proposed approach in\nthe quantitative and qualitative experiments that generalize well to the real\nimage super-resolution and it is easy to deploy for the mobile/embedded\ndevices. In addition, our SR results on the AIM 2020 Real Image SR Challenge\ndatasets demonstrate that the proposed SR approach achieves comparable results\nas the other state-of-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 11:11:18 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Umer", "Rao Muhammad", ""], ["Micheloni", "Christian", ""]]}, {"id": "2009.03696", "submitter": "Matteo Polsinelli", "authors": "Giuseppe Placidi, Luigi Cinque, Matteo Polsinelli", "title": "Convolutional Neural Networks for Automatic Detection of Artifacts from\n  Independent Components Represented in Scalp Topographies of EEG Signals", "comments": null, "journal-ref": "Computers in Biology and Medicine. 132 (2021) 104347", "doi": "10.1016/j.compbiomed.2021.104347", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalography (EEG) measures the electrical brain activity in\nreal-time by using sensors placed on the scalp. Artifacts, due to eye movements\nand blink, muscular/cardiac activity and generic electrical disturbances, have\nto be recognized and eliminated to allow a correct interpretation of the useful\nbrain signals (UBS) of EEG. Independent Component Analysis (ICA) is effective\nto split the signal into independent components (ICs) whose re-projections on\n2D scalp topographies (images), also called topoplots, allow to\nrecognize/separate artifacts and by UBS. Until now, IC topoplot analysis, a\ngold standard in EEG, has been carried on visually by human experts and, hence,\nnot usable in automatic, fast-response EEG. We present a completely automatic\nand effective framework for EEG artifact recognition by IC topoplots, based on\n2D Convolutional Neural Networks (CNNs), capable to divide topoplots in 4\nclasses: 3 types of artifacts and UBS. The framework setup is described and\nresults are presented, discussed and compared with those obtained by other\ncompetitive strategies. Experiments, carried on public EEG datasets, have shown\nan overall accuracy of above 98%, employing 1.4 sec on a standard PC to\nclassify 32 topoplots, that is to drive an EEG system of 32 sensors. Though not\nreal-time, the proposed framework is efficient enough to be used in\nfast-response EEG-based Brain-Computer Interfaces (BCI) and faster than other\nautomatic methods based on ICs.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 12:40:10 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Placidi", "Giuseppe", ""], ["Cinque", "Luigi", ""], ["Polsinelli", "Matteo", ""]]}, {"id": "2009.03728", "submitter": "Gabriel Machado Resende", "authors": "Gabriel Resende Machado, Eug\\^enio Silva and Ronaldo Ribeiro\n  Goldschmidt", "title": "Adversarial Machine Learning in Image Classification: A Survey Towards\n  the Defender's Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning algorithms have achieved the state-of-the-art performance for\nImage Classification and have been used even in security-critical applications,\nsuch as biometric recognition systems and self-driving cars. However, recent\nworks have shown those algorithms, which can even surpass the human\ncapabilities, are vulnerable to adversarial examples. In Computer Vision,\nadversarial examples are images containing subtle perturbations generated by\nmalicious optimization algorithms in order to fool classifiers. As an attempt\nto mitigate these vulnerabilities, numerous countermeasures have been\nconstantly proposed in literature. Nevertheless, devising an efficient defense\nmechanism has proven to be a difficult task, since many approaches have already\nshown to be ineffective to adaptive attackers. Thus, this self-containing paper\naims to provide all readerships with a review of the latest research progress\non Adversarial Machine Learning in Image Classification, however with a\ndefender's perspective. Here, novel taxonomies for categorizing adversarial\nattacks and defenses are introduced and discussions about the existence of\nadversarial examples are provided. Further, in contrast to exisiting surveys,\nit is also given relevant guidance that should be taken into consideration by\nresearchers when devising and evaluating defenses. Finally, based on the\nreviewed literature, it is discussed some promising paths for future research.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 13:21:55 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Machado", "Gabriel Resende", ""], ["Silva", "Eug\u00eanio", ""], ["Goldschmidt", "Ronaldo Ribeiro", ""]]}, {"id": "2009.03782", "submitter": "Sara Hahner", "authors": "Sara Hahner, Rodrigo Iza-Teran, Jochen Garcke", "title": "Analysis and Prediction of Deforming 3D Shapes using Oriented Bounding\n  Boxes and LSTM Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For sequences of complex 3D shapes in time we present a general approach to\ndetect patterns for their analysis and to predict the deformation by making use\nof structural components of the complex shape. We incorporate long short-term\nmemory (LSTM) layers into an autoencoder to create low dimensional\nrepresentations that allow the detection of patterns in the data and\nadditionally detect the temporal dynamics in the deformation behavior. This is\nachieved with two decoders, one for reconstruction and one for prediction of\nfuture time steps of the sequence. In a preprocessing step the components of\nthe studied object are converted to oriented bounding boxes which capture the\nimpact of plastic deformation and allow reducing the dimensionality of the data\ndescribing the structure. The architecture is tested on the results of 196 car\ncrash simulations of a model with 133 different components, where material\nproperties are varied. In the latent representation we can detect patterns in\nthe plastic deformation for the different components. The predicted bounding\nboxes give an estimate of the final simulation result and their quality is\nimproved in comparison to different baselines.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 08:07:32 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Hahner", "Sara", ""], ["Iza-Teran", "Rodrigo", ""], ["Garcke", "Jochen", ""]]}, {"id": "2009.03787", "submitter": "Brandon Wagstaff", "authors": "Brandon Wagstaff, Jonathan Kelly", "title": "Self-Supervised Scale Recovery for Monocular Depth and Egomotion\n  Estimation", "comments": "Manuscript accepted to the IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The self-supervised loss formulation for jointly training depth and egomotion\nneural networks with monocular images is well studied and has demonstrated\nstate-of-the-art accuracy. One of the main limitations of this approach,\nhowever, is that the depth and egomotion estimates are only determined up to an\nunknown scale. In this paper, we present a novel scale recovery loss that\nenforces consistency between a known camera height and the estimated camera\nheight, generating metric (scaled) depth and egomotion predictions. We show\nthat our proposed method is competitive with other scale recovery techniques\nthat require more information. Further, we demonstrate that our method\nfacilitates network retraining within new environments, whereas other\nscale-resolving approaches are incapable of doing so. Notably, our egomotion\nnetwork is able to produce more accurate estimates than a similar method which\nrecovers scale at test time only.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 14:30:21 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 18:33:46 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 18:02:16 GMT"}, {"version": "v4", "created": "Fri, 16 Jul 2021 14:57:04 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Wagstaff", "Brandon", ""], ["Kelly", "Jonathan", ""]]}, {"id": "2009.03807", "submitter": "Ronak Kosti", "authors": "Prathmesh Madhu, Tilman Marquart, Ronak Kosti, Peter Bell, Andreas\n  Maier and Vincent Christlein", "title": "Understanding Compositional Structures in Art Historical Images using\n  Pose and Gaze Priors", "comments": "To be Published in ECCV 2020 Workshops (VISART V)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image compositions as a tool for analysis of artworks is of extreme\nsignificance for art historians. These compositions are useful in analyzing the\ninteractions in an image to study artists and their artworks. Max Imdahl in his\nwork called Ikonik, along with other prominent art historians of the 20th\ncentury, underlined the aesthetic and semantic importance of the structural\ncomposition of an image. Understanding underlying compositional structures\nwithin images is challenging and a time consuming task. Generating these\nstructures automatically using computer vision techniques (1) can help art\nhistorians towards their sophisticated analysis by saving lot of time;\nproviding an overview and access to huge image repositories and (2) also\nprovide an important step towards an understanding of man made imagery by\nmachines. In this work, we attempt to automate this process using the existing\nstate of the art machine learning techniques, without involving any form of\ntraining. Our approach, inspired by Max Imdahl's pioneering work, focuses on\ntwo central themes of image composition: (a) detection of action regions and\naction lines of the artwork; and (b) pose-based segmentation of foreground and\nbackground. Currently, our approach works for artworks comprising of\nprotagonists (persons) in an image. In order to validate our approach\nqualitatively and quantitatively, we conduct a user study involving experts and\nnon-experts. The outcome of the study highly correlates with our approach and\nalso demonstrates its domain-agnostic capability. We have open-sourced the code\nat https://github.com/image-compostion-canvas-group/image-compostion-canvas.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 15:01:56 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Madhu", "Prathmesh", ""], ["Marquart", "Tilman", ""], ["Kosti", "Ronak", ""], ["Bell", "Peter", ""], ["Maier", "Andreas", ""], ["Christlein", "Vincent", ""]]}, {"id": "2009.03817", "submitter": "Chenhui Li", "authors": "Peiying Zhang, Chenhui Li, Changbo Wang", "title": "VisCode: Embedding Information in Visualization Images using\n  Encoder-Decoder Network", "comments": "11 pages, 16 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach called VisCode for embedding information into\nvisualization images. This technology can implicitly embed data information\nspecified by the user into a visualization while ensuring that the encoded\nvisualization image is not distorted. The VisCode framework is based on a deep\nneural network. We propose to use visualization images and QR codes data as\ntraining data and design a robust deep encoder-decoder network. The designed\nmodel considers the salient features of visualization images to reduce the\nexplicit visual loss caused by encoding. To further support large-scale\nencoding and decoding, we consider the characteristics of information\nvisualization and propose a saliency-based QR code layout algorithm. We present\na variety of practical applications of VisCode in the context of information\nvisualization and conduct a comprehensive evaluation of the perceptual quality\nof encoding, decoding success rate, anti-attack capability, time performance,\netc. The evaluation results demonstrate the effectiveness of VisCode.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 15:48:48 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Zhang", "Peiying", ""], ["Li", "Chenhui", ""], ["Wang", "Changbo", ""]]}, {"id": "2009.03820", "submitter": "Niall O' Mahony", "authors": "Niall O' Mahony, Sean Campbell, Anderson Carvalho, Lenka Krpalkova,\n  Gustavo Velasco-Hernandez, Daniel Riordan, Joseph Walsh", "title": "Understanding and Exploiting Dependent Variables with Deep Metric\n  Learning", "comments": null, "journal-ref": "Proceedings of the 2020 Intelligent Systems Conference\n  (IntelliSys) Volume 1, B. R. Arai K., Kapoor S., Ed. Springer, Cham, 2020,\n  pp. 97 to 113", "doi": "10.1007/978-3-030-55180-3_8", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Deep Metric Learning (DML) approaches learn to represent inputs to a\nlower-dimensional latent space such that the distance between representations\nin this space corresponds with a predefined notion of similarity. This paper\ninvestigates how the mapping element of DML may be exploited in situations\nwhere the salient features in arbitrary classification problems vary over time\nor due to changing underlying variables. Examples of such variable features\ninclude seasonal and time-of-day variations in outdoor scenes in place\nrecognition tasks for autonomous navigation and age/gender variations in\nhuman/animal subjects in classification tasks for medical/ethological studies.\nThrough the use of visualisation tools for observing the distribution of DML\nrepresentations per each query variable for which prior information is\navailable, the influence of each variable on the classification task may be\nbetter understood. Based on these relationships, prior information on these\nsalient background variables may be exploited at the inference stage of the DML\napproach by using a clustering algorithm to improve classification performance.\nThis research proposes such a methodology establishing the saliency of query\nbackground variables and formulating clustering algorithms for better\nseparating latent-space representations at run-time. The paper also discusses\nonline management strategies to preserve the quality and diversity of data and\nthe representation of each class in the gallery of embeddings in the DML\napproach. We also discuss latent works towards understanding the relevance of\nunderlying/multiple variables with DML.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 15:30:45 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Mahony", "Niall O'", ""], ["Campbell", "Sean", ""], ["Carvalho", "Anderson", ""], ["Krpalkova", "Lenka", ""], ["Velasco-Hernandez", "Gustavo", ""], ["Riordan", "Daniel", ""], ["Walsh", "Joseph", ""]]}, {"id": "2009.03863", "submitter": "Koushik Biswas", "authors": "Koushik Biswas, Sandeep Kumar, Shilpak Banerjee, Ashish Kumar Pandey", "title": "TanhSoft -- a family of activation functions combining Tanh and Softplus", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning at its core, contains functions that are composition of a\nlinear transformation with a non-linear function known as activation function.\nIn past few years, there is an increasing interest in construction of novel\nactivation functions resulting in better learning. In this work, we propose a\nfamily of novel activation functions, namely TanhSoft, with four undetermined\nhyper-parameters of the form\ntanh({\\alpha}x+{\\beta}e^{{\\gamma}x})ln({\\delta}+e^x) and tune these\nhyper-parameters to obtain activation functions which are shown to outperform\nseveral well known activation functions. For instance, replacing ReLU with\nxtanh(0.6e^x)improves top-1 classification accuracy on CIFAR-10 by 0.46% for\nDenseNet-169 and 0.7% for Inception-v3 while with tanh(0.87x)ln(1 +e^x) top-1\nclassification accuracy on CIFAR-100 improves by 1.24% for DenseNet-169 and\n2.57% for SimpleNet model.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 16:59:28 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Biswas", "Koushik", ""], ["Kumar", "Sandeep", ""], ["Banerjee", "Shilpak", ""], ["Pandey", "Ashish Kumar", ""]]}, {"id": "2009.03871", "submitter": "Simone Foti", "authors": "Simone Foti, Bongjin Koo, Thomas Dowrick, Joao Ramalhinho, Moustafa\n  Allam, Brian Davidson, Danail Stoyanov and Matthew J. Clarkson", "title": "Intraoperative Liver Surface Completion with Graph Convolutional VAE", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-60365-6_19", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a method based on geometric deep learning to predict\nthe complete surface of the liver, given a partial point cloud of the organ\nobtained during the surgical laparoscopic procedure. We introduce a new data\naugmentation technique that randomly perturbs shapes in their frequency domain\nto compensate the limited size of our dataset. The core of our method is a\nvariational autoencoder (VAE) that is trained to learn a latent space for\ncomplete shapes of the liver. At inference time, the generative part of the\nmodel is embedded in an optimisation procedure where the latent representation\nis iteratively updated to generate a model that matches the intraoperative\npartial point cloud. The effect of this optimisation is a progressive non-rigid\ndeformation of the initially generated shape. Our method is qualitatively\nevaluated on real data and quantitatively evaluated on synthetic data. We\ncompared with a state-of-the-art rigid registration algorithm, that our method\noutperformed in visible areas.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 17:19:31 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 18:28:56 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Foti", "Simone", ""], ["Koo", "Bongjin", ""], ["Dowrick", "Thomas", ""], ["Ramalhinho", "Joao", ""], ["Allam", "Moustafa", ""], ["Davidson", "Brian", ""], ["Stoyanov", "Danail", ""], ["Clarkson", "Matthew J.", ""]]}, {"id": "2009.03878", "submitter": "Sanidhya Mangal", "authors": "Sanidhya Mangal, Aanchal Chaurasia and Ayush Khajanchi", "title": "Convolution Neural Networks for diagnosing colon and lung cancer\n  histopathological images", "comments": "10 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung and Colon cancer are one of the leading causes of mortality and\nmorbidity in adults. Histopathological diagnosis is one of the key components\nto discern cancer type. The aim of the present research is to propose a\ncomputer aided diagnosis system for diagnosing squamous cell carcinomas and\nadenocarcinomas of lung as well as adenocarcinomas of colon using convolutional\nneural networks by evaluating the digital pathology images for these cancers.\nHereby, rendering artificial intelligence as useful technology in the near\nfuture. A total of 2500 digital images were acquired from LC25000 dataset\ncontaining 5000 images for each class. A shallow neural network architecture\nwas used classify the histopathological slides into squamous cell carcinomas,\nadenocarcinomas and benign for the lung. Similar model was used to classify\nadenocarcinomas and benign for colon. The diagnostic accuracy of more than 97%\nand 96% was recorded for lung and colon respectively.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 17:36:24 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Mangal", "Sanidhya", ""], ["Chaurasia", "Aanchal", ""], ["Khajanchi", "Ayush", ""]]}, {"id": "2009.03949", "submitter": "Zeyu Wang", "authors": "Zeyu Wang, Berthy Feng, Karthik Narasimhan, Olga Russakovsky", "title": "Towards Unique and Informative Captioning of Images", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite considerable progress, state of the art image captioning models\nproduce generic captions, leaving out important image details. Furthermore,\nthese systems may even misrepresent the image in order to produce a simpler\ncaption consisting of common concepts. In this paper, we first analyze both\nmodern captioning systems and evaluation metrics through empirical experiments\nto quantify these phenomena. We find that modern captioning systems return\nhigher likelihoods for incorrect distractor sentences compared to ground truth\ncaptions, and that evaluation metrics like SPICE can be 'topped' using simple\ncaptioning systems relying on object detectors. Inspired by these observations,\nwe design a new metric (SPICE-U) by introducing a notion of uniqueness over the\nconcepts generated in a caption. We show that SPICE-U is better correlated with\nhuman judgements compared to SPICE, and effectively captures notions of\ndiversity and descriptiveness. Finally, we also demonstrate a general technique\nto improve any existing captioning model -- by using mutual information as a\nre-ranking objective during decoding. Empirically, this results in more unique\nand informative captions, and improves three different state-of-the-art models\non SPICE-U as well as average score over existing metrics.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 19:01:33 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Wang", "Zeyu", ""], ["Feng", "Berthy", ""], ["Narasimhan", "Karthik", ""], ["Russakovsky", "Olga", ""]]}, {"id": "2009.03964", "submitter": "Hunter Goforth", "authors": "Hunter Goforth, Xiaoyan Hu, Michael Happold, Simon Lucey", "title": "Joint Pose and Shape Estimation of Vehicles from LiDAR Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating the pose and shape of vehicles from\nLiDAR scans, a common problem faced by the autonomous vehicle community. Recent\nwork has tended to address pose and shape estimation separately in isolation,\ndespite the inherent connection between the two. We investigate a method of\njointly estimating shape and pose where a single encoding is learned from which\nshape and pose may be decoded in an efficient yet effective manner. We\nadditionally introduce a novel joint pose and shape loss, and show that this\njoint training method produces better results than independently-trained pose\nand shape estimators. We evaluate our method on both synthetic data and\nreal-world data, and show superior performance against a state-of-the-art\nbaseline.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 19:22:23 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Goforth", "Hunter", ""], ["Hu", "Xiaoyan", ""], ["Happold", "Michael", ""], ["Lucey", "Simon", ""]]}, {"id": "2009.03977", "submitter": "Karl Kaiser", "authors": "Maxfield E. Green, Karl Kaiser, Nat Shenton", "title": "Modeling Wildfire Perimeter Evolution using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased size and frequency of wildfire eventsworldwide, accurate\nreal-time prediction of evolving wildfirefronts is a crucial component of\nfirefighting efforts and for-est management practices. We propose a wildfire\nspreadingmodel that predicts the evolution of the wildfire perimeter in24 hour\nperiods. The fire spreading simulation is based ona deep convolutional neural\nnetwork (CNN) that is trainedon remotely sensed atmospheric and environmental\ntime se-ries data. We show that the model is able to learn wildfirespreading\ndynamics from real historic data sets from a seriesof wildfires in the Western\nSierra Nevada Mountains in Cal-ifornia. We validate the model on a previously\nunseen wild-fire and produce realistic results that significantly\noutperformhistoric alternatives with validation accuracies ranging from78% -\n98%\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 20:06:01 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Green", "Maxfield E.", ""], ["Kaiser", "Karl", ""], ["Shenton", "Nat", ""]]}, {"id": "2009.03998", "submitter": "Tai-Xiang Jiang", "authors": "Guangjing Song, Michael K. Ng, Tai-Xiang Jiang", "title": "Tangent Space Based Alternating Projections for Nonnegative Low Rank\n  Matrix Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a new alternating projection method to compute\nnonnegative low rank matrix approximation for nonnegative matrices. In the\nnonnegative low rank matrix approximation method, the projection onto the\nmanifold of fixed rank matrices can be expensive as the singular value\ndecomposition is required. We propose to use the tangent space of the point in\nthe manifold to approximate the projection onto the manifold in order to reduce\nthe computational cost. We show that the sequence generated by the alternating\nprojections onto the tangent spaces of the fixed rank matrices manifold and the\nnonnegative matrix manifold, converge linearly to a point in the intersection\nof the two manifolds where the convergent point is sufficiently close to\noptimal solutions. This convergence result based inexact projection onto the\nmanifold is new and is not studied in the literature. Numerical examples in\ndata clustering, pattern recognition and hyperspectral data analysis are given\nto demonstrate that the performance of the proposed method is better than that\nof nonnegative matrix factorization methods in terms of computational time and\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 05:25:16 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Song", "Guangjing", ""], ["Ng", "Michael K.", ""], ["Jiang", "Tai-Xiang", ""]]}, {"id": "2009.04004", "submitter": "Achyut Mani Tripathi", "authors": "Achyut Mani Tripathi, Ashish Mishra", "title": "Fuzzy Unique Image Transformation: Defense Against Adversarial Attacks\n  On Deep COVID-19 Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early identification of COVID-19 using a deep model trained on Chest X-Ray\nand CT images has gained considerable attention from researchers to speed up\nthe process of identification of active COVID-19 cases. These deep models act\nas an aid to hospitals that suffer from the unavailability of specialists or\nradiologists, specifically in remote areas. Various deep models have been\nproposed to detect the COVID-19 cases, but few works have been performed to\nprevent the deep models against adversarial attacks capable of fooling the deep\nmodel by using a small perturbation in image pixels. This paper presents an\nevaluation of the performance of deep COVID-19 models against adversarial\nattacks. Also, it proposes an efficient yet effective Fuzzy Unique Image\nTransformation (FUIT) technique that downsamples the image pixels into an\ninterval. The images obtained after the FUIT transformation are further\nutilized for training the secure deep model that preserves high accuracy of the\ndiagnosis of COVID-19 cases and provides reliable defense against the\nadversarial attacks. The experiments and results show the proposed model\nprevents the deep model against the six adversarial attacks and maintains high\naccuracy to classify the COVID-19 cases from the Chest X-Ray image and CT image\nDatasets. The results also recommend that a careful inspection is required\nbefore practically applying the deep models to diagnose the COVID-19 cases.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 21:35:24 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Tripathi", "Achyut Mani", ""], ["Mishra", "Ashish", ""]]}, {"id": "2009.04009", "submitter": "Reuben Dorent", "authors": "Reuben Dorent, Thomas Booth, Wenqi Li, Carole H. Sudre, Sina\n  Kafiabadi, Jorge Cardoso, Sebastien Ourselin, Tom Vercauteren", "title": "Learning joint segmentation of tissues and brain lesions from\n  task-specific hetero-modal domain-shifted datasets", "comments": "MIDL 2019 special issue - Medical Image Analysis", "journal-ref": null, "doi": "10.1016/j.media.2020.101862", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain tissue segmentation from multimodal MRI is a key building block of many\nneuroimaging analysis pipelines. Established tissue segmentation approaches\nhave, however, not been developed to cope with large anatomical changes\nresulting from pathology, such as white matter lesions or tumours, and often\nfail in these cases. In the meantime, with the advent of deep neural networks\n(DNNs), segmentation of brain lesions has matured significantly. However, few\nexisting approaches allow for the joint segmentation of normal tissue and brain\nlesions. Developing a DNN for such a joint task is currently hampered by the\nfact that annotated datasets typically address only one specific task and rely\non task-specific imaging protocols including a task-specific set of imaging\nmodalities. In this work, we propose a novel approach to build a joint tissue\nand lesion segmentation model from aggregated task-specific hetero-modal\ndomain-shifted and partially-annotated datasets. Starting from a variational\nformulation of the joint problem, we show how the expected risk can be\ndecomposed and optimised empirically. We exploit an upper bound of the risk to\ndeal with heterogeneous imaging modalities across datasets. To deal with\npotential domain shift, we integrated and tested three conventional techniques\nbased on data augmentation, adversarial learning and pseudo-healthy generation.\nFor each individual task, our joint approach reaches comparable performance to\ntask-specific and fully-supervised models. The proposed framework is assessed\non two different types of brain lesions: White matter lesions and gliomas. In\nthe latter case, lacking a joint ground-truth for quantitative assessment\npurposes, we propose and use a novel clinically-relevant qualitative assessment\nmethodology.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 22:00:00 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Dorent", "Reuben", ""], ["Booth", "Thomas", ""], ["Li", "Wenqi", ""], ["Sudre", "Carole H.", ""], ["Kafiabadi", "Sina", ""], ["Cardoso", "Jorge", ""], ["Ourselin", "Sebastien", ""], ["Vercauteren", "Tom", ""]]}, {"id": "2009.04042", "submitter": "Julian Del Gobbo", "authors": "Juli\\'an Del Gobbo, Rosana Matuk Herrera", "title": "Unconstrained Text Detection in Manga: a New Dataset and Baseline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection and recognition of unconstrained text is an open problem in\nresearch. Text in comic books has unusual styles that raise many challenges for\ntext detection. This work aims to binarize text in a comic genre with highly\nsophisticated text styles: Japanese manga. To overcome the lack of a manga\ndataset with text annotations at a pixel level, we create our own. To improve\nthe evaluation and search of an optimal model, in addition to standard metrics\nin binarization, we implement other special metrics. Using these resources, we\ndesigned and evaluated a deep network model, outperforming current methods for\ntext binarization in manga in most metrics.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 00:16:51 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Del Gobbo", "Juli\u00e1n", ""], ["Herrera", "Rosana Matuk", ""]]}, {"id": "2009.04057", "submitter": "Gongbo Liang", "authors": "Gongbo Liang, Yu Zhang, Xiaoqin Wang, Nathan Jacobs", "title": "Improved Trainable Calibration Method for Neural Networks on Medical\n  Imaging Classification", "comments": "Accepted to the 31th British Machine Vision Conference (BMVC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown that deep neural networks can achieve super-human\nperformance in a wide range of image classification tasks in the medical\nimaging domain. However, these works have primarily focused on classification\naccuracy, ignoring the important role of uncertainty quantification.\nEmpirically, neural networks are often miscalibrated and overconfident in their\npredictions. This miscalibration could be problematic in any automatic\ndecision-making system, but we focus on the medical field in which neural\nnetwork miscalibration has the potential to lead to significant treatment\nerrors. We propose a novel calibration approach that maintains the overall\nclassification accuracy while significantly improving model calibration. The\nproposed approach is based on expected calibration error, which is a common\nmetric for quantifying miscalibration. Our approach can be easily integrated\ninto any classification task as an auxiliary loss term, thus not requiring an\nexplicit training round for calibration. We show that our approach reduces\ncalibration error significantly across various architectures and datasets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 01:25:53 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Liang", "Gongbo", ""], ["Zhang", "Yu", ""], ["Wang", "Xiaoqin", ""], ["Jacobs", "Nathan", ""]]}, {"id": "2009.04065", "submitter": "Numair Khan", "authors": "Numair Khan, Min H. Kim, James Tompkin", "title": "View-consistent 4D Light Field Depth Estimation", "comments": "BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to compute depth maps for every sub-aperture image in a\nlight field in a view consistent way. Previous light field depth estimation\nmethods typically estimate a depth map only for the central sub-aperture view,\nand struggle with view consistent estimation. Our method precisely defines\ndepth edges via EPIs, then we diffuse these edges spatially within the central\nview. These depth estimates are then propagated to all other views in an\nocclusion-aware way. Finally, disoccluded regions are completed by diffusion in\nEPI space. Our method runs efficiently with respect to both other classical and\ndeep learning-based approaches, and achieves competitive quantitative metrics\nand qualitative performance on both synthetic and real-world light fields\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 01:47:34 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Khan", "Numair", ""], ["Kim", "Min H.", ""], ["Tompkin", "James", ""]]}, {"id": "2009.04083", "submitter": "Chase Gaudet", "authors": "Chase J Gaudet and Anthony S Maida", "title": "Generalizing Complex/Hyper-complex Convolutions to Vector Map\n  Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the core reasons that complex and hypercomplex valued neural\nnetworks offer improvements over their real-valued counterparts is the weight\nsharing mechanism and treating multidimensional data as a single entity. Their\nalgebra linearly combines the dimensions, making each dimension related to the\nothers. However, both are constrained to a set number of dimensions, two for\ncomplex and four for quaternions. Here we introduce novel vector map\nconvolutions which capture both of these properties provided by\ncomplex/hypercomplex convolutions, while dropping the unnatural dimensionality\nconstraints they impose. This is achieved by introducing a system that mimics\nthe unique linear combination of input dimensions, such as the Hamilton product\nfor quaternions. We perform three experiments to show that these novel vector\nmap convolutions seem to capture all the benefits of complex and hyper-complex\nnetworks, such as their ability to capture internal latent relations, while\navoiding the dimensionality restriction.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 03:00:03 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Gaudet", "Chase J", ""], ["Maida", "Anthony S", ""]]}, {"id": "2009.04091", "submitter": "Binh Nguyen Xuan", "authors": "Binh X. Nguyen, Binh D. Nguyen, Gustavo Carneiro, Erman Tjiputra,\n  Quang D. Tran, Thanh-Toan Do", "title": "Deep Metric Learning Meets Deep Clustering: An Novel Unsupervised\n  Approach for Feature Embedding", "comments": "Accepted in BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised Deep Distance Metric Learning (UDML) aims to learn sample\nsimilarities in the embedding space from an unlabeled dataset. Traditional UDML\nmethods usually use the triplet loss or pairwise loss which requires the mining\nof positive and negative samples w.r.t. anchor data points. This is, however,\nchallenging in an unsupervised setting as the label information is not\navailable. In this paper, we propose a new UDML method that overcomes that\nchallenge. In particular, we propose to use a deep clustering loss to learn\ncentroids, i.e., pseudo labels, that represent semantic classes. During\nlearning, these centroids are also used to reconstruct the input samples. It\nhence ensures the representativeness of centroids - each centroid represents\nvisually similar samples. Therefore, the centroids give information about\npositive (visually similar) and negative (visually dissimilar) samples. Based\non pseudo labels, we propose a novel unsupervised metric loss which enforces\nthe positive concentration and negative separation of samples in the embedding\nspace. Experimental results on benchmarking datasets show that the proposed\napproach outperforms other UDML methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 04:02:04 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Nguyen", "Binh X.", ""], ["Nguyen", "Binh D.", ""], ["Carneiro", "Gustavo", ""], ["Tjiputra", "Erman", ""], ["Tran", "Quang D.", ""], ["Do", "Thanh-Toan", ""]]}, {"id": "2009.04110", "submitter": "Asim Khan", "authors": "Asim Khan, Umair Nawaz, Anwaar Ulhaq and Randall W. Robinson", "title": "Real-time Plant Health Assessment Via Implementing Cloud-based Scalable\n  Transfer Learning On AWS DeepLens", "comments": "10 Pages, 12 Figures and 6 Tables", "journal-ref": null, "doi": "10.1371/journal.pone.0243243", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Agriculture sector, control of plant leaf diseases is crucial as it\ninfluences the quality and production of plant species with an impact on the\neconomy of any country. Therefore, automated identification and classification\nof plant leaf disease at an early stage is essential to reduce economic loss\nand to conserve the specific species. Previously, to detect and classify plant\nleaf disease, various Machine Learning models have been proposed; however, they\nlack usability due to hardware incompatibility, limited scalability and\ninefficiency in practical usage. Our proposed DeepLens Classification and\nDetection Model (DCDM) approach deal with such limitations by introducing\nautomated detection and classification of the leaf diseases in fruits (apple,\ngrapes, peach and strawberry) and vegetables (potato and tomato) via scalable\ntransfer learning on AWS SageMaker and importing it on AWS DeepLens for\nreal-time practical usability. Cloud integration provides scalability and\nubiquitous access to our approach. Our experiments on extensive image data set\nof healthy and unhealthy leaves of fruits and vegetables showed an accuracy of\n98.78% with a real-time diagnosis of plant leaves diseases. We used forty\nthousand images for the training of deep learning model and then evaluated it\non ten thousand images. The process of testing an image for disease diagnosis\nand classification using AWS DeepLens on average took 0.349s, providing disease\ninformation to the user in less than a second.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 05:23:34 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 16:58:20 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Khan", "Asim", ""], ["Nawaz", "Umair", ""], ["Ulhaq", "Anwaar", ""], ["Robinson", "Randall W.", ""]]}, {"id": "2009.04127", "submitter": "Jesper Christensen", "authors": "Jesper Haahr Christensen, Lars Valdemar Mogensen, Ole Ravn", "title": "Single Image Super-Resolution for Domain-Specific Ultra-Low Bandwidth\n  Image Transmission", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low-bandwidth communication, such as underwater acoustic communication, is\nlimited by best-case data rates of 30--50 kbit/s. This renders such channels\nunusable or inefficient at best for single image, video, or other\nbandwidth-demanding sensor-data transmission. To combat data-transmission\nbottlenecks, we consider practical use-cases within the maritime domain and\ninvestigate the prospect of Single Image Super-Resolution methodologies. This\nis investigated on a large, diverse dataset obtained during years of trawl\nfishing where cameras have been placed in the fishing nets. We propose\ndown-sampling images to a low-resolution low-size version of about 1 kB that\nsatisfies underwater acoustic bandwidth requirements for even several frames\nper second. A neural network is then trained to perform up-sampling, trying to\nreconstruct the original image. We aim to investigate the quality of\nreconstructed images and prospects for such methods in practical use-cases in\ngeneral. Our focus in this work is solely on learning to reconstruct the\nhigh-resolution images on \"real-world\" data. We show that our method achieves\nbetter perceptual quality and superior reconstruction than generic bicubic\nup-sampling and motivates further work in this area for underwater\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 06:44:30 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 06:27:21 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Christensen", "Jesper Haahr", ""], ["Mogensen", "Lars Valdemar", ""], ["Ravn", "Ole", ""]]}, {"id": "2009.04153", "submitter": "Minghui Qiu", "authors": "Mengli Cheng, Minghui Qiu, Xing Shi, Jun Huang, Wei Lin", "title": "One-shot Text Field Labeling using Attention and Belief Propagation for\n  Structure Information Extraction", "comments": "9 pages", "journal-ref": "ACMMM 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Structured information extraction from document images usually consists of\nthree steps: text detection, text recognition, and text field labeling. While\ntext detection and text recognition have been heavily studied and improved a\nlot in literature, text field labeling is less explored and still faces many\nchallenges. Existing learning based methods for text labeling task usually\nrequire a large amount of labeled examples to train a specific model for each\ntype of document. However, collecting large amounts of document images and\nlabeling them is difficult and sometimes impossible due to privacy issues.\nDeploying separate models for each type of document also consumes a lot of\nresources. Facing these challenges, we explore one-shot learning for the text\nfield labeling task. Existing one-shot learning methods for the task are mostly\nrule-based and have difficulty in labeling fields in crowded regions with few\nlandmarks and fields consisting of multiple separate text regions. To alleviate\nthese problems, we proposed a novel deep end-to-end trainable approach for\none-shot text field labeling, which makes use of attention mechanism to\ntransfer the layout information between document images. We further applied\nconditional random field on the transferred layout information for the\nrefinement of field labeling. We collected and annotated a real-world one-shot\nfield labeling dataset with a large variety of document types and conducted\nextensive experiments to examine the effectiveness of the proposed model. To\nstimulate research in this direction, the collected dataset and the one-shot\nmodel will be released1.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 08:11:34 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Cheng", "Mengli", ""], ["Qiu", "Minghui", ""], ["Shi", "Xing", ""], ["Huang", "Jun", ""], ["Lin", "Wei", ""]]}, {"id": "2009.04160", "submitter": "Mihaela Breaban", "authors": "Radu Miron, Cosmin Moisii, Mihaela Breaban", "title": "Revealing Lung Affections from CTs. A Comparative Analysis of Various\n  Deep Learning Approaches for Dealing with Volumetric Data", "comments": "ImageClef2020 Tuberculosis task", "journal-ref": "Working Notes of CLEF 2020 - Conference and Labs of the Evaluation\n  Forum Thessaloniki, Greece, September22-25, 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper presents and comparatively analyses several deep learning\napproaches to automatically detect tuberculosis related lesions in lung CTs, in\nthe context of the ImageClef 2020 Tuberculosis task. Three classes of methods,\ndifferent with respect to the way the volumetric data is given as input to\nneural network-based classifiers are discussed and evaluated. All these come\nwith a rich experimental analysis comprising a variety of neural network\narchitectures, various segmentation algorithms and data augmentation schemes.\nThe reported work belongs to the SenticLab.UAIC team, which obtained the best\nresults in the competition.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 08:34:18 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Miron", "Radu", ""], ["Moisii", "Cosmin", ""], ["Breaban", "Mihaela", ""]]}, {"id": "2009.04170", "submitter": "Wonpyo Park", "authors": "Wonpyo Park, Wonjae Kim, Kihyun You, Minsu Cho", "title": "Diversified Mutual Learning for Deep Metric Learning", "comments": "Accepted to ECCV Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutual learning is an ensemble training strategy to improve generalization by\ntransferring individual knowledge to each other while simultaneously training\nmultiple models. In this work, we propose an effective mutual learning method\nfor deep metric learning, called Diversified Mutual Metric Learning, which\nenhances embedding models with diversified mutual learning. We transfer\nrelational knowledge for deep metric learning by leveraging three kinds of\ndiversities in mutual learning: (1) model diversity from different\ninitializations of models, (2) temporal diversity from different frequencies of\nparameter update, and (3) view diversity from different augmentations of\ninputs. Our method is particularly adequate for inductive transfer learning at\nthe lack of large-scale data, where the embedding model is initialized with a\npretrained model and then fine-tuned on a target dataset. Extensive experiments\nshow that our method significantly improves individual models as well as their\nensemble. Finally, the proposed method with a conventional triplet loss\nachieves the state-of-the-art performance of Recall@1 on standard datasets:\n69.9 on CUB-200-2011 and 89.1 on CARS-196.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 09:00:16 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Park", "Wonpyo", ""], ["Kim", "Wonjae", ""], ["You", "Kihyun", ""], ["Cho", "Minsu", ""]]}, {"id": "2009.04177", "submitter": "Ke Zhang", "authors": "Ke Zhang, Yukun Su, Xiwang Guo, Liang Qi, and Zhenbing Zhao", "title": "MU-GAN: Facial Attribute Editing based on Multi-attention Mechanism", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial attribute editing has mainly two objectives: 1) translating image from\na source domain to a target one, and 2) only changing the facial regions\nrelated to a target attribute and preserving the attribute-excluding details.\nIn this work, we propose a Multi-attention U-Net-based Generative Adversarial\nNetwork (MU-GAN). First, we replace a classic convolutional encoder-decoder\nwith a symmetric U-Net-like structure in a generator, and then apply an\nadditive attention mechanism to build attention-based U-Net connections for\nadaptively transferring encoder representations to complement a decoder with\nattribute-excluding detail and enhance attribute editing ability. Second, a\nself-attention mechanism is incorporated into convolutional layers for modeling\nlong-range and multi-level dependencies across image regions. experimental\nresults indicate that our method is capable of balancing attribute editing\nability and details preservation ability, and can decouple the correlation\namong attributes. It outperforms the state-of-the-art methods in terms of\nattribute manipulation accuracy and image quality.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 09:25:04 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Zhang", "Ke", ""], ["Su", "Yukun", ""], ["Guo", "Xiwang", ""], ["Qi", "Liang", ""], ["Zhao", "Zhenbing", ""]]}, {"id": "2009.04181", "submitter": "Jiawei Liu", "authors": "Jiawei Liu, Xierong Zhu, Zheng-Jun Zha", "title": "Temporal Attribute-Appearance Learning Network for Video-based Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video-based person re-identification aims to match a specific pedestrian in\nsurveillance videos across different time and locations. Human attributes and\nappearance are complementary to each other, both of them contribute to\npedestrian matching. In this work, we propose a novel Temporal\nAttribute-Appearance Learning Network (TALNet) for video-based person\nre-identification. TALNet simultaneously exploits human attributes and\nappearance to learn comprehensive and effective pedestrian representations from\nvideos. It explores hard visual attention and temporal-semantic context for\nattributes, and spatial-temporal dependencies among body parts for appearance,\nto boost the learning of them. Specifically, an attribute branch network is\nproposed with a spatial attention block and a temporal-semantic context block\nfor learning robust attribute representation. The spatial attention block\nfocuses the network on corresponding regions within video frames related to\neach attribute, the temporal-semantic context block learns both the temporal\ncontext for each attribute across video frames and the semantic context among\nattributes in each video frame. The appearance branch network is designed to\nlearn effective appearance representation from both whole body and body parts\nwith spatial-temporal dependencies among them. TALNet leverages the\ncomplementation between attribute and appearance representations, and jointly\noptimizes them by multi-task learning fashion. Moreover, we annotate ID-level\nattributes for each pedestrian in the two commonly used video datasets.\nExtensive experiments on these datasets, have verified the superiority of\nTALNet over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 09:28:07 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Liu", "Jiawei", ""], ["Zhu", "Xierong", ""], ["Zha", "Zheng-Jun", ""]]}, {"id": "2009.04185", "submitter": "Yi Zhou", "authors": "Yi Zhou, Yin Cui, Xiaoke Xu, Jidong Suo, Xiaoming Liu", "title": "Small-floating Target Detection in Sea Clutter via Visual Feature\n  Classifying in the Time-Doppler Spectra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is challenging to detect small-floating object in the sea clutter for a\nsurface radar. In this paper, we have observed that the backscatters from the\ntarget brake the continuity of the underlying motion of the sea surface in the\ntime-Doppler spectra (TDS) images. Following this visual clue, we exploit the\nlocal binary pattern (LBP) to measure the variations of texture in the TDS\nimages. It is shown that the radar returns containing target and those only\nhaving clutter are separable in the feature space of LBP. An unsupervised\none-class support vector machine (SVM) is then utilized to detect the deviation\nof the LBP histogram of the clutter. The outiler of the detector is classified\nas the target. In the real-life IPIX radar data sets, our visual feature based\ndetector shows favorable detection rate compared to other three existing\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 09:35:03 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Zhou", "Yi", ""], ["Cui", "Yin", ""], ["Xu", "Xiaoke", ""], ["Suo", "Jidong", ""], ["Liu", "Xiaoming", ""]]}, {"id": "2009.04247", "submitter": "Hanlin Chen", "authors": "Hanlin Chen, Li'an Zhuo, Baochang Zhang, Xiawu Zheng, Jianzhuang Liu,\n  Rongrong Ji, David Doermann, Guodong Guo", "title": "Binarized Neural Architecture Search for Efficient Object Recognition", "comments": "arXiv admin note: substantial text overlap with arXiv:1911.10862", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional neural architecture search (NAS) has a significant impact in\ncomputer vision by automatically designing network architectures for various\ntasks. In this paper, binarized neural architecture search (BNAS), with a\nsearch space of binarized convolutions, is introduced to produce extremely\ncompressed models to reduce huge computational cost on embedded devices for\nedge computing. The BNAS calculation is more challenging than NAS due to the\nlearning inefficiency caused by optimization requirements and the huge\narchitecture space, and the performance loss when handling the wild data in\nvarious computing applications. To address these issues, we introduce operation\nspace reduction and channel sampling into BNAS to significantly reduce the cost\nof searching. This is accomplished through a performance-based strategy that is\nrobust to wild data, which is further used to abandon less potential\noperations. Furthermore, we introduce the Upper Confidence Bound (UCB) to solve\n1-bit BNAS. Two optimization methods for binarized neural networks are used to\nvalidate the effectiveness of our BNAS. Extensive experiments demonstrate that\nthe proposed BNAS achieves a comparable performance to NAS on both CIFAR and\nImageNet databases. An accuracy of $96.53\\%$ vs. $97.22\\%$ is achieved on the\nCIFAR-10 dataset, but with a significantly compressed model, and a $40\\%$\nfaster search than the state-of-the-art PC-DARTS. On the wild face recognition\ntask, our binarized models achieve a performance similar to their corresponding\nfull-precision models.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 15:51:23 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Chen", "Hanlin", ""], ["Zhuo", "Li'an", ""], ["Zhang", "Baochang", ""], ["Zheng", "Xiawu", ""], ["Liu", "Jianzhuang", ""], ["Ji", "Rongrong", ""], ["Doermann", "David", ""], ["Guo", "Guodong", ""]]}, {"id": "2009.04264", "submitter": "Sandro Braun", "authors": "Sandro Braun, Patrick Esser, Bj\\\"orn Ommer", "title": "Unsupervised Part Discovery by Unsupervised Disentanglement", "comments": "GCPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of discovering part segmentations of articulated\nobjects without supervision. In contrast to keypoints, part segmentations\nprovide information about part localizations on the level of individual pixels.\nCapturing both locations and semantics, they are an attractive target for\nsupervised learning approaches. However, large annotation costs limit the\nscalability of supervised algorithms to other object categories than humans.\nUnsupervised approaches potentially allow to use much more data at a lower\ncost. Most existing unsupervised approaches focus on learning abstract\nrepresentations to be refined with supervision into the final representation.\nOur approach leverages a generative model consisting of two disentangled\nrepresentations for an object's shape and appearance and a latent variable for\nthe part segmentation. From a single image, the trained model infers a semantic\npart segmentation map. In experiments, we compare our approach to previous\nstate-of-the-art approaches and observe significant gains in segmentation\naccuracy and shape consistency. Our work demonstrates the feasibility to\ndiscover semantic part segmentations without supervision.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 12:34:37 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 09:31:48 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Braun", "Sandro", ""], ["Esser", "Patrick", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "2009.04284", "submitter": "Hung Tuan  Nguyen Mr.", "authors": "Hung Tuan Nguyen, Tsubasa Nakamura, Cuong Tuan Nguyen and Masaki\n  Nakagawa", "title": "Online trajectory recovery from offline handwritten Japanese kanji\n  characters", "comments": "9 pages, ICPR2020 (reviewing)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, it is straightforward to render an offline handwriting image from\nan online handwriting pattern. However, it is challenging to reconstruct an\nonline handwriting pattern given an offline handwriting image, especially for\nmultiple-stroke character as Japanese kanji. The multiple-stroke character\nrequires not only point coordinates but also stroke orders whose difficulty is\nexponential growth by the number of strokes. Besides, several crossed and touch\npoints might increase the difficulty of the recovered task. We propose a deep\nneural network-based method to solve the recovered task using a large online\nhandwriting database. Our proposed model has two main components: Convolutional\nNeural Network-based encoder and Long Short-Term Memory Network-based decoder\nwith an attention layer. The encoder focuses on feature extraction while the\ndecoder refers to the extracted features and generates the time-sequences of\ncoordinates. We also demonstrate the effect of the attention layer to guide the\ndecoder during the reconstruction. We evaluate the performance of the proposed\nmethod by both visual verification and handwritten character recognition.\nAlthough the visual verification reveals some problems, the recognition\nexperiments demonstrate the effect of trajectory recovery in improving the\naccuracy of offline handwritten character recognition when online recognition\nfor the recovered trajectories are combined.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 13:05:50 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Nguyen", "Hung Tuan", ""], ["Nakamura", "Tsubasa", ""], ["Nguyen", "Cuong Tuan", ""], ["Nakagawa", "Masaki", ""]]}, {"id": "2009.04286", "submitter": "Rui Zhao", "authors": "Rui Zhao and Daniel P.K. Lun and Kin-Man Lam", "title": "Enhancing and Learning Denoiser without Clean Reference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on learning-based image denoising have achieved promising\nperformance on various noise reduction tasks. Most of these deep denoisers are\ntrained either under the supervision of clean references, or unsupervised on\nsynthetic noise. The assumption with the synthetic noise leads to poor\ngeneralization when facing real photographs. To address this issue, we propose\na novel deep image-denoising method by regarding the noise reduction task as a\nspecial case of the noise transference task. Learning noise transference\nenables the network to acquire the denoising ability by observing the corrupted\nsamples. The results on real-world denoising benchmarks demonstrate that our\nproposed method achieves promising performance on removing realistic noises,\nmaking it a potential solution to practical noise reduction problems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 13:15:31 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 13:13:17 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhao", "Rui", ""], ["Lun", "Daniel P. K.", ""], ["Lam", "Kin-Man", ""]]}, {"id": "2009.04299", "submitter": "Alex Postnikov", "authors": "A. Postnikov, A. Gamayunov, G. Ferrer", "title": "HSFM-$\\Sigma$nn: Combining a Feedforward Motion Prediction Network and\n  Covariance Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new method for motion prediction:\nHSFM-$\\Sigma$nn. Our proposed method combines two different approaches: a\nfeedforward network whose layers are model-based transition functions using the\nHSFM and a Neural Network (NN), on each of these layers, for covariance\nprediction. We will compare our method with classical methods for covariance\nestimation showing their limitations. We will also compare with a\nlearning-based approach, social-LSTM, showing that our method is more precise\nand efficient.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 13:46:35 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Postnikov", "A.", ""], ["Gamayunov", "A.", ""], ["Ferrer", "G.", ""]]}, {"id": "2009.04365", "submitter": "Flavio de Barros Vidal", "authors": "Andre S. Abade, Paulo Afonso Ferreira and Flavio de Barros Vidal", "title": "Plant Diseases recognition on images using Convolutional Neural\n  Networks: A Systematic Review", "comments": "47 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plant diseases are considered one of the main factors influencing food\nproduction and minimize losses in production, and it is essential that crop\ndiseases have fast detection and recognition. The recent expansion of deep\nlearning methods has found its application in plant disease detection, offering\na robust tool with highly accurate results. In this context, this work presents\na systematic review of the literature that aims to identify the state of the\nart of the use of convolutional neural networks(CNN) in the process of\nidentification and classification of plant diseases, delimiting trends, and\nindicating gaps. In this sense, we present 121 papers selected in the last ten\nyears with different approaches to treat aspects related to disease detection,\ncharacteristics of the data set, the crops and pathogens investigated. From the\nresults of the systematic review, it is possible to understand the innovative\ntrends regarding the use of CNNs in the identification of plant diseases and to\nidentify the gaps that need the attention of the research community.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 15:36:04 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Abade", "Andre S.", ""], ["Ferreira", "Paulo Afonso", ""], ["Vidal", "Flavio de Barros", ""]]}, {"id": "2009.04420", "submitter": "Yixing Huang", "authors": "Yixing Huang, Fuxin Fan, Christopher Syben, Philipp Roser, Leonid\n  Mill, Andreas Maier", "title": "Cephalogram Synthesis and Landmark Detection in Dental Cone-Beam CT\n  Systems", "comments": null, "journal-ref": "Medical Image Analysis 2021", "doi": "10.1016/j.media.2021.102028", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the lack of standardized 3D cephalometric analytic methodology, 2D\ncephalograms synthesized from 3D cone-beam computed tomography (CBCT) volumes\nare widely used for cephalometric analysis in dental CBCT systems. However,\ncompared with conventional X-ray film based cephalograms, such synthetic\ncephalograms lack image contrast and resolution. In addition, the radiation\ndose during the scan for 3D reconstruction causes potential health risks. In\nthis work, we propose a sigmoid-based intensity transform that uses the\nnonlinear optical property of X-ray films to increase image contrast of\nsynthetic cephalograms. To improve image resolution, super resolution deep\nlearning techniques are investigated. For low dose purpose, the pixel-to-pixel\ngenerative adversarial network (pix2pixGAN) is proposed for 2D cephalogram\nsynthesis directly from two CBCT projections. For landmark detection in the\nsynthetic cephalograms, an efficient automatic landmark detection method using\nthe combination of LeNet-5 and ResNet50 is proposed. Our experiments\ndemonstrate the efficacy of pix2pixGAN in 2D cephalogram synthesis, achieving\nan average peak signal-to-noise ratio (PSNR) value of 33.8 with reference to\nthe cephalograms synthesized from 3D CBCT volumes. Pix2pixGAN also achieves the\nbest performance in super resolution, achieving an average PSNR value of 32.5\nwithout the introduction of checkerboard or jagging artifacts. Our proposed\nautomatic landmark detection method achieves 86.7% successful detection rate in\nthe 2 mm clinical acceptable range on the ISBI Test1 data, which is comparable\nto the state-of-the-art methods. The method trained on conventional\ncephalograms can be directly applied to landmark detection in the synthetic\ncephalograms, achieving 93.0% and 80.7% successful detection rate in 4 mm\nprecision range for synthetic cephalograms from 3D volumes and 2D projections\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 17:06:54 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 18:08:38 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Huang", "Yixing", ""], ["Fan", "Fuxin", ""], ["Syben", "Christopher", ""], ["Roser", "Philipp", ""], ["Mill", "Leonid", ""], ["Maier", "Andreas", ""]]}, {"id": "2009.04433", "submitter": "Akash Srivastava", "authors": "Seungwook Han, Akash Srivastava, Cole Hurwitz, Prasanna Sattigeri and\n  David D. Cox", "title": "not-so-BigGAN: Generating High-Fidelity Images on Small Compute with\n  Wavelet-based Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art models for high-resolution image generation, such as BigGAN\nand VQVAE-2, require an incredible amount of compute resources and/or time (512\nTPU-v3 cores) to train, putting them out of reach for the larger research\ncommunity. On the other hand, GAN-based image super-resolution models, such as\nESRGAN, can not only upscale images to high dimensions, but also are efficient\nto train. In this paper, we present not-so-big-GAN (nsb-GAN), a simple yet\ncost-effective two-step training framework for deep generative models (DGMs) of\nhigh-dimensional natural images. First, we generate images in low-frequency\nbands by training a sampler in the wavelet domain. Then, we super-resolve these\nimages from the wavelet domain back to the pixel-space with our novel wavelet\nsuper-resolution decoder network. Wavelet-based down-sampling method preserves\nmore structural information than pixel-based methods, leading to significantly\nbetter generative quality of the low-resolution sampler (e.g., 64x64). Since\nthe sampler and decoder can be trained in parallel and operate on much lower\ndimensional spaces than end-to-end models, the training cost is substantially\nreduced. On ImageNet 512x512, our model achieves a Fr\\'echet Inception Distance\n(FID) of 10.59 -- beating the baseline BigGAN model -- at half the compute (256\nTPU-v3 cores).\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 17:29:40 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 18:41:09 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Han", "Seungwook", ""], ["Srivastava", "Akash", ""], ["Hurwitz", "Cole", ""], ["Sattigeri", "Prasanna", ""], ["Cox", "David D.", ""]]}, {"id": "2009.04448", "submitter": "Jieneng Chen", "authors": "Xiangde Luo, Jieneng Chen, Tao Song, Guotai Wang", "title": "Semi-supervised Medical Image Segmentation through Dual-task Consistency", "comments": "AAAI2021; Code is available at https://github.com/HiLab-git/DTC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based semi-supervised learning (SSL) algorithms have led to\npromising results in medical images segmentation and can alleviate doctors'\nexpensive annotations by leveraging unlabeled data. However, most of the\nexisting SSL algorithms in literature tend to regularize the model training by\nperturbing networks and/or data. Observing that multi/dual-task learning\nattends to various levels of information which have inherent prediction\nperturbation, we ask the question in this work: can we explicitly build\ntask-level regularization rather than implicitly constructing networks- and/or\ndata-level perturbation-and-transformation for SSL? To answer this question, we\npropose a novel dual-task-consistency semi-supervised framework for the first\ntime. Concretely, we use a dual-task deep network that jointly predicts a\npixel-wise segmentation map and a geometry-aware level set representation of\nthe target. The level set representation is converted to an approximated\nsegmentation map through a differentiable task transform layer. Simultaneously,\nwe introduce a dual-task consistency regularization between the level\nset-derived segmentation maps and directly predicted segmentation maps for both\nlabeled and unlabeled data. Extensive experiments on two public datasets show\nthat our method can largely improve the performance by incorporating the\nunlabeled data. Meanwhile, our framework outperforms the state-of-the-art\nsemi-supervised medical image segmentation methods. Code is available at:\nhttps://github.com/Luoxd1996/DTC\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 17:49:21 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 08:49:39 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Luo", "Xiangde", ""], ["Chen", "Jieneng", ""], ["Song", "Tao", ""], ["Wang", "Guotai", ""]]}, {"id": "2009.04450", "submitter": "Micol Marchetti-Bowick", "authors": "Lingyao Zhang, Po-Hsun Su, Jerrick Hoang, Galen Clark Haynes, Micol\n  Marchetti-Bowick", "title": "Map-Adaptive Goal-Based Trajectory Prediction", "comments": "Published at CoRL 2020", "journal-ref": "Conference on Robot Learning (CoRL) 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for multi-modal, long-term vehicle trajectory\nprediction. Our approach relies on using lane centerlines captured in rich maps\nof the environment to generate a set of proposed goal paths for each vehicle.\nUsing these paths -- which are generated at run time and therefore dynamically\nadapt to the scene -- as spatial anchors, we predict a set of goal-based\ntrajectories along with a categorical distribution over the goals. This\napproach allows us to directly model the goal-directed behavior of traffic\nactors, which unlocks the potential for more accurate long-term prediction. Our\nexperimental results on both a large-scale internal driving dataset and on the\npublic nuScenes dataset show that our model outperforms state-of-the-art\napproaches for vehicle trajectory prediction over a 6-second horizon. We also\nempirically demonstrate that our model is better able to generalize to road\nscenes from a completely new city than existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 17:57:01 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 23:20:43 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Zhang", "Lingyao", ""], ["Su", "Po-Hsun", ""], ["Hoang", "Jerrick", ""], ["Haynes", "Galen Clark", ""], ["Marchetti-Bowick", "Micol", ""]]}, {"id": "2009.04507", "submitter": "Axel Aguerreberry", "authors": "Axel Aguerreberry, Ezequiel de la Rosa, Alain Lalande and Elmer\n  Fernandez", "title": "Segmentation-free Estimation of Aortic Diameters from MRI Using Deep\n  Learning", "comments": "To be presented at the STACOM workshop at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and reproducible measurements of the aortic diameters are crucial\nfor the diagnosis of cardiovascular diseases and for therapeutic decision\nmaking. Currently, these measurements are manually performed by healthcare\nprofessionals, being time consuming, highly variable, and suffering from lack\nof reproducibility. In this work we propose a supervised deep learning method\nfor the direct estimation of aortic diameters. The approach is devised and\ntested over 100 magnetic resonance angiography scans without contrast agent.\nAll data was expert-annotated at six aortic locations typically used in\nclinical practice. Our approach makes use of a 3D+2D convolutional neural\nnetwork (CNN) that takes as input a 3D scan and outputs the aortic diameter at\na given location. In a 5-fold cross-validation comparison against a fully 3D\nCNN and against a 3D multiresolution CNN, our approach was consistently\nsuperior in predicting the aortic diameters. Overall, the 3D+2D CNN achieved a\nmean absolute error between 2.2-2.4 mm depending on the considered aortic\nlocation. These errors are less than 1 mm higher than the inter-observer\nvariability. Thus, suggesting that our method makes predictions almost reaching\nthe expert's performance. We conclude that the work allows to further explore\nautomatic algorithms for direct estimation of anatomical structures without the\nnecessity of a segmentation step. It also opens possibilities for the\nautomation of cardiovascular measurements in clinical settings.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 18:28:00 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Aguerreberry", "Axel", ""], ["de la Rosa", "Ezequiel", ""], ["Lalande", "Alain", ""], ["Fernandez", "Elmer", ""]]}, {"id": "2009.04532", "submitter": "Mohammad Abuzar Shaikh", "authors": "Mohammad Abuzar Shaikh, Tiehang Duan, Mihir Chauhan, Sargur Srihari", "title": "Attention based Writer Independent Handwriting Verification", "comments": "7 pages, 6 figures, Published in 2020 17th International Conference\n  on Frontiers in Handwriting Recognition (ICFHR)", "journal-ref": null, "doi": "10.1109/ICFHR2020.2020.00074", "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of writer verification is to provide a likelihood score for whether\nthe queried and known handwritten image samples belong to the same writer or\nnot. Such a task calls for the neural network to make it's outcome\ninterpretable, i.e. provide a view into the network's decision making process.\nWe implement and integrate cross-attention and soft-attention mechanisms to\ncapture the highly correlated and salient points in feature space of 2D inputs.\nThe attention maps serve as an explanation premise for the network's output\nlikelihood score. The attention mechanism also allows the network to focus more\non relevant areas of the input, thus improving the classification performance.\nOur proposed approach achieves a precision of 86\\% for detecting intra-writer\ncases in CEDAR cursive \"AND\" dataset. Furthermore, we generate meaningful\nexplanations for the provided decision by extracting attention maps from\nmultiple levels of the network.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 16:28:16 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 13:56:44 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 00:59:36 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Shaikh", "Mohammad Abuzar", ""], ["Duan", "Tiehang", ""], ["Chauhan", "Mihir", ""], ["Srihari", "Sargur", ""]]}, {"id": "2009.04554", "submitter": "Can Chen", "authors": "Can Chen, Luca Zanotti Fragonara, and Antonios Tsourdos", "title": "RoIFusion: 3D Object Detection from LiDAR and Vision", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When localizing and detecting 3D objects for autonomous driving scenes,\nobtaining information from multiple sensor (e.g. camera, LIDAR) typically\nincreases the robustness of 3D detectors. However, the efficient and effective\nfusion of different features captured from LIDAR and camera is still\nchallenging, especially due to the sparsity and irregularity of point cloud\ndistributions. This notwithstanding, point clouds offer useful complementary\ninformation. In this paper, we would like to leverage the advantages of LIDAR\nand camera sensors by proposing a deep neural network architecture for the\nfusion and the efficient detection of 3D objects by identifying their\ncorresponding 3D bounding boxes with orientation. In order to achieve this\ntask, instead of densely combining the point-wise feature of the point cloud\nand the related pixel features, we propose a novel fusion algorithm by\nprojecting a set of 3D Region of Interests (RoIs) from the point clouds to the\n2D RoIs of the corresponding the images. Finally, we demonstrate that our deep\nfusion approach achieves state-of-the-art performance on the KITTI 3D object\ndetection challenging benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 20:23:27 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Chen", "Can", ""], ["Fragonara", "Luca Zanotti", ""], ["Tsourdos", "Antonios", ""]]}, {"id": "2009.04583", "submitter": "Abdelaziz Djelouah", "authors": "Leonhard Helminger, Michael Bernasconi, Abdelaziz Djelouah, Markus\n  Gross, Christopher Schroers", "title": "Blind Image Restoration with Flow Based Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image restoration has seen great progress in the last years thanks to the\nadvances in deep neural networks. Most of these existing techniques are trained\nusing full supervision with suitable image pairs to tackle a specific\ndegradation. However, in a blind setting with unknown degradations this is not\npossible and a good prior remains crucial. Recently, neural network based\napproaches have been proposed to model such priors by leveraging either\ndenoising autoencoders or the implicit regularization captured by the neural\nnetwork structure itself. In contrast to this, we propose using normalizing\nflows to model the distribution of the target content and to use this as a\nprior in a maximum a posteriori (MAP) formulation. By expressing the MAP\noptimization process in the latent space through the learned bijective mapping,\nwe are able to obtain solutions through gradient descent. To the best of our\nknowledge, this is the first work that explores normalizing flows as prior in\nimage enhancement problems. Furthermore, we present experimental results for a\nnumber of different degradations on data sets varying in complexity and show\ncompetitive results when comparing with the deep image prior approach.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 21:40:11 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Helminger", "Leonhard", ""], ["Bernasconi", "Michael", ""], ["Djelouah", "Abdelaziz", ""], ["Gross", "Markus", ""], ["Schroers", "Christopher", ""]]}, {"id": "2009.04592", "submitter": "Dan Casas", "authors": "Raquel Vidaurre, Igor Santesteban, Elena Garces, Dan Casas", "title": "Fully Convolutional Graph Neural Networks for Parametric Virtual Try-On", "comments": "Project website\n  http://mslab.es/projects/FullyConvolutionalGraphVirtualTryOn . Accepted to\n  ACM SIGGRAPH / Eurographics Symposium on Computer Animation, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning-based approach for virtual try-on applications based on\na fully convolutional graph neural network. In contrast to existing data-driven\nmodels, which are trained for a specific garment or mesh topology, our fully\nconvolutional model can cope with a large family of garments, represented as\nparametric predefined 2D panels with arbitrary mesh topology, including long\ndresses, shirts, and tight tops. Under the hood, our novel geometric deep\nlearning approach learns to drape 3D garments by decoupling the three different\nsources of deformations that condition the fit of clothing: garment type,\ntarget body shape, and material. Specifically, we first learn a regressor that\npredicts the 3D drape of the input parametric garment when worn by a mean body\nshape. Then, after a mesh topology optimization step where we generate a\nsufficient level of detail for the input garment type, we further deform the\nmesh to reproduce deformations caused by the target body shape. Finally, we\npredict fine-scale details such as wrinkles that depend mostly on the garment\nmaterial. We qualitatively and quantitatively demonstrate that our fully\nconvolutional approach outperforms existing methods in terms of generalization\ncapabilities and memory requirements, and therefore it opens the door to more\ngeneral learning-based models for virtual try-on applications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 22:38:03 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Vidaurre", "Raquel", ""], ["Santesteban", "Igor", ""], ["Garces", "Elena", ""], ["Casas", "Dan", ""]]}, {"id": "2009.04626", "submitter": "Dongchao Wen", "authors": "Junjie Liu, Dongchao Wen, Deyu Wang, Wei Tao, Tse-Wei Chen, Kinya Osa,\n  and Masami Kato", "title": "QuantNet: Learning to Quantize by Learning within Fully Differentiable\n  Framework", "comments": "Accepted for publication in ECCV Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the achievements of recent binarization methods on reducing the\nperformance degradation of Binary Neural Networks (BNNs), gradient mismatching\ncaused by the Straight-Through-Estimator (STE) still dominates quantized\nnetworks. This paper proposes a meta-based quantizer named QuantNet, which\nutilizes a differentiable sub-network to directly binarize the full-precision\nweights without resorting to STE and any learnable gradient estimators. Our\nmethod not only solves the problem of gradient mismatching, but also reduces\nthe impact of discretization errors, caused by the binarizing operation in the\ndeployment, on performance. Generally, the proposed algorithm is implemented\nwithin a fully differentiable framework, and is easily extended to the general\nnetwork quantization with any bits. The quantitative experiments on CIFAR-100\nand ImageNet demonstrate that QuantNet achieves the signifficant improvements\ncomparing with previous binarization methods, and even bridges gaps of\naccuracies between binarized models and full-precision models.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 01:41:05 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Liu", "Junjie", ""], ["Wen", "Dongchao", ""], ["Wang", "Deyu", ""], ["Tao", "Wei", ""], ["Chen", "Tse-Wei", ""], ["Osa", "Kinya", ""], ["Kato", "Masami", ""]]}, {"id": "2009.04629", "submitter": "WeiQin Chuah", "authors": "WeiQin Chuah, Ruwan Tennakoon, Reza Hoseinnezhad, Alireza\n  Bab-Hadiashar, David Suter", "title": "Adjusting Bias in Long Range Stereo Matching: A semantics guided\n  approach", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo vision generally involves the computation of pixel correspondences and\nestimation of disparities between rectified image pairs. In many applications,\nincluding simultaneous localization and mapping (SLAM) and 3D object detection,\nthe disparities are primarily needed to calculate depth values and the accuracy\nof depth estimation is often more compelling than disparity estimation. The\naccuracy of disparity estimation, however, does not directly translate to the\naccuracy of depth estimation, especially for faraway objects. In the context of\nlearning-based stereo systems, this is largely due to biases imposed by the\nchoices of the disparity-based loss function and the training data.\nConsequently, the learning algorithms often produce unreliable depth estimates\nof foreground objects, particularly at large distances~($>50$m). To resolve\nthis issue, we first analyze the effect of those biases and then propose a pair\nof novel depth-based loss functions for foreground and background, separately.\nThese loss functions are tunable and can balance the inherent bias of the\nstereo learning algorithms. The efficacy of our solution is demonstrated by an\nextensive set of experiments, which are benchmarked against state of the art.\nWe show on KITTI~2015 benchmark that our proposed solution yields substantial\nimprovements in disparity and depth estimation, particularly for objects\nlocated at distances beyond 50 meters, outperforming the previous state of the\nart by $10\\%$.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 01:47:53 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 01:30:54 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Chuah", "WeiQin", ""], ["Tennakoon", "Ruwan", ""], ["Hoseinnezhad", "Reza", ""], ["Bab-Hadiashar", "Alireza", ""], ["Suter", "David", ""]]}, {"id": "2009.04632", "submitter": "Dmitrij Sitenko", "authors": "D. Sitenko, B. Boll, C. Schn\\\"orr", "title": "Assignment Flow for Order-Constrained OCT Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the present time Optical Coherence Tomography (OCT) is among the most\ncommonly used non-invasive imaging methods for the acquisition of large\nvolumetric scans of human retinal tissues and vasculature. To resolve decisive\ninformation from extracted OCT volumes and to make it applicable for further\ndiagnostic analysis, the exact identification of retinal layer thicknesses\nserves as an essential task be done for each patient separately. However, the\nmanual examination of multiple OCT scans in a row is a demanding and time\nconsuming task, which results in a lengthy qualification process and is\nfrequently confounded in the presence of tissue-dependent speckle noise.\nTherefore, the elaboration of automated segmentation models has become an\nimportant task in the field of medical image processing. We propose a novel,\npurely data driven \\textit{geometric approach to order-constrained 3D OCT\nretinal cell layer segmentation} which takes as input data in any metric space\nand comes along with basic operations that can be effectively computed in\nparallel. As opposed to many established retina detection methods, our\npresented formulation avoids the use of any shape prior and accomplishes the\nnatural order of the retina in a purely geometric way. This makes the approach\nunbiased and hence suited for the detection of local anatomical changes of\nretinal tissue structure. To demonstrate robustness of the proposed approach,\nwe compare two different choices of features on a data set of manually\nannotated 3D OCT volumes of healthy human retina. The quality of computed\nsegmentations is compared to the state of the art in terms of mean absolute\nerror and the Dice similarity coefficient. The results indicate a great\npotential for applying our method to the classification of diseased retina and\nopens a new research direction regarding the joint segmentation of retinal cell\nlayers and blood vessel structures.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 01:57:53 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Sitenko", "D.", ""], ["Boll", "B.", ""], ["Schn\u00f6rr", "C.", ""]]}, {"id": "2009.04634", "submitter": "Satyam Mohla Mr.", "authors": "Satyam Mohla, Sidharth Mohla, Anupam Guha and Biplab Banerjee", "title": "Multimodal Noisy Segmentation based fragmented burn scars identification\n  in Amazon Rainforest", "comments": "5 pages, 5 figures. Accepted at IEEE International Conference on\n  Systems, Man and Cybernetics 2020. Earlier draft presented at Harvard CRCS AI\n  for Social Good Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Detection of burn marks due to wildfires in inaccessible rain forests is\nimportant for various disaster management and ecological studies. The\nfragmented nature of arable landscapes and diverse cropping patterns often\nthwart the precise mapping of burn scars. Recent advances in remote-sensing and\navailability of multimodal data offer a viable solution to this mapping\nproblem. However, the task to segment burn marks is difficult because of its\nindistinguishably with similar looking land patterns, severe fragmented nature\nof burn marks and partially labelled noisy datasets. In this work we present\nAmazonNET -- a convolutional based network that allows extracting of burn\npatters from multimodal remote sensing images. The network consists of UNet: a\nwell-known encoder decoder type of architecture with skip connections commonly\nused in biomedical segmentation. The proposed framework utilises stacked\nRGB-NIR channels to segment burn scars from the pastures by training on a new\nweakly labelled noisy dataset from Amazonia. Our model illustrates superior\nperformance by correctly identifying partially labelled burn scars and\nrejecting incorrectly labelled samples, demonstrating our approach as one of\nthe first to effectively utilise deep learning based segmentation models in\nmultimodal burn scar identification.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 02:04:50 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Mohla", "Satyam", ""], ["Mohla", "Sidharth", ""], ["Guha", "Anupam", ""], ["Banerjee", "Biplab", ""]]}, {"id": "2009.04642", "submitter": "Yihao Liu", "authors": "Yihao Liu and Liangbin Xie and Li Siyao and Wenxiu Sun and Yu Qiao and\n  Chao Dong", "title": "Enhanced Quadratic Video Interpolation", "comments": "Winning solution of AIM2020 VTSR Challenge (in conjunction with ECCV\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prosperity of digital video industry, video frame interpolation has\narisen continuous attention in computer vision community and become a new\nupsurge in industry. Many learning-based methods have been proposed and\nachieved progressive results. Among them, a recent algorithm named quadratic\nvideo interpolation (QVI) achieves appealing performance. It exploits\nhigher-order motion information (e.g. acceleration) and successfully models the\nestimation of interpolated flow. However, its produced intermediate frames\nstill contain some unsatisfactory ghosting, artifacts and inaccurate motion,\nespecially when large and complex motion occurs. In this work, we further\nimprove the performance of QVI from three facets and propose an enhanced\nquadratic video interpolation (EQVI) model. In particular, we adopt a rectified\nquadratic flow prediction (RQFP) formulation with least squares method to\nestimate the motion more accurately. Complementary with image pixel-level\nblending, we introduce a residual contextual synthesis network (RCSN) to employ\ncontextual information in high-dimensional feature space, which could help the\nmodel handle more complicated scenes and motion patterns. Moreover, to further\nboost the performance, we devise a novel multi-scale fusion network (MS-Fusion)\nwhich can be regarded as a learnable augmentation process. The proposed EQVI\nmodel won the first place in the AIM2020 Video Temporal Super-Resolution\nChallenge.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 02:31:50 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Liu", "Yihao", ""], ["Xie", "Liangbin", ""], ["Siyao", "Li", ""], ["Sun", "Wenxiu", ""], ["Qiao", "Yu", ""], ["Dong", "Chao", ""]]}, {"id": "2009.04645", "submitter": "Hoyeon Ahn", "authors": "Hoyeon Ahn", "title": "Non-contact Real time Eye Gaze Mapping System Based on Deep\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-Computer Interaction(HCI) is a field that studies interactions between\nhuman users and computer systems. With the development of HCI, individuals or\ngroups of people can use various digital technologies to achieve the optimal\nuser experience. Human visual attention and visual intelligence are related to\ncognitive science, psychology, and marketing informatics, and are used in\nvarious applications of HCI. Gaze recognition is closely related to the HCI\nfield because it is meaningful in that it can enhance understanding of basic\nhuman behavior. We can obtain reliable visual attention by the Gaze Matching\nmethod that finds the area the user is staring at. In the previous methods, the\nuser wears a glasses-type device which in the form of glasses equipped with a\ngaze tracking function and performs gaze tracking within a limited monitor\narea. Also, the gaze estimation within a limited range is performed while the\nuser's posture is fixed. We overcome the physical limitations of the previous\nmethod in this paper and propose a non-contact gaze mapping system applicable\nin real-world environments. In addition, we introduce the GIST Gaze Mapping\n(GGM) dataset, a Gaze mapping dataset created to learn and evaluate gaze\nmapping.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 02:37:37 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Ahn", "Hoyeon", ""]]}, {"id": "2009.04650", "submitter": "Zehui Chen", "authors": "Zehui Chen and Qiaofei Li and Feng Zhao", "title": "Towards Fine-grained Large Object Segmentation 1st Place Solution to 3D\n  AI Challenge 2020 -- Instance Segmentation Track", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report introduces our solutions of Team 'FineGrainedSeg' for\nInstance Segmentation track in 3D AI Challenge 2020. In order to handle\nextremely large objects in 3D-FUTURE, we adopt PointRend as our basic\nframework, which outputs more fine-grained masks compared to HTC and SOLOv2.\nOur final submission is an ensemble of 5 PointRend models, which achieves the\n1st place on both validation and test leaderboards. The code is available at\nhttps://github.com/zehuichen123/3DFuture_ins_seg.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 02:55:27 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Chen", "Zehui", ""], ["Li", "Qiaofei", ""], ["Zhao", "Feng", ""]]}, {"id": "2009.04659", "submitter": "Ryne Roady", "authors": "Ryne Roady, Tyler L. Hayes, Christopher Kanan", "title": "Improved Robustness to Open Set Inputs via Tempered Mixup", "comments": "Proceedings of the ECCV 2020 Workshop on Adversarial Robustness in\n  the Real World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised classification methods often assume that evaluation data is drawn\nfrom the same distribution as training data and that all classes are present\nfor training. However, real-world classifiers must handle inputs that are far\nfrom the training distribution including samples from unknown classes. Open set\nrobustness refers to the ability to properly label samples from previously\nunseen categories as novel and avoid high-confidence, incorrect predictions.\nExisting approaches have focused on either novel inference methods, unique\ntraining architectures, or supplementing the training data with additional\nbackground samples. Here, we propose a simple regularization technique easily\napplied to existing convolutional neural network architectures that improves\nopen set robustness without a background dataset. Our method achieves\nstate-of-the-art results on open set classification baselines and easily scales\nto large-scale open set classification problems.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 04:01:31 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Roady", "Ryne", ""], ["Hayes", "Tyler L.", ""], ["Kanan", "Christopher", ""]]}, {"id": "2009.04660", "submitter": "JIehong Lin", "authors": "Jiehong Lin, Xian Shi, Yuan Gao, Ke Chen, Kui Jia", "title": "CAD-PU: A Curvature-Adaptive Deep Learning Solution for Point Set\n  Upsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point set is arguably the most direct approximation of an object or scene\nsurface, yet its practical acquisition often suffers from the shortcoming of\nbeing noisy, sparse, and possibly incomplete, which restricts its use for a\nhigh-quality surface recovery. Point set upsampling aims to increase its\ndensity and regularity such that a better surface recovery could be achieved.\nThe problem is severely ill-posed and challenging, considering that the\nupsampling target itself is only an approximation of the underlying surface.\nMotivated to improve the surface approximation via point set upsampling, we\nidentify the factors that are critical to the objective, by pairing the surface\napproximation error bounds of the input and output point sets. It suggests that\ngiven a fixed budget of points in the upsampling result, more points should be\ndistributed onto the surface regions where local curvatures are relatively\nhigh. To implement the motivation, we propose a novel design of\nCurvature-ADaptive Point set Upsampling network (CAD-PU), the core of which is\na module of curvature-adaptive feature expansion. To train CAD-PU, we follow\nthe same motivation and propose geometrically intuitive surrogates that\napproximate discrete notions of surface curvature for the upsampled point set.\nWe further integrate the proposed surrogates into an adversarial learning based\ncurvature minimization objective, which gives a practically effective learning\nof CAD-PU. We conduct thorough experiments that show the efficacy of our\ncontributions and the advantages of our method over existing ones. Our\nimplementation codes are publicly available at\nhttps://github.com/JiehongLin/CAD-PU.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 04:03:19 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Lin", "Jiehong", ""], ["Shi", "Xian", ""], ["Gao", "Yuan", ""], ["Chen", "Ke", ""], ["Jia", "Kui", ""]]}, {"id": "2009.04693", "submitter": "Marc Louis Maurice Francois", "authors": "M.L.M. Fran\\c{c}ois (GeM)", "title": "Virtual Image Correlation uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Virtual Image Correlation method applies for the measurement of\nsilhouettes boundaries with sub-pixel precision. It consists in a correlation\nbetween the image of interest and a virtual image based on a parametrized\ncurve. Thanks to a new formulation, it is shown that the method is exact in 1D,\ninsensitive to local curvature and to contrast variation, and that the bias\ninduced by luminance variation can be easily corrected. Optimal value of the\nvirtual image width, the sole parameter of the method, and optimal numerical\nsettings are established. An estimator is proposed to assess the relevance of\nthe user-chosen curve to describe the contour with a sub-pixel precision.\nAnalytical formulas are given for the measurement uncertainty in both cases of\nnoiseless and noisy images and their prediction is successfully compared to\nnumerical tests.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 07:04:05 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Fran\u00e7ois", "M. L. M.", "", "GeM"]]}, {"id": "2009.04709", "submitter": "Ricardo Bigolin Lanfredi", "authors": "Ricardo Bigolin Lanfredi, Joyce D. Schroeder, Tolga Tasdizen", "title": "Quantifying the Preferential Direction of the Model Gradient in\n  Adversarial Training With Projected Gradient Descent", "comments": "v3: new counting of Tables and Figures in the SM; fixed typo in eq.\n  of Lemma 1; new evaluation of linearity of models; new evaluation of\n  correlation of alignment metrics and robustness for a fixed training method;\n  new citations to python libraries; changed range of some graphs to simplify\n  the axes' digits; new comment about the difference of input gradient in\n  proposed and baseline metric; editing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training, especially projected gradient descent (PGD), has been a\nsuccessful approach for improving robustness against adversarial attacks. After\nadversarial training, gradients of models with respect to their inputs have a\npreferential direction. However, the direction of alignment is not\nmathematically well established, making it difficult to evaluate\nquantitatively. We propose a novel definition of this direction as the\ndirection of the vector pointing toward the closest point of the support of the\nclosest inaccurate class in decision space. To evaluate the alignment with this\ndirection after adversarial training, we apply a metric that uses generative\nadversarial networks to produce the smallest residual needed to change the\nclass present in the image. We show that PGD-trained models have a higher\nalignment than the baseline according to our definition, that our metric\npresents higher alignment values than a competing metric formulation, and that\nenforcing this alignment increases the robustness of models.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 07:48:42 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 07:58:22 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 23:49:44 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Lanfredi", "Ricardo Bigolin", ""], ["Schroeder", "Joyce D.", ""], ["Tasdizen", "Tolga", ""]]}, {"id": "2009.04724", "submitter": "Siteng Huang", "authors": "Siteng Huang, Min Zhang, Yachen Kang, Donglin Wang", "title": "Attributes-Guided and Pure-Visual Attention Alignment for Few-Shot\n  Recognition", "comments": "An expanded version of the same-name paper accepted by AAAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of few-shot recognition is to recognize novel categories with a\nlimited number of labeled examples in each class. To encourage learning from a\nsupplementary view, recent approaches have introduced auxiliary semantic\nmodalities into effective metric-learning frameworks that aim to learn a\nfeature similarity between training samples (support set) and test samples\n(query set). However, these approaches only augment the representations of\nsamples with available semantics while ignoring the query set, which loses the\npotential for the improvement and may lead to a shift between the modalities\ncombination and the pure-visual representation. In this paper, we devise an\nattributes-guided attention module (AGAM) to utilize human-annotated attributes\nand learn more discriminative features. This plug-and-play module enables\nvisual contents and corresponding attributes to collectively focus on important\nchannels and regions for the support set. And the feature selection is also\nachieved for query set with only visual information while the attributes are\nnot available. Therefore, representations from both sets are improved in a\nfine-grained manner. Moreover, an attention alignment mechanism is proposed to\ndistill knowledge from the guidance of attributes to the pure-visual branch for\nsamples without attributes. Extensive experiments and analysis show that our\nproposed module can significantly improve simple metric-based approaches to\nachieve state-of-the-art performance on different datasets and settings.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 08:38:32 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 13:56:10 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 07:26:49 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Huang", "Siteng", ""], ["Zhang", "Min", ""], ["Kang", "Yachen", ""], ["Wang", "Donglin", ""]]}, {"id": "2009.04746", "submitter": "Soha Sadat Mahdi", "authors": "Soha Sadat Mahdi (1), Nele Nauwelaers (1), Philip Joris (1), Giorgos\n  Bouritsas (2), Shunwang Gong (2), Sergiy Bokhnyak (3), Susan Walsh (4), Mark\n  D. Shriver (5), Michael Bronstein (2,3,6), Peter Claes (1,7). ((1) KU Leuven,\n  ESAT/PSI - UZ Leuven, MIRC, (2) Imperial College London, Department of\n  Computing, (3) USI Lugano, Institute of Computational Science, (4) Indiana\n  University-Purdue University-Indianapolis, Department of Biology, (5) Penn\n  State University, Department of Anthropology, (6) Twitter, (7) KU Leuven,\n  Department of Human Genetics)", "title": "3D Facial Matching by Spiral Convolutional Metric Learning and a\n  Biometric Fusion-Net of Demographic Properties", "comments": "Accepted at International Conference on Pattern Recognition (ICPR\n  2020). Mahdi and Nauwelaers contributed equally and are ordered\n  alphabetically", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition is a widely accepted biometric verification tool, as the\nface contains a lot of information about the identity of a person. In this\nstudy, a 2-step neural-based pipeline is presented for matching 3D facial shape\nto multiple DNA-related properties (sex, age, BMI and genomic background). The\nfirst step consists of a triplet loss-based metric learner that compresses\nfacial shape into a lower dimensional embedding while preserving information\nabout the property of interest. Most studies in the field of metric learning\nhave only focused on 2D Euclidean data. In this work, geometric deep learning\nis employed to learn directly from 3D facial meshes. To this end, spiral\nconvolutions are used along with a novel mesh-sampling scheme that retains\nuniformly sampled 3D points at different levels of resolution. The second step\nis a multi-biometric fusion by a fully connected neural network. The network\ntakes an ensemble of embeddings and property labels as input and returns\ngenuine and imposter scores. Since embeddings are accepted as an input, there\nis no need to train classifiers for the different properties and available data\ncan be used more efficiently. Results obtained by a 10-fold cross-validation\nfor biometric verification show that combining multiple properties leads to\nstronger biometric systems. Furthermore, the proposed neural-based pipeline\noutperforms a linear baseline, which consists of principal component analysis,\nfollowed by classification with linear support vector machines and a Naive\nBayes-based score-fuser.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 09:31:47 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 14:44:31 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Mahdi", "Soha Sadat", ""], ["Nauwelaers", "Nele", ""], ["Joris", "Philip", ""], ["Bouritsas", "Giorgos", ""], ["Gong", "Shunwang", ""], ["Bokhnyak", "Sergiy", ""], ["Walsh", "Susan", ""], ["Shriver", "Mark D.", ""], ["Bronstein", "Michael", ""], ["Claes", "Peter", ""], [".", "", ""]]}, {"id": "2009.04759", "submitter": "Ningning Ma", "authors": "Ningning Ma, Xiangyu Zhang, Ming Liu and Jian Sun", "title": "Activate or Not: Learning Customized Activation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple, effective, and general activation function we term ACON\nwhich learns to activate the neurons or not. Interestingly, we find Swish, the\nrecent popular NAS-searched activation, can be interpreted as a smooth\napproximation to ReLU. Intuitively, in the same way, we approximate the more\ngeneral Maxout family to our novel ACON family, which remarkably improves the\nperformance and makes Swish a special case of ACON. Next, we present meta-ACON,\nwhich explicitly learns to optimize the parameter switching between non-linear\n(activate) and linear (inactivate) and provides a new design space. By simply\nchanging the activation function, we show its effectiveness on both small\nmodels and highly optimized large models (e.g. it improves the ImageNet top-1\naccuracy rate by 6.7% and 1.8% on MobileNet-0.25 and ResNet-152, respectively).\nMoreover, our novel ACON can be naturally transferred to object detection and\nsemantic segmentation, showing that ACON is an effective alternative in a\nvariety of tasks. Code is available at https://github.com/nmaac/acon.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 09:59:19 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 09:56:15 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Ma", "Ningning", ""], ["Zhang", "Xiangyu", ""], ["Liu", "Ming", ""], ["Sun", "Jian", ""]]}, {"id": "2009.04776", "submitter": "Akhmedkhan Shabanov", "authors": "Akhmedkhan Shabanov, Ilya Krotov, Nikolay Chinaev, Vsevolod Poletaev,\n  Sergei Kozlukov, Igor Pasechnik, Bulat Yakupov, Artsiom Sanakoyeu, Vadim\n  Lebedev, Dmitry Ulyanov", "title": "Self-supervised Depth Denoising Using Lower- and Higher-quality RGB-D\n  sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumer-level depth cameras and depth sensors embedded in mobile devices\nenable numerous applications, such as AR games and face identification.\nHowever, the quality of the captured depth is sometimes insufficient for 3D\nreconstruction, tracking and other computer vision tasks. In this paper, we\npropose a self-supervised depth denoising approach to denoise and refine depth\ncoming from a low quality sensor. We record simultaneous RGB-D sequences with\nunzynchronized lower- and higher-quality cameras and solve a challenging\nproblem of aligning sequences both temporally and spatially. We then learn a\ndeep neural network to denoise the lower-quality depth using the matched\nhigher-quality data as a source of supervision signal. We experimentally\nvalidate our method against state-of-the-art filtering-based and deep denoising\ntechniques and show its application for 3D object reconstruction tasks where\nour approach leads to more detailed fused surfaces and better tracking.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 11:18:11 GMT"}, {"version": "v2", "created": "Sun, 13 Sep 2020 18:45:15 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Shabanov", "Akhmedkhan", ""], ["Krotov", "Ilya", ""], ["Chinaev", "Nikolay", ""], ["Poletaev", "Vsevolod", ""], ["Kozlukov", "Sergei", ""], ["Pasechnik", "Igor", ""], ["Yakupov", "Bulat", ""], ["Sanakoyeu", "Artsiom", ""], ["Lebedev", "Vadim", ""], ["Ulyanov", "Dmitry", ""]]}, {"id": "2009.04787", "submitter": "Thijs Kuipers", "authors": "Thijs P. Kuipers, Devanshu Arya, Deepak K. Gupta", "title": "Hard Occlusions in Visual Object Tracking", "comments": "Accepted at ECCV 2020 Workshop RLQ-TOD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual object tracking is among the hardest problems in computer vision, as\ntrackers have to deal with many challenging circumstances such as illumination\nchanges, fast motion, occlusion, among others. A tracker is assessed to be good\nor not based on its performance on the recent tracking datasets, e.g., VOT2019,\nand LaSOT. We argue that while the recent datasets contain large sets of\nannotated videos that to some extent provide a large bandwidth for training\ndata, the hard scenarios such as occlusion and in-plane rotation are still\nunderrepresented. For trackers to be brought closer to the real-world scenarios\nand deployed in safety-critical devices, even the rarest hard scenarios must be\nproperly addressed. In this paper, we particularly focus on hard occlusion\ncases and benchmark the performance of recent state-of-the-art trackers (SOTA)\non them. We created a small-scale dataset containing different categories\nwithin hard occlusions, on which the selected trackers are evaluated. Results\nshow that hard occlusions remain a very challenging problem for SOTA trackers.\nFurthermore, it is observed that tracker performance varies wildly between\ndifferent categories of hard occlusions, where a top-performing tracker on one\ncategory performs significantly worse on a different category. The varying\nnature of tracker performance based on specific categories suggests that the\ncommon tracker rankings using averaged single performance scores are not\nadequate to gauge tracker performance in real-world scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 11:42:21 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Kuipers", "Thijs P.", ""], ["Arya", "Devanshu", ""], ["Gupta", "Deepak K.", ""]]}, {"id": "2009.04790", "submitter": "Neil Cronin", "authors": "Neil J. Cronin, Taija Finni, Olivier Seynnes", "title": "Fully automated analysis of muscle architecture from B-mode ultrasound\n  images with deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  B-mode ultrasound is commonly used to image musculoskeletal tissues, but one\nmajor bottleneck is data interpretation, and analyses of muscle thickness,\npennation angle and fascicle length are often still performed manually. In this\nstudy we trained deep neural networks (based on U-net) to detect muscle\nfascicles and aponeuroses using a set of labelled musculoskeletal ultrasound\nimages. We then compared neural network predictions on new, unseen images to\nthose obtained via manual analysis and two existing semi/automated analysis\napproaches (SMA and Ultratrack). With a GPU, inference time for a single image\nwith the new approach was around 0.7s, compared to 4.6s with a CPU. Our method\ndetects the locations of the superficial and deep aponeuroses, as well as\nmultiple fascicle fragments per image. For single images, the method gave\nsimilar results to those produced by a non-trainable automated method (SMA;\nmean difference in fascicle length: 1.1 mm) or human manual analysis (mean\ndifference: 2.1 mm). Between-method differences in pennation angle were within\n1$^\\circ$, and mean differences in muscle thickness were less than 0.2 mm.\nSimilarly, for videos, there was strong overlap between the results produced\nwith Ultratrack and our method, with a mean ICC of 0.73, despite the fact that\nthe analysed trials included hundreds of frames. Our method is fully automated\nand open source, and can estimate fascicle length, pennation angle and muscle\nthickness from single images or videos, as well as from multiple superficial\nmuscles. We also provide all necessary code and training data for custom model\ndevelopment.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 11:44:00 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Cronin", "Neil J.", ""], ["Finni", "Taija", ""], ["Seynnes", "Olivier", ""]]}, {"id": "2009.04794", "submitter": "Shoudong Han", "authors": "Shoudong Han, Piao Huang, Hongwei Wang, En Yu, Donghaisheng Liu,\n  Xiaofeng Pan, Jun Zhao", "title": "MAT: Motion-Aware Multi-Object Tracking", "comments": "13 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern multi-object tracking (MOT) systems usually model the trajectories by\nassociating per-frame detections. However, when camera motion, fast motion, and\nocclusion challenges occur, it is difficult to ensure long-range tracking or\neven the tracklet purity, especially for small objects. Although\nre-identification is often employed, due to noisy partial-detections, similar\nappearance, and lack of temporal-spatial constraints, it is not only unreliable\nand time-consuming, but still cannot address the false negatives for occluded\nand blurred objects. In this paper, we propose an enhanced MOT paradigm, namely\nMotion-Aware Tracker (MAT), focusing more on various motion patterns of\ndifferent objects. The rigid camera motion and nonrigid pedestrian motion are\nblended compatibly to form the integrated motion localization module.\nMeanwhile, we introduce the dynamic reconnection context module, which aims to\nbalance the robustness of long-range motion-based reconnection, and includes\nthe cyclic pseudo-observation updating strategy to smoothly fill in the\ntracking fragments caused by occlusion or blur. Additionally, the 3D integral\nimage module is presented to efficiently cut useless track-detection\nassociation connections with temporal-spatial constraints. Extensive\nexperiments on MOT16 and MOT17 challenging benchmarks demonstrate that our MAT\napproach can achieve the superior performance by a large margin with high\nefficiency, in contrast to other state-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 11:51:33 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 05:13:06 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Han", "Shoudong", ""], ["Huang", "Piao", ""], ["Wang", "Hongwei", ""], ["Yu", "En", ""], ["Liu", "Donghaisheng", ""], ["Pan", "Xiaofeng", ""], ["Zhao", "Jun", ""]]}, {"id": "2009.04801", "submitter": "Marco Karrer", "authors": "Marco Karrer and Margarita Chli", "title": "Distributed Variable-Baseline Stereo SLAM from two UAVs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VIO has been widely used and researched to control and aid the automation of\nnavigation of robots especially in the absence of absolute position\nmeasurements, such as GPS. However, when observable landmarks in the scene lie\nfar away from the robot's sensor suite, as it is the case at high altitude\nflights, the fidelity of estimates and the observability of the metric scale\ndegrades greatly for these methods. Aiming to tackle this issue, in this\narticle, we employ two UAVs equipped with one monocular camera and one IMU\neach, to exploit their view overlap and relative distance measurements between\nthem using UWB modules onboard to enable collaborative VIO. In particular, we\npropose a novel, distributed fusion scheme enabling the formation of a virtual\nstereo camera rig with adjustable baseline from the two UAVs. In order to\ncontrol the \\gls{uav} agents autonomously, we propose a decentralized\ncollaborative estimation scheme, where each agent hold its own local map,\nachieving an average pose estimation latency of 11ms, while ensuring\nconsistency of the agents' estimates via consensus based optimization.\nFollowing a thorough evaluation on photorealistic simulations, we demonstrate\nthe effectiveness of the approach at high altitude flights of up to 160m, going\nsignificantly beyond the capabilities of state-of-the-art VIO methods. Finally,\nwe show the advantage of actively adjusting the baseline on-the-fly over a\nfixed, target baseline, reducing the error in our experiments by a factor of\ntwo.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 12:16:10 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Karrer", "Marco", ""], ["Chli", "Margarita", ""]]}, {"id": "2009.04806", "submitter": "Alexander Wang", "authors": "Alexander Wang, Mengye Ren, Richard S. Zemel", "title": "SketchEmbedNet: Learning Novel Concepts by Imitating Drawings", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch drawings capture the salient information of visual concepts. Previous\nwork has shown that neural networks are capable of producing sketches of\nnatural objects drawn from a small number of classes. While earlier approaches\nfocus on generation quality or retrieval, we explore properties of image\nrepresentations learned by training a model to produce sketches of images. We\nshow that this generative, class-agnostic model produces informative embeddings\nof images from novel examples, classes, and even novel datasets in a few-shot\nsetting. Additionally, we find that these learned representations exhibit\ninteresting structure and compositionality.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 16:43:28 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 17:15:39 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 18:51:51 GMT"}, {"version": "v4", "created": "Tue, 22 Jun 2021 19:45:09 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Wang", "Alexander", ""], ["Ren", "Mengye", ""], ["Zemel", "Richard S.", ""]]}, {"id": "2009.04809", "submitter": "Rao Muhammad Umer", "authors": "Rao Muhammad Umer, Gian Luca Foresti, Christian Micheloni", "title": "Deep Iterative Residual Convolutional Network for Single Image\n  Super-Resolution", "comments": "To be appeared in proceedings of the 25th IEEE International\n  Conference on Pattern Recognition (ICPR). arXiv admin note: text overlap with\n  arXiv:2005.00953, arXiv:2009.03693", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have recently achieved great\nsuccess for single image super-resolution (SISR) task due to their powerful\nfeature representation capabilities. The most recent deep learning based SISR\nmethods focus on designing deeper / wider models to learn the non-linear\nmapping between low-resolution (LR) inputs and high-resolution (HR) outputs.\nThese existing SR methods do not take into account the image observation\n(physical) model and thus require a large number of network's trainable\nparameters with a great volume of training data. To address these issues, we\npropose a deep Iterative Super-Resolution Residual Convolutional Network\n(ISRResCNet) that exploits the powerful image regularization and large-scale\noptimization techniques by training the deep network in an iterative manner\nwith a residual learning approach. Extensive experimental results on various\nsuper-resolution benchmarks demonstrate that our method with a few trainable\nparameters improves the results for different scaling factors in comparison\nwith the state-of-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 12:54:14 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Umer", "Rao Muhammad", ""], ["Foresti", "Gian Luca", ""], ["Micheloni", "Christian", ""]]}, {"id": "2009.04836", "submitter": "Austen Groener", "authors": "Gary Chern, Austen Groener, Michael Harner, Tyler Kuhns, Andy Lam,\n  Stephen O'Neill, and Mark Pritt", "title": "Globally-scalable Automated Target Recognition (GATR)", "comments": "7 pages, 18 figures, 2019 IEEE Applied Imagery Pattern Recognition\n  Workshop (AIPR)", "journal-ref": "2019 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),\n  Washington, DC, USA, 2019, pp. 1-7", "doi": "10.1109/AIPR47015.2019.9174585", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GATR (Globally-scalable Automated Target Recognition) is a Lockheed Martin\nsoftware system for real-time object detection and classification in satellite\nimagery on a worldwide basis. GATR uses GPU-accelerated deep learning software\nto quickly search large geographic regions. On a single GPU it processes\nimagery at a rate of over 16 square km/sec (or more than 10 Mpixels/sec), and\nit requires only two hours to search the entire state of Pennsylvania for gas\nfracking wells. The search time scales linearly with the geographic area, and\nthe processing rate scales linearly with the number of GPUs. GATR has a\nmodular, cloud-based architecture that uses the Maxar GBDX platform and\nprovides an ATR analytic as a service. Applications include broad area search,\nwatch boxes for monitoring ports and airfields, and site characterization. ATR\nis performed by deep learning models including RetinaNet and Faster R-CNN.\nResults are presented for the detection of aircraft and fracking wells and show\nthat the recalls exceed 90% even in geographic regions never seen before. GATR\nis extensible to new targets, such as cars and ships, and it also handles radar\nand infrared imagery.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 13:20:50 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Chern", "Gary", ""], ["Groener", "Austen", ""], ["Harner", "Michael", ""], ["Kuhns", "Tyler", ""], ["Lam", "Andy", ""], ["O'Neill", "Stephen", ""], ["Pritt", "Mark", ""]]}, {"id": "2009.04857", "submitter": "Austen Groener", "authors": "Austen Groener, Gary Chern, Mark Pritt", "title": "A Comparison of Deep Learning Object Detection Models for Satellite\n  Imagery", "comments": "10 pages, 9 figures, 3 tables. 2019 IEEE Applied Imagery Pattern\n  Recognition Workshop (AIPR)", "journal-ref": "2019 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),\n  Washington, DC, USA, 2019, pp. 1-10", "doi": "10.1109/AIPR47015.2019.9174593", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we compare the detection accuracy and speed of several\nstate-of-the-art models for the task of detecting oil and gas fracking wells\nand small cars in commercial electro-optical satellite imagery. Several models\nare studied from the single-stage, two-stage, and multi-stage object detection\nfamilies of techniques. For the detection of fracking well pads (50m - 250m),\nwe find single-stage detectors provide superior prediction speed while also\nmatching detection performance of their two and multi-stage counterparts.\nHowever, for detecting small cars, two-stage and multi-stage models provide\nsubstantially higher accuracies at the cost of some speed. We also measure\ntiming results of the sliding window object detection algorithm to provide a\nbaseline for comparison. Some of these models have been incorporated into the\nLockheed Martin Globally-Scalable Automated Target Recognition (GATR)\nframework.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 13:43:14 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Groener", "Austen", ""], ["Chern", "Gary", ""], ["Pritt", "Mark", ""]]}, {"id": "2009.04866", "submitter": "Austen Groener", "authors": "Michael Harner, Austen Groener, and Mark Pritt", "title": "Detecting the Presence of Vehicles and Equipment in SAR Imagery Using\n  Image Texture Features", "comments": "6 pages, 6 figures, 2019 IEEE Applied Imagery Pattern Recognition\n  Workshop (AIPR)", "journal-ref": "2019 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),\n  Washington, DC, USA, 2019, pp. 1-6", "doi": "10.1109/AIPR47015.2019.9174598", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a methodology for monitoring man-made,\nconstruction-like activities in low-resolution SAR imagery. Our source of data\nis the European Space Agency Sentinel-l satellite which provides global\ncoverage at a 12-day revisit rate. Despite limitations in resolution, our\nmethodology enables us to monitor activity levels (i.e. presence of vehicles,\nequipment) of a pre-defined location by analyzing the texture of detected SAR\nimagery. Using an exploratory dataset, we trained a support vector machine\n(SVM), a random binary forest, and a fully-connected neural network for\nclassification. We use Haralick texture features in the VV and VH polarization\nchannels as the input features to our classifiers. Each classifier showed\npromising results in being able to distinguish between two possible types of\nconstruction-site activity levels. This paper documents a case study that is\ncentered around monitoring the construction process for oil and gas fracking\nwells.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 13:59:52 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Harner", "Michael", ""], ["Groener", "Austen", ""], ["Pritt", "Mark", ""]]}, {"id": "2009.04877", "submitter": "Hung Tuan Nguyen Dr.", "authors": "Hung Tuan Nguyen, Cuong Tuan Nguyen, Takeya Ino, Bipin Indurkhya,\n  Masaki Nakagawa", "title": "Text-independent writer identification using convolutional neural\n  network", "comments": "12 pages", "journal-ref": "Pattern Recognition Letters, Volume 121, 2019, Pages 104-112", "doi": "10.1016/j.patrec.2018.07.022", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The text-independent approach to writer identification does not require the\nwriter to write some predetermined text. Previous research on text-independent\nwriter identification has been based on identifying writer-specific features\ndesigned by experts. However, in the last decade, deep learning methods have\nbeen successfully applied to learn features from data automatically. We propose\nhere an end-to-end deep-learning method for text-independent writer\nidentification that does not require prior identification of features. A\nConvolutional Neural Network (CNN) is trained initially to extract local\nfeatures, which represent characteristics of individual handwriting in the\nwhole character images and their sub-regions. Randomly sampled tuples of images\nfrom the training set are used to train the CNN and aggregate the extracted\nlocal features of images from the tuples to form global features. For every\ntraining epoch, the process of randomly sampling tuples is repeated, which is\nequivalent to a large number of training patterns being prepared for training\nthe CNN for text-independent writer identification. We conducted experiments on\nthe JEITA-HP database of offline handwritten Japanese character patterns. With\n200 characters, our method achieved an accuracy of 99.97% to classify 100\nwriters. Even when using 50 characters for 100 writers or 100 characters for\n400 writers, our method achieved accuracy levels of 92.80% or 93.82%,\nrespectively. We conducted further experiments on the Firemaker and IAM\ndatabases of offline handwritten English text. Using only one page per writer\nto train, our method achieved over 91.81% accuracy to classify 900 writers.\nOverall, we achieved a better performance than the previously published best\nresult based on handcrafted features and clustering algorithms, which\ndemonstrates the effectiveness of our method for handwritten English text also.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 14:18:03 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Nguyen", "Hung Tuan", ""], ["Nguyen", "Cuong Tuan", ""], ["Ino", "Takeya", ""], ["Indurkhya", "Bipin", ""], ["Nakagawa", "Masaki", ""]]}, {"id": "2009.04878", "submitter": "Alessandro Piva", "authors": "Massimo Iuliani, Marco Fontani, Alessandro Piva", "title": "A leak in PRNU based source identification. Questioning fingerprint\n  uniqueness", "comments": "Final paper in : https://ieeexplore.ieee.org/document/9393356", "journal-ref": "IEEE Access, 2021", "doi": "10.1109/ACCESS.2021.3070478", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Photo Response Non-Uniformity (PRNU) is considered the most effective trace\nfor the image source attribution task. Its uniqueness ensures that the sensor\npattern noises extracted from different cameras are strongly uncorrelated, even\nwhen they belong to the same camera model. However, with the advent of\ncomputational photography, most recent devices heavily process the acquired\npixels, possibly introducing non-unique artifacts that may reduce PRNU noise's\ndistinctiveness, especially when several exemplars of the same device model are\ninvolved in the analysis. Considering that PRNU is an image forensic technology\nthat finds actual and wide use by law enforcement agencies worldwide, it is\nessential to keep validating such technology on recent devices as they appear.\nIn this paper, we perform an extensive testing campaign on over 33.000 Flickr\nimages belonging to 45 smartphone and 25 DSLR camera models released recently\nto determine how widespread the issue is and which is the plausible cause.\nExperiments highlight that most brands, like Samsung, Huawei, Canon, Nikon,\nFujifilm, Sigma, and Leica, are strongly affected by this issue. We show that\nthe primary cause of high false alarm rates cannot be directly related to\nspecific camera models, firmware, nor image contents. It is evident that the\neffectiveness of \\prnu based source identification on the most recent devices\nmust be reconsidered in light of these results. Therefore, this paper is\nintended as a call to action for the scientific community rather than a\ncomplete treatment of the subject. Moreover, we believe publishing these data\nis important to raise awareness about a possible issue with PRNU reliability in\nthe law enforcement world.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 14:18:38 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 12:51:11 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Iuliani", "Massimo", ""], ["Fontani", "Marco", ""], ["Piva", "Alessandro", ""]]}, {"id": "2009.04893", "submitter": "Lisa Schneider", "authors": "Lisa Schneider, Annika Niemann, Oliver Beuing, Bernhard Preim and\n  Sylvia Saalfeld", "title": "MedMeshCNN -- Enabling MeshCNN for Medical Surface Models", "comments": "7 pages, 7 figures, 1 table, Submitted to Computer Methods and\n  Programs in Biomedicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and objective: MeshCNN is a recently proposed Deep Learning\nframework that drew attention due to its direct operation on irregular,\nnon-uniform 3D meshes. On selected benchmarking datasets, it outperformed\nstate-of-the-art methods within classification and segmentation tasks.\nEspecially, the medical domain provides a large amount of complex 3D surface\nmodels that may benefit from processing with MeshCNN. However, several\nlimitations prevent outstanding performances of MeshCNN on highly diverse\nmedical surface models. Within this work, we propose MedMeshCNN as an expansion\nfor complex, diverse, and fine-grained medical data. Methods: MedMeshCNN\nfollows the functionality of MeshCNN with a significantly increased memory\nefficiency that allows retaining patient-specific properties during the\nsegmentation process. Furthermore, it enables the segmentation of pathological\nstructures that often come with highly imbalanced class distributions. Results:\nWe tested the performance of MedMeshCNN on a complex part segmentation task of\nintracranial aneurysms and their surrounding vessel structures and reached a\nmean Intersection over Union of 63.24\\%. The pathological aneurysm is segmented\nwith an Intersection over Union of 71.4\\%. Conclusions: These results\ndemonstrate that MedMeshCNN enables the application of MeshCNN on complex,\nfine-grained medical surface meshes. The imbalanced class distribution deriving\nfrom the pathological finding is considered by MedMeshCNN and patient-specific\nproperties are mostly retained during the segmentation process.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 14:40:28 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Schneider", "Lisa", ""], ["Niemann", "Annika", ""], ["Beuing", "Oliver", ""], ["Preim", "Bernhard", ""], ["Saalfeld", "Sylvia", ""]]}, {"id": "2009.04924", "submitter": "Wenxuan Wang", "authors": "Jiali Liu, Wenxuan Wang, Tianyao Guan, Ningbo Zhao, Xiaoguang Han, and\n  Zhen Li", "title": "Ultrasound Liver Fibrosis Diagnosis using Multi-indicator guided Deep\n  Neural Networks", "comments": "Jiali Liu and Wenxuan Wang are equal contribution", "journal-ref": "Machine Learning in Medical Imaging 2019", "doi": "10.1007/978-3-030-32692-0_27", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate analysis of the fibrosis stage plays very important roles in\nfollow-up of patients with chronic hepatitis B infection. In this paper, a deep\nlearning framework is presented for automatically liver fibrosis prediction. On\ncontrary of previous works, our approach can take use of the information\nprovided by multiple ultrasound images. An indicator-guided learning mechanism\nis further proposed to ease the training of the proposed model. This follows\nthe workflow of clinical diagnosis and make the prediction procedure\ninterpretable. To support the training, a dataset is well-collected which\ncontains the ultrasound videos/images, indicators and labels of 229 patients.\nAs demonstrated in the experimental results, our proposed model shows its\neffectiveness by achieving the state-of-the-art performance, specifically, the\naccuracy is 65.6%(20% higher than previous best).\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 15:05:45 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Liu", "Jiali", ""], ["Wang", "Wenxuan", ""], ["Guan", "Tianyao", ""], ["Zhao", "Ningbo", ""], ["Han", "Xiaoguang", ""], ["Li", "Zhen", ""]]}, {"id": "2009.04930", "submitter": "Ronald Clark", "authors": "Martin Fisch, Ronald Clark", "title": "Orientation Keypoints for 6D Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most realtime human pose estimation approaches are based on detecting joint\npositions. Using the detected joint positions, the yaw and pitch of the limbs\ncan be computed. However, the roll along the limb, which is critical for\napplication such as sports analysis and computer animation, cannot be computed\nas this axis of rotation remains unobserved. In this paper we therefore\nintroduce orientation keypoints, a novel approach for estimating the full\nposition and rotation of skeletal joints, using only single-frame RGB images.\nInspired by how motion-capture systems use a set of point markers to estimate\nfull bone rotations, our method uses virtual markers to generate sufficient\ninformation to accurately infer rotations with simple post processing. The\nrotation predictions improve upon the best reported mean error for joint angles\nby 48\\% and achieves 93\\% accuracy across 15 bone rotations. The method also\nimproves the current state-of-the-art results for joint positions by 14\\% as\nmeasured by MPJPE on the principle dataset, and generalizes well to in-the-wild\ndatasets. Video available at: https://youtu.be/1EBUrfu_CaE\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 15:15:12 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Fisch", "Martin", ""], ["Clark", "Ronald", ""]]}, {"id": "2009.04932", "submitter": "Alberto Sabater", "authors": "Alberto Sabater, Luis Montesano, Ana C. Murillo", "title": "Performance of object recognition in wearable videos", "comments": "Emerging Technologies and Factory Automation, ETFA, 2019", "journal-ref": null, "doi": "10.1109/ETFA.2019.8869019", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable technologies are enabling plenty of new applications of computer\nvision, from life logging to health assistance. Many of them are required to\nrecognize the elements of interest in the scene captured by the camera. This\nwork studies the problem of object detection and localization on videos\ncaptured by this type of camera. Wearable videos are a much more challenging\nscenario for object detection than standard images or even another type of\nvideos, due to lower quality images (e.g. poor focus) or high clutter and\nocclusion common in wearable recordings. Existing work typically focuses on\ndetecting the objects of focus or those being manipulated by the user wearing\nthe camera. We perform a more general evaluation of the task of object\ndetection in this type of video, because numerous applications, such as\nmarketing studies, also need detecting objects which are not in focus by the\nuser. This work presents a thorough study of the well known YOLO architecture,\nthat offers an excellent trade-off between accuracy and speed, for the\nparticular case of object detection in wearable video. We focus our study on\nthe public ADL Dataset, but we also use additional public data for\ncomplementary evaluations. We run an exhaustive set of experiments with\ndifferent variations of the original architecture and its training strategy.\nOur experiments drive to several conclusions about the most promising\ndirections for our goal and point us to further research steps to improve\ndetection in wearable videos.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 15:20:17 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Sabater", "Alberto", ""], ["Montesano", "Luis", ""], ["Murillo", "Ana C.", ""]]}, {"id": "2009.04960", "submitter": "Baoquan Zhang", "authors": "Baoquan Zhang, Xutao Li, Yunming Ye, Zhichao Huang, and Lisai Zhang", "title": "Prototype Completion with Primitive Knowledge for Few-Shot Learning", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning is a challenging task, which aims to learn a classifier for\nnovel classes with few examples. Pre-training based meta-learning methods\neffectively tackle the problem by pre-training a feature extractor and then\nfine-tuning it through the nearest centroid based meta-learning. However,\nresults show that the fine-tuning step makes very marginal improvements. In\nthis paper, 1) we figure out the key reason, i.e., in the pre-trained feature\nspace, the base classes already form compact clusters while novel classes\nspread as groups with large variances, which implies that fine-tuning the\nfeature extractor is less meaningful; 2) instead of fine-tuning the feature\nextractor, we focus on estimating more representative prototypes during\nmeta-learning. Consequently, we propose a novel prototype completion based\nmeta-learning framework. This framework first introduces primitive knowledge\n(i.e., class-level part or attribute annotations) and extracts representative\nattribute features as priors. Then, we design a prototype completion network to\nlearn to complete prototypes with these priors. To avoid the prototype\ncompletion error caused by primitive knowledge noises or class differences, we\nfurther develop a Gaussian based prototype fusion strategy that combines the\nmean-based and completed prototypes by exploiting the unlabeled samples.\nExtensive experiments show that our method: (i) can obtain more accurate\nprototypes; (ii) outperforms state-of-the-art techniques by 2% - 9% in terms of\nclassification accuracy. Our code is available online.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 16:09:34 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 01:05:24 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 14:49:39 GMT"}, {"version": "v4", "created": "Sun, 9 May 2021 01:32:12 GMT"}, {"version": "v5", "created": "Fri, 4 Jun 2021 02:08:29 GMT"}, {"version": "v6", "created": "Thu, 24 Jun 2021 03:41:34 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Zhang", "Baoquan", ""], ["Li", "Xutao", ""], ["Ye", "Yunming", ""], ["Huang", "Zhichao", ""], ["Zhang", "Lisai", ""]]}, {"id": "2009.04965", "submitter": "Meng-Jiun Chiou", "authors": "Meng-Jiun Chiou, Roger Zimmermann, Jiashi Feng", "title": "Visual Relationship Detection with Visual-Linguistic Knowledge from\n  Multimodal Representations", "comments": "Published in IEEE Access", "journal-ref": "IEEE Access, 2021", "doi": "10.1109/ACCESS.2021.3069041", "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual relationship detection aims to reason over relationships among salient\nobjects in images, which has drawn increasing attention over the past few\nyears. Inspired by human reasoning mechanisms, it is believed that external\nvisual commonsense knowledge is beneficial for reasoning visual relationships\nof objects in images, which is however rarely considered in existing methods.\nIn this paper, we propose a novel approach named Relational Visual-Linguistic\nBidirectional Encoder Representations from Transformers (RVL-BERT), which\nperforms relational reasoning with both visual and language commonsense\nknowledge learned via self-supervised pre-training with multimodal\nrepresentations. RVL-BERT also uses an effective spatial module and a novel\nmask attention module to explicitly capture spatial information among the\nobjects. Moreover, our model decouples object detection from visual\nrelationship recognition by taking in object names directly, enabling it to be\nused on top of any object detection system. We show through quantitative and\nqualitative experiments that, with the transferred knowledge and novel modules,\nRVL-BERT achieves competitive results on two challenging visual relationship\ndetection datasets. The source code is available at\nhttps://github.com/coldmanck/RVL-BERT.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 16:15:09 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 14:01:49 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 07:48:10 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Chiou", "Meng-Jiun", ""], ["Zimmermann", "Roger", ""], ["Feng", "Jiashi", ""]]}, {"id": "2009.04985", "submitter": "Julian Alberto Palladino", "authors": "Julian Alberto Palladino, Diego Fernandez Slezak and Enzo Ferrante", "title": "Unsupervised Domain Adaptation via CycleGAN for White Matter\n  Hyperintensity Segmentation in Multicenter MR Images", "comments": "Accepted for publication in the International Seminar on Medical\n  Information Processing and Analysis (SIPAIM 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of white matter hyperintensities in magnetic resonance\nimages is of paramount clinical and research importance. Quantification of\nthese lesions serve as a predictor for risk of stroke, dementia and mortality.\nDuring the last years, convolutional neural networks (CNN) specifically\ntailored for biomedical image segmentation have outperformed all previous\ntechniques in this task. However, they are extremely data-dependent, and\nmaintain a good performance only when data distribution between training and\ntest datasets remains unchanged. When such distribution changes but we still\naim at performing the same task, we incur in a domain adaptation problem (e.g.\nusing a different MR machine or different acquisition parameters for training\nand test data). In this work, we explore the use of cycle-consistent\nadversarial networks (CycleGAN) to perform unsupervised domain adaptation on\nmulticenter MR images with brain lesions. We aim at learning a mapping function\nto transform volumetric MR images between domains, which are characterized by\ndifferent medical centers and MR machines with varying brand, model and\nconfiguration parameters. Our experiments show that CycleGAN allows us to\nreduce the Jensen-Shannon divergence between MR domains, enabling automatic\nsegmentation with CNN models on domains where no labeled data was available.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 16:48:19 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Palladino", "Julian Alberto", ""], ["Slezak", "Diego Fernandez", ""], ["Ferrante", "Enzo", ""]]}, {"id": "2009.04989", "submitter": "Qi Qian", "authors": "Lei Chen, Qi Qian, Hao Li", "title": "Semi-Anchored Detector for One-Stage Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard one-stage detector is comprised of two tasks: classification and\nregression. Anchors of different shapes are introduced for each location in the\nfeature map to mitigate the challenge of regression for multi-scale objects.\nHowever, the performance of classification can degrade due to the highly\nclass-imbalanced problem in anchors. Recently, many anchor-free algorithms have\nbeen proposed to classify locations directly. The anchor-free strategy benefits\nthe classification task but can lead to sup-optimum for the regression task due\nto the lack of prior bounding boxes. In this work, we propose a semi-anchored\nframework. Concretely, we identify positive locations in classification, and\nassociate multiple anchors to the positive locations in regression. With\nResNet-101 as the backbone, the proposed semi-anchored detector achieves 43.6%\nmAP on COCO data set, which demonstrates the state-of-art performance among\none-stage detectors.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 16:57:09 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Chen", "Lei", ""], ["Qian", "Qi", ""], ["Li", "Hao", ""]]}, {"id": "2009.04998", "submitter": "Alberto Bailoni", "authors": "Alberto Bailoni, Constantin Pape, Steffen Wolf, Anna Kreshuk, Fred A.\n  Hamprecht", "title": "Proposal-Free Volumetric Instance Segmentation from Latent\n  Single-Instance Masks", "comments": "Presented at GCPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a new proposal-free instance segmentation method that\nbuilds on single-instance segmentation masks predicted across the entire image\nin a sliding window style. In contrast to related approaches, our method\nconcurrently predicts all masks, one for each pixel, and thus resolves any\nconflict jointly across the entire image. Specifically, predictions from\noverlapping masks are combined into edge weights of a signed graph that is\nsubsequently partitioned to obtain all final instances concurrently. The result\nis a parameter-free method that is strongly robust to noise and prioritizes\npredictions with the highest consensus across overlapping masks. All masks are\ndecoded from a low dimensional latent representation, which results in great\nmemory savings strictly required for applications to large volumetric images.\nWe test our method on the challenging CREMI 2016 neuron segmentation benchmark\nwhere it achieves competitive scores.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 17:09:23 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Bailoni", "Alberto", ""], ["Pape", "Constantin", ""], ["Wolf", "Steffen", ""], ["Kreshuk", "Anna", ""], ["Hamprecht", "Fred A.", ""]]}, {"id": "2009.05004", "submitter": "Cara Monical", "authors": "Antonio Gonzales, Cara Monical, Tony Perkins", "title": "HSolo: Homography from a single affine aware correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of existing robust homography estimation algorithms is highly\ndependent on the inlier rate of feature point correspondences. In this paper,\nwe present a novel procedure for homography estimation that is particularly\nwell suited for inlier-poor domains. By utilizing the scale and rotation\nbyproducts created by affine aware feature detectors such as SIFT and SURF, we\nobtain an initial homography estimate from a single correspondence pair. This\nestimate allows us to filter the correspondences to an inlier-rich subset for\nuse with a robust estimator. Especially at low inlier rates, our novel\nalgorithm provides dramatic performance improvements.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 17:13:23 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Gonzales", "Antonio", ""], ["Monical", "Cara", ""], ["Perkins", "Tony", ""]]}, {"id": "2009.05014", "submitter": "Ekdeep Singh Lubana", "authors": "Ekdeep Singh Lubana, Puja Trivedi, Conrad Hougen, Robert P. Dick,\n  Alfred O. Hero", "title": "OrthoReg: Robust Network Pruning Using Orthonormality Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network pruning in Convolutional Neural Networks (CNNs) has been extensively\ninvestigated in recent years. To determine the impact of pruning a group of\nfilters on a network's accuracy, state-of-the-art pruning methods consistently\nassume filters of a CNN are independent. This allows the importance of a group\nof filters to be estimated as the sum of importances of individual filters.\nHowever, overparameterization in modern networks results in highly correlated\nfilters that invalidate this assumption, thereby resulting in incorrect\nimportance estimates. To address this issue, we propose OrthoReg, a principled\nregularization strategy that enforces orthonormality on a network's filters to\nreduce inter-filter correlation, thereby allowing reliable, efficient\ndetermination of group importance estimates, improved trainability of pruned\nnetworks, and efficient, simultaneous pruning of large groups of filters. When\nused for iterative pruning on VGG-13, MobileNet-V1, and ResNet-34, OrthoReg\nconsistently outperforms five baseline techniques, including the\nstate-of-the-art, on CIFAR-100 and Tiny-ImageNet. For the recently proposed\nEarly-Bird Ticket hypothesis, which claims networks become amenable to pruning\nearly-on in training and can be pruned after a few epochs to minimize training\nexpenditure, we find OrthoReg significantly outperforms prior work. Code\navailable at https://github.com/EkdeepSLubana/OrthoReg.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 17:21:21 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Lubana", "Ekdeep Singh", ""], ["Trivedi", "Puja", ""], ["Hougen", "Conrad", ""], ["Dick", "Robert P.", ""], ["Hero", "Alfred O.", ""]]}, {"id": "2009.05023", "submitter": "Md Motiur Rahman Sagar", "authors": "Md Motiur Rahman Sagar, Martin Dyrba", "title": "Learning Shape Features and Abstractions in 3D Convolutional Neural\n  Networks for Detecting Alzheimer's Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks - especially Convolutional Neural Network (ConvNet) has\nbecome the state-of-the-art for image classification, pattern recognition and\nvarious computer vision tasks. ConvNet has a huge potential in medical domain\nfor analyzing medical data to diagnose diseases in an efficient way. Based on\nextracted features by ConvNet model from MRI data, early diagnosis is very\ncrucial for preventing progress and treating the Alzheimer's disease. Despite\nhaving the ability to deliver great performance, absence of interpretability of\nthe model's decision can lead to misdiagnosis which can be life threatening. In\nthis thesis, learned shape features and abstractions by 3D ConvNets for\ndetecting Alzheimer's disease were investigated using various visualization\ntechniques. How changes in network structures, used filters sizes and filters\nshapes affects the overall performance and learned features of the model were\nalso inspected. LRP relevance map of different models revealed which parts of\nthe brain were more relevant for the classification decision. Comparing the\nlearned filters by Activation Maximization showed how patterns were encoded in\ndifferent layers of the network. Finally, transfer learning from a\nconvolutional autoencoder was implemented to check whether increasing the\nnumber of training samples with patches of input to extract the low-level\nfeatures improves learned features and the model performance.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 17:41:03 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Sagar", "Md Motiur Rahman", ""], ["Dyrba", "Martin", ""]]}, {"id": "2009.05041", "submitter": "David Bau iii", "authors": "David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou,\n  Antonio Torralba", "title": "Understanding the Role of Individual Units in a Deep Neural Network", "comments": "Proceedings of the National Academy of Sciences 2020. Code at\n  https://github.com/davidbau/dissect/ and website at\n  https://dissect.csail.mit.edu/", "journal-ref": null, "doi": "10.1073/pnas.1907375117", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks excel at finding hierarchical representations that solve\ncomplex tasks over large data sets. How can we humans understand these learned\nrepresentations? In this work, we present network dissection, an analytic\nframework to systematically identify the semantics of individual hidden units\nwithin image classification and image generation networks. First, we analyze a\nconvolutional neural network (CNN) trained on scene classification and discover\nunits that match a diverse set of object concepts. We find evidence that the\nnetwork has learned many object classes that play crucial roles in classifying\nscene classes. Second, we use a similar analytic method to analyze a generative\nadversarial network (GAN) model trained to generate scenes. By analyzing\nchanges made when small sets of units are activated or deactivated, we find\nthat objects can be added and removed from the output scenes while adapting to\nthe context. Finally, we apply our analytic framework to understanding\nadversarial attacks and to semantic image editing.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 17:59:10 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 18:58:32 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Bau", "David", ""], ["Zhu", "Jun-Yan", ""], ["Strobelt", "Hendrik", ""], ["Lapedriza", "Agata", ""], ["Zhou", "Bolei", ""], ["Torralba", "Antonio", ""]]}, {"id": "2009.05096", "submitter": "Shervin Minaee", "authors": "Shakib Yazdani, Shervin Minaee, Rahele Kafieh, Narges Saeedizadeh,\n  Milan Sonka", "title": "COVID CT-Net: Predicting Covid-19 From Chest CT Images Using Attentional\n  Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The novel corona-virus disease (COVID-19) pandemic has caused a major\noutbreak in more than 200 countries around the world, leading to a severe\nimpact on the health and life of many people globally. As of Aug 25th of 2020,\nmore than 20 million people are infected, and more than 800,000 death are\nreported. Computed Tomography (CT) images can be used as a as an alternative to\nthe time-consuming \"reverse transcription polymerase chain reaction (RT-PCR)\"\ntest, to detect COVID-19. In this work we developed a deep learning framework\nto predict COVID-19 from CT images. We propose to use an attentional\nconvolution network, which can focus on the infected areas of chest, enabling\nit to perform a more accurate prediction. We trained our model on a dataset of\nmore than 2000 CT images, and report its performance in terms of various\npopular metrics, such as sensitivity, specificity, area under the curve, and\nalso precision-recall curve, and achieve very promising results. We also\nprovide a visualization of the attention maps of the model for several test\nimages, and show that our model is attending to the infected regions as\nintended. In addition to developing a machine learning modeling framework, we\nalso provide the manual annotation of the potentionally infected regions of\nchest, with the help of a board-certified radiologist, and make that publicly\navailable for other researchers.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 19:00:51 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Yazdani", "Shakib", ""], ["Minaee", "Shervin", ""], ["Kafieh", "Rahele", ""], ["Saeedizadeh", "Narges", ""], ["Sonka", "Milan", ""]]}, {"id": "2009.05101", "submitter": "Zilong Ji", "authors": "Zilong Ji, Xiaolong Zou, Tiejun Huang, Si Wu", "title": "Vision at A Glance: Interplay between Fine and Coarse Information\n  Processing Pathways", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition is often viewed as a feedforward, bottom-up process in\nmachine learning, but in real neural systems, object recognition is a\ncomplicated process which involves the interplay between two signal pathways.\nOne is the parvocellular pathway (P-pathway), which is slow and extracts fine\nfeatures of objects; the other is the magnocellular pathway (M-pathway), which\nis fast and extracts coarse features of objects. It has been suggested that the\ninterplay between the two pathways endows the neural system with the capacity\nof processing visual information rapidly, adaptively, and robustly. However,\nthe underlying computational mechanisms remain largely unknown. In this study,\nwe build a computational model to elucidate the computational advantages\nassociated with the interactions between two pathways. Our model consists of\ntwo convolution neural networks: one mimics the P-pathway, referred to as\nFineNet, which is deep, has small-size kernels, and receives detailed visual\ninputs; the other mimics the M-pathway, referred to as CoarseNet, which is\nshallow, has large-size kernels, and receives low-pass filtered or binarized\nvisual inputs. The two pathways interact with each other via a Restricted\nBoltzmann Machine. We find that: 1) FineNet can teach CoarseNet through\nimitation and improve its performance considerably; 2) CoarseNet can improve\nthe noise robustness of FineNet through association; 3) the output of CoarseNet\ncan serve as a cognitive bias to improve the performance of FineNet. We hope\nthat this study will provide insight into understanding visual information\nprocessing and inspire the development of new object recognition architectures.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 06:46:26 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Ji", "Zilong", ""], ["Zou", "Xiaolong", ""], ["Huang", "Tiejun", ""], ["Wu", "Si", ""]]}, {"id": "2009.05102", "submitter": "Chenglizhao Chen", "authors": "Xuehao Wang, Shuai Li, Chenglizhao Chen, Yuming Fang, Aimin Hao, Hong\n  Qin", "title": "Data-Level Recombination and Lightweight Fusion Scheme for RGB-D Salient\n  Object Detection", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.3037470", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing RGB-D salient object detection methods treat depth information as an\nindependent component to complement its RGB part, and widely follow the\nbi-stream parallel network architecture. To selectively fuse the CNNs features\nextracted from both RGB and depth as a final result, the state-of-the-art\n(SOTA) bi-stream networks usually consist of two independent subbranches; i.e.,\none subbranch is used for RGB saliency and the other aims for depth saliency.\nHowever, its depth saliency is persistently inferior to the RGB saliency\nbecause the RGB component is intrinsically more informative than the depth\ncomponent. The bi-stream architecture easily biases its subsequent fusion\nprocedure to the RGB subbranch, leading to a performance bottleneck. In this\npaper, we propose a novel data-level recombination strategy to fuse RGB with D\n(depth) before deep feature extraction, where we cyclically convert the\noriginal 4-dimensional RGB-D into \\textbf{D}GB, R\\textbf{D}B and RG\\textbf{D}.\nThen, a newly lightweight designed triple-stream network is applied over these\nnovel formulated data to achieve an optimal channel-wise complementary fusion\nstatus between the RGB and D, achieving a new SOTA performance.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 10:13:05 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Wang", "Xuehao", ""], ["Li", "Shuai", ""], ["Chen", "Chenglizhao", ""], ["Fang", "Yuming", ""], ["Hao", "Aimin", ""], ["Qin", "Hong", ""]]}, {"id": "2009.05103", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Yaxian Li, Xingxu Yao, Weizhi Nie, Pengfei Xu, Jufeng\n  Yang, Kurt Keutzer", "title": "Emotion-Based End-to-End Matching Between Image and Music in\n  Valence-Arousal Space", "comments": "Accepted by ACM Multimedia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both images and music can convey rich semantics and are widely used to induce\nspecific emotions. Matching images and music with similar emotions might help\nto make emotion perceptions more vivid and stronger. Existing emotion-based\nimage and music matching methods either employ limited categorical emotion\nstates which cannot well reflect the complexity and subtlety of emotions, or\ntrain the matching model using an impractical multi-stage pipeline. In this\npaper, we study end-to-end matching between image and music based on emotions\nin the continuous valence-arousal (VA) space. First, we construct a large-scale\ndataset, termed Image-Music-Emotion-Matching-Net (IMEMNet), with over 140K\nimage-music pairs. Second, we propose cross-modal deep continuous metric\nlearning (CDCML) to learn a shared latent embedding space which preserves the\ncross-modal similarity relationship in the continuous matching space. Finally,\nwe refine the embedding space by further preserving the single-modal emotion\nrelationship in the VA spaces of both images and music. The metric learning in\nthe embedding space and task regression in the label space are jointly\noptimized for both cross-modal matching and single-modal VA prediction. The\nextensive experiments conducted on IMEMNet demonstrate the superiority of CDCML\nfor emotion-based image and music matching as compared to the state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 20:12:23 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Zhao", "Sicheng", ""], ["Li", "Yaxian", ""], ["Yao", "Xingxu", ""], ["Nie", "Weizhi", ""], ["Xu", "Pengfei", ""], ["Yang", "Jufeng", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2009.05105", "submitter": "Ali Ayub", "authors": "Ali Ayub, Alan R. Wagner", "title": "What am I allowed to do here?: Online Learning of Context-Specific Norms\n  by Pepper", "comments": "The final authenticated publication is available online at\n  https://doi.org/10.1007/978-3-030-62056-1_19", "journal-ref": "International Conference on Social Robotics (ICSR), 2020", "doi": "10.1007/978-3-030-62056-1_19", "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social norms support coordination and cooperation in society. With social\nrobots becoming increasingly involved in our society, they also need to follow\nthe social norms of the society. This paper presents a computational framework\nfor learning contexts and the social norms present in a context in an online\nmanner on a robot. The paper utilizes a recent state-of-the-art approach for\nincremental learning and adapts it for online learning of scenes (contexts).\nThe paper further utilizes Dempster-Schafer theory to model context-specific\nnorms. After learning the scenes (contexts), we use active learning to learn\nrelated norms. We test our approach on the Pepper robot by taking it through\ndifferent scene locations. Our results show that Pepper can learn different\nscenes and related norms simply by communicating with a human partner in an\nonline manner.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 07:27:02 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 06:57:12 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Ayub", "Ali", ""], ["Wagner", "Alan R.", ""]]}, {"id": "2009.05107", "submitter": "Yuexin Xiang", "authors": "Yuexin Xiang, Wei Ren, Tiantian Li, Xianghan Zheng, Tianqing Zhu and\n  Kim-Kwang Raymond Choo", "title": "Efficiently Constructing Adversarial Examples by Feature Watermarking", "comments": "15 pages, 17figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing attentions of deep learning models, attacks are also\nupcoming for such models. For example, an attacker may carefully construct\nimages in specific ways (also referred to as adversarial examples) aiming to\nmislead the deep learning models to output incorrect classification results.\nSimilarly, many efforts are proposed to detect and mitigate adversarial\nexamples, usually for certain dedicated attacks. In this paper, we propose a\nnovel digital watermark based method to generate adversarial examples for deep\nlearning models. Specifically, partial main features of the watermark image are\nembedded into the host image invisibly, aiming to tamper and damage the\nrecognition capabilities of the deep learning models. We devise an efficient\nmechanism to select host images and watermark images, and utilize the improved\ndiscrete wavelet transform (DWT) based Patchwork watermarking algorithm and the\nmodified discrete cosine transform (DCT) based Patchwork watermarking\nalgorithm. The experimental results showed that our scheme is able to generate\na large number of adversarial examples efficiently. In addition, we find that\nusing the extracted features of the image as the watermark images, can increase\nthe success rate of an attack under certain conditions with minimal changes to\nthe host image. To ensure repeatability, reproducibility, and code sharing, the\nsource code is available on GitHub\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 09:03:26 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Xiang", "Yuexin", ""], ["Ren", "Wei", ""], ["Li", "Tiantian", ""], ["Zheng", "Xianghan", ""], ["Zhu", "Tianqing", ""], ["Choo", "Kim-Kwang Raymond", ""]]}, {"id": "2009.05108", "submitter": "Youshan Zhang", "authors": "Youshan Zhang", "title": "Bayesian Geodesic Regression on Riemannian Manifolds", "comments": "BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geodesic regression has been proposed for fitting the geodesic curve.\nHowever, it cannot automatically choose the dimensionality of data. In this\npaper, we develop a Bayesian geodesic regression model on Riemannian manifolds\n(BGRM) model. To avoid the overfitting problem, we add a regularization term to\ncontrol the effectiveness of the model. To automatically select the\ndimensionality, we develop a prior for the geodesic regression model, which can\nautomatically select the number of relevant dimensions by driving unnecessary\ntangent vectors to zero. To show the validation of our model, we first apply it\nin the 3D synthetic sphere and 2D pentagon data. We then demonstrate the\neffectiveness of our model in reducing the dimensionality and analyzing shape\nvariations of human corpus callosum and mandible data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 00:42:58 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 13:34:35 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Zhang", "Youshan", ""]]}, {"id": "2009.05109", "submitter": "Yi Yuan", "authors": "Wenheng Chen, He Wang, Yi Yuan, Tianjia Shao, Kun Zhou", "title": "Dynamic Future Net: Diversified Human Motion Generation", "comments": "Accepted by ACMMM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion modelling is crucial in many areas such as computer graphics,\nvision and virtual reality. Acquiring high-quality skeletal motions is\ndifficult due to the need for specialized equipment and laborious manual\npost-posting, which necessitates maximizing the use of existing data to\nsynthesize new data. However, it is a challenge due to the intrinsic motion\nstochasticity of human motion dynamics, manifested in the short and long terms.\nIn the short term, there is strong randomness within a couple frames, e.g. one\nframe followed by multiple possible frames leading to different motion styles;\nwhile in the long term, there are non-deterministic action transitions. In this\npaper, we present Dynamic Future Net, a new deep learning model where we\nexplicitly focuses on the aforementioned motion stochasticity by constructing a\ngenerative model with non-trivial modelling capacity in temporal stochasticity.\nGiven limited amounts of data, our model can generate a large number of\nhigh-quality motions with arbitrary duration, and visually-convincing\nvariations in both space and time. We evaluate our model on a wide range of\nmotions and compare it with the state-of-the-art methods. Both qualitative and\nquantitative results show the superiority of our method, for its robustness,\nversatility and high-quality.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 02:31:41 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Chen", "Wenheng", ""], ["Wang", "He", ""], ["Yuan", "Yi", ""], ["Shao", "Tianjia", ""], ["Zhou", "Kun", ""]]}, {"id": "2009.05132", "submitter": "SeungKee Jeon", "authors": "SeungKee Jeon", "title": "1st Place Solution to Google Landmark Retrieval 2020", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the 1st place solution to the Google Landmark Retrieval\n2020 Competition on Kaggle. The solution is based on metric learning to\nclassify numerous landmark classes, and uses transfer learning with two train\ndatasets, fine-tuning on bigger images, adjusting loss weight for cleaner\nsamples, and esemble to enhance the model's performance further. Finally, it\nscored 0.38677 mAP@100 on the private leaderboard.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 05:45:20 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Jeon", "SeungKee", ""]]}, {"id": "2009.05139", "submitter": "Ali Beikmohammadi", "authors": "Ali Beikmohammadi, Karim Faez, Ali Motallebi", "title": "SWP-Leaf NET: a novel multistage approach for plant leaf identification\n  based on deep learning", "comments": "38 pages, 16 figures, 5 tables, submitted to Expert Systems with\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern scientific and technological advances are allowing botanists to use\ncomputer vision-based approaches for plant identification tasks. These\napproaches have their own challenges. Leaf classification is a computer-vision\ntask performed for the automated identification of plant species, a serious\nchallenge due to variations in leaf morphology, including its size, texture,\nshape, and venation. Researchers have recently become more inclined toward deep\nlearning-based methods rather than conventional feature-based methods due to\nthe popularity and successful implementation of deep learning methods in image\nanalysis, object recognition, and speech recognition. In this paper, a\nbotanist's behavior was modeled in leaf identification by proposing a\nhighly-efficient method of maximum behavioral resemblance developed through\nthree deep learning-based models. Different layers of the three models were\nvisualized to ensure that the botanist's behavior was modeled accurately. The\nfirst and second models were designed from scratch.Regarding the third model,\nthe pre-trained architecture MobileNetV2 was employed along with the\ntransfer-learning technique. The proposed method was evaluated on two\nwell-known datasets: Flavia and MalayaKew. According to a comparative analysis,\nthe suggested approach was more accurate than hand-crafted feature extraction\nmethods and other deep learning techniques in terms of 99.67% and 99.81%\naccuracy. Unlike conventional techniques that have their own specific\ncomplexities and depend on datasets, the proposed method required no\nhand-crafted feature extraction, and also increased accuracy and\ndistributability as compared with other deep learning techniques. It was\nfurther considerably faster than other methods because it used shallower\nnetworks with fewer parameters and did not use all three models recurrently.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 20:28:57 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Beikmohammadi", "Ali", ""], ["Faez", "Karim", ""], ["Motallebi", "Ali", ""]]}, {"id": "2009.05144", "submitter": "Gagik Gavalian", "authors": "Gagik Gavalian", "title": "Auto-encoders for Track Reconstruction in Drift Chambers for CLAS12", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we describe the development of machine learning models to\nassist the CLAS12 tracking algorithm by identifying tracks through inferring\nmissing segments in the drift chambers. Auto encoders are used to reconstruct\nmissing segments from track trajectory. Implemented neural network was able to\nreliably reconstruct missing segment positions with accuracy of $\\approx 0.35$\nwires, and lead to recovery of missing tracks with accuracy of $>99.8\\%$.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 20:46:14 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Gavalian", "Gagik", ""]]}, {"id": "2009.05147", "submitter": "Andre Nguyen", "authors": "Andre T. Nguyen, Luke E. Richards, Gaoussou Youssouf Kebe, Edward\n  Raff, Kasra Darvish, Frank Ferraro, Cynthia Matuszek", "title": "Practical Cross-modal Manifold Alignment for Grounded Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a cross-modality manifold alignment procedure that leverages\ntriplet loss to jointly learn consistent, multi-modal embeddings of\nlanguage-based concepts of real-world items. Our approach learns these\nembeddings by sampling triples of anchor, positive, and negative data points\nfrom RGB-depth images and their natural language descriptions. We show that our\napproach can benefit from, but does not require, post-processing steps such as\nProcrustes analysis, in contrast to some of our baselines which require it for\nreasonable performance. We demonstrate the effectiveness of our approach on two\ndatasets commonly used to develop robotic-based grounded language learning\nsystems, where our approach outperforms four baselines, including a\nstate-of-the-art approach, across five evaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 04:16:48 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Nguyen", "Andre T.", ""], ["Richards", "Luke E.", ""], ["Kebe", "Gaoussou Youssouf", ""], ["Raff", "Edward", ""], ["Darvish", "Kasra", ""], ["Ferraro", "Frank", ""], ["Matuszek", "Cynthia", ""]]}, {"id": "2009.05148", "submitter": "Sabarish Vadarevu", "authors": "Sabarish Vadarevu and Vijay Karamcheti", "title": "A new heuristic algorithm for fast k-segmentation", "comments": "10 pages, 10 figures, 5 tables, and 1 pseudo-code. Submitted to IEEE\n  BigData 2020. Supplementary material (200 segmented videos) at\n  https://figshare.com/articles/media/7-segmentation of 200 scenes from\n  BDD100k/12859493/1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-segmentation of a video stream is used to partition it into $k$\npiecewise-linear segments, so that each linear segment has a meaningful\ninterpretation. Such segmentation may be used to summarize large videos using a\nsmall set of images, to identify anomalies within segments and change points\nbetween segments, and to select critical subsets for training machine learning\nmodels. Exact and approximate segmentation methods for $k$-segmentation exist\nin the literature. Each of these algorithms occupies a different spot in the\ntrade-off between computational complexity and accuracy. A novel heuristic\nalgorithm is proposed in this paper to improve upon existing methods. It is\nempirically found to provide accuracies competitive with exact methods at a\nfraction of the computational expense.\n  The new algorithm is inspired by Lloyd's algorithm for K-Means and Lloyd-Max\nalgorithm for scalar quantization, and is called the LM algorithm for\nconvenience. It works by iteratively minimizing a cost function from any given\ninitialisation; the commonly used $L_2$ cost is chosen in this paper. While the\ngreedy minimization makes the algorithm sensitive to initialisation, the\nability to converge from any initial guess to a local optimum allows the\nalgorithm to be integrated into other existing algorithms. Three variants of\nthe algorithm are tested over a large number of synthetic datasets, one being a\nstandalone LM implementation, and two others that combine with existing\nalgorithms. One of the latter two -- LM-enhanced-Bottom-Up segmentation -- is\nfound to have the best accuracy and the lowest computational complexity among\nall algorithms. This variant of LM can provide $k$-segmentations over data sets\nwith up to a million image frames within several seconds.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 04:50:17 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Vadarevu", "Sabarish", ""], ["Karamcheti", "Vijay", ""]]}, {"id": "2009.05158", "submitter": "Hailey James", "authors": "Hailey James, Otkrist Gupta, Dan Raviv", "title": "OCR Graph Features for Manipulation Detection in Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting manipulations in digital documents is becoming increasingly\nimportant for information verification purposes. Due to the proliferation of\nimage editing software, altering key information in documents has become widely\naccessible. Nearly all approaches in this domain rely on a procedural approach,\nusing carefully generated features and a hand-tuned scoring system, rather than\na data-driven and generalizable approach. We frame this issue as a graph\ncomparison problem using the character bounding boxes, and propose a model that\nleverages graph features using OCR (Optical Character Recognition). Our model\nrelies on a data-driven approach to detect alterations by training a random\nforest classifier on the graph-based OCR features. We evaluate our algorithm's\nforgery detection performance on dataset constructed from real business\ndocuments with slight forgery imperfections. Our proposed model dramatically\noutperforms the most closely-related document manipulation detection model on\nthis task.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 21:50:45 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 15:52:09 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["James", "Hailey", ""], ["Gupta", "Otkrist", ""], ["Raviv", "Dan", ""]]}, {"id": "2009.05175", "submitter": "Khyathi Raghavi Chandu", "authors": "Khyathi Raghavi Chandu, Piyush Sharma, Soravit Changpinyo, Ashish\n  Thapliyal, Radu Soricut", "title": "Denoising Large-Scale Image Captioning from Alt-text Data using Content\n  Selection Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training large-scale image captioning (IC) models demands access to a rich\nand diverse set of training examples, gathered from the wild, often from noisy\nalt-text data. However, recent modeling approaches to IC often fall short in\nterms of performance in this case, because they assume a clean annotated\ndataset (as opposed to the noisier alt-text--based annotations), and employ an\nend-to-end generation approach, which often lacks both controllability and\ninterpretability. We address these problems by breaking down the task into two\nsimpler, more controllable tasks -- skeleton prediction and skeleton-based\ncaption generation. Specifically, we show that selecting content words as\nskeletons} helps in generating improved and denoised captions when leveraging\nrich yet noisy alt-text--based uncurated datasets. We also show that the\npredicted English skeletons can be further cross-lingually leveraged to\ngenerate non-English captions, and present experimental results covering\ncaption generation in French, Italian, German, Spanish and Hindi. We also show\nthat skeleton-based prediction allows for better control of certain caption\nproperties, such as length, content, and gender expression, providing a handle\nto perform human-in-the-loop semi-automatic corrections.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 23:31:38 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 23:11:48 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chandu", "Khyathi Raghavi", ""], ["Sharma", "Piyush", ""], ["Changpinyo", "Soravit", ""], ["Thapliyal", "Ashish", ""], ["Soricut", "Radu", ""]]}, {"id": "2009.05205", "submitter": "Jingchao Liu", "authors": "Jingchao Liu, Ye Du, Zehua Fu, Qingjie Liu, Yunhong Wang", "title": "ARM: A Confidence-Based Adversarial Reweighting Module for Coarse\n  Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coarsely-labeled semantic segmentation annotations are easy to obtain, but\ntherefore bear the risk of losing edge details and introducing background\npixels. Impeded by the inherent noise, existing coarse annotations are only\ntaken as a bonus for model pre-training. In this paper, we try to exploit their\npotentials with a confidence-based reweighting strategy. To expand, loss-based\nreweighting strategies usually take the high loss value to identify two\ncompletely different types of pixels, namely, valuable pixels in noise-free\nannotations and mislabeled pixels in noisy annotations. This makes it\nimpossible to perform two tasks of mining valuable pixels and suppressing\nmislabeled pixels at the same time. However, with the help of the prediction\nconfidence, we successfully solve this dilemma and simultaneously perform two\nsubtasks with a single reweighting strategy. Furthermore, we generalize this\nstrategy into an Adversarial Reweighting Module (ARM) and prove its convergence\nstrictly. Experiments on standard datasets shows our ARM can bring consistent\nimprovements for both coarse annotations and fine annotations. Specifically,\nbuilt on top of DeepLabv3+, ARM improves the mIoU on the coarsely-labeled\nCityscapes by a considerable margin and increases the mIoU on the ADE20K\ndataset to 47.50.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 02:31:46 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 04:53:59 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Liu", "Jingchao", ""], ["Du", "Ye", ""], ["Fu", "Zehua", ""], ["Liu", "Qingjie", ""], ["Wang", "Yunhong", ""]]}, {"id": "2009.05214", "submitter": "Jinghua Wang", "authors": "Jinghua Wang and Jianmin Jiang", "title": "Adversarial Learning for Zero-shot Domain Adaptation", "comments": null, "journal-ref": "ECCV2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot domain adaptation (ZSDA) is a category of domain adaptation\nproblems where neither data sample nor label is available for parameter\nlearning in the target domain. With the hypothesis that the shift between a\ngiven pair of domains is shared across tasks, we propose a new method for ZSDA\nby transferring domain shift from an irrelevant task (IrT) to the task of\ninterest (ToI). Specifically, we first identify an IrT, where dual-domain\nsamples are available, and capture the domain shift with a coupled generative\nadversarial networks (CoGAN) in this task. Then, we train a CoGAN for the ToI\nand restrict it to carry the same domain shift as the CoGAN for IrT does. In\naddition, we introduce a pair of co-training classifiers to regularize the\ntraining procedure of CoGAN in the ToI. The proposed method not only derives\nmachine learning models for the non-available target-domain data, but also\nsynthesizes the data themselves. We evaluate the proposed method on benchmark\ndatasets and achieve the state-of-the-art performances.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 03:41:32 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Wang", "Jinghua", ""], ["Jiang", "Jianmin", ""]]}, {"id": "2009.05224", "submitter": "Jihoon Chung Mr", "authors": "Jihoon Chung, Cheng-hsin Wuu, Hsuan-ru Yang, Yu-Wing Tai, Chi-Keung\n  Tang", "title": "HAA500: Human-Centric Atomic Action Dataset with Curated Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We contribute HAA500, a manually annotated human-centric atomic action\ndataset for action recognition on 500 classes with over 591k labeled frames.\nUnlike existing atomic action datasets, where coarse-grained atomic actions\nwere labeled with action-verbs, e.g., \"Throw\", HAA500 contains fine-grained\natomic actions where only consistent actions fall under the same label, e.g.,\n\"Baseball Pitching\" vs \"Free Throw in Basketball\", to minimize ambiguities in\naction classification. HAA500 has been carefully curated to capture the\nmovement of human figures with less spatio-temporal label noises to greatly\nenhance the training of deep neural networks. The advantages of HAA500 include:\n1) human-centric actions with a high average of 69.7% detectable joints for the\nrelevant human poses; 2) each video captures the essential elements of an\natomic action without irrelevant frames; 3) fine-grained atomic action classes.\nOur extensive experiments validate the benefits of human-centric and atomic\ncharacteristics of HAA, which enables the trained model to improve prediction\nby attending to atomic human poses. We detail the HAA500 dataset statistics and\ncollection methodology, and compare quantitatively with existing action\nrecognition datasets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 04:18:41 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Chung", "Jihoon", ""], ["Wuu", "Cheng-hsin", ""], ["Yang", "Hsuan-ru", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "2009.05228", "submitter": "Jinghua Wang", "authors": "Jinghua Wang and Jianmin Jiang", "title": "Conditional Coupled Generative Adversarial Networks for Zero-Shot Domain\n  Adaptation", "comments": null, "journal-ref": "ICCV2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models trained in one domain perform poorly in the other\ndomains due to the existence of domain shift. Domain adaptation techniques\nsolve this problem by training transferable models from the label-rich source\ndomain to the label-scarce target domain. Unfortunately, a majority of the\nexisting domain adaptation techniques rely on the availability of target-domain\ndata, and thus limit their applications to a small community across few\ncomputer vision problems. In this paper, we tackle the challenging zero-shot\ndomain adaptation (ZSDA) problem, where target-domain data is non-available in\nthe training stage. For this purpose, we propose conditional coupled generative\nadversarial networks (CoCoGAN) by extending the coupled generative adversarial\nnetworks (CoGAN) into a conditioning model. Compared with the existing state of\nthe arts, our proposed CoCoGAN is able to capture the joint distribution of\ndual-domain samples in two different tasks, i.e. the relevant task (RT) and an\nirrelevant task (IRT). We train CoCoGAN with both source-domain samples in RT\nand dual-domain samples in IRT to complete the domain adaptation. While the\nformer provide high-level concepts of the non-available target-domain data, the\nlatter carry the sharing correlation between the two domains in RT and IRT. To\ntrain CoCoGAN in the absence of target-domain data for RT, we propose a new\nsupervisory signal, i.e. the alignment between representations across tasks.\nExtensive experiments carried out demonstrate that our proposed CoCoGAN\noutperforms existing state of the arts in image classifications.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 04:36:42 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Wang", "Jinghua", ""], ["Jiang", "Jianmin", ""]]}, {"id": "2009.05234", "submitter": "Jinghua Wang", "authors": "Jinghua Wang and Jianmin Jiang", "title": "An unsupervised deep learning framework via integrated optimization of\n  representation learning and GMM-based modeling", "comments": null, "journal-ref": "ACCV2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While supervised deep learning has achieved great success in a range of\napplications, relatively little work has studied the discovery of knowledge\nfrom unlabeled data. In this paper, we propose an unsupervised deep learning\nframework to provide a potential solution for the problem that existing deep\nlearning techniques require large labeled data sets for completing the training\nprocess. Our proposed introduces a new principle of joint learning on both deep\nrepresentations and GMM (Gaussian Mixture Model)-based deep modeling, and thus\nan integrated objective function is proposed to facilitate the principle. In\ncomparison with the existing work in similar areas, our objective function has\ntwo learning targets, which are created to be jointly optimized to achieve the\nbest possible unsupervised learning and knowledge discovery from unlabeled data\nsets. While maximizing the first target enables the GMM to achieve the best\npossible modeling of the data representations and each Gaussian component\ncorresponds to a compact cluster, maximizing the second term will enhance the\nseparability of the Gaussian components and hence the inter-cluster distances.\nAs a result, the compactness of clusters is significantly enhanced by reducing\nthe intra-cluster distances, and the separability is improved by increasing the\ninter-cluster distances. Extensive experimental results show that the propose\nmethod can improve the clustering performance compared with benchmark methods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 04:57:03 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Wang", "Jinghua", ""], ["Jiang", "Jianmin", ""]]}, {"id": "2009.05235", "submitter": "Jinghua Wang", "authors": "Jinghua Wang, Adrian Hilton and Jianmin Jiang", "title": "Spectral Analysis Network for Deep Representation Learning and Image\n  Clustering", "comments": null, "journal-ref": "ICME2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep representation learning is a crucial procedure in multimedia analysis\nand attracts increasing attention. Most of the popular techniques rely on\nconvolutional neural network and require a large amount of labeled data in the\ntraining procedure. However, it is time consuming or even impossible to obtain\nthe label information in some tasks due to cost limitation. Thus, it is\nnecessary to develop unsupervised deep representation learning techniques. This\npaper proposes a new network structure for unsupervised deep representation\nlearning based on spectral analysis, which is a popular technique with solid\ntheory foundations. Compared with the existing spectral analysis methods, the\nproposed network structure has at least three advantages. Firstly, it can\nidentify the local similarities among images in patch level and thus more\nrobust against occlusion. Secondly, through multiple consecutive spectral\nanalysis procedures, the proposed network can learn more clustering-friendly\nrepresentations and is capable to reveal the deep correlations among data\nsamples. Thirdly, it can elegantly integrate different spectral analysis\nprocedures, so that each spectral analysis procedure can have their individual\nstrengths in dealing with different data sample distributions. Extensive\nexperimental results show the effectiveness of the proposed methods on various\nimage clustering tasks.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 05:07:15 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Wang", "Jinghua", ""], ["Hilton", "Adrian", ""], ["Jiang", "Jianmin", ""]]}, {"id": "2009.05236", "submitter": "Yuke Wang", "authors": "Yuke Wang, Boyuan Feng, Xueqiao Peng, Yufei Ding", "title": "Optimizing Convolutional Neural Network Architecture via Information\n  Field", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNN architecture design has attracted tremendous attention of improving model\naccuracy or reducing model complexity. However, existing works either introduce\nrepeated training overhead in the search process or lack an interpretable\nmetric to guide the design. To clear the hurdles, we propose Information Field\n(IF), an explainable and easy-to-compute metric, to estimate the quality of a\nCNN architecture and guide the search process of designs. To validate the\neffectiveness of IF, we build a static optimizer to improve the CNN\narchitectures at both the stage level and the kernel level. Our optimizer not\nonly provides a clear and reproducible procedure but also mitigates unnecessary\ntraining efforts in the architecture search process. Experiments show that the\nmodels generated by our optimizer can achieve up to 5.47% accuracy improvement\nand up to 65.38% parameters deduction, compared with state-of-the-art CNN\nstructures like MobileNet and ResNet.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 05:14:34 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Wang", "Yuke", ""], ["Feng", "Boyuan", ""], ["Peng", "Xueqiao", ""], ["Ding", "Yufei", ""]]}, {"id": "2009.05244", "submitter": "Shao-Yuan Lo", "authors": "Shao-Yuan Lo, Vishal M. Patel", "title": "Defending Against Multiple and Unforeseen Adversarial Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial robustness of deep neural networks has been actively\ninvestigated. However, most existing defense approaches are limited to a\nspecific type of adversarial perturbations. Specifically, they often fail to\noffer resistance to multiple attack types simultaneously, i.e., they lack\nmulti-perturbation robustness. Furthermore, compared to image recognition\nproblems, the adversarial robustness of video recognition models is relatively\nunexplored. While several studies have proposed how to generate adversarial\nvideos, only a handful of approaches about the defense strategies have been\npublished in the literature. In this paper, we propose one of the first defense\nstrategies against multiple types of adversarial videos for video recognition.\nThe proposed method, referred to as MultiBN, performs adversarial training on\nmultiple adversarial video types using multiple independent batch normalization\n(BN) layers with a learning-based BN selection module. With a multiple BN\nstructure, each BN brach is responsible for learning the distribution of a\nsingle perturbation type and thus provides more precise distribution\nestimations. This mechanism benefits dealing with multiple perturbation types.\nThe BN selection module detects the attack type of an input video and sends it\nto the corresponding BN branch, making MultiBN fully automatic and allow\nend-to-end training. Compared to present adversarial training approaches, the\nproposed MultiBN exhibits stronger multi-perturbation robustness against\ndifferent and even unforeseen adversarial video types, ranging from Lp-bounded\nattacks and physically realizable attacks. This holds true on different\ndatasets and target models. Moreover, we conduct an extensive analysis to study\nthe properties of the multiple BN structure.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 06:07:14 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 01:18:59 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Lo", "Shao-Yuan", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2009.05250", "submitter": "Xinyang Jiang", "authors": "Fufu Yu, Xinyang Jiang, Yifei Gong, Shizhen Zhao, Xiaowei Guo, Wei-Shi\n  Zheng, Feng Zheng, Xing Sun", "title": "Devil's in the Details: Aligning Visual Clues for Conditional Embedding\n  in Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Person Re-Identification has made impressive progress, difficult\ncases like occlusion, change of view-pointand similar clothing still bring\ngreat challenges. Besides overall visual features, matching and comparing\ndetailed information is also essential for tackling these challenges. This\npaper proposes two key recognition patterns to better utilize the detail\ninformation of pedestrian images, that most of the existing methods are unable\nto satisfy. Firstly, Visual Clue Alignment requires the model to select and\nalign decisive regions pairs from two images for pair-wise comparison, while\nexisting methods only align regions with predefined rules like high feature\nsimilarity or same semantic labels. Secondly, the Conditional Feature Embedding\nrequires the overall feature of a query image to be dynamically adjusted based\non the gallery image it matches, while most of the existing methods ignore the\nreference images. By introducing novel techniques including correspondence\nattention module and discrepancy-based GCN, we propose an end-to-end ReID\nmethod that integrates both patterns into a unified framework, called\nCACE-Net((C)lue(A)lignment and (C)onditional (E)mbedding). The experiments show\nthat CACE-Net achieves state-of-the-art performance on three public datasets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 06:28:56 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 11:07:51 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Yu", "Fufu", ""], ["Jiang", "Xinyang", ""], ["Gong", "Yifei", ""], ["Zhao", "Shizhen", ""], ["Guo", "Xiaowei", ""], ["Zheng", "Wei-Shi", ""], ["Zheng", "Feng", ""], ["Sun", "Xing", ""]]}, {"id": "2009.05252", "submitter": "Kuo-Liang Chung", "authors": "Kuo-Liang Chung and De-Wei Hsieh", "title": "Novel and Effective CNN-Based Binarization for Historically Degraded\n  As-built Drawing Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binarizing historically degraded as-built drawing (HDAD) maps is a new\nchallenging job, especially in terms of removing the three artifacts, namely\nnoise, the yellowing areas, and the folded lines, while preserving the\nforeground components well. In this paper, we first propose a semi-automatic\nlabeling method to create the HDAD-pair dataset of which each HDAD-pair\nconsists of one HDAD map and its binarized HDAD map. Based on the created\ntraining HDAD-pair dataset, we propose a convolutional neural network-based\n(CNN-based) binarization method to produce high-quality binarized HDAD maps.\nBased on the testing HDAD maps, the thorough experimental data demonstrated\nthat in terms of the accuracy, PSNR (peak-signal-to-noise-ratio), and the\nperceptual effect of the binarized HDAD maps, our method substantially\noutperforms the nine existing binarization methods. In addition, with similar\naccuracy, the experimental results demonstrated the significant execution-time\nreduction merit of our method relative to the retrained version of the\nstate-of-the-art CNN-based binarization methods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 06:49:28 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Chung", "Kuo-Liang", ""], ["Hsieh", "De-Wei", ""]]}, {"id": "2009.05254", "submitter": "Saroj Sahoo", "authors": "Saroj Sahoo and Matthew Berger", "title": "Visually Analyzing and Steering Zero Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a visual analytics system to help a user analyze and steer\nzero-shot learning models. Zero-shot learning has emerged as a viable scenario\nfor categorizing data that consists of no labeled examples, and thus a\npromising approach to minimize data annotation from humans. However, it is\nchallenging to understand where zero-shot learning fails, the cause of such\nfailures, and how a user can modify the model to prevent such failures. Our\nvisualization system is designed to help users diagnose and understand\nmispredictions in such models, so that they may gain insight on the behavior of\na model when applied to data associated with categories not seen during\ntraining. Through usage scenarios, we highlight how our system can help a user\nimprove performance in zero-shot learning.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 06:58:13 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Sahoo", "Saroj", ""], ["Berger", "Matthew", ""]]}, {"id": "2009.05267", "submitter": "Weihua Liu", "authors": "Weihua Liu, Xiabi Liua, Xiongbiao Luo, Murong Wang, Guanghui Han,\n  Xinming Zhao, Zheng Zhu", "title": "PiaNet: A pyramid input augmented convolutional neural network for GGO\n  detection in 3D lung CT scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new convolutional neural network with multiscale\nprocessing for detecting ground-glass opacity (GGO) nodules in 3D computed\ntomography (CT) images, which is referred to as PiaNet for short. PiaNet\nconsists of a feature-extraction module and a prediction module. The former\nmodule is constructed by introducing pyramid multiscale source connections into\na contracting-expanding structure. The latter module includes a bounding-box\nregressor and a classifier that are employed to simultaneously recognize GGO\nnodules and estimate bounding boxes at multiple scales. To train the proposed\nPiaNet, a two-stage transfer learning strategy is developed. In the first\nstage, the feature-extraction module is embedded into a classifier network that\nis trained on a large data set of GGO and non-GGO patches, which are generated\nby performing data augmentation from a small number of annotated CT scans. In\nthe second stage, the pretrained feature-extraction module is loaded into\nPiaNet, and then PiaNet is fine-tuned using the annotated CT scans. We evaluate\nthe proposed PiaNet on the LIDC-IDRI data set. The experimental results\ndemonstrate that our method outperforms state-of-the-art counterparts,\nincluding the Subsolid CAD and Aidence systems and S4ND and GA-SSD methods.\nPiaNet achieves a sensitivity of 91.75% with only one false positive per scan\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 07:52:17 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 05:23:46 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Liu", "Weihua", ""], ["Liua", "Xiabi", ""], ["Luo", "Xiongbiao", ""], ["Wang", "Murong", ""], ["Han", "Guanghui", ""], ["Zhao", "Xinming", ""], ["Zhu", "Zheng", ""]]}, {"id": "2009.05269", "submitter": "Neeraj Baghel", "authors": "Neeraj Baghel, Suresh C. Raikwar, Charul Bhatnagar", "title": "Image Conditioned Keyframe-Based Video Summarization Using Object\n  Detection", "comments": "Paper is submitted in Pattern Analysis and Applications\n  PAAA-D-19-00604", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summarization plays an important role in selecting keyframe for\nunderstanding a video. Traditionally, it aims to find the most representative\nand diverse contents (or frames) in a video for short summaries. Recently,\nquery-conditioned video summarization has been introduced, which considers user\nqueries to learn more user-oriented summaries and its preference. However,\nthere are obstacles in text queries for user subjectivity and finding\nsimilarity between the user query and input frames. In this work, (i) Image is\nintroduced as a query for user preference (ii) a mathematical model is proposed\nto minimize redundancy based on the loss function & summary variance and (iii)\nthe similarity score between the query image and input video to obtain the\nsummarized video. Furthermore, the Object-based Query Image (OQI) dataset has\nbeen introduced, which contains the query images. The proposed method has been\nvalidated using UT Egocentric (UTE) dataset. The proposed model successfully\nresolved the issues of (i) user preference, (ii) recognize important frames and\nselecting that keyframe in daily life videos, with different illumination\nconditions. The proposed method achieved 57.06% average F1-Score for UTE\ndataset and outperforms the existing state-of-theart by 11.01%. The process\ntime is 7.81 times faster than actual time of video Experiments on a recently\nproposed UTE dataset show the efficiency of the proposed method\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 07:56:17 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Baghel", "Neeraj", ""], ["Raikwar", "Suresh C.", ""], ["Bhatnagar", "Charul", ""]]}, {"id": "2009.05277", "submitter": "Shujaat Khan", "authors": "Shujaat Khan, Muhammad Usman, Abdul Wahab", "title": "AFP-SRC: Identification of Antifreeze Proteins Using Sparse\n  Representation Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Species living in the extreme cold environment fight against the harsh\nconditions using antifreeze proteins (AFPs), that manipulates the freezing\nmechanism of water in more than one way. This amazing nature of AFP turns out\nto be extremely useful in several industrial and medical applications. The lack\nof similarity in their structure and sequence makes their prediction an arduous\ntask and identifying them experimentally in the wet-lab is time-consuming and\nexpensive. In this research, we propose a computational framework for the\nprediction of AFPs which is essentially based on a sample-specific\nclassification method using the sparse reconstruction. A linear model and an\nover-complete dictionary matrix of known AFPs are used to predict a sparse\nclass-label vector that provides a sample-association score. Delta-rule is\napplied for the reconstruction of two pseudo-samples using lower and upper\nparts of the sample-association vector and based on the minimum recovery score,\nclass labels are assigned. We compare our approach with contemporary methods on\na standard dataset and the proposed method is found to outperform in terms of\nBalanced accuracy and Youden's index. The MATLAB implementation of the proposed\nmethod is available at the author's GitHub page\n(\\{https://github.com/Shujaat123/AFP-SRC}{https://github.com/Shujaat123/AFP-SRC}).\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 08:24:50 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 07:01:30 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Khan", "Shujaat", ""], ["Usman", "Muhammad", ""], ["Wahab", "Abdul", ""]]}, {"id": "2009.05283", "submitter": "David Berend", "authors": "Yushi Cao, David Berend, Palina Tolmach, Guy Amit, Moshe Levy, Yang\n  Liu, Asaf Shabtai, Yuval Elovici", "title": "Out-of-distribution detection and generalization to enhance fairness in\n  age prediction", "comments": "15 pages, 8 Figures, pre-print, adjusted title and content", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based facial recognition systems have experienced increased\nmedia attention due to exhibiting unfair behavior. Large enterprises, such as\nIBM, shut down their facial recognition and age prediction systems as a\nconsequence. Age prediction is an especially difficult application with the\nissue of fairness remaining an open research problem (e.g. predicting age for\ndifferent ethnicity equally accurate). One of the main causes of unfair\nbehavior in age prediction methods lies in the distribution and diversity of\nthe training data. In this work, we present two novel approaches for dataset\ncuration and data augmentation in order to increase fairness through\ndistribution aware curation and increase diversity through distribution aware\naugmentation. To achieve this, we created an out-of-distribution technique\nwhich is used to select the data most relevant to the deep neural network's\n(DNN) task when balancing the data among age, ethnicity, and gender. Our\napproach shows promising results. Our best-trained DNN model outperformed all\nacademic and industrial baselines in terms of fairness by up to 4.92 times.\nWhen it comes to generalization, the increase in diversity also enhanced the\nDNN's performance, outperforming state-of-the-art approaches of prior research\non the Age Estimation Benchmark dataset AFAD by 30.40% and the Amazon AWS and\nMicrosoft Azure public cloud systems by 31.88% and 10.95%, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 08:32:36 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 01:14:56 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 05:52:51 GMT"}, {"version": "v4", "created": "Tue, 20 Apr 2021 13:02:48 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Cao", "Yushi", ""], ["Berend", "David", ""], ["Tolmach", "Palina", ""], ["Amit", "Guy", ""], ["Levy", "Moshe", ""], ["Liu", "Yang", ""], ["Shabtai", "Asaf", ""], ["Elovici", "Yuval", ""]]}, {"id": "2009.05284", "submitter": "Jianan Li", "authors": "Jianan Li, Jimei Yang, Jianming Zhang, Chang Liu, Christina Wang,\n  Tingfa Xu", "title": "Attribute-conditioned Layout GAN for Automatic Graphic Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling layout is an important first step for graphic design. Recently,\nmethods for generating graphic layouts have progressed, particularly with\nGenerative Adversarial Networks (GANs). However, the problem of specifying the\nlocations and sizes of design elements usually involves constraints with\nrespect to element attributes, such as area, aspect ratio and reading-order.\nAutomating attribute conditional graphic layouts remains a complex and unsolved\nproblem. In this paper, we introduce Attribute-conditioned Layout GAN to\nincorporate the attributes of design elements for graphic layout generation by\nforcing both the generator and the discriminator to meet attribute conditions.\nDue to the complexity of graphic designs, we further propose an element dropout\nmethod to make the discriminator look at partial lists of elements and learn\ntheir local patterns. In addition, we introduce various loss designs following\ndifferent design principles for layout optimization. We demonstrate that the\nproposed method can synthesize graphic layouts conditioned on different element\nattributes. It can also adjust well-designed layouts to new sizes while\nretaining elements' original reading-orders. The effectiveness of our method is\nvalidated through a user study.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 08:34:17 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Li", "Jianan", ""], ["Yang", "Jimei", ""], ["Zhang", "Jianming", ""], ["Liu", "Chang", ""], ["Wang", "Christina", ""], ["Xu", "Tingfa", ""]]}, {"id": "2009.05290", "submitter": "Yi Fang", "authors": "Xiang Li, Lingjing Wang, Yi Fang", "title": "Unsupervised Partial Point Set Registration via Joint Shape Completion\n  and Registration", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-supervised method for partial point set registration. While\nrecent proposed learning-based methods have achieved impressive registration\nperformance on the full shape observations, these methods mostly suffer from\nperformance degradation when dealing with partial shapes. To bridge the\nperformance gaps between partial point set registration with full point set\nregistration, we proposed to incorporate a shape completion network to benefit\nthe registration process. To achieve this, we design a latent code for each\npair of shapes, which can be regarded as a geometric encoding of the target\nshape. By doing so, our model does need an explicit feature embedding network\nto learn the feature encodings. More importantly, both our shape completion\nnetwork and the point set registration network take the shared latent codes as\ninput, which are optimized along with the parameters of two decoder networks in\nthe training process. Therefore, the point set registration process can thus\nbenefit from the joint optimization process of latent codes, which are enforced\nto represent the information of full shape instead of partial ones. In the\ninference stage, we fix the network parameter and optimize the latent codes to\nget the optimal shape completion and registration results. Our proposed method\nis pure unsupervised and does not need any ground truth supervision.\nExperiments on the ModelNet40 dataset demonstrate the effectiveness of our\nmodel for partial point set registration.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 08:50:53 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Li", "Xiang", ""], ["Wang", "Lingjing", ""], ["Fang", "Yi", ""]]}, {"id": "2009.05300", "submitter": "Markus Borg", "authors": "August Lidfelt, Daniel Isaksson, Ludwig Hedlund, Simon {\\AA}berg,\n  Markus Borg, Erik Larsson", "title": "Enabling Image Recognition on Constrained Devices Using Neural Network\n  Pruning and a CycleGAN", "comments": "Accepted for publication in the Proc. of the 1st international\n  workshop on Internet of Things for Emergency Management", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart cameras are increasingly used in surveillance solutions in public\nspaces. Contemporary computer vision applications can be used to recognize\nevents that require intervention by emergency services. Smart cameras can be\nmounted in locations where citizens feel particularly unsafe, e.g., pathways\nand underpasses with a history of incidents. One promising approach for smart\ncameras is edge AI, i.e., deploying AI technology on IoT devices. However,\nimplementing resource-demanding technology such as image recognition using deep\nneural networks (DNN) on constrained devices is a substantial challenge. In\nthis paper, we explore two approaches to reduce the need for compute in\ncontemporary image recognition in an underpass. First, we showcase successful\nneural network pruning, i.e., we retain comparable classification accuracy with\nonly 1.1\\% of the neurons remaining from the state-of-the-art DNN architecture.\nSecond, we demonstrate how a CycleGAN can be used to transform\nout-of-distribution images to the operational design domain. We posit that both\npruning and CycleGANs are promising enablers for efficient edge AI in smart\ncameras.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 09:12:52 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Lidfelt", "August", ""], ["Isaksson", "Daniel", ""], ["Hedlund", "Ludwig", ""], ["\u00c5berg", "Simon", ""], ["Borg", "Markus", ""], ["Larsson", "Erik", ""]]}, {"id": "2009.05307", "submitter": "Jie Li", "authors": "Jie Li, Yu Hu", "title": "A Density-Aware PointRCNN for 3D Object Detection in Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an improved version of PointRCNN for 3D object detection, in which\na multi-branch backbone network is adopted to handle the non-uniform density of\npoint clouds. An uncertainty-based sampling policy is proposed to deal with the\ndistribution differences of different point clouds. The new model can achieve\nabout 0.8 AP higher performance than the baseline PointRCNN on KITTI val set.\nIn addition, a simplified model using a single scale grouping for each\nset-abstraction layer can achieve competitive performance with less\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 09:35:13 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 04:33:52 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Li", "Jie", ""], ["Hu", "Yu", ""]]}, {"id": "2009.05317", "submitter": "Baozhou Zhu", "authors": "Zhu Baozhou, Peter Hofstee, Jinho Lee, Zaid Al-Ars", "title": "SoFAr: Shortcut-based Fractal Architectures for Binary Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary Convolutional Neural Networks (BCNNs) can significantly improve the\nefficiency of Deep Convolutional Neural Networks (DCNNs) for their deployment\non resource-constrained platforms, such as mobile and embedded systems.\nHowever, the accuracy degradation of BCNNs is still considerable compared with\ntheir full precision counterpart, impeding their practical deployment. Because\nof the inevitable binarization error in the forward propagation and gradient\nmismatch problem in the backward propagation, it is nontrivial to train BCNNs\nto achieve satisfactory accuracy. To ease the difficulty of training, the\nshortcut-based BCNNs, such as residual connection-based Bi-real ResNet and\ndense connection-based BinaryDenseNet, introduce additional shortcuts in\naddition to the shortcuts already present in their full precision counterparts.\nFurthermore, fractal architectures have been also been used to improve the\ntraining process of full-precision DCNNs since the fractal structure triggers\neffects akin to deep supervision and lateral student-teacher information flow.\nInspired by the shortcuts and fractal architectures, we propose two\nShortcut-based Fractal Architectures (SoFAr) specifically designed for BCNNs:\n1. residual connection-based fractal architectures for binary ResNet, and 2.\ndense connection-based fractal architectures for binary DenseNet. Our proposed\nSoFAr combines the adoption of shortcuts and the fractal architectures in one\nunified model, which is helpful in the training of BCNNs. Results show that our\nproposed SoFAr achieves better accuracy compared with shortcut-based BCNNs.\nSpecifically, the Top-1 accuracy of our proposed RF-c4d8 ResNet37(41) and\nDRF-c2d2 DenseNet51(53) on ImageNet outperforms Bi-real ResNet18(64) and\nBinaryDenseNet51(32) by 3.29% and 1.41%, respectively, with the same\ncomputational complexity overhead.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 10:00:47 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Baozhou", "Zhu", ""], ["Hofstee", "Peter", ""], ["Lee", "Jinho", ""], ["Al-Ars", "Zaid", ""]]}, {"id": "2009.05331", "submitter": "Alvaro Quintanar", "authors": "A. Quintanar, R. Izquierdo, I. Parra, D. Fern\\'andez-Llorca, and M. A.\n  Sotelo", "title": "The PREVENTION Challenge: How Good Are Humans Predicting Lane Changes?", "comments": "This work was accepted and presented at IEEE Intelligent Vehicles\n  Symposium 2020", "journal-ref": "2020 IEEE Intelligent Vehicles Symposium (IV), 2020, pp. 45-50", "doi": "10.1109/IV47402.2020.9304640", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While driving on highways, every driver tries to be aware of the behavior of\nsurrounding vehicles, including possible emergency braking, evasive maneuvers\ntrying to avoid obstacles, unexpected lane changes, or other emergencies that\ncould lead to an accident. In this paper, human's ability to predict lane\nchanges in highway scenarios is analyzed through the use of video sequences\nextracted from the PREVENTION dataset, a database focused on the development of\nresearch on vehicle intention and trajectory prediction. Thus, users had to\nindicate the moment at which they considered that a lane change maneuver was\ntaking place in a target vehicle, subsequently indicating its direction: left\nor right. The results retrieved have been carefully analyzed and compared to\nground truth labels, evaluating statistical models to understand whether humans\ncan actually predict. The study has revealed that most participants are unable\nto anticipate lane-change maneuvers, detecting them after they have started.\nThese results might serve as a baseline for AI's prediction ability evaluation,\ngrading if those systems can outperform human skills by analyzing hidden cues\nthat seem unnoticed, improving the detection time, and even anticipating\nmaneuvers in some cases.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 10:47:07 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 23:49:54 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Quintanar", "A.", ""], ["Izquierdo", "R.", ""], ["Parra", "I.", ""], ["Fern\u00e1ndez-Llorca", "D.", ""], ["Sotelo", "M. A.", ""]]}, {"id": "2009.05346", "submitter": "Nicolo Colombo", "authors": "Nicolo Colombo and Yang Gao", "title": "Disentangling Neural Architectures and Weights: A Case Study in\n  Supervised Classification", "comments": "22 pages and 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The history of deep learning has shown that human-designed problem-specific\nnetworks can greatly improve the classification performance of general neural\nmodels. In most practical cases, however, choosing the optimal architecture for\na given task remains a challenging problem. Recent architecture-search methods\nare able to automatically build neural models with strong performance but fail\nto fully appreciate the interaction between neural architecture and weights.\nThis work investigates the problem of disentangling the role of the neural\nstructure and its edge weights, by showing that well-trained architectures may\nnot need any link-specific fine-tuning of the weights. We compare the\nperformance of such weight-free networks (in our case these are binary networks\nwith {0, 1}-valued weights) with random, weight-agnostic, pruned and standard\nfully connected networks. To find the optimal weight-agnostic network, we use a\nnovel and computationally efficient method that translates the hard\narchitecture-search problem into a feasible optimization problem.More\nspecifically, we look at the optimal task-specific architectures as the optimal\nconfiguration of binary networks with {0, 1}-valued weights, which can be found\nthrough an approximate gradient descent strategy. Theoretical convergence\nguarantees of the proposed algorithm are obtained by bounding the error in the\ngradient approximation and its practical performance is evaluated on two\nreal-world data sets. For measuring the structural similarities between\ndifferent architectures, we use a novel spectral approach that allows us to\nunderline the intrinsic differences between real-valued networks and\nweight-free architectures.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 11:22:22 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Colombo", "Nicolo", ""], ["Gao", "Yang", ""]]}, {"id": "2009.05353", "submitter": "Gabriel Dahia", "authors": "Gabriel Dahia, Maur\\'icio Pamplona Segundo", "title": "Meta Learning for Few-Shot One-class Classification", "comments": null, "journal-ref": null, "doi": "10.3390/ai2020012", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method that can perform one-class classification given only a\nsmall number of examples from the target class and none from the others. We\nformulate the learning of meaningful features for one-class classification as a\nmeta-learning problem in which the meta-training stage repeatedly simulates\none-class classification, using the classification loss of the chosen algorithm\nto learn a feature representation. To learn these representations, we require\nonly multiclass data from similar tasks. We show how the Support Vector Data\nDescription method can be used with our method, and also propose a simpler\nvariant based on Prototypical Networks that obtains comparable performance,\nindicating that learning feature representations directly from data may be more\nimportant than which one-class algorithm we choose. We validate our approach by\nadapting few-shot classification datasets to the few-shot one-class\nclassification scenario, obtaining similar results to the state-of-the-art of\ntraditional one-class classification, and that improves upon that of one-class\nclassification baselines employed in the few-shot setting. Our code is\navailable at https://github.com/gdahia/meta_occ\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 11:35:28 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 12:13:17 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Dahia", "Gabriel", ""], ["Segundo", "Maur\u00edcio Pamplona", ""]]}, {"id": "2009.05369", "submitter": "Franz G\\\"otz-Hahn", "authors": "Franz G\\\"otz-Hahn and Vlad Hosu and Dietmar Saupe", "title": "Critical analysis on the reproducibility of visual quality assessment\n  using deep features", "comments": "27 pages, 8 figures, PLOS ONE journal. arXiv admin note: text overlap\n  with arXiv:2005.04400", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data used to train supervised machine learning models are commonly split into\nindependent training, validation, and test sets. This paper illustrates that\ncomplex data leakage cases have occurred in the no-reference image and video\nquality assessment literature. Recently, papers in several journals reported\nperformance results well above the best in the field. However, our analysis\nshows that information from the test set was inappropriately used in the\ntraining process in different ways and that the claimed performance results\ncannot be achieved. When correcting for the data leakage, the performances of\nthe approaches drop even below the state-of-the-art by a large margin.\nAdditionally, we investigate end-to-end variations to the discussed approaches,\nwhich do not improve upon the original.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 09:51:18 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 09:28:44 GMT"}, {"version": "v3", "created": "Mon, 1 Mar 2021 10:59:22 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["G\u00f6tz-Hahn", "Franz", ""], ["Hosu", "Vlad", ""], ["Saupe", "Dietmar", ""]]}, {"id": "2009.05381", "submitter": "Jianfeng Dong", "authors": "Jianfeng Dong, Xirong Li, Chaoxi Xu, Xun Yang, Gang Yang, Xun Wang,\n  Meng Wang", "title": "Dual Encoding for Video Retrieval by Text", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence. Code and data will be available at\n  https://github.com/danieljf24/hybrid_space. Conference version:\n  arXiv:1809.06181", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3059295", "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper attacks the challenging problem of video retrieval by text. In\nsuch a retrieval paradigm, an end user searches for unlabeled videos by ad-hoc\nqueries described exclusively in the form of a natural-language sentence, with\nno visual example provided. Given videos as sequences of frames and queries as\nsequences of words, an effective sequence-to-sequence cross-modal matching is\ncrucial. To that end, the two modalities need to be first encoded into\nreal-valued vectors and then projected into a common space. In this paper we\nachieve this by proposing a dual deep encoding network that encodes videos and\nqueries into powerful dense representations of their own. Our novelty is\ntwo-fold. First, different from prior art that resorts to a specific\nsingle-level encoder, the proposed network performs multi-level encoding that\nrepresents the rich content of both modalities in a coarse-to-fine fashion.\nSecond, different from a conventional common space learning algorithm which is\neither concept based or latent space based, we introduce hybrid space learning\nwhich combines the high performance of the latent space and the good\ninterpretability of the concept space. Dual encoding is conceptually simple,\npractically effective and end-to-end trained with hybrid space learning.\nExtensive experiments on four challenging video datasets show the viability of\nthe new method.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 15:49:39 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 09:26:20 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Dong", "Jianfeng", ""], ["Li", "Xirong", ""], ["Xu", "Chaoxi", ""], ["Yang", "Xun", ""], ["Yang", "Gang", ""], ["Wang", "Xun", ""], ["Wang", "Meng", ""]]}, {"id": "2009.05383", "submitter": "Alexander Wong", "authors": "Hayden Gunraj, Linda Wang, and Alexander Wong", "title": "COVIDNet-CT: A Tailored Deep Convolutional Neural Network Design for\n  Detection of COVID-19 Cases from Chest CT Images", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronavirus disease 2019 (COVID-19) pandemic continues to have a\ntremendous impact on patients and healthcare systems around the world. In the\nfight against this novel disease, there is a pressing need for rapid and\neffective screening tools to identify patients infected with COVID-19, and to\nthis end CT imaging has been proposed as one of the key screening methods which\nmay be used as a complement to RT-PCR testing, particularly in situations where\npatients undergo routine CT scans for non-COVID-19 related reasons, patients\nwith worsening respiratory status or developing complications that require\nexpedited care, and patients suspected to be COVID-19-positive but have\nnegative RT-PCR test results. Motivated by this, in this study we introduce\nCOVIDNet-CT, a deep convolutional neural network architecture that is tailored\nfor detection of COVID-19 cases from chest CT images via a machine-driven\ndesign exploration approach. Additionally, we introduce COVIDx-CT, a benchmark\nCT image dataset derived from CT imaging data collected by the China National\nCenter for Bioinformation comprising 104,009 images across 1,489 patient cases.\nFurthermore, in the interest of reliability and transparency, we leverage an\nexplainability-driven performance validation strategy to investigate the\ndecision-making behaviour of COVIDNet-CT, and in doing so ensure that\nCOVIDNet-CT makes predictions based on relevant indicators in CT images. Both\nCOVIDNet-CT and the COVIDx-CT dataset are available to the general public in an\nopen-source and open access manner as part of the COVID-Net initiative. While\nCOVIDNet-CT is not yet a production-ready screening solution, we hope that\nreleasing the model and dataset will encourage researchers, clinicians, and\ncitizen data scientists alike to leverage and build upon them.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 15:49:55 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Gunraj", "Hayden", ""], ["Wang", "Linda", ""], ["Wong", "Alexander", ""]]}, {"id": "2009.05388", "submitter": "Hannes Fassold", "authors": "Hannes Fassold", "title": "Automatic cinematography for 360 video", "comments": "Accepted as demo paper for IEEE MMSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our method for automatic generation of a visually interesting\ncamera path (automatic cinematography)from a 360 video. Based on the\ninformation from the scene objects, multiple shot hypotheses for different shot\ntypes are constructed and the best one is rendered.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 17:14:05 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Fassold", "Hannes", ""]]}, {"id": "2009.05389", "submitter": "Abassin Sourou Fangbemi", "authors": "Abassin Sourou Fangbemi, Yi Fei Lu, Mao Yuan Xu, Xiao Wu Luo, Alexis\n  Rolland, Chedy Raissi", "title": "ZooBuilder: 2D and 3D Pose Estimation for Quadrupeds Using Synthetic\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a novel strategy for generating synthetic training data\nfor 2D and 3D pose estimation of animals using keyframe animations. With the\nobjective to automate the process of creating animations for wildlife, we train\nseveral 2D and 3D pose estimation models with synthetic data, and put in place\nan end-to-end pipeline called ZooBuilder. The pipeline takes as input a video\nof an animal in the wild, and generates the corresponding 2D and 3D coordinates\nfor each joint of the animal's skeleton. With this approach, we produce motion\ncapture data that can be used to create animations for wildlife.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 07:41:20 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Fangbemi", "Abassin Sourou", ""], ["Lu", "Yi Fei", ""], ["Xu", "Mao Yuan", ""], ["Luo", "Xiao Wu", ""], ["Rolland", "Alexis", ""], ["Raissi", "Chedy", ""]]}, {"id": "2009.05406", "submitter": "Zhenzhou Wang", "authors": "Zhenzhou Wang", "title": "Phase Sampling Profilometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured light 3D surface imaging is a school of techniques in which\nstructured light patterns are used for measuring the depth map of the object.\nAmong all the designed structured light patterns, phase pattern has become most\npopular because of its high resolution and high accuracy. Accordingly, phase\nmeasuring profolimetry (PMP) has become the mainstream of structured light\ntechnology. In this letter, we introduce the concept of phase sampling\nprofilometry (PSP) that calculates the phase unambiguously in the\nspatial-frequency domain with only one pattern image. Therefore, PSP is capable\nof measuring the 3D shapes of the moving objects robustly with single-shot.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 09:36:05 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Wang", "Zhenzhou", ""]]}, {"id": "2009.05427", "submitter": "Joonas L\\~omps", "authors": "Joonas Lomps, Artjom Lind, Amnir Hadachi", "title": "Evaluation of the Robustness of Visual SLAM Methods in Different\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Determining the position and orientation of a sensor vis-a-vis its\nsurrounding, while simultaneously mapping the environment around that sensor or\nsimultaneous localization and mapping is quickly becoming an important\nadvancement in embedded vision with a large number of different possible\napplications. This paper presents a comprehensive comparison of the latest\nopen-source SLAM algorithms with the main focus being their performance in\ndifferent environmental surroundings. The chosen algorithms are evaluated on\ncommon publicly available datasets and the results reasoned with respect to the\ndatasets' environment. This is the first stage of our main target of testing\nthe methods in off-road scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 13:21:34 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Lomps", "Joonas", ""], ["Lind", "Artjom", ""], ["Hadachi", "Amnir", ""]]}, {"id": "2009.05429", "submitter": "Steven Morad", "authors": "Steven D. Morad, Roberto Mecca, Rudra P.K. Poudel, Stephan Liwicki,\n  and Roberto Cipolla", "title": "Embodied Visual Navigation with Automatic Curriculum Learning in Real\n  Environments", "comments": null, "journal-ref": null, "doi": "10.1109/LRA.2020.3048662", "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present NavACL, a method of automatic curriculum learning tailored to the\nnavigation task. NavACL is simple to train and efficiently selects relevant\ntasks using geometric features. In our experiments, deep reinforcement learning\nagents trained using NavACL significantly outperform state-of-the-art agents\ntrained with uniform sampling -- the current standard. Furthermore, our agents\ncan navigate through unknown cluttered indoor environments to\nsemantically-specified targets using only RGB images. Obstacle-avoiding\npolicies and frozen feature networks support transfer to unseen real-world\nenvironments, without any modification or retraining requirements. We evaluate\nour policies in simulation, and in the real world on a ground robot and a\nquadrotor drone. Videos of real-world results are available in the\nsupplementary material.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 13:28:26 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 18:29:42 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Morad", "Steven D.", ""], ["Mecca", "Roberto", ""], ["Poudel", "Rudra P. K.", ""], ["Liwicki", "Stephan", ""], ["Cipolla", "Roberto", ""]]}, {"id": "2009.05436", "submitter": "Lei Liu", "authors": "Lei Liu, Wentao Lei, Yongfang Luo, Cheng Feng, Xiang Wan, Li Liu", "title": "Semi-Supervised Active Learning for COVID-19 Lung Ultrasound\n  Multi-symptom Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound (US) is a non-invasive yet effective medical diagnostic imaging\ntechnique for the COVID-19 global pandemic. However, due to complex feature\nbehaviors and expensive annotations of US images, it is difficult to apply\nArtificial Intelligence (AI) assisting approaches for lung's multi-symptom\n(multi-label) classification. To overcome these difficulties, we propose a\nnovel semi-supervised Two-Stream Active Learning (TSAL) method to model\ncomplicated features and reduce labeling costs in an iterative procedure. The\ncore component of TSAL is the multi-label learning mechanism, in which label\ncorrelations information is used to design multi-label margin (MLM) strategy\nand confidence validation for automatically selecting informative samples and\nconfident labels. On this basis, a multi-symptom multi-label (MSML)\nclassification network is proposed to learn discriminative features of lung\nsymptoms, and a human-machine interaction is exploited to confirm the final\nannotations that are used to fine-tune MSML with progressively labeled data.\nMoreover, a novel lung US dataset named COVID19-LUSMS is built, currently\ncontaining 71 clinical patients with 6,836 images sampled from 678 videos.\nExperimental evaluations show that TSAL using only 20% data can achieve\nsuperior performance to the baseline and the state-of-the-art. Qualitatively,\nvisualization of both attention map and sample distribution confirms the good\nconsistency with the clinic knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 10:45:34 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 08:47:52 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Liu", "Lei", ""], ["Lei", "Wentao", ""], ["Luo", "Yongfang", ""], ["Feng", "Cheng", ""], ["Wan", "Xiang", ""], ["Liu", "Li", ""]]}, {"id": "2009.05440", "submitter": "Abhijit Suprem", "authors": "Abhijit Suprem, Joy Arulraj, Calton Pu, Joao Ferreira", "title": "ODIN: Automated Drift Detection and Recovery in Video Analytics", "comments": null, "journal-ref": "PVLDB, 13(11):2453-2465, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in computer vision have led to a resurgence of interest in\nvisual data analytics. Researchers are developing systems for effectively and\nefficiently analyzing visual data at scale. A significant challenge that these\nsystems encounter lies in the drift in real-world visual data. For instance, a\nmodel for self-driving vehicles that is not trained on images containing snow\ndoes not work well when it encounters them in practice. This drift phenomenon\nlimits the accuracy of models employed for visual data analytics. In this\npaper, we present a visual data analytics system, called ODIN, that\nautomatically detects and recovers from drift. ODIN uses adversarial\nautoencoders to learn the distribution of high-dimensional images. We present\nan unsupervised algorithm for detecting drift by comparing the distributions of\nthe given data against that of previously seen data. When ODIN detects drift,\nit invokes a drift recovery algorithm to deploy specialized models tailored\ntowards the novel data points. These specialized models outperform their\nnon-specialized counterpart on accuracy, performance, and memory footprint.\nLastly, we present a model selection algorithm for picking an ensemble of\nbest-fit specialized models to process a given input. We evaluate the efficacy\nand efficiency of ODIN on high-resolution dashboard camera videos captured\nunder diverse environments from the Berkeley DeepDrive dataset. We demonstrate\nthat ODIN's models deliver 6x higher throughput, 2x higher accuracy, and 6x\nsmaller memory footprint compared to a baseline system without automated drift\ndetection and recovery.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 12:13:40 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Suprem", "Abhijit", ""], ["Arulraj", "Joy", ""], ["Pu", "Calton", ""], ["Ferreira", "Joao", ""]]}, {"id": "2009.05448", "submitter": "Yufei Wang", "authors": "Yufei Wang (1 and 2), Haoliang Li (2), and Alex C. Kot (2)((1)\n  University of Electronic Science and Technology of China, China, (2) Nanyang\n  Technological University, Singapore)", "title": "Heterogeneous Domain Generalization via Domain Mixup", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9053273", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main drawbacks of deep Convolutional Neural Networks (DCNN) is\nthat they lack generalization capability. In this work, we focus on the problem\nof heterogeneous domain generalization which aims to improve the generalization\ncapability across different tasks, which is, how to learn a DCNN model with\nmultiple domain data such that the trained feature extractor can be generalized\nto supporting recognition of novel categories in a novel target domain. To\nsolve this problem, we propose a novel heterogeneous domain generalization\nmethod by mixing up samples across multiple source domains with two different\nsampling strategies. Our experimental results based on the Visual Decathlon\nbenchmark demonstrates the effectiveness of our proposed method. The code is\nreleased in \\url{https://github.com/wyf0912/MIXALL}\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 13:53:56 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Wang", "Yufei", "", "1 and 2"], ["Li", "Haoliang", ""], ["Kot", "Alex C.", ""]]}, {"id": "2009.05455", "submitter": "Klaus Ackermann", "authors": "Klaus Ackermann, Alexey Chernikov, Nandini Anantharama, Miethy Zaman,\n  Paul A Raschky", "title": "Object Recognition for Economic Development from Daytime Satellite\n  Imagery", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.CV eess.IV q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable data about the stock of physical capital and infrastructure in\ndeveloping countries is typically very scarce. This is particular a problem for\ndata at the subnational level where existing data is often outdated, not\nconsistently measured or coverage is incomplete. Traditional data collection\nmethods are time and labor-intensive costly, which often prohibits developing\ncountries from collecting this type of data. This paper proposes a novel method\nto extract infrastructure features from high-resolution satellite images. We\ncollected high-resolution satellite images for 5 million 1km $\\times$ 1km grid\ncells covering 21 African countries. We contribute to the growing body of\nliterature in this area by training our machine learning algorithm on\nground-truth data. We show that our approach strongly improves the predictive\naccuracy. Our methodology can build the foundation to then predict subnational\nindicators of economic development for areas where this data is either missing\nor unreliable.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 14:07:12 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Ackermann", "Klaus", ""], ["Chernikov", "Alexey", ""], ["Anantharama", "Nandini", ""], ["Zaman", "Miethy", ""], ["Raschky", "Paul A", ""]]}, {"id": "2009.05475", "submitter": "Alexia Jolicoeur-Martineau", "authors": "Alexia Jolicoeur-Martineau, R\\'emi Pich\\'e-Taillefer, R\\'emi Tachet\n  des Combes, Ioannis Mitliagkas", "title": "Adversarial score matching and improved sampling for image generation", "comments": "Code at\n  https://github.com/AlexiaJM/AdversarialConsistentScoreMatching", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Denoising Score Matching with Annealed Langevin Sampling (DSM-ALS) has\nrecently found success in generative modeling. The approach works by first\ntraining a neural network to estimate the score of a distribution, and then\nusing Langevin dynamics to sample from the data distribution assumed by the\nscore network. Despite the convincing visual quality of samples, this method\nappears to perform worse than Generative Adversarial Networks (GANs) under the\nFr\\'echet Inception Distance, a standard metric for generative models.\n  We show that this apparent gap vanishes when denoising the final Langevin\nsamples using the score network. In addition, we propose two improvements to\nDSM-ALS: 1) Consistent Annealed Sampling as a more stable alternative to\nAnnealed Langevin Sampling, and 2) a hybrid training formulation, composed of\nboth Denoising Score Matching and adversarial objectives. By combining these\ntwo techniques and exploring different network architectures, we elevate score\nmatching methods and obtain results competitive with state-of-the-art image\ngeneration on CIFAR-10.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 14:49:53 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 19:47:12 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Jolicoeur-Martineau", "Alexia", ""], ["Pich\u00e9-Taillefer", "R\u00e9mi", ""], ["Combes", "R\u00e9mi Tachet des", ""], ["Mitliagkas", "Ioannis", ""]]}, {"id": "2009.05489", "submitter": "Yichuan Liu", "authors": "Yichuan Liu, Hailey James, Otkrist Gupta, Dan Raviv", "title": "MRZ code extraction from visa and passport documents using convolutional\n  neural networks", "comments": "This is a preprint of https://doi.org/10.1007/s10032-021-00384-2", "journal-ref": null, "doi": "10.1007/s10032-021-00384-2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and extracting information from Machine-Readable Zone (MRZ) on\npassports and visas is becoming increasingly important for verifying document\nauthenticity. However, computer vision methods for performing similar tasks,\nsuch as optical character recognition (OCR), fail to extract the MRZ given\ndigital images of passports with reasonable accuracy. We present a specially\ndesigned model based on convolutional neural networks that is able to\nsuccessfully extract MRZ information from digital images of passports of\narbitrary orientation and size. Our model achieved 100% MRZ detection rate and\n98.36% character recognition macro-f1 score on a passport and visa dataset.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 15:12:16 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 19:09:32 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Liu", "Yichuan", ""], ["James", "Hailey", ""], ["Gupta", "Otkrist", ""], ["Raviv", "Dan", ""]]}, {"id": "2009.05505", "submitter": "Siyu Huang", "authors": "Siyu Huang, Fangbo Qin, Pengfei Xiong, Ning Ding, Yijia He, Xiao Liu", "title": "TP-LSD: Tri-Points Based Line Segment Detector", "comments": "Accepted by ECCV 2020", "journal-ref": "Europeon Conference on Computer Vision (2020)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel deep convolutional model, Tri-Points Based Line\nSegment Detector (TP-LSD), to detect line segments in an image at real-time\nspeed. The previous related methods typically use the two-step strategy,\nrelying on either heuristic post-process or extra classifier. To realize\none-step detection with a faster and more compact model, we introduce the\ntri-points representation, converting the line segment detection to the\nend-to-end prediction of a root-point and two endpoints for each line segment.\nTP-LSD has two branches: tri-points extraction branch and line segmentation\nbranch. The former predicts the heat map of root-points and the two\ndisplacement maps of endpoints. The latter segments the pixels on straight\nlines out from background. Moreover, the line segmentation map is reused in the\nfirst branch as structural prior. We propose an additional novel evaluation\nmetric and evaluate our method on Wireframe and YorkUrban datasets,\ndemonstrating not only the competitive accuracy compared to the most recent\nmethods, but also the real-time run speed up to 78 FPS with the $320\\times 320$\ninput.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 16:08:12 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Huang", "Siyu", ""], ["Qin", "Fangbo", ""], ["Xiong", "Pengfei", ""], ["Ding", "Ning", ""], ["He", "Yijia", ""], ["Liu", "Xiao", ""]]}, {"id": "2009.05576", "submitter": "Hang Zhang", "authors": "Hang Zhang, Jinwei Zhang, Rongguang Wang, Qihao Zhang, Pascal\n  Spincemaille, Thanh D. Nguyen, and Yi Wang", "title": "Efficient Folded Attention for 3D Medical Image Reconstruction and\n  Segmentation", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, 3D medical image reconstruction (MIR) and segmentation (MIS) based\non deep neural networks have been developed with promising results, and\nattention mechanism has been further designed to capture global contextual\ninformation for performance enhancement. However, the large size of 3D volume\nimages poses a great computational challenge to traditional attention methods.\nIn this paper, we propose a folded attention (FA) approach to improve the\ncomputational efficiency of traditional attention methods on 3D medical images.\nThe main idea is that we apply tensor folding and unfolding operations with\nfour permutations to build four small sub-affinity matrices to approximate the\noriginal affinity matrix. Through four consecutive sub-attention modules of FA,\neach element in the feature tensor can aggregate spatial-channel information\nfrom all other elements. Compared to traditional attention methods, with\nmoderate improvement of accuracy, FA can substantially reduce the computational\ncomplexity and GPU memory consumption. We demonstrate the superiority of our\nmethod on two challenging tasks for 3D MIR and MIS, which are quantitative\nsusceptibility mapping and multiple sclerosis lesion segmentation.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 19:18:04 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Zhang", "Hang", ""], ["Zhang", "Jinwei", ""], ["Wang", "Rongguang", ""], ["Zhang", "Qihao", ""], ["Spincemaille", "Pascal", ""], ["Nguyen", "Thanh D.", ""], ["Wang", "Yi", ""]]}, {"id": "2009.05596", "submitter": "Juan Eugenio Iglesias", "authors": "Henry Tregidgo, Adria Casamitjana, Caitlin Latimer, Mitchell Kilgore,\n  Eleanor Robinson, Emily Blackburn, Koen Van Leemput, Bruce Fischl, Adrian\n  Dalca, Christine Mac Donald, Dirk Keene, Juan Eugenio Iglesias", "title": "3D Reconstruction and Segmentation of Dissection Photographs for\n  MRI-free Neuropathology", "comments": "Accepted at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroimaging to neuropathology correlation (NTNC) promises to enable the\ntransfer of microscopic signatures of pathology to in vivo imaging with MRI,\nultimately enhancing clinical care. NTNC traditionally requires a volumetric\nMRI scan, acquired either ex vivo or a short time prior to death.\nUnfortunately, ex vivo MRI is difficult and costly, and recent premortem scans\nof sufficient quality are seldom available. To bridge this gap, we present\nmethodology to 3D reconstruct and segment full brain image volumes from brain\ndissection photographs, which are routinely acquired at many brain banks and\nneuropathology departments. The 3D reconstruction is achieved via a joint\nregistration framework, which uses a reference volume other than MRI. This\nvolume may represent either the sample at hand (e.g., a surface 3D scan) or the\ngeneral population (a probabilistic atlas). In addition, we present a Bayesian\nmethod to segment the 3D reconstructed photographic volumes into 36\nneuroanatomical structures, which is robust to nonuniform brightness within and\nacross photographs. We evaluate our methods on a dataset with 24 brains, using\nDice scores and volume correlations. The results show that dissection\nphotography is a valid replacement for ex vivo MRI in many volumetric analyses,\nopening an avenue for MRI-free NTNC, including retrospective data. The code is\navailable at https://github.com/htregidgo/DissectionPhotoVolumes.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 18:21:00 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Tregidgo", "Henry", ""], ["Casamitjana", "Adria", ""], ["Latimer", "Caitlin", ""], ["Kilgore", "Mitchell", ""], ["Robinson", "Eleanor", ""], ["Blackburn", "Emily", ""], ["Van Leemput", "Koen", ""], ["Fischl", "Bruce", ""], ["Dalca", "Adrian", ""], ["Mac Donald", "Christine", ""], ["Keene", "Dirk", ""], ["Iglesias", "Juan Eugenio", ""]]}, {"id": "2009.05609", "submitter": "Haomin Chen", "authors": "Haomin Chen, Shun Miao, Daguang Xu, Gregory D. Hager, Adam P. Harrison", "title": "Deep Hiearchical Multi-Label Classification Applied to Chest X-Ray\n  Abnormality Taxonomies", "comments": null, "journal-ref": "MEDIMA 101811, 5 September 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CXRs are a crucial and extraordinarily common diagnostic tool, leading to\nheavy research for CAD solutions. However, both high classification accuracy\nand meaningful model predictions that respect and incorporate clinical\ntaxonomies are crucial for CAD usability. To this end, we present a deep HMLC\napproach for CXR CAD. Different than other hierarchical systems, we show that\nfirst training the network to model conditional probability directly and then\nrefining it with unconditional probabilities is key in boosting performance. In\naddition, we also formulate a numerically stable cross-entropy loss function\nfor unconditional probabilities that provides concrete performance\nimprovements. Finally, we demonstrate that HMLC can be an effective means to\nmanage missing or incomplete labels. To the best of our knowledge, we are the\nfirst to apply HMLC to medical imaging CAD. We extensively evaluate our\napproach on detecting abnormality labels from the CXR arm of the PLCO dataset,\nwhich comprises over $198,000$ manually annotated CXRs. When using complete\nlabels, we report a mean AUC of 0.887, the highest yet reported for this\ndataset. These results are supported by ancillary experiments on the PadChest\ndataset, where we also report significant improvements, 1.2% and 4.1% in AUC\nand AP, respectively over strong \"flat\" classifiers. Finally, we demonstrate\nthat our HMLC approach can much better handle incompletely labelled data. These\nperformance improvements, combined with the inherent usefulness of taxonomic\npredictions, indicate that our approach represents a useful step forward for\nCXR CAD.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 18:50:23 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 15:47:54 GMT"}, {"version": "v3", "created": "Wed, 30 Dec 2020 18:47:52 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Chen", "Haomin", ""], ["Miao", "Shun", ""], ["Xu", "Daguang", ""], ["Hager", "Gregory D.", ""], ["Harrison", "Adam P.", ""]]}, {"id": "2009.05668", "submitter": "Li Yang", "authors": "Li Yang, Zhezhi He, Junshan Zhang, Deliang Fan", "title": "KSM: Fast Multiple Task Adaption via Kernel-wise Soft Mask Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNN) could forget the knowledge about earlier tasks\nwhen learning new tasks, and this is known as \\textit{catastrophic forgetting}.\nWhile recent continual learning methods are capable of alleviating the\ncatastrophic problem on toy-sized datasets, some issues still remain to be\ntackled when applying them in real-world problems. Recently, the fast\nmask-based learning method (e.g. piggyback \\cite{mallya2018piggyback}) is\nproposed to address these issues by learning only a binary element-wise mask in\na fast manner, while keeping the backbone model fixed. However, the binary mask\nhas limited modeling capacity for new tasks. A more recent work\n\\cite{hung2019compacting} proposes a compress-grow-based method (CPG) to\nachieve better accuracy for new tasks by partially training backbone model, but\nwith order-higher training cost, which makes it infeasible to be deployed into\npopular state-of-the-art edge-/mobile-learning. The primary goal of this work\nis to simultaneously achieve fast and high-accuracy multi task adaption in\ncontinual learning setting. Thus motivated, we propose a new training method\ncalled \\textit{kernel-wise Soft Mask} (KSM), which learns a kernel-wise hybrid\nbinary and real-value soft mask for each task, while using the same backbone\nmodel. Such a soft mask can be viewed as a superposition of a binary mask and a\nproperly scaled real-value tensor, which offers a richer representation\ncapability without low-level kernel support to meet the objective of low\nhardware overhead. We validate KSM on multiple benchmark datasets against\nrecent state-of-the-art methods (e.g. Piggyback, Packnet, CPG, etc.), which\nshows good improvement in both accuracy and training cost.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 21:48:39 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Yang", "Li", ""], ["He", "Zhezhi", ""], ["Zhang", "Junshan", ""], ["Fan", "Deliang", ""]]}, {"id": "2009.05671", "submitter": "Nicky Bayat", "authors": "Nicky Bayat, Vahid Reza Khazaie, Yalda Mohsenzadeh", "title": "Inverse mapping of face GANs", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) synthesize realistic images from a\nrandom latent vector. While many studies have explored various training\nconfigurations and architectures for GANs, the problem of inverting a\ngenerative model to extract latent vectors of given input images has been\ninadequately investigated. Although there is exactly one generated image per\ngiven random vector, the mapping from an image to its recovered latent vector\ncan have more than one solution. We train a ResNet architecture to recover a\nlatent vector for a given face that can be used to generate a face nearly\nidentical to the target. We use a perceptual loss to embed face details in the\nrecovered latent vector while maintaining visual quality using a pixel loss.\nThe vast majority of studies on latent vector recovery perform well only on\ngenerated images, we argue that our method can be used to determine a mapping\nbetween real human faces and latent-space vectors that contain most of the\nimportant face style details. In addition, our proposed method projects\ngenerated faces to their latent-space with high fidelity and speed. At last, we\ndemonstrate the performance of our approach on both real and generated faces.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 22:06:56 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Bayat", "Nicky", ""], ["Khazaie", "Vahid Reza", ""], ["Mohsenzadeh", "Yalda", ""]]}, {"id": "2009.05681", "submitter": "Li Yang", "authors": "Li Yang, Zhezhi He, Yu Cao, Deliang Fan", "title": "A Progressive Sub-Network Searching Framework for Dynamic Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many techniques have been developed, such as model compression, to make Deep\nNeural Networks (DNNs) inference more efficiently. Nevertheless, DNNs still\nlack excellent run-time dynamic inference capability to enable users trade-off\naccuracy and computation complexity (i.e., latency on target hardware) after\nmodel deployment, based on dynamic requirements and environments. Such research\ndirection recently draws great attention, where one realization is to train the\ntarget DNN through a multiple-term objective function, which consists of\ncross-entropy terms from multiple sub-nets. Our investigation in this work show\nthat the performance of dynamic inference highly relies on the quality of\nsub-net sampling. With objective to construct a dynamic DNN and search multiple\nhigh quality sub-nets with minimal searching cost, we propose a progressive\nsub-net searching framework, which is embedded with several effective\ntechniques, including trainable noise ranking, channel group and fine-tuning\nthreshold setting, sub-nets re-selection. The proposed framework empowers the\ntarget DNN with better dynamic inference capability, which outperforms prior\nworks on both CIFAR-10 and ImageNet dataset via comprehensive experiments on\ndifferent network structures. Taken ResNet18 as an example, our proposed method\nachieves much better dynamic inference accuracy compared with prior popular\nUniversally-Slimmable-Network by 4.4%-maximally and 2.3%-averagely in ImageNet\ndataset with the same model size.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 22:56:02 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Yang", "Li", ""], ["He", "Zhezhi", ""], ["Cao", "Yu", ""], ["Fan", "Deliang", ""]]}, {"id": "2009.05684", "submitter": "Vivek Mittal", "authors": "Vivek Mittal", "title": "AttnGrounder: Talking to Cars with Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose Attention Grounder (AttnGrounder), a single-stage end-to-end\ntrainable model for the task of visual grounding. Visual grounding aims to\nlocalize a specific object in an image based on a given natural language text\nquery. Unlike previous methods that use the same text representation for every\nimage region, we use a visual-text attention module that relates each word in\nthe given query with every region in the corresponding image for constructing a\nregion dependent text representation. Furthermore, for improving the\nlocalization ability of our model, we use our visual-text attention module to\ngenerate an attention mask around the referred object. The attention mask is\ntrained as an auxiliary task using a rectangular mask generated with the\nprovided ground-truth coordinates. We evaluate AttnGrounder on the Talk2Car\ndataset and show an improvement of 3.26% over the existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 23:18:55 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 10:00:22 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Mittal", "Vivek", ""]]}, {"id": "2009.05695", "submitter": "Niluthpol Chowdhury Mithun", "authors": "Niluthpol Chowdhury Mithun, Karan Sikka, Han-Pang Chiu, Supun\n  Samarasekera, Rakesh Kumar", "title": "RGB2LIDAR: Towards Solving Large-Scale Cross-Modal Visual Localization", "comments": "ACM Multimedia 2020", "journal-ref": null, "doi": "10.1145/3394171.3413647", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an important, yet largely unexplored problem of large-scale\ncross-modal visual localization by matching ground RGB images to a\ngeo-referenced aerial LIDAR 3D point cloud (rendered as depth images). Prior\nworks were demonstrated on small datasets and did not lend themselves to\nscaling up for large-scale applications. To enable large-scale evaluation, we\nintroduce a new dataset containing over 550K pairs (covering 143 km^2 area) of\nRGB and aerial LIDAR depth images. We propose a novel joint embedding based\nmethod that effectively combines the appearance and semantic cues from both\nmodalities to handle drastic cross-modal variations. Experiments on the\nproposed dataset show that our model achieves a strong result of a median rank\nof 5 in matching across a large test set of 50K location pairs collected from a\n14km^2 area. This represents a significant advancement over prior works in\nperformance and scale. We conclude with qualitative results to highlight the\nchallenging nature of this task and the benefits of the proposed model. Our\nwork provides a foundation for further research in cross-modal visual\nlocalization.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 01:18:45 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Mithun", "Niluthpol Chowdhury", ""], ["Sikka", "Karan", ""], ["Chiu", "Han-Pang", ""], ["Samarasekera", "Supun", ""], ["Kumar", "Rakesh", ""]]}, {"id": "2009.05697", "submitter": "Yuxuan Cai", "authors": "Yuxuan Cai, Hongjia Li, Geng Yuan, Wei Niu, Yanyu Li, Xulong Tang, Bin\n  Ren, Yanzhi Wang", "title": "YOLObile: Real-Time Object Detection on Mobile Devices via\n  Compression-Compilation Co-Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development and wide utilization of object detection techniques\nhave aroused attention on both accuracy and speed of object detectors. However,\nthe current state-of-the-art object detection works are either\naccuracy-oriented using a large model but leading to high latency or\nspeed-oriented using a lightweight model but sacrificing accuracy. In this\nwork, we propose YOLObile framework, a real-time object detection on mobile\ndevices via compression-compilation co-design. A novel block-punched pruning\nscheme is proposed for any kernel size. To improve computational efficiency on\nmobile devices, a GPU-CPU collaborative scheme is adopted along with advanced\ncompiler-assisted optimizations. Experimental results indicate that our pruning\nscheme achieves 14$\\times$ compression rate of YOLOv4 with 49.0 mAP. Under our\nYOLObile framework, we achieve 17 FPS inference speed using GPU on Samsung\nGalaxy S20. By incorporating our proposed GPU-CPU collaborative scheme, the\ninference speed is increased to 19.1 FPS, and outperforms the original YOLOv4\nby 5$\\times$ speedup. Source code is at:\n\\url{https://github.com/nightsnack/YOLObile}.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 01:41:08 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 15:55:43 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Cai", "Yuxuan", ""], ["Li", "Hongjia", ""], ["Yuan", "Geng", ""], ["Niu", "Wei", ""], ["Li", "Yanyu", ""], ["Tang", "Xulong", ""], ["Ren", "Bin", ""], ["Wang", "Yanzhi", ""]]}, {"id": "2009.05721", "submitter": "Ang Li", "authors": "Ang Li, Shanshan Zhao, Xingjun Ma, Mingming Gong, Jianzhong Qi, Rui\n  Zhang, Dacheng Tao, Ramamohanarao Kotagiri", "title": "Short-Term and Long-Term Context Aggregation Network for Video\n  Inpainting", "comments": "Accepted by ECCV 2020 as a spotlight paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video inpainting aims to restore missing regions of a video and has many\napplications such as video editing and object removal. However, existing\nmethods either suffer from inaccurate short-term context aggregation or rarely\nexplore long-term frame information. In this work, we present a novel context\naggregation network to effectively exploit both short-term and long-term frame\ninformation for video inpainting. In the encoding stage, we propose\nboundary-aware short-term context aggregation, which aligns and aggregates,\nfrom neighbor frames, local regions that are closely related to the boundary\ncontext of missing regions into the target frame. Furthermore, we propose\ndynamic long-term context aggregation to globally refine the feature map\ngenerated in the encoding stage using long-term frame features, which are\ndynamically updated throughout the inpainting process. Experiments show that it\noutperforms state-of-the-art methods with better inpainting results and fast\ninpainting speed.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 03:50:56 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Li", "Ang", ""], ["Zhao", "Shanshan", ""], ["Ma", "Xingjun", ""], ["Gong", "Mingming", ""], ["Qi", "Jianzhong", ""], ["Zhang", "Rui", ""], ["Tao", "Dacheng", ""], ["Kotagiri", "Ramamohanarao", ""]]}, {"id": "2009.05722", "submitter": "Zhang Yunlong", "authors": "Zhang Yunlong, Li Chenxin, Lin Xin, Sun Liyan, Zhuang Yihong, Huang\n  Yue, Ding Xinghao, Liu Xiaoqing, Yu Yizhou", "title": "Generator Versus Segmentor: Pseudo-healthy Synthesis", "comments": "Accepted by MICCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of pseudo-healthy synthesis that is\ndefined as synthesizing a subject-specific pathology-free image from a\npathological one. Recent approaches based on Generative Adversarial Network\n(GAN) have been developed for this task. However, these methods will inevitably\nfall into the trade-off between preserving the subject-specific identity and\ngenerating healthy-like appearances. To overcome this challenge, we propose a\nnovel adversarial training regime, Generator versus Segmentor (GVS), to\nalleviate this trade-off by a divide-and-conquer strategy. We further consider\nthe deteriorating generalization performance of the segmentor throughout the\ntraining and develop a pixel-wise weighted loss by muting the well-transformed\npixels to promote it. Moreover, we propose a new metric to measure how healthy\nthe synthetic images look. The qualitative and quantitative experiments on the\npublic dataset BraTS demonstrate that the proposed method outperforms the\nexisting methods. Besides, we also certify the effectiveness of our method on\ndatasets LiTS. Our implementation and pre-trained networks are publicly\navailable at https://github.com/Au3C2/Generator-Versus-Segmentor.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 03:54:22 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 02:26:06 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 13:59:39 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Yunlong", "Zhang", ""], ["Chenxin", "Li", ""], ["Xin", "Lin", ""], ["Liyan", "Sun", ""], ["Yihong", "Zhuang", ""], ["Yue", "Huang", ""], ["Xinghao", "Ding", ""], ["Xiaoqing", "Liu", ""], ["Yizhou", "Yu", ""]]}, {"id": "2009.05728", "submitter": "Shreeshiv Laxmichand Patel", "authors": "Shreeshiv Patel, Dvijesh Bhatt", "title": "Abstractive Information Extraction from Scanned Invoices (AIESI) using\n  End-to-end Sequential Approach", "comments": "6 pages, 7 images, to be published in upcoming relevant conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent proliferation in the field of Machine Learning and Deep Learning\nallows us to generate OCR models with higher accuracy. Optical Character\nRecognition(OCR) is the process of extracting text from documents and scanned\nimages. For document data streamlining, we are interested in data like, Payee\nname, total amount, address, and etc. Extracted information helps to get\ncomplete insight of data, which can be helpful for fast document searching,\nefficient indexing in databases, data analytics, and etc. Using AIESI we can\neliminate human effort for key parameters extraction from scanned documents.\nAbstract Information Extraction from Scanned Invoices (AIESI) is a process of\nextracting information like, date, total amount, payee name, and etc from\nscanned receipts. In this paper we proposed an improved method to ensemble all\nvisual and textual features from invoices to extract key invoice parameters\nusing Word wise BiLSTM.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 05:14:28 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Patel", "Shreeshiv", ""], ["Bhatt", "Dvijesh", ""]]}, {"id": "2009.05738", "submitter": "Tim De Jong", "authors": "Tim De Jong (Statistics Netherlands), Stefano Bromuri (Open\n  Universiteit Nederland), Xi Chang (Open Universiteit Nederland), Marc\n  Debusschere (Statbel), Natalie Rosenski (Destatis), Clara Schartner\n  (Destatis), Katharina Strauch (IT.NRW), Marion Boehmer (IT.NRW), Lyana Curier\n  (Statistics Netherlands)", "title": "Monitoring Spatial Sustainable Development: semi-automated analysis of\n  Satellite and Aerial Images for Energy Transition and Sustainability\n  Indicators", "comments": "81 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report presents the results of the DeepSolaris project that was carried\nout under the ESS action 'Merging Geostatistics and Geospatial Information in\nMember States'. During the project several deep learning algorithms were\nevaluated to detect solar panels in remote sensing data. The aim of the project\nwas to evaluate whether deep learning models could be developed, that worked\nacross different member states in the European Union. Two remote sensing data\nsources were considered: aerial images on the one hand, and satellite images on\nthe other. Two flavours of deep learning models were evaluated: classification\nmodels and object detection models. For the evaluation of the deep learning\nmodels we used a cross-site evaluation approach: the deep learning models where\ntrained in one geographical area and then evaluated on a different geographical\narea, previously unseen by the algorithm. The cross-site evaluation was\nfurthermore carried out twice: deep learning models trained on he Netherlands\nwere evaluated on Germany and vice versa. While the deep learning models were\nable to detect solar panels successfully, false detection remained a problem.\nMoreover, model performance decreased dramatically when evaluated in a\ncross-border fashion. Hence, training a model that performs reliably across\ndifferent countries in the European Union is a challenging task. That being\nsaid, the models detected quite a share of solar panels not present in current\nsolar panel registers and therefore can already be used as-is to help reduced\nmanual labor in checking these registers.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 07:09:59 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["De Jong", "Tim", "", "Statistics Netherlands"], ["Bromuri", "Stefano", "", "Open\n  Universiteit Nederland"], ["Chang", "Xi", "", "Open Universiteit Nederland"], ["Debusschere", "Marc", "", "Statbel"], ["Rosenski", "Natalie", "", "Destatis"], ["Schartner", "Clara", "", "Destatis"], ["Strauch", "Katharina", "", "IT.NRW"], ["Boehmer", "Marion", "", "IT.NRW"], ["Curier", "Lyana", "", "Statistics Netherlands"]]}, {"id": "2009.05743", "submitter": "Chaojie Ji", "authors": "Chaojie Ji, Hongwei Chen, Ruxin Wang, Yunpeng Cai, Hongyan Wu", "title": "Smoothness Sensor: Adaptive Smoothness-Transition Graph Convolutions for\n  Attributed Graph Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering techniques attempt to group objects with similar properties into a\ncluster. Clustering the nodes of an attributed graph, in which each node is\nassociated with a set of feature attributes, has attracted significant\nattention. Graph convolutional networks (GCNs) represent an effective approach\nfor integrating the two complementary factors of node attributes and structural\ninformation for attributed graph clustering. However, oversmoothing of GCNs\nproduces indistinguishable representations of nodes, such that the nodes in a\ngraph tend to be grouped into fewer clusters, and poses a challenge due to the\nresulting performance drop. In this study, we propose a smoothness sensor for\nattributed graph clustering based on adaptive smoothness-transition graph\nconvolutions, which senses the smoothness of a graph and adaptively terminates\nthe current convolution once the smoothness is saturated to prevent\noversmoothing. Furthermore, as an alternative to graph-level smoothness, a\nnovel fine-gained node-wise level assessment of smoothness is proposed, in\nwhich smoothness is computed in accordance with the neighborhood conditions of\na given node at a certain order of graph convolution. In addition, a\nself-supervision criterion is designed considering both the tightness within\nclusters and the separation between clusters to guide the whole neural network\ntraining process. Experiments show that the proposed methods significantly\noutperform 12 other state-of-the-art baselines in terms of three different\nmetrics across four benchmark datasets. In addition, an extensive study reveals\nthe reasons for their effectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 08:12:27 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Ji", "Chaojie", ""], ["Chen", "Hongwei", ""], ["Wang", "Ruxin", ""], ["Cai", "Yunpeng", ""], ["Wu", "Hongyan", ""]]}, {"id": "2009.05750", "submitter": "Alberto Pretto", "authors": "Mulham Fawakherji, Ciro Potena, Alberto Pretto, Domenico D. Bloisi,\n  Daniele Nardi", "title": "Multi-Spectral Image Synthesis for Crop/Weed Segmentation in Precision\n  Farming", "comments": "Submitted to Robotics and Autonomous Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An effective perception system is a fundamental component for farming robots,\nas it enables them to properly perceive the surrounding environment and to\ncarry out targeted operations. The most recent approaches make use of\nstate-of-the-art machine learning techniques to learn an effective model for\nthe target task. However, those methods need a large amount of labelled data\nfor training. A recent approach to deal with this issue is data augmentation\nthrough Generative Adversarial Networks (GANs), where entire synthetic scenes\nare added to the training data, thus enlarging and diversifying their\ninformative content. In this work, we propose an alternative solution with\nrespect to the common data augmentation techniques, applying it to the\nfundamental problem of crop/weed segmentation in precision farming. Starting\nfrom real images, we create semi-artificial samples by replacing the most\nrelevant object classes (i.e., crop and weeds) with their synthesized\ncounterparts. To do that, we employ a conditional GAN (cGAN), where the\ngenerative model is trained by conditioning the shape of the generated object.\nMoreover, in addition to RGB data, we take into account also near-infrared\n(NIR) information, generating four channel multi-spectral synthetic images.\nQuantitative experiments, carried out on three publicly available datasets,\nshow that (i) our model is capable of generating realistic multi-spectral\nimages of plants and (ii) the usage of such synthetic images in the training\nprocess improves the segmentation performance of state-of-the-art semantic\nsegmentation Convolutional Networks.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 08:49:36 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Fawakherji", "Mulham", ""], ["Potena", "Ciro", ""], ["Pretto", "Alberto", ""], ["Bloisi", "Domenico D.", ""], ["Nardi", "Daniele", ""]]}, {"id": "2009.05752", "submitter": "Hazrat Ali", "authors": "Faizan Munawar, Shoaib Azmat, Talha Iqbal, Christer Gr\\\"onlund, Hazrat\n  Ali", "title": "Segmentation of Lungs in Chest X-Ray Image Using Generative Adversarial\n  Networks", "comments": "Volume 8, August 2020, Pages 153535 - 153545", "journal-ref": "in IEEE Access, vol. 8, pp. 153535-153545, 2020", "doi": "10.1109/ACCESS.2020.3017915", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chest X-ray (CXR) is a low-cost medical imaging technique. It is a common\nprocedure for the identification of many respiratory diseases compared to MRI,\nCT, and PET scans. This paper presents the use of generative adversarial\nnetworks (GAN) to perform the task of lung segmentation on a given CXR. GANs\nare popular to generate realistic data by learning the mapping from one domain\nto another. In our work, the generator of the GAN is trained to generate a\nsegmented mask of a given input CXR. The discriminator distinguishes between a\nground truth and the generated mask, and updates the generator through the\nadversarial loss measure. The objective is to generate masks for the input CXR,\nwhich are as realistic as possible compared to the ground truth masks. The\nmodel is trained and evaluated using four different discriminators referred to\nas D1, D2, D3, and D4, respectively. Experimental results on three different\nCXR datasets reveal that the proposed model is able to achieve a dice-score of\n0.9740, and IOU score of 0.943, which are better than other reported\nstate-of-the art results.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 08:54:54 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Munawar", "Faizan", ""], ["Azmat", "Shoaib", ""], ["Iqbal", "Talha", ""], ["Gr\u00f6nlund", "Christer", ""], ["Ali", "Hazrat", ""]]}, {"id": "2009.05757", "submitter": "Jinpeng Wang", "authors": "Jinpeng Wang, Yuting Gao, Ke Li, Jianguo Hu, Xinyang Jiang, Xiaowei\n  Guo, Rongrong Ji, Xing Sun", "title": "Enhancing Unsupervised Video Representation Learning by Decoupling the\n  Scene and the Motion", "comments": "AAAI2021 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One significant factor we expect the video representation learning to\ncapture, especially in contrast with the image representation learning, is the\nobject motion. However, we found that in the current mainstream video datasets,\nsome action categories are highly related with the scene where the action\nhappens, making the model tend to degrade to a solution where only the scene\ninformation is encoded. For example, a trained model may predict a video as\nplaying football simply because it sees the field, neglecting that the subject\nis dancing as a cheerleader on the field. This is against our original\nintention towards the video representation learning and may bring scene bias on\ndifferent dataset that can not be ignored. In order to tackle this problem, we\npropose to decouple the scene and the motion (DSM) with two simple operations,\nso that the model attention towards the motion information is better paid.\nSpecifically, we construct a positive clip and a negative clip for each video.\nCompared to the original video, the positive/negative is\nmotion-untouched/broken but scene-broken/untouched by Spatial Local Disturbance\nand Temporal Local Disturbance. Our objective is to pull the positive closer\nwhile pushing the negative farther to the original clip in the latent space. In\nthis way, the impact of the scene is weakened while the temporal sensitivity of\nthe network is further enhanced. We conduct experiments on two tasks with\nvarious backbones and different pre-training datasets, and find that our method\nsurpass the SOTA methods with a remarkable 8.1% and 8.8% improvement towards\naction recognition task on the UCF101 and HMDB51 datasets respectively using\nthe same backbone.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 09:54:11 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 14:32:54 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 10:45:17 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Wang", "Jinpeng", ""], ["Gao", "Yuting", ""], ["Li", "Ke", ""], ["Hu", "Jianguo", ""], ["Jiang", "Xinyang", ""], ["Guo", "Xiaowei", ""], ["Ji", "Rongrong", ""], ["Sun", "Xing", ""]]}, {"id": "2009.05769", "submitter": "Jinpeng Wang", "authors": "Jinpeng Wang, Yuting Gao, Ke Li, Yiqi Lin, Andy J. Ma, Hao Cheng, Pai\n  Peng, Feiyue Huang, Rongrong Ji, Xing Sun", "title": "Removing the Background by Adding the Background: Towards Background\n  Robust Self-supervised Video Representation Learning", "comments": "CVPR2021 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning has shown great potentials in improving the video\nrepresentation ability of deep neural networks by getting supervision from the\ndata itself. However, some of the current methods tend to cheat from the\nbackground, i.e., the prediction is highly dependent on the video background\ninstead of the motion, making the model vulnerable to background changes. To\nmitigate the model reliance towards the background, we propose to remove the\nbackground impact by adding the background. That is, given a video, we randomly\nselect a static frame and add it to every other frames to construct a\ndistracting video sample. Then we force the model to pull the feature of the\ndistracting video and the feature of the original video closer, so that the\nmodel is explicitly restricted to resist the background influence, focusing\nmore on the motion changes. We term our method as \\emph{Background Erasing}\n(BE). It is worth noting that the implementation of our method is so simple and\nneat and can be added to most of the SOTA methods without much efforts.\nSpecifically, BE brings 16.4% and 19.1% improvements with MoCo on the severely\nbiased datasets UCF101 and HMDB51, and 14.5% improvement on the less biased\ndataset Diving48.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 11:25:13 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 16:42:53 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 11:52:16 GMT"}, {"version": "v4", "created": "Thu, 22 Apr 2021 03:37:30 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Wang", "Jinpeng", ""], ["Gao", "Yuting", ""], ["Li", "Ke", ""], ["Lin", "Yiqi", ""], ["Ma", "Andy J.", ""], ["Cheng", "Hao", ""], ["Peng", "Pai", ""], ["Huang", "Feiyue", ""], ["Ji", "Rongrong", ""], ["Sun", "Xing", ""]]}, {"id": "2009.05778", "submitter": "Thyagharajan K K", "authors": "S. D. Lalitha, K. K. Thyagharajan", "title": "Micro-Facial Expression Recognition Based on Deep-Rooted Learning\n  Algorithm", "comments": "20 pages, 7 figures, \"for the published version of the article, see\n  https://www.atlantis-press.com/journals/ijcis/125915627\"", "journal-ref": "12 (2) 903 - 913 2019/08 International Journal of Computational\n  Intelligence Systems", "doi": "10.2991/IJCIS.D.190801.001", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expressions are important cues to observe human emotions. Facial\nexpression recognition has attracted many researchers for years, but it is\nstill a challenging topic since expression features vary greatly with the head\nposes, environments, and variations in the different persons involved. In this\nwork, three major steps are involved to improve the performance of micro-facial\nexpression recognition. First, an Adaptive Homomorphic Filtering is used for\nface detection and rotation rectification processes. Secondly, Micro-facial\nfeatures were used to extract the appearance variations of a testing\nimage-spatial analysis. The features of motion information are used for\nexpression recognition in a sequence of facial images. An effective\nMicro-Facial Expression Based Deep-Rooted Learning (MFEDRL) classifier is\nproposed in this paper to better recognize spontaneous micro-expressions by\nlearning parameters on the optimal features. This proposed method includes two\nloss functions such as cross entropy loss function and centre loss function.\nThen the performance of the algorithm will be evaluated using recognition rate\nand false measures. Simulation results show that the predictive performance of\nthe proposed method outperforms that of the existing classifiers such as\nConvolutional Neural Network (CNN), Deep Neural Network (DNN), Artificial\nNeural Network (ANN), Support Vector Machine (SVM), and k-Nearest Neighbours\n(KNN) in terms of accuracy and Mean Absolute Error (MAE).\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 12:23:27 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Lalitha", "S. D.", ""], ["Thyagharajan", "K. K.", ""]]}, {"id": "2009.05792", "submitter": "Fotios Logothetis Dr", "authors": "Fotios Logothetis, Ignas Budvytis, Roberto Mecca, Roberto Cipolla", "title": "A CNN Based Approach for the Near-Field Photometric Stereo Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing the 3D shape of an object using several images under different\nlight sources is a very challenging task, especially when realistic assumptions\nsuch as light propagation and attenuation, perspective viewing geometry and\nspecular light reflection are considered. Many of works tackling Photometric\nStereo (PS) problems often relax most of the aforementioned assumptions.\nEspecially they ignore specular reflection and global illumination effects. In\nthis work, we propose the first CNN based approach capable of handling these\nrealistic assumptions in Photometric Stereo. We leverage recent improvements of\ndeep neural networks for far-field Photometric Stereo and adapt them to near\nfield setup. We achieve this by employing an iterative procedure for shape\nestimation which has two main steps. Firstly we train a per-pixel CNN to\npredict surface normals from reflectance samples. Secondly, we compute the\ndepth by integrating the normal field in order to iteratively estimate light\ndirections and attenuation which is used to compensate the input images to\ncompute reflectance samples for the next iteration. To the best of our\nknowledge this is the first near-field framework which is able to accurately\npredict 3D shape from highly specular objects. Our method outperforms competing\nstate-of-the-art near-field Photometric Stereo approaches on both synthetic and\nreal experiments.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 13:28:28 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Logothetis", "Fotios", ""], ["Budvytis", "Ignas", ""], ["Mecca", "Roberto", ""], ["Cipolla", "Roberto", ""]]}, {"id": "2009.05796", "submitter": "John Lim", "authors": "John Lim, True Price, Fabian Monrose, Jan-Michael Frahm", "title": "Revisiting the Threat Space for Vision-based Keystroke Inference Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vision-based keystroke inference attack is a side-channel attack in which\nan attacker uses an optical device to record users on their mobile devices and\ninfer their keystrokes. The threat space for these attacks has been studied in\nthe past, but we argue that the defining characteristics for this threat space,\nnamely the strength of the attacker, are outdated. Previous works do not study\nadversaries with vision systems that have been trained with deep neural\nnetworks because these models require large amounts of training data and\ncurating such a dataset is expensive. To address this, we create a large-scale\nsynthetic dataset to simulate the attack scenario for a keystroke inference\nattack. We show that first pre-training on synthetic data, followed by adopting\ntransfer learning techniques on real-life data, increases the performance of\nour deep learning models. This indicates that these models are able to learn\nrich, meaningful representations from our synthetic data and that training on\nthe synthetic data can help overcome the issue of having small, real-life\ndatasets for vision-based key stroke inference attacks. For this work, we focus\non single keypress classification where the input is a frame of a keypress and\nthe output is a predicted key. We are able to get an accuracy of 95.6% after\npre-training a CNN on our synthetic data and training on a small set of\nreal-life data in an adversarial domain adaptation framework. Source Code for\nSimulator:\nhttps://github.com/jlim13/keystroke-inference-attack-synthetic-dataset-generator-\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 14:03:55 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Lim", "John", ""], ["Price", "True", ""], ["Monrose", "Fabian", ""], ["Frahm", "Jan-Michael", ""]]}, {"id": "2009.05812", "submitter": "Ashutosh Kumar Tiwari", "authors": "Ashutosh Tiwari and Sandeep Varma", "title": "Learning semantic Image attributes using Image recognition and knowledge\n  graph embeddings", "comments": "7 Pages, 6 figures, Accepted at Future Technologies Conference (FTC)\n  2020, Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting structured knowledge from texts has traditionally been used for\nknowledge base generation. However, other sources of information, such as\nimages can be leveraged into this process to build more complete and richer\nknowledge bases. Structured semantic representation of the content of an image\nand knowledge graph embeddings can provide a unique representation of semantic\nrelationships between image entities. Linking known entities in knowledge\ngraphs and learning open-world images using language models has attracted lots\nof interest over the years. In this paper, we propose a shared learning\napproach to learn semantic attributes of images by combining a knowledge graph\nembedding model with the recognized attributes of images. The proposed model\npremises to help us understand the semantic relationship between the entities\nof an image and implicitly provide a link for the extracted entities through a\nknowledge graph embedding model. Under the limitation of using a custom\nuser-defined knowledge base with limited data, the proposed model presents\nsignificant accuracy and provides a new alternative to the earlier approaches.\nThe proposed approach is a step towards bridging the gap between frameworks\nwhich learn from large amounts of data and frameworks which use a limited set\nof predicates to infer new knowledge.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 15:18:48 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Tiwari", "Ashutosh", ""], ["Varma", "Sandeep", ""]]}, {"id": "2009.05814", "submitter": "Lukas Kiefer", "authors": "Lukas Kiefer, Stefania Petra, Martin Storath, Andreas Weinmann", "title": "Multi-Channel Potts-Based Reconstruction for Multi-Spectral Computed\n  Tomography", "comments": "37 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider reconstructing multi-channel images from measurements performed\nby photon-counting and energy-discriminating detectors in the setting of\nmulti-spectral X-ray computed tomography (CT). Our aim is to exploit the strong\nstructural correlation that is known to exist between the channels of\nmulti-spectral CT images. To that end, we adopt the multi-channel Potts prior\nto jointly reconstruct all channels. This prior produces piecewise constant\nsolutions with strongly correlated channels. In particular, edges are enforced\nto have the same spatial position across channels which is a benefit over\nTV-based methods. We consider the Potts prior in two frameworks: (a) in the\ncontext of a variational Potts model, and (b) in a Potts-superiorization\napproach that perturbs the iterates of a basic iterative least squares solver.\nWe identify an alternating direction method of multipliers (ADMM) approach as\nwell as a Potts-superiorized conjugate gradient method as particularly\nsuitable. In numerical experiments, we compare the Potts prior based approaches\nto existing TV-type approaches on realistically simulated multi-spectral CT\ndata and obtain improved reconstruction for compound solid bodies.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 15:33:47 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 09:31:27 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Kiefer", "Lukas", ""], ["Petra", "Stefania", ""], ["Storath", "Martin", ""], ["Weinmann", "Andreas", ""]]}, {"id": "2009.05819", "submitter": "Andrey Bokovoy", "authors": "Andrey Bokovoy, Kirill Muraviev and Konstantin Yakovlev", "title": "Map-merging Algorithms for Visual SLAM: Feasibility Study and Empirical\n  Evaluation", "comments": "Camera-ready version as submitted to RCAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous localization and mapping, especially the one relying solely on\nvideo data (vSLAM), is a challenging problem that has been extensively studied\nin robotics and computer vision. State-of-the-art vSLAM algorithms are capable\nof constructing accurate-enough maps that enable a mobile robot to autonomously\nnavigate an unknown environment. In this work, we are interested in an\nimportant problem related to vSLAM, i.e. map merging, that might appear in\nvarious practically important scenarios, e.g. in a multi-robot coverage\nscenario. This problem asks whether different vSLAM maps can be merged into a\nconsistent single representation. We examine the existing 2D and 3D map-merging\nalgorithms and conduct an extensive empirical evaluation in realistic simulated\nenvironment (Habitat). Both qualitative and quantitative comparison is carried\nout and the obtained results are reported and analyzed.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 16:15:16 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Bokovoy", "Andrey", ""], ["Muraviev", "Kirill", ""], ["Yakovlev", "Konstantin", ""]]}, {"id": "2009.05834", "submitter": "Shuyang Sun", "authors": "Yi Zhou, Shuyang Sun, Chao Zhang, Yikang Li, Wanli Ouyang", "title": "Exploring the Hierarchy in Relation Labels for Scene Graph Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By assigning each relationship a single label, current approaches formulate\nthe relationship detection as a classification problem. Under this formulation,\npredicate categories are treated as completely different classes. However,\ndifferent from the object labels where different classes have explicit\nboundaries, predicates usually have overlaps in their semantic meanings. For\nexample, sit\\_on and stand\\_on have common meanings in vertical relationships\nbut different details of how these two objects are vertically placed. In order\nto leverage the inherent structures of the predicate categories, we propose to\nfirst build the language hierarchy and then utilize the Hierarchy Guided\nFeature Learning (HGFL) strategy to learn better region features of both the\ncoarse-grained level and the fine-grained level. Besides, we also propose the\nHierarchy Guided Module (HGM) to utilize the coarse-grained level to guide the\nlearning of fine-grained level features. Experiments show that the proposed\nsimple yet effective method can improve several state-of-the-art baselines by a\nlarge margin (up to $33\\%$ relative gain) in terms of Recall@50 on the task of\nScene Graph Generation in different datasets.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 17:36:53 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Zhou", "Yi", ""], ["Sun", "Shuyang", ""], ["Zhang", "Chao", ""], ["Li", "Yikang", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2009.05835", "submitter": "Alexander Wong", "authors": "Alexander Wong, Xiao Yu Wang, and Andrew Hryniowski", "title": "How Much Can We Really Trust You? Towards Simple, Interpretable Trust\n  Quantification Metrics for Deep Neural Networks", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A critical step to building trustworthy deep neural networks is trust\nquantification, where we ask the question: How much can we trust a deep neural\nnetwork? In this study, we take a step towards simple, interpretable metrics\nfor trust quantification by introducing a suite of metrics for assessing the\noverall trustworthiness of deep neural networks based on their behaviour when\nanswering a set of questions. We conduct a thought experiment and explore two\nkey questions about trust in relation to confidence: 1) How much trust do we\nhave in actors who give wrong answers with great confidence? and 2) How much\ntrust do we have in actors who give right answers hesitantly? Based on insights\ngained, we introduce the concept of question-answer trust to quantify\ntrustworthiness of an individual answer based on confident behaviour under\ncorrect and incorrect answer scenarios, and the concept of trust density to\ncharacterize the distribution of overall trust for an individual answer\nscenario. We further introduce the concept of trust spectrum for representing\noverall trust with respect to the spectrum of possible answer scenarios across\ncorrectly and incorrectly answered questions. Finally, we introduce\nNetTrustScore, a scalar metric summarizing overall trustworthiness. The suite\nof metrics aligns with past social psychology studies that study the\nrelationship between trust and confidence. Leveraging these metrics, we\nquantify the trustworthiness of several well-known deep neural network\narchitectures for image recognition to get a deeper understanding of where\ntrust breaks down. The proposed metrics are by no means perfect, but the hope\nis to push the conversation towards better metrics to help guide practitioners\nand regulators in producing, deploying, and certifying deep learning solutions\nthat can be trusted to operate in real-world, mission-critical scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 17:37:36 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 02:45:36 GMT"}, {"version": "v3", "created": "Sat, 3 Apr 2021 15:08:50 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wong", "Alexander", ""], ["Wang", "Xiao Yu", ""], ["Hryniowski", "Andrew", ""]]}, {"id": "2009.05871", "submitter": "Eran Dahan", "authors": "Eran Dahan and Yosi Keller", "title": "A Unified Approach to Kinship Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a deep learning-based approach for kin verification\nusing a unified multi-task learning scheme where all kinship classes are\njointly learned. This allows us to better utilize small training sets that are\ntypical of kin verification. We introduce a novel approach for fusing the\nembeddings of kin images, to avoid overfitting, which is a common issue in\ntraining such networks. An adaptive sampling scheme is derived for the training\nset images to resolve the inherent imbalance in kin verification datasets. A\nthorough ablation study exemplifies the effectivity of our approach, which is\nexperimentally shown to outperform contemporary state-of-the-art kin\nverification results when applied to the Families In the Wild, FG2018, and\nFG2020 datasets.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 22:13:56 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Dahan", "Eran", ""], ["Keller", "Yosi", ""]]}, {"id": "2009.05880", "submitter": "Abolfazl Zargari", "authors": "Abolfazl Zargari Khuzani, Najmeh Mashhadi, Morteza Heidari, Donya\n  Khaledyan", "title": "An approach to human iris recognition using quantitative analysis of\n  image features and machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Iris pattern is a unique biological feature for each individual, making\nit a valuable and powerful tool for human identification. In this paper, an\nefficient framework for iris recognition is proposed in four steps. (1) Iris\nsegmentation (using a relative total variation combined with Coarse Iris\nLocalization), (2) feature extraction (using Shape&density, FFT, GLCM, GLDM,\nand Wavelet), (3) feature reduction (employing Kernel-PCA) and (4)\nclassification (applying multi-layer neural network) to classify 2000 iris\nimages of CASIA-Iris-Interval dataset obtained from 200 volunteers. The results\nconfirm that the proposed scheme can provide a reliable prediction with an\naccuracy of up to 99.64%.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 23:23:33 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Khuzani", "Abolfazl Zargari", ""], ["Mashhadi", "Najmeh", ""], ["Heidari", "Morteza", ""], ["Khaledyan", "Donya", ""]]}, {"id": "2009.05907", "submitter": "Yucheng Hang", "authors": "Yucheng Hang, Qingmin Liao, Wenming Yang, Yupeng Chen, Jie Zhou", "title": "Attention Cube Network for Image Restoration", "comments": "Accepted by the 28th ACM International Conference on Multimedia (ACM\n  MM 2020); Code is available at https://github.com/YCHang686/A-CubeNet", "journal-ref": null, "doi": "10.1145/3394171.3413564", "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep convolutional neural network (CNN) have been widely used in\nimage restoration and obtained great success. However, most of existing methods\nare limited to local receptive field and equal treatment of different types of\ninformation. Besides, existing methods always use a multi-supervised method to\naggregate different feature maps, which can not effectively aggregate\nhierarchical feature information. To address these issues, we propose an\nattention cube network (A-CubeNet) for image restoration for more powerful\nfeature expression and feature correlation learning. Specifically, we design a\nnovel attention mechanism from three dimensions, namely spatial dimension,\nchannel-wise dimension and hierarchical dimension. The adaptive spatial\nattention branch (ASAB) and the adaptive channel attention branch (ACAB)\nconstitute the adaptive dual attention module (ADAM), which can capture the\nlong-range spatial and channel-wise contextual information to expand the\nreceptive field and distinguish different types of information for more\neffective feature representations. Furthermore, the adaptive hierarchical\nattention module (AHAM) can capture the long-range hierarchical contextual\ninformation to flexibly aggregate different feature maps by weights depending\non the global context. The ADAM and AHAM cooperate to form an \"attention in\nattention\" structure, which means AHAM's inputs are enhanced by ASAB and ACAB.\nExperiments demonstrate the superiority of our method over state-of-the-art\nimage restoration methods in both quantitative comparison and visual analysis.\nCode is available at https://github.com/YCHang686/A-CubeNet.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 03:42:14 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 03:35:45 GMT"}, {"version": "v3", "created": "Sun, 24 Jan 2021 11:32:04 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Hang", "Yucheng", ""], ["Liao", "Qingmin", ""], ["Yang", "Wenming", ""], ["Chen", "Yupeng", ""], ["Zhou", "Jie", ""]]}, {"id": "2009.05934", "submitter": "Xuequan Lu", "authors": "Disheng Feng, Xuequan Lu, Xufeng Lin", "title": "Deep Detection for Face Manipulation", "comments": "accepted to the 27th International Conference on Neural Information\n  Processing Xuequan Lu is the corresponding author (see www.xuequanlu.com for\n  more information)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has become increasingly challenging to distinguish real faces from their\nvisually realistic fake counterparts, due to the great advances of deep\nlearning based face manipulation techniques in recent years. In this paper, we\nintroduce a deep learning method to detect face manipulation. It consists of\ntwo stages: feature extraction and binary classification. To better distinguish\nfake faces from real faces, we resort to the triplet loss function in the first\nstage. We then design a simple linear classification network to bridge the\nlearned contrastive features with the real/fake faces. Experimental results on\npublic benchmark datasets demonstrate the effectiveness of this method, and\nshow that it generates better performance than state-of-the-art techniques in\nmost cases.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 06:48:34 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Feng", "Disheng", ""], ["Lu", "Xuequan", ""], ["Lin", "Xufeng", ""]]}, {"id": "2009.05938", "submitter": "Michael Lyons", "authors": "Michael J. Lyons, Miyuki Kamachi, Jiro Gyoba", "title": "Coding Facial Expressions with Gabor Wavelets (IVC Special Issue)", "comments": "13 pages, 7 figures, 23 references", "journal-ref": null, "doi": "10.5281/zenodo.4029679", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for extracting information about facial expressions from\ndigital images. The method codes facial expression images using a\nmulti-orientation, multi-resolution set of Gabor filters that are\ntopographically ordered and approximately aligned with the face. A similarity\nspace derived from this code is compared with one derived from semantic ratings\nof the images by human observers. Interestingly the low-dimensional structure\nof the image-derived similarity space shares organizational features with the\ncircumplex model of affect, suggesting a bridge between categorical and\ndimensional representations of facial expression. Our results also indicate\nthat it would be possible to construct a facial expression classifier based on\na topographically-linked multi-orientation, multi-resolution Gabor coding of\nthe facial images at the input stage. The significant degree of psychological\nplausibility exhibited by the proposed code may also be useful in the design of\nhuman-computer interfaces.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 07:01:16 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Lyons", "Michael J.", ""], ["Kamachi", "Miyuki", ""], ["Gyoba", "Jiro", ""]]}, {"id": "2009.05942", "submitter": "Danfeng Hong", "authors": "Haixia Bi, Jing Yao, Zhiqiang Wei, Danfeng Hong, Jocelyn Chanussot", "title": "PolSAR Image Classification Based on Robust Low-Rank Feature Extraction\n  and Markov Random Field", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polarimetric synthetic aperture radar (PolSAR) image classification has been\ninvestigated vigorously in various remote sensing applications. However, it is\nstill a challenging task nowadays. One significant barrier lies in the speckle\neffect embedded in the PolSAR imaging process, which greatly degrades the\nquality of the images and further complicates the classification. To this end,\nwe present a novel PolSAR image classification method, which removes speckle\nnoise via low-rank (LR) feature extraction and enforces smoothness priors via\nMarkov random field (MRF). Specifically, we employ the mixture of\nGaussian-based robust LR matrix factorization to simultaneously extract\ndiscriminative features and remove complex noises. Then, a classification map\nis obtained by applying convolutional neural network with data augmentation on\nthe extracted features, where local consistency is implicitly involved, and the\ninsufficient label issue is alleviated. Finally, we refine the classification\nmap by MRF to enforce contextual smoothness. We conduct experiments on two\nbenchmark PolSAR datasets. Experimental results indicate that the proposed\nmethod achieves promising classification performance and preferable spatial\nconsistency.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 07:38:12 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Bi", "Haixia", ""], ["Yao", "Jing", ""], ["Wei", "Zhiqiang", ""], ["Hong", "Danfeng", ""], ["Chanussot", "Jocelyn", ""]]}, {"id": "2009.05946", "submitter": "Anders Eklund", "authors": "Mehdi Foroozandeh, Anders Eklund", "title": "Synthesizing brain tumor images and annotations by combining progressive\n  growing GAN and SPADE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training segmentation networks requires large annotated datasets, but manual\nannotation is time consuming and costly. We here investigate if the combination\nof a noise-to-image GAN and an image-to-image GAN can be used to synthesize\nrealistic brain tumor images as well as the corresponding tumor annotations\n(labels), to substantially increase the number of training images. The\nnoise-to-image GAN is used to synthesize new label images, while the\nimage-to-image GAN generates the corresponding MR image from the label image.\nOur results indicate that the two GANs can synthesize label images and MR\nimages that look realistic, and that adding synthetic images improves the\nsegmentation performance, although the effect is small.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 07:56:10 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Foroozandeh", "Mehdi", ""], ["Eklund", "Anders", ""]]}, {"id": "2009.05951", "submitter": "Nguyen Quoc Khanh Le Dr.", "authors": "Hieu X. Le, Phuong D. Nguyen, Thang H. Nguyen, Khanh N.Q. Le, Thanh T.\n  Nguyen", "title": "Interpretation of smartphone-captured radiographs utilizing a deep\n  learning-based approach", "comments": "10 pages, 5 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, computer-aided diagnostic systems (CADs) that could automatically\ninterpret medical images effectively have been the emerging subject of recent\nacademic attention. For radiographs, several deep learning-based systems or\nmodels have been developed to study the multi-label diseases recognition tasks.\nHowever, none of them have been trained to work on smartphone-captured chest\nradiographs. In this study, we proposed a system that comprises a sequence of\ndeep learning-based neural networks trained on the newly released CheXphoto\ndataset to tackle this issue. The proposed approach achieved promising results\nof 0.684 in AUC and 0.699 in average F1 score. To the best of our knowledge,\nthis is the first published study that showed to be capable of processing\nsmartphone-captured radiographs.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 08:26:10 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Le", "Hieu X.", ""], ["Nguyen", "Phuong D.", ""], ["Nguyen", "Thang H.", ""], ["Le", "Khanh N. Q.", ""], ["Nguyen", "Thanh T.", ""]]}, {"id": "2009.05964", "submitter": "Khanh-Hung Tran", "authors": "Khanh-Hung Tran, Fred-Maurice Ngole-Mboula, Jean-Luc Starck and\n  Vincent Prost", "title": "Semi-supervised dictionary learning with graph regularization and active\n  points", "comments": null, "journal-ref": "SIAM Journal on Imaging Sciences, 2020, Vol. 13, No. 2 : pp.\n  724-745", "doi": "10.1137/19M1285469", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised Dictionary Learning has gained much interest in the recent decade\nand has shown significant performance improvements in image classification.\nHowever, in general, supervised learning needs a large number of labelled\nsamples per class to achieve an acceptable result. In order to deal with\ndatabases which have just a few labelled samples per class, semi-supervised\nlearning, which also exploits unlabelled samples in training phase is used.\nIndeed, unlabelled samples can help to regularize the learning model, yielding\nan improvement of classification accuracy. In this paper, we propose a new\nsemi-supervised dictionary learning method based on two pillars: on one hand,\nwe enforce manifold structure preservation from the original data into sparse\ncode space using Locally Linear Embedding, which can be considered a\nregularization of sparse code; on the other hand, we train a semi-supervised\nclassifier in sparse code space. We show that our approach provides an\nimprovement over state-of-the-art semi-supervised dictionary learning methods.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 09:24:51 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Tran", "Khanh-Hung", ""], ["Ngole-Mboula", "Fred-Maurice", ""], ["Starck", "Jean-Luc", ""], ["Prost", "Vincent", ""]]}, {"id": "2009.05972", "submitter": "Junhui Yin", "authors": "Junhui Yin, Jiayan Qiu, Siqing Zhang, Zhanyu Ma, Jun Guo", "title": "SSKD: Self-Supervised Knowledge Distillation for Cross Domain Adaptive\n  Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptive person re-identification (re-ID) is a challenging task due to\nthe large discrepancy between the source domain and the target domain. To\nreduce the domain discrepancy, existing methods mainly attempt to generate\npseudo labels for unlabeled target images by clustering algorithms. However,\nclustering methods tend to bring noisy labels and the rich fine-grained details\nin unlabeled images are not sufficiently exploited. In this paper, we seek to\nimprove the quality of labels by capturing feature representation from multiple\naugmented views of unlabeled images. To this end, we propose a Self-Supervised\nKnowledge Distillation (SSKD) technique containing two modules, the identity\nlearning and the soft label learning. Identity learning explores the\nrelationship between unlabeled samples and predicts their one-hot labels by\nclustering to give exact information for confidently distinguished images. Soft\nlabel learning regards labels as a distribution and induces an image to be\nassociated with several related classes for training peer network in a\nself-supervised manner, where the slowly evolving network is a core to obtain\nsoft labels as a gentle constraint for reliable images. Finally, the two\nmodules can resist label noise for re-ID by enhancing each other and\nsystematically integrating label information from unlabeled images. Extensive\nexperiments on several adaptation tasks demonstrate that the proposed method\noutperforms the current state-of-the-art approaches by large margins.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 10:12:02 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Yin", "Junhui", ""], ["Qiu", "Jiayan", ""], ["Zhang", "Siqing", ""], ["Ma", "Zhanyu", ""], ["Guo", "Jun", ""]]}, {"id": "2009.05982", "submitter": "Zhihao Hu", "authors": "Zhihao Hu (1), Zhenghao Chen (2), Dong Xu (2), Guo Lu (3), Wanli\n  Ouyang (2), Shuhang Gu (2) ((1) College of Software, Beihang University,\n  China, (2) School of Electrical and Information Engineering, The University\n  of Sydney, Australia, (3) School of Computer Science & Technology, Beijing\n  Institute of Technology, China)", "title": "Improving Deep Video Compression by Resolution-adaptive Flow Coding", "comments": "ECCV 2020(oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the learning based video compression approaches, it is an essential issue\nto compress pixel-level optical flow maps by developing new motion vector (MV)\nencoders. In this work, we propose a new framework called Resolution-adaptive\nFlow Coding (RaFC) to effectively compress the flow maps globally and locally,\nin which we use multi-resolution representations instead of single-resolution\nrepresentations for both the input flow maps and the output motion features of\nthe MV encoder. To handle complex or simple motion patterns globally, our\nframe-level scheme RaFC-frame automatically decides the optimal flow map\nresolution for each video frame. To cope different types of motion patterns\nlocally, our block-level scheme called RaFC-block can also select the optimal\nresolution for each local block of motion features. In addition, the\nrate-distortion criterion is applied to both RaFC-frame and RaFC-block and\nselect the optimal motion coding mode for effective flow coding. Comprehensive\nexperiments on four benchmark datasets HEVC, VTL, UVG and MCL-JCV clearly\ndemonstrate the effectiveness of our overall RaFC framework after combing\nRaFC-frame and RaFC-block for video compression.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 12:10:34 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Hu", "Zhihao", ""], ["Chen", "Zhenghao", ""], ["Xu", "Dong", ""], ["Lu", "Guo", ""], ["Ouyang", "Wanli", ""], ["Gu", "Shuhang", ""]]}, {"id": "2009.05983", "submitter": "Mengdi Xu", "authors": "Wentai Lei, Mengdi Xu, Feifei Hou, Wensi Jiang", "title": "Calibration Venus: An Interactive Camera Calibration Method Based on\n  Search Algorithm and Pose Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scenarios where cameras are applied, such as robot positioning and\nunmanned driving, camera calibration is one of the most important pre-work. The\ninteractive calibration method based on the plane board is becoming popular in\ncamera calibration field due to its repeatability and operation advantages.\nHowever, the existing methods select suggestions from a fixed dataset of\npre-defined poses based on subjective experience, which leads to a certain\ndegree of one-sidedness. Moreover, they does not give users clear instructions\non how to place the board in the specified pose.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 12:12:10 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Lei", "Wentai", ""], ["Xu", "Mengdi", ""], ["Hou", "Feifei", ""], ["Jiang", "Wensi", ""]]}, {"id": "2009.05994", "submitter": "Aritra Mukherjee", "authors": "Aritra Mukherjee, Sourya Dipta Das, Jasorsi Ghosh, Ananda S.\n  Chowdhury, Sanjoy Kumar Saha", "title": "Semantic Segmentation of Surface from Lidar Point Cloud", "comments": "Accepted in Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of SLAM (Simultaneous Localization And Mapping) for robot\nnavigation, mapping the environment is an important task. In this regard the\nLidar sensor can produce near accurate 3D map of the environment in the format\nof point cloud, in real time. Though the data is adequate for extracting\ninformation related to SLAM, processing millions of points in the point cloud\nis computationally quite expensive. The methodology presented proposes a fast\nalgorithm that can be used to extract semantically labelled surface segments\nfrom the cloud, in real time, for direct navigational use or higher level\ncontextual scene reconstruction. First, a single scan from a spinning Lidar is\nused to generate a mesh of subsampled cloud points online. The generated mesh\nis further used for surface normal computation of those points on the basis of\nwhich surface segments are estimated. A novel descriptor to represent the\nsurface segments is proposed and utilized to determine the surface class of the\nsegments (semantic label) with the help of classifier. These semantic surface\nsegments can be further utilized for geometric reconstruction of objects in the\nscene, or can be used for optimized trajectory planning by a robot. The\nproposed methodology is compared with number of point cloud segmentation\nmethods and state of the art semantic segmentation methods to emphasize its\nefficacy in terms of speed and accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 13:06:26 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Mukherjee", "Aritra", ""], ["Das", "Sourya Dipta", ""], ["Ghosh", "Jasorsi", ""], ["Chowdhury", "Ananda S.", ""], ["Saha", "Sanjoy Kumar", ""]]}, {"id": "2009.06001", "submitter": "Thyagharajan K K", "authors": "K. K. Thyagharajan, I. Kiruba Raji", "title": "A Review of Visual Descriptors and Classification Techniques Used in\n  Leaf Species Identification", "comments": "44 pages, 7 figures, \"for final published version, see\n  https://link.springer.com/article/10.1007/s11831-018-9266-3\"", "journal-ref": "Sept. 2019", "doi": "10.1007/s11831-018-9266-3", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plants are fundamentally important to life. Key research areas in plant\nscience include plant species identification, weed classification using hyper\nspectral images, monitoring plant health and tracing leaf growth, and the\nsemantic interpretation of leaf information. Botanists easily identify plant\nspecies by discriminating between the shape of the leaf, tip, base, leaf margin\nand leaf vein, as well as the texture of the leaf and the arrangement of\nleaflets of compound leaves. Because of the increasing demand for experts and\ncalls for biodiversity, there is a need for intelligent systems that recognize\nand characterize leaves so as to scrutinize a particular species, the diseases\nthat affect them, the pattern of leaf growth, and so on. We review several\nimage processing methods in the feature extraction of leaves, given that\nfeature extraction is a crucial technique in computer vision. As computers\ncannot comprehend images, they are required to be converted into features by\nindividually analysing image shapes, colours, textures and moments. Images that\nlook the same may deviate in terms of geometric and photometric variations. In\nour study, we also discuss certain machine learning classifiers for an analysis\nof different species of leaves.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 14:11:00 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Thyagharajan", "K. K.", ""], ["Raji", "I. Kiruba", ""]]}, {"id": "2009.06024", "submitter": "Gurpreet Singh", "authors": "Gurpreet Singh, Soumyajit Gupta, Matthew Lease", "title": "Extracting Optimal Solution Manifolds using Constrained Neural\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained Optimization solution algorithms are restricted to point based\nsolutions. In practice, single or multiple objectives must be satisfied,\nwherein both the objective function and constraints can be non-convex resulting\nin multiple optimal solutions. Real world scenarios include intersecting\nsurfaces as Implicit Functions, Hyperspectral Unmixing and Pareto Optimal\nfronts. Local or global convexification is a common workaround when faced with\nnon-convex forms. However, such an approach is often restricted to a strict\nclass of functions, deviation from which results in sub-optimal solution to the\noriginal problem. We present neural solutions for extracting optimal sets as\napproximate manifolds, where unmodified, non-convex objectives and constraints\nare defined as modeler guided, domain-informed $L_2$ loss function. This\npromotes interpretability since modelers can confirm the results against known\nanalytical forms in their specific domains. We present synthetic and realistic\ncases to validate our approach and compare against known solvers for\nbench-marking in terms of accuracy and computational efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 15:37:44 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 16:37:04 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 04:15:20 GMT"}, {"version": "v4", "created": "Sun, 3 Jan 2021 18:46:54 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Singh", "Gurpreet", ""], ["Gupta", "Soumyajit", ""], ["Lease", "Matthew", ""]]}, {"id": "2009.06053", "submitter": "Xuyang Shen", "authors": "Xuyang Shen, Jo Plested, Yue Yao, Tom Gedeon", "title": "Pairwise-GAN: Pose-based View Synthesis through Pair-Wise Training", "comments": "The 27th International Conference on Neural Information\n  Processing(ICONIP2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Three-dimensional face reconstruction is one of the popular applications in\ncomputer vision. However, even state-of-the-art models still require frontal\nface as inputs, which restricts its usage scenarios in the wild. A similar\ndilemma also happens in face recognition. New research designed to recover the\nfrontal face from a single side-pose facial image has emerged. The\nstate-of-the-art in this area is the Face-Transformation generative adversarial\nnetwork, which is based on the CycleGAN. This inspired our research which\nexplores the performance of two models from pixel transformation in frontal\nfacial synthesis, Pix2Pix and CycleGAN. We conducted the experiments on five\ndifferent loss functions on Pix2Pix to improve its performance, then followed\nby proposing a new network Pairwise-GAN in frontal facial synthesis.\nPairwise-GAN uses two parallel U-Nets as the generator and PatchGAN as the\ndiscriminator. The detailed hyper-parameters are also discussed. Based on the\nquantitative measurement by face similarity comparison, our results showed that\nPix2Pix with L1 loss, gradient difference loss, and identity loss results in\n2.72% of improvement at average similarity compared to the default Pix2Pix\nmodel. Additionally, the performance of Pairwise-GAN is 5.4% better than the\nCycleGAN and 9.1% than the Pix2Pix at average similarity.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 18:02:15 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Shen", "Xuyang", ""], ["Plested", "Jo", ""], ["Yao", "Yue", ""], ["Gedeon", "Tom", ""]]}, {"id": "2009.06066", "submitter": "Unnikrishnan R Nair", "authors": "Nivedita Rufus, Unni Krishnan R Nair, K. Madhava Krishna and Vineet\n  Gandhi", "title": "Cosine meets Softmax: A tough-to-beat baseline for visual grounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a simple baseline for visual grounding for\nautonomous driving which outperforms the state of the art methods, while\nretaining minimal design choices. Our framework minimizes the cross-entropy\nloss over the cosine distance between multiple image ROI features with a text\nembedding (representing the give sentence/phrase). We use pre-trained networks\nfor obtaining the initial embeddings and learn a transformation layer on top of\nthe text embedding. We perform experiments on the Talk2Car dataset and achieve\n68.7% AP50 accuracy, improving upon the previous state of the art by 8.6%. Our\ninvestigation suggests reconsideration towards more approaches employing\nsophisticated attention mechanisms or multi-stage reasoning or complex metric\nlearning loss functions by showing promise in simpler alternatives.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 19:35:43 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Rufus", "Nivedita", ""], ["Nair", "Unni Krishnan R", ""], ["Krishna", "K. Madhava", ""], ["Gandhi", "Vineet", ""]]}, {"id": "2009.06114", "submitter": "Peipei Xu", "authors": "Peipei Xu and Wenjie Ruan and Xiaowei Huang", "title": "Towards the Quantification of Safety Risks in Deep Neural Networks", "comments": "19 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safety concerns on the deep neural networks (DNNs) have been raised when they\nare applied to critical sectors. In this paper, we define safety risks by\nrequesting the alignment of the network's decision with human perception. To\nenable a general methodology for quantifying safety risks, we define a generic\nsafety property and instantiate it to express various safety risks. For the\nquantification of risks, we take the maximum radius of safe norm balls, in\nwhich no safety risk exists. The computation of the maximum safe radius is\nreduced to the computation of their respective Lipschitz metrics - the\nquantities to be computed. In addition to the known adversarial example,\nreachability example, and invariant example, in this paper we identify a new\nclass of risk - uncertainty example - on which humans can tell easily but the\nnetwork is unsure. We develop an algorithm, inspired by derivative-free\noptimization techniques and accelerated by tensor-based parallelization on\nGPUs, to support efficient computation of the metrics. We perform evaluations\non several benchmark neural networks, including ACSC-Xu, MNIST, CIFAR-10, and\nImageNet networks. The experiments show that, our method can achieve\ncompetitive performance on safety quantification in terms of the tightness and\nthe efficiency of computation. Importantly, as a generic approach, our method\ncan work with a broad class of safety risks and without restrictions on the\nstructure of neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 23:30:09 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Xu", "Peipei", ""], ["Ruan", "Wenjie", ""], ["Huang", "Xiaowei", ""]]}, {"id": "2009.06115", "submitter": "Vaibhav Patel", "authors": "Apurva Pandya, Catherine Samuel, Nisargkumar Patel, Vaibhavkumar\n  Patel, Thangarajah Akilan", "title": "Multi-channel MRI Embedding: An EffectiveStrategy for Enhancement of\n  Human Brain WholeTumor Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the most important tasks in medical image processing is the brain's\nwhole tumor segmentation. It assists in quicker clinical assessment and early\ndetection of brain tumors, which is crucial for lifesaving treatment procedures\nof patients. Because, brain tumors often can be malignant or benign, if they\nare detected at an early stage. A brain tumor is a collection or a mass of\nabnormal cells in the brain. The human skull encloses the brain very rigidly\nand any growth inside this restricted place can cause severe health issues. The\ndetection of brain tumors requires careful and intricate analysis for surgical\nplanning and treatment. Most physicians employ Magnetic Resonance Imaging (MRI)\nto diagnose such tumors. A manual diagnosis of the tumors using MRI is known to\nbe time-consuming; approximately, it takes up to eighteen hours per sample.\nThus, the automatic segmentation of tumors has become an optimal solution for\nthis problem. Studies have shown that this technique provides better accuracy\nand it is faster than manual analysis resulting in patients receiving the\ntreatment at the right time. Our research introduces an efficient strategy\ncalled Multi-channel MRI embedding to improve the result of deep learning-based\ntumor segmentation. The experimental analysis on the Brats-2019 dataset wrt the\nU-Net encoder-decoder (EnDec) model shows significant improvement. The\nembedding strategy surmounts the state-of-the-art approaches with an\nimprovement of 2% without any timing overheads.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 23:44:16 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Pandya", "Apurva", ""], ["Samuel", "Catherine", ""], ["Patel", "Nisargkumar", ""], ["Patel", "Vaibhavkumar", ""], ["Akilan", "Thangarajah", ""]]}, {"id": "2009.06116", "submitter": "Jannis Born", "authors": "Jannis Born, Nina Wiedemann, Gabriel Br\\\"andle, Charlotte Buhre,\n  Bastian Rieck, Karsten Borgwardt", "title": "Accelerating COVID-19 Differential Diagnosis with Explainable Ultrasound\n  Image Analysis", "comments": "8 pages, 4 figures", "journal-ref": "Applied Sciences 2021 (special issue on: \"Fighting COVID-19:\n  Emerging Techniques and Aid Systems for Prevention, Forecasting and\n  Diagnosis\")", "doi": "10.3390/app11020672", "report-no": null, "categories": "cs.CV cs.DB cs.DL cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Controlling the COVID-19 pandemic largely hinges upon the existence of fast,\nsafe, and highly-available diagnostic tools. Ultrasound, in contrast to CT or\nX-Ray, has many practical advantages and can serve as a globally-applicable\nfirst-line examination technique. We provide the largest publicly available\nlung ultrasound (US) dataset for COVID-19 consisting of 106 videos from three\nclasses (COVID-19, bacterial pneumonia, and healthy controls); curated and\napproved by medical experts. On this dataset, we perform an in-depth study of\nthe value of deep learning methods for differential diagnosis of COVID-19. We\npropose a frame-based convolutional neural network that correctly classifies\nCOVID-19 US videos with a sensitivity of 0.98+-0.04 and a specificity of\n0.91+-08 (frame-based sensitivity 0.93+-0.05, specificity 0.87+-0.07). We\nfurther employ class activation maps for the spatio-temporal localization of\npulmonary biomarkers, which we subsequently validate for human-in-the-loop\nscenarios in a blindfolded study with medical experts. Aiming for scalability\nand robustness, we perform ablation studies comparing mobile-friendly, frame-\nand video-based architectures and show reliability of the best model by\naleatoric and epistemic uncertainty estimates. We hope to pave the road for a\ncommunity effort toward an accessible, efficient and interpretable screening\nmethod and we have started to work on a clinical validation of the proposed\nmethod. Data and code are publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 23:52:03 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Born", "Jannis", ""], ["Wiedemann", "Nina", ""], ["Br\u00e4ndle", "Gabriel", ""], ["Buhre", "Charlotte", ""], ["Rieck", "Bastian", ""], ["Borgwardt", "Karsten", ""]]}, {"id": "2009.06127", "submitter": "Bardia Yousefi", "authors": "Hossein Memarzadeh Sharifipour, Bardia Yousefi", "title": "Mathematical Morphology via Category Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical morphology contributes many profitable tools to image processing\narea. Some of these methods considered to be basic but the most important\nfundamental of data processing in many various applications. In this paper, we\nmodify the fundamental of morphological operations such as dilation and erosion\nmaking use of limit and co-limit preserving functors within (Category Theory).\nAdopting the well-known matrix representation of images, the category of\nmatrix, called Mat, can be represented as an image. With enriching Mat over\nvarious semirings such as Boolean and (max,+) semirings, one can arrive at\nclassical definition of binary and gray-scale images using the categorical\ntensor product in Mat. With dilation operation in hand, the erosion can be\nreached using the famous tensor-hom adjunction. This approach enables us to\ndefine new types of dilation and erosion between two images represented by\nmatrices using different semirings other than Boolean and (max,+) semirings.\nThe viewpoint of morphological operations from category theory can also shed\nlight to the claimed concept that mathematical morphology is a model for linear\nlogic.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 00:44:34 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Sharifipour", "Hossein Memarzadeh", ""], ["Yousefi", "Bardia", ""]]}, {"id": "2009.06138", "submitter": "Liangzhi Li", "authors": "Liangzhi Li, Bowen Wang, Manisha Verma, Yuta Nakashima, Ryo Kawasaki,\n  Hajime Nagahara", "title": "SCOUTER: Slot Attention-based Classifier for Explainable Image\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable artificial intelligence has been gaining attention in the past\nfew years. However, most existing methods are based on gradients or\nintermediate features, which are not directly involved in the decision-making\nprocess of the classifier. In this paper, we propose a slot attention-based\nclassifier called SCOUTER for transparent yet accurate classification. Two\nmajor differences from other attention-based methods include: (a) SCOUTER's\nexplanation is involved in the final confidence for each category, offering\nmore intuitive interpretation, and (b) all the categories have their\ncorresponding positive or negative explanation, which tells \"why the image is\nof a certain category\" or \"why the image is not of a certain category.\" We\ndesign a new loss tailored for SCOUTER that controls the model's behavior to\nswitch between positive and negative explanations, as well as the size of\nexplanatory regions. Experimental results show that SCOUTER can give better\nvisual explanations while keeping good accuracy on small and medium-sized\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 01:34:56 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 12:02:07 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 03:32:14 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Li", "Liangzhi", ""], ["Wang", "Bowen", ""], ["Verma", "Manisha", ""], ["Nakashima", "Yuta", ""], ["Kawasaki", "Ryo", ""], ["Nagahara", "Hajime", ""]]}, {"id": "2009.06160", "submitter": "Tianyi Wu", "authors": "Tianyi Wu, Yu Lu, Yu Zhu, Chuang Zhang, Ming Wu, Zhanyu Ma, Guodong\n  Guo", "title": "GINet: Graph Interaction Network for Scene Parsing", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, context reasoning using image regions beyond local convolution has\nshown great potential for scene parsing. In this work, we explore how to\nincorporate the linguistic knowledge to promote context reasoning over image\nregions by proposing a Graph Interaction unit (GI unit) and a Semantic Context\nLoss (SC-loss). The GI unit is capable of enhancing feature representations of\nconvolution networks over high-level semantics and learning the semantic\ncoherency adaptively to each sample. Specifically, the dataset-based linguistic\nknowledge is first incorporated in the GI unit to promote context reasoning\nover the visual graph, then the evolved representations of the visual graph are\nmapped to each local representation to enhance the discriminated capability for\nscene parsing. GI unit is further improved by the SC-loss to enhance the\nsemantic representations over the exemplar-based semantic graph. We perform\nfull ablation studies to demonstrate the effectiveness of each component in our\napproach. Particularly, the proposed GINet outperforms the state-of-the-art\napproaches on the popular benchmarks, including Pascal-Context and COCO Stuff.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 02:52:45 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Wu", "Tianyi", ""], ["Lu", "Yu", ""], ["Zhu", "Yu", ""], ["Zhang", "Chuang", ""], ["Wu", "Ming", ""], ["Ma", "Zhanyu", ""], ["Guo", "Guodong", ""]]}, {"id": "2009.06168", "submitter": "Hengtong Hu", "authors": "Hengtong Hu, Lingxi Xie, Zewei Du, Richang Hong, Qi Tian", "title": "One-bit Supervision for Image Classification", "comments": null, "journal-ref": "NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents one-bit supervision, a novel setting of learning from\nincomplete annotations, in the scenario of image classification. Instead of\ntraining a model upon the accurate label of each sample, our setting requires\nthe model to query with a predicted label of each sample and learn from the\nanswer whether the guess is correct. This provides one bit (yes or no) of\ninformation, and more importantly, annotating each sample becomes much easier\nthan finding the accurate label from many candidate classes. There are two keys\nto training a model upon one-bit supervision: improving the guess accuracy and\nmaking use of incorrect guesses. For these purposes, we propose a multi-stage\ntraining paradigm which incorporates negative label suppression into an\noff-the-shelf semi-supervised learning algorithm. In three popular image\nclassification benchmarks, our approach claims higher efficiency in utilizing\nthe limited amount of annotations.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 03:06:23 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 09:19:06 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 06:35:01 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Hu", "Hengtong", ""], ["Xie", "Lingxi", ""], ["Du", "Zewei", ""], ["Hong", "Richang", ""], ["Tian", "Qi", ""]]}, {"id": "2009.06169", "submitter": "Xusen Guo", "authors": "Xusen Guo, Jiangfeng Gu, Silu Guo, Zixiao Xu, Chengzhang Yang,\n  Shanghua Liu, Long Cheng, Kai Huang", "title": "3D Object Detection and Tracking Based on Streaming Data", "comments": "Accepted by ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches for 3D object detection have made tremendous progresses due\nto the development of deep learning. However, previous researches are mostly\nbased on individual frames, leading to limited exploitation of information\nbetween frames. In this paper, we attempt to leverage the temporal information\nin streaming data and explore 3D streaming based object detection as well as\ntracking. Toward this goal, we set up a dual-way network for 3D object\ndetection based on keyframes, and then propagate predictions to non-key frames\nthrough a motion based interpolation algorithm guided by temporal information.\nOur framework is not only shown to have significant improvements on object\ndetection compared with frame-by-frame paradigm, but also proven to produce\ncompetitive results on KITTI Object Tracking Benchmark, with 76.68% in MOTA and\n81.65% in MOTP respectively.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 03:15:41 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Guo", "Xusen", ""], ["Gu", "Jiangfeng", ""], ["Guo", "Silu", ""], ["Xu", "Zixiao", ""], ["Yang", "Chengzhang", ""], ["Liu", "Shanghua", ""], ["Cheng", "Long", ""], ["Huang", "Kai", ""]]}, {"id": "2009.06184", "submitter": "Yifan Wang", "authors": "Yifan Wang, Guoli Yan, Haikuan Zhu, Sagar Buch, Ying Wang, Ewart Mark\n  Haacke, Jing Hua, and Zichun Zhong", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and\n  Visualization of Highly Sparse and Noisy Image Data", "comments": "15 pages, 10 figures, proceeding to IEEE Transactions on\n  Visualization and Computer Graphics (TVCG) (IEEE SciVis 2020), October, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motivation of our work is to present a new visualization-guided computing\nparadigm to combine direct 3D volume processing and volume rendered clues for\neffective 3D exploration such as extracting and visualizing microstructures\nin-vivo. However, it is still challenging to extract and visualize high\nfidelity 3D vessel structure due to its high sparseness, noisiness, and complex\ntopology variations. In this paper, we present an end-to-end deep learning\nmethod, VC-Net, for robust extraction of 3D microvasculature through embedding\nthe image composition, generated by maximum intensity projection (MIP), into 3D\nvolume image learning to enhance the performance. The core novelty is to\nautomatically leverage the volume visualization technique (MIP) to enhance the\n3D data exploration at deep learning level. The MIP embedding features can\nenhance the local vessel signal and are adaptive to the geometric variability\nand scalability of vessels, which is crucial in microvascular tracking. A\nmulti-stream convolutional neural network is proposed to learn the 3D volume\nand 2D MIP features respectively and then explore their inter-dependencies in a\njoint volume-composition embedding space by unprojecting the MIP features into\n3D volume embedding space. The proposed framework can better capture small /\nmicro vessels and improve vessel connectivity. To our knowledge, this is the\nfirst deep learning framework to construct a joint convolutional embedding\nspace, where the computed vessel probabilities from volume rendering based 2D\nprojection and 3D volume can be explored and integrated synergistically.\nExperimental results are compared with the traditional 3D vessel segmentation\nmethods and the deep learning state-of-the-art on public and real patient\n(micro-)cerebrovascular image datasets. Our method demonstrates the potential\nin a powerful MR arteriogram and venogram diagnosis of vascular diseases.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 04:15:02 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Wang", "Yifan", ""], ["Yan", "Guoli", ""], ["Zhu", "Haikuan", ""], ["Buch", "Sagar", ""], ["Wang", "Ying", ""], ["Haacke", "Ewart Mark", ""], ["Hua", "Jing", ""], ["Zhong", "Zichun", ""]]}, {"id": "2009.06193", "submitter": "Hao Tan", "authors": "Hao Tan, Ran Cheng, Shihua Huang, Cheng He, Changxiao Qiu, Fan Yang,\n  Ping Luo", "title": "RelativeNAS: Relative Neural Architecture Search via Slow-Fast Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Despite the remarkable successes of Convolutional Neural Networks (CNNs) in\ncomputer vision, it is time-consuming and error-prone to manually design a CNN.\nAmong various Neural Architecture Search (NAS) methods that are motivated to\nautomate designs of high-performance CNNs, the differentiable NAS and\npopulation-based NAS are attracting increasing interests due to their unique\ncharacters. To benefit from the merits while overcoming the deficiencies of\nboth, this work proposes a novel NAS method, RelativeNAS. As the key to\nefficient search, RelativeNAS performs joint learning between fast-learners\n(i.e. networks with relatively higher accuracy) and slow-learners in a pairwise\nmanner. Moreover, since RelativeNAS only requires low-fidelity performance\nestimation to distinguish each pair of fast-learner and slow-learner, it saves\ncertain computation costs for training the candidate architectures. The\nproposed RelativeNAS brings several unique advantages: (1) it achieves\nstate-of-the-art performance on ImageNet with top-1 error rate of 24.88%, i.e.\noutperforming DARTS and AmoebaNet-B by 1.82% and 1.12% respectively; (2) it\nspends only nine hours with a single 1080Ti GPU to obtain the discovered cells,\ni.e. 3.75x and 7875x faster than DARTS and AmoebaNet respectively; (3) it\nprovides that the discovered cells obtained on CIFAR-10 can be directly\ntransferred to object detection, semantic segmentation, and keypoint detection,\nyielding competitive results of 73.1% mAP on PASCAL VOC, 78.7% mIoU on\nCityscapes, and 68.5% AP on MSCOCO, respectively. The implementation of\nRelativeNAS is available at https://github.com/EMI-Group/RelativeNAS\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 04:38:07 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 07:56:54 GMT"}, {"version": "v3", "created": "Tue, 13 Jul 2021 14:54:46 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Tan", "Hao", ""], ["Cheng", "Ran", ""], ["Huang", "Shihua", ""], ["He", "Cheng", ""], ["Qiu", "Changxiao", ""], ["Yang", "Fan", ""], ["Luo", "Ping", ""]]}, {"id": "2009.06200", "submitter": "Bruno Adriano D.Eng.", "authors": "Bruno Adriano, Naoto Yokoya, Junshi Xia, Hiroyuki Miura, Wen Liu,\n  Masashi Matsuoka, Shunichi Koshimura", "title": "Learning from Multimodal and Multitemporal Earth Observation Data for\n  Building Damage Mapping", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Earth observation technologies, such as optical imaging and synthetic\naperture radar (SAR), provide excellent means to monitor ever-growing urban\nenvironments continuously. Notably, in the case of large-scale disasters (e.g.,\ntsunamis and earthquakes), in which a response is highly time-critical, images\nfrom both data modalities can complement each other to accurately convey the\nfull damage condition in the disaster's aftermath. However, due to several\nfactors, such as weather and satellite coverage, it is often uncertain which\ndata modality will be the first available for rapid disaster response efforts.\nHence, novel methodologies that can utilize all accessible EO datasets are\nessential for disaster management. In this study, we have developed a global\nmultisensor and multitemporal dataset for building damage mapping. We included\nbuilding damage characteristics from three disaster types, namely, earthquakes,\ntsunamis, and typhoons, and considered three building damage categories. The\nglobal dataset contains high-resolution optical imagery and\nhigh-to-moderate-resolution multiband SAR data acquired before and after each\ndisaster. Using this comprehensive dataset, we analyzed five data modality\nscenarios for damage mapping: single-mode (optical and SAR datasets),\ncross-modal (pre-disaster optical and post-disaster SAR datasets), and mode\nfusion scenarios. We defined a damage mapping framework for the semantic\nsegmentation of damaged buildings based on a deep convolutional neural network\nalgorithm. We compare our approach to another state-of-the-art baseline model\nfor damage mapping. The results indicated that our dataset, together with a\ndeep learning network, enabled acceptable predictions for all the data modality\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 05:04:19 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Adriano", "Bruno", ""], ["Yokoya", "Naoto", ""], ["Xia", "Junshi", ""], ["Miura", "Hiroyuki", ""], ["Liu", "Wen", ""], ["Matsuoka", "Masashi", ""], ["Koshimura", "Shunichi", ""]]}, {"id": "2009.06205", "submitter": "Yu Guo", "authors": "Yu Guo, Qiyu Jin, Gabriele Facciolo, Tieyong Zeng, Jean-Michel Morel", "title": "Residual Learning for Effective joint Demosaicing-Denoising", "comments": "10 pages, 40 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image demosaicking and denoising are the two key steps for color image\nproduction pipeline. The classical processing sequence consists of applying\ndenoising first, and then demosaicking. However, this sequence leads to\noversmoothing and unpleasant checkerboard effect. Moreover, it is very\ndifficult to change this order, because once the image is demosaicked, the\nstatistical properties of the noise will be changed dramatically. This is\nextremely challenging for traditional denoising models that strongly rely on\nstatistical assumptions. In this paper, we attempt to tackle this prickly\nproblem. Indeed, here we invert the traditional CFA processing pipeline by\nfirst demosaicking and then denoising. In the first stage, we design a\ndemosaicking algorithm that combines traditional methods and a convolutional\nneural network (CNN) to reconstruct a full color image ignoring the noise. To\nimprove the performance in image demosaicking, we modify an Inception\narchitecture for fusing R, G and B three channels information. This stage\nretains all known information that is the key point to obtain pleasurable final\nresults. After demosaicking, we get a noisy full-color image and use another\nCNN to learn the demosaicking residual noise (including artifacts) of it, that\nallows to obtain a restored full color image. Our proposed algorithm completely\navoids the checkerboard effect and retains more image detail. Furthermore, it\ncan process very high-level noise while the performances of other CNN based\nmethods for noise higher than 20 are rather limited. Experimental results show\nclearly that our method outperforms state-of-the-art methods both\nquantitatively as well as in terms of visual quality.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 05:23:58 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 08:12:11 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Guo", "Yu", ""], ["Jin", "Qiyu", ""], ["Facciolo", "Gabriele", ""], ["Zeng", "Tieyong", ""], ["Morel", "Jean-Michel", ""]]}, {"id": "2009.06223", "submitter": "Yukuan Lou", "authors": "Chunlai Chai, Yukuan Lou, Shijin Zhang", "title": "Cascade Network for Self-Supervised Monocular Depth Estimation", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a classical compute vision problem to obtain real scene depth maps by\nusing a monocular camera, which has been widely concerned in recent years.\nHowever, training this model usually requires a large number of artificially\nlabeled samples. To solve this problem, some researchers use a self-supervised\nlearning model to overcome this problem and reduce the dependence on manually\nlabeled data. Nevertheless, the accuracy and reliability of these methods have\nnot reached the expected standard. In this paper, we propose a new\nself-supervised learning method based on cascade networks. Compared with the\nprevious self-supervised methods, our method has improved accuracy and\nreliability, and we have proved this by experiments. We show a cascaded neural\nnetwork that divides the target scene into parts of different sight distances\nand trains them separately to generate a better depth map. Our approach is\ndivided into the following four steps. In the first step, we use the\nself-supervised model to estimate the depth of the scene roughly. In the second\nstep, the depth of the scene generated in the first step is used as a label to\ndivide the scene into different depth parts. The third step is to use models\nwith different parameters to generate depth maps of different depth parts in\nthe target scene, and the fourth step is to fuse the depth map. Through the\nablation study, we demonstrated the effectiveness of each component\nindividually and showed high-quality, state-of-the-art results in the KITTI\nbenchmark.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 06:50:05 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Chai", "Chunlai", ""], ["Lou", "Yukuan", ""], ["Zhang", "Shijin", ""]]}, {"id": "2009.06226", "submitter": "Yukuan Lou", "authors": "Chunlai Chai, Yukuan Lou, Shijin Zhang", "title": "Prior Knowledge about Attributes: Learning a More Effective Potential\n  Space for Zero-Shot Recognition", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) aims to recognize unseen classes accurately by\nlearning seen classes and known attributes, but correlations in attributes were\nignored by previous study which lead to classification results confused. To\nsolve this problem, we build an Attribute Correlation Potential Space\nGeneration (ACPSG) model which uses a graph convolution network and attribute\ncorrelation to generate a more discriminating potential space. Combining\npotential discrimination space and user-defined attribute space, we can better\nclassify unseen classes. Our approach outperforms some existing\nstate-of-the-art methods on several benchmark datasets, whether it is\nconventional ZSL or generalized ZSL.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 06:57:23 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Chai", "Chunlai", ""], ["Lou", "Yukuan", ""], ["Zhang", "Shijin", ""]]}, {"id": "2009.06254", "submitter": "Qian Ning", "authors": "Qian Ning, Weisheng Dong, Guangming Shi, Leida Li, Xin Li", "title": "Accurate and Lightweight Image Super-Resolution with Model-Guided Deep\n  Unfolding Network", "comments": "Image Super-resolution, in IEEE Journal of Selected Topics in Signal\n  Processing", "journal-ref": null, "doi": "10.1109/JSTSP.2020.3037516", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) based methods have achieved great success in\nsingle image super-resolution (SISR). However, existing state-of-the-art SISR\ntechniques are designed like black boxes lacking transparency and\ninterpretability. Moreover, the improvement in visual quality is often at the\nprice of increased model complexity due to black-box design. In this paper, we\npresent and advocate an explainable approach toward SISR named model-guided\ndeep unfolding network (MoG-DUN). Targeting at breaking the coherence barrier,\nwe opt to work with a well-established image prior named nonlocal\nauto-regressive model and use it to guide our DNN design. By integrating deep\ndenoising and nonlocal regularization as trainable modules within a deep\nlearning framework, we can unfold the iterative process of model-based SISR\ninto a multi-stage concatenation of building blocks with three interconnected\nmodules (denoising, nonlocal-AR, and reconstruction). The design of all three\nmodules leverages the latest advances including dense/skip connections as well\nas fast nonlocal implementation. In addition to explainability, MoG-DUN is\naccurate (producing fewer aliasing artifacts), computationally efficient (with\nreduced model parameters), and versatile (capable of handling multiple\ndegradations). The superiority of the proposed MoG-DUN method to existing\nstate-of-the-art image SR methods including RCAN, SRMDNF, and SRFBN is\nsubstantiated by extensive experiments on several popular datasets and various\ndegradation scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 08:23:37 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 08:53:20 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ning", "Qian", ""], ["Dong", "Weisheng", ""], ["Shi", "Guangming", ""], ["Li", "Leida", ""], ["Li", "Xin", ""]]}, {"id": "2009.06288", "submitter": "Javier Juan-Albarrac\\'in", "authors": "Javier Juan-Albarrac\\'in", "title": "Unsupervised learning for vascular heterogeneity assessment of\n  glioblastoma based on magnetic resonance imaging: The Hemodynamic Tissue\n  Signature", "comments": "PhD thesis. Supervisors: Juan M. Garc\\'ia-G\\'omez and Elies\n  Fuster-Garcia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis focuses on the research and development of the Hemodynamic Tissue\nSignature (HTS) method: an unsupervised machine learning approach to describe\nthe vascular heterogeneity of glioblastomas by means of perfusion MRI analysis.\nThe HTS builds on the concept of habitats. An habitat is defined as a\nsub-region of the lesion with a particular MRI profile describing a specific\nphysiological behavior. The HTS method delineates four habitats within the\nglioblastoma: the High Angiogenic Tumor (HAT) habitat, as the most perfused\nregion of the enhancing tumor; the Low Angiogenic Tumor (LAT) habitat, as the\nregion of the enhancing tumor with a lower angiogenic profile; the potentially\nInfiltrated Peripheral Edema (IPE) habitat, as the non-enhancing region\nadjacent to the tumor with elevated perfusion indexes; and the Vasogenic\nPeripheral Edema (VPE) habitat, as the remaining edema of the lesion with the\nlowest perfusion profile.\n  The results of this thesis have been published in ten scientific\ncontributions, including top-ranked journals and conferences in the areas of\nMedical Informatics, Statistics and Probability, Radiology & Nuclear Medicine,\nMachine Learning and Data Mining and Biomedical Engineering. An industrial\npatent registered in Spain (ES201431289A), Europe (EP3190542A1) and EEUU\n(US20170287133A1) was also issued, summarizing the efforts of the thesis to\ngenerate tangible assets besides the academic revenue obtained from research\npublications. Finally, the methods, technologies and original ideas conceived\nin this thesis led to the foundation of ONCOANALYTICS CDX, a company framed\ninto the business model of companion diagnostics for pharmaceutical compounds,\nthought as a vehicle to facilitate the industrialization of the ONCOhabitats\ntechnology.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 09:35:01 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Juan-Albarrac\u00edn", "Javier", ""]]}, {"id": "2009.06290", "submitter": "Dario Fuoli", "authors": "Dario Fuoli, Zhiwu Huang, Shuhang Gu, Radu Timofte, Arnau Raventos,\n  Aryan Esfandiari, Salah Karout, Xuan Xu, Xin Li, Xin Xiong, Jinge Wang, Pablo\n  Navarrete Michelini, Wenhao Zhang, Dongyang Zhang, Hanwei Zhu, Dan Xia, Haoyu\n  Chen, Jinjin Gu, Zhi Zhang, Tongtong Zhao, Shanshan Zhao, Kazutoshi Akita,\n  Norimichi Ukita, Hrishikesh P S, Densen Puthussery, and Jiji C V", "title": "AIM 2020 Challenge on Video Extreme Super-Resolution: Methods and\n  Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the video extreme super-resolution challenge associated\nwith the AIM 2020 workshop at ECCV 2020. Common scaling factors for learned\nvideo super-resolution (VSR) do not go beyond factor 4. Missing information can\nbe restored well in this region, especially in HR videos, where the\nhigh-frequency content mostly consists of texture details. The task in this\nchallenge is to upscale videos with an extreme factor of 16, which results in\nmore serious degradations that also affect the structural integrity of the\nvideos. A single pixel in the low-resolution (LR) domain corresponds to 256\npixels in the high-resolution (HR) domain. Due to this massive information\nloss, it is hard to accurately restore the missing information. Track 1 is set\nup to gauge the state-of-the-art for such a demanding task, where fidelity to\nthe ground truth is measured by PSNR and SSIM. Perceptually higher quality can\nbe achieved in trade-off for fidelity by generating plausible high-frequency\ncontent. Track 2 therefore aims at generating visually pleasing results, which\nare ranked according to human perception, evaluated by a user study. In\ncontrast to single image super-resolution (SISR), VSR can benefit from\nadditional information in the temporal domain. However, this also imposes an\nadditional requirement, as the generated frames need to be consistent along\ntime.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 09:36:25 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Fuoli", "Dario", ""], ["Huang", "Zhiwu", ""], ["Gu", "Shuhang", ""], ["Timofte", "Radu", ""], ["Raventos", "Arnau", ""], ["Esfandiari", "Aryan", ""], ["Karout", "Salah", ""], ["Xu", "Xuan", ""], ["Li", "Xin", ""], ["Xiong", "Xin", ""], ["Wang", "Jinge", ""], ["Michelini", "Pablo Navarrete", ""], ["Zhang", "Wenhao", ""], ["Zhang", "Dongyang", ""], ["Zhu", "Hanwei", ""], ["Xia", "Dan", ""], ["Chen", "Haoyu", ""], ["Gu", "Jinjin", ""], ["Zhang", "Zhi", ""], ["Zhao", "Tongtong", ""], ["Zhao", "Shanshan", ""], ["Akita", "Kazutoshi", ""], ["Ukita", "Norimichi", ""], ["S", "Hrishikesh P", ""], ["Puthussery", "Densen", ""], ["C", "Jiji", "V"]]}, {"id": "2009.06292", "submitter": "Murat Kirtay", "authors": "Murat Kirtay and Guido Schillaci and Verena V. Hafner", "title": "A Multisensory Learning Architecture for Rotation-invariant Object\n  Recognition", "comments": "The manuscript consists of 8 pages with 6 figures and two results\n  tables. Additionally, we provide a dedicated website to reach the dataset\n  that we employed for this study: http://www.robotmultimodal.com/datasets/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a multisensory machine learning architecture for object\nrecognition by employing a novel dataset that was constructed with the iCub\nrobot, which is equipped with three cameras and a depth sensor. The proposed\narchitecture combines convolutional neural networks to form representations\n(i.e., features) for grayscaled color images and a multi-layer perceptron\nalgorithm to process depth data. To this end, we aimed to learn joint\nrepresentations of different modalities (e.g., color and depth) and employ them\nfor recognizing objects. We evaluate the performance of the proposed\narchitecture by benchmarking the results obtained with the models trained\nseparately with the input of different sensors and a state-of-the-art data\nfusion technique, namely decision level fusion. The results show that our\narchitecture improves the recognition accuracy compared with the models that\nuse inputs from a single modality and decision level multimodal fusion method.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 09:39:48 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Kirtay", "Murat", ""], ["Schillaci", "Guido", ""], ["Hafner", "Verena V.", ""]]}, {"id": "2009.06295", "submitter": "Hassan Ahmed Sial", "authors": "Hassan Sial, Ramon Baldrich, Maria Vanrell", "title": "Deep intrinsic decomposition trained on surreal scenes yet with\n  realistic light effects", "comments": null, "journal-ref": "JOSA A 2020", "doi": "10.1364/JOSAA.37.000001", "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of intrinsic images still remains a challenging task due to\nweaknesses of ground-truth datasets, which either are too small or present\nnon-realistic issues. On the other hand, end-to-end deep learning architectures\nstart to achieve interesting results that we believe could be improved if\nimportant physical hints were not ignored. In this work, we present a twofold\nframework: (a) a flexible generation of images overcoming some classical\ndataset problems such as larger size jointly with coherent lighting appearance;\nand (b) a flexible architecture tying physical properties through intrinsic\nlosses. Our proposal is versatile, presents low computation time, and achieves\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 09:45:49 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Sial", "Hassan", ""], ["Baldrich", "Ramon", ""], ["Vanrell", "Maria", ""]]}, {"id": "2009.06308", "submitter": "Ruben Tolosana", "authors": "Ruben Tolosana, Paula Delgado-Santos, Andres Perez-Uribe, Ruben\n  Vera-Rodriguez, Julian Fierrez, Aythami Morales", "title": "DeepWriteSYN: On-Line Handwriting Synthesis via Deep Short-Term\n  Representations", "comments": null, "journal-ref": "Proc. 35th AAAI Conference on Artificial Intelligence, 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This study proposes DeepWriteSYN, a novel on-line handwriting synthesis\napproach via deep short-term representations. It comprises two modules: i) an\noptional and interchangeable temporal segmentation, which divides the\nhandwriting into short-time segments consisting of individual or multiple\nconcatenated strokes; and ii) the on-line synthesis of those short-time\nhandwriting segments, which is based on a sequence-to-sequence Variational\nAutoencoder (VAE). The main advantages of the proposed approach are that the\nsynthesis is carried out in short-time segments (that can run from a character\nfraction to full characters) and that the VAE can be trained on a configurable\nhandwriting dataset. These two properties give a lot of flexibility to our\nsynthesiser, e.g., as shown in our experiments, DeepWriteSYN can generate\nrealistic handwriting variations of a given handwritten structure corresponding\nto the natural variation within a given population or a given subject. These\ntwo cases are developed experimentally for individual digits and handwriting\nsignatures, respectively, achieving in both cases remarkable results.\n  Also, we provide experimental results for the task of on-line signature\nverification showing the high potential of DeepWriteSYN to improve\nsignificantly one-shot learning scenarios. To the best of our knowledge, this\nis the first synthesis approach capable of generating realistic on-line\nhandwriting in the short term (including handwritten signatures) via deep\nlearning. This can be very useful as a module toward long-term realistic\nhandwriting generation either completely synthetic or as natural variation of\ngiven handwriting samples.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 10:17:55 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 11:03:11 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Tolosana", "Ruben", ""], ["Delgado-Santos", "Paula", ""], ["Perez-Uribe", "Andres", ""], ["Vera-Rodriguez", "Ruben", ""], ["Fierrez", "Julian", ""], ["Morales", "Aythami", ""]]}, {"id": "2009.06357", "submitter": "Jairo Arturo Ayala Godoy Jairo A. Ayala-Godoy", "authors": "Jairo A. Ayala-Godoy, Rosa E. Lillo, Juan Romo", "title": "Automatic elimination of the pectoral muscle in mammograms based on\n  anatomical features", "comments": null, "journal-ref": "International Journal of Computer Science Issues; 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital mammogram inspection is the most popular technique for early\ndetection of abnormalities in human breast tissue. When mammograms are analyzed\nthrough a computational method, the presence of the pectoral muscle might\naffect the results of breast lesions detection. This problem is particularly\nevident in the mediolateral oblique view (MLO), where pectoral muscle occupies\na large part of the mammography. Therefore, identifying and eliminating the\npectoral muscle are essential steps for improving the automatic discrimination\nof breast tissue. In this paper, we propose an approach based on anatomical\nfeatures to tackle this problem. Our method consists of two steps: (1) a\nprocess to remove the noisy elements such as labels, markers, scratches and\nwedges, and (2) application of an intensity transformation based on the Beta\ndistribution. The novel methodology is tested with 322 digital mammograms from\nthe Mammographic Image Analysis Society (mini-MIAS) database and with a set of\n84 mammograms for which the area normalized error was previously calculated.\nThe results show a very good performance of the method.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 20:36:46 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Ayala-Godoy", "Jairo A.", ""], ["Lillo", "Rosa E.", ""], ["Romo", "Juan", ""]]}, {"id": "2009.06360", "submitter": "Yuxin Mao", "authors": "Zhexiong Wan, Yuxin Mao, Yuchao Dai", "title": "PRAFlow_RVC: Pyramid Recurrent All-Pairs Field Transforms for Optical\n  Flow Estimation in Robust Vision Challenge 2020", "comments": "ECCV 2020 workshop, Robust Vision Challenge, 2nd place of Optical\n  Flow track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical flow estimation is an important computer vision task, which aims at\nestimating the dense correspondences between two frames. RAFT (Recurrent All\nPairs Field Transforms) currently represents the state-of-the-art in optical\nflow estimation. It has excellent generalization ability and has obtained\noutstanding results across several benchmarks. To further improve the\nrobustness and achieve accurate optical flow estimation, we present PRAFlow\n(Pyramid Recurrent All-Pairs Flow), which builds upon the pyramid network\nstructure. Due to computational limitation, our proposed network structure only\nuses two pyramid layers. At each layer, the RAFT unit is used to estimate the\noptical flow at the current resolution. Our model was trained on several\nsimulate and real-image datasets, submitted to multiple leaderboards using the\nsame model and parameters, and won the 2nd place in the optical flow task of\nECCV 2020 workshop: Robust Vision Challenge.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 12:27:52 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Wan", "Zhexiong", ""], ["Mao", "Yuxin", ""], ["Dai", "Yuchao", ""]]}, {"id": "2009.06364", "submitter": "Patrick Wenzel", "authors": "Patrick Wenzel, Rui Wang, Nan Yang, Qing Cheng, Qadeer Khan, Lukas von\n  Stumberg, Niclas Zeller, Daniel Cremers", "title": "4Seasons: A Cross-Season Dataset for Multi-Weather SLAM in Autonomous\n  Driving", "comments": "German Conference on Pattern Recognition (GCPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel dataset covering seasonal and challenging perceptual\nconditions for autonomous driving. Among others, it enables research on visual\nodometry, global place recognition, and map-based re-localization tracking. The\ndata was collected in different scenarios and under a wide variety of weather\nconditions and illuminations, including day and night. This resulted in more\nthan 350 km of recordings in nine different environments ranging from\nmulti-level parking garage over urban (including tunnels) to countryside and\nhighway. We provide globally consistent reference poses with up-to centimeter\naccuracy obtained from the fusion of direct stereo visual-inertial odometry\nwith RTK-GNSS. The full dataset is available at\nhttps://www.4seasons-dataset.com.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 12:31:20 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 13:30:00 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Wenzel", "Patrick", ""], ["Wang", "Rui", ""], ["Yang", "Nan", ""], ["Cheng", "Qing", ""], ["Khan", "Qadeer", ""], ["von Stumberg", "Lukas", ""], ["Zeller", "Niclas", ""], ["Cremers", "Daniel", ""]]}, {"id": "2009.06382", "submitter": "Qihao Zhao", "authors": "Wei Hu, QiHao Zhao, Yangyu Huang and Fan Zhang", "title": "P-DIFF: Learning Classifier with Noisy Labels based on Probability\n  Difference Distributions", "comments": "ICPR 2020. Codes are available at https://github.com/fistyee/P-DIFF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning deep neural network (DNN) classifier with noisy labels is a\nchallenging task because the DNN can easily over-fit on these noisy labels due\nto its high capability. In this paper, we present a very simple but effective\ntraining paradigm called P-DIFF, which can train DNN classifiers but obviously\nalleviate the adverse impact of noisy labels. Our proposed probability\ndifference distribution implicitly reflects the probability of a training\nsample to be clean, then this probability is employed to re-weight the\ncorresponding sample during the training process. P-DIFF can also achieve good\nperformance even without prior knowledge on the noise rate of training samples.\nExperiments on benchmark datasets also demonstrate that P-DIFF is superior to\nthe state-of-the-art sample selection methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 12:35:54 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 07:28:32 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Hu", "Wei", ""], ["Zhao", "QiHao", ""], ["Huang", "Yangyu", ""], ["Zhang", "Fan", ""]]}, {"id": "2009.06385", "submitter": "Faik Boray Tek", "authors": "F. Boray Tek, \\.Ilker \\c{C}am, Deniz Karl{\\i}", "title": "Adaptive Convolution Kernel for Artificial Neural Networks", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many deep neural networks are built by using stacked convolutional layers of\nfixed and single size (often 3$\\times$3) kernels. This paper describes a method\nfor training the size of convolutional kernels to provide varying size kernels\nin a single layer. The method utilizes a differentiable, and therefore\nbackpropagation-trainable Gaussian envelope which can grow or shrink in a base\ngrid. Our experiments compared the proposed adaptive layers to ordinary\nconvolution layers in a simple two-layer network, a deeper residual network,\nand a U-Net architecture. The results in the popular image classification\ndatasets such as MNIST, MNIST-CLUTTERED, CIFAR-10, Fashion, and ``Faces in the\nWild'' showed that the adaptive kernels can provide statistically significant\nimprovements on ordinary convolution kernels. A segmentation experiment in the\nOxford-Pets dataset demonstrated that replacing a single ordinary convolution\nlayer in a U-shaped network with a single 7$\\times$7 adaptive layer can improve\nits learning performance and ability to generalize.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 12:36:50 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Tek", "F. Boray", ""], ["\u00c7am", "\u0130lker", ""], ["Karl\u0131", "Deniz", ""]]}, {"id": "2009.06415", "submitter": "Alexandre Lacoste", "authors": "Alexandre Lacoste, Pau Rodr\\'iguez, Fr\\'ed\\'eric Branchaud-Charron,\n  Parmida Atighehchian, Massimo Caccia, Issam Laradji, Alexandre Drouin, Matt\n  Craddock, Laurent Charlin, David V\\'azquez", "title": "Synbols: Probing Learning Algorithms with Synthetic Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in the field of machine learning has been fueled by the introduction\nof benchmark datasets pushing the limits of existing algorithms. Enabling the\ndesign of datasets to test specific properties and failure modes of learning\nalgorithms is thus a problem of high interest, as it has a direct impact on\ninnovation in the field. In this sense, we introduce Synbols -- Synthetic\nSymbols -- a tool for rapidly generating new datasets with a rich composition\nof latent features rendered in low resolution images. Synbols leverages the\nlarge amount of symbols available in the Unicode standard and the wide range of\nartistic font provided by the open font community. Our tool's high-level\ninterface provides a language for rapidly generating new distributions on the\nlatent features, including various types of textures and occlusions. To\nshowcase the versatility of Synbols, we use it to dissect the limitations and\nflaws in standard learning algorithms in various learning setups including\nsupervised learning, active learning, out of distribution generalization,\nunsupervised representation learning, and object counting.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 13:03:27 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 21:57:37 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Lacoste", "Alexandre", ""], ["Rodr\u00edguez", "Pau", ""], ["Branchaud-Charron", "Fr\u00e9d\u00e9ric", ""], ["Atighehchian", "Parmida", ""], ["Caccia", "Massimo", ""], ["Laradji", "Issam", ""], ["Drouin", "Alexandre", ""], ["Craddock", "Matt", ""], ["Charlin", "Laurent", ""], ["V\u00e1zquez", "David", ""]]}, {"id": "2009.06420", "submitter": "Deepak Babu Sam", "authors": "Deepak Babu Sam, Abhinav Agarwalla, Jimmy Joseph, Vishwanath A.\n  Sindagi, R. Venkatesh Babu, Vishal M. Patel", "title": "Completely Self-Supervised Crowd Counting via Distribution Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense crowd counting is a challenging task that demands millions of head\nannotations for training models. Though existing self-supervised approaches\ncould learn good representations, they require some labeled data to map these\nfeatures to the end task of density estimation. We mitigate this issue with the\nproposed paradigm of complete self-supervision, which does not need even a\nsingle labeled image. The only input required to train, apart from a large set\nof unlabeled crowd images, is the approximate upper limit of the crowd count\nfor the given dataset. Our method dwells on the idea that natural crowds follow\na power law distribution, which could be leveraged to yield error signals for\nbackpropagation. A density regressor is first pretrained with self-supervision\nand then the distribution of predictions is matched to the prior by optimizing\nSinkhorn distance between the two. Experiments show that this results in\neffective learning of crowd features and delivers significant counting\nperformance. Furthermore, we establish the superiority of our method in less\ndata setting as well. The code and models for our approach is available at\nhttps://github.com/val-iisc/css-ccnn.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 13:20:12 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Sam", "Deepak Babu", ""], ["Agarwalla", "Abhinav", ""], ["Joseph", "Jimmy", ""], ["Sindagi", "Vishwanath A.", ""], ["Babu", "R. Venkatesh", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2009.06432", "submitter": "Ujwal Krothapalli", "authors": "Ujwal Krothapalli and A. Lynn Abbott", "title": "Adaptive Label Smoothing", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the use of objectness measures to improve the calibration\nperformance of Convolutional Neural Networks (CNNs). CNNs have proven to be\nvery good classifiers and generally localize objects well; however, the loss\nfunctions typically used to train classification CNNs do not penalize inability\nto localize an object, nor do they take into account an object's relative size\nin the given image. During training on ImageNet-1K almost all approaches use\nrandom crops on the images and this transformation sometimes provides the CNN\nwith background only samples. This causes the classifiers to depend on context.\nContext dependence is harmful for safety-critical applications. We present a\nnovel approach to classification that combines the ideas of objectness and\nlabel smoothing during training. Unlike previous methods, we compute a\nsmoothing factor that is \\emph{adaptive} based on relative object size within\nan image. This causes our approach to produce confidences that are grounded in\nthe size of the object being classified instead of relying on context to make\nthe correct predictions. We present extensive results using ImageNet to\ndemonstrate that CNNs trained using adaptive label smoothing are much less\nlikely to be overconfident in their predictions. We show qualitative results\nusing class activation maps and quantitative results using classification and\ntransfer learning tasks. Our approach is able to produce an order of magnitude\nreduction in confidence when predicting on context only images when compared to\nbaselines. Using transfer learning, we gain 2.1mAP on MS COCO compared to the\nhard label approach.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 13:37:30 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 23:19:08 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Krothapalli", "Ujwal", ""], ["Abbott", "A. Lynn", ""]]}, {"id": "2009.06435", "submitter": "Shih-Yuan Yu", "authors": "Shih-Yuan Yu, Arnav V. Malawade, Deepan Muthirayan, Pramod P.\n  Khargonekar, Mohammad A. Al Faruque", "title": "Scene-Graph Augmented Data-Driven Risk Assessment of Autonomous Vehicle\n  Decisions", "comments": "10 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite impressive advancements in Autonomous Driving Systems (ADS),\nnavigation in complex road conditions remains a challenging problem. There is\nconsiderable evidence that evaluating the subjective risk level of various\ndecisions can improve ADS' safety in both normal and complex driving scenarios.\nHowever, existing deep learning-based methods often fail to model the\nrelationships between traffic participants and can suffer when faced with\ncomplex real-world scenarios. Besides, these methods lack transferability and\nexplainability. To address these limitations, we propose a novel data-driven\napproach that uses scene-graphs as intermediate representations. Our approach\nincludes a Multi-Relation Graph Convolution Network, a Long-Short Term Memory\nNetwork, and attention layers for modeling the subjective risk of driving\nmaneuvers. To train our model, we formulate this task as a supervised scene\nclassification problem. We consider a typical use case to demonstrate our\nmodel's capabilities: lane changes. We show that our approach achieves a higher\nclassification accuracy than the state-of-the-art approach on both large (96.4%\nvs. 91.2%) and small (91.8% vs. 71.2%) synthesized datasets, also illustrating\nthat our approach can learn effectively even from smaller datasets. We also\nshow that our model trained on a synthesized dataset achieves an average\naccuracy of 87.8% when tested on a real-world dataset compared to the 70.3%\naccuracy achieved by the state-of-the-art model trained on the same synthesized\ndataset, showing that our approach can more effectively transfer knowledge.\nFinally, we demonstrate that the use of spatial and temporal attention layers\nimproves our model's performance by 2.7% and 0.7% respectively, and increases\nits explainability.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 07:41:27 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Yu", "Shih-Yuan", ""], ["Malawade", "Arnav V.", ""], ["Muthirayan", "Deepan", ""], ["Khargonekar", "Pramod P.", ""], ["Faruque", "Mohammad A. Al", ""]]}, {"id": "2009.06456", "submitter": "Qingsong Yao", "authors": "Qingsong Yao, Li Xiao, Peihang Liu and S. Kevin Zhou", "title": "Label-Free Segmentation of COVID-19 Lesions in Lung CT", "comments": "Accepted by Transaction on Medical Imaging 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scarcity of annotated images hampers the building of automated solution for\nreliable COVID-19 diagnosis and evaluation from CT. To alleviate the burden of\ndata annotation, we herein present a label-free approach for segmenting\nCOVID-19 lesions in CT via pixel-level anomaly modeling that mines out the\nrelevant knowledge from normal CT lung scans. Our modeling is inspired by the\nobservation that the parts of tracheae and vessels, which lay in the\nhigh-intensity range where lesions belong to, exhibit strong patterns. To\nfacilitate the learning of such patterns at a pixel level, we synthesize\n`lesions' using a set of surprisingly simple operations and insert the\nsynthesized `lesions' into normal CT lung scans to form training pairs, from\nwhich we learn a normalcy-converting network (NormNet) that turns an 'abnormal'\nimage back to normal. Our experiments on three different datasets validate the\neffectiveness of NormNet, which conspicuously outperforms a variety of\nunsupervised anomaly detection (UAD) methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 12:38:34 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 13:33:32 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 03:01:58 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Yao", "Qingsong", ""], ["Xiao", "Li", ""], ["Liu", "Peihang", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2009.06469", "submitter": "Yusuf H. Sahin", "authors": "Vahit Bugra Yesilkaynak, Yusuf H. Sahin, Gozde Unal", "title": "EfficientSeg: An Efficient Semantic Segmentation Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Deep neural network training without pre-trained weights and few data is\nshown to need more training iterations. It is also known that, deeper models\nare more successful than their shallow counterparts for semantic segmentation\ntask. Thus, we introduce EfficientSeg architecture, a modified and scalable\nversion of U-Net, which can be efficiently trained despite its depth. We\nevaluated EfficientSeg architecture on Minicity dataset and outperformed U-Net\nbaseline score (40% mIoU) using the same parameter count (51.5% mIoU). Our most\nsuccessful model obtained 58.1% mIoU score and got the fourth place in semantic\nsegmentation track of ECCV 2020 VIPriors challenge.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 14:25:19 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 14:36:24 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Yesilkaynak", "Vahit Bugra", ""], ["Sahin", "Yusuf H.", ""], ["Unal", "Gozde", ""]]}, {"id": "2009.06483", "submitter": "Tobias Ringwald", "authors": "Tobias Ringwald, Rainer Stiefelhagen", "title": "Unsupervised Domain Adaptation by Uncertain Feature Alignment", "comments": "Accepted at the 31st British Machine Vision Virtual Conference (BMVC\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) deals with the adaptation of models from\na given source domain with labeled data to an unlabeled target domain. In this\npaper, we utilize the inherent prediction uncertainty of a model to accomplish\nthe domain adaptation task. The uncertainty is measured by Monte-Carlo dropout\nand used for our proposed Uncertainty-based Filtering and Feature Alignment\n(UFAL) that combines an Uncertain Feature Loss (UFL) function and an\nUncertainty-Based Filtering (UBF) approach for alignment of features in\nEuclidean space. Our method surpasses recently proposed architectures and\nachieves state-of-the-art results on multiple challenging datasets. Code is\navailable on the project website.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 14:42:41 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Ringwald", "Tobias", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2009.06488", "submitter": "Anton Trusov Mr.", "authors": "Anton Trusov, Elena Limonova, Dmitry Slugin, Dmitry Nikolaev, Vladimir\n  V. Arlazarov", "title": "Fast Implementation of 4-bit Convolutional Neural Networks for Mobile\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantized low-precision neural networks are very popular because they require\nless computational resources for inference and can provide high performance,\nwhich is vital for real-time and embedded recognition systems. However, their\nadvantages are apparent for FPGA and ASIC devices, while general-purpose\nprocessor architectures are not always able to perform low-bit integer\ncomputations efficiently. The most frequently used low-precision neural network\nmodel for mobile central processors is an 8-bit quantized network. However, in\na number of cases, it is possible to use fewer bits for weights and\nactivations, and the only problem is the difficulty of efficient\nimplementation. We introduce an efficient implementation of 4-bit matrix\nmultiplication for quantized neural networks and perform time measurements on a\nmobile ARM processor. It shows 2.9 times speedup compared to standard\nfloating-point multiplication and is 1.5 times faster than 8-bit quantized one.\nWe also demonstrate a 4-bit quantized neural network for OCR recognition on the\nMIDV-500 dataset. 4-bit quantization gives 95.0% accuracy and 48% overall\ninference speedup, while an 8-bit quantized network gives 95.4% accuracy and\n39% speedup. The results show that 4-bit quantization perfectly suits mobile\ndevices, yielding good enough accuracy and low inference time.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 14:48:40 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 15:23:20 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Trusov", "Anton", ""], ["Limonova", "Elena", ""], ["Slugin", "Dmitry", ""], ["Nikolaev", "Dmitry", ""], ["Arlazarov", "Vladimir V.", ""]]}, {"id": "2009.06502", "submitter": "Viresh Ranjan", "authors": "Raji Annadi, Yupei Chen, Viresh Ranjan, Dimitris Samaras, Gregory\n  Zelinsky, Minh Hoai", "title": "A Study of Human Gaze Behavior During Visual Crowd Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe our study on how humans allocate their attention\nduring visual crowd counting. Using an eye tracker, we collect gaze behavior of\nhuman participants who are tasked with counting the number of people in crowd\nimages. Analyzing the collected gaze behavior of ten human participants on\nthirty crowd images, we observe some common approaches for visual counting. For\nan image of a small crowd, the approach is to enumerate over all people or\ngroups of people in the crowd, and this explains the high level of similarity\nbetween the fixation density maps of different human participants. For an image\nof a large crowd, our participants tend to focus on one section of the image,\ncount the number of people in that section, and then extrapolate to the other\nsections. In terms of count accuracy, our human participants are not as good at\nthe counting task, compared to the performance of the current state-of-the-art\ncomputer algorithms. Interestingly, there is a tendency to under count the\nnumber of people in all crowd images. Gaze behavior data and images can be\ndownloaded from\nhttps://www3.cs.stonybrook.edu/~minhhoai/projects/crowd_counting_gaze/.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 15:05:13 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 19:47:50 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Annadi", "Raji", ""], ["Chen", "Yupei", ""], ["Ranjan", "Viresh", ""], ["Samaras", "Dimitris", ""], ["Zelinsky", "Gregory", ""], ["Hoai", "Minh", ""]]}, {"id": "2009.06529", "submitter": "Jonas Wulff", "authors": "Jonas Wulff and Antonio Torralba", "title": "Improving Inversion and Generation Diversity in StyleGAN using a\n  Gaussianized Latent Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Generative Adversarial Networks are capable of creating artificial,\nphotorealistic images from latent vectors living in a low-dimensional learned\nlatent space. It has been shown that a wide range of images can be projected\ninto this space, including images outside of the domain that the generator was\ntrained on. However, while in this case the generator reproduces the pixels and\ntextures of the images, the reconstructed latent vectors are unstable and small\nperturbations result in significant image distortions. In this work, we propose\nto explicitly model the data distribution in latent space. We show that, under\na simple nonlinear operation, the data distribution can be modeled as Gaussian\nand therefore expressed using sufficient statistics. This yields a simple\nGaussian prior, which we use to regularize the projection of images into the\nlatent space. The resulting projections lie in smoother and better behaved\nregions of the latent space, as shown using interpolation performance for both\nreal and generated images. Furthermore, the Gaussian model of the distribution\nin latent space allows us to investigate the origins of artifacts in the\ngenerator output, and provides a method for reducing these artifacts while\nmaintaining diversity of the generated images.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 15:45:58 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Wulff", "Jonas", ""], ["Torralba", "Antonio", ""]]}, {"id": "2009.06549", "submitter": "Mark Kliger", "authors": "Imry Kissos, Lior Fritz, Matan Goldman, Omer Meir, Eduard Oks and Mark\n  Kliger", "title": "Beyond Weak Perspective for Monocular 3D Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of 3D joints location and orientation prediction from a\nmonocular video with the skinned multi-person linear (SMPL) model. We first\ninfer 2D joints locations with an off-the-shelf pose estimation algorithm. We\nuse the SPIN algorithm and estimate initial predictions of body pose, shape and\ncamera parameters from a deep regression neural network. We then adhere to the\nSMPLify algorithm which receives those initial parameters, and optimizes them\nso that inferred 3D joints from the SMPL model would fit the 2D joints\nlocations. This algorithm involves a projection step of 3D joints to the 2D\nimage plane. The conventional approach is to follow weak perspective\nassumptions which use ad-hoc focal length. Through experimentation on the 3D\nPoses in the Wild (3DPW) dataset, we show that using full perspective\nprojection, with the correct camera center and an approximated focal length,\nprovides favorable results. Our algorithm has resulted in a winning entry for\nthe 3DPW Challenge, reaching first place in joints orientation accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 16:23:14 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Kissos", "Imry", ""], ["Fritz", "Lior", ""], ["Goldman", "Matan", ""], ["Meir", "Omer", ""], ["Oks", "Eduard", ""], ["Kliger", "Mark", ""]]}, {"id": "2009.06586", "submitter": "Yunhao Ge", "authors": "Yunhao Ge, Sami Abu-El-Haija, Gan Xin and Laurent Itti", "title": "Zero-shot Synthesis with Group-Supervised Learning", "comments": "Published at ICLR 2021 (16 pages including appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual cognition of primates is superior to that of artificial neural\nnetworks in its ability to 'envision' a visual object, even a newly-introduced\none, in different attributes including pose, position, color, texture, etc. To\naid neural networks to envision objects with different attributes, we propose a\nfamily of objective functions, expressed on groups of examples, as a novel\nlearning framework that we term Group-Supervised Learning (GSL). GSL allows us\nto decompose inputs into a disentangled representation with swappable\ncomponents, that can be recombined to synthesize new samples. For instance,\nimages of red boats & blue cars can be decomposed and recombined to synthesize\nnovel images of red cars. We propose an implementation based on auto-encoder,\ntermed group-supervised zero-shot synthesis network (GZS-Net) trained with our\nlearning framework, that can produce a high-quality red car even if no such\nexample is witnessed during training. We test our model and learning framework\non existing benchmarks, in addition to anew dataset that we open-source. We\nqualitatively and quantitatively demonstrate that GZS-Net trained with GSL\noutperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 17:17:49 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 19:43:03 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2021 21:19:12 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Ge", "Yunhao", ""], ["Abu-El-Haija", "Sami", ""], ["Xin", "Gan", ""], ["Itti", "Laurent", ""]]}, {"id": "2009.06599", "submitter": "Yue Bai", "authors": "Yue Bai, Zhiqiang Tao, Lichen Wang, Sheng Li, Yu Yin and Yun Fu", "title": "Collaborative Attention Mechanism for Multi-View Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view action recognition (MVAR) leverages complementary temporal\ninformation from different views to improve the learning performance. Obtaining\ninformative view-specific representation plays an essential role in MVAR.\nAttention has been widely adopted as an effective strategy for discovering\ndiscriminative cues underlying temporal data. However, most existing MVAR\nmethods only utilize attention to extract representation for each view\nindividually, ignoring the potential to dig latent patterns based on\nmutual-support information in attention space. To this end, we propose a\ncollaborative attention mechanism (CAM) for solving the MVAR problem in this\npaper. The proposed CAM detects the attention differences among multi-view, and\nadaptively integrates frame-level information to benefit each other.\nSpecifically, we extend the long short-term memory (LSTM) to a Mutual-Aid RNN\n(MAR) to achieve the multi-view collaboration process. CAM takes advantages of\nview-specific attention pattern to guide another view and discover potential\ninformation which is hard to be explored by itself. It paves a novel way to\nleverage attention information and enhances the multi-view representation\nlearning. Extensive experiments on four action datasets illustrate the proposed\nCAM achieves better results for each view and also boosts multi-view\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 17:33:10 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 20:30:54 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Bai", "Yue", ""], ["Tao", "Zhiqiang", ""], ["Wang", "Lichen", ""], ["Li", "Sheng", ""], ["Yin", "Yu", ""], ["Fu", "Yun", ""]]}, {"id": "2009.06604", "submitter": "Zibo Meng", "authors": "Zibo Meng, Runsheng Xu, Chiu Man Ho", "title": "GIA-Net: Global Information Aware Network for Low-light Imaging", "comments": "16 pages 6 figures; accepted to AIM at ECCV 2020", "journal-ref": "Computer Vision -- ECCV 2020 Workshops, 2020, 327--342", "doi": "10.1007/978-3-030-67070-2_20", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is extremely challenging to acquire perceptually plausible images under\nlow-light conditions due to low SNR. Most recently, U-Nets have shown promising\nresults for low-light imaging. However, vanilla U-Nets generate images with\nartifacts such as color inconsistency due to the lack of global color\ninformation. In this paper, we propose a global information aware (GIA) module,\nwhich is capable of extracting and integrating the global information into the\nnetwork to improve the performance of low-light imaging. The GIA module can be\ninserted into a vanilla U-Net with negligible extra learnable parameters or\ncomputational cost. Moreover, a GIA-Net is constructed, trained and evaluated\non a large scale real-world low-light imaging dataset. Experimental results\nshow that the proposed GIA-Net outperforms the state-of-the-art methods in\nterms of four metrics, including deep metrics that measure perceptual\nsimilarities. Extensive ablation studies have been conducted to verify the\neffectiveness of the proposed GIA-Net for low-light imaging by utilizing global\ninformation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 17:38:38 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Meng", "Zibo", ""], ["Xu", "Runsheng", ""], ["Ho", "Chiu Man", ""]]}, {"id": "2009.06610", "submitter": "Chuhan Zhang", "authors": "Chuhan Zhang, Ankush Gupta, Andrew Zisserman", "title": "Adaptive Text Recognition through Visual Matching", "comments": "ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, our objective is to address the problems of generalization and\nflexibility for text recognition in documents. We introduce a new model that\nexploits the repetitive nature of characters in languages, and decouples the\nvisual representation learning and linguistic modelling stages. By doing this,\nwe turn text recognition into a shape matching problem, and thereby achieve\ngeneralization in appearance and flexibility in classes. We evaluate the new\nmodel on both synthetic and real datasets across different alphabets and show\nthat it can handle challenges that traditional architectures are not able to\nsolve without expensive retraining, including: (i) it can generalize to unseen\nfonts without new exemplars from them; (ii) it can flexibly change the number\nof classes, simply by changing the exemplars provided; and (iii) it can\ngeneralize to new languages and new characters that it has not been trained for\nby providing a new glyph set. We show significant improvements over\nstate-of-the-art models for all these cases.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 17:48:53 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Zhang", "Chuhan", ""], ["Gupta", "Ankush", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2009.06613", "submitter": "Haichao Yu", "authors": "Haichao Yu, Ning Xu, Zilong Huang, Yuqian Zhou, Humphrey Shi", "title": "High-Resolution Deep Image Matting", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image matting is a key technique for image and video editing and composition.\nConventionally, deep learning approaches take the whole input image and an\nassociated trimap to infer the alpha matte using convolutional neural networks.\nSuch approaches set state-of-the-arts in image matting; however, they may fail\nin real-world matting applications due to hardware limitations, since\nreal-world input images for matting are mostly of very high resolution. In this\npaper, we propose HDMatt, a first deep learning based image matting approach\nfor high-resolution inputs. More concretely, HDMatt runs matting in a\npatch-based crop-and-stitch manner for high-resolution inputs with a novel\nmodule design to address the contextual dependency and consistency issues\nbetween different patches. Compared with vanilla patch-based inference which\ncomputes each patch independently, we explicitly model the cross-patch\ncontextual dependency with a newly-proposed Cross-Patch Contextual module (CPC)\nguided by the given trimap. Extensive experiments demonstrate the effectiveness\nof the proposed method and its necessity for high-resolution inputs. Our HDMatt\napproach also sets new state-of-the-art performance on Adobe Image Matting and\nAlphaMatting benchmarks and produce impressive visual results on more\nreal-world high-resolution images.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 17:53:15 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 08:14:55 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Yu", "Haichao", ""], ["Xu", "Ning", ""], ["Huang", "Zilong", ""], ["Zhou", "Yuqian", ""], ["Shi", "Humphrey", ""]]}, {"id": "2009.06678", "submitter": "Densen Puthussery", "authors": "Densen Puthussery, Hrishikesh P.S., Melvin Kuriakose and Jiji C.V", "title": "WDRN : A Wavelet Decomposed RelightNet for Image Relighting", "comments": "Presented at ECCV-2020 AIM workshop, 14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The task of recalibrating the illumination settings in an image to a target\nconfiguration is known as relighting. Relighting techniques have potential\napplications in digital photography, gaming industry and in augmented reality.\nIn this paper, we address the one-to-one relighting problem where an image at a\ntarget illumination settings is predicted given an input image with specific\nillumination conditions. To this end, we propose a wavelet decomposed\nRelightNet called WDRN which is a novel encoder-decoder network employing\nwavelet based decomposition followed by convolution layers under a\nmuti-resolution framework. We also propose a novel loss function called gray\nloss that ensures efficient learning of gradient in illumination along\ndifferent directions of the ground truth image giving rise to visually superior\nrelit images. The proposed solution won the first position in the relighting\nchallenge event in advances in image manipulation (AIM) 2020 workshop which\nproves its effectiveness measured in terms of a Mean Perceptual Score which in\nturn is measured using SSIM and a Learned Perceptual Image Patch Similarity\nscore.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 18:23:10 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Puthussery", "Densen", ""], ["S.", "Hrishikesh P.", ""], ["Kuriakose", "Melvin", ""], ["C.", "Jiji", "V"]]}, {"id": "2009.06679", "submitter": "Mohamed Nafzi", "authors": "Mohamed Nafzi, Michael Brauckmann, Tobias Glasmachers", "title": "Data Augmentation and Clustering for Vehicle Make/Model Classification", "comments": "Proceedings of the 2020 Computing Conference, Volume 1-3, SAI 16-17\n  July 2020 London", "journal-ref": null, "doi": "10.1007/978-3-030-52249-0_24", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Vehicle shape information is very important in Intelligent Traffic Systems\n(ITS). In this paper we present a way to exploit a training data set of\nvehicles released in different years and captured under different perspectives.\nAlso the efficacy of clustering to enhance the make/model classification is\npresented. Both steps led to improved classification results and a greater\nrobustness. Deeper convolutional neural network based on ResNet architecture\nhas been designed for the training of the vehicle make/model classification.\nThe unequal class distribution of training data produces an a priori\nprobability. Its elimination, obtained by removing of the bias and through hard\nnormalization of the centroids in the classification layer, improves the\nclassification results. A developed application has been used to test the\nvehicle re-identification on video data manually based on make/model and color\nclassification. This work was partially funded under the grant.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 18:24:31 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Nafzi", "Mohamed", ""], ["Brauckmann", "Michael", ""], ["Glasmachers", "Tobias", ""]]}, {"id": "2009.06680", "submitter": "Ayyappa Pambala", "authors": "Ayyappa Kumar Pambala, Titir Dutta, Soma Biswas", "title": "SML: Semantic Meta-learning for Few-shot Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The significant amount of training data required for training Convolutional\nNeural Networks has become a bottleneck for applications like semantic\nsegmentation. Few-shot semantic segmentation algorithms address this problem,\nwith an aim to achieve good performance in the low-data regime, with few\nannotated training images. Recently, approaches based on class-prototypes\ncomputed from available training data have achieved immense success for this\ntask. In this work, we propose a novel meta-learning framework, Semantic\nMeta-Learning (SML) which incorporates class level semantic descriptions in the\ngenerated prototypes for this problem. In addition, we propose to use the well\nestablished technique, ridge regression, to not only bring in the class-level\nsemantic information, but also to effectively utilise the information available\nfrom multiple images present in the training data for prototype computation.\nThis has a simple closed-form solution, and thus can be implemented easily and\nefficiently. Extensive experiments on the benchmark PASCAL-5i dataset under\ndifferent experimental settings show the effectiveness of the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 18:26:46 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Pambala", "Ayyappa Kumar", ""], ["Dutta", "Titir", ""], ["Biswas", "Soma", ""]]}, {"id": "2009.06687", "submitter": "Mohamed Nafzi", "authors": "Mohamed Nafzi, Michael Brauckmann, Tobias Glasmachers", "title": "Methods of the Vehicle Re-identification", "comments": "Proceedings of the 2020 Intelligent Systems Conference (IntelliSys)\n  Volume 1-3, 3-4 Sep. 2020 Amsterdam", "journal-ref": null, "doi": "10.1007/978-3-030-55180-3_38", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Most of researchers use the vehicle re-identification based on\nclassification. This always requires an update with the new vehicle models in\nthe market. In this paper, two types of vehicle re-identification will be\npresented. First, the standard method, which needs an image from the search\nvehicle. VRIC and VehicleID data set are suitable for training this module. It\nwill be explained in detail how to improve the performance of this method using\na trained network, which is designed for the classification. The second method\ntakes as input a representative image of the search vehicle with similar\nmake/model, released year and colour. It is very useful when an image from the\nsearch vehicle is not available. It produces as output a shape and a colour\nfeatures. This could be used by the matching across a database to re-identify\nvehicles, which look similar to the search vehicle. To get a robust module for\nthe re-identification, a fine-grained classification has been trained, which\nits class consists of four elements: the make of a vehicle refers to the\nvehicle's manufacturer, e.g. Mercedes-Benz, the model of a vehicle refers to\ntype of model within that manufacturer's portfolio, e.g. C Class, the year\nrefers to the iteration of the model, which may receive progressive alterations\nand upgrades by its manufacturer and the perspective of the vehicle. Thus, all\nfour elements describe the vehicle at increasing degree of specificity. The aim\nof the vehicle shape classification is to classify the combination of these\nfour elements. The colour classification has been separately trained. The\nresults of vehicle re-identification will be shown. Using a developed tool, the\nre-identification of vehicles on video images and on controlled data set will\nbe demonstrated. This work was partially funded under the grant.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 18:50:50 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Nafzi", "Mohamed", ""], ["Brauckmann", "Michael", ""], ["Glasmachers", "Tobias", ""]]}, {"id": "2009.06701", "submitter": "Takami Sato", "authors": "Takami Sato, Junjie Shen, Ningfei Wang, Yunhan Jack Jia, Xue Lin, Qi\n  Alfred Chen", "title": "Dirty Road Can Attack: Security of Deep Learning based Automated Lane\n  Centering under Physical-World Attack", "comments": "Accepted to Usenix Security '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated Lane Centering (ALC) systems are convenient and widely deployed\ntoday, but also highly security and safety critical. In this work, we are the\nfirst to systematically study the security of state-of-the-art deep learning\nbased ALC systems in their designed operational domains under physical-world\nadversarial attacks. We formulate the problem with a safety-critical attack\ngoal, and a novel and domain-specific attack vector: dirty road patches. To\nsystematically generate the attack, we adopt an optimization-based approach and\novercome domain-specific design challenges such as camera frame\ninter-dependencies due to attack-influenced vehicle control, and the lack of\nobjective function design for lane detection models.\n  We evaluate our attack on a production ALC using 80 scenarios from real-world\ndriving traces. The results show that our attack is highly effective with over\n97.5% success rates and less than 0.903 sec average success time, which is\nsubstantially lower than the average driver reaction time. This attack is also\nfound (1) robust to various real-world factors such as lighting conditions and\nview angles, (2) general to different model designs, and (3) stealthy from the\ndriver's view. To understand the safety impacts, we conduct experiments using\nsoftware-in-the-loop simulation and attack trace injection in a real vehicle.\nThe results show that our attack can cause a 100% collision rate in different\nscenarios, including when tested with common safety features such as automatic\nemergency braking. We also evaluate and discuss defenses.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 19:22:39 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 22:38:38 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sato", "Takami", ""], ["Shen", "Junjie", ""], ["Wang", "Ningfei", ""], ["Jia", "Yunhan Jack", ""], ["Lin", "Xue", ""], ["Chen", "Qi Alfred", ""]]}, {"id": "2009.06732", "submitter": "Yi Tay", "authors": "Yi Tay, Mostafa Dehghani, Dara Bahri, Donald Metzler", "title": "Efficient Transformers: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer model architectures have garnered immense interest lately due to\ntheir effectiveness across a range of domains like language, vision and\nreinforcement learning. In the field of natural language processing for\nexample, Transformers have become an indispensable staple in the modern deep\nlearning stack. Recently, a dizzying number of \"X-former\" models have been\nproposed - Reformer, Linformer, Performer, Longformer, to name a few - which\nimprove upon the original Transformer architecture, many of which make\nimprovements around computational and memory efficiency. With the aim of\nhelping the avid researcher navigate this flurry, this paper characterizes a\nlarge and thoughtful selection of recent efficiency-flavored \"X-former\" models,\nproviding an organized and comprehensive overview of existing work and models\nacross multiple domains.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 20:38:14 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 07:23:37 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Tay", "Yi", ""], ["Dehghani", "Mostafa", ""], ["Bahri", "Dara", ""], ["Metzler", "Donald", ""]]}, {"id": "2009.06742", "submitter": "Prabuddha Chakraborty", "authors": "Prabuddha Chakraborty, Jonathan Cruz, Swarup Bhunia", "title": "Leveraging Domain Knowledge using Machine Learning for Image Compression\n  in Internet-of-Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergent ecosystems of intelligent edge devices in diverse Internet of\nThings (IoT) applications, from automatic surveillance to precision\nagriculture, increasingly rely on recording and processing variety of image\ndata. Due to resource constraints, e.g., energy and communication bandwidth\nrequirements, these applications require compressing the recorded images before\ntransmission. For these applications, image compression commonly requires: (1)\nmaintaining features for coarse-grain pattern recognition instead of the\nhigh-level details for human perception due to machine-to-machine\ncommunications; (2) high compression ratio that leads to improved energy and\ntransmission efficiency; (3) large dynamic range of compression and an easy\ntrade-off between compression factor and quality of reconstruction to\naccommodate a wide diversity of IoT applications as well as their time-varying\nenergy/performance needs. To address these requirements, we propose, MAGIC, a\nnovel machine learning (ML) guided image compression framework that judiciously\nsacrifices visual quality to achieve much higher compression when compared to\ntraditional techniques, while maintaining accuracy for coarse-grained vision\ntasks. The central idea is to capture application-specific domain knowledge and\nefficiently utilize it in achieving high compression. We demonstrate that the\nMAGIC framework is configurable across a wide range of compression/quality and\nis capable of compressing beyond the standard quality factor limits of both\nJPEG 2000 and WebP. We perform experiments on representative IoT applications\nusing two vision datasets and show up to 42.65x compression at similar accuracy\nwith respect to the source. We highlight low variance in compression rate\nacross images using our technique as compared to JPEG 2000 and WebP.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 20:59:19 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Chakraborty", "Prabuddha", ""], ["Cruz", "Jonathan", ""], ["Bhunia", "Swarup", ""]]}, {"id": "2009.06757", "submitter": "Bo Zhou", "authors": "Bo Zhou, Yu-Jung Tsai, Chi Liu", "title": "Simultaneous Denoising and Motion Estimation for Low-dose Gated PET\n  using a Siamese Adversarial Network with Gate-to-Gate Consistency Learning", "comments": "Accepted at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gating is commonly used in PET imaging to reduce respiratory motion blurring\nand facilitate more sophisticated motion correction methods. In the\napplications of low dose PET, however, reducing injection dose causes increased\nnoise and reduces signal-to-noise ratio (SNR), subsequently corrupting the\nmotion estimation/correction steps, causing inferior image quality. To tackle\nthese issues, we first propose a Siamese adversarial network (SAN) that can\nefficiently recover high dose gated image volume from low dose gated image\nvolume. To ensure the appearance consistency between the recovered gated\nvolumes, we then utilize a pre-trained motion estimation network incorporated\ninto SAN that enables the constraint of gate-to-gate (G2G) consistency. With\nhigh-quality recovered gated volumes, gate-to-gate motion vectors can be\nsimultaneously outputted from the motion estimation network. Comprehensive\nevaluations on a low dose gated PET dataset of 29 subjects demonstrate that our\nmethod can effectively recover the low dose gated PET volumes, with an average\nPSNR of 37.16 and SSIM of 0.97, and simultaneously generate robust motion\nestimation that could benefit subsequent motion corrections.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 21:46:33 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Zhou", "Bo", ""], ["Tsai", "Yu-Jung", ""], ["Liu", "Chi", ""]]}, {"id": "2009.06767", "submitter": "Debanjan Konar Research Scholar", "authors": "Debanjan Konar, Siddhartha Bhattacharyya, Bijaya K. Panigrahi, and\n  Elizabeth Behrman", "title": "Qutrit-inspired Fully Self-supervised Shallow Quantum Learning Network\n  for Brain Tumor Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classical self-supervised networks suffer from convergence problems and\nreduced segmentation accuracy due to forceful termination. Qubits or bi-level\nquantum bits often describe quantum neural network models. In this article, a\nnovel self-supervised shallow learning network model exploiting the\nsophisticated three-level qutrit-inspired quantum information system referred\nto as Quantum Fully Self-Supervised Neural Network (QFS-Net) is presented for\nautomated segmentation of brain MR images. The QFS-Net model comprises a\ntrinity of a layered structure of qutrits inter-connected through parametric\nHadamard gates using an 8-connected second-order neighborhood-based topology.\nThe non-linear transformation of the qutrit states allows the underlying\nquantum neural network model to encode the quantum states, thereby enabling a\nfaster self-organized counter-propagation of these states between the layers\nwithout supervision. The suggested QFS-Net model is tailored and extensively\nvalidated on Cancer Imaging Archive (TCIA) data set collected from Nature\nrepository and also compared with state of the art supervised (U-Net and\nURes-Net architectures) and the self-supervised QIS-Net model. Results shed\npromising segmented outcome in detecting tumors in terms of dice similarity and\naccuracy with minimum human intervention and computational resources.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 22:15:22 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Konar", "Debanjan", ""], ["Bhattacharyya", "Siddhartha", ""], ["Panigrahi", "Bijaya K.", ""], ["Behrman", "Elizabeth", ""]]}, {"id": "2009.06808", "submitter": "Timoleon Moraitis", "authors": "Timoleon Moraitis, Abu Sebastian, Evangelos Eleftheriou (IBM Research\n  - Zurich)", "title": "Optimality of short-term synaptic plasticity in modelling certain\n  dynamic environments", "comments": "Main paper: 12 pages, 4 figures. Supplementary Information: 13 pages,\n  4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological neurons and their in-silico emulations for neuromorphic artificial\nintelligence (AI) use extraordinarily energy-efficient mechanisms, such as\nspike-based communication and local synaptic plasticity. It remains unclear\nwhether these neuronal mechanisms only offer efficiency or also underlie the\nsuperiority of biological intelligence. Here, we prove rigorously that, indeed,\nthe Bayes-optimal prediction and inference of randomly but continuously\ntransforming environments, a common natural setting, relies on short-term\nspike-timing-dependent plasticity, a hallmark of biological synapses. Further,\nthis dynamic Bayesian inference through plasticity enables circuits of the\ncerebral cortex in simulations to recognize previously unseen, highly distorted\ndynamic stimuli. Strikingly, this also introduces a biologically-modelled AI,\nthe first to overcome multiple limitations of deep learning and outperform\nartificial neural networks in a visual task. The cortical-like network is\nspiking and event-based, trained only with unsupervised and local plasticity,\non a small, narrow, and static training dataset, but achieves recognition of\nunseen, transformed, and dynamic data better than deep neural networks with\ncontinuous activations, trained with supervised backpropagation on the\ntransforming data. These results link short-term plasticity to high-level\ncortical function, suggest optimality of natural intelligence for natural\nenvironments, and repurpose neuromorphic AI from mere efficiency to\ncomputational supremacy altogether.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 01:04:28 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 22:14:34 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Moraitis", "Timoleon", "", "IBM Research\n  - Zurich"], ["Sebastian", "Abu", "", "IBM Research\n  - Zurich"], ["Eleftheriou", "Evangelos", "", "IBM Research\n  - Zurich"]]}, {"id": "2009.06816", "submitter": "Jun Zhang", "authors": "Jun Zhang, Kuan Tian, Pei Dong, Haocheng Shen, Kezhou Yan, Jianhua\n  Yao, Junzhou Huang, Xiao Han", "title": "Microscope Based HER2 Scoring System", "comments": "11 pages, 5 figures, and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The overexpression of human epidermal growth factor receptor 2 (HER2) has\nbeen established as a therapeutic target in multiple types of cancers, such as\nbreast and gastric cancers. Immunohistochemistry (IHC) is employed as a basic\nHER2 test to identify the HER2-positive, borderline, and HER2-negative\npatients. However, the reliability and accuracy of HER2 scoring are affected by\nmany factors, such as pathologists' experience. Recently, artificial\nintelligence (AI) has been used in various disease diagnosis to improve\ndiagnostic accuracy and reliability, but the interpretation of diagnosis\nresults is still an open problem. In this paper, we propose a real-time HER2\nscoring system, which follows the HER2 scoring guidelines to complete the\ndiagnosis, and thus each step is explainable. Unlike the previous scoring\nsystems based on whole-slide imaging, our HER2 scoring system is integrated\ninto an augmented reality (AR) microscope that can feedback AI results to the\npathologists while reading the slide. The pathologists can help select\ninformative fields of view (FOVs), avoiding the confounding regions, such as\nDCIS. Importantly, we illustrate the intermediate results with membrane\nstaining condition and cell classification results, making it possible to\nevaluate the reliability of the diagnostic results. Also, we support the\ninteractive modification of selecting regions-of-interest, making our system\nmore flexible in clinical practice. The collaboration of AI and pathologists\ncan significantly improve the robustness of our system. We evaluate our system\nwith 285 breast IHC HER2 slides, and the classification accuracy of 95\\% shows\nthe effectiveness of our HER2 scoring system.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 01:44:39 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Zhang", "Jun", ""], ["Tian", "Kuan", ""], ["Dong", "Pei", ""], ["Shen", "Haocheng", ""], ["Yan", "Kezhou", ""], ["Yao", "Jianhua", ""], ["Huang", "Junzhou", ""], ["Han", "Xiao", ""]]}, {"id": "2009.06847", "submitter": "Guansong Pang", "authors": "Guansong Pang, Anton van den Hengel, Chunhua Shen, Longbing Cao", "title": "Toward Deep Supervised Anomaly Detection: Reinforcement Learning from\n  Partially Labeled Anomaly Data", "comments": "Accepted to KDD 2021", "journal-ref": null, "doi": "10.1145/3447548.3467417", "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of anomaly detection with a small set of partially\nlabeled anomaly examples and a large-scale unlabeled dataset. This is a common\nscenario in many important applications. Existing related methods either\nexclusively fit the limited anomaly examples that typically do not span the\nentire set of anomalies, or proceed with unsupervised learning from the\nunlabeled data. We propose here instead a deep reinforcement learning-based\napproach that enables an end-to-end optimization of the detection of both\nlabeled and unlabeled anomalies. This approach learns the known abnormality by\nautomatically interacting with an anomaly-biased simulation environment, while\ncontinuously extending the learned abnormality to novel classes of anomaly\n(i.e., unknown anomalies) by actively exploring possible anomalies in the\nunlabeled data. This is achieved by jointly optimizing the exploitation of the\nsmall labeled anomaly data and the exploration of the rare unlabeled anomalies.\nExtensive experiments on 48 real-world datasets show that our model\nsignificantly outperforms five state-of-the-art competing methods.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 03:05:39 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 13:40:11 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Pang", "Guansong", ""], ["Hengel", "Anton van den", ""], ["Shen", "Chunhua", ""], ["Cao", "Longbing", ""]]}, {"id": "2009.06869", "submitter": "Aydogan Ozcan", "authors": "Md Sadman Sakib Rahman, Jingxi Li, Deniz Mengu, Yair Rivenson and\n  Aydogan Ozcan", "title": "Ensemble learning of diffractive optical networks", "comments": "22 Pages, 4 Figures, 1 Table", "journal-ref": "Light: Science & Applications (2021)", "doi": "10.1038/s41377-020-00446-w", "report-no": null, "categories": "cs.NE cs.CV cs.LG eess.IV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A plethora of research advances have emerged in the fields of optics and\nphotonics that benefit from harnessing the power of machine learning.\nSpecifically, there has been a revival of interest in optical computing\nhardware, due to its potential advantages for machine learning tasks in terms\nof parallelization, power efficiency and computation speed. Diffractive Deep\nNeural Networks (D2NNs) form such an optical computing framework, which\nbenefits from deep learning-based design of successive diffractive layers to\nall-optically process information as the input light diffracts through these\npassive layers. D2NNs have demonstrated success in various tasks, including\ne.g., object classification, spectral-encoding of information, optical pulse\nshaping and imaging, among others. Here, we significantly improve the inference\nperformance of diffractive optical networks using feature engineering and\nensemble learning. After independently training a total of 1252 D2NNs that were\ndiversely engineered with a variety of passive input filters, we applied a\npruning algorithm to select an optimized ensemble of D2NNs that collectively\nimprove their image classification accuracy. Through this pruning, we\nnumerically demonstrated that ensembles of N=14 and N=30 D2NNs achieve blind\ntesting accuracies of 61.14% and 62.13%, respectively, on the classification of\nCIFAR-10 test images, providing an inference improvement of >16% compared to\nthe average performance of the individual D2NNs within each ensemble. These\nresults constitute the highest inference accuracies achieved to date by any\ndiffractive optical neural network design on the same dataset and might provide\na significant leapfrog to extend the application space of diffractive optical\nimage classification and machine vision systems.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 05:02:50 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Rahman", "Md Sadman Sakib", ""], ["Li", "Jingxi", ""], ["Mengu", "Deniz", ""], ["Rivenson", "Yair", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "2009.06886", "submitter": "Jinquan Li", "authors": "Jinquan Li, Ling Pei, Danping Zou, Songpengcheng Xia, Qi Wu, Tao Li,\n  Zhen Sun, Wenxian Yu", "title": "Attention-SLAM: A Visual Monocular SLAM Learning from Human Gaze", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel simultaneous localization and mapping (SLAM)\napproach, namely Attention-SLAM, which simulates human navigation mode by\ncombining a visual saliency model (SalNavNet) with traditional monocular visual\nSLAM. Most SLAM methods treat all the features extracted from the images as\nequal importance during the optimization process. However, the salient feature\npoints in scenes have more significant influence during the human navigation\nprocess. Therefore, we first propose a visual saliency model called SalVavNet\nin which we introduce a correlation module and propose an adaptive Exponential\nMoving Average (EMA) module. These modules mitigate the center bias to enable\nthe saliency maps generated by SalNavNet to pay more attention to the same\nsalient object. Moreover, the saliency maps simulate the human behavior for the\nrefinement of SLAM results. The feature points extracted from the salient\nregions have greater importance in optimization process. We add semantic\nsaliency information to the Euroc dataset to generate an open-source saliency\nSLAM dataset. Comprehensive test results prove that Attention-SLAM outperforms\nbenchmarks such as Direct Sparse Odometry (DSO), ORB-SLAM, and Salient DSO in\nterms of efficiency, accuracy, and robustness in most test cases.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 06:59:12 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Li", "Jinquan", ""], ["Pei", "Ling", ""], ["Zou", "Danping", ""], ["Xia", "Songpengcheng", ""], ["Wu", "Qi", ""], ["Li", "Tao", ""], ["Sun", "Zhen", ""], ["Yu", "Wenxian", ""]]}, {"id": "2009.06887", "submitter": "Shengtian Liu", "authors": "Yuanpeng Liu, Jun Zhou, Yuqi Zhang, Chao Ding, Jun Wang", "title": "3DPVNet: Patch-level 3D Hough Voting Network for 6D Pose Estimation", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on estimating the 6D pose of objects in point clouds.\nAlthough the topic has been widely studied, pose estimation in point clouds\nremains a challenging problem due to the noise and occlusion. To address the\nproblem, a novel 3DPVNet is presented in this work, which utilizes 3D local\npatches to vote for the object 6D poses. 3DPVNet is comprised of three modules.\nIn particular, a Patch Unification (\\textbf{PU}) module is first introduced to\nnormalize the input patch, and also create a standard local coordinate frame on\nit to generate a reliable vote. We then devise a Weight-guided Neighboring\nFeature Fusion (\\textbf{WNFF}) module in the network, which fuses the\nneighboring features to yield a semi-global feature for the center patch. WNFF\nmodule mines the neighboring information of a local patch, such that the\nrepresentation capability to local geometric characteristics is significantly\nenhanced, making the method robust to a certain level of noise. Moreover, we\npresent a Patch-level Voting (\\textbf{PV}) module to regress transformations\nand generates pose votes. After the aggregation of all votes from patches and a\nrefinement step, the final pose of the object can be obtained. Compared to\nrecent voting-based methods, 3DPVNet is patch-level, and directly carried out\non point clouds. Therefore, 3DPVNet achieves less computation than\npoint/pixel-level voting scheme, and has robustness to partial data.\nExperiments on several datasets demonstrate that 3DPVNet achieves the\nstate-of-the-art performance, and is also robust against noise and occlusions.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 06:59:57 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Liu", "Yuanpeng", ""], ["Zhou", "Jun", ""], ["Zhang", "Yuqi", ""], ["Ding", "Chao", ""], ["Wang", "Jun", ""]]}, {"id": "2009.06902", "submitter": "Haisheng Su", "authors": "Haisheng Su, Jing Su, Dongliang Wang, Weihao Gan, Wei Wu, Mengmeng\n  Wang, Junjie Yan, Yu Qiao", "title": "Collaborative Distillation in the Parameter and Spectrum Domains for\n  Video Action Recognition", "comments": "Submmited to AAAI21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the significant progress of action recognition\ntask with deep networks. However, most of current video networks require large\nmemory and computational resources, which hinders their applications in\npractice. Existing knowledge distillation methods are limited to the\nimage-level spatial domain, ignoring the temporal and frequency information\nwhich provide structural knowledge and are important for video analysis. This\npaper explores how to train small and efficient networks for action\nrecognition. Specifically, we propose two distillation strategies in the\nfrequency domain, namely the feature spectrum and parameter distribution\ndistillations respectively. Our insight is that appealing performance of action\nrecognition requires \\textit{explicitly} modeling the temporal frequency\nspectrum of video features. Therefore, we introduce a spectrum loss that\nenforces the student network to mimic the temporal frequency spectrum from the\nteacher network, instead of \\textit{implicitly} distilling features as many\nprevious works. Second, the parameter frequency distribution is further adopted\nto guide the student network to learn the appearance modeling process from the\nteacher. Besides, a collaborative learning strategy is presented to optimize\nthe training process from a probabilistic view. Extensive experiments are\nconducted on several action recognition benchmarks, such as Kinetics,\nSomething-Something, and Jester, which consistently verify effectiveness of our\napproach, and demonstrate that our method can achieve higher performance than\nstate-of-the-art methods with the same backbone.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 07:29:57 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Su", "Haisheng", ""], ["Su", "Jing", ""], ["Wang", "Dongliang", ""], ["Gan", "Weihao", ""], ["Wu", "Wei", ""], ["Wang", "Mengmeng", ""], ["Yan", "Junjie", ""], ["Qiao", "Yu", ""]]}, {"id": "2009.06903", "submitter": "Dongrui Liu", "authors": "Dongrui Liu, Chuanchuan Chen, Changqing Xu, Qi Cai, Lei Chu and Robert\n  Caiming Qiu", "title": "A Self Contour-based Rotation and Translation-Invariant Transformation\n  for Point Clouds Recognition", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several direct processing point cloud models have achieved\nstate-of-the-art performances for classification and segmentation tasks.\nHowever, these methods lack rotation robustness, and their performances degrade\nseverely under random rotations, failing to extend to real-world applications\nwith varying orientations. To address this problem, we propose a method named\nSelf Contour-based Transformation (SCT), which can be flexibly integrated into\na variety of existing point cloud recognition models against arbitrary\nrotations without any extra modifications. The SCT provides efficient and\nmathematically proved rotation and translation invariance by introducing\nRotation and Translation-Invariant Transformation. It linearly transforms\nCartesian coordinates of points to the self contour-based rotation-invariant\nrepresentations while maintaining the global geometric structure. Moreover, to\nenhance discriminative feature extraction, the Frame Alignment module is\nfurther introduced, aiming to capture contours and transform self contour-based\nframes to the intra-class frame. Extensive experimental results and\nmathematical analyses show that the proposed method outperforms the\nstate-of-the-art approaches under arbitrary rotations without any rotation\naugmentation on standard benchmarks, including ModelNet40, ScanObjectNN and\nShapeNet.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 07:30:16 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Liu", "Dongrui", ""], ["Chen", "Chuanchuan", ""], ["Xu", "Changqing", ""], ["Cai", "Qi", ""], ["Chu", "Lei", ""], ["Qiu", "Robert Caiming", ""]]}, {"id": "2009.06911", "submitter": "Soham Chattopadhyay", "authors": "Soham Chattopadhyay, Hritam Basak", "title": "Multi-scale Attention U-Net (MsAUNet): A Modified U-Net Architecture for\n  Scene Segmentation", "comments": "12 Pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the growing success of Convolution neural networks (CNN) in the\nrecent past in the task of scene segmentation, the standard models lack some of\nthe important features that might result in sub-optimal segmentation outputs.\nThe widely used encoder-decoder architecture extracts and uses several\nredundant and low-level features at different steps and different scales. Also,\nthese networks fail to map the long-range dependencies of local features, which\nresults in discriminative feature maps corresponding to each semantic class in\nthe resulting segmented image. In this paper, we propose a novel multi-scale\nattention network for scene segmentation purposes by using the rich contextual\ninformation from an image. Different from the original UNet architecture we\nhave used attention gates which take the features from the encoder and the\noutput of the pyramid pool as input and produced out-put is further\nconcatenated with the up-sampled output of the previous pyramid-pool layer and\nmapped to the next subsequent layer. This network can map local features with\ntheir global counterparts with improved accuracy and emphasize on\ndiscriminative image regions by focusing on relevant local features only. We\nalso propose a compound loss function by optimizing the IoU loss and fusing\nDice Loss and Weighted Cross-entropy loss with it to achieve an optimal\nsolution at a faster convergence rate. We have evaluated our model on two\nstandard datasets named PascalVOC2012 and ADE20k and was able to achieve mean\nIoU of 79.88% and 44.88% on the two datasets respectively, and compared our\nresult with the widely known models to prove the superiority of our model over\nthem.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 08:03:41 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Chattopadhyay", "Soham", ""], ["Basak", "Hritam", ""]]}, {"id": "2009.06912", "submitter": "Jianwei Li", "authors": "Jianwei Li, Yongtao Wang, Haihua Xie, Kai-Kuang Ma", "title": "Learning a Single Model with a Wide Range of Quality Factors for JPEG\n  Image Artifacts Removal", "comments": "Accepted for publication in the IEEE Transactions on Image Processing", "journal-ref": "IEEE Transactions on Image Processing, vol. 29, pp.8842-8854, 2020", "doi": "10.1109/TIP.2020.3020389", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy compression brings artifacts into the compressed image and degrades the\nvisual quality. In recent years, many compression artifacts removal methods\nbased on convolutional neural network (CNN) have been developed with great\nsuccess. However, these methods usually train a model based on one specific\nvalue or a small range of quality factors. Obviously, if the test image's\nquality factor does not match to the assumed value range, then degraded\nperformance will be resulted. With this motivation and further consideration of\npractical usage, a highly robust compression artifacts removal network is\nproposed in this paper. Our proposed network is a single model approach that\ncan be trained for handling a wide range of quality factors while consistently\ndelivering superior or comparable image artifacts removal performance. To\ndemonstrate, we focus on the JPEG compression with quality factors, ranging\nfrom 1 to 60. Note that a turnkey success of our proposed network lies in the\nnovel utilization of the quantization tables as part of the training data.\nFurthermore, it has two branches in parallel---i.e., the restoration branch and\nthe global branch. The former effectively removes the local artifacts, such as\nringing artifacts removal. On the other hand, the latter extracts the global\nfeatures of the entire image that provides highly instrumental image quality\nimprovement, especially effective on dealing with the global artifacts, such as\nblocking, color shifting. Extensive experimental results performed on color and\ngrayscale images have clearly demonstrated the effectiveness and efficacy of\nour proposed single-model approach on the removal of compression artifacts from\nthe decoded image.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 08:16:58 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Li", "Jianwei", ""], ["Wang", "Yongtao", ""], ["Xie", "Haihua", ""], ["Ma", "Kai-Kuang", ""]]}, {"id": "2009.06924", "submitter": "Ashesh Ashesh", "authors": "Ashesh Mishra, Hsuan-Tien Lin", "title": "360-Degree Gaze Estimation in the Wild Using Multiple Zoom Scales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaze estimation involves predicting where the person is looking at, given\neither a single input image or a sequence of images. One challenging task, gaze\nestimation in the wild, concerns data collected in unconstrained environments\nwith varying camera-person distances, like the Gaze360 dataset. The varying\ndistances result in varying face sizes in the images, which makes it hard for\ncurrent CNN backbones to estimate the gaze robustly. Inspired by our natural\nskill to identify the gaze by taking a focused look at the face area, we\npropose a novel architecture that similarly zooms in on the face area of the\nimage at multiple scales to improve prediction accuracy. Another challenging\ntask, 360-degree gaze estimation (also introduced by the Gaze360 dataset),\nconsists of estimating not only the forward gazes, but also the backward ones.\nThe backward gazes introduce discontinuity in the yaw angle values of the gaze,\nmaking the deep learning models affected by some huge loss around the\ndiscontinuous points. We propose to convert the angle values by sine-cosine\ntransform to avoid the discontinuity and represent the physical meaning of the\nyaw angle better. We conduct ablation studies on both ideas, the novel\narchitecture and the transform, to validate their effectiveness. The two ideas\nallow our proposed model to achieve state-of-the-art performance for both the\nGaze360 dataset and the RT-Gene dataset when using single images. Furthermore,\nwe extend the model to a sequential version that systematically zooms in on a\ngiven sequence of images. The sequential version again achieves\nstate-of-the-art performance on the Gaze360 dataset, which further demonstrates\nthe usefulness of our proposed ideas.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 08:45:12 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Mishra", "Ashesh", ""], ["Lin", "Hsuan-Tien", ""]]}, {"id": "2009.06943", "submitter": "Yawei Li", "authors": "Kai Zhang, Martin Danelljan, Yawei Li, Radu Timofte, Jie Liu, Jie\n  Tang, Gangshan Wu, Yu Zhu, Xiangyu He, Wenjie Xu, Chenghua Li, Cong Leng,\n  Jian Cheng, Guangyang Wu, Wenyi Wang, Xiaohong Liu, Hengyuan Zhao, Xiangtao\n  Kong, Jingwen He, Yu Qiao, Chao Dong, Xiaotong Luo, Liang Chen, Jiangtao\n  Zhang, Maitreya Suin, Kuldeep Purohit, A. N. Rajagopalan, Xiaochuan Li,\n  Zhiqiang Lang, Jiangtao Nie, Wei Wei, Lei Zhang, Abdul Muqeet, Jiwon Hwang,\n  Subin Yang, JungHeum Kang, Sung-Ho Bae, Yongwoo Kim, Liang Chen, Jiangtao\n  Zhang, Xiaotong Luo, Yanyun Qu, Geun-Woo Jeon, Jun-Ho Choi, Jun-Hyuk Kim,\n  Jong-Seok Lee, Steven Marty, Eric Marty, Dongliang Xiong, Siang Chen, Lin\n  Zha, Jiande Jiang, Xinbo Gao, Wen Lu, Haicheng Wang, Vineeth Bhaskara, Alex\n  Levinshtein, Stavros Tsogkas, Allan Jepson, Xiangzhen Kong, Tongtong Zhao,\n  Shanshan Zhao, Hrishikesh P S, Densen Puthussery, Jiji C V, Nan Nan, Shuai\n  Liu, Jie Cai, Zibo Meng, Jiaming Ding, Chiu Man Ho, Xuehui Wang, Qiong Yan,\n  Yuzhi Zhao, Long Chen, Jiangtao Zhang, Xiaotong Luo, Liang Chen, Yanyun Qu,\n  Long Sun, Wenhao Wang, Zhenbing Liu, Rushi Lan, Rao Muhammad Umer, and\n  Christian Micheloni", "title": "AIM 2020 Challenge on Efficient Super-Resolution: Methods and Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the AIM 2020 challenge on efficient single image\nsuper-resolution with focus on the proposed solutions and results. The\nchallenge task was to super-resolve an input image with a magnification factor\nx4 based on a set of prior examples of low and corresponding high resolution\nimages. The goal is to devise a network that reduces one or several aspects\nsuch as runtime, parameter count, FLOPs, activations, and memory consumption\nwhile at least maintaining PSNR of MSRResNet. The track had 150 registered\nparticipants, and 25 teams submitted the final results. They gauge the\nstate-of-the-art in efficient single image super-resolution.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 09:25:51 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Zhang", "Kai", ""], ["Danelljan", "Martin", ""], ["Li", "Yawei", ""], ["Timofte", "Radu", ""], ["Liu", "Jie", ""], ["Tang", "Jie", ""], ["Wu", "Gangshan", ""], ["Zhu", "Yu", ""], ["He", "Xiangyu", ""], ["Xu", "Wenjie", ""], ["Li", "Chenghua", ""], ["Leng", "Cong", ""], ["Cheng", "Jian", ""], ["Wu", "Guangyang", ""], ["Wang", "Wenyi", ""], ["Liu", "Xiaohong", ""], ["Zhao", "Hengyuan", ""], ["Kong", "Xiangtao", ""], ["He", "Jingwen", ""], ["Qiao", "Yu", ""], ["Dong", "Chao", ""], ["Luo", "Xiaotong", ""], ["Chen", "Liang", ""], ["Zhang", "Jiangtao", ""], ["Suin", "Maitreya", ""], ["Purohit", "Kuldeep", ""], ["Rajagopalan", "A. N.", ""], ["Li", "Xiaochuan", ""], ["Lang", "Zhiqiang", ""], ["Nie", "Jiangtao", ""], ["Wei", "Wei", ""], ["Zhang", "Lei", ""], ["Muqeet", "Abdul", ""], ["Hwang", "Jiwon", ""], ["Yang", "Subin", ""], ["Kang", "JungHeum", ""], ["Bae", "Sung-Ho", ""], ["Kim", "Yongwoo", ""], ["Chen", "Liang", ""], ["Zhang", "Jiangtao", ""], ["Luo", "Xiaotong", ""], ["Qu", "Yanyun", ""], ["Jeon", "Geun-Woo", ""], ["Choi", "Jun-Ho", ""], ["Kim", "Jun-Hyuk", ""], ["Lee", "Jong-Seok", ""], ["Marty", "Steven", ""], ["Marty", "Eric", ""], ["Xiong", "Dongliang", ""], ["Chen", "Siang", ""], ["Zha", "Lin", ""], ["Jiang", "Jiande", ""], ["Gao", "Xinbo", ""], ["Lu", "Wen", ""], ["Wang", "Haicheng", ""], ["Bhaskara", "Vineeth", ""], ["Levinshtein", "Alex", ""], ["Tsogkas", "Stavros", ""], ["Jepson", "Allan", ""], ["Kong", "Xiangzhen", ""], ["Zhao", "Tongtong", ""], ["Zhao", "Shanshan", ""], ["S", "Hrishikesh P", ""], ["Puthussery", "Densen", ""], ["C", "Jiji", "V"], ["Nan", "Nan", ""], ["Liu", "Shuai", ""], ["Cai", "Jie", ""], ["Meng", "Zibo", ""], ["Ding", "Jiaming", ""], ["Ho", "Chiu Man", ""], ["Wang", "Xuehui", ""], ["Yan", "Qiong", ""], ["Zhao", "Yuzhi", ""], ["Chen", "Long", ""], ["Zhang", "Jiangtao", ""], ["Luo", "Xiaotong", ""], ["Chen", "Liang", ""], ["Qu", "Yanyun", ""], ["Sun", "Long", ""], ["Wang", "Wenhao", ""], ["Liu", "Zhenbing", ""], ["Lan", "Rushi", ""], ["Umer", "Rao Muhammad", ""], ["Micheloni", "Christian", ""]]}, {"id": "2009.06962", "submitter": "Jang-Hyun Kim", "authors": "Jang-Hyun Kim, Wonho Choo, Hyun Oh Song", "title": "Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup", "comments": "Published at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks achieve great performance on fitting the training\ndistribution, the learned networks are prone to overfitting and are susceptible\nto adversarial attacks. In this regard, a number of mixup based augmentation\nmethods have been recently proposed. However, these approaches mainly focus on\ncreating previously unseen virtual examples and can sometimes provide\nmisleading supervisory signal to the network. To this end, we propose Puzzle\nMix, a mixup method for explicitly utilizing the saliency information and the\nunderlying statistics of the natural examples. This leads to an interesting\noptimization problem alternating between the multi-label objective for optimal\nmixing mask and saliency discounted optimal transport objective. Our\nexperiments show Puzzle Mix achieves the state of the art generalization and\nthe adversarial robustness results compared to other mixup methods on\nCIFAR-100, Tiny-ImageNet, and ImageNet datasets. The source code is available\nat https://github.com/snu-mllab/PuzzleMix.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 10:10:23 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 10:45:39 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Kim", "Jang-Hyun", ""], ["Choo", "Wonho", ""], ["Song", "Hyun Oh", ""]]}, {"id": "2009.06963", "submitter": "Dario Zanca", "authors": "Dario Zanca, Marco Gori, Stefano Melacci, Alessandra Rufa", "title": "Gravitational Models Explain Shifts on Human Visual Attention", "comments": null, "journal-ref": null, "doi": "10.1038/s41598-020-73494-2", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attention refers to the human brain's ability to select relevant\nsensory information for preferential processing, improving performance in\nvisual and cognitive tasks. It proceeds in two phases. One in which visual\nfeature maps are acquired and processed in parallel. Another where the\ninformation from these maps is merged in order to select a single location to\nbe attended for further and more complex computations and reasoning. Its\ncomputational description is challenging, especially if the temporal dynamics\nof the process are taken into account. Numerous methods to estimate saliency\nhave been proposed in the last three decades. They achieve almost perfect\nperformance in estimating saliency at the pixel level, but the way they\ngenerate shifts in visual attention fully depends on winner-take-all (WTA)\ncircuitry. WTA is implemented} by the biological hardware in order to select a\nlocation with maximum saliency, towards which to direct overt attention. In\nthis paper we propose a gravitational model (GRAV) to describe the attentional\nshifts. Every single feature acts as an attractor and {the shifts are the\nresult of the joint effects of the attractors. In the current framework, the\nassumption of a single, centralized saliency map is no longer necessary, though\nstill plausible. Quantitative results on two large image datasets show that\nthis model predicts shifts more accurately than winner-take-all.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 10:12:41 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Zanca", "Dario", ""], ["Gori", "Marco", ""], ["Melacci", "Stefano", ""], ["Rufa", "Alessandra", ""]]}, {"id": "2009.06996", "submitter": "Yufei Wang", "authors": "Haoliang Li (1), Yufei Wang (1), Xiaofei Xie (1), Yang Liu (1), Shiqi\n  Wang (2), Renjie Wan (1), Lap-Pui Chau (1), and Alex C. Kot (1) ((1) Nanyang\n  Technological University, Singapore, (2) City University of Hong Kong)", "title": "Light Can Hack Your Face! Black-box Backdoor Attack on Face Recognition\n  Systems", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) have shown great success in many computer vision\napplications. However, they are also known to be susceptible to backdoor\nattacks. When conducting backdoor attacks, most of the existing approaches\nassume that the targeted DNN is always available, and an attacker can always\ninject a specific pattern to the training data to further fine-tune the DNN\nmodel. However, in practice, such attack may not be feasible as the DNN model\nis encrypted and only available to the secure enclave.\n  In this paper, we propose a novel black-box backdoor attack technique on face\nrecognition systems, which can be conducted without the knowledge of the\ntargeted DNN model. To be specific, we propose a backdoor attack with a novel\ncolor stripe pattern trigger, which can be generated by modulating LED in a\nspecialized waveform. We also use an evolutionary computing strategy to\noptimize the waveform for backdoor attack. Our backdoor attack can be conducted\nin a very mild condition: 1) the adversary cannot manipulate the input in an\nunnatural way (e.g., injecting adversarial noise); 2) the adversary cannot\naccess the training database; 3) the adversary has no knowledge of the training\nmodel as well as the training set used by the victim party.\n  We show that the backdoor trigger can be quite effective, where the attack\nsuccess rate can be up to $88\\%$ based on our simulation study and up to $40\\%$\nbased on our physical-domain study by considering the task of face recognition\nand verification based on at most three-time attempts during authentication.\nFinally, we evaluate several state-of-the-art potential defenses towards\nbackdoor attacks, and find that our attack can still be effective. We highlight\nthat our study revealed a new physical backdoor attack, which calls for the\nattention of the security issue of the existing face recognition/verification\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 11:50:29 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Li", "Haoliang", ""], ["Wang", "Yufei", ""], ["Xie", "Xiaofei", ""], ["Liu", "Yang", ""], ["Wang", "Shiqi", ""], ["Wan", "Renjie", ""], ["Chau", "Lap-Pui", ""], ["Kot", "Alex C.", ""]]}, {"id": "2009.07000", "submitter": "Natalia Efremova", "authors": "Sagar Vaze, James Foley, Mohamed Seddiq, Alexey Unagaev, Natalia\n  Efremova", "title": "Optimal Use of Multi-spectral Satellite Data with Convolutional Neural\n  Networks", "comments": "AI for Social Good workshop - Harvard CRCS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of satellite imagery will prove a crucial tool in the pursuit of\nsustainable development. While Convolutional Neural Networks (CNNs) have made\nlarge gains in natural image analysis, their application to multi-spectral\nsatellite images (wherein input images have a large number of channels) remains\nrelatively unexplored. In this paper, we compare different methods of\nleveraging multi-band information with CNNs, demonstrating the performance of\nall compared methods on the task of semantic segmentation of agricultural\nvegetation (vineyards). We show that standard industry practice of using bands\nselected by a domain expert leads to a significantly worse test accuracy than\nthe other methods compared. Specifically, we compare: using bands specified by\nan expert; using all available bands; learning attention maps over the input\nbands; and leveraging Bayesian optimisation to dictate band choice. We show\nthat simply using all available band information already increases test time\nperformance, and show that the Bayesian optimisation, first applied to band\nselection in this work, can be used to further boost accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 11:55:45 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Vaze", "Sagar", ""], ["Foley", "James", ""], ["Seddiq", "Mohamed", ""], ["Unagaev", "Alexey", ""], ["Efremova", "Natalia", ""]]}, {"id": "2009.07011", "submitter": "Mateusz Kozi\\'nski", "authors": "Doruk Oner and Mateusz Kozi\\'nski and Leonardo Citraro and Nathan C.\n  Dadap and Alexandra G. Konings and Pascal Fua", "title": "Promoting Connectivity of Network-Like Structures by Enforcing Region\n  Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel, connectivity-oriented loss function for training deep\nconvolutional networks to reconstruct network-like structures, like roads and\nirrigation canals, from aerial images. The main idea behind our loss is to\nexpress the connectivity of roads, or canals, in terms of disconnections that\nthey create between background regions of the image. In simple terms, a gap in\nthe predicted road causes two background regions, that lie on the opposite\nsides of a ground truth road, to touch in prediction. Our loss function is\ndesigned to prevent such unwanted connections between background regions, and\ntherefore close the gaps in predicted roads. It also prevents predicting false\npositive roads and canals by penalizing unwarranted disconnections of\nbackground regions. In order to capture even short, dead-ending road segments,\nwe evaluate the loss in small image crops. We show, in experiments on two\nstandard road benchmarks and a new data set of irrigation canals, that convnets\ntrained with our loss function recover road connectivity so well, that it\nsuffices to skeletonize their output to produce state of the art maps. A\ndistinct advantage of our approach is that the loss can be plugged in to any\nexisting training setup without further modifications.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 12:21:35 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Oner", "Doruk", ""], ["Kozi\u0144ski", "Mateusz", ""], ["Citraro", "Leonardo", ""], ["Dadap", "Nathan C.", ""], ["Konings", "Alexandra G.", ""], ["Fua", "Pascal", ""]]}, {"id": "2009.07013", "submitter": "Dominique Vaufreydaz", "authors": "Anastasia Petrova (PERVASIVE), Dominique Vaufreydaz (PERVASIVE),\n  Philippe Dessus (LaRAC)", "title": "Group-Level Emotion Recognition Using a Unimodal Privacy-Safe\n  Non-Individual Approach", "comments": null, "journal-ref": "EmotiW2020 Challenge at the 22nd ACM International Conference on\n  Multimodal Interaction (ICMI2020), Oct 2020, Utrecht, Netherlands", "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents our unimodal privacy-safe and non-individual proposal\nfor the audio-video group emotion recognition subtask at the Emotion\nRecognition in the Wild (EmotiW) Challenge 2020 1. This sub challenge aims to\nclassify in the wild videos into three categories: Positive, Neutral and\nNegative. Recent deep learning models have shown tremendous advances in\nanalyzing interactions between people, predicting human behavior and affective\nevaluation. Nonetheless, their performance comes from individual-based\nanalysis, which means summing up and averaging scores from individual\ndetections, which inevitably leads to some privacy issues. In this research, we\ninvestigated a frugal approach towards a model able to capture the global moods\nfrom the whole image without using face or pose detection, or any\nindividual-based feature as input. The proposed methodology mixes\nstate-of-the-art and dedicated synthetic corpora as training sources. With an\nin-depth exploration of neural network architectures for group-level emotion\nrecognition, we built a VGG-based model achieving 59.13% accuracy on the VGAF\ntest set (eleventh place of the challenge). Given that the analysis is unimodal\nbased only on global features and that the performance is evaluated on a\nreal-world dataset, these results are promising and let us envision extending\nthis model to multimodality for classroom ambiance evaluation, our final target\napplication.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 12:25:33 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Petrova", "Anastasia", "", "PERVASIVE"], ["Vaufreydaz", "Dominique", "", "PERVASIVE"], ["Dessus", "Philippe", "", "LaRAC"]]}, {"id": "2009.07024", "submitter": "Jing Wu", "authors": "Jing Wu, Mingyi Zhou, Shuaicheng Liu, Yipeng Liu, Ce Zhu", "title": "Decision-based Universal Adversarial Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A single perturbation can pose the most natural images to be misclassified by\nclassifiers. In black-box setting, current universal adversarial attack methods\nutilize substitute models to generate the perturbation, then apply the\nperturbation to the attacked model. However, this transfer often produces\ninferior results. In this study, we directly work in the black-box setting to\ngenerate the universal adversarial perturbation. Besides, we aim to design an\nadversary generating a single perturbation having texture like stripes based on\northogonal matrix, as the top convolutional layers are sensitive to stripes. To\nthis end, we propose an efficient Decision-based Universal Attack (DUAttack).\nWith few data, the proposed adversary computes the perturbation based solely on\nthe final inferred labels, but good transferability has been realized not only\nacross models but also span different vision tasks. The effectiveness of\nDUAttack is validated through comparisons with other state-of-the-art attacks.\nThe efficiency of DUAttack is also demonstrated on real world settings\nincluding the Microsoft Azure. In addition, several representative defense\nmethods are struggling with DUAttack, indicating the practicability of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 12:49:03 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 09:46:56 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2021 05:49:43 GMT"}, {"version": "v4", "created": "Tue, 5 Jan 2021 11:01:01 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Wu", "Jing", ""], ["Zhou", "Mingyi", ""], ["Liu", "Shuaicheng", ""], ["Liu", "Yipeng", ""], ["Zhu", "Ce", ""]]}, {"id": "2009.07025", "submitter": "Aythami Morales", "authors": "Alejandro Pe\\~na and Ignacio Serna and Aythami Morales and Julian\n  Fierrez", "title": "FairCVtest Demo: Understanding Bias in Multimodal Learning with a\n  Testbed in Fair Automatic Recruitment", "comments": "ACM Intl. Conf. on Multimodal Interaction (ICMI). arXiv admin note:\n  substantial text overlap with arXiv:2004.07173", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the aim of studying how current multimodal AI algorithms based on\nheterogeneous sources of information are affected by sensitive elements and\ninner biases in the data, this demonstrator experiments over an automated\nrecruitment testbed based on Curriculum Vitae: FairCVtest. The presence of\ndecision-making algorithms in society is rapidly increasing nowadays, while\nconcerns about their transparency and the possibility of these algorithms\nbecoming new sources of discrimination are arising. This demo shows the\ncapacity of the Artificial Intelligence (AI) behind a recruitment tool to\nextract sensitive information from unstructured data, and exploit it in\ncombination to data biases in undesirable (unfair) ways. Aditionally, the demo\nincludes a new algorithm (SensitiveNets) for discrimination-aware learning\nwhich eliminates sensitive information in our multimodal AI framework.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 17:45:09 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Pe\u00f1a", "Alejandro", ""], ["Serna", "Ignacio", ""], ["Morales", "Aythami", ""], ["Fierrez", "Julian", ""]]}, {"id": "2009.07026", "submitter": "Jinghua Wang", "authors": "Jinghua Wang and Jianmin Jiang", "title": "SA-Net: A deep spectral analysis network for image clustering", "comments": "arXiv admin note: text overlap with arXiv:2009.05235", "journal-ref": "Neurocomputing 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although supervised deep representation learning has attracted enormous\nattentions across areas of pattern recognition and computer vision, little\nprogress has been made towards unsupervised deep representation learning for\nimage clustering. In this paper, we propose a deep spectral analysis network\nfor unsupervised representation learning and image clustering. While spectral\nanalysis is established with solid theoretical foundations and has been widely\napplied to unsupervised data mining, its essential weakness lies in the fact\nthat it is difficult to construct a proper affinity matrix and determine the\ninvolving Laplacian matrix for a given dataset. In this paper, we propose a\nSA-Net to overcome these weaknesses and achieve improved image clustering by\nextending the spectral analysis procedure into a deep learning framework with\nmultiple layers. The SA-Net has the capability to learn deep representations\nand reveal deep correlations among data samples. Compared with the existing\nspectral analysis, the SA-Net achieves two advantages: (i) Given the fact that\none spectral analysis procedure can only deal with one subset of the given\ndataset, our proposed SA-Net elegantly integrates multiple parallel and\nconsecutive spectral analysis procedures together to enable interactive\nlearning across different units towards a coordinated clustering model; (ii)\nOur SA-Net can identify the local similarities among different images at patch\nlevel and hence achieves a higher level of robustness against occlusions.\nExtensive experiments on a number of popular datasets support that our proposed\nSA-Net outperforms 11 benchmarks across a number of image clustering\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 05:27:23 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Wang", "Jinghua", ""], ["Jiang", "Jianmin", ""]]}, {"id": "2009.07036", "submitter": "Zhenyu He", "authors": "Honghu Pan, Fanyang Meng, Nana Fan, Zhenyu He", "title": "TCDesc: Learning Topology Consistent Descriptors for Image Matching", "comments": "arXiv admin note: text overlap with arXiv:2006.03254", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constraint of neighborhood consistency or local consistency is widely\nused for robust image matching. In this paper, we focus on learning\nneighborhood topology consistent descriptors (TCDesc), while former works of\nlearning descriptors, such as HardNet and DSM, only consider point-to-point\nEuclidean distance among descriptors and totally neglect neighborhood\ninformation of descriptors. To learn topology consistent descriptors, first we\npropose the linear combination weights to depict the topological relationship\nbetween center descriptor and its kNN descriptors, where the difference between\ncenter descriptor and the linear combination of its kNN descriptors is\nminimized. Then we propose the global mapping function which maps the local\nlinear combination weights to the global topology vector and define the\ntopology distance of matching descriptors as l1 distance between their topology\nvectors. Last we employ adaptive weighting strategy to jointly minimize\ntopology distance and Euclidean distance, which automatically adjust the weight\nor attention of two distances in triplet loss. Our method has the following two\nadvantages: (1) We are the first to consider neighborhood information of\ndescriptors, while former works mainly focus on neighborhood consistency of\nfeature points; (2) Our method can be applied in any former work of learning\ndescriptors by triplet loss. Experimental results verify the generalization of\nour method: We can improve the performances of both HardNet and DSM on several\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 03:32:37 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Pan", "Honghu", ""], ["Meng", "Fanyang", ""], ["Fan", "Nana", ""], ["He", "Zhenyu", ""]]}, {"id": "2009.07044", "submitter": "Ilianna Kollia", "authors": "D. Kollias, N. Bouas, Y. Vlaxos, V. Brillakis, M. Seferis, I. Kollia,\n  L. Sukissian, J. Wingate, and S. Kollias", "title": "Deep Transparent Prediction through Latent Representation Analysis", "comments": "16 pages, 8 figures, to be published at Foundations of Trustworthy AI\n  integrating Learning, Optimisation and Reasoning (TAILOR) Workshop of\n  European Conference on Artificial Intelligence (ECAI) 2020. arXiv admin note:\n  substantial text overlap with arXiv:1911.10653", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a novel deep learning approach, which extracts latent\ninformation from trained Deep Neural Networks (DNNs) and derives concise\nrepresentations that are analyzed in an effective, unified way for prediction\npurposes. It is well known that DNNs are capable of analyzing complex data;\nhowever, they lack transparency in their decision making, in the sense that it\nis not straightforward to justify their prediction, or to visualize the\nfeatures on which the decision was based. Moreover, they generally require\nlarge amounts of data in order to learn and become able to adapt to different\nenvironments. This makes their use difficult in healthcare, where trust and\npersonalization are key issues. Transparency combined with high prediction\naccuracy are the targeted goals of the proposed approach. It includes both\nsupervised DNN training and unsupervised learning of latent variables extracted\nfrom the trained DNNs. Domain Adaptation from multiple sources is also\npresented as an extension, where the extracted latent variable representations\nare used to generate predictions in other, non-annotated, environments.\nSuccessful application is illustrated through a large experimental study in\nvarious fields: prediction of Parkinson's disease from MRI and DaTScans;\nprediction of COVID-19 and pneumonia from CT scans and X-rays; optical\ncharacter verification in retail food packaging.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 19:21:40 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 22:06:43 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Kollias", "D.", ""], ["Bouas", "N.", ""], ["Vlaxos", "Y.", ""], ["Brillakis", "V.", ""], ["Seferis", "M.", ""], ["Kollia", "I.", ""], ["Sukissian", "L.", ""], ["Wingate", "J.", ""], ["Kollias", "S.", ""]]}, {"id": "2009.07047", "submitter": "Ziyu Wan", "authors": "Ziyu Wan, Bo Zhang, Dongdong Chen, Pan Zhang, Dong Chen, Jing Liao,\n  Fang Wen", "title": "Old Photo Restoration via Deep Latent Space Translation", "comments": "15 pages. arXiv admin note: substantial text overlap with\n  arXiv:2004.09484", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to restore old photos that suffer from severe degradation through\na deep learning approach. Unlike conventional restoration tasks that can be\nsolved through supervised learning, the degradation in real photos is complex\nand the domain gap between synthetic images and real old photos makes the\nnetwork fail to generalize. Therefore, we propose a novel triplet domain\ntranslation network by leveraging real photos along with massive synthetic\nimage pairs. Specifically, we train two variational autoencoders (VAEs) to\nrespectively transform old photos and clean photos into two latent spaces. And\nthe translation between these two latent spaces is learned with synthetic\npaired data. This translation generalizes well to real photos because the\ndomain gap is closed in the compact latent space. Besides, to address multiple\ndegradations mixed in one old photo, we design a global branch with apartial\nnonlocal block targeting to the structured defects, such as scratches and dust\nspots, and a local branch targeting to the unstructured defects, such as noises\nand blurriness. Two branches are fused in the latent space, leading to improved\ncapability to restore old photos from multiple defects. Furthermore, we apply\nanother face refinement network to recover fine details of faces in the old\nphotos, thus ultimately generating photos with enhanced perceptual quality.\nWith comprehensive experiments, the proposed pipeline demonstrates superior\nperformance over state-of-the-art methods as well as existing commercial tools\nin terms of visual quality for old photos restoration.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 08:51:53 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Wan", "Ziyu", ""], ["Zhang", "Bo", ""], ["Chen", "Dongdong", ""], ["Zhang", "Pan", ""], ["Chen", "Dong", ""], ["Liao", "Jing", ""], ["Wen", "Fang", ""]]}, {"id": "2009.07061", "submitter": "Huan Yin", "authors": "Huan Yin, Runjian Chen, Yue Wang and Rong Xiong", "title": "RaLL: End-to-end Radar Localization on Lidar Map Using Differentiable\n  Measurement Model", "comments": "This paper has been accepted for publication by IEEE TITS. The\n  published version is available at\n  https://ieeexplore.ieee.org/document/9370010 . A demonstration video is\n  available at https://youtu.be/a3wEv-eVlcg . For open source code, see\n  https://github.com/ZJUYH/RaLL", "journal-ref": null, "doi": "10.1109/TITS.2021.3061165", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to the onboard camera and laser scanner, radar sensor provides\nlighting and weather invariant sensing, which is naturally suitable for\nlong-term localization under adverse conditions. However, radar data is sparse\nand noisy, resulting in challenges for radar mapping. On the other hand, the\nmost popular available map currently is built by lidar. In this paper, we\npropose an end-to-end deep learning framework for Radar Localization on Lidar\nMap (RaLL) to bridge the gap, which not only achieves the robust radar\nlocalization but also exploits the mature lidar mapping technique, thus\nreducing the cost of radar mapping. We first embed both sensor modals into a\ncommon feature space by a neural network. Then multiple offsets are added to\nthe map modal for exhaustive similarity evaluation against the current radar\nmodal, yielding the regression of the current pose. Finally, we apply this\ndifferentiable measurement model to a Kalman Filter (KF) to learn the whole\nsequential localization process in an end-to-end manner. \\textit{The whole\nlearning system is differentiable with the network based measurement model at\nthe front-end and KF at the back-end.} To validate the feasibility and\neffectiveness, we employ multi-session multi-scene datasets collected from the\nreal world, and the results demonstrate that our proposed system achieves\nsuperior performance over $90km$ driving, even in generalization scenarios\nwhere the model training is in UK, while testing in South Korea. We also\nrelease the source code publicly.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 13:13:38 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 07:28:54 GMT"}, {"version": "v3", "created": "Sat, 6 Mar 2021 03:17:49 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Yin", "Huan", ""], ["Chen", "Runjian", ""], ["Wang", "Yue", ""], ["Xiong", "Rong", ""]]}, {"id": "2009.07092", "submitter": "Arnaud Boutillon", "authors": "Arnaud Boutillon, Bhushan Borotikar, Val\\'erie Burdin and Pierre-Henri\n  Conze", "title": "Multi-structure bone segmentation in pediatric MR images with combined\n  regularization from shape priors and adversarial network", "comments": "12 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Morphological and diagnostic evaluation of pediatric\nmusculoskeletal system is crucial in clinical practice. However, most\nsegmentation models do not perform well on scarce pediatric imaging data.\nMethods: We propose a regularized convolutional encoder-decoder network for the\nchallenging task of segmenting pediatric magnetic resonance (MR) images. To\novercome the scarcity and heterogeneity of pediatric datasets, we adopt a\nregularization strategy to improve the generalization of segmentation models.\nTo this end, we have conceived a novel optimization scheme for the segmentation\nnetwork which comprises additional regularization terms to the loss function.\nIn order to obtain globally consistent predictions, we incorporate a shape\npriors based regularization, derived from a non-linear shape representation\nlearnt by an auto-encoder. Additionally, an adversarial regularization computed\nby a discriminator is integrated to encourage plausible delineations. Results:\nThe proposed method is evaluated for the task of multi-bone segmentation on two\nscarce pediatric imaging datasets from ankle and shoulder joints, comprising\npathological as well as healthy examinations. The proposed method performed\neither better or at par with previously proposed approaches on the metrics of\nDice, sensitivity, specificity, mean surface distance, relative absolute volume\ndifference, and Hausdorff distance. Conclusion: We illustrate that the proposed\napproach can be easily integrated into various multi-structure strategies and\ncan improve the prediction accuracy of state-of-the-art models. Significance:\nThe obtained results bring new perspectives for the management of pediatric\nmusculoskeletal disorders.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 13:39:53 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 09:39:00 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 09:53:44 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Boutillon", "Arnaud", ""], ["Borotikar", "Bhushan", ""], ["Burdin", "Val\u00e9rie", ""], ["Conze", "Pierre-Henri", ""]]}, {"id": "2009.07100", "submitter": "Sorachi Kato", "authors": "Sorachi Kato, Takeru Fukushima, Tomoki Murakami, Hirantha Abeysekera,\n  Yusuke Iwasaki, Takuya Fujihashi, Takashi Watanabe, Shunsuke Saruwatari", "title": "CSI2Image: Image Reconstruction from Channel State Information Using\n  Generative Adversarial Networks", "comments": "12 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study aims to find the upper limit of the wireless sensing capability of\nacquiring physical space information. This is a challenging objective, because\nat present, wireless sensing studies continue to succeed in acquiring novel\nphenomena. Thus, although a complete answer cannot be obtained yet, a step is\ntaken towards it here. To achieve this, CSI2Image, a novel\nchannel-state-information (CSI)-to-image conversion method based on generative\nadversarial networks (GANs), is proposed. The type of physical information\nacquired using wireless sensing can be estimated by checking wheth\\-er the\nreconstructed image captures the desired physical space information. Three\ntypes of learning methods are demonstrated: gen\\-er\\-a\\-tor-only learning,\nGAN-only learning, and hybrid learning. Evaluating the performance of CSI2Image\nis difficult, because both the clarity of the image and the presence of the\ndesired physical space information must be evaluated. To solve this problem, a\nquantitative evaluation methodology using an object detection library is also\nproposed. CSI2Image was implemented using IEEE 802.11ac compressed CSI, and the\nevaluation results show that the image was successfully reconstructed. The\nresults demonstrate that gen\\-er\\-a\\-tor-only learning is sufficient for simple\nwireless sensing problems, but in complex wireless sensing problems, GANs are\nimportant for reconstructing generalized images with more accurate physical\nspace information.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 13:49:07 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 23:58:49 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Kato", "Sorachi", ""], ["Fukushima", "Takeru", ""], ["Murakami", "Tomoki", ""], ["Abeysekera", "Hirantha", ""], ["Iwasaki", "Yusuke", ""], ["Fujihashi", "Takuya", ""], ["Watanabe", "Takashi", ""], ["Saruwatari", "Shunsuke", ""]]}, {"id": "2009.07109", "submitter": "Roger David Soberanis-Mukul", "authors": "Roger D. Soberanis-Mukul, Shadi Albarqouni, Nassir Navab", "title": "Polyp-artifact relationship analysis using graph inductive learned\n  representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diagnosis process of colorectal cancer mainly focuses on the localization\nand characterization of abnormal growths in the colon tissue known as polyps.\nDespite recent advances in deep object localization, the localization of polyps\nremains challenging due to the similarities between tissues, and the high level\nof artifacts. Recent studies have shown the negative impact of the presence of\nartifacts in the polyp detection task, and have started to take them into\naccount within the training process. However, the use of prior knowledge\nrelated to the spatial interaction of polyps and artifacts has not yet been\nconsidered. In this work, we incorporate artifact knowledge in a\npost-processing step. Our method models this task as an inductive graph\nrepresentation learning problem, and is composed of training and inference\nsteps. Detected bounding boxes around polyps and artifacts are considered as\nnodes connected by a defined criterion. The training step generates a node\nclassifier with ground truth bounding boxes. In inference, we use this\nclassifier to analyze a second graph, generated from artifact and polyp\npredictions given by region proposal networks. We evaluate how the choices in\nthe connectivity and artifacts affect the performance of our method and show\nthat it has the potential to reduce the false positives in the results of a\nregion proposal network.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 13:56:39 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Soberanis-Mukul", "Roger D.", ""], ["Albarqouni", "Shadi", ""], ["Navab", "Nassir", ""]]}, {"id": "2009.07133", "submitter": "D. M. Anisuzzaman", "authors": "D. M. Anisuzzaman (1), Yash Patel (1), Jeffrey Niezgoda (2), Sandeep\n  Gopalakrishnan (3), and Zeyun Yu (1,4) ((1) Department of Computer Science,\n  University of Wisconsin-Milwaukee, Milwaukee, WI, USA,(2) Advancing the\n  Zenith of Healthcare (AZH) Wound and Vascular Center, Milwaukee, WI, USA, (3)\n  College of Nursing, University of Wisconsin Milwaukee, Milwaukee, WI, USA,(4)\n  Department of Biomedical Engineering, University of Wisconsin-Milwaukee,\n  Milwaukee, WI, USA.)", "title": "A Mobile App for Wound Localization using Deep Learning", "comments": "8 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an automated wound localizer from 2D wound and ulcer images by\nusing deep neural network, as the first step towards building an automated and\ncomplete wound diagnostic system. The wound localizer has been developed by\nusing YOLOv3 model, which is then turned into an iOS mobile application. The\ndeveloped localizer can detect the wound and its surrounding tissues and\nisolate the localized wounded region from images, which would be very helpful\nfor future processing such as wound segmentation and classification due to the\nremoval of unnecessary regions from wound images. For Mobile App development\nwith video processing, a lighter version of YOLOv3 named tiny-YOLOv3 has been\nused. The model is trained and tested on our own image dataset in collaboration\nwith AZH Wound and Vascular Center, Milwaukee, Wisconsin. The YOLOv3 model is\ncompared with SSD model, showing that YOLOv3 gives a mAP value of 93.9%, which\nis much better than the SSD model (86.4%). The robustness and reliability of\nthese models are also tested on a publicly available dataset named Medetec and\nshows a very good performance as well.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 14:35:29 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Anisuzzaman", "D. M.", ""], ["Patel", "Yash", ""], ["Niezgoda", "Jeffrey", ""], ["Gopalakrishnan", "Sandeep", ""], ["Yu", "Zeyun", ""]]}, {"id": "2009.07140", "submitter": "Yuying Chen", "authors": "Yuying Chen, Congcong Liu, Bertram E. Shi and Ming Liu", "title": "HGCN-GJS: Hierarchical Graph Convolutional Network with Groupwise Joint\n  Sampling for Trajectory Prediction", "comments": "8 pages, 5 figures, in submission to conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate pedestrian trajectory prediction is of great importance for\ndownstream tasks such as autonomous driving and mobile robot navigation. Fully\ninvestigating the social interactions within the crowd is crucial for accurate\npedestrian trajectory prediction. However, most existing methods do not capture\ngroup level interactions well, focusing only on pairwise interactions and\nneglecting group-wise interactions. In this work, we propose a hierarchical\ngraph convolutional network, HGCN-GJS, for trajectory prediction which well\nleverages group level interactions within the crowd. Furthermore, we introduce\na novel joint sampling scheme for modeling the joint distribution of multiple\npedestrians in the future trajectories. Based on the group information, this\nscheme associates the trajectory of one person with the trajectory of other\npeople in the group, but maintains the independence of the trajectories of\noutsiders. We demonstrate the performance of our network on several trajectory\nprediction datasets, achieving state-of-the-art results on all datasets\nconsidered.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 14:51:10 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Chen", "Yuying", ""], ["Liu", "Congcong", ""], ["Shi", "Bertram E.", ""], ["Liu", "Ming", ""]]}, {"id": "2009.07141", "submitter": "D. M. Anisuzzaman", "authors": "D. M. Anisuzzaman (1), Chuanbo Wang (1), Behrouz Rostami (2), Sandeep\n  Gopalakrishnan (3), Jeffrey Niezgoda (4), and Zeyun Yu (1) ((1) Department of\n  Computer Science, University of Wisconsin-Milwaukee, Milwaukee, WI, USA, (2)\n  Department of Electrical Engineering, University of Wisconsin-Milwaukee,\n  Milwaukee, WI, USA, (3) College of Nursing, University of\n  Wisconsin-Milwaukee, Milwaukee, WI, USA, (4) Jeffrey Niezgoda is with the AZH\n  Wound Center, Milwaukee, WI, USA.)", "title": "Image Based Artificial Intelligence in Wound Assessment: A Systematic\n  Review", "comments": "18 pages, 9 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Efficient and effective assessment of acute and chronic wounds can help wound\ncare teams in clinical practice to greatly improve wound diagnosis, optimize\ntreatment plans, ease the workload and achieve health related quality of life\nto the patient population. While artificial intelligence (AI) has found wide\napplications in health-related sciences and technology, AI-based systems remain\nto be developed clinically and computationally for high-quality wound care. To\nthis end, we have carried out a systematic review of intelligent image-based\ndata analysis and system developments for wound assessment. Specifically, we\nprovide an extensive review of research methods on wound measurement\n(segmentation) and wound diagnosis (classification). We also reviewed recent\nwork on wound assessment systems (including hardware, software, and mobile\napps). More than 250 articles were retrieved from various publication databases\nand online resources, and 115 of them were carefully selected to cover the\nbreadth and depth of most recent and relevant work to convey the current review\nto its fulfillment.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 14:52:14 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Anisuzzaman", "D. M.", ""], ["Wang", "Chuanbo", ""], ["Rostami", "Behrouz", ""], ["Gopalakrishnan", "Sandeep", ""], ["Niezgoda", "Jeffrey", ""], ["Yu", "Zeyun", ""]]}, {"id": "2009.07151", "submitter": "Zhe Xu", "authors": "Zhe Xu, Jie Luo, Jiangpeng Yan, Xiu Li, Jagadeesan Jayender", "title": "F3RNet: Full-Resolution Residual Registration Network for Deformable\n  Image Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable image registration (DIR) is essential for many image-guided\ntherapies. Recently, deep learning approaches have gained substantial\npopularity and success in DIR. Most deep learning approaches use the so-called\nmono-stream \"high-to-low, low-to-high\" network structure, and can achieve\nsatisfactory overall registration results. However, accurate alignments for\nsome severely deformed local regions, which are crucial for pinpointing\nsurgical targets, are often overlooked. Consequently, these approaches are not\nsensitive to some hard-to-align regions, e.g., intra-patient registration of\ndeformed liver lobes. In this paper, we propose a novel unsupervised\nregistration network, namely the Full-Resolution Residual Registration Network\n(F3RNet), for deformable registration of severely deformed organs. The proposed\nmethod combines two parallel processing streams in a residual learning fashion.\nOne stream takes advantage of the full-resolution information that facilitates\naccurate voxel-level registration. The other stream learns the deep multi-scale\nresidual representations to obtain robust recognition. We also factorize the 3D\nconvolution to reduce the training parameters and enhance network efficiency.\nWe validate the proposed method on a clinically acquired intra-patient\nabdominal CT-MRI dataset and a public inspiratory and expiratory thorax CT\ndataset. Experiments on both multimodal and unimodal registration demonstrate\npromising results compared to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 15:05:54 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 19:42:57 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 03:08:38 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Xu", "Zhe", ""], ["Luo", "Jie", ""], ["Yan", "Jiangpeng", ""], ["Li", "Xiu", ""], ["Jayender", "Jagadeesan", ""]]}, {"id": "2009.07168", "submitter": "Zhiwei Wei", "authors": "Zhiwei Wei, Chenzhen Duan, Xinghao Song, Ye Tian, Hongpeng Wang", "title": "AMRNet: Chips Augmentation in Aerial Images Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in aerial images is a challenging task due to the following\nreasons: (1) objects are small and dense relative to images; (2) the object\nscale varies in a wide range; (3) the number of object in different classes is\nimbalanced. Many current methods adopt cropping idea: splitting high resolution\nimages into serials subregions (chips) and detecting on them. However, some\nproblems such as scale variation, object sparsity, and class imbalance exist in\nthe process of training network with chips. In this work, three augmentation\nmethods are introduced to relieve these problems. Specifically, we propose a\nscale adaptive module, which dynamically adjusts chip size to balance object\nscale, narrowing scale variation in training. In addtion, we introduce mosaic\nto augment datasets, relieving object sparity problem. To balance catgory, we\npresent mask resampling to paste object in chips with panoramic segmentation.\nOur model achieves state-of-the-art perfomance on two popular aerial image\ndatasets of VisDrone and UAVDT. Remarkably, three methods can be independently\napplied to detectiors, increasing performance steady without the sacrifice of\ninference efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 15:16:06 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 08:38:25 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Wei", "Zhiwei", ""], ["Duan", "Chenzhen", ""], ["Song", "Xinghao", ""], ["Tian", "Ye", ""], ["Wang", "Hongpeng", ""]]}, {"id": "2009.07190", "submitter": "Elena Limonova", "authors": "Elena Limonova, Daniil Alfonso, Dmitry Nikolaev, Vladimir V. Arlazarov", "title": "ResNet-like Architecture with Low Hardware Requirements", "comments": "Accepted to ICPR 2020. Corrected typos and bibliography formatting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most computationally intensive parts in modern recognition systems\nis an inference of deep neural networks that are used for image classification,\nsegmentation, enhancement, and recognition. The growing popularity of edge\ncomputing makes us look for ways to reduce its time for mobile and embedded\ndevices. One way to decrease the neural network inference time is to modify a\nneuron model to make it moreefficient for computations on a specific device.\nThe example ofsuch a model is a bipolar morphological neuron model. The bipolar\nmorphological neuron is based on the idea of replacing multiplication with\naddition and maximum operations. This model has been demonstrated for simple\nimage classification with LeNet-like architectures [1]. In the paper, we\nintroduce a bipolar morphological ResNet (BM-ResNet) model obtained from a much\nmore complex ResNet architecture by converting its layers to bipolar\nmorphological ones. We apply BM-ResNet to image classification on MNIST and\nCIFAR-10 datasets with only a moderate accuracy decrease from 99.3% to 99.1%\nand from 85.3% to 85.1%. We also estimate the computational complexity of the\nresulting model. We show that for the majority of ResNet layers, the considered\nmodel requires 2.1-2.9 times fewer logic gates for implementation and 15-30%\nlower latency.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 15:54:28 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 16:28:07 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Limonova", "Elena", ""], ["Alfonso", "Daniil", ""], ["Nikolaev", "Dmitry", ""], ["Arlazarov", "Vladimir V.", ""]]}, {"id": "2009.07191", "submitter": "Chen Ma", "authors": "Chen Ma, Shuyu Cheng, Li Chen, Jun Zhu, Junhai Yong", "title": "Switching Transferable Gradient Directions for Query-Efficient Black-Box\n  Adversarial Attacks", "comments": "18 pages, including the supplementary material after the reference\n  section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple and highly query-efficient black-box adversarial attack\nnamed SWITCH, which has a state-of-the-art performance in the score-based\nsetting. SWITCH features a highly efficient and effective utilization of the\ngradient of a surrogate model $\\hat{\\mathbf{g}}$ w.r.t. the input image, i.e.,\nthe transferable gradient. In each iteration, SWITCH first tries to update the\ncurrent sample along the direction of $\\hat{\\mathbf{g}}$, but considers\nswitching to its opposite direction $-\\hat{\\mathbf{g}}$ if our algorithm\ndetects that it does not increase the value of the attack objective function.\nWe justify the choice of switching to the opposite direction by a local\napproximate linearity assumption. In SWITCH, only one or two queries are needed\nper iteration, but it is still effective due to the rich information provided\nby the transferable gradient, thereby resulting in unprecedented query\nefficiency. To improve the robustness of SWITCH, we further propose\nSWITCH$_\\text{RGF}$ in which the update follows the direction of a random\ngradient-free (RGF) estimate when neither $\\hat{\\mathbf{g}}$ nor its opposite\ndirection can increase the objective, while maintaining the advantage of SWITCH\nin terms of query efficiency. Experimental results conducted on CIFAR-10,\nCIFAR-100 and TinyImageNet show that compared with other methods, SWITCH\nachieves a satisfactory attack success rate using much fewer queries, and\nSWITCH$_\\text{RGF}$ achieves the state-of-the-art attack success rate with\nfewer queries overall. Our approach can serve as a strong baseline for future\nblack-box attacks because of its simplicity. The PyTorch source code is\nreleased on https://github.com/machanic/SWITCH.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 15:55:08 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 12:47:32 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ma", "Chen", ""], ["Cheng", "Shuyu", ""], ["Chen", "Li", ""], ["Zhu", "Jun", ""], ["Yong", "Junhai", ""]]}, {"id": "2009.07213", "submitter": "Sudhakaran Jain", "authors": "Sudhakaran Jain and Hamidreza Kasaei", "title": "3D_DEN: Open-ended 3D Object Recognition using Dynamically Expandable\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service robots, in general, have to work independently and adapt to the\ndynamic changes happening in the environment in real-time. One important aspect\nin such scenarios is to continually learn to recognize newer object categories\nwhen they become available. This combines two main research problems namely\ncontinual learning and 3D object recognition. Most of the existing research\napproaches include the use of deep Convolutional Neural Networks (CNNs)\nfocusing on image datasets. A modified approach might be needed for continually\nlearning 3D object categories. A major concern in using CNNs is the problem of\ncatastrophic forgetting when a model tries to learn a new task. Despite various\nproposed solutions to mitigate this problem, there still exist some downsides\nof such solutions, e.g., computational complexity, especially when learning\nsubstantial number of tasks. These downsides can pose major problems in robotic\nscenarios where real-time response plays an essential role. Towards addressing\nthis challenge, we propose a new deep transfer learning approach based on a\ndynamic architectural method to make robots capable of open-ended learning\nabout new 3D object categories. Furthermore, we make sure that the mentioned\ndownsides are minimized to a great extent. Experimental results showed that the\nproposed model outperformed state-of-the-art approaches with regards to\naccuracy and also substantially minimizes computational overhead.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 16:44:18 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 19:41:06 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Jain", "Sudhakaran", ""], ["Kasaei", "Hamidreza", ""]]}, {"id": "2009.07250", "submitter": "Fatema Zohora", "authors": "Fatema Tuz Zohora, M Ziaur Rahman, Ngoc Hieu Tran, Lei Xin, Baozhen\n  Shan, Ming Li", "title": "PointIso: Point Cloud Based Deep Learning Model for Detecting\n  Arbitrary-Precision Peptide Features in LC-MS Map through Attention Based\n  Segmentation", "comments": "16 pages (main text) with 10 figures, then supplementary material of\n  about 5 pages. preprint of journal submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A promising technique of discovering disease biomarkers is to measure the\nrelative protein abundance in multiple biofluid samples through liquid\nchromatography with tandem mass spectrometry (LC-MS/MS) based quantitative\nproteomics. The key step involves peptide feature detection in LC-MS map, along\nwith its charge and intensity. Existing heuristic algorithms suffer from\ninaccurate parameters since different settings of the parameters result in\nsignificantly different outcomes. Therefore, we propose PointIso, to serve the\nnecessity of an automated system for peptide feature detection that is able to\nfind out the proper parameters itself, and is easily adaptable to different\ntypes of datasets. It consists of an attention based scanning step for\nsegmenting the multi-isotopic pattern of peptide features along with charge and\na sequence classification step for grouping those isotopes into potential\npeptide features. PointIso is the first point cloud based, arbitrary-precision\ndeep learning network to address the problem and achieves 98% detection of high\nquality MS/MS identifications in a benchmark dataset, which is higher than\nseveral other widely used algorithms. Besides contributing to the proteomics\nstudy, we believe our novel segmentation technique should serve the general\nimage processing domain as well.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 17:34:14 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Zohora", "Fatema Tuz", ""], ["Rahman", "M Ziaur", ""], ["Tran", "Ngoc Hieu", ""], ["Xin", "Lei", ""], ["Shan", "Baozhen", ""], ["Li", "Ming", ""]]}, {"id": "2009.07265", "submitter": "Kelvin C.K. Chan", "authors": "Kelvin C.K. Chan, Xintao Wang, Ke Yu, Chao Dong, Chen Change Loy", "title": "Understanding Deformable Alignment in Video Super-Resolution", "comments": "Tech report, 15 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable convolution, originally proposed for the adaptation to geometric\nvariations of objects, has recently shown compelling performance in aligning\nmultiple frames and is increasingly adopted for video super-resolution. Despite\nits remarkable performance, its underlying mechanism for alignment remains\nunclear. In this study, we carefully investigate the relation between\ndeformable alignment and the classic flow-based alignment. We show that\ndeformable convolution can be decomposed into a combination of spatial warping\nand convolution. This decomposition reveals the commonality of deformable\nalignment and flow-based alignment in formulation, but with a key difference in\ntheir offset diversity. We further demonstrate through experiments that the\nincreased diversity in deformable alignment yields better-aligned features, and\nhence significantly improves the quality of video super-resolution output.\nBased on our observations, we propose an offset-fidelity loss that guides the\noffset learning with optical flow. Experiments show that our loss successfully\navoids the overflow of offsets and alleviates the instability problem of\ndeformable alignment. Aside from the contributions to deformable alignment, our\nformulation inspires a more flexible approach to introduce offset diversity to\nflow-based alignment, improving its performance.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 17:55:06 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Chan", "Kelvin C. K.", ""], ["Wang", "Xintao", ""], ["Yu", "Ke", ""], ["Dong", "Chao", ""], ["Loy", "Chen Change", ""]]}, {"id": "2009.07327", "submitter": "Przemys{\\l}aw Spurek", "authors": "Szymon Knop, Marcin Mazur, Przemys{\\l}aw Spurek, Jacek Tabor, Igor\n  Podolak", "title": "Generative models with kernel distance in data space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models dealing with modeling a~joint data distribution are\ngenerally either autoencoder or GAN based. Both have their pros and cons,\ngenerating blurry images or being unstable in training or prone to mode\ncollapse phenomenon, respectively. The objective of this paper is to construct\na~model situated between above architectures, one that does not inherit their\nmain weaknesses. The proposed LCW generator (Latent Cramer-Wold generator)\nresembles a classical GAN in transforming Gaussian noise into data space. What\nis of utmost importance, instead of a~discriminator, LCW generator uses kernel\ndistance. No adversarial training is utilized, hence the name generator. It is\ntrained in two phases. First, an autoencoder based architecture, using kernel\nmeasures, is built to model a manifold of data. We propose a Latent Trick\nmapping a Gaussian to latent in order to get the final model. This results in\nvery competitive FID values.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 19:11:47 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Knop", "Szymon", ""], ["Mazur", "Marcin", ""], ["Spurek", "Przemys\u0142aw", ""], ["Tabor", "Jacek", ""], ["Podolak", "Igor", ""]]}, {"id": "2009.07335", "submitter": "Md. Mushfiqur Rahman", "authors": "Md. Mushfiqur Rahman, Thasin Abedin, Khondokar S. S. Prottoy, Ayana\n  Moshruba, Fazlul Hasan Siddiqui", "title": "Video captioning with stacked attention and semantic hard pull", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video captioning, i.e. the task of generating captions from video sequences\ncreates a bridge between the Natural Language Processing and Computer Vision\ndomains of computer science. The task of generating a semantically accurate\ndescription of a video is quite complex. Considering the complexity, of the\nproblem, the results obtained in recent research works are praiseworthy.\nHowever, there is plenty of scope for further investigation. This paper\naddresses this scope and proposes a novel solution. Most video captioning\nmodels comprise two sequential/recurrent layers - one as a video-to-context\nencoder and the other as a context-to-caption decoder. This paper proposes a\nnovel architecture, namely Semantically Sensible Video Captioning (SSVC) which\nmodifies the context generation mechanism by using two novel approaches -\n\"stacked attention\" and \"spatial hard pull\". As there are no exclusive metrics\nfor evaluating video captioning models, we emphasize both quantitative and\nqualitative analysis of our model. Hence, we have used the BLEU scoring metric\nfor quantitative analysis and have proposed a human evaluation metric for\nqualitative analysis, namely the Semantic Sensibility (SS) scoring metric. SS\nScore overcomes the shortcomings of common automated scoring metrics. This\npaper reports that the use of the aforementioned novelties improves the\nperformance of state-of-the-art architectures.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 19:34:37 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 15:10:44 GMT"}, {"version": "v3", "created": "Fri, 16 Jul 2021 18:06:58 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Rahman", "Md. Mushfiqur", ""], ["Abedin", "Thasin", ""], ["Prottoy", "Khondokar S. S.", ""], ["Moshruba", "Ayana", ""], ["Siddiqui", "Fazlul Hasan", ""]]}, {"id": "2009.07338", "submitter": "Logan Courtney", "authors": "Logan Courtney, Ramavarapu Sreenivas", "title": "Comparison of Spatiotemporal Networks for Learning Video Related Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many methods for learning from video sequences involve temporally processing\n2D CNN features from the individual frames or directly utilizing 3D\nconvolutions within high-performing 2D CNN architectures. The focus typically\nremains on how to incorporate the temporal processing within an already stable\nspatial architecture. This work constructs an MNIST-based video dataset with\nparameters controlling relevant facets of common video-related tasks:\nclassification, ordering, and speed estimation. Models trained on this dataset\nare shown to differ in key ways depending on the task and their use of 2D\nconvolutions, 3D convolutions, or convolutional LSTMs. An empirical analysis\nindicates a complex, interdependent relationship between the spatial and\ntemporal dimensions with design choices having a large impact on a network's\nability to learn the appropriate spatiotemporal features.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 19:57:50 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Courtney", "Logan", ""], ["Sreenivas", "Ramavarapu", ""]]}, {"id": "2009.07378", "submitter": "Tomas Hodan", "authors": "Tomas Hodan, Martin Sundermeyer, Bertram Drost, Yann Labbe, Eric\n  Brachmann, Frank Michel, Carsten Rother, Jiri Matas", "title": "BOP Challenge 2020 on 6D Object Localization", "comments": "In ECCV 2020 Workshops Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the evaluation methodology, datasets, and results of the\nBOP Challenge 2020, the third in a series of public competitions organized with\nthe goal to capture the status quo in the field of 6D object pose estimation\nfrom an RGB-D image. In 2020, to reduce the domain gap between synthetic\ntraining and real test RGB images, the participants were provided 350K\nphotorealistic training images generated by BlenderProc4BOP, a new open-source\nand light-weight physically-based renderer (PBR) and procedural data generator.\nMethods based on deep neural networks have finally caught up with methods based\non point pair features, which were dominating previous editions of the\nchallenge. Although the top-performing methods rely on RGB-D image channels,\nstrong results were achieved when only RGB channels were used at both training\nand test time - out of the 26 evaluated methods, the third method was trained\non RGB channels of PBR and real images, while the fifth on RGB channels of PBR\nimages only. Strong data augmentation was identified as a key component of the\ntop-performing CosyPose method, and the photorealism of PBR images was\ndemonstrated effective despite the augmentation. The online evaluation system\nstays open and is available on the project website: bop.felk.cvut.cz.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 22:35:14 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 12:09:44 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Hodan", "Tomas", ""], ["Sundermeyer", "Martin", ""], ["Drost", "Bertram", ""], ["Labbe", "Yann", ""], ["Brachmann", "Eric", ""], ["Michel", "Frank", ""], ["Rother", "Carsten", ""], ["Matas", "Jiri", ""]]}, {"id": "2009.07386", "submitter": "Mehdi Moradi", "authors": "Alexandros Karargyris, Satyananda Kashyap, Ismini Lourentzou, Joy Wu,\n  Arjun Sharma, Matthew Tong, Shafiq Abedin, David Beymer, Vandana Mukherjee,\n  Elizabeth A Krupinski, Mehdi Moradi", "title": "Creation and Validation of a Chest X-Ray Dataset with Eye-tracking and\n  Report Dictation for AI Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a rich dataset of Chest X-Ray (CXR) images to assist\ninvestigators in artificial intelligence. The data were collected using an eye\ntracking system while a radiologist reviewed and reported on 1,083 CXR images.\nThe dataset contains the following aligned data: CXR image, transcribed\nradiology report text, radiologist's dictation audio and eye gaze coordinates\ndata. We hope this dataset can contribute to various areas of research\nparticularly towards explainable and multimodal deep learning / machine\nlearning methods. Furthermore, investigators in disease classification and\nlocalization, automated radiology report generation, and human-machine\ninteraction can benefit from these data. We report deep learning experiments\nthat utilize the attention maps produced by eye gaze dataset to show the\npotential utility of this data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 23:12:49 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 22:59:02 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2020 05:54:40 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Karargyris", "Alexandros", ""], ["Kashyap", "Satyananda", ""], ["Lourentzou", "Ismini", ""], ["Wu", "Joy", ""], ["Sharma", "Arjun", ""], ["Tong", "Matthew", ""], ["Abedin", "Shafiq", ""], ["Beymer", "David", ""], ["Mukherjee", "Vandana", ""], ["Krupinski", "Elizabeth A", ""], ["Moradi", "Mehdi", ""]]}, {"id": "2009.07395", "submitter": "Gabriel Maher", "authors": "Gabriel Maher, Casey Fleeter, Daniele Schiavazzi, Alison Marsden", "title": "Geometric Uncertainty in Patient-Specific Cardiovascular Modeling with\n  Convolutional Dropout Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to generate samples from the conditional\ndistribution of patient-specific cardiovascular models given a clinically\naquired image volume. A convolutional neural network architecture with dropout\nlayers is first trained for vessel lumen segmentation using a regression\napproach, to enable Bayesian estimation of vessel lumen surfaces. This network\nis then integrated into a path-planning patient-specific modeling pipeline to\ngenerate families of cardiovascular models. We demonstrate our approach by\nquantifying the effect of geometric uncertainty on the hemodynamics for three\npatient-specific anatomies, an aorto-iliac bifurcation, an abdominal aortic\naneurysm and a sub-model of the left coronary arteries. A key innovation\nintroduced in the proposed approach is the ability to learn geometric\nuncertainty directly from training data. The results show how geometric\nuncertainty produces coefficients of variation comparable to or larger than\nother sources of uncertainty for wall shear stress and velocity magnitude, but\nhas limited impact on pressure. Specifically, this is true for anatomies\ncharacterized by small vessel sizes, and for local vessel lesions seen\ninfrequently during network training.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 00:13:12 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Maher", "Gabriel", ""], ["Fleeter", "Casey", ""], ["Schiavazzi", "Daniele", ""], ["Marsden", "Alison", ""]]}, {"id": "2009.07409", "submitter": "Ching-Chen Wang", "authors": "Ching-Chen Wang, Ching-Te Chiu, Jheng-Yi Chang", "title": "EfficientNet-eLite: Extremely Lightweight and Efficient CNN Models for\n  Edge Devices by Network Candidate Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding Convolutional Neural Network (CNN) into edge devices for inference\nis a very challenging task because such lightweight hardware is not born to\nhandle this heavyweight software, which is the common overhead from the modern\nstate-of-the-art CNN models. In this paper, targeting at reducing the overhead\nwith trading the accuracy as less as possible, we propose a novel of Network\nCandidate Search (NCS), an alternative way to study the trade-off between the\nresource usage and the performance through grouping concepts and elimination\ntournament. Besides, NCS can also be generalized across any neural network. In\nour experiment, we collect candidate CNN models from EfficientNet-B0 to be\nscaled down in varied way through width, depth, input resolution and compound\nscaling down, applying NCS to research the scaling-down trade-off. Meanwhile, a\nfamily of extremely lightweight EfficientNet is obtained, called\nEfficientNet-eLite. For further embracing the CNN edge application with\nApplication-Specific Integrated Circuit (ASIC), we adjust the architectures of\nEfficientNet-eLite to build the more hardware-friendly version,\nEfficientNet-HF. Evaluation on ImageNet dataset, both proposed\nEfficientNet-eLite and EfficientNet-HF present better parameter usage and\naccuracy than the previous start-of-the-art CNNs. Particularly, the smallest\nmember of EfficientNet-eLite is more lightweight than the best and smallest\nexisting MnasNet with 1.46x less parameters and 0.56% higher accuracy. Code is\navailable at https://github.com/Ching-Chen-Wang/EfficientNet-eLite\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 01:11:10 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Wang", "Ching-Chen", ""], ["Chiu", "Ching-Te", ""], ["Chang", "Jheng-Yi", ""]]}, {"id": "2009.07414", "submitter": "Stephan Lemmer", "authors": "Stephan J. Lemmer and Jason J. Corso", "title": "Ground-truth or DAER: Selective Re-query of Secondary Information", "comments": "Main: 12 pages, 7 figures. Supplementary: 4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many vision tasks use secondary information at inference time -- a seed -- to\nassist a computer vision model in solving a problem. For example, an initial\nbounding box is needed to initialize visual object tracking. To date, all such\nwork makes the tacit assumption that the seed is a good one. However, in\npractice, from crowdsourcing to noisy automated seeds, this is often not the\ncase. We hence propose the problem of seed rejection -- determining whether to\nreject a seed based on the expected performance degradation when it is provided\nin place of a gold-standard seed. We provide a formal definition to this\nproblem, and focus on two meaningful subgoals: understanding causes of error\nand understanding the model's response to noisy seeds conditioned on the\nprimary input. With these goals in mind, we propose a novel training method and\nevaluation metrics for the seed rejection problem. We then use seeded versions\nof viewpoint estimation and fine-grained classification tasks to evaluate these\ncontributions. In these experiments, we show our method can reduce the number\nof seeds that need to be reviewed for a target performance by over 23% compared\nto strong baselines.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 01:44:19 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 19:39:57 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Lemmer", "Stephan J.", ""], ["Corso", "Jason J.", ""]]}, {"id": "2009.07420", "submitter": "Yanyi Zhang", "authors": "Yanyi Zhang, Xinyu Li, Ivan Marsic", "title": "Multi-Label Activity Recognition using Activity-specific Features and\n  Activity Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label activity recognition is designed for recognizing multiple\nactivities that are performed simultaneously or sequentially in each video.\nMost recent activity recognition networks focus on single-activities, that\nassume only one activity in each video. These networks extract shared features\nfor all the activities, which are not designed for multi-label activities. We\nintroduce an approach to multi-label activity recognition that extracts\nindependent feature descriptors for each activity and learns activity\ncorrelations. This structure can be trained end-to-end and plugged into any\nexisting network structures for video classification. Our method outperformed\nstate-of-the-art approaches on four multi-label activity recognition datasets.\nTo better understand the activity-specific features that the system generated,\nwe visualized these activity-specific features in the Charades dataset.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 01:57:34 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 22:37:16 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Zhang", "Yanyi", ""], ["Li", "Xinyu", ""], ["Marsic", "Ivan", ""]]}, {"id": "2009.07432", "submitter": "Mirek Janatka Mr", "authors": "Mirek Janatka, Hani J. Marcus, Neil L. Dorward, Danail Stoyanov", "title": "Surgical Video Motion Magnification with Suppression of Instrument\n  Artefacts", "comments": "Early accept to the Internation Conference on Medical Imaging\n  Computing and Computer Assisted Intervention (MICCAI) 2020 Presentation\n  available here: https://www.youtube.com/watch?v=kKI_Ygny76Q Supplementary\n  video available here: https://www.youtube.com/watch?v=8DUkcHI149Y", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video motion magnification could directly highlight subsurface blood vessels\nin endoscopic video in order to prevent inadvertent damage and bleeding.\nApplying motion filters to the full surgical image is however sensitive to\nresidual motion from the surgical instruments and can impede practical\napplication due to aberration motion artefacts. By storing the temporal filter\nresponse from local spatial frequency information for a single cardiovascular\ncycle prior to tool introduction to the scene, a filter can be used to\ndetermine if motion magnification should be active for a spatial region of the\nsurgical image. In this paper, we propose a strategy to reduce aberration due\nto non-physiological motion for surgical video motion magnification. We present\npromising results on endoscopic transnasal transsphenoidal pituitary surgery\nwith a quantitative comparison to recent methods using Structural Similarity\n(SSIM), as well as qualitative analysis by comparing spatio-temporal cross\nsections of the videos and individual frames.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 02:42:46 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Janatka", "Mirek", ""], ["Marcus", "Hani J.", ""], ["Dorward", "Neil L.", ""], ["Stoyanov", "Danail", ""]]}, {"id": "2009.07433", "submitter": "Pawan Kumar Singh Dr.", "authors": "Pawan Kumar Singh, Iman Chatterjee, Ram Sarkar, Mita Nasipuri", "title": "Handwritten Script Identification from Text Lines", "comments": "12 pages, 4 figures, conference", "journal-ref": "Proc. of 7th International Conference on Advances in\n  Communication, Network and Computing (CNC), 2016", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a multilingual country like India where 12 different official scripts are\nin use, automatic identification of handwritten script facilitates many\nimportant applications such as automatic transcription of multilingual\ndocuments, searching for documents on the web/digital archives containing a\nparticular script and for the selection of script specific Optical Character\nRecognition (OCR) system in a multilingual environment. In this paper, we\npropose a robust method towards identifying scripts from the handwritten\ndocuments at text line-level. The recognition is based upon features extracted\nusing Chain Code Histogram (CCH) and Discrete Fourier Transform (DFT). The\nproposed method is experimented on 800 handwritten text lines written in seven\nIndic scripts namely, Gujarati, Kannada, Malayalam, Oriya, Tamil, Telugu, Urdu\nalong with Roman script and yielded an average identification rate of 95.14%\nusing Support Vector Machine (SVM) classifier.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 02:43:24 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Singh", "Pawan Kumar", ""], ["Chatterjee", "Iman", ""], ["Sarkar", "Ram", ""], ["Nasipuri", "Mita", ""]]}, {"id": "2009.07435", "submitter": "Pawan Kumar Singh Dr.", "authors": "Pawan Kumar Singh, Supratim Das, Ram Sarkar, Mita Nasipuri", "title": "A New Approach for Texture based Script Identification At Block Level\n  using Quad Tree Decomposition", "comments": "13 pages, 5 figures, conference", "journal-ref": "7th International Conference on Advances in Communication, Network\n  and Computing (CNC), pp. 247-259, 2016", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A considerable amount of success has been achieved in developing monolingual\nOCR systems for Indic scripts. But in a country like India, where multi-script\nscenario is prevalent, identifying scripts beforehand becomes obligatory. In\nthis paper, we present the significance of Gabor wavelets filters in extracting\ndirectional energy and entropy distributions for 11 official handwritten\nscripts namely, Bangla, Devanagari, Gujarati, Gurumukhi, Kannada, Malayalam,\nOriya, Tamil, Telugu, Urdu and Roman. The experimentation is conducted at block\nlevel based on a quad-tree decomposition approach and evaluated using six\ndifferent well-known classifiers. Finally, the best identification accuracy of\n96.86% has been achieved by Multi Layer Perceptron (MLP) classifier for 3-fold\ncross validation at level-2 decomposition. The results serve to establish the\nefficacy of the present approach to the classification of handwritten Indic\nscripts\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 02:50:03 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Singh", "Pawan Kumar", ""], ["Das", "Supratim", ""], ["Sarkar", "Ram", ""], ["Nasipuri", "Mita", ""]]}, {"id": "2009.07436", "submitter": "Yu-Wei Zhan", "authors": "Yu-Wei Zhan, Xin Luo, Yu Sun, Yongxin Wang, Zhen-Duo Chen, Xin-Shun Xu", "title": "Weakly-Supervised Online Hashing", "comments": "Accepted by ICME 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of social websites, recent years have witnessed an\nexplosive growth of social images with user-provided tags which continuously\narrive in a streaming fashion. Due to the fast query speed and low storage\ncost, hashing-based methods for image search have attracted increasing\nattention. However, existing hashing methods for social image retrieval are\nbased on batch mode which violates the nature of social images, i.e., social\nimages are usually generated periodically or collected in a stream fashion.\nAlthough there exist many online image hashing methods, they either adopt\nunsupervised learning which ignore the relevant tags, or are designed in the\nsupervised manner which needs high-quality labels. In this paper, to overcome\nthe above limitations, we propose a new method named Weakly-supervised Online\nHashing (WOH). In order to learn high-quality hash codes, WOH exploits the weak\nsupervision by considering the semantics of tags and removing the noise.\nBesides, We develop a discrete online optimization algorithm for WOH, which is\nefficient and scalable. Extensive experiments conducted on two real-world\ndatasets demonstrate the superiority of WOH compared with several\nstate-of-the-art hashing baselines.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 02:50:22 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 13:30:36 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Zhan", "Yu-Wei", ""], ["Luo", "Xin", ""], ["Sun", "Yu", ""], ["Wang", "Yongxin", ""], ["Chen", "Zhen-Duo", ""], ["Xu", "Xin-Shun", ""]]}, {"id": "2009.07447", "submitter": "Yizhi Wang", "authors": "Yizhi Wang and Zhouhui Lian", "title": "Exploring Font-independent Features for Scene Text Recognition", "comments": "accepted by ACM MM 2020. Code: https://actasidiot.github.io/EFIFSTR/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text recognition (STR) has been extensively studied in last few years.\nMany recently-proposed methods are specially designed to accommodate the\narbitrary shape, layout and orientation of scene texts, but ignoring that\nvarious font (or writing) styles also pose severe challenges to STR. These\nmethods, where font features and content features of characters are tangled,\nperform poorly in text recognition on scene images with texts in novel font\nstyles. To address this problem, we explore font-independent features of scene\ntexts via attentional generation of glyphs in a large number of font styles.\nSpecifically, we introduce trainable font embeddings to shape the font styles\nof generated glyphs, with the image feature of scene text only representing its\nessential patterns. The generation process is directed by the spatial attention\nmechanism, which effectively copes with irregular texts and generates\nhigher-quality glyphs than existing image-to-image translation methods.\nExperiments conducted on several STR benchmarks demonstrate the superiority of\nour method compared to the state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 03:36:59 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Wang", "Yizhi", ""], ["Lian", "Zhouhui", ""]]}, {"id": "2009.07451", "submitter": "Lei Zhou", "authors": "Yang Liu, Lei Zhou, Xiao Bai, Lin Gu, Tatsuya Harada, Jun Zhou", "title": "Information Bottleneck Constrained Latent Bidirectional Embedding for\n  Zero-Shot Learning", "comments": "The new version is not complete", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) aims to recognize novel classes by transferring\nsemantic knowledge from seen classes to unseen classes. Though many ZSL methods\nrely on a direct mapping between the visual and the semantic space, the\ncalibration deviation and hubness problem limit the generalization capability\nto unseen classes. Recently emerged generative ZSL methods generate unseen\nimage features to transform ZSL into a supervised classification problem.\nHowever, most generative models still suffer from the seen-unseen bias problem\nas only seen data is used for training. To address these issues, we propose a\nnovel bidirectional embedding based generative model with a tight\nvisual-semantic coupling constraint. We learn a unified latent space that\ncalibrates the embedded parametric distributions of both visual and semantic\nspaces. Since the embedding from high-dimensional visual features comprise much\nnon-semantic information, the alignment of visual and semantic in latent space\nwould inevitably been deviated. Therefore, we introduce information bottleneck\n(IB) constraint to ZSL for the first time to preserve essential attribute\ninformation during the mapping. Specifically, we utilize the uncertainty\nestimation and the wake-sleep procedure to alleviate the feature noises and\nimprove model abstraction capability. In addition, our method can be easily\nextended to transductive ZSL setting by generating labels for unseen images. We\nthen introduce a robust loss to solve this label noise problem. Extensive\nexperimental results show that our method outperforms the state-of-the-art\nmethods in different ZSL settings on most benchmark datasets. The code will be\navailable at https://github.com/osierboy/IBZSL.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 03:54:12 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 07:24:44 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 07:50:10 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Liu", "Yang", ""], ["Zhou", "Lei", ""], ["Bai", "Xiao", ""], ["Gu", "Lin", ""], ["Harada", "Tatsuya", ""], ["Zhou", "Jun", ""]]}, {"id": "2009.07460", "submitter": "Sung-En Chang", "authors": "Sung-En Chang, Yanyu Li, Mengshu Sun, Weiwen Jiang, Runbin Shi, Xue\n  Lin, Yanzhi Wang", "title": "MSP: An FPGA-Specific Mixed-Scheme, Multi-Precision Deep Neural Network\n  Quantization Framework", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the tremendous success of deep learning, there exists imminent need to\ndeploy deep learning models onto edge devices. To tackle the limited computing\nand storage resources in edge devices, model compression techniques have been\nwidely used to trim deep neural network (DNN) models for on-device inference\nexecution. This paper targets the commonly used FPGA (field programmable gate\narray) devices as the hardware platforms for DNN edge computing. We focus on\nthe DNN quantization as the main model compression technique, since DNN\nquantization has been of great importance for the implementations of DNN models\non the hardware platforms. The novelty of this work comes in twofold: (i) We\npropose a mixed-scheme DNN quantization method that incorporates both the\nlinear and non-linear number systems for quantization, with the aim to boost\nthe utilization of the heterogeneous computing resources, i.e., LUTs (look up\ntables) and DSPs (digital signal processors) on an FPGA. Note that all the\nexisting (single-scheme) quantization methods can only utilize one type of\nresources (either LUTs or DSPs for the MAC (multiply-accumulate) operations in\ndeep learning computations. (ii) We use a quantization method that supports\nmultiple precisions along the intra-layer dimension, while the existing\nquantization methods apply multi-precision quantization along the inter-layer\ndimension. The intra-layer multi-precision method can uniform the hardware\nconfigurations for different layers to reduce computation overhead and at the\nsame time preserve the model accuracy as the inter-layer approach.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 04:24:18 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 01:58:38 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Chang", "Sung-En", ""], ["Li", "Yanyu", ""], ["Sun", "Mengshu", ""], ["Jiang", "Weiwen", ""], ["Shi", "Runbin", ""], ["Lin", "Xue", ""], ["Wang", "Yanzhi", ""]]}, {"id": "2009.07462", "submitter": "Qiang Fu", "authors": "Qiang Fu, Jialong Wang, Hongshan Yu, Islam Ali, Feng Guo, Yijia He,\n  Hong Zhang", "title": "PL-VINS: Real-Time Monocular Visual-Inertial SLAM with Point and Line\n  Features", "comments": "Visual-Inertial SLAM, LSD, Lines, SLAM, VINS-Mono", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging line features to improve localization accuracy of point-based\nvisual-inertial SLAM (VINS) is gaining interest as they provide additional\nconstraints on scene structure. However, real-time performance when\nincorporating line features in VINS has not been addressed. This paper presents\nPL-VINS, a real-time optimization-based monocular VINS method with point and\nline features, developed based on the state-of-the-art point-based VINS-Mono\n\\cite{vins}. We observe that current works use the LSD \\cite{lsd} algorithm to\nextract line features; however, LSD is designed for scene shape representation\ninstead of the pose estimation problem, which becomes the bottleneck for the\nreal-time performance due to its high computational cost. In this paper, a\nmodified LSD algorithm is presented by studying a hidden parameter tuning and\nlength rejection strategy. The modified LSD can run at least three times as\nfast as LSD. Further, by representing space lines with the Pl\\\"{u}cker\ncoordinates, the residual error in line estimation is modeled in terms of the\npoint-to-line distance, which is then minimized by iteratively updating the\nminimum four-parameter orthonormal representation of the Pl\\\"{u}cker\ncoordinates. Experiments in a public benchmark dataset show that the\nlocalization error of our method is 12-16\\% less than that of VINS-Mono at the\nsame pose update frequency. %For the benefit of the community, The source code\nof our method is available at: https://github.com/cnqiangfu/PL-VINS.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 04:27:33 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 20:14:30 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Fu", "Qiang", ""], ["Wang", "Jialong", ""], ["Yu", "Hongshan", ""], ["Ali", "Islam", ""], ["Guo", "Feng", ""], ["He", "Yijia", ""], ["Zhang", "Hong", ""]]}, {"id": "2009.07469", "submitter": "Lequan Yu", "authors": "Lequan Yu, Zhicheng Zhang, Xiaomeng Li, Lei Xing", "title": "Deep Sinogram Completion with Image Prior for Metal Artifact Reduction\n  in CT Images", "comments": "Accepted by IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed tomography (CT) has been widely used for medical diagnosis,\nassessment, and therapy planning and guidance. In reality, CT images may be\naffected adversely in the presence of metallic objects, which could lead to\nsevere metal artifacts and influence clinical diagnosis or dose calculation in\nradiation therapy. In this paper, we propose a generalizable framework for\nmetal artifact reduction (MAR) by simultaneously leveraging the advantages of\nimage domain and sinogram domain-based MAR techniques. We formulate our\nframework as a sinogram completion problem and train a neural network (SinoNet)\nto restore the metal-affected projections. To improve the continuity of the\ncompleted projections at the boundary of metal trace and thus alleviate new\nartifacts in the reconstructed CT images, we train another neural network\n(PriorNet) to generate a good prior image to guide sinogram learning, and\nfurther design a novel residual sinogram learning strategy to effectively\nutilize the prior image information for better sinogram completion. The two\nnetworks are jointly trained in an end-to-end fashion with a differentiable\nforward projection (FP) operation so that the prior image generation and deep\nsinogram completion procedures can benefit from each other. Finally, the\nartifact-reduced CT images are reconstructed using the filtered backward\nprojection (FBP) from the completed sinogram. Extensive experiments on\nsimulated and real artifacts data demonstrate that our method produces superior\nartifact-reduced results while preserving the anatomical structures and\noutperforms other MAR methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 04:43:35 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Yu", "Lequan", ""], ["Zhang", "Zhicheng", ""], ["Li", "Xiaomeng", ""], ["Xing", "Lei", ""]]}, {"id": "2009.07470", "submitter": "Sathyanarayanan Aakur", "authors": "Sathyanarayanan N. Aakur, Sanjoy Kundu, Nikhil Gunti", "title": "Knowledge Guided Learning: Towards Open Domain Egocentric Action\n  Recognition with Zero Supervision", "comments": "8 pages, 2 figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in deep learning have enabled the development of models that have\nexhibited a remarkable tendency to recognize and even localize actions in\nvideos. However, they tend to experience errors when faced with scenes or\nexamples beyond their initial training environment. Hence, they fail to adapt\nto new domains without significant retraining with large amounts of annotated\ndata. Current algorithms are trained in an inductive learning environment where\nthey use data-driven models to learn associations between input observations\nwith a fixed set of known classes. In this paper, we propose to overcome these\nlimitations by moving to an open world setting by decoupling the ideas of\nrecognition and reasoning. Building upon the compositional representation\noffered by Grenander's Pattern Theory formalism, we show that attention and\ncommonsense knowledge can be used to enable the self-supervised discovery of\nnovel actions in egocentric videos in an open-world setting, a considerably\nmore difficult task than zero-shot learning and (un)supervised domain\nadaptation tasks where target domain data (both labeled and unlabeled) are\navailable during training. We show that our approach can be used to infer and\nlearn novel classes for open vocabulary classification in egocentric videos and\nnovel object detection with zero supervision. Extensive experiments show that\nit performs competitively with fully supervised baselines on publicly available\ndatasets under open-world conditions. This is one of the first works to address\nthe problem of open-world action recognition in egocentric videos with zero\nhuman supervision to the best of our knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 04:44:51 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Aakur", "Sathyanarayanan N.", ""], ["Kundu", "Sanjoy", ""], ["Gunti", "Nikhil", ""]]}, {"id": "2009.07480", "submitter": "Shahroz Tariq", "authors": "Shahroz Tariq, Sangyup Lee and Simon S. Woo", "title": "A Convolutional LSTM based Residual Network for Deepfake Video Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning-based video manipulation methods have become\nwidely accessible to masses. With little to no effort, people can easily learn\nhow to generate deepfake videos with only a few victims or target images. This\ncreates a significant social problem for everyone whose photos are publicly\navailable on the Internet, especially on social media websites. Several deep\nlearning-based detection methods have been developed to identify these\ndeepfakes. However, these methods lack generalizability, because they perform\nwell only for a specific type of deepfake method. Therefore, those methods are\nnot transferable to detect other deepfake methods. Also, they do not take\nadvantage of the temporal information of the video. In this paper, we addressed\nthese limitations. We developed a Convolutional LSTM based Residual Network\n(CLRNet), which takes a sequence of consecutive images as an input from a video\nto learn the temporal information that helps in detecting unnatural looking\nartifacts that are present between frames of deepfake videos. We also propose a\ntransfer learning-based approach to generalize different deepfake methods.\nThrough rigorous experimentations using the FaceForensics++ dataset, we showed\nthat our method outperforms five of the previously proposed state-of-the-art\ndeepfake detection methods by better generalizing at detecting different\ndeepfake methods using the same model.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 05:57:06 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Tariq", "Shahroz", ""], ["Lee", "Sangyup", ""], ["Woo", "Simon S.", ""]]}, {"id": "2009.07485", "submitter": "Hossein Khosravi", "authors": "Hossein Gholamalinezhad and Hossein Khosravi", "title": "Pooling Methods in Deep Neural Networks, a Review", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, Deep Neural Networks are among the main tools used in various\nsciences. Convolutional Neural Network is a special type of DNN consisting of\nseveral convolution layers, each followed by an activation function and a\npooling layer. The pooling layer is an important layer that executes the\ndown-sampling on the feature maps coming from the previous layer and produces\nnew feature maps with a condensed resolution. This layer drastically reduces\nthe spatial dimension of input. It serves two main purposes. The first is to\nreduce the number of parameters or weights, thus lessening the computational\ncost. The second is to control the overfitting of the network. An ideal pooling\nmethod is expected to extract only useful information and discard irrelevant\ndetails. There are a lot of methods for the implementation of pooling operation\nin Deep Neural Networks. In this paper, we reviewed some of the famous and\nuseful pooling methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 06:11:40 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Gholamalinezhad", "Hossein", ""], ["Khosravi", "Hossein", ""]]}, {"id": "2009.07491", "submitter": "Zhikang Wang", "authors": "Zhikang Wang, Lihuo He, Xinbo Gao, Jane Shen", "title": "Robust Person Re-Identification through Contextual Mutual Boosting", "comments": "8 pages, 4 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-Identification (Re-ID) has witnessed great advance, driven by the\ndevelopment of deep learning. However, modern person Re-ID is still challenged\nby background clutter, occlusion and large posture variation which are common\nin practice. Previous methods tackle these challenges by localizing pedestrians\nthrough external cues (e.g., pose estimation, human parsing) or attention\nmechanism, suffering from high computation cost and increased model complexity.\nIn this paper, we propose the Contextual Mutual Boosting Network (CMBN). It\nlocalizes pedestrians and recalibrates features by effectively exploiting\ncontextual information and statistical inference. Firstly, we construct two\nbranches with a shared convolutional frontend to learn the foreground and\nbackground features respectively. By enabling interaction between these two\nbranches, they boost the accuracy of the spatial localization mutually.\nSecondly, starting from a statistical perspective, we propose the Mask\nGenerator that exploits the activation distribution of the transformation\nmatrix for generating the static channel mask to the representations. The mask\nrecalibrates the features to amplify the valuable characteristics and diminish\nthe noise. Finally, we propose the Contextual-Detachment Strategy to optimize\nthe two branches jointly and independently, which further enhances the\nlocalization precision. Experiments on the benchmarks demonstrate the\nsuperiority of the architecture compared the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 06:33:35 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Wang", "Zhikang", ""], ["He", "Lihuo", ""], ["Gao", "Xinbo", ""], ["Shen", "Jane", ""]]}, {"id": "2009.07498", "submitter": "Lijian Lin", "authors": "Lijian Lin, Haosheng Chen, Honglun Zhang, Jun Liang, Yu Li, Ying Shan,\n  Hanzi Wang", "title": "Dual Semantic Fusion Network for Video Object Detection", "comments": "9 pages,6 figures", "journal-ref": "ACM Multimedia 2020", "doi": "10.1145/3394171.3413583", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video object detection is a tough task due to the deteriorated quality of\nvideo sequences captured under complex environments. Currently, this area is\ndominated by a series of feature enhancement based methods, which distill\nbeneficial semantic information from multiple frames and generate enhanced\nfeatures through fusing the distilled information. However, the distillation\nand fusion operations are usually performed at either frame level or instance\nlevel with external guidance using additional information, such as optical flow\nand feature memory. In this work, we propose a dual semantic fusion network\n(abbreviated as DSFNet) to fully exploit both frame-level and instance-level\nsemantics in a unified fusion framework without external guidance. Moreover, we\nintroduce a geometric similarity measure into the fusion process to alleviate\nthe influence of information distortion caused by noise. As a result, the\nproposed DSFNet can generate more robust features through the multi-granularity\nfusion and avoid being affected by the instability of external guidance. To\nevaluate the proposed DSFNet, we conduct extensive experiments on the ImageNet\nVID dataset. Notably, the proposed dual semantic fusion network achieves, to\nthe best of our knowledge, the best performance of 84.1\\% mAP among the current\nstate-of-the-art video object detectors with ResNet-101 and 85.4\\% mAP with\nResNeXt-101 without using any post-processing steps.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 06:49:17 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Lin", "Lijian", ""], ["Chen", "Haosheng", ""], ["Zhang", "Honglun", ""], ["Liang", "Jun", ""], ["Li", "Yu", ""], ["Shan", "Ying", ""], ["Wang", "Hanzi", ""]]}, {"id": "2009.07501", "submitter": "Ji Yuanfeng", "authors": "Yuanfeng Ji, Ruimao Zhang, Zhen Li, Jiamin Ren, Shaoting Zhang, Ping\n  Luo", "title": "UXNet: Searching Multi-level Feature Aggregation for 3D Medical Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregating multi-level feature representation plays a critical role in\nachieving robust volumetric medical image segmentation, which is important for\nthe auxiliary diagnosis and treatment. Unlike the recent neural architecture\nsearch (NAS) methods that typically searched the optimal operators in each\nnetwork layer, but missed a good strategy to search for feature aggregations,\nthis paper proposes a novel NAS method for 3D medical image segmentation, named\nUXNet, which searches both the scale-wise feature aggregation strategies as\nwell as the block-wise operators in the encoder-decoder network. UXNet has\nseveral appealing benefits. (1) It significantly improves flexibility of the\nclassical UNet architecture, which only aggregates feature representations of\nencoder and decoder in equivalent resolution. (2) A continuous relaxation of\nUXNet is carefully designed, enabling its searching scheme performed in an\nefficient differentiable manner. (3) Extensive experiments demonstrate the\neffectiveness of UXNet compared with recent NAS methods for medical image\nsegmentation. The architecture discovered by UXNet outperforms existing\nstate-of-the-art models in terms of Dice on several public 3D medical image\nsegmentation benchmarks, especially for the boundary locations and tiny\ntissues. The searching computational complexity of UXNet is cheap, enabling to\nsearch a network with the best performance less than 1.5 days on two TitanXP\nGPUs.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 06:50:57 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Ji", "Yuanfeng", ""], ["Zhang", "Ruimao", ""], ["Li", "Zhen", ""], ["Ren", "Jiamin", ""], ["Zhang", "Shaoting", ""], ["Luo", "Ping", ""]]}, {"id": "2009.07506", "submitter": "Xuehui Yu", "authors": "Xuehui Yu, Zhenjun Han, Yuqi Gong, Nan Jiang, Jian Zhao, Qixiang Ye,\n  Jie Chen, Yuan Feng, Bin Zhang, Xiaodi Wang, Ying Xin, Jingwei Liu, Mingyuan\n  Mao, Sheng Xu, Baochang Zhang, Shumin Han, Cheng Gao, Wei Tang, Lizuo Jin,\n  Mingbo Hong, Yuchao Yang, Shuiwang Li, Huan Luo, Qijun Zhao, and Humphrey Shi", "title": "The 1st Tiny Object Detection Challenge:Methods and Results", "comments": "ECCV2020 Workshop on Real-world Computer Vision from Inputs with\n  Limited Quality (RLQ) and Tiny Object Detection Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 1st Tiny Object Detection (TOD) Challenge aims to encourage research in\ndeveloping novel and accurate methods for tiny object detection in images which\nhave wide views, with a current focus on tiny person detection. The TinyPerson\ndataset was used for the TOD Challenge and is publicly released. It has 1610\nimages and 72651 box-levelannotations. Around 36 participating teams from the\nglobe competed inthe 1st TOD Challenge. In this paper, we provide a brief\nsummary of the1st TOD Challenge including brief introductions to the top three\nmethods.The submission leaderboard will be reopened for researchers that\nareinterested in the TOD challenge. The benchmark dataset and other information\ncan be found at: https://github.com/ucas-vg/TinyBenchmark.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 07:01:38 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 06:31:17 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Yu", "Xuehui", ""], ["Han", "Zhenjun", ""], ["Gong", "Yuqi", ""], ["Jiang", "Nan", ""], ["Zhao", "Jian", ""], ["Ye", "Qixiang", ""], ["Chen", "Jie", ""], ["Feng", "Yuan", ""], ["Zhang", "Bin", ""], ["Wang", "Xiaodi", ""], ["Xin", "Ying", ""], ["Liu", "Jingwei", ""], ["Mao", "Mingyuan", ""], ["Xu", "Sheng", ""], ["Zhang", "Baochang", ""], ["Han", "Shumin", ""], ["Gao", "Cheng", ""], ["Tang", "Wei", ""], ["Jin", "Lizuo", ""], ["Hong", "Mingbo", ""], ["Yang", "Yuchao", ""], ["Li", "Shuiwang", ""], ["Luo", "Huan", ""], ["Zhao", "Qijun", ""], ["Shi", "Humphrey", ""]]}, {"id": "2009.07526", "submitter": "Yuan Chai", "authors": "Jing Yu, Yuan Chai, Yujing Wang, Yue Hu, Qi Wu", "title": "CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation", "comments": "Accepted by IJCAI 2021. SOLE copyright holder is IJCAI (International\n  Joint Conferences on Artificial Intelligence)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene graphs are semantic abstraction of images that encourage visual\nunderstanding and reasoning. However, the performance of Scene Graph Generation\n(SGG) is unsatisfactory when faced with biased data in real-world scenarios.\nConventional debiasing research mainly studies from the view of balancing data\ndistribution or learning unbiased models and representations, ignoring the\ncorrelations among the biased classes. In this work, we analyze this problem\nfrom a novel cognition perspective: automatically building a hierarchical\ncognitive structure from the biased predictions and navigating that hierarchy\nto locate the relationships, making the tail relationships receive more\nattention in a coarse-to-fine mode. To this end, we propose a novel debiasing\nCognition Tree (CogTree) loss for unbiased SGG. We first build a cognitive\nstructure CogTree to organize the relationships based on the prediction of a\nbiased SGG model. The CogTree distinguishes remarkably different relationships\nat first and then focuses on a small portion of easily confused ones. Then, we\npropose a debiasing loss specially for this cognitive structure, which supports\ncoarse-to-fine distinction for the correct relationships. The loss is\nmodel-agnostic and consistently boosting the performance of several\nstate-of-the-art models. The code is available at:\nhttps://github.com/CYVincent/Scene-Graph-Transformer-CogTree.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 07:47:26 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 06:27:33 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Yu", "Jing", ""], ["Chai", "Yuan", ""], ["Wang", "Yujing", ""], ["Hu", "Yue", ""], ["Wu", "Qi", ""]]}, {"id": "2009.07529", "submitter": "Rizhao Cai", "authors": "Rizhao Cai, Haoliang Li, Shiqi Wang, Changsheng Chen, and Alex\n  Chichung Kot", "title": "DRL-FAS: A Novel Framework Based on Deep Reinforcement Learning for Face\n  Anti-Spoofing", "comments": "Accepted by IEEE Transactions on Information Forensics and Security.\n  Code will be released soon", "journal-ref": null, "doi": "10.1109/TIFS.2020.3026553", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the philosophy employed by human beings to determine whether a\npresented face example is genuine or not, i.e., to glance at the example\nglobally first and then carefully observe the local regions to gain more\ndiscriminative information, for the face anti-spoofing problem, we propose a\nnovel framework based on the Convolutional Neural Network (CNN) and the\nRecurrent Neural Network (RNN). In particular, we model the behavior of\nexploring face-spoofing-related information from image sub-patches by\nleveraging deep reinforcement learning. We further introduce a recurrent\nmechanism to learn representations of local information sequentially from the\nexplored sub-patches with an RNN. Finally, for the classification purpose, we\nfuse the local information with the global one, which can be learned from the\noriginal input image through a CNN. Moreover, we conduct extensive experiments,\nincluding ablation study and visualization analysis, to evaluate our proposed\nframework on various public databases. The experiment results show that our\nmethod can generally achieve state-of-the-art performance among all scenarios,\ndemonstrating its effectiveness.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 07:58:01 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 06:08:06 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Cai", "Rizhao", ""], ["Li", "Haoliang", ""], ["Wang", "Shiqi", ""], ["Chen", "Changsheng", ""], ["Kot", "Alex Chichung", ""]]}, {"id": "2009.07530", "submitter": "Luca Parisi", "authors": "Luca Parisi", "title": "m-arcsinh: An Efficient and Reliable Function for SVM and MLP in\n  scikit-learn", "comments": "20 pages, 4 listings/Python code snippets, 2 figures, 15 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the 'm-arcsinh', a modified ('m-') version of the\ninverse hyperbolic sine function ('arcsinh'). Kernel and activation functions\nenable Machine Learning (ML)-based algorithms, such as Support Vector Machine\n(SVM) and Multi-Layer Perceptron (MLP), to learn from data in a supervised\nmanner. m-arcsinh, implemented in the open source Python library\n'scikit-learn', is hereby presented as an efficient and reliable kernel and\nactivation function for SVM and MLP respectively. Improvements in reliability\nand speed to convergence in classification tasks on fifteen (N = 15) datasets\navailable from scikit-learn and the University California Irvine (UCI) Machine\nLearning repository are discussed. Experimental results demonstrate the overall\ncompetitive classification performance of both SVM and MLP, achieved via the\nproposed function. This function is compared to gold standard kernel and\nactivation functions, demonstrating its overall competitive reliability\nregardless of the complexity of the classification tasks involved.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 07:59:15 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Parisi", "Luca", ""]]}, {"id": "2009.07532", "submitter": "Anupiya Nugaliyadde Dr", "authors": "A Nugaliyadde, Kok Wai Wong, Jeremy Parry, Ferdous Sohel, Hamid Laga,\n  Upeka V. Somaratne, Chris Yeomans, Orchid Foster", "title": "RCNN for Region of Interest Detection in Whole Slide Images", "comments": "This paper was accepted to the 27th International Conference on\n  Neural Information Processing (ICONIP 2020) and will be published in the\n  Springer CCIS Series", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital pathology has attracted significant attention in recent years.\nAnalysis of Whole Slide Images (WSIs) is challenging because they are very\nlarge, i.e., of Giga-pixel resolution. Identifying Regions of Interest (ROIs)\nis the first step for pathologists to analyse further the regions of diagnostic\ninterest for cancer detection and other anomalies. In this paper, we\ninvestigate the use of RCNN, which is a deep machine learning technique, for\ndetecting such ROIs only using a small number of labelled WSIs for training.\nFor experimentation, we used real WSIs from a public hospital pathology service\nin Western Australia. We used 60 WSIs for training the RCNN model and another\n12 WSIs for testing. The model was further tested on a new set of unseen WSIs.\nThe results show that RCNN can be effectively used for ROI detection from WSIs.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 08:00:17 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 01:14:25 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Nugaliyadde", "A", ""], ["Wong", "Kok Wai", ""], ["Parry", "Jeremy", ""], ["Sohel", "Ferdous", ""], ["Laga", "Hamid", ""], ["Somaratne", "Upeka V.", ""], ["Yeomans", "Chris", ""], ["Foster", "Orchid", ""]]}, {"id": "2009.07536", "submitter": "Guoqing Zhang", "authors": "Guoqing Zhang, Junchuan Yang, Yuhui Zheng, Yi Wu, Shengyong Chen", "title": "Hybrid-Attention Guided Network with Multiple Resolution Features for\n  Person Re-Identification", "comments": "11 pages, 8 figures, 66 conferences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting effective and discriminative features is very important for\naddressing the challenging person re-identification (re-ID) task. Prevailing\ndeep convolutional neural networks (CNNs) usually use high-level features for\nidentifying pedestrian. However, some essential spatial information resided in\nlow-level features such as shape, texture and color will be lost when learning\nthe high-level features, due to extensive padding and pooling operations in the\ntraining stage. In addition, most existing person re-ID methods are mainly\nbased on hand-craft bounding boxes where images are precisely aligned. It is\nunrealistic in practical applications, since the exploited object detection\nalgorithms often produce inaccurate bounding boxes. This will inevitably\ndegrade the performance of existing algorithms. To address these problems, we\nput forward a novel person re-ID model that fuses high- and low-level\nembeddings to reduce the information loss caused in learning high-level\nfeatures. Then we divide the fused embedding into several parts and reconnect\nthem to obtain the global feature and more significant local features, so as to\nalleviate the affect caused by the inaccurate bounding boxes. In addition, we\nalso introduce the spatial and channel attention mechanisms in our model, which\naims to mine more discriminative features related to the target. Finally, we\nreconstruct the feature extractor to ensure that our model can obtain more\nricher and robust features. Extensive experiments display the superiority of\nour approach compared with existing approaches. Our code is available at\nhttps://github.com/libraflower/MutipleFeature-for-PRID.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 08:12:42 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 03:05:03 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zhang", "Guoqing", ""], ["Yang", "Junchuan", ""], ["Zheng", "Yuhui", ""], ["Wu", "Yi", ""], ["Chen", "Shengyong", ""]]}, {"id": "2009.07557", "submitter": "Daichi Horita", "authors": "Daichi Horita and Kiyoharu Aizawa", "title": "SLGAN: Style- and Latent-guided Generative Adversarial Network for\n  Desirable Makeup Transfer and Removal", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are five features to consider when using generative adversarial\nnetworks to apply makeup to photos of the human face. These features include\n(1) facial components, (2) interactive color adjustments, (3) makeup\nvariations, (4) robustness to poses and expressions, and the (5) use of\nmultiple reference images. Several related works have been proposed, mainly\nusing generative adversarial networks (GAN). Unfortunately, none of them have\naddressed all five features simultaneously. This paper closes the gap with an\ninnovative style- and latent-guided GAN (SLGAN). We provide a novel, perceptual\nmakeup loss and a style-invariant decoder that can transfer makeup styles based\non histogram matching to avoid the identity-shift problem. In our experiments,\nwe show that our SLGAN is better than or comparable to state-of-the-art\nmethods. Furthermore, we show that our proposal can interpolate facial makeup\nimages to determine the unique features, compare existing methods, and help\nusers find desirable makeup configurations.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 08:54:20 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 01:58:37 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2020 13:08:51 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Horita", "Daichi", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "2009.07560", "submitter": "Marija Jegorova", "authors": "Jean de Bodinat, Thomas Guerneve, Jose Vazquez, Marija Jegorova", "title": "Similarity-based data mining for online domain adaptation of a sonar ATR\n  system", "comments": "Accepted for publication in IEEE OCEANS2020", "journal-ref": "IEEE OCEANS2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the expensive nature of field data gathering, the lack of training\ndata often limits the performance of Automatic Target Recognition (ATR)\nsystems. This problem is often addressed with domain adaptation techniques,\nhowever the currently existing methods fail to satisfy the constraints of\nresource and time-limited underwater systems. We propose to address this issue\nvia an online fine-tuning of the ATR algorithm using a novel data-selection\nmethod. Our proposed data-mining approach relies on visual similarity and\noutperforms the traditionally employed hard-mining methods. We present a\ncomparative performance analysis in a wide range of simulated environments and\nhighlight the benefits of using our method for the rapid adaptation to\npreviously unseen environments.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 09:07:54 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["de Bodinat", "Jean", ""], ["Guerneve", "Thomas", ""], ["Vazquez", "Jose", ""], ["Jegorova", "Marija", ""]]}, {"id": "2009.07565", "submitter": "Simone Palazzo", "authors": "Simone Palazzo, Dario C. Guastella, Luciano Cantelli, Paolo Spadaro,\n  Francesco Rundo, Giovanni Muscato, Daniela Giordano, Concetto Spampinato", "title": "Domain Adaptation for Outdoor Robot Traversability Estimation from RGB\n  data with Safety-Preserving Loss", "comments": "Accepted at IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to estimate the traversability of the area surrounding a mobile\nrobot is a fundamental task in the design of a navigation algorithm. However,\nthe task is often complex, since it requires evaluating distances from\nobstacles, type and slope of terrain, and dealing with non-obvious\ndiscontinuities in detected distances due to perspective. In this paper, we\npresent an approach based on deep learning to estimate and anticipate the\ntraversing score of different routes in the field of view of an on-board RGB\ncamera. The backbone of the proposed model is based on a state-of-the-art deep\nsegmentation model, which is fine-tuned on the task of predicting route\ntraversability. We then enhance the model's capabilities by a) addressing\ndomain shifts through gradient-reversal unsupervised adaptation, and b)\naccounting for the specific safety requirements of a mobile robot, by\nencouraging the model to err on the safe side, i.e., penalizing errors that\nwould cause collisions with obstacles more than those that would cause the\nrobot to stop in advance. Experimental results show that our approach is able\nto satisfactorily identify traversable areas and to generalize to unseen\nlocations.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 09:19:33 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Palazzo", "Simone", ""], ["Guastella", "Dario C.", ""], ["Cantelli", "Luciano", ""], ["Spadaro", "Paolo", ""], ["Rundo", "Francesco", ""], ["Muscato", "Giovanni", ""], ["Giordano", "Daniela", ""], ["Spampinato", "Concetto", ""]]}, {"id": "2009.07573", "submitter": "Mark Graham", "authors": "Mark S. Graham, Carole H. Sudre, Thomas Varsavsky, Petru-Daniel\n  Tudosiu, Parashkev Nachev, Sebastien Ourselin, and M. Jorge Cardoso", "title": "Hierarchical brain parcellation with uncertainty", "comments": "To be published in the MICCAI 2020 workshop: Uncertainty for Safe\n  Utilization of Machine Learning in Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many atlases used for brain parcellation are hierarchically organised,\nprogressively dividing the brain into smaller sub-regions. However,\nstate-of-the-art parcellation methods tend to ignore this structure and treat\nlabels as if they are `flat'. We introduce a hierarchically-aware brain\nparcellation method that works by predicting the decisions at each branch in\nthe label tree. We further show how this method can be used to model\nuncertainty separately for every branch in this label tree. Our method exceeds\nthe performance of flat uncertainty methods, whilst also providing decomposed\nuncertainty estimates that enable us to obtain self-consistent parcellations\nand uncertainty maps at any level of the label hierarchy. We demonstrate a\nsimple way these decision-specific uncertainty maps may be used to provided\nuncertainty-thresholded tissue maps at any level of the label tree.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 09:37:53 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Graham", "Mark S.", ""], ["Sudre", "Carole H.", ""], ["Varsavsky", "Thomas", ""], ["Tudosiu", "Petru-Daniel", ""], ["Nachev", "Parashkev", ""], ["Ourselin", "Sebastien", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "2009.07576", "submitter": "Mehmet Kerim Yucel", "authors": "Yunus Can Bilge, Mehmet Kerim Yucel, Ramazan Gokberk Cinbis, Nazli\n  Ikizler-Cinbis, Pinar Duygulu", "title": "Red Carpet to Fight Club: Partially-supervised Domain Transfer for Face\n  Recognition in Violent Videos", "comments": "To appear in WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world problems, there is typically a large discrepancy between\nthe characteristics of data used in training versus deployment. A prime example\nis the analysis of aggression videos: in a criminal incidence, typically\nsuspects need to be identified based on their clean portrait-like photos,\ninstead of their prior video recordings. This results in three major\nchallenges; large domain discrepancy between violence videos and ID-photos, the\nlack of video examples for most individuals and limited training data\navailability. To mimic such scenarios, we formulate a realistic domain-transfer\nproblem, where the goal is to transfer the recognition model trained on clean\nposed images to the target domain of violent videos, where training videos are\navailable only for a subset of subjects. To this end, we introduce the\nWildestFaces dataset, tailored to study cross-domain recognition under a\nvariety of adverse conditions. We divide the task of transferring a recognition\nmodel from the domain of clean images to the violent videos into two\nsub-problems and tackle them using (i) stacked affine-transforms for\nclassifier-transfer, (ii) attention-driven pooling for temporal-adaptation. We\nadditionally formulate a self-attention based model for domain-transfer. We\nestablish a rigorous evaluation protocol for this clean-to-violent recognition\ntask, and present a detailed analysis of the proposed dataset and the methods.\nOur experiments highlight the unique challenges introduced by the WildestFaces\ndataset and the advantages of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 09:45:33 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Bilge", "Yunus Can", ""], ["Yucel", "Mehmet Kerim", ""], ["Cinbis", "Ramazan Gokberk", ""], ["Ikizler-Cinbis", "Nazli", ""], ["Duygulu", "Pinar", ""]]}, {"id": "2009.07604", "submitter": "Bianjiang Yang", "authors": "Bianjiang Yang, Zi Hui, Haoji Hu, Xinyi Hu, Lu Yu", "title": "Compressing Facial Makeup Transfer Networks by Collaborative\n  Distillation and Kernel Decomposition", "comments": "This paper will be published on 2020 IEEE International Conference on\n  Visual Communications and Image Processing (VCIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the facial makeup transfer network has achieved high-quality\nperformance in generating perceptually pleasing makeup images, its capability\nis still restricted by the massive computation and storage of the network\narchitecture. We address this issue by compressing facial makeup transfer\nnetworks with collaborative distillation and kernel decomposition. The main\nidea of collaborative distillation is underpinned by a finding that the\nencoder-decoder pairs construct an exclusive collaborative relationship, which\nis regarded as a new kind of knowledge for low-level vision tasks. For kernel\ndecomposition, we apply the depth-wise separation of convolutional kernels to\nbuild a light-weighted Convolutional Neural Network (CNN) from the original\nnetwork. Extensive experiments show the effectiveness of the compression method\nwhen applied to the state-of-the-art facial makeup transfer network --\nBeautyGAN.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 11:07:04 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Yang", "Bianjiang", ""], ["Hui", "Zi", ""], ["Hu", "Haoji", ""], ["Hu", "Xinyi", ""], ["Yu", "Lu", ""]]}, {"id": "2009.07608", "submitter": "Andreas Selmar Hauptmann", "authors": "Andreas Hauptmann and Ben Cox", "title": "Deep Learning in Photoacoustic Tomography: Current approaches and future\n  directions", "comments": null, "journal-ref": "J. of Biomedical Optics, 25(11), 112903 (2020)", "doi": "10.1117/1.JBO.25.11.112903", "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical photoacoustic tomography, which can provide high resolution 3D\nsoft tissue images based on the optical absorption, has advanced to the stage\nat which translation from the laboratory to clinical settings is becoming\npossible. The need for rapid image formation and the practical restrictions on\ndata acquisition that arise from the constraints of a clinical workflow are\npresenting new image reconstruction challenges. There are many classical\napproaches to image reconstruction, but ameliorating the effects of incomplete\nor imperfect data through the incorporation of accurate priors is challenging\nand leads to slow algorithms. Recently, the application of Deep Learning, or\ndeep neural networks, to this problem has received a great deal of attention.\nThis paper reviews the literature on learned image reconstruction, summarising\nthe current trends, and explains how these new approaches fit within, and to\nsome extent have arisen from, a framework that encompasses classical\nreconstruction methods. In particular, it shows how these new techniques can be\nunderstood from a Bayesian perspective, providing useful insights. The paper\nalso provides a concise tutorial demonstration of three prototypical approaches\nto learned image reconstruction. The code and data sets for these\ndemonstrations are available to researchers. It is anticipated that it is in in\nvivo applications - where data may be sparse, fast imaging critical and priors\ndifficult to construct by hand - that Deep Learning will have the most impact.\nWith this in mind, the paper concludes with some indications of possible future\nresearch directions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 11:33:29 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Hauptmann", "Andreas", ""], ["Cox", "Ben", ""]]}, {"id": "2009.07611", "submitter": "George Adaimi", "authors": "George Adaimi, Sven Kreiss, Alexandre Alahi", "title": "Perceiving Traffic from Aerial Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drones or UAVs, equipped with different sensors, have been deployed in many\nplaces especially for urban traffic monitoring or last-mile delivery. It\nprovides the ability to control the different aspects of traffic given\nreal-time obeservations, an important pillar for the future of transportation\nand smart cities. With the increasing use of such machines, many previous\nstate-of-the-art object detectors, who have achieved high performance on front\nfacing cameras, are being used on UAV datasets. When applied to high-resolution\naerial images captured from such datasets, they fail to generalize to the wide\nrange of objects' scales. In order to address this limitation, we propose an\nobject detection method called Butterfly Detector that is tailored to detect\nobjects in aerial images. We extend the concept of fields and introduce\nbutterfly fields, a type of composite field that describes the spatial\ninformation of output features as well as the scale of the detected object. To\novercome occlusion and viewing angle variations that can hinder the\nlocalization process, we employ a voting mechanism between related butterfly\nvectors pointing to the object center. We evaluate our Butterfly Detector on\ntwo publicly available UAV datasets (UAVDT and VisDrone2019) and show that it\noutperforms previous state-of-the-art methods while remaining real-time.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 11:37:43 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Adaimi", "George", ""], ["Kreiss", "Sven", ""], ["Alahi", "Alexandre", ""]]}, {"id": "2009.07627", "submitter": "Sumeet Badgujar", "authors": "Amit Chavda, Jason Dsouza, Sumeet Badgujar, Ankit Damani", "title": "Multi-Stage CNN Architecture for Face Mask Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The end of 2019 witnessed the outbreak of Coronavirus Disease 2019\n(COVID-19), which has continued to be the cause of plight for millions of lives\nand businesses even in 2020. As the world recovers from the pandemic and plans\nto return to a state of normalcy, there is a wave of anxiety among all\nindividuals, especially those who intend to resume in-person activity. Studies\nhave proved that wearing a face mask significantly reduces the risk of viral\ntransmission as well as provides a sense of protection. However, it is not\nfeasible to manually track the implementation of this policy. Technology holds\nthe key here. We introduce a Deep Learning based system that can detect\ninstances where face masks are not used properly. Our system consists of a\ndual-stage Convolutional Neural Network (CNN) architecture capable of detecting\nmasked and unmasked faces and can be integrated with pre-installed CCTV\ncameras. This will help track safety violations, promote the use of face masks,\nand ensure a safe working environment.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 12:23:21 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 09:08:18 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Chavda", "Amit", ""], ["Dsouza", "Jason", ""], ["Badgujar", "Sumeet", ""], ["Damani", "Ankit", ""]]}, {"id": "2009.07635", "submitter": "Nikhil Churamani", "authors": "Pablo Barros, Nikhil Churamani and Alessandra Sciutti", "title": "The FaceChannel: A Fast & Furious Deep Neural Network for Facial\n  Expression Recognition", "comments": "Accepted for publication at SN Computer Science. arXiv admin note:\n  substantial text overlap with arXiv:2004.08195", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art models for automatic Facial Expression Recognition\n(FER) are based on very deep neural networks that are effective but rather\nexpensive to train. Given the dynamic conditions of FER, this characteristic\nhinders such models of been used as a general affect recognition. In this\npaper, we address this problem by formalizing the FaceChannel, a light-weight\nneural network that has much fewer parameters than common deep neural networks.\nWe introduce an inhibitory layer that helps to shape the learning of facial\nfeatures in the last layer of the network and thus improving performance while\nreducing the number of trainable parameters. To evaluate our model, we perform\na series of experiments on different benchmark datasets and demonstrate how the\nFaceChannel achieves a comparable, if not better, performance to the current\nstate-of-the-art in FER. Our experiments include cross-dataset analysis, to\nestimate how our model behaves on different affective recognition conditions.\nWe conclude our paper with an analysis of how FaceChannel learns and adapt the\nlearned facial features towards the different datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 09:25:37 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Barros", "Pablo", ""], ["Churamani", "Nikhil", ""], ["Sciutti", "Alessandra", ""]]}, {"id": "2009.07637", "submitter": "Zijie Ye", "authors": "Zijie Ye, Haozhe Wu, Jia Jia, Yaohua Bu, Wei Chen, Fanbo Meng, Yanfeng\n  Wang", "title": "ChoreoNet: Towards Music to Dance Synthesis with Choreographic Action\n  Unit", "comments": "10 pages, 5 figures, Accepted by ACM MM 2020", "journal-ref": null, "doi": "10.1145/3394171.3414005", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dance and music are two highly correlated artistic forms. Synthesizing dance\nmotions has attracted much attention recently. Most previous works conduct\nmusic-to-dance synthesis via directly music to human skeleton keypoints\nmapping. Meanwhile, human choreographers design dance motions from music in a\ntwo-stage manner: they firstly devise multiple choreographic dance units\n(CAUs), each with a series of dance motions, and then arrange the CAU sequence\naccording to the rhythm, melody and emotion of the music. Inspired by these, we\nsystematically study such two-stage choreography approach and construct a\ndataset to incorporate such choreography knowledge. Based on the constructed\ndataset, we design a two-stage music-to-dance synthesis framework ChoreoNet to\nimitate human choreography procedure. Our framework firstly devises a CAU\nprediction model to learn the mapping relationship between music and CAU\nsequences. Afterwards, we devise a spatial-temporal inpainting model to convert\nthe CAU sequence into continuous dance motions. Experimental results\ndemonstrate that the proposed ChoreoNet outperforms baseline methods (0.622 in\nterms of CAU BLEU score and 1.59 in terms of user study score).\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 12:38:19 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Ye", "Zijie", ""], ["Wu", "Haozhe", ""], ["Jia", "Jia", ""], ["Bu", "Yaohua", ""], ["Chen", "Wei", ""], ["Meng", "Fanbo", ""], ["Wang", "Yanfeng", ""]]}, {"id": "2009.07641", "submitter": "Haisheng Su", "authors": "Haisheng Su, Weihao Gan, Wei Wu, Yu Qiao, Junjie Yan", "title": "BSN++: Complementary Boundary Regressor with Scale-Balanced Relation\n  Modeling for Temporal Action Proposal Generation", "comments": "Accepted by AAAI 2021. Ranked 1st place in the CVPR19 - ActivityNet\n  Challenge leaderboard on Temporal Action Localization task. arXiv admin note:\n  substantial text overlap with arXiv:2007.09883", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating human action proposals in untrimmed videos is an important yet\nchallenging task with wide applications. Current methods often suffer from the\nnoisy boundary locations and the inferior quality of confidence scores used for\nproposal retrieving. In this paper, we present BSN++, a new framework which\nexploits complementary boundary regressor and relation modeling for temporal\nproposal generation. First, we propose a novel boundary regressor based on the\ncomplementary characteristics of both starting and ending boundary classifiers.\nSpecifically, we utilize the U-shaped architecture with nested skip connections\nto capture rich contexts and introduce bi-directional boundary matching\nmechanism to improve boundary precision. Second, to account for the\nproposal-proposal relations ignored in previous methods, we devise a proposal\nrelation block to which includes two self-attention modules from the aspects of\nposition and channel. Furthermore, we find that there inevitably exists data\nimbalanced problems in the positive/negative proposals and temporal durations,\nwhich harm the model performance on tail distributions. To relieve this issue,\nwe introduce the scale-balanced re-sampling strategy. Extensive experiments are\nconducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, which\ndemonstrate that BSN++ achieves the state-of-the-art performance. Not\nsurprisingly, the proposed BSN++ ranked 1st place in the CVPR19 - ActivityNet\nchallenge leaderboard on temporal action localization task.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 07:08:59 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 06:25:39 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 03:50:47 GMT"}, {"version": "v4", "created": "Tue, 5 Jan 2021 03:35:03 GMT"}, {"version": "v5", "created": "Mon, 1 Mar 2021 08:01:49 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Su", "Haisheng", ""], ["Gan", "Weihao", ""], ["Wu", "Wei", ""], ["Qiao", "Yu", ""], ["Yan", "Junjie", ""]]}, {"id": "2009.07646", "submitter": "Estefania Talavera", "authors": "Estefania Talavera, Andreea Glavan, Alina Matei, Petia Radeva", "title": "Eating Habits Discovery in Egocentric Photo-streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eating habits are learned throughout the early stages of our lives. However,\nit is not easy to be aware of how our food-related routine affects our healthy\nliving. In this work, we address the unsupervised discovery of nutritional\nhabits from egocentric photo-streams. We build a food-related behavioural\npattern discovery model, which discloses nutritional routines from the\nactivities performed throughout the days. To do so, we rely on\nDynamic-Time-Warping for the evaluation of similarity among the collected days.\nWithin this framework, we present a simple, but robust and fast novel\nclassification pipeline that outperforms the state-of-the-art on food-related\nimage classification with a weighted accuracy and F-score of 70% and 63%,\nrespectively. Later, we identify days composed of nutritional activities that\ndo not describe the habits of the person as anomalies in the daily life of the\nuser with the Isolation Forest method. Furthermore, we show an application for\nthe identification of food-related scenes when the camera wearer eats in\nisolation. Results have shown the good performance of the proposed model and\nits relevance to visualize the nutritional habits of individuals.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 12:46:35 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Talavera", "Estefania", ""], ["Glavan", "Andreea", ""], ["Matei", "Alina", ""], ["Radeva", "Petia", ""]]}, {"id": "2009.07652", "submitter": "Zhao Wang", "authors": "Zhao Wang, Quande Liu, and Qi Dou", "title": "Contrastive Cross-site Learning with Redesigned Net for COVID-19 CT\n  Classification", "comments": "Published as a journal paper at IEEE J-BHI; code and dataset are\n  available at https://github.com/med-air/Contrastive-COVIDNet", "journal-ref": null, "doi": "10.1109/JBHI.2020.3023246", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pandemic of coronavirus disease 2019 (COVID-19) has lead to a global\npublic health crisis spreading hundreds of countries. With the continuous\ngrowth of new infections, developing automated tools for COVID-19\nidentification with CT image is highly desired to assist the clinical diagnosis\nand reduce the tedious workload of image interpretation. To enlarge the\ndatasets for developing machine learning methods, it is essentially helpful to\naggregate the cases from different medical systems for learning robust and\ngeneralizable models. This paper proposes a novel joint learning framework to\nperform accurate COVID-19 identification by effectively learning with\nheterogeneous datasets with distribution discrepancy. We build a powerful\nbackbone by redesigning the recently proposed COVID-Net in aspects of network\narchitecture and learning strategy to improve the prediction accuracy and\nlearning efficiency. On top of our improved backbone, we further explicitly\ntackle the cross-site domain shift by conducting separate feature normalization\nin latent space. Moreover, we propose to use a contrastive training objective\nto enhance the domain invariance of semantic embeddings for boosting the\nclassification performance on each dataset. We develop and evaluate our method\nwith two public large-scale COVID-19 diagnosis datasets made up of CT images.\nExtensive experiments show that our approach consistently improves the\nperformances on both datasets, outperforming the original COVID-Net trained on\neach dataset by 12.16% and 14.23% in AUC respectively, also exceeding existing\nstate-of-the-art multi-site learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 11:09:04 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Wang", "Zhao", ""], ["Liu", "Quande", ""], ["Dou", "Qi", ""]]}, {"id": "2009.07683", "submitter": "Patrick Ebel", "authors": "Patrick Ebel, Andrea Meraner, Michael Schmitt, Xiaoxiang Zhu", "title": "Multi-Sensor Data Fusion for Cloud Removal in Global and All-Season\n  Sentinel-2 Imagery", "comments": "This work has been accepted by IEEE TGRS for publication", "journal-ref": null, "doi": "10.1109/TGRS.2020.3024744", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work has been accepted by IEEE TGRS for publication. The majority of\noptical observations acquired via spaceborne earth imagery are affected by\nclouds. While there is numerous prior work on reconstructing cloud-covered\ninformation, previous studies are oftentimes confined to narrowly-defined\nregions of interest, raising the question of whether an approach can generalize\nto a diverse set of observations acquired at variable cloud coverage or in\ndifferent regions and seasons. We target the challenge of generalization by\ncurating a large novel data set for training new cloud removal approaches and\nevaluate on two recently proposed performance metrics of image quality and\ndiversity. Our data set is the first publically available to contain a global\nsample of co-registered radar and optical observations, cloudy as well as\ncloud-free. Based on the observation that cloud coverage varies widely between\nclear skies and absolute coverage, we propose a novel model that can deal with\neither extremes and evaluate its performance on our proposed data set. Finally,\nwe demonstrate the superiority of training models on real over synthetic data,\nunderlining the need for a carefully curated data set of real observations. To\nfacilitate future research, our data set is made available online\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 13:40:42 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Ebel", "Patrick", ""], ["Meraner", "Andrea", ""], ["Schmitt", "Michael", ""], ["Zhu", "Xiaoxiang", ""]]}, {"id": "2009.07698", "submitter": "Reuben Tan", "authors": "Reuben Tan, Bryan A. Plummer, Kate Saenko", "title": "Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News", "comments": "Accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale dissemination of disinformation online intended to mislead or\ndeceive the general population is a major societal problem. Rapid progression\nin image, video, and natural language generative models has only exacerbated\nthis situation and intensified our need for an effective defense mechanism.\nWhile existing approaches have been proposed to defend against neural fake\nnews, they are generally constrained to the very limited setting where articles\nonly have text and metadata such as the title and authors. In this paper, we\nintroduce the more realistic and challenging task of defending against\nmachine-generated news that also includes images and captions. To identify the\npossible weaknesses that adversaries can exploit, we create a NeuralNews\ndataset composed of 4 different types of generated articles as well as conduct\na series of human user study experiments based on this dataset. In addition to\nthe valuable insights gleaned from our user study experiments, we provide a\nrelatively effective approach based on detecting visual-semantic\ninconsistencies, which will serve as an effective first line of defense and a\nuseful reference for future work in defending against machine-generated\ndisinformation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 14:13:15 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 01:17:19 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 21:37:02 GMT"}, {"version": "v4", "created": "Thu, 24 Sep 2020 21:10:15 GMT"}, {"version": "v5", "created": "Wed, 21 Oct 2020 15:16:20 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Tan", "Reuben", ""], ["Plummer", "Bryan A.", ""], ["Saenko", "Kate", ""]]}, {"id": "2009.07714", "submitter": "Robert McCraith", "authors": "Robert McCraith, Lukas Neumann, Andrea Vedaldi", "title": "Calibrating Self-supervised Monocular Depth Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, many methods demonstrated the ability of neural networks\ntolearn depth and pose changes in a sequence of images, using only\nself-supervision as thetraining signal. Whilst the networks achieve good\nperformance, the often over-lookeddetail is that due to the inherent ambiguity\nof monocular vision they predict depth up to aunknown scaling factor. The\nscaling factor is then typically obtained from the LiDARground truth at test\ntime, which severely limits practical applications of these methods.In this\npaper, we show that incorporating prior information about the camera\nconfigu-ration and the environment, we can remove the scale ambiguity and\npredict depth directly,still using the self-supervised formulation and not\nrelying on any additional sensors.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 14:35:45 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["McCraith", "Robert", ""], ["Neumann", "Lukas", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "2009.07717", "submitter": "Sara Ahmed", "authors": "Sara Atito Ali Ahmed, Berrin Yanikoglu", "title": "Relative Attribute Classification with Deep Rank SVM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relative attributes indicate the strength of a particular attribute between\nimage pairs. We introduce a deep Siamese network with rank SVM loss function,\ncalled Deep Rank SVM (DRSVM), in order to decide which one of a pair of images\nhas a stronger presence of a specific attribute. The network is trained in an\nend-to-end fashion to jointly learn the visual features and the ranking\nfunction. We demonstrate the effectiveness of our approach against the\nstate-of-the-art methods on four image benchmark datasets: LFW-10, PubFig,\nUTZap50K-lexi and UTZap50K-2 datasets. DRSVM surpasses state-of-art in terms of\nthe average accuracy across attributes, on three of the four image benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 09:21:39 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Ahmed", "Sara Atito Ali", ""], ["Yanikoglu", "Berrin", ""]]}, {"id": "2009.07719", "submitter": "Hanjiang Hu", "authors": "Hanjiang Hu, Hesheng Wang, Zhe Liu and Weidong Chen", "title": "Domain-invariant Similarity Activation Map Metric Learning for\n  Retrieval-based Long-term Visual Localization", "comments": "Submitted to IEEE/CAA Journal of Automatica Sinica", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual localization is a crucial component in the application of mobile robot\nand autonomous driving. Image retrieval is an efficient and effective technique\nin image-based localization methods. Due to the drastic variability of\nenvironmental conditions, e.g. illumination, seasonal and weather changes,\nretrieval-based visual localization is severely affected and becomes a\nchallenging problem. In this work, a general architecture is first formulated\nprobabilistically to extract domain-invariant feature through multi-domain\nimage translation. And then a novel gradient-weighted similarity activation\nmapping loss (Grad-SAM) is incorporated for finer localization with high\naccuracy. We also propose a new adaptive triplet loss to boost the metric\nlearning of the embedding in a self-supervised manner. The final coarse-to-fine\nimage retrieval pipeline is implemented as the sequential combination of models\nwithout and with Grad-SAM loss. Extensive experiments have been conducted to\nvalidate the effectiveness of the proposed approach on the CMU-Seasons dataset.\nThe strong generalization ability of our approach is verified on RobotCar\ndataset using models pre-trained on urban part of CMU-Seasons dataset. Our\nperformance is on par with or even outperforms the state-of-the-art image-based\nlocalization baselines in medium or high precision, especially under the\nchallenging environments with illumination variance, vegetation and night-time\nimages. Moreover, real-site experiments have been conducted to validate the\nefficiency and effectiveness of the coarse-to-fine strategy for localization.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 14:43:22 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 15:47:53 GMT"}, {"version": "v3", "created": "Fri, 27 Nov 2020 11:16:10 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Hu", "Hanjiang", ""], ["Wang", "Hesheng", ""], ["Liu", "Zhe", ""], ["Chen", "Weidong", ""]]}, {"id": "2009.07724", "submitter": "Colorado J Reed", "authors": "Colorado J Reed, Sean Metzger, Aravind Srinivas, Trevor Darrell, Kurt\n  Keutzer", "title": "SelfAugment: Automatic Augmentation Policies for Self-Supervised\n  Learning", "comments": "Computer Vision and Pattern Recognition (CVPR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A common practice in unsupervised representation learning is to use labeled\ndata to evaluate the quality of the learned representations. This supervised\nevaluation is then used to guide critical aspects of the training process such\nas selecting the data augmentation policy. However, guiding an unsupervised\ntraining process through supervised evaluations is not possible for real-world\ndata that does not actually contain labels (which may be the case, for example,\nin privacy sensitive fields such as medical imaging). Therefore, in this work\nwe show that evaluating the learned representations with a self-supervised\nimage rotation task is highly correlated with a standard set of supervised\nevaluations (rank correlation $> 0.94$). We establish this correlation across\nhundreds of augmentation policies, training settings, and network architectures\nand provide an algorithm (SelfAugment) to automatically and efficiently select\naugmentation policies without using supervised evaluations. Despite not using\nany labeled data, the learned augmentation policies perform comparably with\naugmentation policies that were determined using exhaustive supervised\nevaluations.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 14:49:03 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 17:53:51 GMT"}, {"version": "v3", "created": "Mon, 17 May 2021 15:11:06 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Reed", "Colorado J", ""], ["Metzger", "Sean", ""], ["Srinivas", "Aravind", ""], ["Darrell", "Trevor", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2009.07734", "submitter": "Ruisi Zhang", "authors": "Ruisi Zhang and Luntian Mou and Pengtao Xie", "title": "TreeGAN: Incorporating Class Hierarchy into Image Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional image generation (CIG) is a widely studied problem in computer\nvision and machine learning. Given a class, CIG takes the name of this class as\ninput and generates a set of images that belong to this class. In existing CIG\nworks, for different classes, their corresponding images are generated\nindependently, without considering the relationship among classes. In\nreal-world applications, the classes are organized into a hierarchy and their\nhierarchical relationships are informative for generating high-fidelity images.\nIn this paper, we aim to leverage the class hierarchy for conditional image\ngeneration. We propose two ways of incorporating class hierarchy: prior control\nand post constraint. In prior control, we first encode the class hierarchy,\nthen feed it as a prior into the conditional generator to generate images. In\npost constraint, after the images are generated, we measure their consistency\nwith the class hierarchy and use the consistency score to guide the training of\nthe generator. Based on these two ideas, we propose a TreeGAN model which\nconsists of three modules: (1) a class hierarchy encoder (CHE) which takes the\nhierarchical structure of classes and their textual names as inputs and learns\nan embedding for each class; the embedding captures the hierarchical\nrelationship among classes; (2) a conditional image generator (CIG) which takes\nthe CHE-generated embedding of a class as input and generates a set of images\nbelonging to this class; (3) a consistency checker which performs hierarchical\nclassification on the generated images and checks whether the generated images\nare compatible with the class hierarchy; the consistency score is used to guide\nthe CIG to generate hierarchy-compatible images. Experiments on various\ndatasets demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 15:06:52 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Zhang", "Ruisi", ""], ["Mou", "Luntian", ""], ["Xie", "Pengtao", ""]]}, {"id": "2009.07736", "submitter": "Jonathon Luiten", "authors": "Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip Torr, Andreas\n  Geiger, Laura Leal-Taixe, Bastian Leibe", "title": "HOTA: A Higher Order Metric for Evaluating Multi-Object Tracking", "comments": "Pre-print. Accepted for Publication in the International Journal of\n  Computer Vision, 19 August 2020. Code is available at\n  https://github.com/JonathonLuiten/HOTA-metrics", "journal-ref": "International Journal of Computer Vision (2020)", "doi": "10.1007/s11263-020-01375-2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Object Tracking (MOT) has been notoriously difficult to evaluate.\nPrevious metrics overemphasize the importance of either detection or\nassociation. To address this, we present a novel MOT evaluation metric, HOTA\n(Higher Order Tracking Accuracy), which explicitly balances the effect of\nperforming accurate detection, association and localization into a single\nunified metric for comparing trackers. HOTA decomposes into a family of\nsub-metrics which are able to evaluate each of five basic error types\nseparately, which enables clear analysis of tracking performance. We evaluate\nthe effectiveness of HOTA on the MOTChallenge benchmark, and show that it is\nable to capture important aspects of MOT performance not previously taken into\naccount by established metrics. Furthermore, we show HOTA scores better align\nwith human visual evaluation of tracking performance.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 15:11:30 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 10:40:09 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Luiten", "Jonathon", ""], ["Osep", "Aljosa", ""], ["Dendorfer", "Patrick", ""], ["Torr", "Philip", ""], ["Geiger", "Andreas", ""], ["Leal-Taixe", "Laura", ""], ["Leibe", "Bastian", ""]]}, {"id": "2009.07823", "submitter": "Prune Truong", "authors": "Prune Truong, Martin Danelljan, Luc Van Gool, Radu Timofte", "title": "GOCor: Bringing Globally Optimized Correspondence Volumes into Your\n  Neural Network", "comments": "code: https://github.com/PruneTruong/GOCor. Website:\n  https://prunetruong.com/research/gocor. Video:\n  https://www.youtube.com/watch?v=V22MyFChBCs. NeurIPS 2020", "journal-ref": "Annual Conference on Neural Information Processing Systems,\n  NeurIPS, 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The feature correlation layer serves as a key neural network module in\nnumerous computer vision problems that involve dense correspondences between\nimage pairs. It predicts a correspondence volume by evaluating dense scalar\nproducts between feature vectors extracted from pairs of locations in two\nimages. However, this point-to-point feature comparison is insufficient when\ndisambiguating multiple similar regions in an image, severely affecting the\nperformance of the end task. We propose GOCor, a fully differentiable dense\nmatching module, acting as a direct replacement to the feature correlation\nlayer. The correspondence volume generated by our module is the result of an\ninternal optimization procedure that explicitly accounts for similar regions in\nthe scene. Moreover, our approach is capable of effectively learning spatial\nmatching priors to resolve further matching ambiguities. We analyze our GOCor\nmodule in extensive ablative experiments. When integrated into state-of-the-art\nnetworks, our approach significantly outperforms the feature correlation layer\nfor the tasks of geometric matching, optical flow, and dense semantic matching.\nThe code and trained models will be made available at\ngithub.com/PruneTruong/GOCor.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 17:33:01 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 14:08:10 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 15:11:00 GMT"}, {"version": "v4", "created": "Mon, 5 Apr 2021 14:00:51 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Truong", "Prune", ""], ["Danelljan", "Martin", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2009.07827", "submitter": "Kaili Wang", "authors": "Kaili Wang, Jose Oramas, Tinne Tuytelaars", "title": "Multiple Exemplars-based Hallucinationfor Face Super-resolution and\n  Editing", "comments": "accepted in ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a really low-resolution input image of a face (say 16x16 or 8x8\npixels), the goal of this paper is to reconstruct a high-resolution version\nthereof. This, by itself, is an ill-posed problem, as the high-frequency\ninformation is missing in the low-resolution input and needs to be\nhallucinated, based on prior knowledge about the image content. Rather than\nrelying on a generic face prior, in this paper, we explore the use of a set of\nexemplars, i.e. other high-resolution images of the same person. These guide\nthe neural network as we condition the output on them. Multiple exemplars work\nbetter than a single one. To combine the information from multiple exemplars\neffectively, we introduce a pixel-wise weight generation module. Besides\nstandard face super-resolution, our method allows to perform subtle face\nediting simply by replacing the exemplars with another set with different\nfacial features. A user study is conducted and shows the super-resolved images\ncan hardly be distinguished from real images on the CelebA dataset. A\nqualitative comparison indicates our model outperforms methods proposed in the\nliterature on the CelebA and WebFace dataset.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 17:35:26 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 07:25:22 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2021 09:55:02 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wang", "Kaili", ""], ["Oramas", "Jose", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "2009.07833", "submitter": "Erika Lu", "authors": "Erika Lu, Forrester Cole, Tali Dekel, Weidi Xie, Andrew Zisserman,\n  David Salesin, William T. Freeman, Michael Rubinstein", "title": "Layered Neural Rendering for Retiming People in Video", "comments": "To appear in SIGGRAPH Asia 2020. Project webpage:\n  https://retiming.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for retiming people in an ordinary, natural\nvideo---manipulating and editing the time in which different motions of\nindividuals in the video occur. We can temporally align different motions,\nchange the speed of certain actions (speeding up/slowing down, or entirely\n\"freezing\" people), or \"erase\" selected people from the video altogether. We\nachieve these effects computationally via a dedicated learning-based layered\nvideo representation, where each frame in the video is decomposed into separate\nRGBA layers, representing the appearance of different people in the video. A\nkey property of our model is that it not only disentangles the direct motions\nof each person in the input video, but also correlates each person\nautomatically with the scene changes they generate---e.g., shadows,\nreflections, and motion of loose clothing. The layers can be individually\nretimed and recombined into a new video, allowing us to achieve realistic,\nhigh-quality renderings of retiming effects for real-world videos depicting\ncomplex actions and involving multiple individuals, including dancing,\ntrampoline jumping, or group running.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 17:48:26 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Lu", "Erika", ""], ["Cole", "Forrester", ""], ["Dekel", "Tali", ""], ["Xie", "Weidi", ""], ["Zisserman", "Andrew", ""], ["Salesin", "David", ""], ["Freeman", "William T.", ""], ["Rubinstein", "Michael", ""]]}, {"id": "2009.07838", "submitter": "Julio Cezar Silveira Jacques Junior", "authors": "Tom\\'a\\v{s} Sixta, Julio C. S. Jacques Junior, Pau Buch-Cardona, Neil\n  M. Robertson, Eduard Vazquez, Sergio Escalera", "title": "FairFace Challenge at ECCV 2020: Analyzing Bias in Face Recognition", "comments": "accepted on ECCV'2020 Fair Face Recognition and Analysis Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work summarizes the 2020 ChaLearn Looking at People Fair Face\nRecognition and Analysis Challenge and provides a description of the\ntop-winning solutions and analysis of the results. The aim of the challenge was\nto evaluate accuracy and bias in gender and skin colour of submitted algorithms\non the task of 1:1 face verification in the presence of other confounding\nattributes. Participants were evaluated using an in-the-wild dataset based on\nreannotated IJB-C, further enriched by 12.5K new images and additional labels.\nThe dataset is not balanced, which simulates a real world scenario where\nAI-based models supposed to present fair outcomes are trained and evaluated on\nimbalanced data. The challenge attracted 151 participants, who made more than\n1.8K submissions in total. The final phase of the challenge attracted 36 active\nteams out of which 10 exceeded 0.999 AUC-ROC while achieving very low scores in\nthe proposed bias metrics. Common strategies by the participants were face\npre-processing, homogenization of data distributions, the use of bias aware\nloss functions and ensemble models. The analysis of top-10 teams shows higher\nfalse positive rates (and lower false negative rates) for females with dark\nskin tone as well as the potential of eyeglasses and young age to increase the\nfalse positive rates too.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 17:56:22 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 17:17:48 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Sixta", "Tom\u00e1\u0161", ""], ["Junior", "Julio C. S. Jacques", ""], ["Buch-Cardona", "Pau", ""], ["Robertson", "Neil M.", ""], ["Vazquez", "Eduard", ""], ["Escalera", "Sergio", ""]]}, {"id": "2009.07879", "submitter": "Qiong Liu", "authors": "Qiong Liu, Yanxia Zhang", "title": "Using Sensory Time-cue to enable Unsupervised Multimodal Meta-learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data from IoT (Internet of Things) sensors become ubiquitous,\nstate-of-the-art machine learning algorithms face many challenges on directly\nusing sensor data. To overcome these challenges, methods must be designed to\nlearn directly from sensors without manual annotations. This paper introduces\nSensory Time-cue for Unsupervised Meta-learning (STUM). Different from\ntraditional learning approaches that either heavily depend on labels or on\ntime-independent feature extraction assumptions, such as Gaussian distribution\nfeatures, the STUM system uses time relation of inputs to guide the feature\nspace formation within and across modalities. The fact that STUM learns from a\nvariety of small tasks may put this method in the camp of Meta-Learning.\nDifferent from existing Meta-Learning approaches, STUM learning tasks are\ncomposed within and across multiple modalities based on time-cue co-exist with\nthe IoT streaming data. In an audiovisual learning example, because consecutive\nvisual frames usually comprise the same object, this approach provides a unique\nway to organize features from the same object together. The same method can\nalso organize visual object features with the object's spoken-name features\ntogether if the spoken name is presented with the object at about the same\ntime. This cross-modality feature organization may further help the\norganization of visual features that belong to similar objects but acquired at\ndifferent location and time. Promising results are achieved through\nevaluations.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 18:18:49 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Liu", "Qiong", ""], ["Zhang", "Yanxia", ""]]}, {"id": "2009.07970", "submitter": "Bardia Yousefi", "authors": "Hossein Memarzadeh Sharifipour, Bardia Yousefi, Xavier P.V. Maldague", "title": "Skeletonization and Reconstruction based on Graph Morphological\n  Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiscale shape skeletonization on pixel adjacency graphs is an advanced\nintriguing research subject in the field of image processing, computer vision\nand data mining. The previous works in this area almost focused on the graph\nvertices. We proposed novel structured based graph morphological\ntransformations based on edges opposite to the current node based\ntransformations and used them for deploying skeletonization and reconstruction\nof infrared thermal images represented by graphs. The advantage of this method\nis that many widely used path based approaches become available within this\ndefinition of morphological operations. For instance, we use distance maps and\nimage foresting transform (IFT) as two main path based methods are utilized for\ncomputing the skeleton of an image. Moreover, In addition, the open question\nproposed by Maragos et al (2013) about connectivity of graph skeletonization\nmethod are discussed and shown to be quite difficult to decide in general case.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 22:58:06 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Sharifipour", "Hossein Memarzadeh", ""], ["Yousefi", "Bardia", ""], ["Maldague", "Xavier P. V.", ""]]}, {"id": "2009.07974", "submitter": "Shuyue Guan", "authors": "Shuyue Guan, Murray Loew", "title": "Analysis of Generalizability of Deep Neural Networks Based on the\n  Complexity of Decision Boundary", "comments": "7 pages, 11 figures. Accepted by ICMLA 2020", "journal-ref": "19th IEEE International Conference on Machine Learning and\n  Applications (ICMLA), 2020, pp. 101-106", "doi": "10.1109/ICMLA51294.2020.00025", "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For supervised learning models, the analysis of generalization ability\n(generalizability) is vital because the generalizability expresses how well a\nmodel will perform on unseen data. Traditional generalization methods, such as\nthe VC dimension, do not apply to deep neural network (DNN) models. Thus, new\ntheories to explain the generalizability of DNNs are required. In this study,\nwe hypothesize that the DNN with a simpler decision boundary has better\ngeneralizability by the law of parsimony (Occam's Razor). We create the\ndecision boundary complexity (DBC) score to define and measure the complexity\nof decision boundary of DNNs. The idea of the DBC score is to generate data\npoints (called adversarial examples) on or near the decision boundary. Our new\napproach then measures the complexity of the boundary using the entropy of\neigenvalues of these data. The method works equally well for high-dimensional\ndata. We use training data and the trained model to compute the DBC score. And,\nthe ground truth for model's generalizability is its test accuracy. Experiments\nbased on the DBC score have verified our hypothesis. The DBC is shown to\nprovide an effective method to measure the complexity of a decision boundary\nand gives a quantitative measure of the generalizability of DNNs.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 23:25:52 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Guan", "Shuyue", ""], ["Loew", "Murray", ""]]}, {"id": "2009.07975", "submitter": "Param Hanji", "authors": "Param Hanji, Fangcheng Zhong, Rafal K. Mantiuk", "title": "Noise-Aware Merging of High Dynamic Range Image Stacks without Camera\n  Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A near-optimal reconstruction of the radiance of a High Dynamic Range scene\nfrom an exposure stack can be obtained by modeling the camera noise\ndistribution. The latent radiance is then estimated using Maximum Likelihood\nEstimation. But this requires a well-calibrated noise model of the camera,\nwhich is difficult to obtain in practice. We show that an unbiased estimation\nof comparable variance can be obtained with a simpler Poisson noise estimator,\nwhich does not require the knowledge of camera-specific noise parameters. We\ndemonstrate this empirically for four different cameras, ranging from a\nsmartphone camera to a full-frame mirrorless camera. Our experimental results\nare consistent for simulated as well as real images, and across different\ncamera settings.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 23:26:17 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Hanji", "Param", ""], ["Zhong", "Fangcheng", ""], ["Mantiuk", "Rafal K.", ""]]}, {"id": "2009.07988", "submitter": "Xiang Deng", "authors": "Xiang Deng and Zhongfei (Mark) Zhang", "title": "Deep Collective Learning: Learning Optimal Inputs and Weights Jointly in\n  Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well observed that in deep learning and computer vision literature,\nvisual data are always represented in a manually designed coding scheme (eg.,\nRGB images are represented as integers ranging from 0 to 255 for each channel)\nwhen they are input to an end-to-end deep neural network (DNN) for any learning\ntask. We boldly question whether the manually designed inputs are good for DNN\ntraining for different tasks and study whether the input to a DNN can be\noptimally learned end-to-end together with learning the weights of the DNN. In\nthis paper, we propose the paradigm of {\\em deep collective learning} which\naims to learn the weights of DNNs and the inputs to DNNs simultaneously for\ngiven tasks. We note that collective learning has been implicitly but widely\nused in natural language processing while it has almost never been studied in\ncomputer vision. Consequently, we propose the lookup vision networks\n(Lookup-VNets) as a solution to deep collective learning in computer vision.\nThis is achieved by associating each color in each channel with a vector in\nlookup tables. As learning inputs in computer vision has almost never been\nstudied in the existing literature, we explore several aspects of this question\nthrough varieties of experiments on image classification tasks. Experimental\nresults on four benchmark datasets, i.e., CIFAR-10, CIFAR-100, Tiny ImageNet,\nand ImageNet (ILSVRC2012) have shown several surprising characteristics of\nLookup-VNets and have demonstrated the advantages and promise of Lookup-VNets\nand deep collective learning.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 00:33:04 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Deng", "Xiang", "", "Mark"], ["Zhongfei", "", "", "Mark"], ["Zhang", "", ""]]}, {"id": "2009.07994", "submitter": "Yanlun Tu", "authors": "Yanlun Tu, Jianxing Feng, Yang Yang", "title": "AAG: Self-Supervised Representation Learning by Auxiliary Augmentation\n  with GNT-Xent Loss", "comments": "8 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised representation learning is an emerging research topic for its\npowerful capacity in learning with unlabeled data. As a mainstream\nself-supervised learning method, augmentation-based contrastive learning has\nachieved great success in various computer vision tasks that lack manual\nannotations. Despite current progress, the existing methods are often limited\nby extra cost on memory or storage, and their performance still has large room\nfor improvement. Here we present a self-supervised representation learning\nmethod, namely AAG, which is featured by an auxiliary augmentation strategy and\nGNT-Xent loss. The auxiliary augmentation is able to promote the performance of\ncontrastive learning by increasing the diversity of images. The proposed\nGNT-Xent loss enables a steady and fast training process and yields competitive\naccuracy. Experiment results demonstrate the superiority of AAG to previous\nstate-of-the-art methods on CIFAR10, CIFAR100, and SVHN. Especially, AAG\nachieves 94.5% top-1 accuracy on CIFAR10 with batch size 64, which is 0.5%\nhigher than the best result of SimCLR with batch size 1024.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 00:54:35 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 08:15:20 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Tu", "Yanlun", ""], ["Feng", "Jianxing", ""], ["Yang", "Yang", ""]]}, {"id": "2009.07995", "submitter": "Junnan Li Dr", "authors": "Junnan Li, Caiming Xiong, Steven C.H. Hoi", "title": "MoPro: Webly Supervised Learning with Momentum Prototypes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a webly-supervised representation learning method that does not\nsuffer from the annotation unscalability of supervised learning, nor the\ncomputation unscalability of self-supervised learning. Most existing works on\nwebly-supervised representation learning adopt a vanilla supervised learning\nmethod without accounting for the prevalent noise in the training data, whereas\nmost prior methods in learning with label noise are less effective for\nreal-world large-scale noisy data. We propose momentum prototypes (MoPro), a\nsimple contrastive learning method that achieves online label noise correction,\nout-of-distribution sample removal, and representation learning. MoPro achieves\nstate-of-the-art performance on WebVision, a weakly-labeled noisy dataset.\nMoPro also shows superior performance when the pretrained model is transferred\nto down-stream image classification and detection tasks. It outperforms the\nImageNet supervised pretrained model by +10.5 on 1-shot classification on VOC,\nand outperforms the best self-supervised pretrained model by +17.3 when\nfinetuned on 1\\% of ImageNet labeled samples. Furthermore, MoPro is more robust\nto distribution shifts. Code and pretrained models are available at\nhttps://github.com/salesforce/MoPro.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 00:59:59 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Li", "Junnan", ""], ["Xiong", "Caiming", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2009.08003", "submitter": "Yingying Deng", "authors": "Yingying Deng, Fan Tang, Weiming Dong, Haibin Huang, Chongyang Ma,\n  Changsheng Xu", "title": "Arbitrary Video Style Transfer via Multi-Channel Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video style transfer is getting more attention in AI community for its\nnumerous applications such as augmented reality and animation productions.\nCompared with traditional image style transfer, performing this task on video\npresents new challenges: how to effectively generate satisfactory stylized\nresults for any specified style, and maintain temporal coherence across frames\nat the same time. Towards this end, we propose Multi-Channel Correction network\n(MCCNet), which can be trained to fuse the exemplar style features and input\ncontent features for efficient style transfer while naturally maintaining the\ncoherence of input videos. Specifically, MCCNet works directly on the feature\nspace of style and content domain where it learns to rearrange and fuse style\nfeatures based on their similarity with content features. The outputs generated\nby MCC are features containing the desired style patterns which can further be\ndecoded into images with vivid style textures. Moreover, MCCNet is also\ndesigned to explicitly align the features to input which ensures the output\nmaintains the content structures as well as the temporal continuity. To further\nimprove the performance of MCCNet under complex light conditions, we also\nintroduce the illumination loss during training. Qualitative and quantitative\nevaluations demonstrate that MCCNet performs well in both arbitrary video and\nimage style transfer tasks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 01:30:46 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 03:22:05 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Deng", "Yingying", ""], ["Tang", "Fan", ""], ["Dong", "Weiming", ""], ["Huang", "Haibin", ""], ["Ma", "Chongyang", ""], ["Xu", "Changsheng", ""]]}, {"id": "2009.08005", "submitter": "Essam Rashed", "authors": "Sachiko Kodera, Akimasa Hirata, Fumiaki Miura, Essam A. Rashed,\n  Natsuko Hatsusaka, Naoki Yamamoto, Eri Kubo, Hiroshi Sasaki", "title": "Model-based approach for analyzing prevalence of nuclear cataracts in\n  elderly residents", "comments": "Submitted to Computers in Biology and Medicine", "journal-ref": "Computers in Biology and Medicine, 2020", "doi": "10.1016/j.compbiomed.2020.104009", "report-no": null, "categories": "physics.med-ph cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent epidemiological studies have hypothesized that the prevalence of\ncortical cataracts is closely related to ultraviolet radiation. However, the\nprevalence of nuclear cataracts is higher in elderly people in tropical areas\nthan in temperate areas. The dominant factors inducing nuclear cataracts have\nbeen widely debated. In this study, the temperature increase in the lens due to\nexposure to ambient conditions was computationally quantified in subjects of\n50-60 years of age in tropical and temperate areas, accounting for differences\nin thermoregulation. A thermoregulatory response model was extended to consider\nelderly people in tropical areas. The time course of lens temperature for\ndifferent weather conditions in five cities in Asia was computed. The\ntemperature was higher around the mid and posterior part of the lens, which\ncoincides with the position of the nuclear cataract. The duration of higher\ntemperatures in the lens varied, although the daily maximum temperatures were\ncomparable. A strong correlation (adjusted R2 > 0.85) was observed between the\nprevalence of nuclear cataract and the computed cumulative thermal dose in the\nlens. We propose the use of a cumulative thermal dose to assess the prevalence\nof nuclear cataracts. Cumulative wet-bulb globe temperature, a new metric\ncomputed from weather data, would be useful for practical assessment in\ndifferent cities.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 01:35:58 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Kodera", "Sachiko", ""], ["Hirata", "Akimasa", ""], ["Miura", "Fumiaki", ""], ["Rashed", "Essam A.", ""], ["Hatsusaka", "Natsuko", ""], ["Yamamoto", "Naoki", ""], ["Kubo", "Eri", ""], ["Sasaki", "Hiroshi", ""]]}, {"id": "2009.08012", "submitter": "Chaoyou Fu", "authors": "Chaoyou Fu, Guoli Wang, Xiang Wu, Qian Zhang, Ran He", "title": "Deep Momentum Uncertainty Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial optimization (CO) has been a hot research topic because of its\ntheoretic and practical importance. As a classic CO problem, deep hashing aims\nto find an optimal code for each data from finite discrete possibilities, while\nthe discrete nature brings a big challenge to the optimization process.\nPrevious methods usually mitigate this challenge by binary approximation,\nsubstituting binary codes for real-values via activation functions or\nregularizations. However, such approximation leads to uncertainty between\nreal-values and binary ones, degrading retrieval performance. In this paper, we\npropose a novel Deep Momentum Uncertainty Hashing (DMUH). It explicitly\nestimates the uncertainty during training and leverages the uncertainty\ninformation to guide the approximation process. Specifically, we model\nbit-level uncertainty via measuring the discrepancy between the output of a\nhashing network and that of a momentum-updated network. The discrepancy of each\nbit indicates the uncertainty of the hashing network to the approximate output\nof that bit. Meanwhile, the mean discrepancy of all bits in a hashing code can\nbe regarded as image-level uncertainty. It embodies the uncertainty of the\nhashing network to the corresponding input image. The hashing bit and image\nwith higher uncertainty are paid more attention during optimization. To the\nbest of our knowledge, this is the first work to study the uncertainty in\nhashing bits. Extensive experiments are conducted on four datasets to verify\nthe superiority of our method, including CIFAR-10, NUS-WIDE, MS-COCO, and a\nmillion-scale dataset Clothing1M. Our method achieves the best performance on\nall of the datasets and surpasses existing state-of-the-art methods by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 01:57:45 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 04:07:24 GMT"}, {"version": "v3", "created": "Tue, 13 Jul 2021 07:25:50 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Fu", "Chaoyou", ""], ["Wang", "Guoli", ""], ["Wu", "Xiang", ""], ["Zhang", "Qian", ""], ["He", "Ran", ""]]}, {"id": "2009.08016", "submitter": "Linhai Ma", "authors": "Liang Liang, Linhai Ma, Linchen Qian, Jiasong Chen", "title": "An Algorithm for Out-Of-Distribution Attack to Neural Network Encoder", "comments": "26 pages, 25 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs), especially convolutional neural networks, have\nachieved superior performance on image classification tasks. However, such\nperformance is only guaranteed if the input to a trained model is similar to\nthe training samples, i.e., the input follows the probability distribution of\nthe training set. Out-Of-Distribution (OOD) samples do not follow the\ndistribution of training set, and therefore the predicted class labels on OOD\nsamples become meaningless. Classification-based methods have been proposed for\nOOD detection; however, in this study we show that this type of method has no\ntheoretical guarantee and is practically breakable by our OOD Attack algorithm\nbecause of dimensionality reduction in the DNN models. We also show that Glow\nlikelihood-based OOD detection is breakable as well.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 02:10:36 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 20:47:08 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 22:31:12 GMT"}, {"version": "v4", "created": "Wed, 27 Jan 2021 17:58:34 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Liang", "Liang", ""], ["Ma", "Linhai", ""], ["Qian", "Linchen", ""], ["Chen", "Jiasong", ""]]}, {"id": "2009.08020", "submitter": "Shoaib Azam", "authors": "Farzeen Munir, Shoaib Azam, and Moongu Jeon", "title": "LDNet: End-to-End Lane Detection Approach usinga Dynamic Vision Sensor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern vehicles are equipped with various driver-assistance systems,\nincluding automatic lane keeping, which prevents unintended lane departures.\nTraditional lane detection methods incorporate handcrafted or deep\nlearning-based features followed by postprocessing techniques for lane\nextraction using RGB cameras. The utilization of a RGB camera for lane\ndetection tasks is prone to illumination variations, sun glare, and motion\nblur, which limits the performance of the lane detection method. The\nincorporation of an event camera for lane detection tasks in the perception\nstack of autonomous driving is one of the most promising solutions for\nmitigating challenges encountered by RGB cameras. In this work, Lane Detection\nusing dynamic vision sensor (LDNet), is proposed, that is designed in an\nencoder-decoder manner with an atrous spatial pyramid pooling block followed by\nan attention-guided decoder for predicting and reducing false predictions in\nlane detection tasks. This decoder eliminates the implicit need for a\npostprocessing step. The experimental results show the significant improvement\nof $5.54\\%$ and $5.03\\%$ on the $F1$ scores in the multiclass and binary class\nlane detection tasks, respectively. Additionally, the $IoU$ scores of the\nproposed method surpass those of the best-performing state-of-the-art method by\n$6.50\\%$ and $9.37\\%$ in the multiclass and binary class tasks, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 02:15:41 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Munir", "Farzeen", ""], ["Azam", "Shoaib", ""], ["Jeon", "Moongu", ""]]}, {"id": "2009.08026", "submitter": "R. Kenny Jones", "authors": "R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang,\n  Paul Guerrero, Niloy J. Mitra, and Daniel Ritchie", "title": "ShapeAssembly: Learning to Generate Programs for 3D Shape Structure\n  Synthesis", "comments": "Accepted to Siggraph Asia 2020; project page:\n  https://rkjones4.github.io/shapeAssembly.html", "journal-ref": null, "doi": "10.1145/3414685.3417812", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manually authoring 3D shapes is difficult and time consuming; generative\nmodels of 3D shapes offer compelling alternatives. Procedural representations\nare one such possibility: they offer high-quality and editable results but are\ndifficult to author and often produce outputs with limited diversity. On the\nother extreme are deep generative models: given enough data, they can learn to\ngenerate any class of shape but their outputs have artifacts and the\nrepresentation is not editable. In this paper, we take a step towards achieving\nthe best of both worlds for novel 3D shape synthesis. We propose ShapeAssembly,\na domain-specific \"assembly-language\" for 3D shape structures. ShapeAssembly\nprograms construct shapes by declaring cuboid part proxies and attaching them\nto one another, in a hierarchical and symmetrical fashion. Its functions are\nparameterized with free variables, so that one program structure is able to\ncapture a family of related shapes. We show how to extract ShapeAssembly\nprograms from existing shape structures in the PartNet dataset. Then we train a\ndeep generative model, a hierarchical sequence VAE, that learns to write novel\nShapeAssembly programs. The program captures the subset of variability that is\ninterpretable and editable. The deep model captures correlations across shape\ncollections that are hard to express procedurally. We evaluate our approach by\ncomparing shapes output by our generated programs to those from other recent\nshape structure synthesis models. We find that our generated shapes are more\nplausible and physically-valid than those of other methods. Additionally, we\nassess the latent spaces of these models, and find that ours is better\nstructured and produces smoother interpolations. As an application, we use our\ngenerative model and differentiable program interpreter to infer and fit shape\nprograms to unstructured geometry, such as point clouds.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 02:26:45 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Jones", "R. Kenny", ""], ["Barton", "Theresa", ""], ["Xu", "Xianghao", ""], ["Wang", "Kai", ""], ["Jiang", "Ellen", ""], ["Guerrero", "Paul", ""], ["Mitra", "Niloy J.", ""], ["Ritchie", "Daniel", ""]]}, {"id": "2009.08027", "submitter": "Yifan Zhao", "authors": "Xin Guo, Jia Li, Yifan Zhao", "title": "DanceIt: Music-inspired Dancing Video Synthesis", "comments": "14 pages, 18 figures", "journal-ref": null, "doi": "10.1109/TIP.2021.3086082", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Close your eyes and listen to music, one can easily imagine an actor dancing\nrhythmically along with the music. These dance movements are usually made up of\ndance movements you have seen before. In this paper, we propose to reproduce\nsuch an inherent capability of the human-being within a computer vision system.\nThe proposed system consists of three modules. To explore the relationship\nbetween music and dance movements, we propose a cross-modal alignment module\nthat focuses on dancing video clips, accompanied on pre-designed music, to\nlearn a system that can judge the consistency between the visual features of\npose sequences and the acoustic features of music. The learned model is then\nused in the imagination module to select a pose sequence for the given music.\nSuch pose sequence selected from the music, however, is usually discontinuous.\nTo solve this problem, in the spatial-temporal alignment module we develop a\nspatial alignment algorithm based on the tendency and periodicity of dance\nmovements to predict dance movements between discontinuous fragments. In\naddition, the selected pose sequence is often misaligned with the music beat.\nTo solve this problem, we further develop a temporal alignment algorithm to\nalign the rhythm of music and dance. Finally, the processed pose sequence is\nused to synthesize realistic dancing videos in the imagination module. The\ngenerated dancing videos match the content and rhythm of the music.\nExperimental results and subjective evaluations show that the proposed approach\ncan perform the function of generating promising dancing videos by inputting\nmusic.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 02:29:13 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Guo", "Xin", ""], ["Li", "Jia", ""], ["Zhao", "Yifan", ""]]}, {"id": "2009.08037", "submitter": "Pawan Kumar Singh Dr.", "authors": "Pawan Kumar Singh, Shubham Sinha, Sagnik Pal Chowdhury, Ram Sarkar,\n  Mita Nasipuri", "title": "Word Segmentation from Unconstrained Handwritten Bangla Document Images\n  using Distance Transform", "comments": "12 pages, 5 figures, conference", "journal-ref": "7th International Conference on Advances in Communication, Network\n  and Computing (CNC),pp. 271-282, 2016", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Segmentation of handwritten document images into text lines and words is one\nof the most significant and challenging tasks in the development of a complete\nOptical Character Recognition (OCR) system. This paper addresses the automatic\nsegmentation of text words directly from unconstrained Bangla handwritten\ndocument images. The popular Distance transform (DT) algorithm is applied for\nlocating the outer boundary of the word images. This technique is free from\ngenerating the over-segmented words. A simple post-processing procedure is\napplied to isolate the under-segmented word images, if any. The proposed\ntechnique is tested on 50 random images taken from CMATERdb1.1.1 database.\nSatisfactory result is achieved with a segmentation accuracy of 91.88% which\nconfirms the robustness of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 03:14:27 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Singh", "Pawan Kumar", ""], ["Sinha", "Shubham", ""], ["Chowdhury", "Sagnik Pal", ""], ["Sarkar", "Ram", ""], ["Nasipuri", "Mita", ""]]}, {"id": "2009.08040", "submitter": "Xianqi He", "authors": "Xianqi He, Zirui Li, Xufeng Yin, Jianwei Gong, Cheng Gong", "title": "High-precision target positioning system for unmanned vehicles based on\n  binocular vision", "comments": "15 pages, in Chinese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned vehicles often need to locate targets with high precision during\nwork. In the unmanned material handling workshop, the unmanned vehicle needs to\nperform high-precision pose estimation of the workpiece to accurately grasp the\nworkpiece. In this context, this paper proposes a high-precision unmanned\nvehicle target positioning system based on binocular vision. The system uses a\nregion-based stereo matching algorithm to obtain a disparity map, and uses the\nRANSAC algorithm to extract position and posture features, which achives the\nestimation of the position and attitude of a six-degree-of-freedom cylindrical\nworkpiece. In order to verify the effect of the system, this paper collects the\naccuracy and calculation time of the output results of the cylinder in\ndifferent poses. The experimental data shows that the position accuracy of the\nsystem is 0.61~1.17mm and the angular accuracy is 1.95~5.13{\\deg}, which can\nachieve better high-precision positioning effect.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 03:33:05 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["He", "Xianqi", ""], ["Li", "Zirui", ""], ["Yin", "Xufeng", ""], ["Gong", "Jianwei", ""], ["Gong", "Cheng", ""]]}, {"id": "2009.08049", "submitter": "Shen Yan", "authors": "Shen Yan, Yang Pen, Shiming Lai, Yu Liu, Maojun Zhang", "title": "Image Retrieval for Structure-from-Motion via Graph Convolutional\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional image retrieval techniques for Structure-from-Motion (SfM)\nsuffer from the limit of effectively recognizing repetitive patterns and cannot\nguarantee to create just enough match pairs with high precision and high\nrecall. In this paper, we present a novel retrieval method based on Graph\nConvolutional Network (GCN) to generate accurate pairwise matches without\ncostly redundancy. We formulate image retrieval task as a node binary\nclassification problem in graph data: a node is marked as positive if it shares\nthe scene overlaps with the query image. The key idea is that we find that the\nlocal context in feature space around a query image contains rich information\nabout the matchable relation between this image and its neighbors. By\nconstructing a subgraph surrounding the query image as input data, we adopt a\nlearnable GCN to exploit whether nodes in the subgraph have overlapping regions\nwith the query photograph. Experiments demonstrate that our method performs\nremarkably well on the challenging dataset of highly ambiguous and duplicated\nscenes. Besides, compared with state-of-the-art matchable retrieval methods,\nthe proposed approach significantly reduces useless attempted matches without\nsacrificing the accuracy and completeness of reconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 04:03:51 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Yan", "Shen", ""], ["Pen", "Yang", ""], ["Lai", "Shiming", ""], ["Liu", "Yu", ""], ["Zhang", "Maojun", ""]]}, {"id": "2009.08058", "submitter": "Shao-Yuan Lo", "authors": "Shao-Yuan Lo, Vishal M. Patel", "title": "MultAV: Multiplicative Adversarial Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of adversarial machine learning research focuses on additive\nthreat models, which add adversarial perturbation to input data. On the other\nhand, unlike image recognition problems, only a handful of threat models have\nbeen explored in the video domain. In this paper, we propose a novel\nadversarial attack against video recognition models, Multiplicative Adversarial\nVideos (MultAV), which imposes perturbation on video data by multiplication.\nMultAV has different noise distributions to the additive counterparts and thus\nchallenges the defense methods tailored to resisting additive attacks.\nMoreover, it can be generalized to not only Lp-norm attacks with a new\nadversary constraint called ratio bound, but also different types of physically\nrealizable attacks. Experimental results show that the model adversarially\ntrained against additive attack is less robust to MultAV.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 04:34:39 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Lo", "Shao-Yuan", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2009.08083", "submitter": "Li Su", "authors": "Cheng-Che Lee, Wan-Yi Lin, Yen-Ting Shih, Pei-Yi Patricia Kuo, Li Su", "title": "Crossing You in Style: Cross-modal Style Transfer from Music to Visual\n  Arts", "comments": null, "journal-ref": null, "doi": "10.1145/3394171.3413624", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Music-to-visual style transfer is a challenging yet important cross-modal\nlearning problem in the practice of creativity. Its major difference from the\ntraditional image style transfer problem is that the style information is\nprovided by music rather than images. Assuming that musical features can be\nproperly mapped to visual contents through semantic links between the two\ndomains, we solve the music-to-visual style transfer problem in two steps:\nmusic visualization and style transfer. The music visualization network\nutilizes an encoder-generator architecture with a conditional generative\nadversarial network to generate image-based music representations from music\ndata. This network is integrated with an image style transfer method to\naccomplish the style transfer process. Experiments are conducted on\nWikiArt-IMSLP, a newly compiled dataset including Western music recordings and\npaintings listed by decades. By utilizing such a label to learn the semantic\nconnection between paintings and music, we demonstrate that the proposed\nframework can generate diverse image style representations from a music piece,\nand these representations can unveil certain art forms of the same era.\nSubjective testing results also emphasize the role of the era label in\nimproving the perceptual quality on the compatibility between music and visual\ncontent.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 05:58:13 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Lee", "Cheng-Che", ""], ["Lin", "Wan-Yi", ""], ["Shih", "Yen-Ting", ""], ["Kuo", "Pei-Yi Patricia", ""], ["Su", "Li", ""]]}, {"id": "2009.08107", "submitter": "Alessia Bertugli", "authors": "Alessia Bertugli, Stefano Vincenzi, Simone Calderara, Andrea Passerini", "title": "Few-Shot Unsupervised Continual Learning through Meta-Examples", "comments": "Accepted at 34th Conference on Neural Information Processing Systems\n  (NeurIPS 2020), 4th Workshop on Meta-Learning, 16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world applications, data do not reflect the ones commonly used for\nneural networks training, since they are usually few, unlabeled and can be\navailable as a stream. Hence many existing deep learning solutions suffer from\na limited range of applications, in particular in the case of online streaming\ndata that evolve over time. To narrow this gap, in this work we introduce a\nnovel and complex setting involving unsupervised meta-continual learning with\nunbalanced tasks. These tasks are built through a clustering procedure applied\nto a fitted embedding space. We exploit a meta-learning scheme that\nsimultaneously alleviates catastrophic forgetting and favors the generalization\nto new tasks. Moreover, to encourage feature reuse during the\nmeta-optimization, we exploit a single inner loop taking advantage of an\naggregated representation achieved through the use of a self-attention\nmechanism. Experimental results on few-shot learning benchmarks show\ncompetitive performance even compared to the supervised case. Additionally, we\nempirically observe that in an unsupervised scenario, the small tasks and the\nvariability in the clusters pooling play a crucial role in the generalization\ncapability of the network. Further, on complex datasets, the exploitation of\nmore clusters than the true number of classes leads to higher results, even\ncompared to the ones obtained with full supervision, suggesting that a\npredefined partitioning into classes can miss relevant structural information.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 07:02:07 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 13:01:57 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 13:38:40 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Bertugli", "Alessia", ""], ["Vincenzi", "Stefano", ""], ["Calderara", "Simone", ""], ["Passerini", "Andrea", ""]]}, {"id": "2009.08110", "submitter": "Guanbin Li", "authors": "Haofeng Li, Yirui Zeng, Guanbin Li, Liang Lin, Yizhou Yu", "title": "Online Alternate Generator against Adversarial Attacks", "comments": "Accepted as a Regular paper in the IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": "10.1109/TIP.2020.3025404", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of computer vision has witnessed phenomenal progress in recent\nyears partially due to the development of deep convolutional neural networks.\nHowever, deep learning models are notoriously sensitive to adversarial examples\nwhich are synthesized by adding quasi-perceptible noises on real images. Some\nexisting defense methods require to re-train attacked target networks and\naugment the train set via known adversarial attacks, which is inefficient and\nmight be unpromising with unknown attack types. To overcome the above issues,\nwe propose a portable defense method, online alternate generator, which does\nnot need to access or modify the parameters of the target networks. The\nproposed method works by online synthesizing another image from scratch for an\ninput image, instead of removing or destroying adversarial noises. To avoid\npretrained parameters exploited by attackers, we alternately update the\ngenerator and the synthesized image at the inference stage. Experimental\nresults demonstrate that the proposed defensive scheme and method outperforms a\nseries of state-of-the-art defending models against gray-box adversarial\nattacks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 07:11:16 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Li", "Haofeng", ""], ["Zeng", "Yirui", ""], ["Li", "Guanbin", ""], ["Lin", "Liang", ""], ["Yu", "Yizhou", ""]]}, {"id": "2009.08119", "submitter": "Guanbin Li", "authors": "Ganlong Zhao, Guanbin Li, Ruijia Xu, Liang Lin", "title": "Collaborative Training between Region Proposal Localization and\n  Classification for Domain Adaptive Object Detection", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detectors are usually trained with large amount of labeled data, which\nis expensive and labor-intensive. Pre-trained detectors applied to unlabeled\ndataset always suffer from the difference of dataset distribution, also called\ndomain shift. Domain adaptation for object detection tries to adapt the\ndetector from labeled datasets to unlabeled ones for better performance. In\nthis paper, we are the first to reveal that the region proposal network (RPN)\nand region proposal classifier~(RPC) in the endemic two-stage detectors (e.g.,\nFaster RCNN) demonstrate significantly different transferability when facing\nlarge domain gap. The region classifier shows preferable performance but is\nlimited without RPN's high-quality proposals while simple alignment in the\nbackbone network is not effective enough for RPN adaptation. We delve into the\nconsistency and the difference of RPN and RPC, treat them individually and\nleverage high-confidence output of one as mutual guidance to train the other.\nMoreover, the samples with low-confidence are used for discrepancy calculation\nbetween RPN and RPC and minimax optimization. Extensive experimental results on\nvarious scenarios have demonstrated the effectiveness of our proposed method in\nboth domain-adaptive region proposal generation and object detection. Code is\navailable at https://github.com/GanlongZhao/CST_DA_detection.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 07:39:52 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 03:34:05 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Zhao", "Ganlong", ""], ["Li", "Guanbin", ""], ["Xu", "Ruijia", ""], ["Lin", "Liang", ""]]}, {"id": "2009.08123", "submitter": "Akshay Smit", "authors": "Damir Vrabac, Akshay Smit, Rebecca Rojansky, Yasodha Natkunam, Ranjana\n  H. Advani, Andrew Y. Ng, Sebastian Fernandez-Pol, Pranav Rajpurkar", "title": "DLBCL-Morph: Morphological features computed using deep learning for an\n  annotated digital DLBCL image set", "comments": "Corrections to folder structure figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffuse Large B-Cell Lymphoma (DLBCL) is the most common non-Hodgkin\nlymphoma. Though histologically DLBCL shows varying morphologies, no\nmorphologic features have been consistently demonstrated to correlate with\nprognosis. We present a morphologic analysis of histology sections from 209\nDLBCL cases with associated clinical and cytogenetic data. Duplicate tissue\ncore sections were arranged in tissue microarrays (TMAs), and replicate\nsections were stained with H&E and immunohistochemical stains for CD10, BCL6,\nMUM1, BCL2, and MYC. The TMAs are accompanied by pathologist-annotated\nregions-of-interest (ROIs) that identify areas of tissue representative of\nDLBCL. We used a deep learning model to segment all tumor nuclei in the ROIs,\nand computed several geometric features for each segmented nucleus. We fit a\nCox proportional hazards model to demonstrate the utility of these geometric\nfeatures in predicting survival outcome, and found that it achieved a C-index\n(95% CI) of 0.635 (0.574,0.691). Our finding suggests that geometric features\ncomputed from tumor nuclei are of prognostic importance, and should be\nvalidated in prospective studies.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 07:43:42 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 07:09:07 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2020 11:02:28 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Vrabac", "Damir", ""], ["Smit", "Akshay", ""], ["Rojansky", "Rebecca", ""], ["Natkunam", "Yasodha", ""], ["Advani", "Ranjana H.", ""], ["Ng", "Andrew Y.", ""], ["Fernandez-Pol", "Sebastian", ""], ["Rajpurkar", "Pranav", ""]]}, {"id": "2009.08136", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley", "title": "Multidimensional Scaling, Sammon Mapping, and Isomap: Tutorial and\n  Survey", "comments": "To appear as a part of an upcoming academic book on dimensionality\n  reduction and manifold learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional Scaling (MDS) is one of the first fundamental manifold\nlearning methods. It can be categorized into several methods, i.e., classical\nMDS, kernel classical MDS, metric MDS, and non-metric MDS. Sammon mapping and\nIsomap can be considered as special cases of metric MDS and kernel classical\nMDS, respectively. In this tutorial and survey paper, we review the theory of\nMDS, Sammon mapping, and Isomap in detail. We explain all the mentioned\ncategories of MDS. Then, Sammon mapping, Isomap, and kernel Isomap are\nexplained. Out-of-sample embedding for MDS and Isomap using eigenfunctions and\nkernel mapping are introduced. Then, Nystrom approximation and its use in\nlandmark MDS and landmark Isomap are introduced for big data embedding. We also\nprovide some simulations for illustrating the embedding by these methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 08:12:25 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Ghodsi", "Ali", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "2009.08140", "submitter": "Francesco Giuliari", "authors": "Yiming Wang, Francesco Giuliari, Riccardo Berra, Alberto Castellini,\n  Alessio Del Bue, Alessandro Farinelli, Marco Cristani, Francesco Setti", "title": "POMP: Pomcp-based Online Motion Planning for active visual search in\n  indoor environments", "comments": "Accepted at BMVC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we focus on the problem of learning an optimal policy for\nActive Visual Search (AVS) of objects in known indoor environments with an\nonline setup. Our POMP method uses as input the current pose of an agent (e.g.\na robot) and a RGB-D frame. The task is to plan the next move that brings the\nagent closer to the target object. We model this problem as a Partially\nObservable Markov Decision Process solved by a Monte-Carlo planning approach.\nThis allows us to make decisions on the next moves by iterating over the known\nscenario at hand, exploring the environment and searching for the object at the\nsame time. Differently from the current state of the art in Reinforcement\nLearning, POMP does not require extensive and expensive (in time and\ncomputation) labelled data so being very agile in solving AVS in small and\nmedium real scenarios. We only require the information of the floormap of the\nenvironment, an information usually available or that can be easily extracted\nfrom an a priori single exploration run. We validate our method on the publicly\navailable AVD benchmark, achieving an average success rate of 0.76 with an\naverage path length of 17.1, performing close to the state of the art but\nwithout any training needed. Additionally, we show experimentally the\nrobustness of our method when the quality of the object detection goes from\nideal to faulty.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 08:23:50 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Wang", "Yiming", ""], ["Giuliari", "Francesco", ""], ["Berra", "Riccardo", ""], ["Castellini", "Alberto", ""], ["Del Bue", "Alessio", ""], ["Farinelli", "Alessandro", ""], ["Cristani", "Marco", ""], ["Setti", "Francesco", ""]]}, {"id": "2009.08169", "submitter": "Fabian Timm", "authors": "Lukas Enderich and Fabian Timm and Wolfram Burgard", "title": "Holistic Filter Pruning for Efficient Deep Neural Networks", "comments": "preprint, accepted at WACV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are usually over-parameterized to increase the\nlikelihood of getting adequate initial weights by random initialization.\nConsequently, trained DNNs have many redundancies which can be pruned from the\nmodel to reduce complexity and improve the ability to generalize. Structural\nsparsity, as achieved by filter pruning, directly reduces the tensor sizes of\nweights and activations and is thus particularly effective for reducing\ncomplexity. We propose \"Holistic Filter Pruning\" (HFP), a novel approach for\ncommon DNN training that is easy to implement and enables to specify accurate\npruning rates for the number of both parameters and multiplications. After each\nforward pass, the current model complexity is calculated and compared to the\ndesired target size. By gradient descent, a global solution can be found that\nallocates the pruning budget over the individual layers such that the desired\ntarget size is fulfilled. In various experiments, we give insights into the\ntraining and achieve state-of-the-art performance on CIFAR-10 and ImageNet (HFP\nprunes 60% of the multiplications of ResNet-50 on ImageNet with no significant\nloss in the accuracy). We believe our simple and powerful pruning approach to\nconstitute a valuable contribution for users of DNNs in low-cost applications.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 09:23:36 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Enderich", "Lukas", ""], ["Timm", "Fabian", ""], ["Burgard", "Wolfram", ""]]}, {"id": "2009.08182", "submitter": "Baran Ataman", "authors": "Baran Ataman and Esin Guldogan", "title": "Single Frame Deblurring with Laplacian Filters", "comments": "6 pages, 3 figures, 1 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind single image deblurring has been a challenge over many decades due to\nthe ill-posed nature of the problem. In this paper, we propose a single-frame\nblind deblurring solution with the aid of Laplacian filters. Utilized Residual\nDense Network has proven its strengths in superresolution task, thus we\nselected it as a baseline architecture. We evaluated the proposed solution with\nstate-of-art DNN methods on a benchmark dataset. The proposed method shows\nsignificant improvement in image quality measured objectively and subjectively.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 09:49:25 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Ataman", "Baran", ""], ["Guldogan", "Esin", ""]]}, {"id": "2009.08188", "submitter": "Devis Tuia", "authors": "John E. Vargas-Mu\\~noz, Devis Tuia, Alexandre X. Falc\\~ao", "title": "Deploying machine learning to assist digital humanitarians: making image\n  annotation in OpenStreetMap more efficient", "comments": null, "journal-ref": null, "doi": "10.1080/13658816.2020.1814303", "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Locating populations in rural areas of developing countries has attracted the\nattention of humanitarian mapping projects since it is important to plan\nactions that affect vulnerable areas. Recent efforts have tackled this problem\nas the detection of buildings in aerial images. However, the quality and the\namount of rural building annotated data in open mapping services like\nOpenStreetMap (OSM) is not sufficient for training accurate models for such\ndetection. Although these methods have the potential of aiding in the update of\nrural building information, they are not accurate enough to automatically\nupdate the rural building maps. In this paper, we explore a human-computer\ninteraction approach and propose an interactive method to support and optimize\nthe work of volunteers in OSM. The user is asked to verify/correct the\nannotation of selected tiles during several iterations and therefore improving\nthe model with the new annotated data. The experimental results, with simulated\nand real user annotation corrections, show that the proposed method greatly\nreduces the amount of data that the volunteers of OSM need to verify/correct.\nThe proposed methodology could benefit humanitarian mapping projects, not only\nby making more efficient the process of annotation but also by improving the\nengagement of volunteers.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 10:05:30 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Vargas-Mu\u00f1oz", "John E.", ""], ["Tuia", "Devis", ""], ["Falc\u00e3o", "Alexandre X.", ""]]}, {"id": "2009.08194", "submitter": "Thomas Gittings", "authors": "T. Gittings, S. Schneider and J. Collomosse", "title": "Vax-a-Net: Training-time Defence Against Adversarial Patch Attacks", "comments": "16 pages, 10 figures, ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Vax-a-Net; a technique for immunizing convolutional neural\nnetworks (CNNs) against adversarial patch attacks (APAs). APAs insert visually\novert, local regions (patches) into an image to induce misclassification. We\nintroduce a conditional Generative Adversarial Network (GAN) architecture that\nsimultaneously learns to synthesise patches for use in APAs, whilst exploiting\nthose attacks to adapt a pre-trained target CNN to reduce its susceptibility to\nthem. This approach enables resilience against APAs to be conferred to\npre-trained models, which would be impractical with conventional adversarial\ntraining due to the slow convergence of APA methods. We demonstrate\ntransferability of this protection to defend against existing APAs, and show\nits efficacy across several contemporary CNN architectures.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 10:27:08 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Gittings", "T.", ""], ["Schneider", "S.", ""], ["Collomosse", "J.", ""]]}, {"id": "2009.08219", "submitter": "Junaid Ahmed Ghauri", "authors": "Chanjong Im, Junaid Ghauri, John Rothman, Thomas Mandl", "title": "Deep Learning Approaches to Classification of Production Technology for\n  19th Century Books", "comments": "LWDA 2018: Mannheim, Germany", "journal-ref": "Proceedings of the Conference \"Lernen, Wissen, Daten, Analysen\",\n  {LWDA} 2018, Mannheim, Germany, August 22-24, 2018", "doi": null, "report-no": "ceur-ws.org/Vol-2191", "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cultural research is dedicated to understanding the processes of knowledge\ndissemination and the social and technological practices in the book industry.\nResearch on children books in the 19th century can be supported by computer\nsystems. Specifically, the advances in digital image processing seem to offer\ngreat opportunities for analyzing and quantifying the visual components in the\nbooks. The production technology for illustrations in books in the 19th century\nwas characterized by a shift from wood or copper engraving to lithography. We\nreport classification experiments which intend to classify images based on the\nproduction technology. For a classification task that is also difficult for\nhumans, the classification quality reaches only around 70%. We analyze some\nfurther error sources and identify reasons for the low performance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 11:39:35 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Im", "Chanjong", ""], ["Ghauri", "Junaid", ""], ["Rothman", "John", ""], ["Mandl", "Thomas", ""]]}, {"id": "2009.08233", "submitter": "Chaohao Fu", "authors": "Chaohao Fu, Hongbin Chen, Na Ruan, Weijia Jia", "title": "Label Smoothing and Adversarial Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies indicate that current adversarial attack methods are flawed\nand easy to fail when encountering some deliberately designed defense.\nSometimes even a slight modification in the model details will invalidate the\nattack. We find that training model with label smoothing can easily achieve\nstriking accuracy under most gradient-based attacks. For instance, the robust\naccuracy of a WideResNet model trained with label smoothing on CIFAR-10\nachieves 75% at most under PGD attack. To understand the reason underlying the\nsubtle robustness, we investigate the relationship between label smoothing and\nadversarial robustness. Through theoretical analysis about the characteristics\nof the network trained with label smoothing and experiment verification of its\nperformance under various attacks. We demonstrate that the robustness produced\nby label smoothing is incomplete based on the fact that its defense effect is\nvolatile, and it cannot defend attacks transferred from a naturally trained\nmodel. Our study enlightens the research community to rethink how to evaluate\nthe model's robustness appropriately.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 12:36:35 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Fu", "Chaohao", ""], ["Chen", "Hongbin", ""], ["Ruan", "Na", ""], ["Jia", "Weijia", ""]]}, {"id": "2009.08246", "submitter": "Anyong Qin", "authors": "Anyong Qin and Zhaowei Shang and Zhuolin Tan and Taiping Zhang and\n  Yuan Yan Tang", "title": "Learning a Deep Part-based Representation by Preserving Data\n  Distribution", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised dimensionality reduction is one of the commonly used techniques\nin the field of high dimensional data recognition problems. The deep\nautoencoder network which constrains the weights to be non-negative, can learn\na low dimensional part-based representation of data. On the other hand, the\ninherent structure of the each data cluster can be described by the\ndistribution of the intraclass samples. Then one hopes to learn a new low\ndimensional representation which can preserve the intrinsic structure embedded\nin the original high dimensional data space perfectly. In this paper, by\npreserving the data distribution, a deep part-based representation can be\nlearned, and the novel algorithm is called Distribution Preserving Network\nEmbedding (DPNE). In DPNE, we first need to estimate the distribution of the\noriginal high dimensional data using the $k$-nearest neighbor kernel density\nestimation, and then we seek a part-based representation which respects the\nabove distribution. The experimental results on the real-world data sets show\nthat the proposed algorithm has good performance in terms of cluster accuracy\nand AMI. It turns out that the manifold structure in the raw data can be well\npreserved in the low dimensional feature space.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 12:49:36 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Qin", "Anyong", ""], ["Shang", "Zhaowei", ""], ["Tan", "Zhuolin", ""], ["Zhang", "Taiping", ""], ["Tang", "Yuan Yan", ""]]}, {"id": "2009.08250", "submitter": "Longguang Wang", "authors": "Longguang Wang and Yulan Guo and Yingqian Wang and Zhengfa Liang and\n  Zaiping Lin and Jungang Yang and Wei An", "title": "Parallax Attention for Unsupervised Stereo Correspondence Learning", "comments": "Accepted by IEEE TPAMI 2020. arXiv admin note: text overlap with\n  arXiv:1903.05784", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo image pairs encode 3D scene cues into stereo correspondences between\nthe left and right images. To exploit 3D cues within stereo images, recent CNN\nbased methods commonly use cost volume techniques to capture stereo\ncorrespondence over large disparities. However, since disparities can vary\nsignificantly for stereo cameras with different baselines, focal lengths and\nresolutions, the fixed maximum disparity used in cost volume techniques hinders\nthem to handle different stereo image pairs with large disparity variations. In\nthis paper, we propose a generic parallax-attention mechanism (PAM) to capture\nstereo correspondence regardless of disparity variations. Our PAM integrates\nepipolar constraints with attention mechanism to calculate feature similarities\nalong the epipolar line to capture stereo correspondence. Based on our PAM, we\npropose a parallax-attention stereo matching network (PASMnet) and a\nparallax-attention stereo image super-resolution network (PASSRnet) for stereo\nmatching and stereo image super-resolution tasks. Moreover, we introduce a new\nand large-scale dataset named Flickr1024 for stereo image super-resolution.\nExperimental results show that our PAM is generic and can effectively learn\nstereo correspondence under large disparity variations in an unsupervised\nmanner. Comparative results show that our PASMnet and PASSRnet achieve the\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 01:30:13 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Wang", "Longguang", ""], ["Guo", "Yulan", ""], ["Wang", "Yingqian", ""], ["Liang", "Zhengfa", ""], ["Lin", "Zaiping", ""], ["Yang", "Jungang", ""], ["An", "Wei", ""]]}, {"id": "2009.08253", "submitter": "Jiju Peethambaran Poovvancheri", "authors": "Sumesh Thakur and Jiju Peethambaran", "title": "Dynamic Edge Weights in Graph Neural Networks for 3D Object Detection", "comments": "11 pages; 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust and accurate 3D detection system is an integral part of autonomous\nvehicles. Traditionally, a majority of 3D object detection algorithms focus on\nprocessing 3D point clouds using voxel grids or bird's eye view (BEV). Recent\nworks, however, demonstrate the utilization of the graph neural network (GNN)\nas a promising approach to 3D object detection. In this work, we propose an\nattention based feature aggregation technique in GNN for detecting objects in\nLiDAR scan. We first employ a distance-aware down-sampling scheme that not only\nenhances the algorithmic performance but also retains maximum geometric\nfeatures of objects even if they lie far from the sensor. In each layer of the\nGNN, apart from the linear transformation which maps the per node input\nfeatures to the corresponding higher level features, a per node masked\nattention by specifying different weights to different nodes in its first ring\nneighborhood is also performed. The masked attention implicitly accounts for\nthe underlying neighborhood graph structure of every node and also eliminates\nthe need of costly matrix operations thereby improving the detection accuracy\nwithout compromising the performance. The experiments on KITTI dataset show\nthat our method yields comparable results for 3D object detection.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 12:56:17 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Thakur", "Sumesh", ""], ["Peethambaran", "Jiju", ""]]}, {"id": "2009.08255", "submitter": "Fangneng Zhan", "authors": "Fangneng Zhan, Shijian Lu, Changgong Zhang, Feiying Ma, Xuansong Xie", "title": "Adversarial Image Composition with Auxiliary Illumination", "comments": "Accepted to ACCV 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dealing with the inconsistency between a foreground object and a background\nimage is a challenging task in high-fidelity image composition.\nState-of-the-art methods strive to harmonize the composed image by adapting the\nstyle of foreground objects to be compatible with the background image, whereas\nthe potential shadow of foreground objects within the composed image which is\ncritical to the composition realism is largely neglected. In this paper, we\npropose an Adversarial Image Composition Net (AIC-Net) that achieves realistic\nimage composition by considering potential shadows that the foreground object\nprojects in the composed image. A novel branched generation mechanism is\nproposed, which disentangles the generation of shadows and the transfer of\nforeground styles for optimal accomplishment of the two tasks simultaneously. A\ndifferentiable spatial transformation module is designed which bridges the\nlocal harmonization and the global harmonization to achieve their joint\noptimization effectively. Extensive experiments on pedestrian and car\ncomposition tasks show that the proposed AIC-Net achieves superior composition\nperformance qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 12:58:16 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 15:05:42 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Zhan", "Fangneng", ""], ["Lu", "Shijian", ""], ["Zhang", "Changgong", ""], ["Ma", "Feiying", ""], ["Xie", "Xuansong", ""]]}, {"id": "2009.08270", "submitter": "Saloni Dash", "authors": "Saloni Dash, Vineeth N Balasubramanian, Amit Sharma", "title": "Evaluating and Mitigating Bias in Image Classifiers: A Causal\n  Perspective Using Counterfactuals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have reported biases in machine learning image classifiers,\nespecially against particular demographic groups. Counterfactual examples for\nan input -- perturbations that change specific features but not others -- have\nbeen shown to be useful for evaluating explainability and fairness of machine\nlearning models. However, generating counterfactual examples for images is\nnon-trivial due to the underlying causal structure governing the various\nfeatures of an image. To be meaningful, generated perturbations need to satisfy\nconstraints implied by the causal model. We present a method for generating\ncounterfactuals by incorporating a structural causal model (SCM) in a novel\nimproved variant of Adversarially Learned Inference (ALI), that generates\ncounterfactuals in accordance with the causal relationships between different\nattributes of an image. Based on the generated counterfactuals, we show how to\nevaluate bias and explain a pre-trained machine learning classifier. We also\npropose a counterfactual regularizer that can mitigate bias in the classifier.\nOn the Morpho-MNIST dataset, our method generates counterfactuals comparable in\nquality to prior work on SCM-based counterfactuals. Our method also works on\nthe more complex CelebA faces dataset; generated counterfactuals are\nindistinguishable from original images in a human evaluation experiment. As a\ndownstream task, we use counterfactuals to evaluate a standard classifier\ntrained on CelebA data and show that it is biased w.r.t. skin and hair color,\nand show how counterfactual regularization can be used to remove the identified\nbiases.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 13:19:31 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 07:52:28 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 09:12:31 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Dash", "Saloni", ""], ["Balasubramanian", "Vineeth N", ""], ["Sharma", "Amit", ""]]}, {"id": "2009.08274", "submitter": "Liangming Chen", "authors": "Liangming Chen, Long Jin, Xiujuan Du, Shuai Li, Mei Liu", "title": "Deforming the Loss Surface to Affect the Behaviour of the Optimizer", "comments": "arXiv admin note: substantial text overlap with arXiv:2007.12515", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deep learning, it is usually assumed that the optimization process is\nconducted on a shape-fixed loss surface. Differently, we first propose a novel\nconcept of deformation mapping in this paper to affect the behaviour of the\noptimizer. Vertical deformation mapping (VDM), as a type of deformation\nmapping, can make the optimizer enter a flat region, which often implies better\ngeneralization performance. Moreover, we design various VDMs, and further\nprovide their contributions to the loss surface. After defining the local M\nregion, theoretical analyses show that deforming the loss surface can enhance\nthe gradient descent optimizer's ability to filter out sharp minima. With\nvisualizations of loss landscapes, we evaluate the flatnesses of minima\nobtained by both the original optimizer and optimizers enhanced by VDMs on\nCIFAR-100. The experimental results show that VDMs do find flatter regions.\nMoreover, we compare popular convolutional neural networks enhanced by VDMs\nwith the corresponding original ones on ImageNet, CIFAR-10, and CIFAR-100. The\nresults are surprising: there are significant improvements on all of the\ninvolved models equipped with VDMs. For example, the top-1 test accuracy of\nResNet-20 on CIFAR-100 increases by 1.46%, with insignificant additional\ncomputational overhead.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 06:43:16 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Chen", "Liangming", ""], ["Jin", "Long", ""], ["Du", "Xiujuan", ""], ["Li", "Shuai", ""], ["Liu", "Mei", ""]]}, {"id": "2009.08276", "submitter": "Jesus Hormigo", "authors": "David Albarrac\\'in, Jes\\'us Hormigo, Jos\\'e David Fern\\'andez", "title": "Video based real-time positional tracker", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a system that uses video as the input to track the position of\nobjects relative to their surrounding environment in real-time. The neural\nnetwork employed is trained on a 100% synthetic dataset coming from our own\nautomated generator. The positional tracker relies on a range of 1 to n video\ncameras placed around an arena of choice.\n  The system returns the positions of the tracked objects relative to the\nbroader world by understanding the overlapping matrices formed by the cameras\nand therefore these can be extrapolated into real world coordinates.\n  In most cases, we achieve a higher update rate and positioning precision than\nany of the existing GPS-based systems, in particular for indoor objects or\nthose occluded from clear sky.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 13:24:39 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 12:47:12 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2020 08:57:00 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Albarrac\u00edn", "David", ""], ["Hormigo", "Jes\u00fas", ""], ["Fern\u00e1ndez", "Jos\u00e9 David", ""]]}, {"id": "2009.08281", "submitter": "Michael Lyons", "authors": "Michael Lyons and Kazunori Morikawa", "title": "A Linked Aggregate Code for Processing Faces (Revised Version)", "comments": "18 pages, 3 figures, 1 table", "journal-ref": null, "doi": "10.5281/zenodo.4034544", "report-no": null, "categories": "cs.CV cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A model of face representation, inspired by the biology of the visual system,\nis compared to experimental data on the perception of facial similarity. The\nface representation model uses aggregate primary visual cortex (V1) cell\nresponses topographically linked to a grid covering the face, allowing\ncomparison of shape and texture at corresponding points in two facial images.\nWhen a set of relatively similar faces was used as stimuli, this Linked\nAggregate Code (LAC) predicted human performance in similarity judgment\nexperiments. When faces of perceivable categories were used, dimensions such as\napparent sex and race emerged from the LAC model without training. The\ndimensional structure of the LAC similarity measure for the mixed category task\ndisplayed some psychologically plausible features but also highlighted\ndifferences between the model and the human similarity judgements. The human\njudgements exhibited a racial perceptual bias that was not shared by the LAC\nmodel. The results suggest that the LAC based similarity measure may offer a\nfertile starting point for further modelling studies of face representation in\nhigher visual areas, including studies of the development of biases in face\nperception.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 13:29:25 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Lyons", "Michael", ""], ["Morikawa", "Kazunori", ""]]}, {"id": "2009.08283", "submitter": "Federico Paredes-Vall\\'es", "authors": "F. Paredes-Vall\\'es, G. C. H. E. de Croon", "title": "Back to Event Basics: Self-Supervised Learning of Image Reconstruction\n  for Event Cameras via Photometric Constancy", "comments": "9 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are novel vision sensors that sample, in an asynchronous\nfashion, brightness increments with low latency and high temporal resolution.\nThe resulting streams of events are of high value by themselves, especially for\nhigh speed motion estimation. However, a growing body of work has also focused\non the reconstruction of intensity frames from the events, as this allows\nbridging the gap with the existing literature on appearance- and frame-based\ncomputer vision. Recent work has mostly approached this problem using neural\nnetworks trained with synthetic, ground-truth data. In this work we approach,\nfor the first time, the intensity reconstruction problem from a self-supervised\nlearning perspective. Our method, which leverages the knowledge of the inner\nworkings of event cameras, combines estimated optical flow and the event-based\nphotometric constancy to train neural networks without the need for any\nground-truth or synthetic data. Results across multiple datasets show that the\nperformance of the proposed self-supervised approach is in line with the\nstate-of-the-art. Additionally, we propose a novel, lightweight neural network\nfor optical flow estimation that achieves high speed inference with only a\nminor drop in performance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 13:30:05 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 15:19:15 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Paredes-Vall\u00e9s", "F.", ""], ["de Croon", "G. C. H. E.", ""]]}, {"id": "2009.08292", "submitter": "Rama Krishna Kandukuri", "authors": "Rama Krishna Kandukuri, Jan Achterhold, Michael M\\\"oller, J\\\"org\n  St\\\"uckler", "title": "Learning to Identify Physical Parameters from Video Using Differentiable\n  Physics", "comments": "Accepted for 42nd German Conference on Pattern Recognition (DAGM-GCPR\n  2020), T\\\"ubingen, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video representation learning has recently attracted attention in computer\nvision due to its applications for activity and scene forecasting or\nvision-based planning and control. Video prediction models often learn a latent\nrepresentation of video which is encoded from input frames and decoded back\ninto images. Even when conditioned on actions, purely deep learning based\narchitectures typically lack a physically interpretable latent space. In this\nstudy, we use a differentiable physics engine within an action-conditional\nvideo representation network to learn a physical latent representation. We\npropose supervised and self-supervised learning methods to train our network\nand identify physical properties. The latter uses spatial transformers to\ndecode physical states back into images. The simulation scenarios in our\nexperiments comprise pushing, sliding and colliding objects, for which we also\nanalyze the observability of the physical properties. In experiments we\ndemonstrate that our network can learn to encode images and identify physical\nproperties like mass and friction from videos and action sequences in the\nsimulated scenarios. We evaluate the accuracy of our supervised and\nself-supervised methods and compare it with a system identification baseline\nwhich directly learns from state trajectories. We also demonstrate the ability\nof our method to predict future video frames from input images and actions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 13:36:57 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Kandukuri", "Rama Krishna", ""], ["Achterhold", "Jan", ""], ["M\u00f6ller", "Michael", ""], ["St\u00fcckler", "J\u00f6rg", ""]]}, {"id": "2009.08297", "submitter": "Anyong Qin", "authors": "Anyong Qin, Lina Xian, Yongliang Yang, Taiping Zhang, and Yuan Yan\n  Tang", "title": "Low-Rank Matrix Recovery from Noise via an MDL Framework-based Atomic\n  Norm", "comments": "14 pages, 13 figures", "journal-ref": "sensors-2020", "doi": "10.3390/s20216111", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recovery of the underlying low-rank structure of clean data corrupted\nwith sparse noise/outliers is attracting increasing interest. However, in many\nlow-level vision problems, the exact target rank of the underlying structure\nand the particular locations and values of the sparse outliers are not known.\nThus, the conventional methods cannot separate the low-rank and sparse\ncomponents completely, especially in the case of gross outliers or deficient\nobservations. Therefore, in this study, we employ the minimum description\nlength (MDL) principle and atomic norm for low-rank matrix recovery to overcome\nthese limitations. First, we employ the atomic norm to find all the candidate\natoms of low-rank and sparse terms, and then we minimize the description length\nof the model in order to select the appropriate atoms of low-rank and the\nsparse matrices, respectively. Our experimental analyses show that the proposed\napproach can obtain a higher success rate than the state-of-the-art methods,\neven when the number of observations is limited or the corruption ratio is\nhigh. Experimental results utilizing synthetic data and real sensing\napplications (high dynamic range imaging, background modeling, removing noise\nand shadows) demonstrate the effectiveness, robustness and efficiency of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 13:45:18 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 01:24:19 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Qin", "Anyong", ""], ["Xian", "Lina", ""], ["Yang", "Yongliang", ""], ["Zhang", "Taiping", ""], ["Tang", "Yuan Yan", ""]]}, {"id": "2009.08319", "submitter": "Michael Laskin", "authors": "Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin", "title": "Decoupling Representation Learning from Reinforcement Learning", "comments": "Improved related works and fixed code hyperlink", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an effort to overcome limitations of reward-driven feature learning in\ndeep reinforcement learning (RL) from images, we propose decoupling\nrepresentation learning from policy learning. To this end, we introduce a new\nunsupervised learning (UL) task, called Augmented Temporal Contrast (ATC),\nwhich trains a convolutional encoder to associate pairs of observations\nseparated by a short time difference, under image augmentations and using a\ncontrastive loss. In online RL experiments, we show that training the encoder\nexclusively using ATC matches or outperforms end-to-end RL in most\nenvironments. Additionally, we benchmark several leading UL algorithms by\npre-training encoders on expert demonstrations and using them, with weights\nfrozen, in RL agents; we find that agents using ATC-trained encoders outperform\nall others. We also train multi-task encoders on data from multiple\nenvironments and show generalization to different downstream RL tasks. Finally,\nwe ablate components of ATC, and introduce a new data augmentation to enable\nreplay of (compressed) latent images from pre-trained encoders when RL requires\naugmentation. Our experiments span visually diverse RL benchmarks in DeepMind\nControl, DeepMind Lab, and Atari, and our complete code is available at\nhttps://github.com/astooke/rlpyt/tree/master/rlpyt/ul.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 19:11:13 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 16:35:40 GMT"}, {"version": "v3", "created": "Sun, 16 May 2021 20:44:18 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Stooke", "Adam", ""], ["Lee", "Kimin", ""], ["Abbeel", "Pieter", ""], ["Laskin", "Michael", ""]]}, {"id": "2009.08321", "submitter": "Hoang-An Le", "authors": "Hoang-An Le, Thomas Mensink, Partha Das, Theo Gevers", "title": "Novel View Synthesis from Single Images via Point Cloud Transformation", "comments": "Accepted at British Machine Vision Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the argument is made that for true novel view synthesis of\nobjects, where the object can be synthesized from any viewpoint, an explicit 3D\nshape representation isdesired. Our method estimates point clouds to capture\nthe geometry of the object, which can be freely rotated into the desired view\nand then projected into a new image. This image, however, is sparse by nature\nand hence this coarse view is used as the input of an image completion network\nto obtain the dense target view. The point cloud is obtained using the\npredicted pixel-wise depth map, estimated from a single RGB input\nimage,combined with the camera intrinsics. By using forward warping and\nbackward warpingbetween the input view and the target view, the network can be\ntrained end-to-end without supervision on depth. The benefit of using point\nclouds as an explicit 3D shape for novel view synthesis is experimentally\nvalidated on the 3D ShapeNet benchmark. Source code and data will be available\nat https://lhoangan.github.io/pc4novis/.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 14:13:19 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 14:54:03 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Le", "Hoang-An", ""], ["Mensink", "Thomas", ""], ["Das", "Partha", ""], ["Gevers", "Theo", ""]]}, {"id": "2009.08325", "submitter": "Elahe Arani", "authors": "Fahad Sarfraz, Elahe Arani and Bahram Zonooz", "title": "Noisy Concurrent Training for Efficient Learning under Label Noise", "comments": "Accepted at IEEE Winter Conference on Applications of Computer Vision\n  (WACV, 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) fail to learn effectively under label noise and\nhave been shown to memorize random labels which affect their generalization\nperformance. We consider learning in isolation, using one-hot encoded labels as\nthe sole source of supervision, and a lack of regularization to discourage\nmemorization as the major shortcomings of the standard training procedure.\nThus, we propose Noisy Concurrent Training (NCT) which leverages collaborative\nlearning to use the consensus between two models as an additional source of\nsupervision. Furthermore, inspired by trial-to-trial variability in the brain,\nwe propose a counter-intuitive regularization technique, target variability,\nwhich entails randomly changing the labels of a percentage of training samples\nin each batch as a deterrent to memorization and over-generalization in DNNs.\nTarget variability is applied independently to each model to keep them diverged\nand avoid the confirmation bias. As DNNs tend to prioritize learning simple\npatterns first before memorizing the noisy labels, we employ a dynamic learning\nscheme whereby as the training progresses, the two models increasingly rely\nmore on their consensus. NCT also progressively increases the target\nvariability to avoid memorization in later stages. We demonstrate the\neffectiveness of our approach on both synthetic and real-world noisy benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 14:22:17 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Sarfraz", "Fahad", ""], ["Arani", "Elahe", ""], ["Zonooz", "Bahram", ""]]}, {"id": "2009.08328", "submitter": "Jeffrey Ede BSc MPhys", "authors": "Jeffrey M. Ede", "title": "Review: Deep Learning in Electron Microscopy", "comments": "33 pages, 16 figures + 2 tables + 65 pages of references", "journal-ref": null, "doi": "10.1088/2632-2153/abd614", "report-no": null, "categories": "eess.IV cond-mat.mtrl-sci cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning is transforming most areas of science and technology, including\nelectron microscopy. This review paper offers a practical perspective aimed at\ndevelopers with limited familiarity. For context, we review popular\napplications of deep learning in electron microscopy. Afterwards, we discuss\nhardware and software needed to get started with deep learning and interface\nwith electron microscopes. We then review neural network components, popular\narchitectures, and their optimization. Finally, we discuss future directions of\ndeep learning in electron microscopy.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 14:23:55 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 14:38:31 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 07:22:38 GMT"}, {"version": "v4", "created": "Tue, 3 Nov 2020 15:38:31 GMT"}, {"version": "v5", "created": "Sun, 27 Dec 2020 22:14:24 GMT"}, {"version": "v6", "created": "Thu, 31 Dec 2020 18:28:54 GMT"}, {"version": "v7", "created": "Mon, 8 Mar 2021 10:12:04 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ede", "Jeffrey M.", ""]]}, {"id": "2009.08348", "submitter": "Karsten Roth", "authors": "Karsten Roth, Timo Milbich, Bj\\\"orn Ommer, Joseph Paul Cohen, Marzyeh\n  Ghassemi", "title": "S2SD: Simultaneous Similarity-based Self-Distillation for Deep Metric\n  Learning", "comments": "Accepted to ICML2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Metric Learning (DML) provides a crucial tool for visual similarity and\nzero-shot applications by learning generalizing embedding spaces, although\nrecent work in DML has shown strong performance saturation across training\nobjectives. However, generalization capacity is known to scale with the\nembedding space dimensionality. Unfortunately, high dimensional embeddings also\ncreate higher retrieval cost for downstream applications. To remedy this, we\npropose \\emph{Simultaneous Similarity-based Self-distillation (S2SD). S2SD\nextends DML with knowledge distillation from auxiliary, high-dimensional\nembedding and feature spaces to leverage complementary context during training\nwhile retaining test-time cost and with negligible changes to the training\ntime. Experiments and ablations across different objectives and standard\nbenchmarks show S2SD offers notable improvements of up to 7% in Recall@1, while\nalso setting a new state-of-the-art. Code available at\nhttps://github.com/MLforHealth/S2SD.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 14:54:24 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 21:12:23 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 23:07:38 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Roth", "Karsten", ""], ["Milbich", "Timo", ""], ["Ommer", "Bj\u00f6rn", ""], ["Cohen", "Joseph Paul", ""], ["Ghassemi", "Marzyeh", ""]]}, {"id": "2009.08369", "submitter": "Narinder Punn", "authors": "G. Jignesh Chowdary, Narinder Singh Punn, Sanjay Kumar Sonbhadra,\n  Sonali Agarwal", "title": "Face Mask Detection using Transfer Learning of InceptionV3", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The world is facing a huge health crisis due to the rapid transmission of\ncoronavirus (COVID-19). Several guidelines were issued by the World Health\nOrganization (WHO) for protection against the spread of coronavirus. According\nto WHO, the most effective preventive measure against COVID-19 is wearing a\nmask in public places and crowded areas. It is very difficult to monitor people\nmanually in these areas. In this paper, a transfer learning model is proposed\nto automate the process of identifying the people who are not wearing mask. The\nproposed model is built by fine-tuning the pre-trained state-of-the-art deep\nlearning model, InceptionV3. The proposed model is trained and tested on the\nSimulated Masked Face Dataset (SMFD). Image augmentation technique is adopted\nto address the limited availability of data for better training and testing of\nthe model. The model outperformed the other recently proposed approaches by\nachieving an accuracy of 99.9% during training and 100% during testing.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 15:34:06 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 19:04:26 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Chowdary", "G. Jignesh", ""], ["Punn", "Narinder Singh", ""], ["Sonbhadra", "Sanjay Kumar", ""], ["Agarwal", "Sonali", ""]]}, {"id": "2009.08371", "submitter": "Nils Eckstein", "authors": "Nils Eckstein and Julia Buhmann and Matthew Cook and Jan Funke", "title": "Microtubule Tracking in Electron Microscopy Volumes", "comments": "Accepted at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for microtubule tracking in electron microscopy volumes.\nOur method first identifies a sparse set of voxels that likely belong to\nmicrotubules. Similar to prior work, we then enumerate potential edges between\nthese voxels, which we represent in a candidate graph. Tracks of microtubules\nare found by selecting nodes and edges in the candidate graph by solving a\nconstrained optimization problem incorporating biological priors on microtubule\nstructure. For this, we present a novel integer linear programming formulation,\nwhich results in speed-ups of three orders of magnitude and an increase of 53%\nin accuracy compared to prior art (evaluated on three 1.2 x 4 x 4$\\mu$m volumes\nof Drosophila neural tissue). We also propose a scheme to solve the\noptimization problem in a block-wise fashion, which allows distributed tracking\nand is necessary to process very large electron microscopy volumes. Finally, we\nrelease a benchmark dataset for microtubule tracking, here used for training,\ntesting and validation, consisting of eight 30 x 1000 x 1000 voxel blocks (1.2\nx 4 x 4$\\mu$m) of densely annotated microtubules in the CREMI data set\n(https://github.com/nilsec/micron).\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 15:37:30 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Eckstein", "Nils", ""], ["Buhmann", "Julia", ""], ["Cook", "Matthew", ""], ["Funke", "Jan", ""]]}, {"id": "2009.08373", "submitter": "Melanie Sclar", "authors": "M. Sclar, G. Bujia, S. Vita, G. Solovey, J. E. Kamienkowski", "title": "Modeling human visual search: A combined Bayesian searcher and saliency\n  map approach for eye movement guidance in natural scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding objects is essential for almost any daily-life visual task. Saliency\nmodels have been useful to predict fixation locations in natural images, but\nare static, i.e., they provide no information about the time-sequence of\nfixations. Nowadays, one of the biggest challenges in the field is to go beyond\nsaliency maps to predict a sequence of fixations related to a visual task, such\nas searching for a given target. Bayesian observer models have been proposed\nfor this task, as they represent visual search as an active sampling process.\nNevertheless, they were mostly evaluated on artificial images, and how they\nadapt to natural images remains largely unexplored.\n  Here, we propose a unified Bayesian model for visual search guided by\nsaliency maps as prior information. We validated our model with a visual search\nexperiment in natural scenes recording eye movements. We show that, although\nstate-of-the-art saliency models perform well in predicting the first two\nfixations in a visual search task, their performance degrades to chance\nafterward. This suggests that saliency maps alone are good to model bottom-up\nfirst impressions, but are not enough to explain the scanpaths when top-down\ntask information is critical. Thus, we propose to use them as priors of\nBayesian searchers. This approach leads to a behavior very similar to humans\nfor the whole scanpath, both in the percentage of target found as a function of\nthe fixation rank and the scanpath similarity, reproducing the entire sequence\nof eye movements.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 15:38:23 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 04:02:44 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Sclar", "M.", ""], ["Bujia", "G.", ""], ["Vita", "S.", ""], ["Solovey", "G.", ""], ["Kamienkowski", "J. E.", ""]]}, {"id": "2009.08395", "submitter": "Tariq Habib Afridi Mr.", "authors": "Tariq Habib Afridi, Aftab Alam, Muhammad Numan Khan, Jawad Khan,\n  Young-Koo Lee", "title": "A Multimodal Memes Classification: A Survey and Open Research Issues", "comments": "This is a survey paper on recent state of the art VL models that can\n  be used for memes classification. it has 15 pages and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memes are graphics and text overlapped so that together they present concepts\nthat become dubious if one of them is absent. It is spread mostly on social\nmedia platforms, in the form of jokes, sarcasm, motivating, etc. After the\nsuccess of BERT in Natural Language Processing (NLP), researchers inclined to\nVisual-Linguistic (VL) multimodal problems like memes classification, image\ncaptioning, Visual Question Answering (VQA), and many more. Unfortunately, many\nmemes get uploaded each day on social media platforms that need automatic\ncensoring to curb misinformation and hate. Recently, this issue has attracted\nthe attention of researchers and practitioners. State-of-the-art methods that\nperformed significantly on other VL dataset, tends to fail on memes\nclassification. In this context, this work aims to conduct a comprehensive\nstudy on memes classification, generally on the VL multimodal problems and\ncutting edge solutions. We propose a generalized framework for VL problems. We\ncover the early and next-generation works on VL problems. Finally, we identify\nand articulate several open research issues and challenges. This is the first\nstudy that presents the generalized view of the advanced classification\ntechniques concerning memes classification to the best of our knowledge. We\nbelieve this study presents a clear road-map for the Machine Learning (ML)\nresearch community to implement and enhance memes classification techniques.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 16:13:21 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Afridi", "Tariq Habib", ""], ["Alam", "Aftab", ""], ["Khan", "Muhammad Numan", ""], ["Khan", "Jawad", ""], ["Lee", "Young-Koo", ""]]}, {"id": "2009.08410", "submitter": "Konstantin Klemmer", "authors": "Konstantin Klemmer, Godwin Yeboah, Jo\\~ao Porto de Albuquerque,\n  Stephen A Jarvis", "title": "Population Mapping in Informal Settlements with High-Resolution\n  Satellite Imagery and Equitable Ground-Truth", "comments": "ML-IRL workshop at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generalizable framework for the population estimation of dense,\ninformal settlements in low-income urban areas--so called 'slums'--using\nhigh-resolution satellite imagery. Precise population estimates are a crucial\nfactor for efficient resource allocations by government authorities and NGO's,\nfor instance in medical emergencies. We utilize equitable ground-truth data,\nwhich is gathered in collaboration with local communities: Through training and\ncommunity mapping, the local population contributes their unique domain\nknowledge, while also maintaining agency over their data. This practice allows\nus to avoid carrying forward potential biases into the modeling pipeline, which\nmight arise from a less rigorous ground-truthing approach. We contextualize our\napproach in respect to the ongoing discussion within the machine learning\ncommunity, aiming to make real-world machine learning applications more\ninclusive, fair and accountable. Because of the resource intensive ground-truth\ngeneration process, our training data is limited. We propose a gridded\npopulation estimation model, enabling flexible and customizable spatial\nresolutions. We test our pipeline on three experimental site in Nigeria,\nutilizing pre-trained and fine-tune vision networks to overcome data sparsity.\nOur findings highlight the difficulties of transferring common benchmark models\nto real-world tasks. We discuss this and propose steps forward.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 16:37:32 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Klemmer", "Konstantin", ""], ["Yeboah", "Godwin", ""], ["de Albuquerque", "Jo\u00e3o Porto", ""], ["Jarvis", "Stephen A", ""]]}, {"id": "2009.08427", "submitter": "Andrei Nicolicioiu", "authors": "Iulia Duta and Andrei Nicolicioiu and Marius Leordeanu", "title": "Discovering Dynamic Salient Regions with Spatio-Temporal Graph Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks are perfectly suited to capture latent interactions\nbetween various entities in the spatio-temporal domain (e.g. videos). However,\nwhen an explicit structure is not available, it is not obvious what atomic\nelements should be represented as nodes. Current works generally use\npre-trained object detectors or fixed, predefined regions to extract graph\nnodes. In turn, our proposed model learns nodes that dynamically attach to\nsalient space-time regions, which are relevant for a higher-level task, without\nusing any object-level supervision. Constructing these localised, adaptive\nnodes gives our model inductive bias towards object-centric representations and\nwe show that it discovers regions that are well correlated with objects in the\nvideo. The localised nodes are the key components of the method and visualising\ntheir regions leads to a more explainable model. In extensive ablation studies\nand experiments on two challenging datasets we show superior performance to\nprevious graph neural networks models for video classification.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 17:23:38 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 13:00:34 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Duta", "Iulia", ""], ["Nicolicioiu", "Andrei", ""], ["Leordeanu", "Marius", ""]]}, {"id": "2009.08428", "submitter": "Ramin Nabati", "authors": "Ramin Nabati, Hairong Qi", "title": "Radar-Camera Sensor Fusion for Joint Object Detection and Distance\n  Estimation in Autonomous Vehicles", "comments": "12th Workshop on Planning, Perception and Navigation for Intelligent\n  Vehicles, IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel radar-camera sensor fusion framework for\naccurate object detection and distance estimation in autonomous driving\nscenarios. The proposed architecture uses a middle-fusion approach to fuse the\nradar point clouds and RGB images. Our radar object proposal network uses radar\npoint clouds to generate 3D proposals from a set of 3D prior boxes. These\nproposals are mapped to the image and fed into a Radar Proposal Refinement\n(RPR) network for objectness score prediction and box refinement. The RPR\nnetwork utilizes both radar information and image feature maps to generate\naccurate object proposals and distance estimations. The radar-based proposals\nare combined with image-based proposals generated by a modified Region Proposal\nNetwork (RPN). The RPN has a distance regression layer for estimating distance\nfor every generated proposal. The radar-based and image-based proposals are\nmerged and used in the next stage for object classification. Experiments on the\nchallenging nuScenes dataset show our method outperforms other existing\nradar-camera fusion methods in the 2D object detection task while at the same\ntime accurately estimates objects' distances.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 17:23:40 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Nabati", "Ramin", ""], ["Qi", "Hairong", ""]]}, {"id": "2009.08435", "submitter": "Youwei Liang", "authors": "Youwei Liang, Dong Huang", "title": "Large Norms of CNN Layers Do Not Hurt Adversarial Robustness", "comments": "15 pages, 4 figures; v5: corrected typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the Lipschitz properties of CNN are widely considered to be related to\nadversarial robustness, we theoretically characterize the $\\ell_1$ norm and\n$\\ell_\\infty$ norm of 2D multi-channel convolutional layers and provide\nefficient methods to compute the exact $\\ell_1$ norm and $\\ell_\\infty$ norm.\nBased on our theorem, we propose a novel regularization method termed norm\ndecay, which can effectively reduce the norms of convolutional layers and\nfully-connected layers. Experiments show that norm-regularization methods,\nincluding norm decay, weight decay, and singular value clipping, can improve\ngeneralization of CNNs. However, they can slightly hurt adversarial robustness.\nObserving this unexpected phenomenon, we compute the norms of layers in the\nCNNs trained with three different adversarial training frameworks and\nsurprisingly find that adversarially robust CNNs have comparable or even larger\nlayer norms than their non-adversarially robust counterparts. Furthermore, we\nprove that under a mild assumption, adversarially robust classifiers can be\nachieved, and can have an arbitrarily large Lipschitz constant. For this\nreason, enforcing small norms on CNN layers may be neither necessary nor\neffective in achieving adversarial robustness. The code is available at\nhttps://github.com/youweiliang/norm_robustness.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 17:33:50 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 06:28:59 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 07:34:27 GMT"}, {"version": "v4", "created": "Tue, 29 Dec 2020 02:58:32 GMT"}, {"version": "v5", "created": "Thu, 10 Jun 2021 06:21:01 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Liang", "Youwei", ""], ["Huang", "Dong", ""]]}, {"id": "2009.08443", "submitter": "Joscha Diehl", "authors": "Joscha Diehl, Kurusch Ebrahimi-Fard, Nikolas Tapia", "title": "Tropical time series, iterated-sums signatures and quasisymmetric\n  functions", "comments": "fix notational errors, clarify certain proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.RA cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by the need for principled extraction of features from time series, we\nintroduce the iterated-sums signature over any commutative semiring. The case\nof the tropical semiring is a central, and our motivating, example, as it leads\nto features of (real-valued) time series that are not easily available using\nexisting signature-type objects.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 17:51:43 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 21:36:52 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Diehl", "Joscha", ""], ["Ebrahimi-Fard", "Kurusch", ""], ["Tapia", "Nikolas", ""]]}, {"id": "2009.08453", "submitter": "Zhiqiang Shen", "authors": "Zhiqiang Shen and Marios Savvides", "title": "MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet\n  without Tricks", "comments": "12 pages. Code and trained models are available at:\n  https://github.com/szq0214/MEAL-V2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple yet effective distillation framework that is able to\nboost the vanilla ResNet-50 to 80%+ Top-1 accuracy on ImageNet without tricks.\nWe construct such a framework through analyzing the problems in the existing\nclassification system and simplify the base method ensemble knowledge\ndistillation via discriminators by: (1) adopting the similarity loss and\ndiscriminator only on the final outputs and (2) using the average of softmax\nprobabilities from all teacher ensembles as the stronger supervision.\nIntriguingly, three novel perspectives are presented for distillation: (1)\nweight decay can be weakened or even completely removed since the soft label\nalso has a regularization effect; (2) using a good initialization for students\nis critical; and (3) one-hot/hard label is not necessary in the distillation\nprocess if the weights are well initialized. We show that such a\nstraight-forward framework can achieve state-of-the-art results without\ninvolving any commonly-used techniques, such as architecture modification;\noutside training data beyond ImageNet; autoaug/randaug; cosine learning rate;\nmixup/cutmix training; label smoothing; etc. Our method obtains 80.67% top-1\naccuracy on ImageNet using a single crop-size of 224x224 with vanilla\nResNet-50, outperforming the previous state-of-the-arts by a significant margin\nunder the same network structure. Our result can be regarded as a strong\nbaseline using knowledge distillation, and to our best knowledge, this is also\nthe first method that is able to boost vanilla ResNet-50 to surpass 80% on\nImageNet without architecture modification or additional training data. On\nsmaller ResNet-18, our distillation framework consistently improves from 69.76%\nto 73.19%, which shows tremendous practical values in real-world applications.\nOur code and models are available at: https://github.com/szq0214/MEAL-V2.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 17:59:33 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 17:40:19 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Shen", "Zhiqiang", ""], ["Savvides", "Marios", ""]]}, {"id": "2009.08497", "submitter": "Tarek Richard Besold", "authors": "Lorijn Zaadnoordijk, Tarek R. Besold, Rhodri Cusack", "title": "The Next Big Thing(s) in Unsupervised Machine Learning: Five Lessons\n  from Infant Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After a surge in popularity of supervised Deep Learning, the desire to reduce\nthe dependence on curated, labelled data sets and to leverage the vast\nquantities of unlabelled data available recently triggered renewed interest in\nunsupervised learning algorithms. Despite a significantly improved performance\ndue to approaches such as the identification of disentangled latent\nrepresentations, contrastive learning, and clustering optimisations, the\nperformance of unsupervised machine learning still falls short of its\nhypothesised potential. Machine learning has previously taken inspiration from\nneuroscience and cognitive science with great success. However, this has mostly\nbeen based on adult learners with access to labels and a vast amount of prior\nknowledge. In order to push unsupervised machine learning forward, we argue\nthat developmental science of infant cognition might hold the key to unlocking\nthe next generation of unsupervised learning approaches. Conceptually, human\ninfant learning is the closest biological parallel to artificial unsupervised\nlearning, as infants too must learn useful representations from unlabelled\ndata. In contrast to machine learning, these new representations are learned\nrapidly and from relatively few examples. Moreover, infants learn robust\nrepresentations that can be used flexibly and efficiently in a number of\ndifferent tasks and contexts. We identify five crucial factors enabling\ninfants' quality and speed of learning, assess the extent to which these have\nalready been exploited in machine learning, and propose how further adoption of\nthese factors can give rise to previously unseen performance levels in\nunsupervised learning.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 18:47:06 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Zaadnoordijk", "Lorijn", ""], ["Besold", "Tarek R.", ""], ["Cusack", "Rhodri", ""]]}, {"id": "2009.08511", "submitter": "Sudipta Banerjee", "authors": "Sudipta Banerjee and Arun Ross", "title": "Smartphone Camera De-identification while Preserving Biometric Utility", "comments": null, "journal-ref": "Proc. of 10th IEEE International Conference on Biometrics: Theory,\n  Applications and Systems (BTAS), (Tampa, USA), September 2019", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The principle of Photo Response Non Uniformity (PRNU) is often exploited to\ndeduce the identity of the smartphone device whose camera or sensor was used to\nacquire a certain image. In this work, we design an algorithm that perturbs a\nface image acquired using a smartphone camera such that (a) sensor-specific\ndetails pertaining to the smartphone camera are suppressed (sensor\nanonymization); (b) the sensor pattern of a different device is incorporated\n(sensor spoofing); and (c) biometric matching using the perturbed image is not\naffected (biometric utility). We employ a simple approach utilizing Discrete\nCosine Transform to achieve the aforementioned objectives. Experiments\nconducted on the MICHE-I and OULU-NPU datasets, which contain periocular and\nfacial data acquired using 12 smartphone cameras, demonstrate the efficacy of\nthe proposed de-identification algorithm on three different PRNU-based sensor\nidentification schemes. This work has application in sensor forensics and\npersonal privacy.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 19:48:43 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Banerjee", "Sudipta", ""], ["Ross", "Arun", ""]]}, {"id": "2009.08539", "submitter": "Peter Moeck", "authors": "Andrew Dempsey and Peter Moeck", "title": "Objective, Probabilistic, and Generalized Noise Level Dependent\n  Classifications of sets of more or less 2D Periodic Images into Plane\n  Symmetry Groups", "comments": "74 pages, 12 figures, 56 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crystallographic symmetry classifications from real-world images with\nperiodicities in two dimensions (2D) are of interest to crystallographers and\npractitioners of computer vision studies alike. Currently, these\nclassifications are typically made by both communities in a subjective manner\nthat relies on arbitrary thresholds for judgments, and are reported under the\npretense of being definitive, which is impossible. Moreover, the computer\nvision community tends to use direct space methods to make such classifications\ninstead of more powerful and computationally efficient Fourier space methods.\nThis is because the proper functioning of those methods requires more periodic\nrepeats of a unit cell motif than are commonly present in images analyzed by\nthe computer vision community. We demonstrate a novel approach to plane\nsymmetry group classifications that is enabled by Kenichi Kanatani's Geometric\nAkaike Information Criterion and associated Geometric Akaike weights. Our\napproach leverages the advantages of working in Fourier space, is well suited\nfor handling the hierarchic nature of crystallographic symmetries, and yields\nprobabilistic results that are generalized noise level dependent. The latter\nfeature means crystallographic symmetry classifications can be updated when\nless noisy image data and more accurate processing algorithms become available.\nWe demonstrate the ability of our approach to objectively estimate the plane\nsymmetry and pseudosymmetries of sets of synthetic 2D-periodic images with\nvarying amounts of red-green-blue and spread noise. Additionally, we suggest a\nsimple solution to the problem of too few periodic repeats in an input image\nfor practical application of Fourier space methods. In doing so, we effectively\nsolve the decades-old and heretofore intractable problem from computer vision\nof symmetry detection and classification from images in the presence of noise.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 21:28:49 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 02:54:51 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Dempsey", "Andrew", ""], ["Moeck", "Peter", ""]]}, {"id": "2009.08563", "submitter": "Saeed Seyyedi", "authors": "Saeed Seyyedi, Margaret J. Wong, Debra M. Ikeda, Curtis P. Langlotz", "title": "SCREENet: A Multi-view Deep Convolutional Neural Network for\n  Classification of High-resolution Synthetic Mammographic Screening Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To develop and evaluate the accuracy of a multi-view deep learning\napproach to the analysis of high-resolution synthetic mammograms from digital\nbreast tomosynthesis screening cases, and to assess the effect on accuracy of\nimage resolution and training set size. Materials and Methods: In a\nretrospective study, 21,264 screening digital breast tomosynthesis (DBT) exams\nobtained at our institution were collected along with associated radiology\nreports. The 2D synthetic mammographic images from these exams, with varying\nresolutions and data set sizes, were used to train a multi-view deep\nconvolutional neural network (MV-CNN) to classify screening images into BI-RADS\nclasses (BI-RADS 0, 1 and 2) before evaluation on a held-out set of exams.\n  Results: Area under the receiver operating characteristic curve (AUC) for\nBI-RADS 0 vs non-BI-RADS 0 class was 0.912 for the MV-CNN trained on the full\ndataset. The model obtained accuracy of 84.8%, recall of 95.9% and precision of\n95.0%. This AUC value decreased when the same model was trained with 50% and\n25% of images (AUC = 0.877, P=0.010 and 0.834, P=0.009 respectively). Also, the\nperformance dropped when the same model was trained using images that were\nunder-sampled by 1/2 and 1/4 (AUC = 0.870, P=0.011 and 0.813, P=0.009\nrespectively).\n  Conclusion: This deep learning model classified high-resolution synthetic\nmammography scans into normal vs needing further workup using tens of thousands\nof high-resolution images. Smaller training data sets and lower resolution\nimages both caused significant decrease in performance.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 00:12:33 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 23:05:47 GMT"}, {"version": "v3", "created": "Fri, 25 Sep 2020 19:36:05 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Seyyedi", "Saeed", ""], ["Wong", "Margaret J.", ""], ["Ikeda", "Debra M.", ""], ["Langlotz", "Curtis P.", ""]]}, {"id": "2009.08566", "submitter": "Tejas Gokhale", "authors": "Tejas Gokhale and Pratyay Banerjee and Chitta Baral and Yezhou Yang", "title": "MUTANT: A Training Paradigm for Out-of-Distribution Generalization in\n  Visual Question Answering", "comments": "Accepted to EMNLP 2020, Long Papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While progress has been made on the visual question answering leaderboards,\nmodels often utilize spurious correlations and priors in datasets under the\ni.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples\nhas emerged as a proxy for generalization. In this paper, we present MUTANT, a\ntraining paradigm that exposes the model to perceptually similar, yet\nsemantically distinct mutations of the input, to improve OOD generalization,\nsuch as the VQA-CP challenge. Under this paradigm, models utilize a\nconsistency-constrained training objective to understand the effect of semantic\nchanges in input (question-image pair) on the output (answer). Unlike existing\nmethods on VQA-CP, MUTANT does not rely on the knowledge about the nature of\ntrain and test answer distributions. MUTANT establishes a new state-of-the-art\naccuracy on VQA-CP with a $10.57\\%$ improvement. Our work opens up avenues for\nthe use of semantic input mutations for OOD generalization in question\nanswering.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 00:22:54 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 01:53:08 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Gokhale", "Tejas", ""], ["Banerjee", "Pratyay", ""], ["Baral", "Chitta", ""], ["Yang", "Yezhou", ""]]}, {"id": "2009.08576", "submitter": "Jonathan Frankle", "authors": "Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, Michael\n  Carbin", "title": "Pruning Neural Networks at Initialization: Why are We Missing the Mark?", "comments": "Published in ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has explored the possibility of pruning neural networks at\ninitialization. We assess proposals for doing so: SNIP (Lee et al., 2019),\nGraSP (Wang et al., 2020), SynFlow (Tanaka et al., 2020), and magnitude\npruning. Although these methods surpass the trivial baseline of random pruning,\nthey remain below the accuracy of magnitude pruning after training, and we\nendeavor to understand why. We show that, unlike pruning after training,\nrandomly shuffling the weights these methods prune within each layer or\nsampling new initial values preserves or improves accuracy. As such, the\nper-weight pruning decisions made by these methods can be replaced by a\nper-layer choice of the fraction of weights to prune. This property suggests\nbroader challenges with the underlying pruning heuristics, the desire to prune\nat initialization, or both.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 01:13:38 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 21:38:32 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Frankle", "Jonathan", ""], ["Dziugaite", "Gintare Karolina", ""], ["Roy", "Daniel M.", ""], ["Carbin", "Michael", ""]]}, {"id": "2009.08591", "submitter": "Zhenyu Weng", "authors": "Zhenyu Weng, Yuesheng Zhu, Ruixin Liu", "title": "Accelerating Search on Binary Codes in Weighted Hamming Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to Hamming distance, weighted Hamming distance as a similarity\nmeasure between binary codes and the binary query point can provide superior\naccuracy in the search tasks. However, how to efficiently find $K$ binary codes\nin the dataset that have the smallest weighted Hamming distance with the query\nis still an open issue. In this paper, a non-exhaustive search framework is\nproposed to accelerate the search speed and guarantee the search accuracy on\nthe binary codes in weighted Hamming space. By separating the binary codes into\nmultiple disjoint substrings as the bucket indices, the search framework\niteratively probes the buckets until the query's nearest neighbors are found.\nThe framework consists of two modules, the search module and the decision\nmodule. The search module successively probes the buckets and takes the\ncandidates according to a proper probing sequence generated by the proposed\nsearch algorithm. And the decision module decides whether the query's nearest\nneighbors are found or more buckets should be probed according to a designed\ndecision criterion. The analysis and experiments indicate that the search\nframework can solve the nearest neighbor search problem in weighted Hamming\nspace and is orders of magnitude faster than the linear scan baseline.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 02:24:44 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Weng", "Zhenyu", ""], ["Zhu", "Yuesheng", ""], ["Liu", "Ruixin", ""]]}, {"id": "2009.08610", "submitter": "Chenhongyi Yang", "authors": "Kaihong Wang, Chenhongyi Yang, Margrit Betke", "title": "Consistency Regularization with High-dimensional Non-adversarial\n  Source-guided Perturbation for Unsupervised Domain Adaptation in Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation for semantic segmentation has been intensively\nstudied due to the low cost of the pixel-level annotation for synthetic data.\nThe most common approaches try to generate images or features mimicking the\ndistribution in the target domain while preserving the semantic contents in the\nsource domain so that a model can be trained with annotations from the latter.\nHowever, such methods highly rely on an image translator or feature extractor\ntrained in an elaborated mechanism including adversarial training, which brings\nin extra complexity and instability in the adaptation process. Furthermore,\nthese methods mainly focus on taking advantage of the labeled source dataset,\nleaving the unlabeled target dataset not fully utilized. In this paper, we\npropose a bidirectional style-induced domain adaptation method, called BiSIDA,\nthat employs consistency regularization to efficiently exploit information from\nthe unlabeled target domain dataset, requiring only a simple neural style\ntransfer model. BiSIDA aligns domains by not only transferring source images\ninto the style of target images but also transferring target images into the\nstyle of source images to perform high-dimensional perturbation on the\nunlabeled target images, which is crucial to the success in applying\nconsistency regularization in segmentation tasks. Extensive experiments show\nthat our BiSIDA achieves new state-of-the-art on two commonly-used\nsynthetic-to-real domain adaptation benchmarks: GTA5-to-CityScapes and\nSYNTHIA-to-CityScapes.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 03:26:44 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Wang", "Kaihong", ""], ["Yang", "Chenhongyi", ""], ["Betke", "Margrit", ""]]}, {"id": "2009.08614", "submitter": "Guanbin Li", "authors": "Jie Wu, Guanbin Li, Xiaoguang Han, Liang Lin", "title": "Reinforcement Learning for Weakly Supervised Temporal Grounding of\n  Natural Language in Untrimmed Videos", "comments": "Accepted by ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal grounding of natural language in untrimmed videos is a fundamental\nyet challenging multimedia task facilitating cross-media visual content\nretrieval. We focus on the weakly supervised setting of this task that merely\naccesses to coarse video-level language description annotation without temporal\nboundary, which is more consistent with reality as such weak labels are more\nreadily available in practice. In this paper, we propose a \\emph{Boundary\nAdaptive Refinement} (BAR) framework that resorts to reinforcement learning\n(RL) to guide the process of progressively refining the temporal boundary. To\nthe best of our knowledge, we offer the first attempt to extend RL to temporal\nlocalization task with weak supervision. As it is non-trivial to obtain a\nstraightforward reward function in the absence of pairwise granular\nboundary-query annotations, a cross-modal alignment evaluator is crafted to\nmeasure the alignment degree of segment-query pair to provide tailor-designed\nrewards. This refinement scheme completely abandons traditional sliding window\nbased solution pattern and contributes to acquiring more efficient,\nboundary-flexible and content-aware grounding results. Extensive experiments on\ntwo public benchmarks Charades-STA and ActivityNet demonstrate that BAR\noutperforms the state-of-the-art weakly-supervised method and even beats some\ncompetitive fully-supervised ones.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 03:32:47 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Wu", "Jie", ""], ["Li", "Guanbin", ""], ["Han", "Xiaoguang", ""], ["Lin", "Liang", ""]]}, {"id": "2009.08618", "submitter": "Harry Zhang Mr.", "authors": "Yahav Avigal, Samuel Paradis, Harry Zhang", "title": "6-DoF Grasp Planning using Fast 3D Reconstruction and Grasp Quality CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent consumer demand for home robots has accelerated performance of robotic\ngrasping. However, a key component of the perception pipeline, the depth\ncamera, is still expensive and inaccessible to most consumers. In addition,\ngrasp planning has significantly improved recently, by leveraging large\ndatasets and cloud robotics, and by limiting the state and action space to\ntop-down grasps with 4 degrees of freedom (DoF). By leveraging multi-view\ngeometry of the object using inexpensive equipment such as off-the-shelf RGB\ncameras and state-of-the-art algorithms such as Learn Stereo Machine\n(LSM\\cite{kar2017learning}), the robot is able to generate more robust grasps\nfrom different angles with 6-DoF. In this paper, we present a modification of\nLSM to graspable objects, evaluate the grasps, and develop a 6-DoF grasp\nplanner based on Grasp-Quality CNN (GQ-CNN\\cite{mahler2017dex}) that exploits\nmultiple camera views to plan a robust grasp, even in the absence of a possible\ntop-down grasp.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 03:53:18 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Avigal", "Yahav", ""], ["Paradis", "Samuel", ""], ["Zhang", "Harry", ""]]}, {"id": "2009.08626", "submitter": "Taeyeong Choi", "authors": "Taeyeong Choi, Benjamin Pyenson, Juergen Liebig, Theodore P. Pavlic", "title": "Identification of Abnormal States in Videos of Ants Undergoing Social\n  Phase Change", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biology is both an important application area and a source of motivation for\ndevelopment of advanced machine learning techniques. Although much attention\nhas been paid to large and complex data sets resulting from high-throughput\nsequencing, advances in high-quality video recording technology have begun to\ngenerate similarly rich data sets requiring sophisticated techniques from both\ncomputer vision and time-series analysis. Moreover, just as studying gene\nexpression patterns in one organism can reveal general principles that apply to\nother organisms, the study of complex social interactions in an experimentally\ntractable model system, such as a laboratory ant colony, can provide general\nprinciples about the dynamics many other social groups. Here, we focus on one\nsuch example from the study of reproductive regulation in small laboratory\ncolonies of $\\sim$50 Harpgenathos ants. These ants can be artificially induced\nto begin a $\\sim$20 day process of hierarchy reformation. Although the\nconclusion of this process is conspicuous to a human observer, it is still\nunclear which behaviors during the transients are contributing to the process.\nTo address this issue, we explore the potential application of One-class\nClassification (OC) to the detection of abnormal states in ant colonies for\nwhich behavioral data is only available for the normal societal conditions\nduring training. Specifically, we build upon the Deep Support Vector Data\nDescription (DSVDD) and introduce the Inner-Outlier Generator (IO-GEN) that\nsynthesizes fake \"inner outlier\" observations during training that are near the\ncenter of the DSVDD data description. We show that IO-GEN increases the\nreliability of the final OC classifier relative to other DSVDD baselines. This\nmethod can be used to screen video frames for which additional human\nobservation is needed.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 04:48:47 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Choi", "Taeyeong", ""], ["Pyenson", "Benjamin", ""], ["Liebig", "Juergen", ""], ["Pavlic", "Theodore P.", ""]]}, {"id": "2009.08650", "submitter": "Feras Dayoub", "authors": "Quazi Marufur Rahman and Niko S\\\"underhauf and Feras Dayoub", "title": "Per-frame mAP Prediction for Continuous Performance Monitoring of Object\n  Detection During Deployment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance monitoring of object detection is crucial for safety-critical\napplications such as autonomous vehicles that operate under varying and complex\nenvironmental conditions. Currently, object detectors are evaluated using\nsummary metrics based on a single dataset that is assumed to be representative\nof all future deployment conditions. In practice, this assumption does not\nhold, and the performance fluctuates as a function of the deployment\nconditions. To address this issue, we propose an introspection approach to\nperformance monitoring during deployment without the need for ground truth\ndata. We do so by predicting when the per-frame mean average precision drops\nbelow a critical threshold using the detector's internal features. We\nquantitatively evaluate and demonstrate our method's ability to reduce risk by\ntrading off making an incorrect decision by raising the alarm and absenting\nfrom detection.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 06:37:52 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 07:11:29 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Rahman", "Quazi Marufur", ""], ["S\u00fcnderhauf", "Niko", ""], ["Dayoub", "Feras", ""]]}, {"id": "2009.08664", "submitter": "Stefan Reinhold", "authors": "Stefan Reinhold, Timo Damm, Sebastian B\\\"usse, Stanislav N. Gorb,\n  Claus-C. Gl\\\"uer, Reinhard Koch", "title": "An Analysis by Synthesis Method that Allows Accurate Spatial Modeling of\n  Thickness of Cortical Bone from Clinical QCT", "comments": "Accepted for publication in MICCAI 2020 conference", "journal-ref": null, "doi": "10.1007/978-3-030-59725-2_62", "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Osteoporosis is a skeletal disorder that leads to increased fracture risk due\nto decreased strength of cortical and trabecular bone. Even with\nstate-of-the-art non-invasive assessment methods there is still a high\nunderdiagnosis rate. Quantitative computed tomography (QCT) permits the\nselective analysis of cortical bone, however the low spatial resolution of\nclinical QCT leads to an overestimation of the thickness of cortical bone\n(Ct.Th) and bone strength.\n  We propose a novel, model based, fully automatic image analysis method that\nallows accurate spatial modeling of the thickness distribution of cortical bone\nfrom clinical QCT. In an analysis-by-synthesis (AbS) fashion a stochastic scan\nis synthesized from a probabilistic bone model, the optimal model parameters\nare estimated using a maximum a-posteriori approach. By exploiting the\ndifferent characteristics of in-plane and out-of-plane point spread functions\nof CT scanners the proposed method is able assess the spatial distribution of\ncortical thickness.\n  The method was evaluated on eleven cadaveric human vertebrae, scanned by\nclinical QCT and analyzed using standard methods and AbS, both compared to high\nresolution peripheral QCT (HR-pQCT) as gold standard. While standard QCT based\nmeasurements overestimated Ct.Th. by 560% and did not show significant\ncorrelation with the gold standard ($r^2 = 0.20,\\, p = 0.169$) the proposed\nmethod eliminated the overestimation and showed a significant tight correlation\nwith the gold standard ($r^2 = 0.98,\\, p < 0.0001$) a root mean square error\nbelow 10%.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 07:30:18 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Reinhold", "Stefan", ""], ["Damm", "Timo", ""], ["B\u00fcsse", "Sebastian", ""], ["Gorb", "Stanislav N.", ""], ["Gl\u00fcer", "Claus-C.", ""], ["Koch", "Reinhard", ""]]}, {"id": "2009.08674", "submitter": "Deepak Keshwani", "authors": "Deepak Keshwani, Yoshiro Kitamura, Satoshi Ihara, Satoshi Iizuka,\n  Edgar Simo-Serra", "title": "TopNet: Topology Preserving Metric Learning for Vessel Tree\n  Reconstruction and Labelling", "comments": "Accepted in MICCAI 2020", "journal-ref": null, "doi": null, "report-no": "603", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Reconstructing Portal Vein and Hepatic Vein trees from contrast enhanced\nabdominal CT scans is a prerequisite for preoperative liver surgery simulation.\nExisting deep learning based methods treat vascular tree reconstruction as a\nsemantic segmentation problem. However, vessels such as hepatic and portal vein\nlook very similar locally and need to be traced to their source for robust\nlabel assignment. Therefore, semantic segmentation by looking at local 3D patch\nresults in noisy misclassifications. To tackle this, we propose a novel\nmulti-task deep learning architecture for vessel tree reconstruction. The\nnetwork architecture simultaneously solves the task of detecting voxels on\nvascular centerlines (i.e. nodes) and estimates connectivity between\ncenter-voxels (edges) in the tree structure to be reconstructed. Further, we\npropose a novel connectivity metric which considers both inter-class distance\nand intra-class topological distance between center-voxel pairs. Vascular trees\nare reconstructed starting from the vessel source using the learned\nconnectivity metric using the shortest path tree algorithm. A thorough\nevaluation on public IRCAD dataset shows that the proposed method considerably\noutperforms existing semantic segmentation based methods. To the best of our\nknowledge, this is the first deep learning based approach which learns\nmulti-label tree structure connectivity from images.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 07:55:58 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Keshwani", "Deepak", ""], ["Kitamura", "Yoshiro", ""], ["Ihara", "Satoshi", ""], ["Iizuka", "Satoshi", ""], ["Simo-Serra", "Edgar", ""]]}, {"id": "2009.08679", "submitter": "Chaofeng Chen", "authors": "Chaofeng Chen, Xiao Tan, and Kwan-Yee K. Wong", "title": "Face Sketch Synthesis with Style Transfer using Pyramid Column Feature", "comments": "WACV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a novel framework based on deep neural networks for\nface sketch synthesis from a photo. Imitating the process of how artists draw\nsketches, our framework synthesizes face sketches in a cascaded manner. A\ncontent image is first generated that outlines the shape of the face and the\nkey facial features. Textures and shadings are then added to enrich the details\nof the sketch. We utilize a fully convolutional neural network (FCNN) to create\nthe content image, and propose a style transfer approach to introduce textures\nand shadings based on a newly proposed pyramid column feature. We demonstrate\nthat our style transfer approach based on the pyramid column feature can not\nonly preserve more sketch details than the common style transfer method, but\nalso surpasses traditional patch based methods. Quantitative and qualitative\nevaluations suggest that our framework outperforms other state-of-the-arts\nmethods, and can also generalize well to different test images. Codes are\navailable at https://github.com/chaofengc/Face-Sketch\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 08:15:55 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Chen", "Chaofeng", ""], ["Tan", "Xiao", ""], ["Wong", "Kwan-Yee K.", ""]]}, {"id": "2009.08688", "submitter": "XiangRui Xu", "authors": "Xiangrui Xu, Yaqin Li, Cao Yuan", "title": "Conditional Image Generation with One-Vs-All Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores conditional image generation with a One-Vs-All classifier\nbased on the Generative Adversarial Networks (GANs). Instead of the real/fake\ndiscriminator used in vanilla GANs, we propose to extend the discriminator to a\nOne-Vs-All classifier (GAN-OVA) that can distinguish each input data to its\ncategory label. Specifically, we feed certain additional information as\nconditions to the generator and take the discriminator as a One-Vs-All\nclassifier to identify each conditional category. Our model can be applied to\ndifferent divergence or distances used to define the objective function, such\nas Jensen-Shannon divergence and Earth-Mover (or called Wasserstein-1)\ndistance. We evaluate GAN-OVAs on MNIST and CelebA-HQ datasets, and the\nexperimental results show that GAN-OVAs make progress toward stable training\nover regular conditional GANs. Furthermore, GAN-OVAs effectively accelerate the\ngeneration process of different classes and improves generation quality.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 08:41:27 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Xu", "Xiangrui", ""], ["Li", "Yaqin", ""], ["Yuan", "Cao", ""]]}, {"id": "2009.08692", "submitter": "Satoshi Iizuka", "authors": "Satoshi Iizuka and Edgar Simo-Serra", "title": "DeepRemaster: Temporal Source-Reference Attention Networks for\n  Comprehensive Video Enhancement", "comments": "Accepted to SIGGRAPH Asia 2019. Project page:\n  http://iizuka.cs.tsukuba.ac.jp/projects/remastering/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The remastering of vintage film comprises of a diversity of sub-tasks\nincluding super-resolution, noise removal, and contrast enhancement which aim\nto restore the deteriorated film medium to its original state. Additionally,\ndue to the technical limitations of the time, most vintage film is either\nrecorded in black and white, or has low quality colors, for which colorization\nbecomes necessary. In this work, we propose a single framework to tackle the\nentire remastering task semi-interactively. Our work is based on temporal\nconvolutional neural networks with attention mechanisms trained on videos with\ndata-driven deterioration simulation. Our proposed source-reference attention\nallows the model to handle an arbitrary number of reference color images to\ncolorize long videos without the need for segmentation while maintaining\ntemporal consistency. Quantitative analysis shows that our framework\noutperforms existing approaches, and that, in contrast to existing approaches,\nthe performance of our framework increases with longer videos and more\nreference color images.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 08:55:11 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Iizuka", "Satoshi", ""], ["Simo-Serra", "Edgar", ""]]}, {"id": "2009.08695", "submitter": "Zhaohui Yang", "authors": "Zhaohui Yang, Yunhe Wang, Kai Han, Chunjing Xu, Chao Xu, Dacheng Tao,\n  Chang Xu", "title": "Searching for Low-Bit Weights in Quantized Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantized neural networks with low-bit weights and activations are attractive\nfor developing AI accelerators. However, the quantization functions used in\nmost conventional quantization methods are non-differentiable, which increases\nthe optimization difficulty of quantized networks. Compared with full-precision\nparameters (i.e., 32-bit floating numbers), low-bit values are selected from a\nmuch smaller set. For example, there are only 16 possibilities in 4-bit space.\nThus, we present to regard the discrete weights in an arbitrary quantized\nneural network as searchable variables, and utilize a differential method to\nsearch them accurately. In particular, each weight is represented as a\nprobability distribution over the discrete value set. The probabilities are\noptimized during training and the values with the highest probability are\nselected to establish the desired quantized network. Experimental results on\nbenchmarks demonstrate that the proposed method is able to produce quantized\nneural networks with higher performance over the state-of-the-art methods on\nboth image classification and super-resolution tasks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 09:13:26 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Yang", "Zhaohui", ""], ["Wang", "Yunhe", ""], ["Han", "Kai", ""], ["Xu", "Chunjing", ""], ["Xu", "Chao", ""], ["Tao", "Dacheng", ""], ["Xu", "Chang", ""]]}, {"id": "2009.08704", "submitter": "Aythami Morales", "authors": "Alejandro Pe\\~na and Julian Fierrez and Agata Lapedriza and Aythami\n  Morales", "title": "Learning Emotional-Blinded Face Representations", "comments": "IAPR Intl. Conf. on Pattern Recognition, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two face representations that are blind to facial expressions\nassociated to emotional responses. This work is in part motivated by new\ninternational regulations for personal data protection, which enforce data\ncontrollers to protect any kind of sensitive information involved in automatic\nprocesses. The advances in Affective Computing have contributed to improve\nhuman-machine interfaces but, at the same time, the capacity to monitorize\nemotional responses triggers potential risks for humans, both in terms of\nfairness and privacy. We propose two different methods to learn these\nexpression-blinded facial features. We show that it is possible to eliminate\ninformation related to emotion recognition tasks, while the performance of\nsubject verification, gender recognition, and ethnicity classification are just\nslightly affected. We also present an application to train fairer classifiers\nin a case study of attractiveness classification with respect to a protected\nfacial expression attribute. The results demonstrate that it is possible to\nreduce emotional information in the face representation while retaining\ncompetitive performance in other face-based artificial intelligence tasks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 09:24:10 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Pe\u00f1a", "Alejandro", ""], ["Fierrez", "Julian", ""], ["Lapedriza", "Agata", ""], ["Morales", "Aythami", ""]]}, {"id": "2009.08709", "submitter": "Chaofeng Chen", "authors": "Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang,\n  Kwan-Yee K. Wong", "title": "Progressive Semantic-Aware Style Transformation for Blind Face\n  Restoration", "comments": "Accepted to CVPR2021, https://github.com/chaofengc/PSFRGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Face restoration is important in face image processing, and has been widely\nstudied in recent years. However, previous works often fail to generate\nplausible high quality (HQ) results for real-world low quality (LQ) face\nimages. In this paper, we propose a new progressive semantic-aware style\ntransformation framework, named PSFR-GAN, for face restoration. Specifically,\ninstead of using an encoder-decoder framework as previous methods, we formulate\nthe restoration of LQ face images as a multi-scale progressive restoration\nprocedure through semantic-aware style transformation. Given a pair of LQ face\nimage and its corresponding parsing map, we first generate a multi-scale\npyramid of the inputs, and then progressively modulate different scale features\nfrom coarse-to-fine in a semantic-aware style transfer way. Compared with\nprevious networks, the proposed PSFR-GAN makes full use of the semantic\n(parsing maps) and pixel (LQ images) space information from different scales of\ninput pairs. In addition, we further introduce a semantic aware style loss\nwhich calculates the feature style loss for each semantic region individually\nto improve the details of face textures. Finally, we pretrain a face parsing\nnetwork which can generate decent parsing maps from real-world LQ face images.\nExperiment results show that our model trained with synthetic data can not only\nproduce more realistic high-resolution results for synthetic LQ inputs and but\nalso generalize better to natural LQ face images compared with state-of-the-art\nmethods. Codes are available at https://github.com/chaofengc/PSFRGAN.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 09:27:33 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 09:35:05 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Chen", "Chaofeng", ""], ["Li", "Xiaoming", ""], ["Yang", "Lingbo", ""], ["Lin", "Xianhui", ""], ["Zhang", "Lei", ""], ["Wong", "Kwan-Yee K.", ""]]}, {"id": "2009.08720", "submitter": "Diego Marcos", "authors": "Diego Marcos, Ruth Fong, Sylvain Lobry, Remi Flamary, Nicolas Courty\n  and Devis Tuia", "title": "Contextual Semantic Interpretability", "comments": null, "journal-ref": "ACCV 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNN) are known to learn an image\nrepresentation that captures concepts relevant to the task, but do so in an\nimplicit way that hampers model interpretability. However, one could argue that\nsuch a representation is hidden in the neurons and can be made explicit by\nteaching the model to recognize semantically interpretable attributes that are\npresent in the scene. We call such an intermediate layer a \\emph{semantic\nbottleneck}. Once the attributes are learned, they can be re-combined to reach\nthe final decision and provide both an accurate prediction and an explicit\nreasoning behind the CNN decision. In this paper, we look into semantic\nbottlenecks that capture context: we want attributes to be in groups of a few\nmeaningful elements and participate jointly to the final decision. We use a\ntwo-layer semantic bottleneck that gathers attributes into interpretable,\nsparse groups, allowing them contribute differently to the final output\ndepending on the context. We test our contextual semantic interpretable\nbottleneck (CSIB) on the task of landscape scenicness estimation and train the\nsemantic interpretable bottleneck using an auxiliary database (SUN Attributes).\nOur model yields in predictions as accurate as a non-interpretable baseline\nwhen applied to a real-world test set of Flickr images, all while providing\nclear and interpretable explanations for each prediction.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 09:47:05 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Marcos", "Diego", ""], ["Fong", "Ruth", ""], ["Lobry", "Sylvain", ""], ["Flamary", "Remi", ""], ["Courty", "Nicolas", ""], ["Tuia", "Devis", ""]]}, {"id": "2009.08724", "submitter": "Youngseok Jang", "authors": "Youngseok Jang, Hojoon Shin, and H. Jin Kim", "title": "Pose Correction Algorithm for Relative Frames between Keyframes in SLAM", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the dominance of keyframe-based SLAM in the field of robotics, the\nrelative frame poses between keyframes have typically been sacrificed for a\nfaster algorithm to achieve online applications. However, those approaches can\nbecome insufficient for applications that may require refined poses of all\nframes, not just keyframes which are relatively sparse compared to all input\nframes. This paper proposes a novel algorithm to correct the relative frames\nbetween keyframes after the keyframes have been updated by a back-end\noptimization process. The correction model is derived using conservation of the\nmeasurement constraint between landmarks and the robot pose. The proposed\nalgorithm is designed to be easily integrable to existing keyframe-based SLAM\nsystems while exhibiting robust and accurate performance superior to existing\ninterpolation methods. The algorithm also requires low computational resources\nand hence has a minimal burden on the whole SLAM pipeline. We provide the\nevaluation of the proposed pose correction algorithm in comparison to existing\ninterpolation methods in various vector spaces, and our method has demonstrated\nexcellent accuracy in both KITTI and EuRoC datasets.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 09:59:10 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Jang", "Youngseok", ""], ["Shin", "Hojoon", ""], ["Kim", "H. Jin", ""]]}, {"id": "2009.08746", "submitter": "Haram Kim", "authors": "Haram Kim, Pyojin Kim, H. Jin Kim", "title": "Moving object detection for visual odometry in a dynamic environment\n  based on occlusion accumulation", "comments": "7 pages", "journal-ref": "ICRA 2020 published", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of moving objects is an essential capability in dealing with\ndynamic environments. Most moving object detection algorithms have been\ndesigned for color images without depth. For robotic navigation where real-time\nRGB-D data is often readily available, utilization of the depth information\nwould be beneficial for obstacle recognition.\n  Here, we propose a simple moving object detection algorithm that uses RGB-D\nimages. The proposed algorithm does not require estimating a background model.\nInstead, it uses an occlusion model which enables us to estimate the camera\npose on a background confused with moving objects that dominate the scene. The\nproposed algorithm allows to separate the moving object detection and visual\nodometry (VO) so that an arbitrary robust VO method can be employed in a\ndynamic situation with a combination of moving object detection, whereas other\nVO algorithms for a dynamic environment are inseparable. In this paper, we use\ndense visual odometry (DVO) as a VO method with a bi-square regression weight.\nExperimental results show the segmentation accuracy and the performance\nimprovement of DVO in the situations. We validate our algorithm in public\ndatasets and our dataset which also publicly accessible.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 11:01:46 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Kim", "Haram", ""], ["Kim", "Pyojin", ""], ["Kim", "H. Jin", ""]]}, {"id": "2009.08753", "submitter": "Yan Hong", "authors": "Yan Hong, Li Niu, Jianfu Zhang, Jing Liang, Liqing Zhang", "title": "DeltaGAN: Towards Diverse Few-shot Image Generation with Sample-Specific\n  Delta", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to generate new images for a novel category based on only a few\nimages, named as few-shot image generation, has attracted increasing research\ninterest. Several state-of-the-art works have yielded impressive results, but\nthe diversity is still limited. In this work, we propose a novel Delta\nGenerative Adversarial Network (DeltaGAN), which consists of a reconstruction\nsubnetwork and a generation subnetwork. The reconstruction subnetwork captures\nintra-category transformation, i.e., \"delta\", between same-category pairs. The\ngeneration subnetwork generates sample-specific \"delta\" for an input image,\nwhich is combined with this input image to generate a new image within the same\ncategory. Besides, an adversarial delta matching loss is designed to link the\nabove two subnetworks together. Extensive experiments on five few-shot image\ndatasets demonstrate the effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 11:25:05 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 01:10:20 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 02:44:46 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Hong", "Yan", ""], ["Niu", "Li", ""], ["Zhang", "Jianfu", ""], ["Liang", "Jing", ""], ["Zhang", "Liqing", ""]]}, {"id": "2009.08792", "submitter": "Thierry Deruyttere", "authors": "Thierry Deruyttere, Simon Vandenhende, Dusan Grujicic, Yu Liu, Luc Van\n  Gool, Matthew Blaschko, Tinne Tuytelaars, Marie-Francine Moens", "title": "Commands 4 Autonomous Vehicles (C4AV) Workshop Summary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of visual grounding requires locating the most relevant region or\nobject in an image, given a natural language query. So far, progress on this\ntask was mostly measured on curated datasets, which are not always\nrepresentative of human spoken language. In this work, we deviate from recent,\npopular task settings and consider the problem under an autonomous vehicle\nscenario. In particular, we consider a situation where passengers can give\nfree-form natural language commands to a vehicle which can be associated with\nan object in the street scene. To stimulate research on this topic, we have\norganized the \\emph{Commands for Autonomous Vehicles} (C4AV) challenge based on\nthe recent \\emph{Talk2Car} dataset (URL:\nhttps://www.aicrowd.com/challenges/eccv-2020-commands-4-autonomous-vehicles).\nThis paper presents the results of the challenge. First, we compare the used\nbenchmark against existing datasets for visual grounding. Second, we identify\nthe aspects that render top-performing models successful, and relate them to\nexisting state-of-the-art models for visual grounding, in addition to detecting\npotential failure cases by evaluating on carefully selected subsets. Finally,\nwe discuss several possibilities for future work.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 12:33:21 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Deruyttere", "Thierry", ""], ["Vandenhende", "Simon", ""], ["Grujicic", "Dusan", ""], ["Liu", "Yu", ""], ["Van Gool", "Luc", ""], ["Blaschko", "Matthew", ""], ["Tuytelaars", "Tinne", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "2009.08796", "submitter": "Riccardo La Grassa", "authors": "Riccardo La Grassa, Ignazio Gallo, Nicola Landro", "title": "$\\sigma^2$R Loss: a Weighted Loss by Multiplicative Factors using\n  Sigmoidal Functions", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In neural networks, the loss function represents the core of the learning\nprocess that leads the optimizer to an approximation of the optimal convergence\nerror. Convolutional neural networks (CNN) use the loss function as a\nsupervisory signal to train a deep model and contribute significantly to\nachieving the state of the art in some fields of artificial vision.\nCross-entropy and Center loss functions are commonly used to increase the\ndiscriminating power of learned functions and increase the generalization\nperformance of the model. Center loss minimizes the class intra-class variance\nand at the same time penalizes the long distance between the deep features\ninside each class. However, the total error of the center loss will be heavily\ninfluenced by the majority of the instances and can lead to a freezing state in\nterms of intra-class variance. To address this, we introduce a new loss\nfunction called sigma squared reduction loss ($\\sigma^2$R loss), which is\nregulated by a sigmoid function to inflate/deflate the error per instance and\nthen continue to reduce the intra-class variance. Our loss has clear intuition\nand geometric interpretation, furthermore, we demonstrate by experiments the\neffectiveness of our proposal on several benchmark datasets showing the\nintra-class variance reduction and overcoming the results obtained with center\nloss and soft nearest neighbour functions.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 12:34:40 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["La Grassa", "Riccardo", ""], ["Gallo", "Ignazio", ""], ["Landro", "Nicola", ""]]}, {"id": "2009.08825", "submitter": "Wonjun Hwang", "authors": "Wonchul Son and Jaemin Na and Junyong Choi and Wonjun Hwang", "title": "Densely Guided Knowledge Distillation using Multiple Teacher Assistants", "comments": "Accepted to ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the success of deep neural networks, knowledge distillation which guides\nthe learning of a small student network from a large teacher network is being\nactively studied for model compression and transfer learning. However, few\nstudies have been performed to resolve the poor learning issue of the student\nnetwork when the student and teacher model sizes significantly differ. In this\npaper, we propose a densely guided knowledge distillation using multiple\nteacher assistants that gradually decreases the model size to efficiently\nbridge the large gap between the teacher and student networks. To stimulate\nmore efficient learning of the student network, we guide each teacher assistant\nto every other smaller teacher assistants iteratively. Specifically, when\nteaching a smaller teacher assistant at the next step, the existing larger\nteacher assistants from the previous step are used as well as the teacher\nnetwork. Moreover, we design stochastic teaching where, for each mini-batch, a\nteacher or teacher assistants are randomly dropped. This acts as a regularizer\nto improve the efficiency of teaching of the student network. Thus, the student\ncan always learn salient distilled knowledge from the multiple sources. We\nverified the effectiveness of the proposed method for a classification task\nusing CIFAR-10, CIFAR-100, and ImageNet. We also achieved significant\nperformance improvements with various backbone architectures such as ResNet,\nWideResNet, and VGG.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 13:12:52 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 03:09:08 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Son", "Wonchul", ""], ["Na", "Jaemin", ""], ["Choi", "Junyong", ""], ["Hwang", "Wonjun", ""]]}, {"id": "2009.08829", "submitter": "Changlu Guo", "authors": "Changlu Guo, M\\'arton Szemenyei, Yugen Yi, Wei Zhou, Haodong Bian", "title": "Residual Spatial Attention Network for Retinal Vessel Segmentation", "comments": "ICONIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable segmentation of retinal vessels can be employed as a way of\nmonitoring and diagnosing certain diseases, such as diabetes and hypertension,\nas they affect the retinal vascular structure. In this work, we propose the\nResidual Spatial Attention Network (RSAN) for retinal vessel segmentation. RSAN\nemploys a modified residual block structure that integrates DropBlock, which\ncan not only be utilized to construct deep networks to extract more complex\nvascular features, but can also effectively alleviate the overfitting.\nMoreover, in order to further improve the representation capability of the\nnetwork, based on this modified residual block, we introduce the spatial\nattention (SA) and propose the Residual Spatial Attention Block (RSAB) to build\nRSAN. We adopt the public DRIVE and CHASE DB1 color fundus image datasets to\nevaluate the proposed RSAN. Experiments show that the modified residual\nstructure and the spatial attention are effective in this work, and our\nproposed RSAN achieves the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 13:17:13 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Guo", "Changlu", ""], ["Szemenyei", "M\u00e1rton", ""], ["Yi", "Yugen", ""], ["Zhou", "Wei", ""], ["Bian", "Haodong", ""]]}, {"id": "2009.08831", "submitter": "Redha Ali", "authors": "Hussin K. Ragb, Ian T. Dover, Redha Ali", "title": "Fused Deep Convolutional Neural Network for Precision Diagnosis of\n  COVID-19 Using Chest X-Ray Images", "comments": "9 Pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With a Coronavirus disease (COVID-19) case count exceeding 10 million\nworldwide, there is an increased need for a diagnostic capability. The main\nvariables in increasing diagnostic capability are reduced cost, turnaround or\ndiagnosis time, and upfront equipment cost and accessibility. Two candidates\nfor machine learning COVID-19 diagnosis are Computed Tomography (CT) scans and\nplain chest X-rays. While CT scans score higher in sensitivity, they have a\nhigher cost, maintenance requirement, and turnaround time as compared to plain\nchest X-rays. The use of portable chest X-radiograph (CXR) is recommended by\nthe American College of Radiology (ACR) since using CT places a massive burden\non radiology services. Therefore, X-ray imagery paired with machine learning\ntechniques is proposed a first-line triage tool for COVID-19 diagnostics. In\nthis paper we propose a computer-aided diagnosis (CAD) to accurately classify\nchest X-ray scans of COVID-19 and normal subjects by fine-tuning several neural\nnetworks (ResNet18, ResNet50, DenseNet201) pre-trained on the ImageNet dataset.\nThese neural networks are fused in a parallel architecture and the voting\ncriteria are applied in the final classification decision between the candidate\nobject classes where the output of each neural network is representing a single\nvote. Several experiments are conducted on the weakly labeled COVID-19-CT-CXR\ndataset consisting of 263 COVID-19 CXR images extracted from PubMed Central\nOpen Access subsets combined with 25 normal classification CXR images. These\nexperiments show an optimistic result and a capability of the proposed model to\noutperforming many state-of-the-art algorithms on several measures. Using\nk-fold cross-validation and a bagging classifier ensemble, we achieve an\naccuracy of 99.7% and a sensitivity of 100%.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 02:27:20 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Ragb", "Hussin K.", ""], ["Dover", "Ian T.", ""], ["Ali", "Redha", ""]]}, {"id": "2009.08835", "submitter": "David Schedl", "authors": "David C. Schedl and Indrajit Kurmi and Oliver Bimber", "title": "Search and Rescue with Airborne Optical Sectioning", "comments": "11 pages, 5 figures, 3 tables, Nature Machine Intelligence (under\n  review)", "journal-ref": null, "doi": "10.1038/s42256-020-00261-3", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that automated person detection under occlusion conditions can be\nsignificantly improved by combining multi-perspective images before\nclassification. Here, we employed image integration by Airborne Optical\nSectioning (AOS)---a synthetic aperture imaging technique that uses camera\ndrones to capture unstructured thermal light fields---to achieve this with a\nprecision/recall of 96/93%. Finding lost or injured people in dense forests is\nnot generally feasible with thermal recordings, but becomes practical with use\nof AOS integral images. Our findings lay the foundation for effective future\nsearch and rescue technologies that can be applied in combination with\nautonomous or manned aircraft. They can also be beneficial for other fields\nthat currently suffer from inaccurate classification of partially occluded\npeople, animals, or objects.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 13:40:19 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Schedl", "David C.", ""], ["Kurmi", "Indrajit", ""], ["Bimber", "Oliver", ""]]}, {"id": "2009.08845", "submitter": "Daniel Ruiz", "authors": "Daniel V. Ruiz and Bruno A. Krinski and Eduardo Todt", "title": "IDA: Improved Data Augmentation Applied to Salient Object Detection", "comments": "Accepted for presentation at SIBGRAPI 2020 - 33rd Conference on\n  Graphics, Patterns and Images", "journal-ref": null, "doi": "10.1109/SIBGRAPI51738.2020.00036", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an Improved Data Augmentation (IDA) technique\nfocused on Salient Object Detection (SOD). Standard data augmentation\ntechniques proposed in the literature, such as image cropping, rotation,\nflipping, and resizing, only generate variations of the existing examples,\nproviding a limited generalization. Our method combines image inpainting,\naffine transformations, and the linear combination of different generated\nbackground images with salient objects extracted from labeled data. Our\nproposed technique enables more precise control of the object's position and\nsize while preserving background information. The background choice is based on\nan inter-image optimization, while object size follows a uniform random\ndistribution within a specified interval, and the object position is\nintra-image optimal. We show that our method improves the segmentation quality\nwhen used for training state-of-the-art neural networks on several famous\ndatasets of the SOD field. Combining our method with others surpasses\ntraditional techniques such as horizontal-flip in 0.52% for F-measure and 1.19%\nfor Precision. We also provide an evaluation in 7 different SOD datasets, with\n9 distinct evaluation metrics and an average ranking of the evaluated methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 14:03:27 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Ruiz", "Daniel V.", ""], ["Krinski", "Bruno A.", ""], ["Todt", "Eduardo", ""]]}, {"id": "2009.08849", "submitter": "Yang He", "authors": "Yang He and Bernt Schiele and Mario Fritz", "title": "Synthetic Convolutional Features for Improved Semantic Segmentation", "comments": "ECCV 2020 Workshop on Assistive Computer Vision and Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, learning-based image synthesis has enabled to generate\nhigh-resolution images, either applying popular adversarial training or a\npowerful perceptual loss. However, it remains challenging to successfully\nleverage synthetic data for improving semantic segmentation with additional\nsynthetic images. Therefore, we suggest to generate intermediate convolutional\nfeatures and propose the first synthesis approach that is catered to such\nintermediate convolutional features. This allows us to generate new features\nfrom label masks and include them successfully into the training procedure in\norder to improve the performance of semantic segmentation. Experimental results\nand analysis on two challenging datasets Cityscapes and ADE20K show that our\ngenerated feature improves performance on segmentation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 14:12:50 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["He", "Yang", ""], ["Schiele", "Bernt", ""], ["Fritz", "Mario", ""]]}, {"id": "2009.08855", "submitter": "Suhwan Cho", "authors": "Suhwan Cho, Heansung Lee, Sungmin Woo, Sungjun Jang, Sangyoun Lee", "title": "PMVOS: Pixel-Level Matching-Based Video Object Segmentation", "comments": "Code: https://github.com/suhwan-cho/PMVOS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised video object segmentation (VOS) aims to segment arbitrary\ntarget objects in video when the ground truth segmentation mask of the initial\nframe is provided. Due to this limitation of using prior knowledge about the\ntarget object, feature matching, which compares template features representing\nthe target object with input features, is an essential step. Recently,\npixel-level matching (PM), which matches every pixel in template features and\ninput features, has been widely used for feature matching because of its high\nperformance. However, despite its effectiveness, the information used to build\nthe template features is limited to the initial and previous frames. We address\nthis issue by proposing a novel method-PM-based video object segmentation\n(PMVOS)-that constructs strong template features containing the information of\nall past frames. Furthermore, we apply self-attention to the similarity maps\ngenerated from PM to capture global dependencies. On the DAVIS 2016 validation\nset, we achieve new state-of-the-art performance among real-time methods (> 30\nfps), with a J&F score of 85.6%. Performance on the DAVIS 2017 and YouTube-VOS\nvalidation sets is also impressive, with J&F scores of 74.0% and 68.2%,\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 14:22:09 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Cho", "Suhwan", ""], ["Lee", "Heansung", ""], ["Woo", "Sungmin", ""], ["Jang", "Sungjun", ""], ["Lee", "Sangyoun", ""]]}, {"id": "2009.08864", "submitter": "Asifullah Khan", "authors": "Saddam Hussain Khan, Anabia Sohail, Asifullah Khan, and Yeon Soo Lee", "title": "Classification and Region Analysis of COVID-19 Infection using Lung CT\n  Images and Deep Convolutional Neural Networks", "comments": "Pages: 32, Tables: 6, Figures: 14", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 is a global health problem. Consequently, early detection and\nanalysis of the infection patterns are crucial for controlling infection spread\nas well as devising a treatment plan. This work proposes a two-stage deep\nConvolutional Neural Networks (CNNs) based framework for delineation of\nCOVID-19 infected regions in Lung CT images. In the first stage, initially,\nCOVID-19 specific CT image features are enhanced using a two-level discrete\nwavelet transformation. These enhanced CT images are then classified using the\nproposed custom-made deep CoV-CTNet. In the second stage, the CT images\nclassified as infectious images are provided to the segmentation models for the\nidentification and analysis of COVID-19 infectious regions. In this regard, we\npropose a novel semantic segmentation model CoV-RASeg, which systematically\nuses average and max pooling operations in the encoder and decoder blocks. This\nsystematic utilization of max and average pooling operations helps the proposed\nCoV-RASeg in simultaneously learning both the boundaries and region\nhomogeneity. Moreover, the idea of attention is incorporated to deal with\nmildly infected regions. The proposed two-stage framework is evaluated on a\nstandard Lung CT image dataset, and its performance is compared with the\nexisting deep CNN models. The performance of the proposed CoV-CTNet is\nevaluated using Mathew Correlation Coefficient (MCC) measure (0.98) and that of\nproposed CoV-RASeg using Dice Similarity (DS) score (0.95). The promising\nresults on an unseen test set suggest that the proposed framework has the\npotential to help the radiologists in the identification and analysis of\nCOVID-19 infected regions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 02:28:46 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Khan", "Saddam Hussain", ""], ["Sohail", "Anabia", ""], ["Khan", "Asifullah", ""], ["Lee", "Yeon Soo", ""]]}, {"id": "2009.08876", "submitter": "Shihong Fang", "authors": "Shihong Fang, Anna Choromanska", "title": "Multi-modal Experts Network for Autonomous Driving", "comments": "Published at the International Conference on Robotics and Automation\n  (ICRA), 2020", "journal-ref": "2020 IEEE International Conference on Robotics and Automation\n  (ICRA), Paris, France, 2020, pp. 6439-6445", "doi": "10.1109/ICRA40945.2020.9197459", "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  End-to-end learning from sensory data has shown promising results in\nautonomous driving. While employing many sensors enhances world perception and\nshould lead to more robust and reliable behavior of autonomous vehicles, it is\nchallenging to train and deploy such network and at least two problems are\nencountered in the considered setting. The first one is the increase of\ncomputational complexity with the number of sensing devices. The other is the\nphenomena of network overfitting to the simplest and most informative input. We\naddress both challenges with a novel, carefully tailored multi-modal experts\nnetwork architecture and propose a multi-stage training procedure. The network\ncontains a gating mechanism, which selects the most relevant input at each\ninference time step using a mixed discrete-continuous policy. We demonstrate\nthe plausibility of the proposed approach on our 1/6 scale truck equipped with\nthree cameras and one LiDAR.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 14:54:54 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Fang", "Shihong", ""], ["Choromanska", "Anna", ""]]}, {"id": "2009.08886", "submitter": "Zixiang Ding", "authors": "Zixiang Ding, Yaran Chen, Nannan Li and Dongbin Zhao", "title": "BNAS-v2: Memory-efficient and Performance-collapse-prevented Broad\n  Neural Architecture Search", "comments": "12 pages, 11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose BNAS-v2 to further improve the efficiency of NAS,\nembodying both superiorities of BCNN simultaneously. To mitigate the unfair\ntraining issue of BNAS, we employ continuous relaxation strategy to make each\nedge of cell in BCNN relevant to all candidate operations for\nover-parameterized BCNN construction. Moreover, the continuous relaxation\nstrategy relaxes the choice of a candidate operation as a softmax over all\npredefined operations. Consequently, BNAS-v2 employs the gradient-based\noptimization algorithm to simultaneously update every possible path of\nover-parameterized BCNN, rather than the single sampled one as BNAS. However,\ncontinuous relaxation leads to another issue named performance collapse, in\nwhich those weight-free operations are prone to be selected by the search\nstrategy. For this consequent issue, two solutions are given: 1) we propose\nConfident Learning Rate (CLR) that considers the confidence of gradient for\narchitecture weights update, increasing with the training time of\nover-parameterized BCNN; 2) we introduce the combination of partial channel\nconnections and edge normalization that also can improve the memory efficiency\nfurther. Moreover, we denote differentiable BNAS (i.e. BNAS with continuous\nrelaxation) as BNAS-D, BNAS-D with CLR as BNAS-v2-CLR, and partial-connected\nBNAS-D as BNAS-v2-PC. Experimental results on CIFAR-10 and ImageNet show that\n1) BNAS-v2 delivers state-of-the-art search efficiency on both CIFAR-10 (0.05\nGPU days that is 4x faster than BNAS) and ImageNet (0.19 GPU days); and 2) the\nproposed CLR is effective to alleviate the performance collapse issue in both\nBNAS-D and vanilla differentiable NAS framework.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 15:25:08 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 07:09:31 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2021 02:59:21 GMT"}, {"version": "v4", "created": "Mon, 25 Jan 2021 09:05:02 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Ding", "Zixiang", ""], ["Chen", "Yaran", ""], ["Li", "Nannan", ""], ["Zhao", "Dongbin", ""]]}, {"id": "2009.08891", "submitter": "Dehua Song", "authors": "Dehua Song, Yunhe Wang, Hanting Chen, Chang Xu, Chunjing Xu, Dacheng\n  Tao", "title": "AdderSR: Towards Energy Efficient Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the single image super-resolution problem using adder\nneural networks (AdderNet). Compared with convolutional neural networks,\nAdderNet utilizing additions to calculate the output features thus avoid\nmassive energy consumptions of conventional multiplications. However, it is\nvery hard to directly inherit the existing success of AdderNet on large-scale\nimage classification to the image super-resolution task due to the different\ncalculation paradigm. Specifically, the adder operation cannot easily learn the\nidentity mapping, which is essential for image processing tasks. In addition,\nthe functionality of high-pass filters cannot be ensured by AdderNet. To this\nend, we thoroughly analyze the relationship between an adder operation and the\nidentity mapping and insert shortcuts to enhance the performance of SR models\nusing adder networks. Then, we develop a learnable power activation for\nadjusting the feature distribution and refining details. Experiments conducted\non several benchmark models and datasets demonstrate that, our image\nsuper-resolution models using AdderNet can achieve comparable performance and\nvisual quality to that of their CNN baselines with an about 2$\\times$ reduction\non the energy consumption.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 15:29:13 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 12:34:16 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 15:28:06 GMT"}, {"version": "v4", "created": "Sun, 27 Sep 2020 13:35:55 GMT"}, {"version": "v5", "created": "Fri, 9 Apr 2021 11:15:20 GMT"}, {"version": "v6", "created": "Wed, 14 Apr 2021 11:37:56 GMT"}, {"version": "v7", "created": "Tue, 4 May 2021 08:01:51 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Song", "Dehua", ""], ["Wang", "Yunhe", ""], ["Chen", "Hanting", ""], ["Xu", "Chang", ""], ["Xu", "Chunjing", ""], ["Tao", "Dacheng", ""]]}, {"id": "2009.08899", "submitter": "Yurio Windiatmoko", "authors": "Dhomas Hatta Fudholi, Yurio Windiatmoko, Nurdi Afrianto, Prastyo Eko\n  Susanto, Magfirah Suyuti, Ahmad Fathan Hidayatullah, Ridho Rahmadi", "title": "Image Captioning with Attention for Smart Local Tourism using\n  EfficientNet", "comments": "10 pages, 7 figures, still in review at ICITDA Conference", "journal-ref": null, "doi": "10.1088/1757-899X/1077/1/012038", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Smart systems have been massively developed to help humans in various tasks.\nDeep Learning technologies push even further in creating accurate assistant\nsystems due to the explosion of data lakes. One of the smart system tasks is to\ndisseminate users needed information. This is crucial in the tourism sector to\npromote local tourism destinations. In this research, we design a model of\nlocal tourism specific image captioning, which later will support the\ndevelopment of AI-powered systems that assist various users. The model is\ndeveloped using a visual Attention mechanism and uses the state-of-the-art\nfeature extractor architecture EfficientNet. A local tourism dataset is\ncollected and is used in the research, along with two different kinds of\ncaptions. Captions that describe the image literally and captions that\nrepresent human logical responses when seeing the image. This is done to make\nthe captioning model more humane when implemented in the assistance system. We\ncompared the performance of two different models using EfficientNet\narchitectures (B0 and B4) with other well known VGG16 and InceptionV3. The best\nBLEU scores we get are 73.39 and 24.51 for the training set and the validation\nset respectively, using EfficientNetB0. The captioning result using the\ndeveloped model shows that the model can produce logical caption for local\ntourism-related images\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 15:47:25 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Fudholi", "Dhomas Hatta", ""], ["Windiatmoko", "Yurio", ""], ["Afrianto", "Nurdi", ""], ["Susanto", "Prastyo Eko", ""], ["Suyuti", "Magfirah", ""], ["Hidayatullah", "Ahmad Fathan", ""], ["Rahmadi", "Ridho", ""]]}, {"id": "2009.08906", "submitter": "Abhishek Banerjee", "authors": "Abhishek Banerjee, Uttaran Bhattacharya, Aniket Bera", "title": "Learning Unseen Emotions from Gestures via Semantically-Conditioned\n  Zero-Shot Perception with Adversarial Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel generalized zero-shot algorithm to recognize perceived\nemotions from gestures. Our task is to map gestures to novel emotion categories\nnot encountered in training. We introduce an adversarial, autoencoder-based\nrepresentation learning that correlates 3D motion-captured gesture sequence\nwith the vectorized representation of the natural-language perceived emotion\nterms using word2vec embeddings. The language-semantic embedding provides a\nrepresentation of the emotion label space, and we leverage this underlying\ndistribution to map the gesture-sequences to the appropriate categorical\nemotion labels. We train our method using a combination of gestures annotated\nwith known emotion terms and gestures not annotated with any emotions. We\nevaluate our method on the MPI Emotional Body Expressions Database (EBEDB) and\nobtain an accuracy of $58.43\\%$. This improves the performance of current\nstate-of-the-art algorithms for generalized zero-shot learning by $25$--$27\\%$\non the absolute.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 15:59:44 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Banerjee", "Abhishek", ""], ["Bhattacharya", "Uttaran", ""], ["Bera", "Aniket", ""]]}, {"id": "2009.08917", "submitter": "Yinxi Wang", "authors": "Yinxi Wang, Kimmo Kartasalo, Masi Valkonen, Christer Larsson, Pekka\n  Ruusuvuori, Johan Hartman, Mattias Rantalainen", "title": "Predicting molecular phenotypes from histopathology images: a\n  transcriptome-wide expression-morphology analysis in breast cancer", "comments": "42 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular phenotyping is central in cancer precision medicine, but remains\ncostly and standard methods only provide a tumour average profile. Microscopic\nmorphological patterns observable in histopathology sections from tumours are\ndetermined by the underlying molecular phenotype and associated with clinical\nfactors. The relationship between morphology and molecular phenotype has a\npotential to be exploited for prediction of the molecular phenotype from the\nmorphology visible in histopathology images.\n  We report the first transcriptome-wide Expression-MOrphology (EMO) analysis\nin breast cancer, where gene-specific models were optimised and validated for\nprediction of mRNA expression both as a tumour average and in spatially\nresolved manner. Individual deep convolutional neural networks (CNNs) were\noptimised to predict the expression of 17,695 genes from hematoxylin and eosin\n(HE) stained whole slide images (WSIs). Predictions for 9,334 (52.75%) genes\nwere significantly associated with RNA-sequencing estimates (FDR adjusted\np-value < 0.05). 1,011 of the genes were brought forward for validation, with\n876 (87%) and 908 (90%) successfully replicated in internal and external test\ndata, respectively. Predicted spatial intra-tumour variabilities in expression\nwere validated in 76 genes, out of which 59 (77.6%) had a significant\nassociation (FDR adjusted p-value < 0.05) with spatial transcriptomics\nestimates. These results suggest that the proposed methodology can be applied\nto predict both tumour average gene expression and intra-tumour spatial\nexpression directly from morphology, thus providing a scalable approach to\ncharacterise intra-tumour heterogeneity.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 16:27:53 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Wang", "Yinxi", ""], ["Kartasalo", "Kimmo", ""], ["Valkonen", "Masi", ""], ["Larsson", "Christer", ""], ["Ruusuvuori", "Pekka", ""], ["Hartman", "Johan", ""], ["Rantalainen", "Mattias", ""]]}, {"id": "2009.08920", "submitter": "Haoming Lu", "authors": "Haoming Lu, Humphrey Shi", "title": "Deep Learning for 3D Point Cloud Understanding: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of practical applications, such as autonomous driving and\nrobotics, has brought increasing attention to 3D point cloud understanding.\nWhile deep learning has achieved remarkable success on image-based tasks, there\nare many unique challenges faced by deep neural networks in processing massive,\nunstructured and noisy 3D points. To demonstrate the latest progress of deep\nlearning for 3D point cloud understanding, this paper summarizes recent\nremarkable research contributions in this area from several different\ndirections (classification, segmentation, detection, tracking, flow estimation,\nregistration, augmentation and completion), together with commonly used\ndatasets, metrics and state-of-the-art performances. More information regarding\nthis survey can be found at:\nhttps://github.com/SHI-Labs/3D-Point-Cloud-Learning.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 16:34:12 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 15:04:30 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Lu", "Haoming", ""], ["Shi", "Humphrey", ""]]}, {"id": "2009.08924", "submitter": "Liuyue Xie", "authors": "Liuyue Xie, Tomotake Furuhata, Kenji Shimada", "title": "Multi-Resolution Graph Neural Network for Large-Scale Pointcloud\n  Segmentation", "comments": null, "journal-ref": "Conference on Robot Learning, 2020, 184", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a multi-resolution deep-learning architecture to\nsemantically segment dense large-scale pointclouds. Dense pointcloud data\nrequire a computationally expensive feature encoding process before semantic\nsegmentation. Previous work has used different approaches to drastically\ndownsample from the original pointcloud so common computing hardware can be\nutilized. While these approaches can relieve the computation burden to some\nextent, they are still limited in their processing capability for multiple\nscans. We present MuGNet, a memory-efficient, end-to-end graph neural network\nframework to perform semantic segmentation on large-scale pointclouds. We\nreduce the computation demand by utilizing a graph neural network on the\npreformed pointcloud graphs and retain the precision of the segmentation with a\nbidirectional network that fuses feature embedding at different resolutions.\nOur framework has been validated on benchmark datasets including Stanford\nLarge-Scale 3D Indoor Spaces Dataset(S3DIS) and Virtual KITTI Dataset. We\ndemonstrate that our framework can process up to 45 room scans at once on a\nsingle 11 GB GPU while still surpassing other graph-based solutions for\nsegmentation on S3DIS with an 88.5\\% (+3\\%) overall accuracy and 69.8\\%\n(+7.7\\%) mIOU accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 16:42:02 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Xie", "Liuyue", ""], ["Furuhata", "Tomotake", ""], ["Shimada", "Kenji", ""]]}, {"id": "2009.08941", "submitter": "Hassan Ahmed Sial", "authors": "Hassan A. Sial, Ramon Baldrich, Maria Vanrell, Dimitris Samaras", "title": "Light Direction and Color Estimation from Single Image with Deep\n  Regression", "comments": "Conference: London Imaging Meeting 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to estimate the direction and color of the scene light\nsource from a single image. Our method is based on two main ideas: (a) we use a\nnew synthetic dataset with strong shadow effects with similar constraints to\nthe SID dataset; (b) we define a deep architecture trained on the mentioned\ndataset to estimate the direction and color of the scene light source. Apart\nfrom showing good performance on synthetic images, we additionally propose a\npreliminary procedure to obtain light positions of the Multi-Illumination\ndataset, and, in this way, we also prove that our trained model achieves good\nperformance when it is applied to real scenes.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 17:33:49 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Sial", "Hassan A.", ""], ["Baldrich", "Ramon", ""], ["Vanrell", "Maria", ""], ["Samaras", "Dimitris", ""]]}, {"id": "2009.08965", "submitter": "Micah Goldblum", "authors": "Manli Shu, Zuxuan Wu, Micah Goldblum, Tom Goldstein", "title": "Prepare for the Worst: Generalizing across Domain Shifts with\n  Adversarial Batch Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training is the industry standard for producing models that are\nrobust to small adversarial perturbations. However, machine learning\npractitioners need models that are robust to domain shifts that occur\nnaturally, such as changes in the style or illumination of input images. Such\nchanges in input distribution have been effectively modeled as shifts in the\nmean and variance of deep image features. We adapt adversarial training by\nadversarially perturbing these feature statistics, rather than image pixels, to\nproduce models that are robust to domain shift. We also visualize images from\nadversarially crafted distributions. Our method, Adversarial Batch\nNormalization (AdvBN), significantly improves the performance of ResNet-50 on\nImageNet-C (+8.1%), Stylized-ImageNet (+6.7%), and ImageNet-Instagram (+3.9%)\nover standard training practices. In addition, we demonstrate that AdvBN can\nalso improve generalization on semantic segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 17:52:34 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 19:41:32 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Shu", "Manli", ""], ["Wu", "Zuxuan", ""], ["Goldblum", "Micah", ""], ["Goldstein", "Tom", ""]]}, {"id": "2009.08997", "submitter": "Arman Garakani", "authors": "Arman Garakani, Martin Malmstedt-Miller, Ionela Manole, Adrian Y.\n  Rossler and John R. Zibert", "title": "Psoriasis Severity Assessment with a Similarity-Clustering Machine\n  Learning Approach Reduces Intra- and Inter-observation variation", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Psoriasis is a complex disease with many variations in genotype and\nphenotype. General advancements in medicine has further complicated both\nassessments and treatment for both physicians and dermatologist alike. Even\nwith all of our technological progress we still primarily use the assessment\ntool Psoriasis Area and Severity Index (PASI) for severity assessments which\nwas developed in the 1970s. In this study we evaluate a method involving\ndigital images, a comparison web application and similarity clustering,\ndeveloped to improve the assessment tool in terms of intra- and inter-observer\nvariation. Images of patients was collected from a mobile device. Images were\ncaptured of the same lesion area taken approximately 1 week apart. Five\ndermatologists evaluated the severity of psoriasis by modified-PASI, absolute\nscoring and a relative pairwise PASI scoring using similarity-clustering and\nconducted using a web-program displaying two images at a time. mPASI scoring of\nsingle photos by the same or different dermatologist showed mPASI ratings of\n50% to 80%, respectively. Repeated mPASI comparison using similarity clustering\nshowed consistent mPASI ratings > 95%. Pearson correlation between absolute\nscoring and pairwise scoring progression was 0.72.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 18:04:32 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 18:21:31 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Garakani", "Arman", ""], ["Malmstedt-Miller", "Martin", ""], ["Manole", "Ionela", ""], ["Rossler", "Adrian Y.", ""], ["Zibert", "John R.", ""]]}, {"id": "2009.09093", "submitter": "Runsheng Xu", "authors": "Runsheng Xu, Faezeh Tafazzoli, Li Zhang, Timo Rehfeld, Gunther Krehl,\n  Arunava Seal", "title": "Holistic Grid Fusion Based Stop Line Estimation", "comments": "Submitted to ICPR2020", "journal-ref": "2020 25th International Conference on Pattern Recognition (ICPR),\n  Milan, Italy, 2021 pp. 8400-8407", "doi": "10.1109/ICPR48806.2021.9413070", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intersection scenarios provide the most complex traffic situations in\nAutonomous Driving and Driving Assistance Systems. Knowing where to stop in\nadvance in an intersection is an essential parameter in controlling the\nlongitudinal velocity of the vehicle. Most of the existing methods in\nliterature solely use cameras to detect stop lines, which is typically not\nsufficient in terms of detection range. To address this issue, we propose a\nmethod that takes advantage of fused multi-sensory data including stereo camera\nand lidar as input and utilizes a carefully designed convolutional neural\nnetwork architecture to detect stop lines. Our experiments show that the\nproposed approach can improve detection range compared to camera data alone,\nworks under heavy occlusion without observing the ground markings explicitly,\nis able to predict stop lines for all lanes and allows detection at a distance\nup to 50 meters.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 21:29:06 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Xu", "Runsheng", ""], ["Tafazzoli", "Faezeh", ""], ["Zhang", "Li", ""], ["Rehfeld", "Timo", ""], ["Krehl", "Gunther", ""], ["Seal", "Arunava", ""]]}, {"id": "2009.09115", "submitter": "Hussein Osman", "authors": "Hussein Osman, Karim Zaghw, Mostafa Hazem, Seifeldin Elsehely", "title": "An Efficient Language-Independent Multi-Font OCR for Arabic Script", "comments": "9 pages, 4 figures, 4 tables, 2 algorithms, accepted in the\n  International Conference of Digital Image Processing and Pattern Recognition\n  (DPPR), London, UK, November 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Character Recognition (OCR) is the process of extracting digitized\ntext from images of scanned documents. While OCR systems have already matured\nin many languages, they still have shortcomings in cursive languages with\noverlapping letters such as the Arabic language. This paper proposes a complete\nArabic OCR system that takes a scanned image of Arabic Naskh script as an input\nand generates a corresponding digital document. Our Arabic OCR system consists\nof the following modules: Pre-processing, Word-level Feature Extraction,\nCharacter Segmentation, Character Recognition, and Post-processing. This paper\nalso proposes an improved font-independent character segmentation algorithm\nthat outperforms the state-of-the-art segmentation algorithms. Lastly, the\npaper proposes a neural network model for the character recognition task. The\nsystem has experimented on several open Arabic corpora datasets with an average\ncharacter segmentation accuracy 98.06%, character recognition accuracy 99.89%,\nand overall system accuracy 97.94% achieving outstanding results compared to\nthe state-of-the-art Arabic OCR systems.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 22:57:03 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Osman", "Hussein", ""], ["Zaghw", "Karim", ""], ["Hazem", "Mostafa", ""], ["Elsehely", "Seifeldin", ""]]}, {"id": "2009.09136", "submitter": "Farhad Pourkamali-Anaraki", "authors": "Farhad Pourkamali-Anaraki, Mohammad Amin Hariri-Ardebili, Lydia\n  Morawiec", "title": "Kernel Ridge Regression Using Importance Sampling with Application to\n  Seismic Response Prediction", "comments": "Accepted for publication in IEEE International Conference on Machine\n  Learning and Applications (ICMLA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable kernel methods, including kernel ridge regression, often rely on\nlow-rank matrix approximations using the Nystrom method, which involves\nselecting landmark points from large data sets. The existing approaches to\nselecting landmarks are typically computationally demanding as they require\nmanipulating and performing computations with large matrices in the input or\nfeature space. In this paper, our contribution is twofold. The first\ncontribution is to propose a novel landmark selection method that promotes\ndiversity using an efficient two-step approach. Our landmark selection\ntechnique follows a coarse to fine strategy, where the first step computes\nimportance scores with a single pass over the whole data. The second step\nperforms K-means clustering on the constructed coreset to use the obtained\ncentroids as landmarks. Hence, the introduced method provides tunable\ntrade-offs between accuracy and efficiency. Our second contribution is to\ninvestigate the performance of several landmark selection techniques using a\nnovel application of kernel methods for predicting structural responses due to\nearthquake load and material uncertainties. Our experiments exhibit the merits\nof our proposed landmark selection scheme against baselines.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 01:44:56 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Pourkamali-Anaraki", "Farhad", ""], ["Hariri-Ardebili", "Mohammad Amin", ""], ["Morawiec", "Lydia", ""]]}, {"id": "2009.09137", "submitter": "Yeejin Lee", "authors": "Yeejin Lee, and Keigo Hirakawa", "title": "Lossless White Balance For Improved Lossless CFA Image and Video\n  Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color filter array is spatial multiplexing of pixel-sized filters placed over\npixel detectors in camera sensors. The state-of-the-art lossless coding\ntechniques of raw sensor data captured by such sensors leverage spatial or\ncross-color correlation using lifting schemes. In this paper, we propose a\nlifting-based lossless white balance algorithm. When applied to the raw sensor\ndata, the spatial bandwidth of the implied chrominance signals decreases. We\npropose to use this white balance as a pre-processing step to lossless CFA\nsubsampled image/video compression, improving the overall coding efficiency of\nthe raw sensor data.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 01:47:26 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Lee", "Yeejin", ""], ["Hirakawa", "Keigo", ""]]}, {"id": "2009.09140", "submitter": "Jindong Gu", "authors": "Jindong Gu and Zhiliang Wu and Volker Tresp", "title": "Introspective Learning by Distilling Knowledge from Online\n  Self-explanation", "comments": null, "journal-ref": "15th Asian Conference on Computer Vision (ACCV) 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, many explanation methods have been proposed to explain\nindividual classifications of deep neural networks. However, how to leverage\nthe created explanations to improve the learning process has been less\nexplored. As the privileged information, the explanations of a model can be\nused to guide the learning process of the model itself. In the community,\nanother intensively investigated privileged information used to guide the\ntraining of a model is the knowledge from a powerful teacher model. The goal of\nthis work is to leverage the self-explanation to improve the learning process\nby borrowing ideas from knowledge distillation. We start by investigating the\neffective components of the knowledge transferred from the teacher network to\nthe student network. Our investigation reveals that both the responses in\nnon-ground-truth classes and class-similarity information in teacher's outputs\ncontribute to the success of the knowledge distillation. Motivated by the\nconclusion, we propose an implementation of introspective learning by\ndistilling knowledge from online self-explanations. The models trained with the\nintrospective learning procedure outperform the ones trained with the standard\nlearning procedure, as well as the ones trained with different regularization\nmethods. When compared to the models learned from peer networks or teacher\nnetworks, our models also show competitive performance and requires neither\npeers nor teachers.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 02:05:32 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Gu", "Jindong", ""], ["Wu", "Zhiliang", ""], ["Tresp", "Volker", ""]]}, {"id": "2009.09169", "submitter": "Wenyan Cong", "authors": "Wenyan Cong, Li Niu, Jianfu Zhang, Jing Liang, Liqing Zhang", "title": "BargainNet: Background-Guided Domain Translation for Image Harmonization", "comments": "Accepted by ICME2021 as Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image composition is a fundamental operation in image editing field. However,\nunharmonious foreground and background downgrade the quality of composite\nimage. Image harmonization, which adjusts the foreground to improve the\nconsistency, is an essential yet challenging task. Previous deep learning based\nmethods mainly focus on directly learning the mapping from composite image to\nreal image, while ignoring the crucial guidance role that background plays. In\nthis work, with the assumption that the foreground needs to be translated to\nthe same domain as background, we formulate image harmonization task as\nbackground-guided domain translation. Therefore, we propose an image\nharmonization network with a novel domain code extractor and well-tailored\ntriplet losses, which could capture the background domain information to guide\nthe foreground harmonization. Extensive experiments on the existing image\nharmonization benchmark demonstrate the effectiveness of our proposed method.\nCode is available at https://github.com/bcmi/BargainNet.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 05:14:08 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 14:56:09 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Cong", "Wenyan", ""], ["Niu", "Li", ""], ["Zhang", "Jianfu", ""], ["Liang", "Jing", ""], ["Zhang", "Liqing", ""]]}, {"id": "2009.09172", "submitter": "Detlef Schmicker", "authors": "Detlef Schmicker", "title": "Few-shot learning using pre-training and shots, enriched by pre-trained\n  samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We use the EMNIST dataset of handwritten digits to test a simple approach for\nfew-shot learning. A fully connected neural network is pre-trained with a\nsubset of the 10 digits and used for few-shot learning with untrained digits.\nTwo basic ideas are introduced: during few-shot learning the learning of the\nfirst layer is disabled, and for every shot a previously unknown digit is used\ntogether with four previously trained digits for the gradient descend, until a\npredefined threshold condition is fulfilled. This way we reach about 90%\naccuracy after 10 shots.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 06:08:07 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Schmicker", "Detlef", ""]]}, {"id": "2009.09179", "submitter": "Chongyang Wang", "authors": "Min Peng, Chongyang Wang, Yuan Gao, Tao Bi, Tong Chen, Yu Shi,\n  Xiang-Dong Zhou", "title": "Recognizing Micro-Expression in Video Clip with Adaptive Key-Frame\n  Mining", "comments": "Submitted for Review in IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a spontaneous expression of emotion on face, micro-expression reveals the\nunderlying emotion that cannot be controlled by human. In micro-expression,\nfacial movement is transient and sparsely localized through time. However, the\nexisting representation based on various deep learning techniques learned from\na full video clip is usually redundant. In addition, methods utilizing the\nsingle apex frame of each video clip require expert annotations and sacrifice\nthe temporal dynamics. To simultaneously localize and recognize such fleeting\nfacial movements, we propose a novel end-to-end deep learning architecture,\nreferred to as adaptive key-frame mining network (AKMNet). Operating on the\nvideo clip of micro-expression, AKMNet is able to learn discriminative\nspatio-temporal representation by combining spatial features of self-learned\nlocal key frames and their global-temporal dynamics. Theoretical analysis and\nempirical evaluation show that the proposed approach improved recognition\naccuracy in comparison with state-of-the-art methods on multiple benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 07:03:16 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 14:38:06 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 07:53:54 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Peng", "Min", ""], ["Wang", "Chongyang", ""], ["Gao", "Yuan", ""], ["Bi", "Tao", ""], ["Chen", "Tong", ""], ["Shi", "Yu", ""], ["Zhou", "Xiang-Dong", ""]]}, {"id": "2009.09182", "submitter": "Zhihang Yuan", "authors": "Zhihang Yuan, Xin Liu, Bingzhe Wu, Guangyu Sun", "title": "ENAS4D: Efficient Multi-stage CNN Architecture Search for Dynamic\n  Inference", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic inference is a feasible way to reduce the computational cost of\nconvolutional neural network(CNN), which can dynamically adjust the computation\nfor each input sample. One of the ways to achieve dynamic inference is to use\nmulti-stage neural network, which contains a sub-network with prediction layer\nat each stage. The inference of a input sample can exit from early stage if the\nprediction of the stage is confident enough. However, design a multi-stage CNN\narchitecture is a non-trivial task. In this paper, we introduce a general\nframework, ENAS4D, which can efficiently search for optimal multi-stage CNN\narchitecture for dynamic inference in a well-designed search space. Firstly, we\npropose a method to construct the search space with multi-stage convolution.\nThe search space include different numbers of layers, different kernel sizes\nand different numbers of channels for each stage and the resolution of input\nsamples. Then, we train a once-for-all network that supports to sample diverse\nmulti-stage CNN architecture. A specialized multi-stage network can be obtained\nfrom the once-for-all network without additional training. Finally, we devise a\nmethod to efficiently search for the optimal multi-stage network that trades\nthe accuracy off the computational cost taking the advantage of once-for-all\nnetwork. The experiments on the ImageNet classification task demonstrate that\nthe multi-stage CNNs searched by ENAS4D consistently outperform the\nstate-of-the-art method for dyanmic inference. In particular, the network\nachieves 74.4% ImageNet top-1 accuracy under 185M average MACs.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 08:08:26 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Yuan", "Zhihang", ""], ["Liu", "Xin", ""], ["Wu", "Bingzhe", ""], ["Sun", "Guangyu", ""]]}, {"id": "2009.09193", "submitter": "Kai Li Lim", "authors": "Kai Li Lim and Thomas Br\\\"aunl", "title": "A Review of Visual Odometry Methods and Its Applications for Autonomous\n  Driving", "comments": "15 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research into autonomous driving applications has observed an increase in\ncomputer vision-based approaches in recent years. In attempts to develop\nexclusive vision-based systems, visual odometry is often considered as a key\nelement to achieve motion estimation and self-localisation, in place of wheel\nodometry or inertial measurements. This paper presents a recent review to\nmethods that are pertinent to visual odometry with an emphasis on autonomous\ndriving. This review covers visual odometry in their monocular, stereoscopic\nand visual-inertial form, individually presenting them with analyses related to\ntheir applications. Discussions are drawn to outline the problems faced in the\ncurrent state of research, and to summarise the works reviewed. This paper\nconcludes with future work suggestions to aid prospective developments in\nvisual odometry.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 09:13:27 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Lim", "Kai Li", ""], ["Br\u00e4unl", "Thomas", ""]]}, {"id": "2009.09196", "submitter": "Sheng Wan", "authors": "Sheng Wan and Chen Gong and Shirui Pan and Jie Yang and Jian Yang", "title": "Multi-Level Graph Convolutional Network with Automatic Graph Learning\n  for Hyperspectral Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, deep learning methods, especially the Graph Convolutional Network\n(GCN), have shown impressive performance in hyperspectral image (HSI)\nclassification. However, the current GCN-based methods treat graph construction\nand image classification as two separate tasks, which often results in\nsuboptimal performance. Another defect of these methods is that they mainly\nfocus on modeling the local pairwise importance between graph nodes while lack\nthe capability to capture the global contextual information of HSI. In this\npaper, we propose a Multi-level GCN with Automatic Graph Learning method\n(MGCN-AGL) for HSI classification, which can automatically learn the graph\ninformation at both local and global levels. By employing attention mechanism\nto characterize the importance among spatially neighboring regions, the most\nrelevant information can be adaptively incorporated to make decisions, which\nhelps encode the spatial context to form the graph information at local level.\nMoreover, we utilize multiple pathways for local-level graph convolution, in\norder to leverage the merits from the diverse spatial context of HSI and to\nenhance the expressive power of the generated representations. To reconstruct\nthe global contextual relations, our MGCN-AGL encodes the long range\ndependencies among image regions based on the expressive representations that\nhave been produced at local level. Then inference can be performed along the\nreconstructed graph edges connecting faraway regions. Finally, the multi-level\ninformation is adaptively fused to generate the network output. In this means,\nthe graph learning and image classification can be integrated into a unified\nframework and benefit each other. Extensive experiments have been conducted on\nthree real-world hyperspectral datasets, which are shown to outperform the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 09:26:20 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Wan", "Sheng", ""], ["Gong", "Chen", ""], ["Pan", "Shirui", ""], ["Yang", "Jie", ""], ["Yang", "Jian", ""]]}, {"id": "2009.09197", "submitter": "Junjie Chen", "authors": "Junjie Chen, Li Niu, Liu Liu, Liqing Zhang", "title": "Weak-shot Fine-grained Classification via Similarity Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing fine-grained categories remains a challenging task, due to the\nsubtle distinctions among different subordinate categories, which results in\nthe need of abundant annotated samples. To alleviate the data-hungry problem,\nwe consider the problem of learning novel categories from web data with the\nsupport of a clean set of base categories, which is referred to as weak-shot\nlearning. Under this setting, we propose to transfer pairwise semantic\nsimilarity from base categories to novel categories, because this similarity is\nhighly transferable and beneficial for learning from web data. Specifically, we\nfirstly train a similarity net on clean data, and then employ two simple yet\neffective strategies to leverage the transferred similarity to denoise web\ntraining data. In addition, we apply adversarial loss on similarity net to\nenhance the transferability of similarity. Comprehensive experiments on three\nfine-grained datasets demonstrate that we could dramatically facilitate webly\nsupervised learning by a clean set and similarity transfer is effective under\nthis setting.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 09:31:52 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Chen", "Junjie", ""], ["Niu", "Li", ""], ["Liu", "Liu", ""], ["Zhang", "Liqing", ""]]}, {"id": "2009.09205", "submitter": "Liming Zhai", "authors": "Liming Zhai, Felix Juefei-Xu, Qing Guo, Xiaofei Xie, Lei Ma, Wei Feng,\n  Shengchao Qin, Yang Liu", "title": "It's Raining Cats or Dogs? Adversarial Rain Attack on DNN Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain is a common phenomenon in nature and an essential factor for many deep\nneural network (DNN) based perception systems. Rain can often post inevitable\nthreats that must be carefully addressed especially in the context of safety\nand security-sensitive scenarios (e.g., autonomous driving). Therefore, a\ncomprehensive investigation of the potential risks of the rain to a DNN is of\ngreat importance. Unfortunately, in practice, it is often rather difficult to\ncollect or synthesize rainy images that can represent all raining situations\nthat possibly occur in the real world. To this end, in this paper, we start\nfrom a new perspective and propose to combine two totally different studies,\ni.e., rainy image synthesis and adversarial attack. We present an adversarial\nrain attack, with which we could simulate various rainy situations with the\nguidance of deployed DNNs and reveal the potential threat factors that can be\nbrought by rain, helping to develop more rain-robust DNNs. In particular, we\npropose a factor-aware rain generation that simulates rain steaks according to\nthe camera exposure process and models the learnable rain factors for\nadversarial attack. With this generator, we further propose the adversarial\nrain attack against the image classification and object detection, where the\nrain factors are guided by the various DNNs. As a result, it enables to\ncomprehensively study the impacts of the rain factors to DNNs. Our largescale\nevaluation on three datasets, i.e., NeurIPS'17 DEV, MS COCO and KITTI,\ndemonstrates that our synthesized rainy images can not only present visually\nrealistic appearances, but also exhibit strong adversarial capability, which\nbuilds the foundation for further rain-robust perception studies.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 10:12:08 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhai", "Liming", ""], ["Juefei-Xu", "Felix", ""], ["Guo", "Qing", ""], ["Xie", "Xiaofei", ""], ["Ma", "Lei", ""], ["Feng", "Wei", ""], ["Qin", "Shengchao", ""], ["Liu", "Yang", ""]]}, {"id": "2009.09209", "submitter": "Kengo Machida", "authors": "Kengo Machida, Kuniaki Uto, Koichi Shinoda and Taiji Suzuki", "title": "MSR-DARTS: Minimum Stable Rank of Differentiable Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neural architecture search (NAS), differentiable architecture search\n(DARTS) has recently attracted much attention due to its high efficiency. It\ndefines an over-parameterized network with mixed edges, each of which\nrepresents all operator candidates, and jointly optimizes the weights of the\nnetwork and its architecture in an alternating manner. However, this method\nfinds a model with the weights converging faster than the others, and such a\nmodel with fastest convergence often leads to overfitting. Accordingly, the\nresulting model cannot always be well-generalized. To overcome this problem, we\npropose a method called minimum stable rank DARTS (MSR-DARTS), for finding a\nmodel with the best generalization error by replacing architecture optimization\nwith the selection process using the minimum stable rank criterion.\nSpecifically, a convolution operator is represented by a matrix, and MSR-DARTS\nselects the one with the smallest stable rank. We evaluated MSR-DARTS on\nCIFAR-10 and ImageNet datasets. It achieves an error rate of 2.54% with 4.0M\nparameters within 0.3 GPU-days on CIFAR-10, and a top-1 error rate of 23.9% on\nImageNet. The official code is available at\nhttps://github.com/mtaecchhi/msrdarts.git.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 11:03:39 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 08:58:01 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Machida", "Kengo", ""], ["Uto", "Kuniaki", ""], ["Shinoda", "Koichi", ""], ["Suzuki", "Taiji", ""]]}, {"id": "2009.09213", "submitter": "Yihao Huang", "authors": "Yihao Huang, Felix Juefei-Xu, Qing Guo, Xiaofei Xie, Lei Ma, Weikai\n  Miao, Yang Liu, Geguang Pu", "title": "FakeRetouch: Evading DeepFakes Detection via the Guidance of Deliberate\n  Noise", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The novelty and creativity of DeepFake generation techniques have attracted\nworldwide media attention. Many researchers focus on detecting fake images\nproduced by these GAN-based image generation methods with fruitful results,\nindicating that the GAN-based image generation methods are not yet perfect.\nMany studies show that the upsampling procedure used in the decoder of\nGAN-based image generation methods inevitably introduce artifact patterns into\nfake images. In order to further improve the fidelity of DeepFake images, in\nthis work, we propose a simple yet powerful framework to reduce the artifact\npatterns of fake images without hurting image quality. The method is based on\nan important observation that adding noise to a fake image can successfully\nreduce the artifact patterns in both spatial and frequency domains. Thus we use\na combination of additive noise and deep image filtering to reconstruct the\nfake images, and we name our method FakeRetouch. The deep image filtering\nprovides a specialized filter for each pixel in the noisy image, taking full\nadvantages of deep learning. The deeply filtered images retain very high\nfidelity to their DeepFake counterparts. Moreover, we use the semantic\ninformation of the image to generate an adversarial guidance map to add noise\nintelligently. Our method aims at improving the fidelity of DeepFake images and\nexposing the problems of existing DeepFake detection methods, and we hope that\nthe found vulnerabilities can help improve the future generation DeepFake\ndetection methods.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 11:26:01 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Huang", "Yihao", ""], ["Juefei-Xu", "Felix", ""], ["Guo", "Qing", ""], ["Xie", "Xiaofei", ""], ["Ma", "Lei", ""], ["Miao", "Weikai", ""], ["Liu", "Yang", ""], ["Pu", "Geguang", ""]]}, {"id": "2009.09231", "submitter": "Yupeng Cheng", "authors": "Yupeng Cheng, Felix Juefei-Xu, Qing Guo, Huazhu Fu, Xiaofei Xie,\n  Shang-Wei Lin, Weisi Lin, Yang Liu", "title": "Adversarial Exposure Attack on Diabetic Retinopathy Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic retinopathy (DR) is a leading cause of vision loss in the world and\nnumerous cutting-edge works have built powerful deep neural networks (DNNs) to\nautomatically classify the DR cases via the retinal fundus images (RFIs).\nHowever, RFIs are usually affected by the widely existing camera exposure while\nthe robustness of DNNs to the exposure is rarely explored. In this paper, we\nstudy this problem from the viewpoint of adversarial attack and identify a\ntotally new task, i.e., adversarial exposure attack generating adversarial\nimages by tuning image exposure to mislead the DNNs with significantly high\ntransferability. To this end, we first implement a straightforward method,\ni.e., multiplicative-perturbation-based exposure attack, and reveal the big\nchallenges of this new task. Then, to make the adversarial image naturalness,\nwe propose the adversarial bracketed exposure fusion that regards the exposure\nattack as an element-wise bracketed exposure fusion problem in the\nLaplacian-pyramid space. Moreover, to realize high transferability, we further\npropose the convolutional bracketed exposure fusion where the element-wise\nmultiplicative operation is extended to the convolution. We validate our method\non the real public DR dataset with the advanced DNNs, e.g., ResNet50,\nMobileNet, and EfficientNet, showing our method can achieve high image quality\nand success rate of the transfer attack. Our method reveals the potential\nthreats to the DNN-based DR automated diagnosis and can definitely benefit the\ndevelopment of exposure-robust automated DR diagnosis method in the future.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 13:47:33 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Cheng", "Yupeng", ""], ["Juefei-Xu", "Felix", ""], ["Guo", "Qing", ""], ["Fu", "Huazhu", ""], ["Xie", "Xiaofei", ""], ["Lin", "Shang-Wei", ""], ["Lin", "Weisi", ""], ["Liu", "Yang", ""]]}, {"id": "2009.09235", "submitter": "Nils Keunecke", "authors": "Nils Keunecke and S. Hamidreza Kasaei", "title": "Open-Ended Fine-Grained 3D Object Categorization by Combining Shape and\n  Texture Features in Multiple Colorspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a consequence of an ever-increasing number of service robots, there is a\ngrowing demand for highly accurate real-time 3D object recognition. Considering\nthe expansion of robot applications in more complex and dynamic environments,it\nis evident that it is not possible to pre-program all object categories and\nanticipate all exceptions in advance. Therefore, robots should have the\nfunctionality to learn about new object categories in an open-ended fashion\nwhile working in the environment.Towards this goal, we propose a deep transfer\nlearning approach to generate a scale- and pose-invariant object representation\nby considering shape and texture information in multiple colorspaces. The\nobtained global object representation is then fed to an instance-based object\ncategory learning and recognition,where a non-expert human user exists in the\nlearning loop and can interactively guide the process of experience acquisition\nby teaching new object categories, or by correcting insufficient or erroneous\ncategories. In this work, shape information encodes the common patterns of all\ncategories, while texture information is used to describes the appearance of\neach instance in detail.Multiple color space combinations and network\narchitectures are evaluated to find the most descriptive system. Experimental\nresults showed that the proposed network architecture out-performed the\nselected state-of-the-art approaches in terms of object classification accuracy\nand scalability. Furthermore, we performed a real robot experiment in the\ncontext of serve-a-beer scenario to show the real-time performance of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 14:06:18 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 16:17:28 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 19:54:03 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Keunecke", "Nils", ""], ["Kasaei", "S. Hamidreza", ""]]}, {"id": "2009.09237", "submitter": "Heon Song", "authors": "Heon Song, Daiki Suehiro and Seiichi Uchida", "title": "AAA: Adaptive Aggregation of Arbitrary Online Trackers with Theoretical\n  Performance Guarantee", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For visual object tracking, it is difficult to realize an almighty online\ntracker due to the huge variations of target appearance depending on an image\nsequence. This paper proposes an online tracking method that adaptively\naggregates arbitrary multiple online trackers. The performance of the proposed\nmethod is theoretically guaranteed to be comparable to that of the best tracker\nfor any image sequence, although the best expert is unknown during tracking.\nThe experimental study on the large variations of benchmark datasets and\naggregated trackers demonstrates that the proposed method can achieve\nstate-of-the-art performance. The code is available at\nhttps://github.com/songheony/AAA-journal.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 14:16:01 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 04:22:11 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Song", "Heon", ""], ["Suehiro", "Daiki", ""], ["Uchida", "Seiichi", ""]]}, {"id": "2009.09238", "submitter": "Qing Guo", "authors": "Qing Guo, Jingyang Sun, Felix Juefei-Xu, Lei Ma, Xiaofei Xie, Wei\n  Feng, Yang Liu", "title": "EfficientDeRain: Learning Pixel-wise Dilation Filtering for\n  High-Efficiency Single-Image Deraining", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-image deraining is rather challenging due to the unknown rain model.\nExisting methods often make specific assumptions of the rain model, which can\nhardly cover many diverse circumstances in the real world, making them have to\nemploy complex optimization or progressive refinement. This, however,\nsignificantly affects these methods' efficiency and effectiveness for many\nefficiency-critical applications. To fill this gap, in this paper, we regard\nthe single-image deraining as a general image-enhancing problem and originally\npropose a model-free deraining method, i.e., EfficientDeRain, which is able to\nprocess a rainy image within 10~ms (i.e., around 6~ms on average), over 80\ntimes faster than the state-of-the-art method (i.e., RCDNet), while achieving\nsimilar de-rain effects. We first propose the novel pixel-wise dilation\nfiltering. In particular, a rainy image is filtered with the pixel-wise kernels\nestimated from a kernel prediction network, by which suitable multi-scale\nkernels for each pixel can be efficiently predicted. Then, to eliminate the gap\nbetween synthetic and real data, we further propose an effective data\naugmentation method (i.e., RainMix) that helps to train network for real rainy\nimage handling.We perform comprehensive evaluation on both synthetic and\nreal-world rainy datasets to demonstrate the effectiveness and efficiency of\nour method. We release the model and code in\nhttps://github.com/tsingqguo/efficientderain.git.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 14:32:50 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Guo", "Qing", ""], ["Sun", "Jingyang", ""], ["Juefei-Xu", "Felix", ""], ["Ma", "Lei", ""], ["Xie", "Xiaofei", ""], ["Feng", "Wei", ""], ["Liu", "Yang", ""]]}, {"id": "2009.09247", "submitter": "Qing Guo", "authors": "Binyu Tian, Qing Guo, Felix Juefei-Xu, Wen Le Chan, Yupeng Cheng,\n  Xiaohong Li, Xiaofei Xie, Shengchao Qin", "title": "Bias Field Poses a Threat to DNN-based X-Ray Recognition", "comments": "6 pages, 5 figures; This work has been accepted to ICME 2021 as the\n  oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chest X-ray plays a key role in screening and diagnosis of many lung\ndiseases including the COVID-19. More recently, many works construct deep\nneural networks (DNNs) for chest X-ray images to realize automated and\nefficient diagnosis of lung diseases. However, bias field caused by the\nimproper medical image acquisition process widely exists in the chest X-ray\nimages while the robustness of DNNs to the bias field is rarely explored, which\ndefinitely poses a threat to the X-ray-based automated diagnosis system. In\nthis paper, we study this problem based on the recent adversarial attack and\npropose a brand new attack, i.e., the adversarial bias field attack where the\nbias field instead of the additive noise works as the adversarial perturbations\nfor fooling the DNNs. This novel attack posts a key problem: how to locally\ntune the bias field to realize high attack success rate while maintaining its\nspatial smoothness to guarantee high realisticity. These two goals contradict\neach other and thus has made the attack significantly challenging. To overcome\nthis challenge, we propose the adversarial-smooth bias field attack that can\nlocally tune the bias field with joint smooth & adversarial constraints. As a\nresult, the adversarial X-ray images can not only fool the DNNs effectively but\nalso retain very high level of realisticity. We validate our method on real\nchest X-ray datasets with powerful DNNs, e.g., ResNet50, DenseNet121, and\nMobileNet, and show different properties to the state-of-the-art attacks in\nboth image realisticity and attack transferability. Our method reveals the\npotential threat to the DNN-based X-ray automated diagnosis and can definitely\nbenefit the development of bias-field-robust automated diagnosis system.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 14:58:02 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 04:00:36 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Tian", "Binyu", ""], ["Guo", "Qing", ""], ["Juefei-Xu", "Felix", ""], ["Chan", "Wen Le", ""], ["Cheng", "Yupeng", ""], ["Li", "Xiaohong", ""], ["Xie", "Xiaofei", ""], ["Qin", "Shengchao", ""]]}, {"id": "2009.09255", "submitter": "Canh Le Duc", "authors": "Duc Canh Le, Chan Hyun Youn", "title": "City-Scale Visual Place Recognition with Deep Local Features Based on\n  Multi-Scale Ordered VLAD Pooling", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual place recognition is the task of recognizing a place depicted in an\nimage based on its pure visual appearance without metadata. In visual place\nrecognition, the challenges lie upon not only the changes in lighting\nconditions, camera viewpoint, and scale, but also the characteristic of scene\nlevel images and the distinct features of the area. To resolve these\nchallenges, one must consider both the local discriminativeness and the global\nsemantic context of images. On the other hand, the diversity of the datasets is\nalso particularly important to develop more general models and advance the\nprogress of the field. In this paper, we present a fully-automated system for\nplace recognition at a city-scale based on content-based image retrieval. Our\nmain contributions to the community lie in three aspects. Firstly, we take a\ncomprehensive analysis of visual place recognition and sketch out the unique\nchallenges of the task compared to general image retrieval tasks. Next, we\npropose yet a simple pooling approach on top of convolutional neural network\nactivations to embed the spatial information into the image representation\nvector. Finally, we introduce new datasets for place recognition, which are\nparticularly essential for application-based research. Furthermore, throughout\nextensive experiments, various issues in both image retrieval and place\nrecognition are analyzed and discussed to give some insights for improving the\nperformance of retrieval models in reality.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 15:21:59 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Le", "Duc Canh", ""], ["Youn", "Chan Hyun", ""]]}, {"id": "2009.09258", "submitter": "Ruijun Gao", "authors": "Ruijun Gao, Qing Guo, Felix Juefei-Xu, Hongkai Yu, Huazhu Fu, Wei\n  Feng, Yang Liu, Song Wang", "title": "Making Images Undiscoverable from Co-Saliency Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-salient object detection (CoSOD) has recently achieved significant\nprogress and played a key role in retrieval-related tasks. However, it\ninevitably poses an entirely new safety and security issue, i.e., highly\npersonal and sensitive content can potentially be extracting by powerful CoSOD\nmethods. In this paper, we address this problem from the perspective of\nadversarial attacks and identify a novel task: adversarial co-saliency attack.\nSpecially, given an image selected from a group of images containing some\ncommon and salient objects, we aim to generate an adversarial version that can\nmislead CoSOD methods to predict incorrect co-salient regions. Note that,\ncompared with general white-box adversarial attacks for classification, this\nnew task faces two additional challenges: (1) low success rate due to the\ndiverse appearance of images in the group; (2) low transferability across CoSOD\nmethods due to the considerable difference between CoSOD pipelines. To address\nthese challenges, we propose the very first black-box joint adversarial\nexposure and noise attack (Jadena), where we jointly and locally tune the\nexposure and additive perturbations of the image according to a newly designed\nhigh-feature-level contrast-sensitive loss function. Our method, without any\ninformation on the state-of-the-art CoSOD methods, leads to significant\nperformance degradation on various co-saliency detection datasets and makes the\nco-salient objects undetectable. This can have strong practical benefits in\nproperly securing the large number of personal photos currently shared on the\ninternet.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 15:43:46 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 02:52:33 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Gao", "Ruijun", ""], ["Guo", "Qing", ""], ["Juefei-Xu", "Felix", ""], ["Yu", "Hongkai", ""], ["Fu", "Huazhu", ""], ["Feng", "Wei", ""], ["Liu", "Yang", ""], ["Wang", "Song", ""]]}, {"id": "2009.09266", "submitter": "Johannes Schneider", "authors": "Johannes Schneider", "title": "Humans learn too: Better Human-AI Interaction using Optimized Human\n  Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans rely more and more on systems with AI components. The AI community\ntypically treats human inputs as a given and optimizes AI models only. This\nthinking is one-sided and it neglects the fact that humans can learn, too. In\nthis work, human inputs are optimized for better interaction with an AI model\nwhile keeping the model fixed. The optimized inputs are accompanied by\ninstructions on how to create them. They allow humans to save time and cut on\nerrors, while keeping required changes to original inputs limited. We propose\ncontinuous and discrete optimization methods modifying samples in an iterative\nfashion. Our quantitative and qualitative evaluation including a human study on\ndifferent hand-generated inputs shows that the generated proposals lead to\nlower error rates, require less effort to create and differ only modestly from\nthe original samples.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 16:30:37 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Schneider", "Johannes", ""]]}, {"id": "2009.09282", "submitter": "Nan Wu", "authors": "Nan Wu and Zhe Huang and Yiqiu Shen and Jungkyu Park and Jason Phang\n  and Taro Makino and S. Gene Kim and Kyunghyun Cho and Laura Heacock and Linda\n  Moy and Krzysztof J. Geras", "title": "Reducing false-positive biopsies with deep neural networks that utilize\n  local and global information in screening mammograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is the most common cancer in women, and hundreds of thousands\nof unnecessary biopsies are done around the world at a tremendous cost. It is\ncrucial to reduce the rate of biopsies that turn out to be benign tissue. In\nthis study, we build deep neural networks (DNNs) to classify biopsied lesions\nas being either malignant or benign, with the goal of using these networks as\nsecond readers serving radiologists to further reduce the number of false\npositive findings. We enhance the performance of DNNs that are trained to learn\nfrom small image patches by integrating global context provided in the form of\nsaliency maps learned from the entire image into their reasoning, similar to\nhow radiologists consider global context when evaluating areas of interest. Our\nexperiments are conducted on a dataset of 229,426 screening mammography exams\nfrom 141,473 patients. We achieve an AUC of 0.8 on a test set consisting of 464\nbenign and 136 malignant lesions.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 18:54:01 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Wu", "Nan", ""], ["Huang", "Zhe", ""], ["Shen", "Yiqiu", ""], ["Park", "Jungkyu", ""], ["Phang", "Jason", ""], ["Makino", "Taro", ""], ["Kim", "S. Gene", ""], ["Cho", "Kyunghyun", ""], ["Heacock", "Laura", ""], ["Moy", "Linda", ""], ["Geras", "Krzysztof J.", ""]]}, {"id": "2009.09283", "submitter": "Kang Liu", "authors": "Kang Liu, Benjamin Tan, Siddharth Garg", "title": "Subverting Privacy-Preserving GANs: Hiding Secrets in Sanitized Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unprecedented data collection and sharing have exacerbated privacy concerns\nand led to increasing interest in privacy-preserving tools that remove\nsensitive attributes from images while maintaining useful information for other\ntasks. Currently, state-of-the-art approaches use privacy-preserving generative\nadversarial networks (PP-GANs) for this purpose, for instance, to enable\nreliable facial expression recognition without leaking users' identity.\nHowever, PP-GANs do not offer formal proofs of privacy and instead rely on\nexperimentally measuring information leakage using classification accuracy on\nthe sensitive attributes of deep learning (DL)-based discriminators. In this\nwork, we question the rigor of such checks by subverting existing\nprivacy-preserving GANs for facial expression recognition. We show that it is\npossible to hide the sensitive identification data in the sanitized output\nimages of such PP-GANs for later extraction, which can even allow for\nreconstruction of the entire input images, while satisfying privacy checks. We\ndemonstrate our approach via a PP-GAN-based architecture and provide\nqualitative and quantitative evaluations using two public datasets. Our\nexperimental results raise fundamental questions about the need for more\nrigorous privacy checks of PP-GANs, and we provide insights into the social\nimpact of these.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 19:02:17 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Liu", "Kang", ""], ["Tan", "Benjamin", ""], ["Garg", "Siddharth", ""]]}, {"id": "2009.09289", "submitter": "Youshan Zhang", "authors": "Youshan Zhang and Brian D. Davison", "title": "Adversarial Consistent Learning on Partial Domain Adaptation of\n  PlantCLEF 2020 Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation is one of the most crucial techniques to mitigate the\ndomain shift problem, which exists when transferring knowledge from an abundant\nlabeled sourced domain to a target domain with few or no labels. Partial domain\nadaptation addresses the scenario when target categories are only a subset of\nsource categories. In this paper, to enable the efficient representation of\ncross-domain plant images, we first extract deep features from pre-trained\nmodels and then develop adversarial consistent learning ($ACL$) in a unified\ndeep architecture for partial domain adaptation. It consists of source domain\nclassification loss, adversarial learning loss, and feature consistency loss.\nAdversarial learning loss can maintain domain-invariant features between the\nsource and target domains. Moreover, feature consistency loss can preserve the\nfine-grained feature transition between two domains. We also find the shared\ncategories of two domains via down-weighting the irrelevant categories in the\nsource domain. Experimental results demonstrate that training features from\nNASNetLarge model with proposed $ACL$ architecture yields promising results on\nthe PlantCLEF 2020 Challenge.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 19:57:41 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhang", "Youshan", ""], ["Davison", "Brian D.", ""]]}, {"id": "2009.09300", "submitter": "Thyagharajan K K", "authors": "S. Kavitha, K.K. Thyagharajan", "title": "Features based Mammogram Image Classification using Weighted Feature\n  Support Vector Machine", "comments": "9 pages, 3 figures, \"submitted to International Conference on\n  Computing and Communication Systems\"", "journal-ref": "Vol. 270, 2012, 320-329", "doi": "10.1007/978-3-642-29216-3_35", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the existing research of mammogram image classification, either clinical\ndata or image features of a specific type is considered along with the\nsupervised classifiers such as Neural Network (NN) and Support Vector Machine\n(SVM). This paper considers automated classification of breast tissue type as\nbenign or malignant using Weighted Feature Support Vector Machine (WFSVM)\nthrough constructing the precomputed kernel function by assigning more weight\nto relevant features using the principle of maximizing deviations. Initially,\nMIAS dataset of mammogram images is divided into training and test set, then\nthe preprocessing techniques such as noise removal and background removal are\napplied to the input images and the Region of Interest (ROI) is identified. The\nstatistical features and texture features are extracted from the ROI and the\nclinical features are obtained directly from the dataset. The extracted\nfeatures of the training dataset are used to construct the weighted features\nand precomputed linear kernel for training the WFSVM, from which the training\nmodel file is created. Using this model file the kernel matrix of test samples\nis classified as benign or malignant. This analysis shows that the texture\nfeatures have resulted in better accuracy than the other features with WFSVM\nand SVM. However, the number of support vectors created in WFSVM is less than\nthe SVM classifier.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 21:28:31 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Kavitha", "S.", ""], ["Thyagharajan", "K. K.", ""]]}, {"id": "2009.09312", "submitter": "Riccardo Marin", "authors": "Riccardo Marin, Simone Melzi, Emanuele Rodol\\`a, Umberto Castellani", "title": "High-Resolution Augmentation for Automatic Template-Based Matching of\n  Human Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for 3D shape matching of deformable human shapes.\nOur approach is based on the joint adoption of three different tools: an\nintrinsic spectral matching pipeline, a morphable model, and an extrinsic\ndetails refinement. By operating in conjunction, these tools allow us to\ngreatly improve the quality of the matching while at the same time resolving\nthe key issues exhibited by each tool individually. In this paper we present an\ninnovative High-Resolution Augmentation (HRA) strategy that enables highly\naccurate correspondence even in the presence of significant mesh resolution\nmismatch between the input shapes. This augmentation provides an effective\nworkaround for the resolution limitations imposed by the adopted morphable\nmodel. The HRA in its global and localized versions represents a novel\nrefinement strategy for surface subdivision methods. We demonstrate the\naccuracy of the proposed pipeline on multiple challenging benchmarks, and\nshowcase its effectiveness in surface registration and texture transfer.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 22:41:24 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Marin", "Riccardo", ""], ["Melzi", "Simone", ""], ["Rodol\u00e0", "Emanuele", ""], ["Castellani", "Umberto", ""]]}, {"id": "2009.09318", "submitter": "Anian Ruoss", "authors": "Anian Ruoss, Maximilian Baader, Mislav Balunovi\\'c, Martin Vechev", "title": "Efficient Certification of Spatial Robustness", "comments": "Conference Paper at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has exposed the vulnerability of computer vision models to vector\nfield attacks. Due to the widespread usage of such models in safety-critical\napplications, it is crucial to quantify their robustness against such spatial\ntransformations. However, existing work only provides empirical robustness\nquantification against vector field deformations via adversarial attacks, which\nlack provable guarantees. In this work, we propose novel convex relaxations,\nenabling us, for the first time, to provide a certificate of robustness against\nvector field transformations. Our relaxations are model-agnostic and can be\nleveraged by a wide range of neural network verifiers. Experiments on various\nnetwork architectures and different datasets demonstrate the effectiveness and\nscalability of our method.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 23:09:11 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 00:24:32 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Ruoss", "Anian", ""], ["Baader", "Maximilian", ""], ["Balunovi\u0107", "Mislav", ""], ["Vechev", "Martin", ""]]}, {"id": "2009.09321", "submitter": "Vincent Lostanlen", "authors": "Christopher Ick and Vincent Lostanlen", "title": "Learning a Lie Algebra from Unlabeled Data Pairs", "comments": "2 pages, 1 figure. Presented at the first DeepMath conference, New\n  York City, NY, USA, November 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.SD stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep convolutional networks (convnets) show a remarkable ability to learn\ndisentangled representations. In recent years, the generalization of deep\nlearning to Lie groups beyond rigid motion in $\\mathbb{R}^n$ has allowed to\nbuild convnets over datasets with non-trivial symmetries, such as patterns over\nthe surface of a sphere. However, one limitation of this approach is the need\nto explicitly define the Lie group underlying the desired invariance property\nbefore training the convnet. Whereas rotations on the sphere have a well-known\nsymmetry group ($\\mathrm{SO}(3)$), the same cannot be said of many real-world\nfactors of variability. For example, the disentanglement of pitch, intensity\ndynamics, and playing technique remains a challenging task in music information\nretrieval.\n  This article proposes a machine learning method to discover a nonlinear\ntransformation of the space $\\mathbb{R}^n$ which maps a collection of\n$n$-dimensional vectors $(\\boldsymbol{x}_i)_i$ onto a collection of target\nvectors $(\\boldsymbol{y}_i)_i$. The key idea is to approximate every target\n$\\boldsymbol{y}_i$ by a matrix--vector product of the form\n$\\boldsymbol{\\widetilde{y}}_i = \\boldsymbol{\\phi}(t_i) \\boldsymbol{x}_i$, where\nthe matrix $\\boldsymbol{\\phi}(t_i)$ belongs to a one-parameter subgroup of\n$\\mathrm{GL}_n (\\mathbb{R})$. Crucially, the value of the parameter $t_i \\in\n\\mathbb{R}$ may change between data pairs $(\\boldsymbol{x}_i,\n\\boldsymbol{y}_i)$ and does not need to be known in advance.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 23:23:52 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 02:08:00 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2020 09:29:36 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Ick", "Christopher", ""], ["Lostanlen", "Vincent", ""]]}, {"id": "2009.09333", "submitter": "Liming Zhang", "authors": "Liming Zhang, Liang Zhao, Dieter Pfoser", "title": "Factorized Deep Generative Models for Trajectory Generation with\n  Spatiotemporal-Validity Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory data generation is an important domain that characterizes the\ngenerative process of mobility data. Traditional methods heavily rely on\npredefined heuristics and distributions and are weak in learning unknown\nmechanisms. Inspired by the success of deep generative neural networks for\nimages and texts, a fast-developing research topic is deep generative models\nfor trajectory data which can learn expressively explanatory models for\nsophisticated latent patterns. This is a nascent yet promising domain for many\napplications. We first propose novel deep generative models factorizing\ntime-variant and time-invariant latent variables that characterize global and\nlocal semantics, respectively. We then develop new inference strategies based\non variational inference and constrained optimization to encapsulate the\nspatiotemporal validity. New deep neural network architectures have been\ndeveloped to implement the inference and generation models with\nnewly-generalized latent variable priors. The proposed methods achieved\nsignificant improvements in quantitative and qualitative evaluations in\nextensive experiments.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 02:06:36 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhang", "Liming", ""], ["Zhao", "Liang", ""], ["Pfoser", "Dieter", ""]]}, {"id": "2009.09343", "submitter": "Yang Mingming", "authors": "Tinghuai Ma, Mingming Yang, Huan Rong, Yurong Qian, Yurong Qian, Yuan\n  Tian, NajlaAl-Nabhan", "title": "Dual-path CNN with Max Gated block for Text-Based Person\n  Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-based person re-identification(Re-id) is an important task in video\nsurveillance, which consists of retrieving the corresponding person's image\ngiven a textual description from a large gallery of images. It is difficult to\ndirectly match visual contents with the textual descriptions due to the\nmodality heterogeneity. On the one hand, the textual embeddings are not\ndiscriminative enough, which originates from the high abstraction of the\ntextual descriptions. One the other hand,Global average pooling (GAP) is\ncommonly utilized to extract more general or smoothed features implicitly but\nignores salient local features, which are more important for the cross-modal\nmatching problem. With that in mind, a novel Dual-path CNN with Max Gated block\n(DCMG) is proposed to extract discriminative word embeddings and make\nvisual-textual association concern more on remarkable features of both\nmodalities. The proposed framework is based on two deep residual CNNs jointly\noptimized with cross-modal projection matching (CMPM) loss and cross-modal\nprojection classification (CMPC) loss to embed the two modalities into a joint\nfeature space. First, the pre-trained language model, BERT, is combined with\nthe convolutional neural network (CNN) to learn better word embeddings in the\ntext-to-image matching domain. Second, the global Max pooling (GMP) layer is\napplied to make the visual-textual features focus more on the salient part. To\nfurther alleviate the noise of the maxed-pooled features, the gated block (GB)\nis proposed to produce an attention map that focuses on meaningful features of\nboth modalities. Finally, extensive experiments are conducted on the benchmark\ndataset, CUHK-PEDES, in which our approach achieves the rank-1 score of 55.81%\nand outperforms the state-of-the-art method by 1.3%.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 03:33:29 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Ma", "Tinghuai", ""], ["Yang", "Mingming", ""], ["Rong", "Huan", ""], ["Qian", "Yurong", ""], ["Qian", "Yurong", ""], ["Tian", "Yuan", ""], ["NajlaAl-Nabhan", "", ""]]}, {"id": "2009.09347", "submitter": "Zhecheng Wang", "authors": "Mingxiang Chen, Qichang Chen, Lei Gao, Yilin Chen, Zhecheng Wang", "title": "Predicting Geographic Information with Neural Cellular Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework using neural cellular automata (NCA) to\nregenerate and predict geographic information. The model extends the idea of\nusing NCA to generate/regenerate a specific image by training the model with\nvarious geographic data, and thus, taking the traffic condition map as an\nexample, the model is able to predict traffic conditions by giving certain\ninduction information. Our research verified the analogy between NCA and gene\nin biology, while the innovation of the model significantly widens the boundary\nof possible applications based on NCAs. From our experimental results, the\nmodel shows great potentials in its usability and versatility which are not\navailable in previous studies. The code for model implementation is available\nat https://redacted.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 03:53:48 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Chen", "Mingxiang", ""], ["Chen", "Qichang", ""], ["Gao", "Lei", ""], ["Chen", "Yilin", ""], ["Wang", "Zhecheng", ""]]}, {"id": "2009.09348", "submitter": "Aayush Chaudhary", "authors": "Aayush K. Chaudhary, Jeff B. Pelz", "title": "$pi_t$- Enhancing the Precision of Eye Tracking using Iris Feature\n  Motion Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new high-precision eye-tracking method has been demonstrated recently by\ntracking the motion of iris features rather than by exploiting pupil edges.\nWhile the method provides high precision, it suffers from temporal drift, an\ninability to track across blinks, and loss of texture matches in the presence\nof motion blur. In this work, we present a new methodology $pi_t$ to address\nthese issues by optimally combining the information from both iris textures and\npupil edges. With this method, we show an improvement in precision (S2S-RMS &\nSTD) of at least 48% and 10% respectively while fixating a series of small\ntargets and following a smoothly moving target. Further, we demonstrate the\ncapability in the identification of microsaccades between targets separated by\n0.2-degree.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 04:57:12 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Chaudhary", "Aayush K.", ""], ["Pelz", "Jeff B.", ""]]}, {"id": "2009.09384", "submitter": "Matthias Treder", "authors": "Matthias S. Treder, Juan Mayor-Torres, Christoph Teufel", "title": "Deriving Visual Semantics from Spatial Context: An Adaptation of LSA and\n  Word2Vec to generate Object and Scene Embeddings from Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embeddings are an important tool for the representation of word meaning.\nTheir effectiveness rests on the distributional hypothesis: words that occur in\nthe same context carry similar semantic information. Here, we adapt this\napproach to index visual semantics in images of scenes. To this end, we\nformulate a distributional hypothesis for objects and scenes: Scenes that\ncontain the same objects (object context) are semantically related. Similarly,\nobjects that appear in the same spatial context (within a scene or subregions\nof a scene) are semantically related. We develop two approaches for learning\nobject and scene embeddings from annotated images. In the first approach, we\nadapt LSA and Word2vec's Skipgram and CBOW models to generate two sets of\nembeddings from object co-occurrences in whole images, one for objects and one\nfor scenes. The representational space spanned by these embeddings suggests\nthat the distributional hypothesis holds for images. In an initial application\nof this approach, we show that our image-based embeddings improve scene\nclassification models such as ResNet18 and VGG-11 (3.72\\% improvement on Top5\naccuracy, 4.56\\% improvement on Top1 accuracy). In the second approach, rather\nthan analyzing whole images of scenes, we focus on co-occurrences of objects\nwithin subregions of an image. We illustrate that this method yields a sensible\nhierarchical decomposition of a scene into collections of semantically related\nobjects. Overall, these results suggest that object and scene embeddings from\nobject co-occurrences and spatial context yield semantically meaningful\nrepresentations as well as computational improvements for downstream\napplications such as scene classification.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 08:26:38 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Treder", "Matthias S.", ""], ["Mayor-Torres", "Juan", ""], ["Teufel", "Christoph", ""]]}, {"id": "2009.09391", "submitter": "Alfa Rossi", "authors": "Alfa Rossi, Nadim Ahmed, Sultanus Salehin, Tashfique Hasnine\n  Choudhury, Golam Sarowar", "title": "Real-time Lane detection and Motion Planning in Raspberry Pi and Arduino\n  for an Autonomous Vehicle Prototype", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses a vehicle prototype that recognizes streets' lanes and\nplans its motion accordingly without any human input. Pi Camera 1.3 captures\nreal-time video, which is then processed by Raspberry-Pi 3.0 Model B. The image\nprocessing algorithms are written in Python 3.7.4 with OpenCV 4.2. Arduino Uno\nis utilized to control the PID algorithm that controls the motor controller,\nwhich in turn controls the wheels. Algorithms that are used to detect the lanes\nare the Canny edge detection algorithm and Hough transformation. Elementary\nalgebra is used to draw the detected lanes. After detection, the lanes are\ntracked using the Kalman filter prediction method. Then the midpoint of the two\nlanes is found, which is the initial steering direction. This initial steering\ndirection is further smoothed by using the Past Accumulation Average Method and\nKalman Filter Prediction Method. The prototype was tested in a controlled\nenvironment in real-time. Results from comprehensive testing suggest that this\nprototype can detect road lanes and plan its motion successfully.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 09:13:15 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Rossi", "Alfa", ""], ["Ahmed", "Nadim", ""], ["Salehin", "Sultanus", ""], ["Choudhury", "Tashfique Hasnine", ""], ["Sarowar", "Golam", ""]]}, {"id": "2009.09393", "submitter": "Densen Puthussery", "authors": "Hrishikesh P.S., Densen Puthussery, Melvin Kuriakose, Jiji C.V", "title": "Transform Domain Pyramidal Dilated Convolution Networks For Restoration\n  of Under Display Camera Images", "comments": "Presented at RLQ-TOD workshop at ECCV 2020, 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Under-display camera (UDC) is a novel technology that can make digital\nimaging experience in handheld devices seamless by providing large\nscreen-to-body ratio. UDC images are severely degraded owing to their\npositioning under a display screen. This work addresses the restoration of\nimages degraded as a result of UDC imaging. Two different networks are proposed\nfor the restoration of images taken with two types of UDC technologies. The\nfirst method uses a pyramidal dilated convolution within a wavelet decomposed\nconvolutional neural network for pentile-organic LED (P-OLED) based display\nsystem. The second method employs pyramidal dilated convolution within a\ndiscrete cosine transform based dual domain network to restore images taken\nusing a transparent-organic LED (T-OLED) based UDC system. The first method\nproduced very good quality restored images and was the winning entry in\nEuropean Conference on Computer Vision (ECCV) 2020 challenge on image\nrestoration for Under-display Camera - Track 2 - P-OLED evaluated based on PSNR\nand SSIM. The second method scored fourth position in Track-1 (T-OLED) of the\nchallenge evaluated based on the same metrics.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 09:26:10 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["S.", "Hrishikesh P.", ""], ["Puthussery", "Densen", ""], ["Kuriakose", "Melvin", ""], ["C.", "Jiji", "V"]]}, {"id": "2009.09399", "submitter": "Chaoyou Fu", "authors": "Chaoyou Fu, Xiang Wu, Yibo Hu, Huaibo Huang, Ran He", "title": "DVG-Face: Dual Variational Generation for Heterogeneous Face Recognition", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous Face Recognition (HFR) refers to matching cross-domain faces\nand plays a crucial role in public security. Nevertheless, HFR is confronted\nwith challenges from large domain discrepancy and insufficient heterogeneous\ndata. In this paper, we formulate HFR as a dual generation problem, and tackle\nit via a novel Dual Variational Generation (DVG-Face) framework. Specifically,\na dual variational generator is elaborately designed to learn the joint\ndistribution of paired heterogeneous images. However, the small-scale paired\nheterogeneous training data may limit the identity diversity of sampling. In\norder to break through the limitation, we propose to integrate abundant\nidentity information of large-scale visible data into the joint distribution.\nFurthermore, a pairwise identity preserving loss is imposed on the generated\npaired heterogeneous images to ensure their identity consistency. As a\nconsequence, massive new diverse paired heterogeneous images with the same\nidentity can be generated from noises. The identity consistency and identity\ndiversity properties allow us to employ these generated images to train the HFR\nnetwork via a contrastive learning mechanism, yielding both domain-invariant\nand discriminative embedding features. Concretely, the generated paired\nheterogeneous images are regarded as positive pairs, and the images obtained\nfrom different samplings are considered as negative pairs. Our method achieves\nsuperior performances over state-of-the-art methods on seven challenging\ndatabases belonging to five HFR tasks, including NIR-VIS, Sketch-Photo,\nProfile-Frontal Photo, Thermal-VIS, and ID-Camera. The related code will be\nreleased at https://github.com/BradyFU.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 09:48:24 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 10:39:50 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Fu", "Chaoyou", ""], ["Wu", "Xiang", ""], ["Hu", "Yibo", ""], ["Huang", "Huaibo", ""], ["He", "Ran", ""]]}, {"id": "2009.09404", "submitter": "Songpengcheng Xia", "authors": "Ling Pei, Songpengcheng Xia, Lei Chu, Fanyi Xiao, Qi Wu, Wenxian Yu,\n  Robert Qiu", "title": "MARS: Mixed Virtual and Real Wearable Sensors for Human Activity\n  Recognition with Multi-Domain Deep Learning Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Together with the rapid development of the Internet of Things (IoT), human\nactivity recognition (HAR) using wearable Inertial Measurement Units (IMUs)\nbecomes a promising technology for many research areas. Recently, deep\nlearning-based methods pave a new way of understanding and performing analysis\nof the complex data in the HAR system. However, the performance of these\nmethods is mostly based on the quality and quantity of the collected data. In\nthis paper, we innovatively propose to build a large database based on virtual\nIMUs and then address technical issues by introducing a multiple-domain deep\nlearning framework consisting of three technical parts. In the first part, we\npropose to learn the single-frame human activity from the noisy IMU data with\nhybrid convolutional neural networks (CNNs) in the semi-supervised form. For\nthe second part, the extracted data features are fused according to the\nprinciple of uncertainty-aware consistency, which reduces the uncertainty by\nweighting the importance of the features. The transfer learning is performed in\nthe last part based on the newly released Archive of Motion Capture as Surface\nShapes (AMASS) dataset, containing abundant synthetic human poses, which\nenhances the variety and diversity of the training dataset and is beneficial\nfor the process of training and feature transfer in the proposed method. The\nefficiency and effectiveness of the proposed method have been demonstrated in\nthe real deep inertial poser (DIP) dataset. The experimental results show that\nthe proposed methods can surprisingly converge within a few iterations and\noutperform all competing methods.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 10:35:14 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 16:21:49 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Pei", "Ling", ""], ["Xia", "Songpengcheng", ""], ["Chu", "Lei", ""], ["Xiao", "Fanyi", ""], ["Wu", "Qi", ""], ["Yu", "Wenxian", ""], ["Qiu", "Robert", ""]]}, {"id": "2009.09405", "submitter": "Yaniv Benny", "authors": "Yaniv Benny, Niv Pekar, and Lior Wolf", "title": "Scale-Localized Abstract Reasoning", "comments": "Presented at Computer Vision and Pattern Recognition (CVPR) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the abstract relational reasoning task, which is commonly used as\nan intelligence test. Since some patterns have spatial rationales, while others\nare only semantic, we propose a multi-scale architecture that processes each\nquery in multiple resolutions. We show that indeed different rules are solved\nby different resolutions and a combined multi-scale approach outperforms the\nexisting state of the art in this task on all benchmarks by 5-54%. The success\nof our method is shown to arise from multiple novelties. First, it searches for\nrelational patterns in multiple resolutions, which allows it to readily detect\nvisual relations, such as location, in higher resolution, while allowing the\nlower resolution module to focus on semantic relations, such as shape type.\nSecond, we optimize the reasoning network of each resolution proportionally to\nits performance, hereby we motivate each resolution to specialize on the rules\nfor which it performs better than the others and ignore cases that are already\nsolved by the other resolutions. Third, we propose a new way to pool\ninformation along the rows and the columns of the illustration-grid of the\nquery. Our work also analyses the existing benchmarks, demonstrating that the\nRAVEN dataset selects the negative examples in a way that is easily exploited.\nWe, therefore, propose a modified version of the RAVEN dataset, named\nRAVEN-FAIR. Our code and pretrained models are available at\nhttps://github.com/yanivbenny/MRNet.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 10:37:29 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 20:11:10 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Benny", "Yaniv", ""], ["Pekar", "Niv", ""], ["Wolf", "Lior", ""]]}, {"id": "2009.09412", "submitter": "Ahmad Droby", "authors": "Ahmad Droby, Jihad El-Sana", "title": "ContourCNN: convolutional neural network for contour data classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel Convolutional Neural Network model for contour\ndata analysis (ContourCNN) and shape classification. A contour is a circular\nsequence of points representing a closed shape. For handling the cyclical\nproperty of the contour representation, we employ circular convolution layers.\nContours are often represented sparsely. To address information sparsity, we\nintroduce priority pooling layers that select features based on their\nmagnitudes. Priority pooling layers pool features with low magnitudes while\nleaving the rest unchanged. We evaluated the proposed model using letters and\ndigits shapes extracted from the EMNIST dataset and obtained a high\nclassification accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 11:56:47 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 11:58:16 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Droby", "Ahmad", ""], ["El-Sana", "Jihad", ""]]}, {"id": "2009.09445", "submitter": "Fabian Dubourvieux Mr", "authors": "Fabian Dubourvieux, Romaric Audigier, Angelique Loesch, Samia Ainouz,\n  Stephane Canu", "title": "Unsupervised Domain Adaptation for Person Re-Identification through\n  Source-Guided Pseudo-Labeling", "comments": "Accepted at ICPR 2020 first round, preprint version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Person Re-Identification (re-ID) aims at retrieving images of the same person\ntaken by different cameras. A challenge for re-ID is the performance\npreservation when a model is used on data of interest (target data) which\nbelong to a different domain from the training data domain (source data).\nUnsupervised Domain Adaptation (UDA) is an interesting research direction for\nthis challenge as it avoids a costly annotation of the target data.\nPseudo-labeling methods achieve the best results in UDA-based re-ID.\nSurprisingly, labeled source data are discarded after this initialization step.\nHowever, we believe that pseudo-labeling could further leverage the labeled\nsource data in order to improve the post-initialization training steps. In\norder to improve robustness against erroneous pseudo-labels, we advocate the\nexploitation of both labeled source data and pseudo-labeled target data during\nall training iterations. To support our guideline, we introduce a framework\nwhich relies on a two-branch architecture optimizing classification and triplet\nloss based metric learning in source and target domains, respectively, in order\nto allow \\emph{adaptability to the target domain} while ensuring\n\\emph{robustness to noisy pseudo-labels}. Indeed, shared low and mid-level\nparameters benefit from the source classification and triplet loss signal while\nhigh-level parameters of the target branch learn domain-specific features. Our\nmethod is simple enough to be easily combined with existing pseudo-labeling UDA\napproaches. We show experimentally that it is efficient and improves\nperformance when the base method has no mechanism to deal with pseudo-label\nnoise or for hard adaptation tasks. Our approach reaches state-of-the-art\nperformance when evaluated on commonly used datasets, Market-1501 and\nDukeMTMC-reID, and outperforms the state of the art when targeting the bigger\nand more challenging dataset MSMT.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 14:54:42 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Dubourvieux", "Fabian", ""], ["Audigier", "Romaric", ""], ["Loesch", "Angelique", ""], ["Ainouz", "Samia", ""], ["Canu", "Stephane", ""]]}, {"id": "2009.09447", "submitter": "Lu Yang", "authors": "Lu Yang, Qing Song, Zhihui Wang, Mengjie Hu, Chun Liu, Xueshi Xin,\n  Wenhe Jia, Songcen Xu", "title": "Renovating Parsing R-CNN for Accurate Multiple Human Parsing", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple human parsing aims to segment various human parts and associate each\npart with the corresponding instance simultaneously. This is a very challenging\ntask due to the diverse human appearance, semantic ambiguity of different body\nparts, and complex background. Through analysis of multiple human parsing task,\nwe observe that human-centric global perception and accurate instance-level\nparsing scoring are crucial for obtaining high-quality results. But the most\nstate-of-the-art methods have not paid enough attention to these issues. To\nreverse this phenomenon, we present Renovating Parsing R-CNN (RP R-CNN), which\nintroduces a global semantic enhanced feature pyramid network and a parsing\nre-scoring network into the existing high-performance pipeline. The proposed RP\nR-CNN adopts global semantic representation to enhance multi-scale features for\ngenerating human parsing maps, and regresses a confidence score to represent\nits quality. Extensive experiments show that RP R-CNN performs favorably\nagainst state-of-the-art methods on CIHP and MHP-v2 datasets. Code and models\nare available at https://github.com/soeaver/RP-R-CNN.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 14:55:35 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Yang", "Lu", ""], ["Song", "Qing", ""], ["Wang", "Zhihui", ""], ["Hu", "Mengjie", ""], ["Liu", "Chun", ""], ["Xin", "Xueshi", ""], ["Jia", "Wenhe", ""], ["Xu", "Songcen", ""]]}, {"id": "2009.09450", "submitter": "Tianshui Chen", "authors": "Tianshui Chen, Liang Lin, Riquan Chen, Xiaolu Hui, and Hefeng Wu", "title": "Knowledge-Guided Multi-Label Few-Shot Learning for General Image\n  Recognition", "comments": "Accepted at TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing multiple labels of an image is a practical yet challenging task,\nand remarkable progress has been achieved by searching for semantic regions and\nexploiting label dependencies. However, current works utilize RNN/LSTM to\nimplicitly capture sequential region/label dependencies, which cannot fully\nexplore mutual interactions among the semantic regions/labels and do not\nexplicitly integrate label co-occurrences. In addition, these works require\nlarge amounts of training samples for each category, and they are unable to\ngeneralize to novel categories with limited samples. To address these issues,\nwe propose a knowledge-guided graph routing (KGGR) framework, which unifies\nprior knowledge of statistical label correlations with deep neural networks.\nThe framework exploits prior knowledge to guide adaptive information\npropagation among different categories to facilitate multi-label analysis and\nreduce the dependency of training samples. Specifically, it first builds a\nstructured knowledge graph to correlate different labels based on statistical\nlabel co-occurrence. Then, it introduces the label semantics to guide learning\nsemantic-specific features to initialize the graph, and it exploits a graph\npropagation network to explore graph node interactions, enabling learning\ncontextualized image feature representations. Moreover, we initialize each\ngraph node with the classifier weights for the corresponding label and apply\nanother propagation network to transfer node messages through the graph. In\nthis way, it can facilitate exploiting the information of correlated labels to\nhelp train better classifiers. We conduct extensive experiments on the\ntraditional multi-label image recognition (MLR) and multi-label few-shot\nlearning (ML-FSL) tasks and show that our KGGR framework outperforms the\ncurrent state-of-the-art methods by sizable margins on the public benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 15:05:29 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Chen", "Tianshui", ""], ["Lin", "Liang", ""], ["Chen", "Riquan", ""], ["Hui", "Xiaolu", ""], ["Wu", "Hefeng", ""]]}, {"id": "2009.09458", "submitter": "Julian Chibane", "authors": "Julian Chibane, Gerard Pons-Moll", "title": "Implicit Feature Networks for Texture Completion from Partial 3D Data", "comments": "SHARP Workshop, European Conference on Computer Vision (ECCV), 2020", "journal-ref": "SHARP Workshop, European Conference on Computer Vision (ECCV),\n  2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work to infer 3D texture use either texture atlases, which require\nuv-mappings and hence have discontinuities, or colored voxels, which are memory\ninefficient and limited in resolution. Recent work, predicts RGB color at every\nXYZ coordinate forming a texture field, but focus on completing texture given a\nsingle 2D image. Instead, we focus on 3D texture and geometry completion from\npartial and incomplete 3D scans. IF-Nets have recently achieved\nstate-of-the-art results on 3D geometry completion using a multi-scale deep\nfeature encoding, but the outputs lack texture. In this work, we generalize\nIF-Nets to texture completion from partial textured scans of humans and\narbitrary objects. Our key insight is that 3D texture completion benefits from\nincorporating local and global deep features extracted from both the 3D partial\ntexture and completed geometry. Specifically, given the partial 3D texture and\nthe 3D geometry completed with IF-Nets, our model successfully in-paints the\nmissing texture parts in consistence with the completed geometry. Our model won\nthe SHARP ECCV'20 challenge, achieving highest performance on all challenges.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 15:48:17 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Chibane", "Julian", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "2009.09465", "submitter": "Junfu Chen", "authors": "Junfu Chen, Yue Pan, Yang Chen", "title": "Remote sensing image fusion based on Bayesian GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing image fusion technology (pan-sharpening) is an important means\nto improve the information capacity of remote sensing images. Inspired by the\nefficient arameter space posteriori sampling of Bayesian neural networks, in\nthis paper we propose a Bayesian Generative Adversarial Network based on\nPreconditioned Stochastic Gradient Langevin Dynamics (PGSLD-BGAN) to improve\npan-sharpening tasks. Unlike many traditional generative models that consider\nonly one optimal solution (might be locally optimal), the proposed PGSLD-BGAN\nperforms Bayesian inference on the network parameters, and explore the\ngenerator posteriori distribution, which assists selecting the appropriate\ngenerator parameters. First, we build a two-stream generator network with PAN\nand MS images as input, which consists of three parts: feature extraction,\nfeature fusion and image reconstruction. Then, we leverage Markov discriminator\nto enhance the ability of generator to reconstruct the fusion image, so that\nthe result image can retain more details. Finally, introducing Preconditioned\nStochastic Gradient Langevin Dynamics policy, we perform Bayesian inference on\nthe generator network. Experiments on QuickBird and WorldView datasets show\nthat the model proposed in this paper can effectively fuse PAN and MS images,\nand be competitive with even superior to state of the arts in terms of\nsubjective and objective metrics.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 16:15:51 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Chen", "Junfu", ""], ["Pan", "Yue", ""], ["Chen", "Yang", ""]]}, {"id": "2009.09485", "submitter": "Ayush Tewari", "authors": "Ayush Tewari, Mohamed Elgharib, Mallikarjun B R., Florian Bernard,\n  Hans-Peter Seidel, Patrick P\\'erez, Michael Zollh\\\"ofer, Christian Theobalt", "title": "PIE: Portrait Image Embedding for Semantic Control", "comments": "To appear in SIGGRAPH Asia 2020. Project webpage:\n  https://gvv.mpi-inf.mpg.de/projects/PIE/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Editing of portrait images is a very popular and important research topic\nwith a large variety of applications. For ease of use, control should be\nprovided via a semantically meaningful parameterization that is akin to\ncomputer animation controls. The vast majority of existing techniques do not\nprovide such intuitive and fine-grained control, or only enable coarse editing\nof a single isolated control parameter. Very recently, high-quality\nsemantically controlled editing has been demonstrated, however only on\nsynthetically created StyleGAN images. We present the first approach for\nembedding real portrait images in the latent space of StyleGAN, which allows\nfor intuitive editing of the head pose, facial expression, and scene\nillumination in the image. Semantic editing in parameter space is achieved\nbased on StyleRig, a pretrained neural network that maps the control space of a\n3D morphable face model to the latent space of the GAN. We design a novel\nhierarchical non-linear optimization problem to obtain the embedding. An\nidentity preservation energy term allows spatially coherent edits while\nmaintaining facial integrity. Our approach runs at interactive frame rates and\nthus allows the user to explore the space of possible edits. We evaluate our\napproach on a wide set of portrait photos, compare it to the current state of\nthe art, and validate the effectiveness of its components in an ablation study.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 17:53:51 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Tewari", "Ayush", ""], ["Elgharib", "Mohamed", ""], ["R.", "Mallikarjun B", ""], ["Bernard", "Florian", ""], ["Seidel", "Hans-Peter", ""], ["P\u00e9rez", "Patrick", ""], ["Zollh\u00f6fer", "Michael", ""], ["Theobalt", "Christian", ""]]}, {"id": "2009.09496", "submitter": "Nidhi Kaushik Vyas", "authors": "Nidhi Vyas, Shreyas Saxena, Thomas Voice", "title": "Learning Soft Labels via Meta Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-hot labels do not represent soft decision boundaries among concepts, and\nhence, models trained on them are prone to overfitting. Using soft labels as\ntargets provide regularization, but different soft labels might be optimal at\ndifferent stages of optimization. Also, training with fixed labels in the\npresence of noisy annotations leads to worse generalization. To address these\nlimitations, we propose a framework, where we treat the labels as learnable\nparameters, and optimize them along with model parameters. The learned labels\ncontinuously adapt themselves to the model's state, thereby providing dynamic\nregularization. When applied to the task of supervised image-classification,\nour method leads to consistent gains across different datasets and\narchitectures. For instance, dynamically learned labels improve ResNet18 by\n2.1% on CIFAR100. When applied to dataset containing noisy labels, the learned\nlabels correct the annotation mistakes, and improves over state-of-the-art by a\nsignificant margin. Finally, we show that learned labels capture semantic\nrelationship between classes, and thereby improve teacher models for the\ndownstream task of distillation.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 18:42:13 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Vyas", "Nidhi", ""], ["Saxena", "Shreyas", ""], ["Voice", "Thomas", ""]]}, {"id": "2009.09560", "submitter": "Xiaoyong Yuan", "authors": "Xiaoyong Yuan, Lei Ding, Lan Zhang, Xiaolin Li, Dapeng Wu", "title": "ES Attack: Model Stealing against Deep Neural Networks without Data\n  Hurdles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have become the essential components for various\ncommercialized machine learning services, such as Machine Learning as a Service\n(MLaaS). Recent studies show that machine learning services face severe privacy\nthreats - well-trained DNNs owned by MLaaS providers can be stolen through\npublic APIs, namely model stealing attacks. However, most existing works\nundervalued the impact of such attacks, where a successful attack has to\nacquire confidential training data or auxiliary data regarding the victim DNN.\nIn this paper, we propose ES Attack, a novel model stealing attack without any\ndata hurdles. By using heuristically generated synthetic data, ES\nAttackiteratively trains a substitute model and eventually achieves a\nfunctionally equivalent copy of the victim DNN. The experimental results reveal\nthe severity of ES Attack: i) ES Attack successfully steals the victim model\nwithout data hurdles, and ES Attack even outperforms most existing model\nstealing attacks using auxiliary data in terms of model accuracy; ii) most\ncountermeasures are ineffective in defending ES Attack; iii) ES Attack\nfacilitates further attacks relying on the stolen model.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 01:26:06 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Yuan", "Xiaoyong", ""], ["Ding", "Lei", ""], ["Zhang", "Lan", ""], ["Li", "Xiaolin", ""], ["Wu", "Dapeng", ""]]}, {"id": "2009.09566", "submitter": "Tsu-Jui Fu", "authors": "Tsu-Jui Fu, Xin Eric Wang, Scott Grafton, Miguel Eckstein, William\n  Yang Wang", "title": "SSCR: Iterative Language-Based Image Editing via Self-Supervised\n  Counterfactual Reasoning", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative Language-Based Image Editing (IL-BIE) tasks follow iterative\ninstructions to edit images step by step. Data scarcity is a significant issue\nfor ILBIE as it is challenging to collect large-scale examples of images before\nand after instruction-based changes. However, humans still accomplish these\nediting tasks even when presented with an unfamiliar image-instruction pair.\nSuch ability results from counterfactual thinking and the ability to think\nabout alternatives to events that have happened already. In this paper, we\nintroduce a Self-Supervised Counterfactual Reasoning (SSCR) framework that\nincorporates counterfactual thinking to overcome data scarcity. SSCR allows the\nmodel to consider out-of-distribution instructions paired with previous images.\nWith the help of cross-task consistency (CTC), we train these counterfactual\ninstructions in a self-supervised scenario. Extensive results show that SSCR\nimproves the correctness of ILBIE in terms of both object identity and\nposition, establishing a new state of the art (SOTA) on two IBLIE datasets\n(i-CLEVR and CoDraw). Even with only 50% of the training data, SSCR achieves a\ncomparable result to using complete data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 01:45:58 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 00:24:25 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Fu", "Tsu-Jui", ""], ["Wang", "Xin Eric", ""], ["Grafton", "Scott", ""], ["Eckstein", "Miguel", ""], ["Wang", "William Yang", ""]]}, {"id": "2009.09571", "submitter": "Zhuangzhuang Zhang", "authors": "Zhuangzhuang Zhang, Tianyu Zhao, Hiram Gay, Baozhou Sun, and Weixiong\n  Zhang", "title": "Semi-supervised Semantic Segmentation of Prostate and Organs-at-Risk on\n  3D Pelvic CT Images", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated segmentation can assist radiotherapy treatment planning by saving\nmanual contouring efforts and reducing intra-observer and inter-observer\nvariations. The recent development of deep learning approaches has revoluted\nmedical data processing, including semantic segmentation, by dramatically\nimproving performance. However, training effective deep learning models usually\nrequire a large amount of high-quality labeled data, which are often costly to\ncollect. We developed a novel semi-supervised adversarial deep learning\napproach for 3D pelvic CT image semantic segmentation. Unlike supervised deep\nlearning methods, the new approach can utilize both annotated and un-annotated\ndata for training. It generates un-annotated synthetic data by a data\naugmentation scheme using generative adversarial networks (GANs). We applied\nthe new approach to segmenting multiple organs in male pelvic CT images, where\nCT images without annotations and GAN-synthesized un-annotated images were used\nin semi-supervised learning. Experimental results, evaluated by three metrics\n(Dice similarity coefficient, average Hausdorff distance, and average surface\nHausdorff distance), showed that the new method achieved either comparable\nperformance with substantially fewer annotated images or better performance\nwith the same amount of annotated data, outperforming the existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 01:57:23 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 00:33:46 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 18:10:25 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Zhang", "Zhuangzhuang", ""], ["Zhao", "Tianyu", ""], ["Gay", "Hiram", ""], ["Sun", "Baozhou", ""], ["Zhang", "Weixiong", ""]]}, {"id": "2009.09574", "submitter": "Jiabo Ma", "authors": "Jiabo Ma, Sibo Liu, Shenghua Cheng, Xiuli Liu, Li Cheng, Shaoqun Zeng", "title": "Reconstruct high-resolution multi-focal plane images from a single 2D\n  wide field image", "comments": "9 pages, 4 figures,3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution 3D medical images are important for analysis and diagnosis,\nbut axial scanning to acquire them is very time-consuming. In this paper, we\npropose a fast end-to-end multi-focal plane imaging network (MFPINet) to\nreconstruct high-resolution multi-focal plane images from a single 2D\nlow-resolution wild filed image without relying on scanning. To acquire\nrealistic MFP images fast, the proposed MFPINet adopts generative adversarial\nnetwork framework and the strategies of post-sampling and refocusing all focal\nplanes at one time. We conduct a series experiments on cytology microscopy\nimages and demonstrate that MFPINet performs well on both axial refocusing and\nhorizontal super resolution. Furthermore, MFPINet is approximately 24 times\nfaster than current refocusing methods for reconstructing the same volume\nimages. The proposed method has the potential to greatly increase the speed of\nhigh-resolution 3D imaging and expand the application of low-resolution\nwide-field images.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 02:09:36 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Ma", "Jiabo", ""], ["Liu", "Sibo", ""], ["Cheng", "Shenghua", ""], ["Liu", "Xiuli", ""], ["Cheng", "Li", ""], ["Zeng", "Shaoqun", ""]]}, {"id": "2009.09583", "submitter": "Mel McCurrie", "authors": "Mel McCurrie, Hamish Nicholson, Walter J. Scheirer, Samuel Anthony", "title": "Modeling Score Distributions and Continuous Covariates: A Bayesian\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer Vision practitioners must thoroughly understand their model's\nperformance, but conditional evaluation is complex and error-prone. In\nbiometric verification, model performance over continuous\ncovariates---real-number attributes of images that affect performance---is\nparticularly challenging to study. We develop a generative model of the match\nand non-match score distributions over continuous covariates and perform\ninference with modern Bayesian methods. We use mixture models to capture\narbitrary distributions and local basis functions to capture non-linear,\nmultivariate trends. Three experiments demonstrate the accuracy and\neffectiveness of our approach. First, we study the relationship between age and\nface verification performance and find previous methods may overstate\nperformance and confidence. Second, we study preprocessing for CNNs and find a\nhighly non-linear, multivariate surface of model performance. Our method is\naccurate and data efficient when evaluated against previous synthetic methods.\nThird, we demonstrate the novel application of our method to pedestrian\ntracking and calculate variable thresholds and expected performance while\ncontrolling for multiple covariates.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 02:41:20 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["McCurrie", "Mel", ""], ["Nicholson", "Hamish", ""], ["Scheirer", "Walter J.", ""], ["Anthony", "Samuel", ""]]}, {"id": "2009.09585", "submitter": "Yang Li", "authors": "Yang Li, Boxun Fu, Fu Li, Guangming Shi, Wenming Zheng", "title": "A Novel Transferability Attention Neural Network Model for EEG Emotion\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existed methods for electroencephalograph (EEG) emotion recognition\nalways train the models based on all the EEG samples indistinguishably.\nHowever, some of the source (training) samples may lead to a negative influence\nbecause they are significant dissimilar with the target (test) samples. So it\nis necessary to give more attention to the EEG samples with strong\ntransferability rather than forcefully training a classification model by all\nthe samples. Furthermore, for an EEG sample, from the aspect of neuroscience,\nnot all the brain regions of an EEG sample contains emotional information that\ncan transferred to the test data effectively. Even some brain region data will\nmake strong negative effect for learning the emotional classification model.\nConsidering these two issues, in this paper, we propose a transferable\nattention neural network (TANN) for EEG emotion recognition, which learns the\nemotional discriminative information by highlighting the transferable EEG brain\nregions data and samples adaptively through local and global attention\nmechanism. This can be implemented by measuring the outputs of multiple\nbrain-region-level discriminators and one single sample-level discriminator. We\nconduct the extensive experiments on three public EEG emotional datasets. The\nresults validate that the proposed model achieves the state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 02:42:30 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Li", "Yang", ""], ["Fu", "Boxun", ""], ["Li", "Fu", ""], ["Shi", "Guangming", ""], ["Zheng", "Wenming", ""]]}, {"id": "2009.09612", "submitter": "Tuan Anh Bui", "authors": "Anh Bui, Trung Le, He Zhao, Paul Montague, Olivier deVel, Tamas\n  Abraham, Dinh Phung", "title": "Improving Ensemble Robustness by Collaboratively Promoting and Demoting\n  Adversarial Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble-based adversarial training is a principled approach to achieve\nrobustness against adversarial attacks. An important technique of this approach\nis to control the transferability of adversarial examples among ensemble\nmembers. We propose in this work a simple yet effective strategy to collaborate\namong committee models of an ensemble model. This is achieved via the secure\nand insecure sets defined for each model member on a given sample, hence help\nus to quantify and regularize the transferability. Consequently, our proposed\nframework provides the flexibility to reduce the adversarial transferability as\nwell as to promote the diversity of ensemble members, which are two crucial\nfactors for better robustness in our ensemble approach. We conduct extensive\nand comprehensive experiments to demonstrate that our proposed method\noutperforms the state-of-the-art ensemble baselines, at the same time can\ndetect a wide range of adversarial examples with a nearly perfect accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 04:54:38 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Bui", "Anh", ""], ["Le", "Trung", ""], ["Zhao", "He", ""], ["Montague", "Paul", ""], ["deVel", "Olivier", ""], ["Abraham", "Tamas", ""], ["Phung", "Dinh", ""]]}, {"id": "2009.09633", "submitter": "Huan Fu", "authors": "Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve\n  Maybank, Dacheng Tao", "title": "3D-FUTURE: 3D Furniture shape with TextURE", "comments": "Project Page:\n  https://tianchi.aliyun.com/specials/promotion/alibaba-3d-future", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3D CAD shapes in current 3D benchmarks are mostly collected from online\nmodel repositories. Thus, they typically have insufficient geometric details\nand less informative textures, making them less attractive for comprehensive\nand subtle research in areas such as high-quality 3D mesh and texture recovery.\nThis paper presents 3D Furniture shape with TextURE (3D-FUTURE): a\nrichly-annotated and large-scale repository of 3D furniture shapes in the\nhousehold scenario. At the time of this technical report, 3D-FUTURE contains\n20,240 clean and realistic synthetic images of 5,000 different rooms. There are\n9,992 unique detailed 3D instances of furniture with high-resolution textures.\nExperienced designers developed the room scenes, and the 3D CAD shapes in the\nscene are used for industrial production. Given the well-organized 3D-FUTURE,\nwe provide baseline experiments on several widely studied tasks, such as joint\n2D instance segmentation and 3D object pose estimation, image-based 3D shape\nretrieval, 3D object reconstruction from a single image, and texture recovery\nfor 3D shapes, to facilitate related future researches on our database.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 06:26:39 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Fu", "Huan", ""], ["Jia", "Rongfei", ""], ["Gao", "Lin", ""], ["Gong", "Mingming", ""], ["Zhao", "Binqiang", ""], ["Maybank", "Steve", ""], ["Tao", "Dacheng", ""]]}, {"id": "2009.09660", "submitter": "Ruibing Jin", "authors": "Ruibing Jin, Guosheng Lin, Changyun Wen, Jianliang Wang and Fayao Liu", "title": "Feature Flow: In-network Feature Flow Estimation for Video Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical flow, which expresses pixel displacement, is widely used in many\ncomputer vision tasks to provide pixel-level motion information. However, with\nthe remarkable progress of the convolutional neural network, recent\nstate-of-the-art approaches are proposed to solve problems directly on\nfeature-level. Since the displacement of feature vector is not consistent to\nthe pixel displacement, a common approach is to:forward optical flow to a\nneural network and fine-tune this network on the task dataset. With this\nmethod,they expect the fine-tuned network to produce tensors encoding\nfeature-level motion information. In this paper, we rethink this de facto\nparadigm and analyze its drawbacks in the video object detection task. To\nmitigate these issues, we propose a novel network (IFF-Net) with an\n\\textbf{I}n-network \\textbf{F}eature \\textbf{F}low estimation module (IFF\nmodule) for video object detection. Without resorting pre-training on any\nadditional dataset, our IFF module is able to directly produce \\textbf{feature\nflow} which indicates the feature displacement. Our IFF module consists of a\nshallow module, which shares the features with the detection branches. This\ncompact design enables our IFF-Net to accurately detect objects, while\nmaintaining a fast inference speed. Furthermore, we propose a transformation\nresidual loss (TRL) based on \\textit{self-supervision}, which further improves\nthe performance of our IFF-Net. Our IFF-Net outperforms existing methods and\nsets a state-of-the-art performance on ImageNet VID.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 07:55:50 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Jin", "Ruibing", ""], ["Lin", "Guosheng", ""], ["Wen", "Changyun", ""], ["Wang", "Jianliang", ""], ["Liu", "Fayao", ""]]}, {"id": "2009.09669", "submitter": "Fei Xie", "authors": "Fei Xie, Wankou Yang, Bo Liu, Kaihua Zhang, Wanli Xue, Wangmeng Zuo", "title": "Learning Spatio-Appearance Memory Network for High-Performance Visual\n  Tracking", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing visual object tracking usually learns a bounding-box based template\nto match the targets across frames, which cannot accurately learn a pixel-wise\nrepresentation, thereby being limited in handling severe appearance variations.\nTo address these issues, much effort has been made on segmentation-based\ntracking, which learns a pixel-wise object-aware template and can achieve\nhigher accuracy than bounding-box template based tracking. However, existing\nsegmentation-based trackers are ineffective in learning the spatio-temporal\ncorrespondence across frames due to no use of the rich temporal information. To\novercome this issue, this paper presents a novel segmentation-based tracking\narchitecture, which is equipped with a spatio-appearance memory network to\nlearn accurate spatio-temporal correspondence. Among it, an appearance memory\nnetwork explores spatio-temporal non-local similarity to learn the dense\ncorrespondence between the segmentation mask and the current frame. Meanwhile,\na spatial memory network is modeled as discriminative correlation filter to\nlearn the mapping between feature map and spatial map. The appearance memory\nnetwork helps to filter out the noisy samples in the spatial memory network\nwhile the latter provides the former with more accurate target geometrical\ncenter. This mutual promotion greatly boosts the tracking performance. Without\nbells and whistles, our simple-yet-effective tracking architecture sets new\nstate-of-the-arts on the VOT2016, VOT2018, VOT2019, GOT-10K, TrackingNet, and\nVOT2020 benchmarks, respectively. Besides, our tracker outperforms the leading\nsegmentation-based trackers SiamMask and D3S on two video object segmentation\nbenchmarks DAVIS16 and DAVIS17 by a large margin. The source codes can be found\nat https://github.com/phiphiphi31/DMB.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 08:12:02 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 00:39:46 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2020 01:27:47 GMT"}, {"version": "v4", "created": "Tue, 17 Nov 2020 09:34:15 GMT"}, {"version": "v5", "created": "Tue, 6 Apr 2021 05:37:10 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Xie", "Fei", ""], ["Yang", "Wankou", ""], ["Liu", "Bo", ""], ["Zhang", "Kaihua", ""], ["Xue", "Wanli", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "2009.09675", "submitter": "Robby Neven", "authors": "Robby Neven, Marian Verhelst, Tinne Tuytelaars and Toon Goedem\\'e", "title": "Feed-Forward On-Edge Fine-tuning Using Static Synthetic Gradient Modules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep learning models on embedded devices is typically avoided since\nthis requires more memory, computation and power over inference. In this work,\nwe focus on lowering the amount of memory needed for storing all activations,\nwhich are required during the backward pass to compute the gradients. Instead,\nduring the forward pass, static Synthetic Gradient Modules (SGMs) predict\ngradients for each layer. This allows training the model in a feed-forward\nmanner without having to store all activations. We tested our method on a robot\ngrasping scenario where a robot needs to learn to grasp new objects given only\na single demonstration. By first training the SGMs in a meta-learning manner on\na set of common objects, during fine-tuning, the SGMs provided the model with\naccurate gradients to successfully learn to grasp new objects. We have shown\nthat our method has comparable results to using standard backpropagation.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 08:27:01 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Neven", "Robby", ""], ["Verhelst", "Marian", ""], ["Tuytelaars", "Tinne", ""], ["Goedem\u00e9", "Toon", ""]]}, {"id": "2009.09687", "submitter": "Xi Peng", "authors": "Yunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, Xi Peng", "title": "Contrastive Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a one-stage online clustering method called\nContrastive Clustering (CC) which explicitly performs the instance- and\ncluster-level contrastive learning. To be specific, for a given dataset, the\npositive and negative instance pairs are constructed through data augmentations\nand then projected into a feature space. Therein, the instance- and\ncluster-level contrastive learning are respectively conducted in the row and\ncolumn space by maximizing the similarities of positive pairs while minimizing\nthose of negative ones. Our key observation is that the rows of the feature\nmatrix could be regarded as soft labels of instances, and accordingly the\ncolumns could be further regarded as cluster representations. By simultaneously\noptimizing the instance- and cluster-level contrastive loss, the model jointly\nlearns representations and cluster assignments in an end-to-end manner.\nExtensive experimental results show that CC remarkably outperforms 17\ncompetitive clustering methods on six challenging image benchmarks. In\nparticular, CC achieves an NMI of 0.705 (0.431) on the CIFAR-10 (CIFAR-100)\ndataset, which is an up to 19\\% (39\\%) performance improvement compared with\nthe best baseline.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 08:54:40 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Li", "Yunfan", ""], ["Hu", "Peng", ""], ["Liu", "Zitao", ""], ["Peng", "Dezhong", ""], ["Zhou", "Joey Tianyi", ""], ["Peng", "Xi", ""]]}, {"id": "2009.09692", "submitter": "Changxing Ding", "authors": "Kan Wang, Pengfei Wang, Changxing Ding, and Dacheng Tao", "title": "Batch Coherence-Driven Network for Part-aware Person Re-Identification", "comments": "Accepted Version to IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2021.3060909", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing part-aware person re-identification methods typically employ two\nseparate steps: namely, body part detection and part-level feature extraction.\nHowever, part detection introduces an additional computational cost and is\ninherently challenging for low-quality images. Accordingly, in this work, we\npropose a simple framework named Batch Coherence-Driven Network (BCD-Net) that\nbypasses body part detection during both the training and testing phases while\nstill learning semantically aligned part features. Our key observation is that\nthe statistics in a batch of images are stable, and therefore that batch-level\nconstraints are robust. First, we introduce a batch coherence-guided channel\nattention (BCCA) module that highlights the relevant channels for each\nrespective part from the output of a deep backbone model. We investigate\nchannelpart correspondence using a batch of training images, then impose a\nnovel batch-level supervision signal that helps BCCA to identify part-relevant\nchannels. Second, the mean position of a body part is robust and consequently\ncoherent between batches throughout the training process. Accordingly, we\nintroduce a pair of regularization terms based on the semantic consistency\nbetween batches. The first term regularizes the high responses of BCD-Net for\neach part on one batch in order to constrain it within a predefined area, while\nthe second encourages the aggregate of BCD-Nets responses for all parts\ncovering the entire human body. The above constraints guide BCD-Net to learn\ndiverse, complementary, and semantically aligned part-level features. Extensive\nexperimental results demonstrate that BCDNet consistently achieves\nstate-of-the-art performance on four large-scale ReID benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 09:04:13 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 07:21:24 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Wang", "Kan", ""], ["Wang", "Pengfei", ""], ["Ding", "Changxing", ""], ["Tao", "Dacheng", ""]]}, {"id": "2009.09703", "submitter": "Anjith George", "authors": "Zohreh Mostaani and Anjith George and Guillaume Heusch and David\n  Geissbuhler and Sebastien Marcel", "title": "The High-Quality Wide Multi-Channel Attack (HQ-WMCA) database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The High-Quality Wide Multi-Channel Attack database (HQ-WMCA) database\nextends the previous Wide Multi-Channel Attack database(WMCA), with more\nchannels including color, depth, thermal, infrared (spectra), and short-wave\ninfrared (spectra), and also a wide variety of attacks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 09:18:19 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Mostaani", "Zohreh", ""], ["George", "Anjith", ""], ["Heusch", "Guillaume", ""], ["Geissbuhler", "David", ""], ["Marcel", "Sebastien", ""]]}, {"id": "2009.09715", "submitter": "Lingchao Guo", "authors": "Lingchao Guo, Zhaoming Lu, Shuang Zhou, Xiangming Wen, Zhihong He", "title": "When Healthcare Meets Off-the-Shelf WiFi: A Non-Wearable and Low-Costs\n  Approach for In-Home Monitoring", "comments": "41 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As elderly population grows, social and health care begin to face validation\nchallenges, in-home monitoring is becoming a focus for professionals in the\nfield. Governments urgently need to improve the quality of healthcare services\nat lower costs while ensuring the comfort and independence of the elderly. This\nwork presents an in-home monitoring approach based on off-the-shelf WiFi, which\nis low-costs, non-wearable and makes all-round daily healthcare information\navailable to caregivers. The proposed approach can capture fine-grained human\npose figures even through a wall and track detailed respiration status\nsimultaneously by off-the-shelf WiFi devices. Based on them, behavioral data,\nphysiological data and the derived information (e.g., abnormal events and\nunderlying diseases), of the elderly could be seen by caregivers directly. We\ndesign a series of signal processing methods and a neural network to capture\nhuman pose figures and extract respiration status curves from WiFi Channel\nState Information (CSI). Extensive experiments are conducted and according to\nthe results, off-the-shelf WiFi devices are capable of capturing fine-grained\nhuman pose figures, similar to cameras, even through a wall and track accurate\nrespiration status, thus demonstrating the effectiveness and feasibility of our\napproach for in-home monitoring.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 09:35:13 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Guo", "Lingchao", ""], ["Lu", "Zhaoming", ""], ["Zhou", "Shuang", ""], ["Wen", "Xiangming", ""], ["He", "Zhihong", ""]]}, {"id": "2009.09718", "submitter": "Yicheng Wang", "authors": "Yicheng Wang, Shuang Xu, Junmin Liu, Zixiang Zhao, Chunxia Zhang,\n  Jiangshe Zhang", "title": "MFIF-GAN: A New Generative Adversarial Network for Multi-Focus Image\n  Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Focus Image Fusion (MFIF) is a promising image enhancement technique to\nobtain all-in-focus images meeting visual needs and it is a precondition of\nother computer vision tasks. One of the research trends of MFIF is to avoid the\ndefocus spread effect (DSE) around the focus/defocus boundary (FDB). In this\npaper,we propose a network termed MFIF-GAN to attenuate the DSE by generating\nfocus maps in which the foreground region are correctly larger than the\ncorresponding objects. The Squeeze and Excitation Residual module is employed\nin the network. By combining the prior knowledge of training condition, this\nnetwork is trained on a synthetic dataset based on an {\\alpha}-matte model. In\naddition, the reconstruction and gradient regularization terms are combined in\nthe loss functions to enhance the boundary details and improve the quality of\nfused images. Extensive experiments demonstrate that the MFIF-GAN outperforms\nseveral state-of-the-art (SOTA) methods in visual perception, quantitative\nanalysis as well as efficiency. Moreover, the edge diffusion and contraction\nmodule is firstly proposed to verify that focus maps generated by our method\nare accurate at the pixel level.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 09:36:34 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 09:35:24 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2020 03:29:45 GMT"}, {"version": "v4", "created": "Mon, 9 Nov 2020 03:36:53 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Wang", "Yicheng", ""], ["Xu", "Shuang", ""], ["Liu", "Junmin", ""], ["Zhao", "Zixiang", ""], ["Zhang", "Chunxia", ""], ["Zhang", "Jiangshe", ""]]}, {"id": "2009.09724", "submitter": "Yong Guo", "authors": "Yixin Liu, Yong Guo, Zichang Liu, Haohua Liu, Jingjie Zhang, Zejun\n  Chen, Jing Liu, Jian Chen", "title": "Conditional Automated Channel Pruning for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2021.3088323", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model compression aims to reduce the redundancy of deep networks to obtain\ncompact models. Recently, channel pruning has become one of the predominant\ncompression methods to deploy deep models on resource-constrained devices. Most\nchannel pruning methods often use a fixed compression rate for all the layers\nof the model, which, however, may not be optimal. To address this issue, given\na target compression rate for the whole model, one can search for the optimal\ncompression rate for each layer. Nevertheless, these methods perform channel\npruning for a specific target compression rate. When we consider multiple\ncompression rates, they have to repeat the channel pruning process multiple\ntimes, which is very inefficient yet unnecessary. To address this issue, we\npropose a Conditional Automated Channel Pruning(CACP) method to obtain the\ncompressed models with different compression rates through single channel\npruning process. To this end, we develop a conditional model that takes an\narbitrary compression rate as input and outputs the corresponding compressed\nmodel. In the experiments, the resultant models with different compression\nrates consistently outperform the models compressed by existing methods with a\nchannel pruning process for each target compression rate.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 09:55:48 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 03:29:23 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Liu", "Yixin", ""], ["Guo", "Yong", ""], ["Liu", "Zichang", ""], ["Liu", "Haohua", ""], ["Zhang", "Jingjie", ""], ["Chen", "Zejun", ""], ["Liu", "Jing", ""], ["Chen", "Jian", ""]]}, {"id": "2009.09725", "submitter": "Luuk Boulogne", "authors": "Coen de Vente, Luuk H. Boulogne, Kiran Vaidhya Venkadesh, Cheryl\n  Sital, Nikolas Lessmann, Colin Jacobs, Clara I. S\\'anchez, Bram van Ginneken", "title": "Improving Automated COVID-19 Grading with Convolutional Neural Networks\n  in Computed Tomography Scans: An Ablation Study", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amidst the ongoing pandemic, several studies have shown that COVID-19\nclassification and grading using computed tomography (CT) images can be\nautomated with convolutional neural networks (CNNs). Many of these studies\nfocused on reporting initial results of algorithms that were assembled from\ncommonly used components. The choice of these components was often pragmatic\nrather than systematic. For instance, several studies used 2D CNNs even though\nthese might not be optimal for handling 3D CT volumes. This paper identifies a\nvariety of components that increase the performance of CNN-based algorithms for\nCOVID-19 grading from CT images. We investigated the effectiveness of using a\n3D CNN instead of a 2D CNN, of using transfer learning to initialize the\nnetwork, of providing automatically computed lesion maps as additional network\ninput, and of predicting a continuous instead of a categorical output. A 3D CNN\nwith these components achieved an area under the ROC curve (AUC) of 0.934 on\nour test set of 105 CT scans and an AUC of 0.923 on a publicly available set of\n742 CT scans, a substantial improvement in comparison with a previously\npublished 2D CNN. An ablation study demonstrated that in addition to using a 3D\nCNN instead of a 2D CNN transfer learning contributed the most and continuous\noutput contributed the least to improving the model performance.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 09:58:57 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["de Vente", "Coen", ""], ["Boulogne", "Luuk H.", ""], ["Venkadesh", "Kiran Vaidhya", ""], ["Sital", "Cheryl", ""], ["Lessmann", "Nikolas", ""], ["Jacobs", "Colin", ""], ["S\u00e1nchez", "Clara I.", ""], ["van Ginneken", "Bram", ""]]}, {"id": "2009.09774", "submitter": "Tao Bai", "authors": "Jinqi Luo, Tao Bai, Jun Zhao", "title": "Generating Adversarial yet Inconspicuous Patches with a Single Image", "comments": "Accepted by AAAI2021 Student Abstract and Poster Program. Full paper\n  available as arXiv:2009.09774.v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been shown vulnerable toadversarial patches, where\nexotic patterns can resultin models wrong prediction. Nevertheless, existing\nap-proaches to adversarial patch generation hardly con-sider the contextual\nconsistency between patches andthe image background, causing such patches to be\neas-ily detected and adversarial attacks to fail. On the otherhand, these\nmethods require a large amount of data fortraining, which is computationally\nexpensive. To over-come these challenges, we propose an approach to gen-erate\nadversarial yet inconspicuous patches with onesingle image. In our approach,\nadversarial patches areproduced in a coarse-to-fine way with multiple scalesof\ngenerators and discriminators. Contextual informa-tion is encoded during the\nMin-Max training to makepatches consistent with surroundings. The selection\nofpatch location is based on the perceptual sensitivity ofvictim models.\nThrough extensive experiments, our ap-proach shows strong attacking ability in\nboth the white-box and black-box setting. Experiments on saliency de-tection\nand user evaluation indicate that our adversar-ial patches can evade human\nobservations, demonstratethe inconspicuousness of our approach. Lastly, we\nshowthat our approach preserves the attack ability in thephysical world.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 11:56:01 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 12:05:48 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Luo", "Jinqi", ""], ["Bai", "Tao", ""], ["Zhao", "Jun", ""]]}, {"id": "2009.09776", "submitter": "Muhammad Umair Khan Kaker", "authors": "Muhammad Umair Khan, Khawar Saeed, Sidra Qadeer", "title": "Weight Training Analysis of Sportsmen with Kinect Bioinformatics for\n  Form Improvement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sports franchises invest a lot in training their athletes. use of latest\ntechnology for this purpose is also very common. We propose a system of\ncapturing motion of athletes during weight training and analyzing that data to\nfind out any shortcomings and imperfections. Our system uses Kinect depth image\nto compute different parameters of athlete's selected joints. These parameters\nare passed through certain algorithms to process them and formulate results on\ntheir basis. Some parameters like range of motion, speed and balance can be\nanalyzed in real time. But for comparison to be performed between motions, data\nis first recorded and stored and then processed for accurate results. Our\nresults depict that this system can be easily deployed and implemented to\nprovide a very valuable insight to dynamics of a work out and help an athlete\nin improving his form.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 04:52:31 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Khan", "Muhammad Umair", ""], ["Saeed", "Khawar", ""], ["Qadeer", "Sidra", ""]]}, {"id": "2009.09780", "submitter": "Lucas Teixeira", "authors": "Lucas O. Teixeira, Rodolfo M. Pereira, Diego Bertolini, Luiz S.\n  Oliveira, Loris Nanni, George D. C. Cavalcanti, Yandre M. G. Costa", "title": "Impact of lung segmentation on the diagnosis and explanation of COVID-19\n  in chest X-ray images", "comments": "Submitted to International Journal of Universal Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic is undoubtedly one of the biggest public health crises\nour society has ever faced in recent history. One of the main complications\ncaused by COVID-19 is pneumonia, which is diagnosed using imaging exams, such\nas chest X-ray (CXR) and computed tomography (CT) scan. The CT scan is more\nprecise than the CXR. However, CXR is suitable in particular situations because\nit is cheaper, faster, more widespread, and exposes the patient to less\nradiation. This study aims to demonstrate the impact of lung segmentation in\nCOVID-19 identification using CXR images and evaluate which contents of the\nimage decisively contribute to its identification. We performed the lung\nsegmentation using a U-Net CNN architecture, and the classification using three\nwell-known CNN architectures: VGG, ResNet, and Inception. To estimate the\nimpact of lung segmentation, we applied some Explainable Artificial\nIntelligence (XAI) techniques, specifically LIME and Grad-CAM. To empirically\nevaluate our approach, we composed a database with three classes: lung opacity\n(pneumonia), COVID-19, and normal. The segmentation achieved a Jaccard distance\nof 0.034 and a Dice coefficient of 0.982. The classification using segmented\nlung achieved an F1-Score of 0.88 for the multi-class setup and 0.83 for\nCOVID-19 identification. Further testing and XAI techniques suggest that\nsegmented CXR images represent a much more realistic and less biased\nperformance. To the best of our knowledge, no other work tried to estimate the\nimpact of lung segmentation in COVID-19 identification using comprehensive XAI\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 12:03:54 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 11:08:55 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 10:09:45 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Teixeira", "Lucas O.", ""], ["Pereira", "Rodolfo M.", ""], ["Bertolini", "Diego", ""], ["Oliveira", "Luiz S.", ""], ["Nanni", "Loris", ""], ["Cavalcanti", "George D. C.", ""], ["Costa", "Yandre M. G.", ""]]}, {"id": "2009.09796", "submitter": "Michael Crawshaw", "authors": "Michael Crawshaw", "title": "Multi-Task Learning with Deep Neural Networks: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) is a subfield of machine learning in which multiple\ntasks are simultaneously learned by a shared model. Such approaches offer\nadvantages like improved data efficiency, reduced overfitting through shared\nrepresentations, and fast learning by leveraging auxiliary information.\nHowever, the simultaneous learning of multiple tasks presents new design and\noptimization challenges, and choosing which tasks should be learned jointly is\nin itself a non-trivial problem. In this survey, we give an overview of\nmulti-task learning methods for deep neural networks, with the aim of\nsummarizing both the well-established and most recent directions within the\nfield. Our discussion is structured according to a partition of the existing\ndeep MTL techniques into three groups: architectures, optimization methods, and\ntask relationship learning. We also provide a summary of common multi-task\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 19:31:04 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Crawshaw", "Michael", ""]]}, {"id": "2009.09803", "submitter": "Usman Roshan", "authors": "Yunzhe Xue, Meiyan Xie, Usman Roshan", "title": "Defending against substitute model black box adversarial attacks with\n  the 01 loss", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.07800;\n  text overlap with arXiv:2008.09148", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Substitute model black box attacks can create adversarial examples for a\ntarget model just by accessing its output labels. This poses a major challenge\nto machine learning models in practice, particularly in security sensitive\napplications. The 01 loss model is known to be more robust to outliers and\nnoise than convex models that are typically used in practice. Motivated by\nthese properties we present 01 loss linear and 01 loss dual layer neural\nnetwork models as a defense against transfer based substitute model black box\nattacks. We compare the accuracy of adversarial examples from substitute model\nblack box attacks targeting our 01 loss models and their convex counterparts\nfor binary classification on popular image benchmarks. Our 01 loss dual layer\nneural network has an adversarial accuracy of 66.2%, 58%, 60.5%, and 57% on\nMNIST, CIFAR10, STL10, and ImageNet respectively whereas the sigmoid activated\nlogistic loss counterpart has accuracies of 63.5%, 19.3%, 14.9%, and 27.6%.\nExcept for MNIST the convex counterparts have substantially lower adversarial\naccuracies. We show practical applications of our models to deter traffic sign\nand facial recognition adversarial attacks. On GTSRB street sign and CelebA\nfacial detection our 01 loss network has 34.6% and 37.1% adversarial accuracy\nrespectively whereas the convex logistic counterpart has accuracy 24% and 1.9%.\nFinally we show that our 01 loss network can attain robustness on par with\nsimple convolutional neural networks and much higher than its convex\ncounterpart even when attacked with a convolutional network substitute model.\nOur work shows that 01 loss models offer a powerful defense against substitute\nmodel black box attacks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 22:32:51 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Xue", "Yunzhe", ""], ["Xie", "Meiyan", ""], ["Roshan", "Usman", ""]]}, {"id": "2009.09805", "submitter": "Shuang Ma", "authors": "Shuang Ma, Zhaoyang Zeng, Daniel McDuff, Yale Song", "title": "Active Contrastive Learning of Audio-Visual Video Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning has been shown to produce generalizable representations\nof audio and visual data by maximizing the lower bound on the mutual\ninformation (MI) between different views of an instance. However, obtaining a\ntight lower bound requires a sample size exponential in MI and thus a large set\nof negative samples. We can incorporate more samples by building a large\nqueue-based dictionary, but there are theoretical limits to performance\nimprovements even with a large number of negative samples. We hypothesize that\n\\textit{random negative sampling} leads to a highly redundant dictionary that\nresults in suboptimal representations for downstream tasks. In this paper, we\npropose an active contrastive learning approach that builds an \\textit{actively\nsampled} dictionary with diverse and informative items, which improves the\nquality of negative samples and improves performances on tasks where there is\nhigh mutual information in the data, e.g., video classification. Our model\nachieves state-of-the-art performance on challenging audio and visual\ndownstream benchmarks including UCF101, HMDB51 and ESC50.\\footnote{Code is\navailable at: \\url{https://github.com/yunyikristy/CM-ACC}}\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 21:18:30 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 22:16:18 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ma", "Shuang", ""], ["Zeng", "Zhaoyang", ""], ["McDuff", "Daniel", ""], ["Song", "Yale", ""]]}, {"id": "2009.09808", "submitter": "Thomas Davies", "authors": "Thomas Davies and Derek Nowrouzezahrai and Alec Jacobson", "title": "On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A neural implicit outputs a number indicating whether the given query point\nin space is inside, outside, or on a surface. Many prior works have focused on\n_latent-encoded_ neural implicits, where a latent vector encoding of a specific\nshape is also fed as input. While affording latent-space interpolation, this\ncomes at the cost of reconstruction accuracy for any _single_ shape. Training a\nspecific network for each 3D shape, a _weight-encoded_ neural implicit may\nforgo the latent vector and focus reconstruction accuracy on the details of a\nsingle shape. While previously considered as an intermediary representation for\n3D scanning tasks or as a toy-problem leading up to latent-encoding tasks,\nweight-encoded neural implicits have not yet been taken seriously as a 3D shape\nrepresentation. In this paper, we establish that weight-encoded neural\nimplicits meet the criteria of a first-class 3D shape representation. We\nintroduce a suite of technical contributions to improve reconstruction\naccuracy, convergence, and robustness when learning the signed distance field\ninduced by a polygonal mesh -- the _de facto_ standard representation. Viewed\nas a lossy compression, our conversion outperforms standard techniques from\ngeometry processing. Compared to previous latent- and weight-encoded neural\nimplicits we demonstrate superior robustness, scalability, and performance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 23:10:19 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 15:17:02 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2021 21:27:01 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Davies", "Thomas", ""], ["Nowrouzezahrai", "Derek", ""], ["Jacobson", "Alec", ""]]}, {"id": "2009.09809", "submitter": "Sounak Dey", "authors": "Andres Mafla, Sounak Dey, Ali Furkan Biten, Lluis Gomez and\n  Dimosthenis Karatzas", "title": "Multi-Modal Reasoning Graph for Scene-Text Based Fine-Grained Image\n  Classification and Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text instances found in natural images carry explicit semantic\ninformation that can provide important cues to solve a wide array of computer\nvision problems. In this paper, we focus on leveraging multi-modal content in\nthe form of visual and textual cues to tackle the task of fine-grained image\nclassification and retrieval. First, we obtain the text instances from images\nby employing a text reading system. Then, we combine textual features with\nsalient image regions to exploit the complementary information carried by the\ntwo sources. Specifically, we employ a Graph Convolutional Network to perform\nmulti-modal reasoning and obtain relationship-enhanced features by learning a\ncommon semantic space between salient objects and text found in an image. By\nobtaining an enhanced set of visual and textual features, the proposed model\ngreatly outperforms the previous state-of-the-art in two different tasks,\nfine-grained classification and image retrieval in the Con-Text and Drink\nBottle datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 12:31:42 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Mafla", "Andres", ""], ["Dey", "Sounak", ""], ["Biten", "Ali Furkan", ""], ["Gomez", "Lluis", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "2009.09813", "submitter": "Naoki Wake", "authors": "Naoki Wake, Kazuhiro Sasabuchi, Katsushi Ikeuchi", "title": "Grasp-type Recognition Leveraging Object Affordance", "comments": "2 pages, 2 figures. Submitted to and accepted by HOBI (IEEE RO-MAN\n  Workshop 2020). Last updated August 26th, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in robot teaching is grasp-type recognition with a single RGB\nimage and a target object name. Here, we propose a simple yet effective\npipeline to enhance learning-based recognition by leveraging a prior\ndistribution of grasp types for each object. In the pipeline, a convolutional\nneural network (CNN) recognizes the grasp type from an RGB image. The\nrecognition result is further corrected using the prior distribution (i.e.,\naffordance), which is associated with the target object name. Experimental\nresults showed that the proposed method outperforms both a CNN-only and an\naffordance-only method. The results highlight the effectiveness of\nlinguistically-driven object affordance for enhancing grasp-type recognition in\nrobot teaching.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 08:40:27 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Wake", "Naoki", ""], ["Sasabuchi", "Kazuhiro", ""], ["Ikeuchi", "Katsushi", ""]]}, {"id": "2009.09818", "submitter": "Umar Asif", "authors": "Umar Asif, Deval Mehta, Stefan von Cavallar, Jianbin Tang, and Stefan\n  Harrer", "title": "DeepActsNet: Spatial and Motion features from Face, Hands, and Body\n  Combined with Convolutional and Graph Networks for Improved Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing action recognition methods mainly focus on joint and bone\ninformation in human body skeleton data due to its robustness to complex\nbackgrounds and dynamic characteristics of the environments. In this paper, we\ncombine body skeleton data with spatial and motion features from face and two\nhands, and present \"Deep Action Stamps (DeepActs)\", a novel data representation\nto encode actions from video sequences. We also present \"DeepActsNet\", a deep\nlearning based ensemble model which learns convolutional and structural\nfeatures from Deep Action Stamps for highly accurate action recognition.\nExperiments on three challenging action recognition datasets (NTU60, NTU120,\nand SYSU) show that the proposed model trained using Deep Action Stamps produce\nconsiderable improvements in the action recognition accuracy with less\ncomputational cost compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 12:41:56 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 22:52:13 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 04:09:54 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Asif", "Umar", ""], ["Mehta", "Deval", ""], ["von Cavallar", "Stefan", ""], ["Tang", "Jianbin", ""], ["Harrer", "Stefan", ""]]}, {"id": "2009.09878", "submitter": "Apratim Bhattacharyya", "authors": "Apratim Bhattacharyya, Christoph-Nikolas Straehle, Mario Fritz, Bernt\n  Schiele", "title": "Haar Wavelet based Block Autoregressive Flows for Trajectories", "comments": "German Conference on Pattern Recognition, 2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of trajectories such as that of pedestrians is crucial to the\nperformance of autonomous agents. While previous works have leveraged\nconditional generative models like GANs and VAEs for learning the likely future\ntrajectories, accurately modeling the dependency structure of these multimodal\ndistributions, particularly over long time horizons remains challenging.\nNormalizing flow based generative models can model complex distributions\nadmitting exact inference. These include variants with split coupling\ninvertible transformations that are easier to parallelize compared to their\nautoregressive counterparts. To this end, we introduce a novel Haar wavelet\nbased block autoregressive model leveraging split couplings, conditioned on\ncoarse trajectories obtained from Haar wavelet based transformations at\ndifferent levels of granularity. This yields an exact inference method that\nmodels trajectories at different spatio-temporal resolutions in a hierarchical\nmanner. We illustrate the advantages of our approach for generating diverse and\naccurate trajectories on two real-world datasets - Stanford Drone and\nIntersection Drone.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 13:57:10 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Bhattacharyya", "Apratim", ""], ["Straehle", "Christoph-Nikolas", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""]]}, {"id": "2009.09893", "submitter": "R. Ian Etheredge", "authors": "R. Ian Etheredge, Manfred Schartl, Alex Jordan", "title": "Decontextualized learning for interpretable hierarchical representations\n  of visual patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apart from discriminative models for classification and object detection\ntasks, the application of deep convolutional neural networks to basic research\nutilizing natural imaging data has been somewhat limited; particularly in cases\nwhere a set of interpretable features for downstream analysis is needed, a key\nrequirement for many scientific investigations. We present an algorithm and\ntraining paradigm designed specifically to address this: decontextualized\nhierarchical representation learning (DHRL). By combining a generative model\nchaining procedure with a ladder network architecture and latent space\nregularization for inference, DHRL address the limitations of small datasets\nand encourages a disentangled set of hierarchically organized features. In\naddition to providing a tractable path for analyzing complex hierarchal\npatterns using variation inference, this approach is generative and can be\ndirectly combined with empirical and theoretical approaches. To highlight the\nextensibility and usefulness of DHRL, we demonstrate this method in application\nto a question from evolutionary biology.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 14:47:55 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Etheredge", "R. Ian", ""], ["Schartl", "Manfred", ""], ["Jordan", "Alex", ""]]}, {"id": "2009.09897", "submitter": "Emilio Garcia-Fidalgo", "authors": "Joan P. Company-Corcoles, Emilio Garcia-Fidalgo, Alberto Ortiz", "title": "LiPo-LCD: Combining Lines and Points for Appearance-based Loop Closure\n  Detection", "comments": "British Machine Vision Conference (BMVC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual SLAM approaches typically depend on loop closure detection to correct\nthe inconsistencies that may arise during the map and camera trajectory\ncalculations, typically making use of point features for detecting and closing\nthe existing loops. In low-textured scenarios, however, it is difficult to find\nenough point features and, hence, the performance of these solutions drops\ndrastically. An alternative for human-made scenarios, due to their structural\nregularity, is the use of geometrical cues such as straight segments,\nfrequently present within these environments. Under this context, in this paper\nwe introduce LiPo-LCD, a novel appearance-based loop closure detection method\nthat integrates lines and points. Adopting the idea of incremental\nBag-of-Binary-Words schemes, we build separate BoW models for each feature, and\nuse them to retrieve previously seen images using a late fusion strategy.\nAdditionally, a simple but effective mechanism, based on the concept of island,\ngroups similar images close in time to reduce the image candidate search\neffort. A final step validates geometrically the loop candidates by\nincorporating the detected lines by means of a process comprising a line\nfeature matching stage, followed by a robust spatial verification stage, now\ncombining both lines and points. As it is reported in the paper, LiPo-LCD\ncompares well with several state-of-the-art solutions for a number of datasets\ninvolving different environmental conditions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 10:43:16 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Company-Corcoles", "Joan P.", ""], ["Garcia-Fidalgo", "Emilio", ""], ["Ortiz", "Alberto", ""]]}, {"id": "2009.09898", "submitter": "William Diggin", "authors": "William Diggin and Michael Diggin", "title": "Efficient Computation of Higher Order 2D Image Moments using the\n  Discrete Radon Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric moments and moment invariants of image artifacts have many uses in\ncomputer vision applications, e.g. shape classification or object position and\norientation. Higher order moments are of interest to provide additional feature\ndescriptors, to measure kurtosis or to resolve n-fold symmetry. This paper\nprovides the method and practical application to extend an efficient algorithm,\nbased on the Discrete Radon Transform, to generate moments greater than the 3rd\norder. The mathematical fundamentals are presented, followed by relevant\nimplementation details. Results of scaling the algorithm based on image area\nand its computational comparison with a standard method demonstrate the\nefficacy of the approach.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 15:26:03 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Diggin", "William", ""], ["Diggin", "Michael", ""]]}, {"id": "2009.09899", "submitter": "Jacob Householder", "authors": "Jacob Householder, Andrew Householder, John Paul Gomez-Reed, Fredrick\n  Park, Shuai Zhang", "title": "Clustering COVID-19 Lung Scans", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent outbreak of COVID-19, creating a means to stop it's spread\nand eventually develop a vaccine are the most important and challenging tasks\nthat the scientific community is facing right now. The first step towards these\ngoals is to correctly identify a patient that is infected with the virus. Our\ngroup applied an unsupervised machine learning technique to identify COVID-19\ncases. This is an important topic as COVID-19 is a novel disease currently\nbeing studied in detail and our methodology has the potential to reveal\nimportant differences between it and other viral pneumonia. This could then, in\nturn, enable doctors to more confidently help each patient. Our experiments\nutilize Principal Component Analysis (PCA), t-distributed Stochastic Neighbor\nEmbedding (t-SNE), and the recently developed Robust Continuous Clustering\nalgorithm (RCC). We display the performance of RCC in identifying COVID-19\npatients and its ability to compete with other unsupervised algorithms, namely\nK-Means++ (KM++). Using a COVID-19 Radiography dataset, we found that RCC\noutperformed KM++; we used the Adjusted Mutual Information Score (AMI) in order\nto measure the effectiveness of both algorithms. The AMI for the two and three\nclass cases of KM++ were 0.0250 and 0.054, respectively. In comparison, RCC\nscored 0.5044 in the two class case and 0.267 in the three class case, clearly\nshowing RCC as the superior algorithm. This not only opens new possible\napplications of RCC, but it could potentially aid in the creation of a new tool\nfor COVID-19 identification.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 00:21:13 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Householder", "Jacob", ""], ["Householder", "Andrew", ""], ["Gomez-Reed", "John Paul", ""], ["Park", "Fredrick", ""], ["Zhang", "Shuai", ""]]}, {"id": "2009.09900", "submitter": "Patrick McClure", "authors": "Patrick McClure, Gabrielle Reimann, Michal Ramot and Francisco Pereira", "title": "A Deep Neural Network Tool for Automatic Segmentation of Human Body\n  Parts in Natural Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short article describes a deep neural network trained to perform\nautomatic segmentation of human body parts in natural scenes. More\nspecifically, we trained a Bayesian SegNet with concrete dropout on the\nPascal-Parts dataset to predict whether each pixel in a given frame was part of\na person's hair, head, ear, eyebrows, legs, arms, mouth, neck, nose, or torso.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 01:20:50 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["McClure", "Patrick", ""], ["Reimann", "Gabrielle", ""], ["Ramot", "Michal", ""], ["Pereira", "Francisco", ""]]}, {"id": "2009.09918", "submitter": "Philipp Terh\\\"orst", "authors": "Philipp Terh\\\"orst, Daniel F\\\"ahrmann, Naser Damer, Florian\n  Kirchbuchner, Arjan Kuijper", "title": "Beyond Identity: What Information Is Stored in Biometric Face Templates?", "comments": "To appear in IJCB 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deeply-learned face representations enable the success of current face\nrecognition systems. Despite the ability of these representations to encode the\nidentity of an individual, recent works have shown that more information is\nstored within, such as demographics, image characteristics, and social traits.\nThis threatens the user's privacy, since for many applications these templates\nare expected to be solely used for recognition purposes. Knowing the encoded\ninformation in face templates helps to develop bias-mitigating and\nprivacy-preserving face recognition technologies. This work aims to support the\ndevelopment of these two branches by analysing face templates regarding 113\nattributes. Experiments were conducted on two publicly available face\nembeddings. For evaluating the predictability of the attributes, we trained a\nmassive attribute classifier that is additionally able to accurately state its\nprediction confidence. This allows us to make more sophisticated statements\nabout the attribute predictability. The results demonstrate that up to 74\nattributes can be accurately predicted from face templates. Especially\nnon-permanent attributes, such as age, hairstyles, haircolors, beards, and\nvarious accessories, found to be easily-predictable. Since face recognition\nsystems aim to be robust against these variations, future research might build\non this work to develop more understandable privacy preserving solutions and\nbuild robust and fair face templates.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 14:41:18 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Terh\u00f6rst", "Philipp", ""], ["F\u00e4hrmann", "Daniel", ""], ["Damer", "Naser", ""], ["Kirchbuchner", "Florian", ""], ["Kuijper", "Arjan", ""]]}, {"id": "2009.09924", "submitter": "Peyman Moghadam", "authors": "Scarlett Raine, Ross Marchant, Peyman Moghadam, Frederic Maire, Brett\n  Kettle, Brano Kusy", "title": "Multi-species Seagrass Detection and Classification from Underwater\n  Images", "comments": "Accepted to DICTA 2020. project page is at:\n  https://github.com/csiro-robotics/deepseagrass", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater surveys conducted using divers or robots equipped with customized\ncamera payloads can generate a large number of images. Manual review of these\nimages to extract ecological data is prohibitive in terms of time and cost,\nthus providing strong incentive to automate this process using machine learning\nsolutions. In this paper, we introduce a multi-species detector and classifier\nfor seagrasses based on a deep convolutional neural network (achieved an\noverall accuracy of 92.4%). We also introduce a simple method to\nsemi-automatically label image patches and therefore minimize manual labelling\nrequirement. We describe and release publicly the dataset collected in this\nstudy as well as the code and pre-trained models to replicate our experiments\nat: https://github.com/csiro-robotics/deepseagrass\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 07:20:44 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Raine", "Scarlett", ""], ["Marchant", "Ross", ""], ["Moghadam", "Peyman", ""], ["Maire", "Frederic", ""], ["Kettle", "Brett", ""], ["Kusy", "Brano", ""]]}, {"id": "2009.09926", "submitter": "Po Li", "authors": "Po Li, Lei Li, Yan Fu, Jun Rong, Yu Zhang", "title": "Cross-Modal Alignment with Mixture Experts Neural Network for\n  Intral-City Retail Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Cross-modal Alignment with mixture experts Neural\nNetwork (CameNN) recommendation model for intral-city retail industry, which\naims to provide fresh foods and groceries retailing within 5 hours delivery\nservice arising for the outbreak of Coronavirus disease (COVID-19) pandemic\naround the world. We propose CameNN, which is a multi-task model with three\ntasks including Image to Text Alignment (ITA) task, Text to Image Alignment\n(TIA) task and CVR prediction task. We use pre-trained BERT to generate the\ntext embedding and pre-trained InceptionV4 to generate image patch embedding\n(each image is split into small patches with the same pixels and treat each\npatch as an image token). Softmax gating networks follow to learn the weight of\neach transformer expert output and choose only a subset of experts conditioned\non the input. Then transformer encoder is applied as the share-bottom layer to\nlearn all input features' shared interaction. Next, mixture of transformer\nexperts (MoE) layer is implemented to model different aspects of tasks. At top\nof the MoE layer, we deploy a transformer layer for each task as task tower to\nlearn task-specific information. On the real word intra-city dataset,\nexperiments demonstrate CameNN outperform baselines and achieve significant\nimprovements on the image and text representation. In practice, we applied\nCameNN on CVR prediction in our intra-city recommender system which is one of\nthe leading intra-city platforms operated in China.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 02:36:52 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Li", "Po", ""], ["Li", "Lei", ""], ["Fu", "Yan", ""], ["Rong", "Jun", ""], ["Zhang", "Yu", ""]]}, {"id": "2009.09928", "submitter": "Yue Liu", "authors": "Yue Liu, Alex Colburn, Mehlika Inanici", "title": "Deep Neural Network Approach for Annual Luminance Simulations", "comments": null, "journal-ref": null, "doi": "10.1080/19401493.2020.1803404", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Annual luminance maps provide meaningful evaluations for occupants' visual\ncomfort, preferences, and perception. However, acquiring long-term luminance\nmaps require labor-intensive and time-consuming simulations or impracticable\nlong-term field measurements. This paper presents a novel data-driven machine\nlearning approach that makes annual luminance-based evaluations more efficient\nand accessible. The methodology is based on predicting the annual luminance\nmaps from a limited number of point-in-time high dynamic range imagery by\nutilizing a deep neural network (DNN). Panoramic views are utilized, as they\ncan be post-processed to study multiple view directions. The proposed DNN model\ncan faithfully predict high-quality annual panoramic luminance maps from one of\nthe three options within 30 minutes training time: a) point-in-time luminance\nimagery spanning 5% of the year, when evenly distributed during daylight hours,\nb) one-month hourly imagery generated or collected continuously during daylight\nhours around the equinoxes (8% of the year); or c) 9 days of hourly data\ncollected around the spring equinox, summer and winter solstices (2.5% of the\nyear) all suffice to predict the luminance maps for the rest of the year. The\nDNN predicted high-quality panoramas are validated against Radiance (RPICT)\nrenderings using a series of quantitative and qualitative metrics. The most\nefficient predictions are achieved with 9 days of hourly data collected around\nthe spring equinox, summer and winter solstices. The results clearly show that\npractitioners and researchers can efficiently incorporate long-term\nluminance-based metrics over multiple view directions into the design and\nresearch processes using the proposed DNN workflow.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 20:19:21 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Liu", "Yue", ""], ["Colburn", "Alex", ""], ["Inanici", "Mehlika", ""]]}, {"id": "2009.09929", "submitter": "Vincenzo Lomonaco PhD", "authors": "Vincenzo Lomonaco, Lorenzo Pellegrini, Pau Rodriguez, Massimo Caccia,\n  Qi She, Yu Chen, Quentin Jodelet, Ruiping Wang, Zheda Mai, David Vazquez,\n  German I. Parisi, Nikhil Churamani, Marc Pickett, Issam Laradji, Davide\n  Maltoni", "title": "CVPR 2020 Continual Learning in Computer Vision Competition: Approaches,\n  Results, Current Challenges and Future Directions", "comments": "Pre-print v1: 12 pages, 3 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, we have witnessed a renewed and fast-growing interest\nin continual learning with deep neural networks with the shared objective of\nmaking current AI systems more adaptive, efficient and autonomous. However,\ndespite the significant and undoubted progress of the field in addressing the\nissue of catastrophic forgetting, benchmarking different continual learning\napproaches is a difficult task by itself. In fact, given the proliferation of\ndifferent settings, training and evaluation protocols, metrics and\nnomenclature, it is often tricky to properly characterize a continual learning\nalgorithm, relate it to other solutions and gauge its real-world applicability.\nThe first Continual Learning in Computer Vision challenge held at CVPR in 2020\nhas been one of the first opportunities to evaluate different continual\nlearning algorithms on a common hardware with a large set of shared evaluation\nmetrics and 3 different settings based on the realistic CORe50 video benchmark.\nIn this paper, we report the main results of the competition, which counted\nmore than 79 teams registered, 11 finalists and 2300$ in prizes. We also\nsummarize the winning approaches, current challenges and future research\ndirections.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 08:53:05 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Lomonaco", "Vincenzo", ""], ["Pellegrini", "Lorenzo", ""], ["Rodriguez", "Pau", ""], ["Caccia", "Massimo", ""], ["She", "Qi", ""], ["Chen", "Yu", ""], ["Jodelet", "Quentin", ""], ["Wang", "Ruiping", ""], ["Mai", "Zheda", ""], ["Vazquez", "David", ""], ["Parisi", "German I.", ""], ["Churamani", "Nikhil", ""], ["Pickett", "Marc", ""], ["Laradji", "Issam", ""], ["Maltoni", "Davide", ""]]}, {"id": "2009.09932", "submitter": "Song Cheng", "authors": "Song Cheng, Lei Wang, Pan Zhang", "title": "Supervised Learning with Projected Entangled Pair States", "comments": "7 pages, 4 figures, 1 table", "journal-ref": "Phys. Rev. B 103, 125117 (2021)", "doi": "10.1103/PhysRevB.103.125117", "report-no": null, "categories": "cs.CV cond-mat.str-el cs.LG quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor networks, a model that originated from quantum physics, has been\ngradually generalized as efficient models in machine learning in recent years.\nHowever, in order to achieve exact contraction, only tree-like tensor networks\nsuch as the matrix product states and tree tensor networks have been\nconsidered, even for modeling two-dimensional data such as images. In this\nwork, we construct supervised learning models for images using the projected\nentangled pair states (PEPS), a two-dimensional tensor network having a similar\nstructure prior to natural images. Our approach first performs a feature map,\nwhich transforms the image data to a product state on a grid, then contracts\nthe product state to a PEPS with trainable parameters to predict image labels.\nThe tensor elements of PEPS are trained by minimizing differences between\ntraining labels and predicted labels. The proposed model is evaluated on image\nclassifications using the MNIST and the Fashion-MNIST datasets. We show that\nour model is significantly superior to existing models using tree-like tensor\nnetworks. Moreover, using the same input features, our method performs as well\nas the multilayer perceptron classifier, but with much fewer parameters and is\nmore stable. Our results shed light on potential applications of\ntwo-dimensional tensor network models in machine learning.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 09:15:00 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Cheng", "Song", ""], ["Wang", "Lei", ""], ["Zhang", "Pan", ""]]}, {"id": "2009.09933", "submitter": "Markus D. Solbach", "authors": "Markus D. Solbach, John K. Tsotsos", "title": "PESAO: Psychophysical Experimental Setup for Active Observers", "comments": "http://data.nvision2.eecs.yorku.ca/PESAO/, technical report, 20\n  pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most past and present research in computer vision involves passively observed\ndata. Humans, however, are active observers outside the lab; they explore,\nsearch, select what and how to look. Nonetheless, how exactly active\nobservation occurs in humans so that it can inform the design of active\ncomputer vision systems is an open problem. PESAO is designed for investigating\nactive, visual observation in a 3D world. The goal was to build an experimental\nsetup for various active perception tasks with human subjects (active\nobservers) in mind that is capable of tracking the head and gaze. While many\nstudies explore human performances, usually, they use line drawings portrayed\nin 2D, and no active observer is involved. PESAO allows us to bring many\nstudies to the three-dimensional world, even involving active observers. In our\ninstantiation, it spans an area of 400cm x 300cm and can track active observers\nat a frequency of 120Hz. Furthermore, PESAO provides tracking and recording of\n6D head motion, gaze, eye movement-type, first-person video, head-mounted IMU\nsensor, birds-eye video, and experimenter notes. All are synchronized at\nmicrosecond resolution.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 15:06:48 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 17:12:50 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Solbach", "Markus D.", ""], ["Tsotsos", "John K.", ""]]}, {"id": "2009.09934", "submitter": "Abhinav Sagar", "authors": "Abhinav Sagar", "title": "Monocular Depth Estimation Using Multi Scale Neural Network And Feature\n  Fusion", "comments": "11 pages, 4 figures, Submitted to Neurips 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth estimation from monocular images is a challenging problem in computer\nvision. In this paper, we tackle this problem using a novel network\narchitecture using multi scale feature fusion. Our network uses two different\nblocks, first which uses different filter sizes for convolution and merges all\nthe individual feature maps. The second block uses dilated convolutions in\nplace of fully connected layers thus reducing computations and increasing the\nreceptive field. We present a new loss function for training the network which\nuses a depth regression term, SSIM loss term and a multinomial logistic loss\nterm combined. We train and test our network on Make 3D dataset, NYU Depth V2\ndataset and Kitti dataset using standard evaluation metrics for depth\nestimation comprised of RMSE loss and SILog loss. Our network outperforms\nprevious state of the art methods with lesser parameters.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 18:08:52 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Sagar", "Abhinav", ""]]}, {"id": "2009.09936", "submitter": "Michela Paganini", "authors": "Michela Paganini", "title": "Prune Responsibly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Irrespective of the specific definition of fairness in a machine learning\napplication, pruning the underlying model affects it. We investigate and\ndocument the emergence and exacerbation of undesirable per-class performance\nimbalances, across tasks and architectures, for almost one million categories\nconsidered across over 100K image classification models that undergo a pruning\nprocess.We demonstrate the need for transparent reporting, inclusive of bias,\nfairness, and inclusion metrics, in real-life engineering decision-making\naround neural network pruning. In response to the calls for quantitative\nevaluation of AI models to be population-aware, we present neural network\npruning as a tangible application domain where the ways in which\naccuracy-efficiency trade-offs disproportionately affect underrepresented or\noutlier groups have historically been overlooked. We provide a simple,\nPareto-based framework to insert fairness considerations into value-based\noperating point selection processes, and to re-evaluate pruning technique\nchoices.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 04:43:11 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Paganini", "Michela", ""]]}, {"id": "2009.09937", "submitter": "Morteza Heidari", "authors": "Morteza Heidari (1), Sivaramakrishnan Lakshmivarahan (2),\n  Seyedehnafiseh Mirniaharikandehei (1), Gopichandh Danala (1), Sai Kiran R.\n  Maryada (2), Hong Liu (1), Bin Zheng (1), ((1) School of Electrical and\n  Computer Engineering, University of Oklahoma, Norman, OK, USA, (2) School of\n  Computer Sciences, University of Oklahoma, Norman, OK, USA)", "title": "Applying a random projection algorithm to optimize machine learning\n  model for breast lesion classification", "comments": "11 pages, 6 figures", "journal-ref": "IEEE Transactions on Biomedical Engineering, 2021", "doi": "10.1109/TBME.2021.3054248", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is widely used in developing computer-aided diagnosis (CAD)\nschemes of medical images. However, CAD usually computes large number of image\nfeatures from the targeted regions, which creates a challenge of how to\nidentify a small and optimal feature vector to build robust machine learning\nmodels. In this study, we investigate feasibility of applying a random\nprojection algorithm to build an optimal feature vector from the initially\nCAD-generated large feature pool and improve performance of machine learning\nmodel. We assemble a retrospective dataset involving 1,487 cases of mammograms\nin which 644 cases have confirmed malignant mass lesions and 843 have benign\nlesions. A CAD scheme is first applied to segment mass regions and initially\ncompute 181 features. Then, support vector machine (SVM) models embedded with\nseveral feature dimensionality reduction methods are built to predict\nlikelihood of lesions being malignant. All SVM models are trained and tested\nusing a leave-one-case-out cross-validation method. SVM generates a likelihood\nscore of each segmented mass region depicting on one-view mammogram. By fusion\nof two scores of the same mass depicting on two-view mammograms, a case-based\nlikelihood score is also evaluated. Comparing with the principle component\nanalyses, nonnegative matrix factorization, and Chi-squared methods, SVM\nembedded with the random projection algorithm yielded a significantly higher\ncase-based lesion classification performance with the area under ROC curve of\n0.84+0.01 (p<0.02). The study demonstrates that the random project algorithm is\na promising method to generate optimal feature vectors to help improve\nperformance of machine learning models of medical images.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 21:27:27 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Heidari", "Morteza", ""], ["Lakshmivarahan", "Sivaramakrishnan", ""], ["Mirniaharikandehei", "Seyedehnafiseh", ""], ["Danala", "Gopichandh", ""], ["Maryada", "Sai Kiran R.", ""], ["Liu", "Hong", ""], ["Zheng", "Bin", ""]]}, {"id": "2009.09938", "submitter": "Wei Wang", "authors": "Wei Wang, Yanjie Zhu, Zhuoxu Cui, Dong Liang", "title": "Is Each Layer Non-trivial in CNN?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) models have achieved great success in many\nfields. With the advent of ResNet, networks used in practice are getting deeper\nand wider. However, is each layer non-trivial in networks? To answer this\nquestion, we trained a network on the training set, then we replace the network\nconvolution kernels with zeros and test the result models on the test set. We\ncompared experimental results with baseline and showed that we can reach\nsimilar or even the same performances. Although convolution kernels are the\ncores of networks, we demonstrate that some of them are trivial and regular in\nResNet.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 02:17:49 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 02:23:09 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Wang", "Wei", ""], ["Zhu", "Yanjie", ""], ["Cui", "Zhuoxu", ""], ["Liang", "Dong", ""]]}, {"id": "2009.09939", "submitter": "Umut \\\"Ozkaya", "authors": "Umut \\\"Ozkaya", "title": "Automatic Target Recognition (ATR) from SAR Imaginary by Using Machine\n  Learning Techniques", "comments": "5 page, 2 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic Target Recognition (ATR) in Synthetic aperture radar (SAR) images\nbecomes a very challenging problem owing to containing high level noise. In\nthis study, a machine learning-based method is proposed to detect different\nmoving and stationary targets using SAR images. First Order Statistical (FOS)\nfeatures were obtained from Fast Fourier Transform (FFT), Discrete Cosine\nTransform (DCT) and Discrete Wavelet Transform (DWT) on gray level SAR images.\nGray Level Co-occurrence Matrix (GLCM), Gray Level Run Length Matrix (GLRLM)\nand Gray Level Size Zone Matrix (GLSZM) algorithms are also used. These\nfeatures are provided as input for the training and testing stage Support\nVector Machine (SVM) model with Gaussian kernels. 4-fold cross-validations were\nimplemented in performance evaluation. Obtained results showed that GLCM + SVM\nalgorithm is the best model with 95.26% accuracy. This proposed method shows\nthat moving and stationary targets in MSTAR database could be recognized with\nhigh performance.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 21:27:59 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["\u00d6zkaya", "Umut", ""]]}, {"id": "2009.09940", "submitter": "Guan Li", "authors": "Guan Li, Junpeng Wang, Han-Wei Shen, Kaixin Chen, Guihua Shan, and\n  Zhonghua Lu", "title": "CNNPruner: Pruning Convolutional Neural Networks with Visual Analytics", "comments": "10 pages,15 figures, Accepted for presentation at IEEE VIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have demonstrated extraordinarily good\nperformance in many computer vision tasks. The increasing size of CNN models,\nhowever, prevents them from being widely deployed to devices with limited\ncomputational resources, e.g., mobile/embedded devices. The emerging topic of\nmodel pruning strives to address this problem by removing less important\nneurons and fine-tuning the pruned networks to minimize the accuracy loss.\nNevertheless, existing automated pruning solutions often rely on a numerical\nthreshold of the pruning criteria, lacking the flexibility to optimally balance\nthe trade-off between model size and accuracy. Moreover, the complicated\ninterplay between the stages of neuron pruning and model fine-tuning makes this\nprocess opaque, and therefore becomes difficult to optimize. In this paper, we\naddress these challenges through a visual analytics approach, named CNNPruner.\nIt considers the importance of convolutional filters through both instability\nand sensitivity, and allows users to interactively create pruning plans\naccording to a desired goal on model size or accuracy. Also, CNNPruner\nintegrates state-of-the-art filter visualization techniques to help users\nunderstand the roles that different filters played and refine their pruning\nplans. Through comprehensive case studies on CNNs with real-world sizes, we\nvalidate the effectiveness of CNNPruner.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 02:08:20 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Li", "Guan", ""], ["Wang", "Junpeng", ""], ["Shen", "Han-Wei", ""], ["Chen", "Kaixin", ""], ["Shan", "Guihua", ""], ["Lu", "Zhonghua", ""]]}, {"id": "2009.09941", "submitter": "Ruoyu Guo", "authors": "Yuning Du, Chenxia Li, Ruoyu Guo, Xiaoting Yin, Weiwei Liu, Jun Zhou,\n  Yifan Bai, Zilin Yu, Yehua Yang, Qingqing Dang, Haoshuang Wang", "title": "PP-OCR: A Practical Ultra Lightweight OCR System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Optical Character Recognition (OCR) systems have been widely used in\nvarious of application scenarios, such as office automation (OA) systems,\nfactory automations, online educations, map productions etc. However, OCR is\nstill a challenging task due to the various of text appearances and the demand\nof computational efficiency. In this paper, we propose a practical ultra\nlightweight OCR system, i.e., PP-OCR. The overall model size of the PP-OCR is\nonly 3.5M for recognizing 6622 Chinese characters and 2.8M for recognizing 63\nalphanumeric symbols, respectively. We introduce a bag of strategies to either\nenhance the model ability or reduce the model size. The corresponding ablation\nexperiments with the real data are also provided. Meanwhile, several\npre-trained models for the Chinese and English recognition are released,\nincluding a text detector (97K images are used), a direction classifier (600K\nimages are used) as well as a text recognizer (17.9M images are used). Besides,\nthe proposed PP-OCR are also verified in several other language recognition\ntasks, including French, Korean, Japanese and German. All of the above\nmentioned models are open-sourced and the codes are available in the GitHub\nrepository, i.e., https://github.com/PaddlePaddle/PaddleOCR.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 14:57:18 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 08:57:29 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 14:21:53 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Du", "Yuning", ""], ["Li", "Chenxia", ""], ["Guo", "Ruoyu", ""], ["Yin", "Xiaoting", ""], ["Liu", "Weiwei", ""], ["Zhou", "Jun", ""], ["Bai", "Yifan", ""], ["Yu", "Zilin", ""], ["Yang", "Yehua", ""], ["Dang", "Qingqing", ""], ["Wang", "Haoshuang", ""]]}, {"id": "2009.09960", "submitter": "Jianzhu Guo", "authors": "Jianzhu Guo, Xiangyu Zhu, Yang Yang, Fan Yang, Zhen Lei and Stan Z. Li", "title": "Towards Fast, Accurate and Stable 3D Dense Face Alignment", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods of 3D dense face alignment mainly concentrate on accuracy,\nthus limiting the scope of their practical applications. In this paper, we\npropose a novel regression framework named 3DDFA-V2 which makes a balance among\nspeed, accuracy and stability. Firstly, on the basis of a lightweight backbone,\nwe propose a meta-joint optimization strategy to dynamically regress a small\nset of 3DMM parameters, which greatly enhances speed and accuracy\nsimultaneously. To further improve the stability on videos, we present a\nvirtual synthesis method to transform one still image to a short-video which\nincorporates in-plane and out-of-plane face moving. On the premise of high\naccuracy and stability, 3DDFA-V2 runs at over 50fps on a single CPU core and\noutperforms other state-of-the-art heavy models simultaneously. Experiments on\nseveral challenging datasets validate the efficiency of our method. Pre-trained\nmodels and code are available at https://github.com/cleardusk/3DDFA_V2.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 15:37:37 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 16:24:15 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Guo", "Jianzhu", ""], ["Zhu", "Xiangyu", ""], ["Yang", "Yang", ""], ["Yang", "Fan", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""]]}, {"id": "2009.09972", "submitter": "Qiuyuan Wang", "authors": "Qiuyuan Wang, Zike Yan, Junqiu Wang, Fei Xue, Wei Ma, Hongbin Zha", "title": "Line Flow based SLAM", "comments": "Accepted for publication in IEEE Transactions on Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a visual SLAM method by predicting and updating line flows that\nrepresent sequential 2D projections of 3D line segments. While feature-based\nSLAM methods have achieved excellent results, they still face problems in\nchallenging scenes containing occlusions, blurred images, and repetitive\ntextures. To address these problems, we leverage a line flow to encode the\ncoherence of line segment observations of the same 3D line along the temporal\ndimension, which has been neglected in prior SLAM systems. Thanks to this line\nflow representation, line segments in a new frame can be predicted according to\ntheir corresponding 3D lines and their predecessors along the temporal\ndimension. We create, update, merge, and discard line flows on-the-fly. We\nmodel the proposed line flow based SLAM (LF-SLAM) using a Bayesian network.\nExtensive experimental results demonstrate that the proposed LF-SLAM method\nachieves state-of-the-art results due to the utilization of line flows.\nSpecifically, LF-SLAM obtains good localization and mapping results in\nchallenging scenes with occlusions, blurred images, and repetitive textures.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 15:55:45 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 14:48:27 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Wang", "Qiuyuan", ""], ["Yan", "Zike", ""], ["Wang", "Junqiu", ""], ["Xue", "Fei", ""], ["Ma", "Wei", ""], ["Zha", "Hongbin", ""]]}, {"id": "2009.09976", "submitter": "Zongwei Wu", "authors": "Zongwei Wu, Guillaume Allibert, Christophe Stolz, Cedric Demonceaux", "title": "Depth-Adapted CNN for RGB-D cameras", "comments": "Accepted manuscript in ACCV 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional 2D Convolutional Neural Networks (CNN) extract features from an\ninput image by applying linear filters. These filters compute the spatial\ncoherence by weighting the photometric information on a fixed neighborhood\nwithout taking into account the geometric information. We tackle the problem of\nimproving the classical RGB CNN methods by using the depth information provided\nby the RGB-D cameras. State-of-the-art approaches use depth as an additional\nchannel or image (HHA) or pass from 2D CNN to 3D CNN. This paper proposes a\nnovel and generic procedure to articulate both photometric and geometric\ninformation in CNN architecture. The depth data is represented as a 2D offset\nto adapt spatial sampling locations. The new model presented is invariant to\nscale and rotation around the X and the Y axis of the camera coordinate system.\nMoreover, when depth data is constant, our model is equivalent to a regular\nCNN. Experiments of benchmarks validate the effectiveness of our model.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 15:58:32 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 09:45:21 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Wu", "Zongwei", ""], ["Allibert", "Guillaume", ""], ["Stolz", "Christophe", ""], ["Demonceaux", "Cedric", ""]]}, {"id": "2009.09981", "submitter": "Marcelin Tworski", "authors": "Marcelin Tworski, St\\'ephane Lathuili\\`ere, Salim Belkarfa, Attilio\n  Fiandrotti, Marco Cagnazzo", "title": "DR2S : Deep Regression with Region Selection for Camera Quality\n  Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we tackle the problem of estimating a camera capability to\npreserve fine texture details at a given lighting condition. Importantly, our\ntexture preservation measurement should coincide with human perception.\nConsequently, we formulate our problem as a regression one and we introduce a\ndeep convolutional network to estimate texture quality score. At training time,\nwe use ground-truth quality scores provided by expert human annotators in order\nto obtain a subjective quality measure. In addition, we propose a region\nselection method to identify the image regions that are better suited at\nmeasuring perceptual quality. Finally, our experimental evaluation shows that\nour learning-based approach outperforms existing methods and that our region\nselection algorithm consistently improves the quality estimation.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 16:05:15 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Tworski", "Marcelin", ""], ["Lathuili\u00e8re", "St\u00e9phane", ""], ["Belkarfa", "Salim", ""], ["Fiandrotti", "Attilio", ""], ["Cagnazzo", "Marco", ""]]}, {"id": "2009.09984", "submitter": "Asad Anwar Butt", "authors": "George Awad, Asad A. Butt, Keith Curtis, Yooyoung Lee, Jonathan\n  Fiscus, Afzal Godil, Andrew Delgado, Jesse Zhang, Eliot Godard, Lukas Diduch,\n  Alan F. Smeaton, Yvette Graham, Wessel Kraaij, Georges Quenot", "title": "TRECVID 2019: An Evaluation Campaign to Benchmark Video Activity\n  Detection, Video Captioning and Matching, and Video Search & Retrieval", "comments": "TRECVID Workshop overview paper. 39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The TREC Video Retrieval Evaluation (TRECVID) 2019 was a TREC-style video\nanalysis and retrieval evaluation, the goal of which remains to promote\nprogress in research and development of content-based exploitation and\nretrieval of information from digital video via open, metrics-based evaluation.\nOver the last nineteen years this effort has yielded a better understanding of\nhow systems can effectively accomplish such processing and how one can reliably\nbenchmark their performance. TRECVID has been funded by NIST (National\nInstitute of Standards and Technology) and other US government agencies. In\naddition, many organizations and individuals worldwide contribute significant\ntime and effort. TRECVID 2019 represented a continuation of four tasks from\nTRECVID 2018. In total, 27 teams from various research organizations worldwide\ncompleted one or more of the following four tasks: 1. Ad-hoc Video Search (AVS)\n2. Instance Search (INS) 3. Activities in Extended Video (ActEV) 4. Video to\nText Description (VTT) This paper is an introduction to the evaluation\nframework, tasks, data, and measures used in the workshop.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 16:08:47 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Awad", "George", ""], ["Butt", "Asad A.", ""], ["Curtis", "Keith", ""], ["Lee", "Yooyoung", ""], ["Fiscus", "Jonathan", ""], ["Godil", "Afzal", ""], ["Delgado", "Andrew", ""], ["Zhang", "Jesse", ""], ["Godard", "Eliot", ""], ["Diduch", "Lukas", ""], ["Smeaton", "Alan F.", ""], ["Graham", "Yvette", ""], ["Kraaij", "Wessel", ""], ["Quenot", "Georges", ""]]}, {"id": "2009.10003", "submitter": "Danfeng Hong", "authors": "Danfeng Hong, Naoto Yokoya, Jocelyn Chanussot, Jian Xu, Xiao Xiang Zhu", "title": "Joint and Progressive Subspace Analysis (JPSA) with Spatial-Spectral\n  Manifold Alignment for Semi-Supervised Hyperspectral Dimensionality Reduction", "comments": null, "journal-ref": "IEEE Transactions on Cybernetics, 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional nonlinear subspace learning techniques (e.g., manifold learning)\nusually introduce some drawbacks in explainability (explicit mapping) and\ncost-effectiveness (linearization), generalization capability (out-of-sample),\nand representability (spatial-spectral discrimination). To overcome these\nshortcomings, a novel linearized subspace analysis technique with\nspatial-spectral manifold alignment is developed for a semi-supervised\nhyperspectral dimensionality reduction (HDR), called joint and progressive\nsubspace analysis (JPSA). The JPSA learns a high-level, semantically\nmeaningful, joint spatial-spectral feature representation from hyperspectral\ndata by 1) jointly learning latent subspaces and a linear classifier to find an\neffective projection direction favorable for classification; 2) progressively\nsearching several intermediate states of subspaces to approach an optimal\nmapping from the original space to a potential more discriminative subspace; 3)\nspatially and spectrally aligning manifold structure in each learned latent\nsubspace in order to preserve the same or similar topological property between\nthe compressed data and the original data. A simple but effective classifier,\ni.e., nearest neighbor (NN), is explored as a potential application for\nvalidating the algorithm performance of different HDR approaches. Extensive\nexperiments are conducted to demonstrate the superiority and effectiveness of\nthe proposed JPSA on two widely-used hyperspectral datasets: Indian Pines\n(92.98\\%) and the University of Houston (86.09\\%) in comparison with previous\nstate-of-the-art HDR methods. The demo of this basic work (i.e., ECCV2018) is\nopenly available at https://github.com/danfenghong/ECCV2018_J-Play.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 16:29:59 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Hong", "Danfeng", ""], ["Yokoya", "Naoto", ""], ["Chanussot", "Jocelyn", ""], ["Xu", "Jian", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2009.10013", "submitter": "Akash Sengupta", "authors": "Akash Sengupta and Ignas Budvytis and Roberto Cipolla", "title": "Synthetic Training for Accurate 3D Human Pose and Shape Estimation in\n  the Wild", "comments": "14 pages, 7 figures, BMVC 2020, Fixed abstract typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of monocular 3D human shape and pose\nestimation from an RGB image. Despite great progress in this field in terms of\npose prediction accuracy, state-of-the-art methods often predict inaccurate\nbody shapes. We suggest that this is primarily due to the scarcity of\nin-the-wild training data with diverse and accurate body shape labels. Thus, we\npropose STRAPS (Synthetic Training for Real Accurate Pose and Shape), a system\nthat utilises proxy representations, such as silhouettes and 2D joints, as\ninputs to a shape and pose regression neural network, which is trained with\nsynthetic training data (generated on-the-fly during training using the SMPL\nstatistical body model) to overcome data scarcity. We bridge the gap between\nsynthetic training inputs and noisy real inputs, which are predicted by\nkeypoint detection and segmentation CNNs at test-time, by using data\naugmentation and corruption during training. In order to evaluate our approach,\nwe curate and provide a challenging evaluation dataset for monocular human\nshape estimation, Sports Shape and Pose 3D (SSP-3D). It consists of RGB images\nof tightly-clothed sports-persons with a variety of body shapes and\ncorresponding pseudo-ground-truth SMPL shape and pose parameters, obtained via\nmulti-frame optimisation. We show that STRAPS outperforms other\nstate-of-the-art methods on SSP-3D in terms of shape prediction accuracy, while\nremaining competitive with the state-of-the-art on pose-centric datasets and\nmetrics.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 16:39:04 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 10:27:05 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Sengupta", "Akash", ""], ["Budvytis", "Ignas", ""], ["Cipolla", "Roberto", ""]]}, {"id": "2009.10026", "submitter": "Mirantha Jayathilaka", "authors": "Mirantha Jayathilaka, Tingting Mu, Uli Sattler", "title": "Visual-Semantic Embedding Model Informed by Structured Knowledge", "comments": "European Starting AI Researchers' Symposium 2020 (STAIRS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to improve a visual-semantic embedding model by\nincorporating concept representations captured from an external structured\nknowledge base. We investigate its performance on image classification under\nboth standard and zero-shot settings. We propose two novel evaluation\nframeworks to analyse classification errors with respect to the class hierarchy\nindicated by the knowledge base. The approach is tested using the ILSVRC 2012\nimage dataset and a WordNet knowledge base. With respect to both standard and\nzero-shot image classification, our approach shows superior performance\ncompared with the original approach, which uses word embeddings.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 17:04:32 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Jayathilaka", "Mirantha", ""], ["Mu", "Tingting", ""], ["Sattler", "Uli", ""]]}, {"id": "2009.10054", "submitter": "Doyup Lee", "authors": "Doyup Lee, Yeongjae Cheon, Wook-Shin Han", "title": "Regularizing Attention Networks for Anomaly Detection in Visual Question\n  Answering", "comments": "16 pages, 7 figures, Accepted to AAAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For stability and reliability of real-world applications, the robustness of\nDNNs in unimodal tasks has been evaluated. However, few studies consider\nabnormal situations that a visual question answering (VQA) model might\nencounter at test time after deployment in the real-world. In this study, we\nevaluate the robustness of state-of-the-art VQA models to five different\nanomalies, including worst-case scenarios, the most frequent scenarios, and the\ncurrent limitation of VQA models. Different from the results in unimodal tasks,\nthe maximum confidence of answers in VQA models cannot detect anomalous inputs,\nand post-training of the outputs, such as outlier exposure, is ineffective for\nVQA models. Thus, we propose an attention-based method, which uses confidence\nof reasoning between input images and questions and shows much more promising\nresults than the previous methods in unimodal tasks. In addition, we show that\na maximum entropy regularization of attention networks can significantly\nimprove the attention-based anomaly detection of the VQA models. Thanks to the\nsimplicity, attention-based anomaly detection and the regularization are\nmodel-agnostic methods, which can be used for various cross-modal attentions in\nthe state-of-the-art VQA models. The results imply that cross-modal attention\nin VQA is important to improve not only VQA accuracy, but also the robustness\nto various anomalies.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 17:47:49 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 12:46:47 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Lee", "Doyup", ""], ["Cheon", "Yeongjae", ""], ["Han", "Wook-Shin", ""]]}, {"id": "2009.10058", "submitter": "Asim Iqbal", "authors": "Hassan Mahmood, Asim Iqbal, Syed Mohammed Shamsul Islam", "title": "Exploring Intensity Invariance in Deep Neural Networks for Brain Image\n  Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration is a widely-used technique in analysing large scale\ndatasets that are captured through various imaging modalities and techniques in\nbiomedical imaging such as MRI, X-Rays, etc. These datasets are typically\ncollected from various sites and under different imaging protocols using a\nvariety of scanners. Such heterogeneity in the data collection process causes\ninhomogeneity or variation in intensity (brightness) and noise distribution.\nThese variations play a detrimental role in the performance of image\nregistration, segmentation and detection algorithms. Classical image\nregistration methods are computationally expensive but are able to handle these\nartifacts relatively better. However, deep learning-based techniques are shown\nto be computationally efficient for automated brain registration but are\nsensitive to the intensity variations. In this study, we investigate the effect\nof variation in intensity distribution among input image pairs for deep\nlearning-based image registration methods. We find a performance degradation of\nthese models when brain image pairs with different intensity distribution are\npresented even with similar structures. To overcome this limitation, we\nincorporate a structural similarity-based loss function in a deep neural\nnetwork and test its performance on the validation split separated before\ntraining as well as on a completely unseen new dataset. We report that the deep\nlearning models trained with structure similarity-based loss seems to perform\nbetter for both datasets. This investigation highlights a possible performance\nlimiting factor in deep learning-based registration models and suggests a\npotential solution to incorporate the intensity distribution variation in the\ninput image pairs. Our code and models are available at\nhttps://github.com/hassaanmahmood/DeepIntense.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 17:49:03 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Mahmood", "Hassan", ""], ["Iqbal", "Asim", ""], ["Islam", "Syed Mohammed Shamsul", ""]]}, {"id": "2009.10066", "submitter": "Dongdong Chen", "authors": "Dengpan Fu and Bo Xin and Jingdong Wang and Dongdong Chen and Jianmin\n  Bao and Gang Hua and Houqiang Li", "title": "Improving Person Re-identification with Iterative Impression Aggregation", "comments": "Accepted by Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2020.3029415", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our impression about one person often updates after we see more aspects of\nhim/her and this process keeps iterating given more meetings. We formulate such\nan intuition into the problem of person re-identification (re-ID), where the\nrepresentation of a query (probe) image is iteratively updated with new\ninformation from the candidates in the gallery. Specifically, we propose a\nsimple attentional aggregation formulation to instantiate this idea and\nshowcase that such a pipeline achieves competitive performance on standard\nbenchmarks including CUHK03, Market-1501 and DukeMTMC. Not only does such a\nsimple method improve the performance of the baseline models, it also achieves\ncomparable performance with latest advanced re-ranking methods. Another\nadvantage of this proposal is its flexibility to incorporate different\nrepresentations and similarity metrics. By utilizing stronger representations\nand metrics, we further demonstrate state-of-the-art person re-ID performance,\nwhich also validates the general applicability of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 17:59:04 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Fu", "Dengpan", ""], ["Xin", "Bo", ""], ["Wang", "Jingdong", ""], ["Chen", "Dongdong", ""], ["Bao", "Jianmin", ""], ["Hua", "Gang", ""], ["Li", "Houqiang", ""]]}, {"id": "2009.10115", "submitter": "Franklin Mendivil", "authors": "Franklin Mendivil and \\\"Orjan Stenflo", "title": "Extreme compression of grayscale images", "comments": "16 pages, 12 figures", "journal-ref": null, "doi": "10.1016/j.cnsns.2020.105546", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an grayscale digital image, and a positive integer $n$, how well can we\nstore the image at a compression ratio of $n:1$?\n  In this paper we address the above question in extreme cases when $n>>50$\nusing \"$\\mathbf{V}$-variable image compression\".\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 18:19:23 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Mendivil", "Franklin", ""], ["Stenflo", "\u00d6rjan", ""]]}, {"id": "2009.10132", "submitter": "Sarah Jabbour", "authors": "Sarah Jabbour, David Fouhey, Ella Kazerooni, Michael W. Sjoding, Jenna\n  Wiens", "title": "Deep Learning Applied to Chest X-Rays: Exploiting and Preventing\n  Shortcuts", "comments": "32 pages, 9 figures, 12 tables, MLHC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning has shown promise in improving the automated diagnosis of\ndisease based on chest X-rays, deep networks may exhibit undesirable behavior\nrelated to shortcuts. This paper studies the case of spurious class skew in\nwhich patients with a particular attribute are spuriously more likely to have\nthe outcome of interest. For instance, clinical protocols might lead to a\ndataset in which patients with pacemakers are disproportionately likely to have\ncongestive heart failure. This skew can lead to models that take shortcuts by\nheavily relying on the biased attribute. We explore this problem across a\nnumber of attributes in the context of diagnosing the cause of acute hypoxemic\nrespiratory failure. Applied to chest X-rays, we show that i) deep nets can\naccurately identify many patient attributes including sex (AUROC = 0.96) and\nage (AUROC >= 0.90), ii) they tend to exploit correlations between such\nattributes and the outcome label when learning to predict a diagnosis, leading\nto poor performance when such correlations do not hold in the test population\n(e.g., everyone in the test set is male), and iii) a simple transfer learning\napproach is surprisingly effective at preventing the shortcut and promoting\ngood generalization performance. On the task of diagnosing congestive heart\nfailure based on a set of chest X-rays skewed towards older patients (age >=\n63), the proposed approach improves generalization over standard training from\n0.66 (95% CI: 0.54-0.77) to 0.84 (95% CI: 0.73-0.92) AUROC. While simple, the\nproposed approach has the potential to improve the performance of models across\npopulations by encouraging reliance on clinically relevant manifestations of\ndisease, i.e., those that a clinician would use to make a diagnosis.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 18:52:43 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Jabbour", "Sarah", ""], ["Fouhey", "David", ""], ["Kazerooni", "Ella", ""], ["Sjoding", "Michael W.", ""], ["Wiens", "Jenna", ""]]}, {"id": "2009.10141", "submitter": "Ali Al-Bawi", "authors": "Ali Al-Bawi, Karrar Ali Al-Kaabi, Mohammed Jeryo, Ahmad Al-Fatlawi", "title": "CCBlock: An Effective Use of Deep Learning for Automatic Diagnosis of\n  COVID-19 Using X-Ray Images", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": "10.1007/s42600-020-00110-7", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Propose: Troubling countries one after another, the COVID-19 pandemic has\ndramatically affected the health and well-being of the world's population. The\ndisease may continue to persist more extensively due to the increasing number\nof new cases daily, the rapid spread of the virus, and delay in the PCR\nanalysis results. Therefore, it is necessary to consider developing assistive\nmethods for detecting and diagnosing the COVID-19 to eradicate the spread of\nthe novel coronavirus among people. Based on convolutional neural networks\n(CNNs), automated detection systems have shown promising results of diagnosing\npatients with the COVID-19 through radiography; thus, they are introduced as a\nworkable solution to the COVID-19 diagnosis. Materials and Methods: Based on\nthe enhancement of the classical visual geometry group (VGG) network with the\nconvolutional COVID block (CCBlock), an efficient screening model was proposed\nin this study to diagnose and distinguish patients with the COVID-19 from those\nwith pneumonia and the healthy people through radiography. The model testing\ndataset included 1,828 x-ray images available on public platforms. 310 images\nwere showing confirmed COVID-19 cases, 864 images indicating pneumonia cases,\nand 654 images showing healthy people. Results: According to the test results,\nenhancing the classical VGG network with radiography provided the highest\ndiagnosis performance and overall accuracy of 98.52% for two classes as well as\naccuracy of 95.34% for three classes. Conclusions: According to the results,\nusing the enhanced VGG deep neural network can help radiologists automatically\ndiagnose the COVID-19 through radiography.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 19:20:01 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Al-Bawi", "Ali", ""], ["Al-Kaabi", "Karrar Ali", ""], ["Jeryo", "Mohammed", ""], ["Al-Fatlawi", "Ahmad", ""]]}, {"id": "2009.10142", "submitter": "Alex Wong", "authors": "Alex Wong, Mukund Mundhra, Stefano Soatto", "title": "Stereopagnosia: Fooling Stereo Networks with Adversarial Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effect of adversarial perturbations of images on the estimates\nof disparity by deep learning models trained for stereo. We show that\nimperceptible additive perturbations can significantly alter the disparity map,\nand correspondingly the perceived geometry of the scene. These perturbations\nnot only affect the specific model they are crafted for, but transfer to models\nwith different architecture, trained with different loss functions. We show\nthat, when used for adversarial data augmentation, our perturbations result in\ntrained models that are more robust, without sacrificing overall accuracy of\nthe model. This is unlike what has been observed in image classification, where\nadding the perturbed images to the training set makes the model less vulnerable\nto adversarial perturbations, but to the detriment of overall accuracy. We test\nour method using the most recent stereo networks and evaluate their performance\non public benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 19:20:09 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 02:56:33 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 10:53:55 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Wong", "Alex", ""], ["Mundhra", "Mukund", ""], ["Soatto", "Stefano", ""]]}, {"id": "2009.10159", "submitter": "Du Nguyen", "authors": "Du Nguyen", "title": "Operator-valued formulas for Riemannian Gradient and Hessian and\n  families of tractable metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an explicit formula for the Levi-Civita connection and Riemannian\nHessian for a Riemannian manifold that is a quotient of a manifold embedded in\nan inner product space with a non-constant metric function. Together with a\nclassical formula for projection, this allows us to evaluate Riemannian\ngradient and Hessian for several families of metrics on classical manifolds,\nincluding a family of metrics on Stiefel manifolds connecting both the constant\nand canonical ambient metrics with closed-form geodesics. Using these formulas,\nwe derive Riemannian optimization frameworks on quotients of Stiefel manifolds,\nincluding flag manifolds, and a new family of complete quotient metrics on the\nmanifold of positive-semidefinite matrices of fixed rank, considered as a\nquotient of a product of Stiefel and positive-definite matrix manifold with\naffine-invariant metrics. The method is procedural, and in many instances, the\nRiemannian gradient and Hessian formulas could be derived by symbolic calculus.\nThe method extends the list of potential metrics that could be used in manifold\noptimization and machine learning.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 20:15:57 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 22:06:30 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Nguyen", "Du", ""]]}, {"id": "2009.10163", "submitter": "Anton Hinneck", "authors": "Arman Alahyari and Anton Hinneck and Rahim Tariverdi and David Pozo", "title": "Segmentation and Defect Classification of the Power Line Insulators: A\n  Deep Learning-based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power transmission networks physically connect the power generators to the\nelectric consumers. Such systems extend over hundreds of kilometers. There are\nmany components in the transmission infrastructure that require a proper\ninspection to guarantee flawless performance and reliable delivery, which, if\ndone manually, can be very costly and time consuming. One essential component\nis the insulator. Its failure can cause an interruption of the entire\ntransmission line or a widespread power failure. Automated fault detection\ncould significantly decrease inspection time and related costs. Recently,\nseveral works have been proposed based on convolutional neural networks, which\naddress the issue mentioned above. However, existing studies focus on a\nspecific type of insulator faults. Thus, in this study, we introduce a\ntwo-stage model that segments insulators from their background to then classify\ntheir states based on four different categories, namely: healthy, broken,\nburned/corroded and missing cap. The test results show that the proposed\napproach can realize the effective segmentation of insulators and achieve high\naccuracy in detecting several types of faults.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 20:25:51 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 13:47:27 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Alahyari", "Arman", ""], ["Hinneck", "Anton", ""], ["Tariverdi", "Rahim", ""], ["Pozo", "David", ""]]}, {"id": "2009.10181", "submitter": "Rayson Laroca", "authors": "Rayson Laroca, Alessandra B. Araujo, Luiz A. Zanlorensi, Eduardo C. de\n  Almeida, David Menotti", "title": "Towards Image-based Automatic Meter Reading in Unconstrained Scenarios:\n  A Robust and Efficient Approach", "comments": null, "journal-ref": "IEEE Access, vol. 9, pp. 67569-67584, 2021", "doi": "10.1109/ACCESS.2021.3077415", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches for image-based Automatic Meter Reading (AMR) have been\nevaluated on images captured in well-controlled scenarios. However, real-world\nmeter reading presents unconstrained scenarios that are way more challenging\ndue to dirt, various lighting conditions, scale variations, in-plane and\nout-of-plane rotations, among other factors. In this work, we present an\nend-to-end approach for AMR focusing on unconstrained scenarios. Our main\ncontribution is the insertion of a new stage in the AMR pipeline, called corner\ndetection and counter classification, which enables the counter region to be\nrectified -- as well as the rejection of illegible/faulty meters -- prior to\nthe recognition stage. We also introduce a publicly available dataset, called\nCopel-AMR, that contains 12,500 meter images acquired in the field by the\nservice company's employees themselves, including 2,500 images of faulty meters\nor cases where the reading is illegible due to occlusions. Experimental\nevaluation demonstrates that the proposed system, which has three networks\noperating in a cascaded mode, outperforms all baselines in terms of recognition\nrate while still being quite efficient. Moreover, as very few reading errors\nare tolerated in real-world applications, we show that our AMR system achieves\nimpressive recognition rates (i.e., > 99%) when rejecting readings made with\nlower confidence values.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 21:21:23 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 22:30:49 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 19:00:09 GMT"}, {"version": "v4", "created": "Fri, 19 Mar 2021 18:49:07 GMT"}, {"version": "v5", "created": "Wed, 12 May 2021 04:26:04 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Laroca", "Rayson", ""], ["Araujo", "Alessandra B.", ""], ["Zanlorensi", "Luiz A.", ""], ["de Almeida", "Eduardo C.", ""], ["Menotti", "David", ""]]}, {"id": "2009.10190", "submitter": "Faisal Mahmood", "authors": "Ming Y. Lu, Dehan Kong, Jana Lipkova, Richard J. Chen, Rajendra Singh,\n  Drew F. K. Williamson, Tiffany Y. Chen, Faisal Mahmood", "title": "Federated Learning for Computational Pathology on Gigapixel Whole Slide\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning-based computational pathology algorithms have demonstrated\nprofound ability to excel in a wide array of tasks that range from\ncharacterization of well known morphological phenotypes to predicting\nnon-human-identifiable features from histology such as molecular alterations.\nHowever, the development of robust, adaptable, and accurate deep learning-based\nmodels often rely on the collection and time-costly curation large high-quality\nannotated training data that should ideally come from diverse sources and\npatient populations to cater for the heterogeneity that exists in such\ndatasets. Multi-centric and collaborative integration of medical data across\nmultiple institutions can naturally help overcome this challenge and boost the\nmodel performance but is limited by privacy concerns amongst other difficulties\nthat may arise in the complex data sharing process as models scale towards\nusing hundreds of thousands of gigapixel whole slide images. In this paper, we\nintroduce privacy-preserving federated learning for gigapixel whole slide\nimages in computational pathology using weakly-supervised attention multiple\ninstance learning and differential privacy. We evaluated our approach on two\ndifferent diagnostic problems using thousands of histology whole slide images\nwith only slide-level labels. Additionally, we present a weakly-supervised\nlearning framework for survival prediction and patient stratification from\nwhole slide images and demonstrate its effectiveness in a federated setting.\nOur results show that using federated learning, we can effectively develop\naccurate weakly supervised deep learning models from distributed data silos\nwithout direct data sharing and its associated complexities, while also\npreserving differential privacy using randomized noise generation.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 21:56:08 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 00:11:40 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Lu", "Ming Y.", ""], ["Kong", "Dehan", ""], ["Lipkova", "Jana", ""], ["Chen", "Richard J.", ""], ["Singh", "Rajendra", ""], ["Williamson", "Drew F. K.", ""], ["Chen", "Tiffany Y.", ""], ["Mahmood", "Faisal", ""]]}, {"id": "2009.10221", "submitter": "Boris Kovalerchuk", "authors": "Boris Kovalerchuk (1), Muhammad Aurangzeb Ahmad (2 and 3), Ankur\n  Teredesai (2 and 3) ((1) Department of Computer Science, Central Washington\n  University, USA (2) Department of Computer Science and Systems, University of\n  Washington Tacoma, USA (3) Kensci Inc., USA)", "title": "Survey of explainable machine learning with visual and granular methods\n  beyond quasi-explanations", "comments": "45 pages, 34 figures", "journal-ref": "In: Interpretable Artificial Intelligence: A Perspective of\n  Granular Computing (Eds. W. Pedrycz, S.M.Chen), Springer, 2021, pp. 217-267", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper surveys visual methods of explainability of Machine Learning (ML)\nwith focus on moving from quasi-explanations that dominate in ML to\ndomain-specific explanation supported by granular visuals. ML interpretation is\nfundamentally a human activity and visual methods are more readily\ninterpretable. While efficient visual representations of high-dimensional data\nexist, the loss of interpretable information, occlusion, and clutter continue\nto be a challenge, which lead to quasi-explanations. We start with the\nmotivation and the different definitions of explainability. The paper focuses\non a clear distinction between quasi-explanations and domain specific\nexplanations, and between explainable and an actually explained ML model that\nare critically important for the explainability domain. We discuss foundations\nof interpretability, overview visual interpretability and present several types\nof methods to visualize the ML models. Next, we present methods of visual\ndiscovery of ML models, with the focus on interpretable models, based on the\nrecently introduced concept of General Line Coordinates (GLC). These methods\ntake the critical step of creating visual explanations that are not merely\nquasi-explanations but are also domain specific visual explanations while these\nmethods themselves are domain-agnostic. The paper includes results on\ntheoretical limits to preserve n-D distances in lower dimensions, based on the\nJohnson-Lindenstrauss lemma, point-to-point and point-to-graph GLC approaches,\nand real-world case studies. The paper also covers traditional visual methods\nfor understanding ML models, which include deep learning and time series\nmodels. We show that many of these methods are quasi-explanations and need\nfurther enhancement to become domain specific explanations. We conclude with\noutlining open problems and current research frontiers.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 23:39:06 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Kovalerchuk", "Boris", "", "2 and 3"], ["Ahmad", "Muhammad Aurangzeb", "", "2 and 3"], ["Teredesai", "Ankur", "", "2 and 3"]]}, {"id": "2009.10259", "submitter": "Weixin Liang", "authors": "Weixin Liang, James Zou, Zhou Yu", "title": "ALICE: Active Learning with Contrastive Natural Language Explanations", "comments": null, "journal-ref": "EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a supervised neural network classifier typically requires many\nannotated training samples. Collecting and annotating a large number of data\npoints are costly and sometimes even infeasible. Traditional annotation process\nuses a low-bandwidth human-machine communication interface: classification\nlabels, each of which only provides several bits of information. We propose\nActive Learning with Contrastive Explanations (ALICE), an expert-in-the-loop\ntraining framework that utilizes contrastive natural language explanations to\nimprove data efficiency in learning. ALICE learns to first use active learning\nto select the most informative pairs of label classes to elicit contrastive\nnatural language explanations from experts. Then it extracts knowledge from\nthese explanations using a semantic parser. Finally, it incorporates the\nextracted knowledge through dynamically changing the learning model's\nstructure. We applied ALICE in two visual recognition tasks, bird species\nclassification and social relationship classification. We found by\nincorporating contrastive explanations, our models outperform baseline models\nthat are trained with 40-100% more training data. We found that adding 1\nexplanation leads to similar performance gain as adding 13-30 labeled training\ndata points.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 01:02:07 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Liang", "Weixin", ""], ["Zou", "James", ""], ["Yu", "Zhou", ""]]}, {"id": "2009.10263", "submitter": "Juan Manuel Carrillo Garcia", "authors": "Juan Carrillo, Daniel Garijo, Mark Crowley, Rober Carrillo, Yolanda\n  Gil, Katherine Borda", "title": "Semantic Workflows and Machine Learning for the Assessment of Carbon\n  Storage by Urban Trees", "comments": "Previously published as part of the SciKnow 2019 Workshop, November\n  19th, 2019. Los Angeles, California, USA. Collocated with the tenth\n  International Conference on Knowledge Capture (K-CAP)", "journal-ref": "Proceedings of the Third International Workshop on Capturing\n  Scientific Knowledge co-located with the 10th International Conference on\n  Knowledge Capture (K-CAP 2019)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.CY eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Climate science is critical for understanding both the causes and\nconsequences of changes in global temperatures and has become imperative for\ndecisive policy-making. However, climate science studies commonly require\naddressing complex interoperability issues between data, software, and\nexperimental approaches from multiple fields. Scientific workflow systems\nprovide unparalleled advantages to address these issues, including\nreproducibility of experiments, provenance capture, software reusability and\nknowledge sharing. In this paper, we introduce a novel workflow with a series\nof connected components to perform spatial data preparation, classification of\nsatellite imagery with machine learning algorithms, and assessment of carbon\nstored by urban trees. To the best of our knowledge, this is the first study\nthat estimates carbon storage for a region in Africa following the guidelines\nfrom the Intergovernmental Panel on Climate Change (IPCC).\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 01:30:29 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Carrillo", "Juan", ""], ["Garijo", "Daniel", ""], ["Crowley", "Mark", ""], ["Carrillo", "Rober", ""], ["Gil", "Yolanda", ""], ["Borda", "Katherine", ""]]}, {"id": "2009.10282", "submitter": "Juan Manuel Carrillo Garcia", "authors": "Juan Carrillo, Mark Crowley, Guangyuan Pan, Liping Fu", "title": "Design of Efficient Deep Learning models for Determining Road Surface\n  Condition from Roadside Camera Images and Weather Data", "comments": "Source code for experiments is available at\n  https://github.com/jmcarrillog/deep-learning-for-road-surface-condition", "journal-ref": "Published also in proceedings of the TAC-ITS 2019 Conference", "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Road maintenance during the Winter season is a safety critical and resource\ndemanding operation. One of its key activities is determining road surface\ncondition (RSC) in order to prioritize roads and allocate cleaning efforts such\nas plowing or salting. Two conventional approaches for determining RSC are:\nvisual examination of roadside camera images by trained personnel and\npatrolling the roads to perform on-site inspections. However, with more than\n500 cameras collecting images across Ontario, visual examination becomes a\nresource-intensive activity, difficult to scale especially during periods of\nsnowstorms. This paper presents the results of a study focused on improving the\nefficiency of road maintenance operations. We use multiple Deep Learning models\nto automatically determine RSC from roadside camera images and weather\nvariables, extending previous research where similar methods have been used to\ndeal with the problem. The dataset we use was collected during the 2017-2018\nWinter season from 40 stations connected to the Ontario Road Weather\nInformation System (RWIS), it includes 14.000 labeled images and 70.000 weather\nmeasurements. We train and evaluate the performance of seven state-of-the-art\nmodels from the Computer Vision literature, including the recent DenseNet,\nNASNet, and MobileNet. Moreover, by following systematic ablation experiments\nwe adapt previously published Deep Learning models and reduce their number of\nparameters to about ~1.3% compared to their original parameter count, and by\nintegrating observations from weather variables the models are able to better\nascertain RSC under poor visibility conditions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 02:30:32 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Carrillo", "Juan", ""], ["Crowley", "Mark", ""], ["Pan", "Guangyuan", ""], ["Fu", "Liping", ""]]}, {"id": "2009.10292", "submitter": "Ty Nguyen", "authors": "Ty Nguyen, Ian D. Miller, Avi Cohen, Dinesh Thakur, Shashank Prasad,\n  Camillo J. Taylor, Pratik Chaudrahi, Vijay Kumar", "title": "PennSyn2Real: Training Object Recognition Models without Human Labeling", "comments": "7 pages, 9 figures, 3 tables. Submitted to R-AL and ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable training data generation is a critical problem in deep learning. We\npropose PennSyn2Real - a photo-realistic synthetic dataset consisting of more\nthan 100,000 4K images of more than 20 types of micro aerial vehicles (MAVs).\nThe dataset can be used to generate arbitrary numbers of training images for\nhigh-level computer vision tasks such as MAV detection and classification. Our\ndata generation framework bootstraps chroma-keying, a mature cinematography\ntechnique with a motion tracking system, providing artifact-free and curated\nannotated images where object orientations and lighting are controlled. This\nframework is easy to set up and can be applied to a broad range of objects,\nreducing the gap between synthetic and real-world data. We show that synthetic\ndata generated using this framework can be directly used to train CNN models\nfor common object recognition tasks such as detection and segmentation. We\ndemonstrate competitive performance in comparison with training using only real\nimages. Furthermore, bootstrapping the generated synthetic data in few-shot\nlearning can significantly improve the overall performance, reducing the number\nof required training data samples to achieve the desired accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 02:53:40 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 04:58:40 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Nguyen", "Ty", ""], ["Miller", "Ian D.", ""], ["Cohen", "Avi", ""], ["Thakur", "Dinesh", ""], ["Prasad", "Shashank", ""], ["Taylor", "Camillo J.", ""], ["Chaudrahi", "Pratik", ""], ["Kumar", "Vijay", ""]]}, {"id": "2009.10295", "submitter": "Cheng Yan", "authors": "Cheng Yan, Guansong Pang, Xiao Bai, Jun Zhou, Lin Gu", "title": "Beyond Triplet Loss: Person Re-identification with Fine-grained\n  Difference-aware Pairwise Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-IDentification (ReID) aims at re-identifying persons from different\nviewpoints across multiple cameras. Capturing the fine-grained appearance\ndifferences is often the key to accurate person ReID, because many identities\ncan be differentiated only when looking into these fine-grained differences.\nHowever, most state-of-the-art person ReID approaches, typically driven by a\ntriplet loss, fail to effectively learn the fine-grained features as they are\nfocused more on differentiating large appearance differences. To address this\nissue, we introduce a novel pairwise loss function that enables ReID models to\nlearn the fine-grained features by adaptively enforcing an exponential\npenalization on the images of small differences and a bounded penalization on\nthe images of large differences. The proposed loss is generic and can be used\nas a plugin to replace the triplet loss to significantly enhance different\ntypes of state-of-the-art approaches. Experimental results on four benchmark\ndatasets show that the proposed loss substantially outperforms a number of\npopular loss functions by large margins; and it also enables significantly\nimproved data efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 03:04:12 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Yan", "Cheng", ""], ["Pang", "Guansong", ""], ["Bai", "Xiao", ""], ["Zhou", "Jun", ""], ["Gu", "Lin", ""]]}, {"id": "2009.10301", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley", "title": "Stochastic Neighbor Embedding with Gaussian and Student-t Distributions:\n  Tutorial and Survey", "comments": "To appear as a part of an upcoming academic book on dimensionality\n  reduction and manifold learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Neighbor Embedding (SNE) is a manifold learning and dimensionality\nreduction method with a probabilistic approach. In SNE, every point is consider\nto be the neighbor of all other points with some probability and this\nprobability is tried to be preserved in the embedding space. SNE considers\nGaussian distribution for the probability in both the input and embedding\nspaces. However, t-SNE uses the Student-t and Gaussian distributions in these\nspaces, respectively. In this tutorial and survey paper, we explain SNE,\nsymmetric SNE, t-SNE (or Cauchy-SNE), and t-SNE with general degrees of\nfreedom. We also cover the out-of-sample extension and acceleration for these\nmethods. Some simulations to visualize the embeddings are also provided.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 03:32:05 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Ghodsi", "Ali", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "2009.10325", "submitter": "Xiaosong Wang", "authors": "Xiaosong Wang, Ziyue Xu, Dong Yang, Leo Tam, Holger Roth, Daguang Xu", "title": "Learning Image Labels On-the-fly for Training Robust Classification\n  Models", "comments": "v2: Minor Corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning paradigms largely benefit from the tremendous amount of\nannotated data. However, the quality of the annotations often varies among\nlabelers. Multi-observer studies have been conducted to study these annotation\nvariances (by labeling the same data for multiple times) and its effects on\ncritical applications like medical image analysis. This process indeed adds an\nextra burden to the already tedious annotation work that usually requires\nprofessional training and expertise in the specific domains. On the other hand,\nautomated annotation methods based on NLP algorithms have recently shown\npromise as a reasonable alternative, relying on the existing diagnostic reports\nof those images that are widely available in the clinical system. Compared to\nhuman labelers, different algorithms provide labels with varying qualities that\nare even noisier. In this paper, we show how noisy annotations (e.g., from\ndifferent algorithm-based labelers) can be utilized together and mutually\nbenefit the learning of classification tasks. Specifically, the concept of\nattention-on-label is introduced to sample better label sets on-the-fly as the\ntraining data. A meta-training based label-sampling module is designed to\nattend the labels that benefit the model learning the most through additional\nback-propagation processes. We apply the attention-on-label scheme on the\nclassification task of a synthetic noisy CIFAR-10 dataset to prove the concept,\nand then demonstrate superior results (3-5% increase on average in multiple\ndisease classification AUCs) on the chest x-ray images from a hospital-scale\ndataset (MIMIC-CXR) and hand-labeled dataset (OpenI) in comparison to regular\ntraining paradigms.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 05:38:44 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 04:35:55 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Wang", "Xiaosong", ""], ["Xu", "Ziyue", ""], ["Yang", "Dong", ""], ["Tam", "Leo", ""], ["Roth", "Holger", ""], ["Xu", "Daguang", ""]]}, {"id": "2009.10338", "submitter": "Weitao Feng", "authors": "Weitao Feng, Zhihao Hu, Baopu Li, Weihao Gan, Wei Wu, Wanli Ouyang", "title": "SAMOT: Switcher-Aware Multi-Object Tracking and Still Another MOT\n  Measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Object Tracking (MOT) is a popular topic in computer vision. However,\nidentity issue, i.e., an object is wrongly associated with another object of a\ndifferent identity, still remains to be a challenging problem. To address it,\nswitchers, i.e., confusing targets thatmay cause identity issues, should be\nfocused. Based on this motivation,this paper proposes a novel switcher-aware\nframework for multi-object tracking, which consists of Spatial Conflict Graph\nmodel (SCG) and Switcher-Aware Association (SAA). The SCG eliminates spatial\nswitch-ers within one frame by building a conflict graph and working out the\noptimal subgraph. The SAA utilizes additional information from potential\ntemporal switcher across frames, enabling more accurate data association.\nBesides, we propose a new MOT evaluation measure, Still Another IDF score\n(SAIDF), aiming to focus more on identity issues.This new measure may overcome\nsome problems of the previous measures and provide a better insight for\nidentity issues in MOT. Finally,the proposed framework is tested under both the\ntraditional measures and the new measure we proposed. Extensive experiments\nshow that ourmethod achieves competitive results on all measure.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 06:22:21 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Feng", "Weitao", ""], ["Hu", "Zhihao", ""], ["Li", "Baopu", ""], ["Gan", "Weihao", ""], ["Wu", "Wei", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2009.10361", "submitter": "Wolfgang Paier", "authors": "Wolfgang Paier and Anna Hilsmann and Peter Eisert", "title": "Neural Face Models for Example-Based Visual Speech Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Creating realistic animations of human faces with computer graphic models is\nstill a challenging task. It is often solved either with tedious manual work or\nmotion capture based techniques that require specialised and costly hardware.\nExample based animation approaches circumvent these problems by re-using\ncaptured data of real people. This data is split into short motion samples that\ncan be looped or concatenated in order to create novel motion sequences. The\nobvious advantages of this approach are the simplicity of use and the high\nrealism, since the data exhibits only real deformations. Rather than tuning\nweights of a complex face rig, the animation task is performed on a higher\nlevel by arranging typical motion samples in a way such that the desired facial\nperformance is achieved. Two difficulties with example based approaches,\nhowever, are high memory requirements as well as the creation of artefact-free\nand realistic transitions between motion samples. We solve these problems by\ncombining the realism and simplicity of example-based animations with the\nadvantages of neural face models. Our neural face model is capable of\nsynthesising high quality 3D face geometry and texture according to a compact\nlatent parameter vector. This latent representation reduces memory requirements\nby a factor of 100 and helps creating seamless transitions between concatenated\nmotion samples. In this paper, we present a marker-less approach for facial\nmotion capture based on multi-view video. Based on the captured data, we learn\na neural representation of facial expressions, which is used to seamlessly\nconcatenate facial performances during the animation procedure. We demonstrate\nthe effectiveness of our approach by synthesising mouthings for Swiss-German\nsign language based on viseme query sequences.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 07:35:33 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Paier", "Wolfgang", ""], ["Hilsmann", "Anna", ""], ["Eisert", "Peter", ""]]}, {"id": "2009.10370", "submitter": "Bassem Seddik", "authors": "Bassem Seddik and Najoua Essoukri Ben Amara", "title": "Visual Methods for Sign Language Recognition: A Modality-Based Review", "comments": "This survey paper is accepted as Springer book chapter, currently\n  under edition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign language visual recognition from continuous multi-modal streams is still\none of the most challenging fields.\n  Recent advances in human actions recognition are exploiting the ascension of\nGPU-based learning from massive data, and are getting closer to human-like\nperformances.\n  They are then prone to creating interactive services for the deaf and\nhearing-impaired communities.\n  A population that is expected to grow considerably in the years to come.\n  This paper aims at reviewing the human actions recognition literature with\nthe sign-language visual understanding as a scope.\n  The methods analyzed will be mainly organized according to the different\ntypes of unimodal inputs exploited, their relative multi-modal combinations and\npipeline steps.\n  In each section, we will detail and compare the related datasets, approaches\nthen distinguish the still open contribution paths suitable for the creation of\nsign language related services.\n  Special attention will be paid to the approaches and commercial solutions\nhandling facial expressions and continuous signing.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 07:56:02 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Seddik", "Bassem", ""], ["Amara", "Najoua Essoukri Ben", ""]]}, {"id": "2009.10390", "submitter": "Jingwen He", "authors": "Jingwen He, Yihao Liu, Yu Qiao, and Chao Dong", "title": "Conditional Sequential Modulation for Efficient Global Image Retouching", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo retouching aims at enhancing the aesthetic visual quality of images\nthat suffer from photographic defects such as over/under exposure, poor\ncontrast, inharmonious saturation. Practically, photo retouching can be\naccomplished by a series of image processing operations. In this paper, we\ninvestigate some commonly-used retouching operations and mathematically find\nthat these pixel-independent operations can be approximated or formulated by\nmulti-layer perceptrons (MLPs). Based on this analysis, we propose an extremely\nlight-weight framework - Conditional Sequential Retouching Network (CSRNet) -\nfor efficient global image retouching. CSRNet consists of a base network and a\ncondition network. The base network acts like an MLP that processes each pixel\nindependently and the condition network extracts the global features of the\ninput image to generate a condition vector. To realize retouching operations,\nwe modulate the intermediate features using Global Feature Modulation (GFM), of\nwhich the parameters are transformed by condition vector. Benefiting from the\nutilization of $1\\times1$ convolution, CSRNet only contains less than 37k\ntrainable parameters, which is orders of magnitude smaller than existing\nlearning-based methods. Extensive experiments show that our method achieves\nstate-of-the-art performance on the benchmark MIT-Adobe FiveK dataset\nquantitively and qualitatively. Code is available at\nhttps://github.com/hejingwenhejingwen/CSRNet.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 08:32:04 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["He", "Jingwen", ""], ["Liu", "Yihao", ""], ["Qiao", "Yu", ""], ["Dong", "Chao", ""]]}, {"id": "2009.10434", "submitter": "Haoyu Tang", "authors": "Haoyu Tang, Jihua Zhu, Meng Liu, Member, IEEE, Zan Gao, and Zhiyong\n  Cheng", "title": "Frame-wise Cross-modal Matching for Video Moment Retrieval", "comments": "12 pages; accepted by IEEE TMM", "journal-ref": "IEEE Transactions on Multimedia 2021", "doi": "10.1109/TMM.2021.3063631", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video moment retrieval targets at retrieving a moment in a video for a given\nlanguage query. The challenges of this task include 1) the requirement of\nlocalizing the relevant moment in an untrimmed video, and 2) bridging the\nsemantic gap between textual query and video contents. To tackle those\nproblems, early approaches adopt the sliding window or uniform sampling to\ncollect video clips first and then match each clip with the query. Obviously,\nthese strategies are time-consuming and often lead to unsatisfied accuracy in\nlocalization due to the unpredictable length of the golden moment. To avoid the\nlimitations, researchers recently attempt to directly predict the relevant\nmoment boundaries without the requirement to generate video clips first. One\nmainstream approach is to generate a multimodal feature vector for the target\nquery and video frames (e.g., concatenation) and then use a regression approach\nupon the multimodal feature vector for boundary detection. Although some\nprogress has been achieved by this approach, we argue that those methods have\nnot well captured the cross-modal interactions between the query and video\nframes.\n  In this paper, we propose an Attentive Cross-modal Relevance Matching (ACRM)\nmodel which predicts the temporal boundaries based on an interaction modeling.\nIn addition, an attention module is introduced to assign higher weights to\nquery words with richer semantic cues, which are considered to be more\nimportant for finding relevant video contents. Another contribution is that we\npropose an additional predictor to utilize the internal frames in the model\ntraining to improve the localization accuracy. Extensive experiments on two\ndatasets TACoS and Charades-STA demonstrate the superiority of our method over\nseveral state-of-the-art methods. Ablation studies have been also conducted to\nexamine the effectiveness of different modules in our ACRM model.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 10:25:41 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 07:32:20 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Tang", "Haoyu", ""], ["Zhu", "Jihua", ""], ["Liu", "Meng", ""], ["Member", "", ""], ["IEEE", "", ""], ["Gao", "Zan", ""], ["Cheng", "Zhiyong", ""]]}, {"id": "2009.10456", "submitter": "Dat Thanh Tran", "authors": "Dat Thanh Tran, Moncef Gabbouj, Alexandros Iosifidis", "title": "Performance Indicator in Multilinear Compressive Learning", "comments": "accepted in 2020 IEEE Symposium Series on Computational Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the Multilinear Compressive Learning (MCL) framework was proposed\nto efficiently optimize the sensing and learning steps when working with\nmultidimensional signals, i.e. tensors. In Compressive Learning in general, and\nin MCL in particular, the number of compressed measurements captured by a\ncompressive sensing device characterizes the storage requirement or the\nbandwidth requirement for transmission. This number, however, does not\ncompletely characterize the learning performance of a MCL system. In this\npaper, we analyze the relationship between the input signal resolution, the\nnumber of compressed measurements and the learning performance of MCL. Our\nempirical analysis shows that the reconstruction error obtained at the\ninitialization step of MCL strongly correlates with the learning performance,\nthus can act as a good indicator to efficiently characterize learning\nperformances obtained from different sensor configurations without optimizing\nthe entire system.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 11:27:50 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Tran", "Dat Thanh", ""], ["Gabbouj", "Moncef", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "2009.10465", "submitter": "Hao Zhang", "authors": "Hao Zhang, Joey Tianyi Zhou, Tianying Wang, Ivor W. Tsang, Rick Siow\n  Mong Goh", "title": "Deep N-ary Error Correcting Output Codes", "comments": "EAI MOBIMEDIA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble learning consistently improves the performance of multi-class\nclassification through aggregating a series of base classifiers. To this end,\ndata-independent ensemble methods like Error Correcting Output Codes (ECOC)\nattract increasing attention due to its easiness of implementation and\nparallelization. Specifically, traditional ECOCs and its general extension\nN-ary ECOC decompose the original multi-class classification problem into a\nseries of independent simpler classification subproblems. Unfortunately,\nintegrating ECOCs, especially N-ary ECOC with deep neural networks, termed as\ndeep N-ary ECOC, is not straightforward and yet fully exploited in the\nliterature, due to the high expense of training base learners. To facilitate\nthe training of N-ary ECOC with deep learning base learners, we further propose\nthree different variants of parameter sharing architectures for deep N-ary\nECOC. To verify the generalization ability of deep N-ary ECOC, we conduct\nexperiments by varying the backbone with different deep neural network\narchitectures for both image and text classification tasks. Furthermore,\nextensive ablation studies on deep N-ary ECOC show its superior performance\nover other deep data-independent ensemble methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 11:35:03 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 03:20:23 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 09:24:31 GMT"}, {"version": "v4", "created": "Tue, 15 Dec 2020 02:33:15 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Zhang", "Hao", ""], ["Zhou", "Joey Tianyi", ""], ["Wang", "Tianying", ""], ["Tsang", "Ivor W.", ""], ["Goh", "Rick Siow Mong", ""]]}, {"id": "2009.10467", "submitter": "Ivan Tishchenko", "authors": "Ivan Tishchenko, Sandro Lombardi, Martin R. Oswald, Marc Pollefeys", "title": "Self-Supervised Learning of Non-Rigid Residual Flow and Ego-Motion", "comments": "Accepted to 3DV 2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the current scene flow methods choose to model scene flow as a per\npoint translation vector without differentiating between static and dynamic\ncomponents of 3D motion. In this work we present an alternative method for\nend-to-end scene flow learning by joint estimation of non-rigid residual flow\nand ego-motion flow for dynamic 3D scenes. We propose to learn the relative\nrigid transformation from a pair of point clouds followed by an iterative\nrefinement. We then learn the non-rigid flow from transformed inputs with the\ndeducted rigid part of the flow. Furthermore, we extend the supervised\nframework with self-supervisory signals based on the temporal consistency\nproperty of a point cloud sequence. Our solution allows both training in a\nsupervised mode complemented by self-supervisory loss terms as well as training\nin a fully self-supervised mode. We demonstrate that decomposition of scene\nflow into non-rigid flow and ego-motion flow along with an introduction of the\nself-supervisory signals allowed us to outperform the current state-of-the-art\nsupervised methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 11:39:19 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 15:21:21 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Tishchenko", "Ivan", ""], ["Lombardi", "Sandro", ""], ["Oswald", "Martin R.", ""], ["Pollefeys", "Marc", ""]]}, {"id": "2009.10468", "submitter": "Xiong Dan", "authors": "Xiong Dan", "title": "Spatial-Temporal Block and LSTM Network for Pedestrian Trajectories\n  Prediction", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian trajectory prediction is a critical to avoid autonomous driving\ncollision. But this prediction is a challenging problem due to social forces\nand cluttered scenes. Such human-human and human-space interactions lead to\nmany socially plausible trajectories. In this paper, we propose a novel\nLSTM-based algorithm. We tackle the problem by considering the static scene and\npedestrian which combine the Graph Convolutional Networks and Temporal\nConvolutional Networks to extract features from pedestrians. Each pedestrian in\nthe scene is regarded as a node, and we can obtain the relationship between\neach node and its neighborhoods by graph embedding. It is LSTM that encode the\nrelationship so that our model predicts nodes trajectories in crowd scenarios\nsimultaneously. To effectively predict multiple possible future trajectories,\nwe further introduce Spatio-Temporal Convolutional Block to make the network\nflexible. Experimental results on two public datasets, i.e. ETH and UCY,\ndemonstrate the effectiveness of our proposed ST-Block and we achieve\nstate-of-the-art approaches in human trajectory prediction.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 11:43:40 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 07:51:39 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Dan", "Xiong", ""]]}, {"id": "2009.10474", "submitter": "Alejandro Martinez", "authors": "Alejandro R. Martinez", "title": "Classification of COVID-19 in CT Scans using Multi-Source Transfer\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since December of 2019, novel coronavirus disease COVID-19 has spread around\nthe world infecting millions of people and upending the global economy. One of\nthe driving reasons behind its high rate of infection is due to the\nunreliability and lack of RT-PCR testing. At times the turnaround results span\nas long as a couple of days, only to yield a roughly 70% sensitivity rate. As\nan alternative, recent research has investigated the use of Computer Vision\nwith Convolutional Neural Networks (CNNs) for the classification of COVID-19\nfrom CT scans. Due to an inherent lack of available COVID-19 CT data, these\nresearch efforts have been forced to leverage the use of Transfer Learning.\nThis commonly employed Deep Learning technique has shown to improve model\nperformance on tasks with relatively small amounts of data, as long as the\nSource feature space somewhat resembles the Target feature space.\nUnfortunately, a lack of similarity is often encountered in the classification\nof medical images as publicly available Source datasets usually lack the visual\nfeatures found in medical images. In this study, we propose the use of\nMulti-Source Transfer Learning (MSTL) to improve upon traditional Transfer\nLearning for the classification of COVID-19 from CT scans. With our\nmulti-source fine-tuning approach, our models outperformed baseline models\nfine-tuned with ImageNet. We additionally, propose an unsupervised label\ncreation process, which enhances the performance of our Deep Residual Networks.\nOur best performing model was able to achieve an accuracy of 0.893 and a Recall\nscore of 0.897, outperforming its baseline Recall score by 9.3%.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 11:53:06 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Martinez", "Alejandro R.", ""]]}, {"id": "2009.10492", "submitter": "Alexander Kern", "authors": "Alexander Kern, Markus Bobbe, Yogesh Khedar and Ulf Bestmann", "title": "OpenREALM: Real-time Mapping for Unmanned Aerial Vehicles", "comments": "Full source code on https://github.com/laxnpander/OpenREALM 2020\n  International Conference on Unmanned Aircraft Systems (ICUAS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents OpenREALM, a real-time mapping framework for Unmanned\nAerial Vehicles (UAVs). A camera attached to the onboard computer of a moving\nUAV is utilized to acquire high resolution image mosaics of a targeted area of\ninterest. Different modes of operation allow OpenREALM to perform simple\nstitching assuming an approximate plane ground, or to fully recover complex 3D\nsurface information to extract both elevation maps and geometrically corrected\northophotos. Additionally, the global position of the UAV is used to\ngeoreference the data. In all modes incremental progress of the resulting map\ncan be viewed live by an operator on the ground. Obtained, up-to-date surface\ninformation will be a push forward to a variety of UAV applications. For the\nbenefit of the community, source code is public at\nhttps://github.com/laxnpander/OpenREALM.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 12:28:14 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Kern", "Alexander", ""], ["Bobbe", "Markus", ""], ["Khedar", "Yogesh", ""], ["Bestmann", "Ulf", ""]]}, {"id": "2009.10521", "submitter": "Edgar Riba Pi", "authors": "E. Riba, D. Mishkin, J. Shi, D. Ponsa, F. Moreno-Noguer and G. Bradski", "title": "A survey on Kornia: an Open Source Differentiable Computer Vision\n  Library for PyTorch", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.02190", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents Kornia, an open source computer vision library built upon\na set of differentiable routines and modules that aims to solve generic\ncomputer vision problems. The package uses PyTorch as its main backend, not\nonly for efficiency but also to take advantage of the reverse\nauto-differentiation engine to define and compute the gradient of complex\nfunctions. Inspired by OpenCV, Kornia is composed of a set of modules\ncontaining operators that can be integrated into neural networks to train\nmodels to perform a wide range of operations including image\ntransformations,camera calibration, epipolar geometry, and low level image\nprocessing techniques, such as filtering and edge detection that operate\ndirectly on high dimensional tensor representations on graphical processing\nunits, generating faster systems. Examples of classical vision problems\nimplemented using our framework are provided including a benchmark comparing to\nexisting vision libraries.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 08:48:28 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Riba", "E.", ""], ["Mishkin", "D.", ""], ["Shi", "J.", ""], ["Ponsa", "D.", ""], ["Moreno-Noguer", "F.", ""], ["Bradski", "G.", ""]]}, {"id": "2009.10537", "submitter": "Yaguan Qian", "authors": "Yaguan Qian, Qiqi Shao, Jiamin Wang, Xiang Lin, Yankai Guo, Zhaoquan\n  Gu, Bin Wang, Chunming Wu", "title": "EI-MTD:Moving Target Defense for Edge Intelligence against Adversarial\n  Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the boom of edge intelligence, its vulnerability to adversarial attacks\nbecomes an urgent problem. The so-called adversarial example can fool a deep\nlearning model on the edge node to misclassify. Due to the property of\ntransferability, the adversary can easily make a black-box attack using a local\nsubstitute model. Nevertheless, the limitation of resource of edge nodes cannot\nafford a complicated defense mechanism as doing on the cloud data center. To\novercome the challenge, we propose a dynamic defense mechanism, namely EI-MTD.\nIt first obtains robust member models with small size through differential\nknowledge distillation from a complicated teacher model on the cloud data\ncenter. Then, a dynamic scheduling policy based on a Bayesian Stackelberg game\nis applied to the choice of a target model for service. This dynamic defense\ncan prohibit the adversary from selecting an optimal substitute model for\nblack-box attacks. Our experimental result shows that this dynamic scheduling\ncan effectively protect edge intelligence against adversarial attacks under the\nblack-box setting.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 09:04:18 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 02:44:15 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 01:13:39 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Qian", "Yaguan", ""], ["Shao", "Qiqi", ""], ["Wang", "Jiamin", ""], ["Lin", "Xiang", ""], ["Guo", "Yankai", ""], ["Gu", "Zhaoquan", ""], ["Wang", "Bin", ""], ["Wu", "Chunming", ""]]}, {"id": "2009.10549", "submitter": "Ran Gu", "authors": "Ran Gu, Guotai Wang, Tao Song, Rui Huang, Michael Aertsen, Jan\n  Deprest, S\\'ebastien Ourselin, Tom Vercauteren, Shaoting Zhang", "title": "CA-Net: Comprehensive Attention Convolutional Neural Networks for\n  Explainable Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2020.3035253", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate medical image segmentation is essential for diagnosis and treatment\nplanning of diseases. Convolutional Neural Networks (CNNs) have achieved\nstate-of-the-art performance for automatic medical image segmentation. However,\nthey are still challenged by complicated conditions where the segmentation\ntarget has large variations of position, shape and scale, and existing CNNs\nhave a poor explainability that limits their application to clinical decisions.\nIn this work, we make extensive use of multiple attentions in a CNN\narchitecture and propose a comprehensive attention-based CNN (CA-Net) for more\naccurate and explainable medical image segmentation that is aware of the most\nimportant spatial positions, channels and scales at the same time. In\nparticular, we first propose a joint spatial attention module to make the\nnetwork focus more on the foreground region. Then, a novel channel attention\nmodule is proposed to adaptively recalibrate channel-wise feature responses and\nhighlight the most relevant feature channels. Also, we propose a scale\nattention module implicitly emphasizing the most salient feature maps among\nmultiple scales so that the CNN is adaptive to the size of an object. Extensive\nexperiments on skin lesion segmentation from ISIC 2018 and multi-class\nsegmentation of fetal MRI found that our proposed CA-Net significantly improved\nthe average segmentation Dice score from 87.77% to 92.08% for skin lesion,\n84.79% to 87.08% for the placenta and 93.20% to 95.88% for the fetal brain\nrespectively compared with U-Net. It reduced the model size to around 15 times\nsmaller with close or even better accuracy compared with state-of-the-art\nDeepLabv3+. In addition, it has a much higher explainability than existing\nnetworks by visualizing the attention weight maps. Our code is available at\nhttps://github.com/HiLab-git/CA-Net\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 13:41:06 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 01:03:45 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Gu", "Ran", ""], ["Wang", "Guotai", ""], ["Song", "Tao", ""], ["Huang", "Rui", ""], ["Aertsen", "Michael", ""], ["Deprest", "Jan", ""], ["Ourselin", "S\u00e9bastien", ""], ["Vercauteren", "Tom", ""], ["Zhang", "Shaoting", ""]]}, {"id": "2009.10569", "submitter": "Ozan Unal", "authors": "Ozan Unal, Luc Van Gool, Dengxin Dai", "title": "Improving Point Cloud Semantic Segmentation by Learning 3D Object\n  Detection", "comments": "Accepted at IEEE Winter Conference on Applications of Computer Vision\n  2021 (WACV'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud semantic segmentation plays an essential role in autonomous\ndriving, providing vital information about drivable surfaces and nearby objects\nthat can aid higher level tasks such as path planning and collision avoidance.\nWhile current 3D semantic segmentation networks focus on convolutional\narchitectures that perform great for well represented classes, they show a\nsignificant drop in performance for underrepresented classes that share similar\ngeometric features. We propose a novel Detection Aware 3D Semantic Segmentation\n(DASS) framework that explicitly leverages localization features from an\nauxiliary 3D object detection task. By utilizing multitask training, the shared\nfeature representation of the network is guided to be aware of per class\ndetection features that aid tackling the differentiation of geometrically\nsimilar classes. We additionally provide a pipeline that uses DASS to generate\nhigh recall proposals for existing 2-stage detectors and demonstrate that the\nadded supervisory signal can be used to improve 3D orientation estimation\ncapabilities. Extensive experiments on both the SemanticKITTI and KITTI object\ndatasets show that DASS can improve 3D semantic segmentation results of\ngeometrically similar classes up to 37.8% IoU in image FOV while maintaining\nhigh precision bird's-eye view (BEV) detection results.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 14:17:40 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 08:18:00 GMT"}, {"version": "v3", "created": "Sat, 7 Nov 2020 15:58:19 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Unal", "Ozan", ""], ["Van Gool", "Luc", ""], ["Dai", "Dengxin", ""]]}, {"id": "2009.10580", "submitter": "Nannan Li", "authors": "Nannan Li, Yu Pan, Yaran Chen, Zixiang Ding, Dongbin Zhao, Zenglin Xu", "title": "Heuristic Rank Selection with Progressively Searching Tensor Ring\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Tensor Ring Networks (TRNs) have been applied in deep networks,\nachieving remarkable successes in compression ratio and accuracy. Although\nhighly related to the performance of TRNs, rank selection is seldom studied in\nprevious works and usually set to equal in experiments. Meanwhile, there is not\nany heuristic method to choose the rank, and an enumerating way to find\nappropriate rank is extremely time-consuming. Interestingly, we discover that\npart of the rank elements is sensitive and usually aggregate in a narrow\nregion, namely an interest region. Therefore, based on the above phenomenon, we\npropose a novel progressive genetic algorithm named Progressively Searching\nTensor Ring Network Search (PSTRN), which has the ability to find optimal rank\nprecisely and efficiently. Through the evolutionary phase and progressive\nphase, PSTRN can converge to the interest region quickly and harvest good\nperformance. Experimental results show that PSTRN can significantly reduce the\ncomplexity of seeking rank, compared with the enumerating method. Furthermore,\nour method is validated on public benchmarks like MNIST, CIFAR10/100, UCF11 and\nHMDB51, achieving the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 14:44:27 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 08:44:25 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Nannan", ""], ["Pan", "Yu", ""], ["Chen", "Yaran", ""], ["Ding", "Zixiang", ""], ["Zhao", "Dongbin", ""], ["Xu", "Zenglin", ""]]}, {"id": "2009.10589", "submitter": "Catherine Ordun", "authors": "Catherine Ordun, Edward Raff, Sanjay Purushotham", "title": "The Use of AI for Thermal Emotion Recognition: A Review of Problems and\n  Limitations in Standard Design and Data", "comments": "Presented at AAAI FSS-20: Artificial Intelligence in Government and\n  Public Sector, Washington, DC, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased attention on thermal imagery for Covid-19 screening, the\npublic sector may believe there are new opportunities to exploit thermal as a\nmodality for computer vision and AI. Thermal physiology research has been\nongoing since the late nineties. This research lies at the intersections of\nmedicine, psychology, machine learning, optics, and affective computing. We\nwill review the known factors of thermal vs. RGB imaging for facial emotion\nrecognition. But we also propose that thermal imagery may provide a\nsemi-anonymous modality for computer vision, over RGB, which has been plagued\nby misuse in facial recognition. However, the transition to adopting thermal\nimagery as a source for any human-centered AI task is not easy and relies on\nthe availability of high fidelity data sources across multiple demographics and\nthorough validation. This paper takes the reader on a short review of machine\nlearning in thermal FER and the limitations of collecting and developing\nthermal FER data for AI training. Our motivation is to provide an introductory\noverview into recent advances for thermal FER and stimulate conversation about\nthe limitations in current datasets.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 14:58:59 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Ordun", "Catherine", ""], ["Raff", "Edward", ""], ["Purushotham", "Sanjay", ""]]}, {"id": "2009.10608", "submitter": "Lipei Zhang", "authors": "Lipei Zhang, Aozhi Liu, Jing Xiao, Paul Taylor", "title": "Dual Encoder Fusion U-Net (DEFU-Net) for Cross-manufacturer Chest X-ray\n  Segmentation", "comments": "6 pages, 6 figures, 3 tables, accepted by ICPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of methods based on deep learning have been applied to medical image\nsegmentation and have achieved state-of-the-art performance. Due to the\nimportance of chest x-ray data in studying COVID-19, there is a demand for\nstate-of-the-art models capable of precisely segmenting soft tissue on the\nchest x-rays. The dataset for exploring best segmentation model is from\nMontgomery and Shenzhen hospital which had opened in 2014. The most famous\ntechnique is U-Net which has been used to many medical datasets including the\nChest X-rays. However, most variant U-Nets mainly focus on extraction of\ncontextual information and skip connections. There is still a large space for\nimproving extraction of spatial features. In this paper, we propose a dual\nencoder fusion U-Net framework for Chest X-rays based on Inception\nConvolutional Neural Network with dilation, Densely Connected Recurrent\nConvolutional Neural Network, which is named DEFU-Net. The densely connected\nrecurrent path extends the network deeper for facilitating contextual feature\nextraction. In order to increase the width of network and enrich representation\nof features, the inception blocks with dilation are adopted. The inception\nblocks can capture globally and locally spatial information from various\nreceptive fields. At the same time, the two paths are fused by summing\nfeatures, thus preserving the contextual and spatial information for decoding\npart. This multi-learning-scale model is benefiting in Chest X-ray dataset from\ntwo different manufacturers (Montgomery and Shenzhen hospital). The DEFU-Net\nachieves the better performance than basic U-Net, residual U-Net, BCDU-Net,\nR2U-Net and attention R2U-Net. This model has proved the feasibility for mixed\ndataset and approaches state-of-the-art. The source code for this proposed\nframework is public https://github.com/uceclz0/DEFU-Net.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 15:57:44 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 08:28:58 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 07:34:17 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zhang", "Lipei", ""], ["Liu", "Aozhi", ""], ["Xiao", "Jing", ""], ["Taylor", "Paul", ""]]}, {"id": "2009.10612", "submitter": "Sayantari Ghosh", "authors": "Babloo Kumar and Sayantari Ghosh", "title": "Detection Of Concrete Cracks using Dual-channel Deep Convolutional\n  Network", "comments": "7 pages, 7 figures, Accepted and presented in IEEE-ICCCNT 2020\n  (https://11icccnt.com/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to cyclic loading and fatigue stress cracks are generated, which affect\nthe safety of any civil infrastructure. Nowadays machine vision is being used\nto assist us for appropriate maintenance, monitoring and inspection of concrete\nstructures by partial replacement of human-conducted onsite inspections. The\ncurrent study proposes a crack detection method based on deep convolutional\nneural network (CNN) for detection of concrete cracks without explicitly\ncalculating the defect features. In the course of the study, a database of 3200\nlabelled images with concrete cracks has been created, where the contrast,\nlighting conditions, orientations and severity of the cracks were extremely\nvariable. In this paper, starting from a deep CNN trained with these images of\n256 x 256 pixel-resolution, we have gradually optimized the model by\nidentifying the difficulties. Using an augmented dataset, which takes into\naccount the variations and degradations compatible to drone videos, like,\nrandom zooming, rotation and intensity scaling and exhaustive ablation studies,\nwe have designed a dual-channel deep CNN which shows high accuracy (~ 92.25%)\nas well as robustness in finding concrete cracks in realis-tic situations. The\nmodel has been tested on the basis of performance and analyzed with the help of\nfeature maps, which establishes the importance of the dual-channel structure.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 15:17:02 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Kumar", "Babloo", ""], ["Ghosh", "Sayantari", ""]]}, {"id": "2009.10623", "submitter": "Ferran Alet", "authors": "Ferran Alet, Maria Bauza, Kenji Kawaguchi, Nurullah Giray Kuru, Tomas\n  Lozano-Perez, Leslie Pack Kaelbling", "title": "Tailoring: encoding inductive biases by optimizing unsupervised\n  objectives at prediction time", "comments": "NeurIPS 2020 workshops on Interpretable Inductive Biases and\n  Meta-learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From CNNs to attention mechanisms, encoding inductive biases into neural\nnetworks has been a fruitful source of improvement in machine learning. Adding\nauxiliary losses to the main objective function is a general way of encoding\nbiases that can help networks learn better representations. However, since\nauxiliary losses are minimized only on training data, they suffer from the same\ngeneralization gap as regular task losses. Moreover, by adding a term to the\nloss function, the model optimizes a different objective than the one we care\nabout. In this work we address both problems: first, we take inspiration from\ntransductive learning and note that, after receiving an input but before making\na prediction, we can fine-tune our networks on any unsupervised loss. We call\nthis process tailoring, because we customize the model to each input to ensure\nour prediction satisfies the inductive bias. Second, we formulate\nmeta-tailoring, a nested optimization similar to that in meta-learning, and\ntrain our models to perform well on the task objective after adapting them\nusing an unsupervised loss.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 15:26:24 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 00:43:24 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 18:43:40 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Alet", "Ferran", ""], ["Bauza", "Maria", ""], ["Kawaguchi", "Kenji", ""], ["Kuru", "Nurullah Giray", ""], ["Lozano-Perez", "Tomas", ""], ["Kaelbling", "Leslie Pack", ""]]}, {"id": "2009.10625", "submitter": "Petru Soviany", "authors": "Petru Soviany", "title": "Curriculum Learning with Diversity for Supervised Computer Vision Tasks", "comments": "Accepted at MRC 2020 @ ECAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Curriculum learning techniques are a viable solution for improving the\naccuracy of automatic models, by replacing the traditional random training with\nan easy-to-hard strategy. However, the standard curriculum methodology does not\nautomatically provide improved results, but it is constrained by multiple\nelements like the data distribution or the proposed model. In this paper, we\nintroduce a novel curriculum sampling strategy which takes into consideration\nthe diversity of the training data together with the difficulty of the inputs.\nWe determine the difficulty using a state-of-the-art estimator based on the\nhuman time required for solving a visual search task. We consider this kind of\ndifficulty metric to be better suited for solving general problems, as it is\nnot based on certain task-dependent elements, but more on the context of each\nimage. We ensure the diversity during training, giving higher priority to\nelements from less visited classes. We conduct object detection and instance\nsegmentation experiments on Pascal VOC 2007 and Cityscapes data sets,\nsurpassing both the randomly-trained baseline and the standard curriculum\napproach. We prove that our strategy is very efficient for unbalanced data\nsets, leading to faster convergence and more accurate results, when other\ncurriculum-based strategies fail.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 15:32:49 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Soviany", "Petru", ""]]}, {"id": "2009.10634", "submitter": "Hans Dolfing", "authors": "Hans J.G.A. Dolfing", "title": "Whole page recognition of historical handwriting", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historical handwritten documents guard an important part of human knowledge\nonly within reach of a few scholars and experts. Recent developments in machine\nlearning and handwriting research have the potential of rendering this\ninformation accessible and searchable to a larger audience. To this end, we\ninvestigate an end-to-end inference approach without text localization which\ntakes a handwritten page and transcribes its full text. No explicit character,\nword or line segmentation is involved in inference which is why we call this\napproach \"segmentation free\". We explore its robustness and accuracy compared\nto a line-by-line segmented approach based on the IAM, RODRIGO and ScribbleLens\ncorpora, in three languages with handwriting styles spanning 400 years. We\nconcentrate on model types and sizes which can be deployed on a hand-held or\nembedded device. We conclude that a whole page inference approach without text\nlocalization and segmentation is competitive.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 15:46:33 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Dolfing", "Hans J. G. A.", ""]]}, {"id": "2009.10639", "submitter": "Yi-Shan Lin", "authors": "Yi-Shan Lin, Wen-Chuan Lee, Z. Berkay Celik", "title": "What Do You See? Evaluation of Explainable Artificial Intelligence (XAI)\n  Interpretability through Neural Backdoors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  EXplainable AI (XAI) methods have been proposed to interpret how a deep\nneural network predicts inputs through model saliency explanations that\nhighlight the parts of the inputs deemed important to arrive a decision at a\nspecific target. However, it remains challenging to quantify correctness of\ntheir interpretability as current evaluation approaches either require\nsubjective input from humans or incur high computation cost with automated\nevaluation. In this paper, we propose backdoor trigger patterns--hidden\nmalicious functionalities that cause misclassification--to automate the\nevaluation of saliency explanations. Our key observation is that triggers\nprovide ground truth for inputs to evaluate whether the regions identified by\nan XAI method are truly relevant to its output. Since backdoor triggers are the\nmost important features that cause deliberate misclassification, a robust XAI\nmethod should reveal their presence at inference time. We introduce three\ncomplementary metrics for systematic evaluation of explanations that an XAI\nmethod generates and evaluate seven state-of-the-art model-free and\nmodel-specific posthoc methods through 36 models trojaned with specifically\ncrafted triggers using color, shape, texture, location, and size. We discovered\nsix methods that use local explanation and feature relevance fail to completely\nhighlight trigger regions, and only a model-free approach can uncover the\nentire trigger region.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 15:53:19 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Lin", "Yi-Shan", ""], ["Lee", "Wen-Chuan", ""], ["Celik", "Z. Berkay", ""]]}, {"id": "2009.10663", "submitter": "Ionut Mironica", "authors": "Ionu\\c{t} Mironic\\u{a}", "title": "A Generative Adversarial Approach with Residual Learning for Dust and\n  Scratches Artifacts Removal", "comments": null, "journal-ref": null, "doi": "10.1145/3423323.3423411", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Retouching can significantly elevate the visual appeal of photos, but many\ncasual photographers lack the expertise to operate in a professional manner.\nOne particularly challenging task for old photo retouching remains the removal\nof dust and scratches artifacts. Traditionally, this task has been completed\nmanually with special image enhancement software and represents a tedious task\nthat requires special know-how of photo editing applications.\n  However, recent research utilizing Generative Adversarial Networks (GANs) has\nbeen proven to obtain good results in various automated image enhancement tasks\ncompared to traditional methods. This motivated us to explore the use of GANs\nin the context of film photo editing. In this paper, we present a GAN based\nmethod that is able to remove dust and scratches errors from film scans.\nSpecifically, residual learning is utilized to speed up the training process,\nas well as boost the denoising performance.\n  An extensive evaluation of our model on a community provided dataset shows\nthat it generalizes remarkably well, not being dependent on any particular type\nof image. Finally, we significantly outperform the state-of-the-art methods and\nsoftware applications, providing superior results.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 16:32:57 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Mironic\u0103", "Ionu\u0163", ""]]}, {"id": "2009.10679", "submitter": "Manish Bhattarai", "authors": "Manish Bhattarai, Aura Rose Jensen-Curtis, Manel Mart\\'iNez-Ram\\'on", "title": "An embedded deep learning system for augmented reality in firefighting\n  applications", "comments": "Accepted to ICMLA Special Session on Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Firefighting is a dynamic activity, in which numerous operations occur\nsimultaneously. Maintaining situational awareness (i.e., knowledge of current\nconditions and activities at the scene) is critical to the accurate\ndecision-making necessary for the safe and successful navigation of a fire\nenvironment by firefighters. Conversely, the disorientation caused by hazards\nsuch as smoke and extreme heat can lead to injury or even fatality. This\nresearch implements recent advancements in technology such as deep learning,\npoint cloud and thermal imaging, and augmented reality platforms to improve a\nfirefighter's situational awareness and scene navigation through improved\ninterpretation of that scene. We have designed and built a prototype embedded\nsystem that can leverage data streamed from cameras built into a firefighter's\npersonal protective equipment (PPE) to capture thermal, RGB color, and depth\nimagery and then deploy already developed deep learning models to analyze the\ninput data in real time. The embedded system analyzes and returns the processed\nimages via wireless streaming, where they can be viewed remotely and relayed\nback to the firefighter using an augmented reality platform that visualizes the\nresults of the analyzed inputs and draws the firefighter's attention to objects\nof interest, such as doors and windows otherwise invisible through smoke and\nflames.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 16:55:44 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Bhattarai", "Manish", ""], ["Jensen-Curtis", "Aura Rose", ""], ["Mart\u00edNez-Ram\u00f3n", "Manel", ""]]}, {"id": "2009.10687", "submitter": "Ananya Jana", "authors": "Ananya Jana, Hui Qu, Puru Rattan, Carlos D. Minacapelli, Vinod Rustgi,\n  Dimitris Metaxas", "title": "Deep Learning based NAS Score and Fibrosis Stage Prediction from CT and\n  Pathology Data", "comments": "6 pages, 3 figures. Accepted in IEEE BIBE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Alcoholic Fatty Liver Disease (NAFLD) is becoming increasingly prevalent\nin the world population. Without diagnosis at the right time, NAFLD can lead to\nnon-alcoholic steatohepatitis (NASH) and subsequent liver damage. The diagnosis\nand treatment of NAFLD depend on the NAFLD activity score (NAS) and the liver\nfibrosis stage, which are usually evaluated from liver biopsies by\npathologists. In this work, we propose a novel method to automatically predict\nNAS score and fibrosis stage from CT data that is non-invasive and inexpensive\nto obtain compared with liver biopsy. We also present a method to combine the\ninformation from CT and H\\&E stained pathology data to improve the performance\nof NAS score and fibrosis stage prediction, when both types of data are\navailable. This is of great value to assist the pathologists in computer-aided\ndiagnosis process. Experiments on a 30-patient dataset illustrate the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 17:02:31 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Jana", "Ananya", ""], ["Qu", "Hui", ""], ["Rattan", "Puru", ""], ["Minacapelli", "Carlos D.", ""], ["Rustgi", "Vinod", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "2009.10692", "submitter": "Ramtin Zand", "authors": "Brendan Reidy, Golareh Jalilvand, Tengfei Jiang, Ramtin Zand", "title": "TSV Extrusion Morphology Classification Using Deep Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we utilize deep convolutional neural networks (CNNs) to\nclassify the morphology of through-silicon via (TSV) extrusion in three\ndimensional (3D) integrated circuits (ICs). TSV extrusion is a crucial\nreliability concern which can deform and crack interconnect layers in 3D ICs\nand cause device failures. Herein, the white light interferometry (WLI)\ntechnique is used to obtain the surface profile of the extruded TSVs. We have\ndeveloped a program that uses raw data obtained from WLI to create a TSV\nextrusion morphology dataset, including TSV images with 54x54 pixels that are\nlabeled and categorized into three morphology classes. Four CNN architectures\nwith different network complexities are implemented and trained for TSV\nextrusion morphology classification application. Data augmentation and dropout\napproaches are utilized to realize a balance between overfitting and\nunderfitting in the CNN models. Results obtained show that the CNN model with\noptimized complexity, dropout, and data augmentation can achieve a\nclassification accuracy comparable to that of a human expert.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 17:05:55 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Reidy", "Brendan", ""], ["Jalilvand", "Golareh", ""], ["Jiang", "Tengfei", ""], ["Zand", "Ramtin", ""]]}, {"id": "2009.10711", "submitter": "Donglai Xiang", "authors": "Donglai Xiang, Fabian Prada, Chenglei Wu, Jessica Hodgins", "title": "MonoClothCap: Towards Temporally Coherent Clothing Capture from\n  Monocular RGB Video", "comments": "3DV 2020 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to capture temporally coherent dynamic clothing\ndeformation from a monocular RGB video input. In contrast to the existing\nliterature, our method does not require a pre-scanned personalized mesh\ntemplate, and thus can be applied to in-the-wild videos. To constrain the\noutput to a valid deformation space, we build statistical deformation models\nfor three types of clothing: T-shirt, short pants and long pants. A\ndifferentiable renderer is utilized to align our captured shapes to the input\nframes by minimizing the difference in both silhouette, segmentation, and\ntexture. We develop a UV texture growing method which expands the visible\ntexture region of the clothing sequentially in order to minimize drift in\ndeformation tracking. We also extract fine-grained wrinkle detail from the\ninput videos by fitting the clothed surface to the normal maps estimated by a\nconvolutional neural network. Our method produces temporally coherent\nreconstruction of body and clothing from monocular video. We demonstrate\nsuccessful clothing capture results from a variety of challenging videos.\nExtensive quantitative experiments demonstrate the effectiveness of our method\non metrics including body pose error and surface reconstruction error of the\nclothing.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 17:54:38 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 16:23:04 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Xiang", "Donglai", ""], ["Prada", "Fabian", ""], ["Wu", "Chenglei", ""], ["Hodgins", "Jessica", ""]]}, {"id": "2009.10762", "submitter": "Anirudh Som", "authors": "Hongjun Choi, Anirudh Som, Pavan Turaga", "title": "Role of Orthogonality Constraints in Improving Properties of Deep\n  Networks for Image Classification", "comments": "8 figures, 4 tables, 1 pseudo-code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard deep learning models that employ the categorical cross-entropy loss\nare known to perform well at image classification tasks. However, many standard\nmodels thus obtained often exhibit issues like feature redundancy, low\ninterpretability, and poor calibration. A body of recent work has emerged that\nhas tried addressing some of these challenges by proposing the use of new\nregularization functions in addition to the cross-entropy loss. In this paper,\nwe present some surprising findings that emerge from exploring the role of\nsimple orthogonality constraints as a means of imposing physics-motivated\nconstraints common in imaging. We propose an Orthogonal Sphere (OS) regularizer\nthat emerges from physics-based latent-representations under simplifying\nassumptions. Under further simplifying assumptions, the OS constraint can be\nwritten in closed-form as a simple orthonormality term and be used along with\nthe cross-entropy loss function. The findings indicate that orthonormality loss\nfunction results in a) rich and diverse feature representations, b) robustness\nto feature sub-selection, c) better semantic localization in the class\nactivation maps, and d) reduction in model calibration error. We demonstrate\nthe effectiveness of the proposed OS regularization by providing quantitative\nand qualitative results on four benchmark datasets - CIFAR10, CIFAR100, SVHN\nand tiny ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 18:46:05 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Choi", "Hongjun", ""], ["Som", "Anirudh", ""], ["Turaga", "Pavan", ""]]}, {"id": "2009.10765", "submitter": "Sherif Abdulatif", "authors": "Karim Armanious, Sherif Abdulatif, Wenbin Shi, Shashank Salian, Thomas\n  K\\\"ustner, Daniel Weiskopf, Tobias Hepp, Sergios Gatidis, Bin Yang", "title": "Age-Net: An MRI-Based Iterative Framework for Brain Biological Age\n  Estimation", "comments": "Accepted to IEEE Transcations on Medical Imaging 2021. 13 pages, 14\n  figures, 4 tables", "journal-ref": null, "doi": "10.1109/TMI.2021.3066857", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of biological age (BA), although important in clinical practice,\nis hard to grasp mainly due to the lack of a clearly defined reference\nstandard. For specific applications, especially in pediatrics, medical image\ndata are used for BA estimation in a routine clinical context. Beyond this\nyoung age group, BA estimation is mostly restricted to whole-body assessment\nusing non-imaging indicators such as blood biomarkers, genetic and cellular\ndata. However, various organ systems may exhibit different aging\ncharacteristics due to lifestyle and genetic factors. Thus, a whole-body\nassessment of the BA does not reflect the deviations of aging behavior between\norgans. To this end, we propose a new imaging-based framework for\norgan-specific BA estimation. In this initial study, we focus mainly on brain\nMRI. As a first step, we introduce a chronological age (CA) estimation\nframework using deep convolutional neural networks (Age-Net). We quantitatively\nassess the performance of this framework in comparison to existing\nstate-of-the-art CA estimation approaches. Furthermore, we expand upon Age-Net\nwith a novel iterative data-cleaning algorithm to segregate atypical-aging\npatients (BA $\\not \\approx$ CA) from the given population. We hypothesize that\nthe remaining population should approximate the true BA behavior. We apply the\nproposed methodology on a brain magnetic resonance image (MRI) dataset\ncontaining healthy individuals as well as Alzheimer's patients with different\ndementia ratings. We demonstrate the correlation between the predicted BAs and\nthe expected cognitive deterioration in Alzheimer's patients. A statistical and\nvisualization-based analysis has provided evidence regarding the potential and\ncurrent challenges of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 19:04:02 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 15:48:23 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Armanious", "Karim", ""], ["Abdulatif", "Sherif", ""], ["Shi", "Wenbin", ""], ["Salian", "Shashank", ""], ["K\u00fcstner", "Thomas", ""], ["Weiskopf", "Daniel", ""], ["Hepp", "Tobias", ""], ["Gatidis", "Sergios", ""], ["Yang", "Bin", ""]]}, {"id": "2009.10769", "submitter": "Amirhossein Bayat", "authors": "Amirhossein Bayat, Suprosanna Shit, Adrian Kilian, J\\\"urgen T.\n  Liechtenstein, Jan S. Kirschke, Bjoern H. Menze", "title": "Cranial Implant Prediction using Low-Resolution 3D Shape Completion and\n  High-Resolution 2D Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing of a cranial implant needs a 3D understanding of the complete skull\nshape. Thus, taking a 2D approach is sub-optimal, since a 2D model lacks a\nholistic 3D view of both the defective and healthy skulls. Further, loading the\nwhole 3D skull shapes at its original image resolution is not feasible in\ncommonly available GPUs. To mitigate these issues, we propose a fully\nconvolutional network composed of two subnetworks. The first subnetwork is\ndesigned to complete the shape of the downsampled defective skull. The second\nsubnetwork upsamples the reconstructed shape slice-wise. We train the 3D and 2D\nnetworks together end-to-end, with a hierarchical loss function. Our proposed\nsolution accurately predicts a high-resolution 3D implant in the challenge test\ncase in terms of dice-score and the Hausdorff distance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 19:16:16 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 13:10:01 GMT"}, {"version": "v3", "created": "Sun, 27 Sep 2020 23:19:08 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Bayat", "Amirhossein", ""], ["Shit", "Suprosanna", ""], ["Kilian", "Adrian", ""], ["Liechtenstein", "J\u00fcrgen T.", ""], ["Kirschke", "Jan S.", ""], ["Menze", "Bjoern H.", ""]]}, {"id": "2009.10777", "submitter": "Thyagharajan K K", "authors": "S. Kavitha, K. K. Thyagharajan", "title": "Efficient DWT-based fusion techniques using genetic algorithm for\n  optimal parameter estimation", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": "10.1007/s00500-015-2009-6", "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image fusion plays a vital role in medical imaging. Image fusion aims to\nintegrate complementary as well as redundant information from multiple\nmodalities into a single fused image without distortion or loss of information.\nIn this research work, discrete wavelet transform (DWT)and undecimated discrete\nwavelet transform (UDWT)-based fusion techniques using genetic algorithm\n(GA)foroptimalparameter(weight)estimationinthefusionprocessareimplemented and\nanalyzed with multi-modality brain images. The lack of shift variance while\nperforming image fusion using DWT is addressed using UDWT. The proposed fusion\nmodel uses an efficient, modified GA in DWT and UDWT for optimal parameter\nestimation, to improve the image quality and contrast. The complexity of the\nbasic GA (pixel level) has been reduced in the modified GA (feature level), by\nlimiting the search space. It is observed from our experiments that fusion\nusing DWT and UDWT techniques with GA for optimal parameter estimation resulted\nin a better fused image in the aspects of retaining the information and\ncontrast without error, both in human perception as well as evaluation using\nobjective metrics. The contributions of this research work are (1) reduced time\nand space complexity in estimating the weight values using GA for fusion (2)\nsystem is scalable for input image of any size with similar time complexity,\nowing to feature level GA implementation and (3) identification of source image\nthat contributes more to the fused image, from the weight values estimated.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 19:28:57 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Kavitha", "S.", ""], ["Thyagharajan", "K. K.", ""]]}, {"id": "2009.10804", "submitter": "Zhengzhong Tu", "authors": "Zhengzhong Tu, Jessie Lin, Yilin Wang, Balu Adsumilli, and Alan C.\n  Bovik", "title": "Adaptive Debanding Filter", "comments": "4 pages, 7 figures, 1 table. Accepted to IEEE Signal Processing\n  Letters", "journal-ref": null, "doi": "10.1109/LSP.2020.3024985", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Banding artifacts, which manifest as staircase-like color bands on pictures\nor video frames, is a common distortion caused by compression of low-textured\nsmooth regions. These false contours can be very noticeable even on\nhigh-quality videos, especially when displayed on high-definition screens. Yet,\nrelatively little attention has been applied to this problem. Here we consider\nbanding artifact removal as a visual enhancement problem, and accordingly, we\nsolve it by applying a form of content-adaptive smoothing filtering followed by\ndithered quantization, as a post-processing module. The proposed debanding\nfilter is able to adaptively smooth banded regions while preserving image edges\nand details, yielding perceptually enhanced gradient rendering with limited\nbit-depths. Experimental results show that our proposed debanding filter\noutperforms state-of-the-art false contour removing algorithms both visually\nand quantitatively.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 20:44:20 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Tu", "Zhengzhong", ""], ["Lin", "Jessie", ""], ["Wang", "Yilin", ""], ["Adsumilli", "Balu", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2009.10814", "submitter": "Mohamed Amine Mahmoudi", "authors": "M.Amine Mahmoudi, Aladine Chetouani, Fatma Boufera and Hedi Tabia", "title": "Kernelized dense layers for facial expression recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully connected layer is an essential component of Convolutional Neural\nNetworks (CNNs), which demonstrates its efficiency in computer vision tasks.\nThe CNN process usually starts with convolution and pooling layers that first\nbreak down the input images into features, and then analyze them independently.\nThe result of this process feeds into a fully connected neural network\nstructure which drives the final classification decision. In this paper, we\npropose a Kernelized Dense Layer (KDL) which captures higher order feature\ninteractions instead of conventional linear relations. We apply this method to\nFacial Expression Recognition (FER) and evaluate its performance on RAF,\nFER2013 and ExpW datasets. The experimental results demonstrate the benefits of\nsuch layer and show that our model achieves competitive results with respect to\nthe state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 21:02:00 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Mahmoudi", "M. Amine", ""], ["Chetouani", "Aladine", ""], ["Boufera", "Fatma", ""], ["Tabia", "Hedi", ""]]}, {"id": "2009.10825", "submitter": "Jia Xue", "authors": "Jia Xue, Matthew Purri, Kristin Dana", "title": "Angular Luminance for Material Segmentation", "comments": "IEEE International Geoscience and Remote Sensing Symposium (IGARSS)\n  2020", "journal-ref": "IEEE International Geoscience and Remote Sensing Symposium\n  (IGARSS) 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Moving cameras provide multiple intensity measurements per pixel, yet often\nsemantic segmentation, material recognition, and object recognition do not\nutilize this information. With basic alignment over several frames of a moving\ncamera sequence, a distribution of intensities over multiple angles is\nobtained. It is well known from prior work that luminance histograms and the\nstatistics of natural images provide a strong material recognition cue. We\nutilize per-pixel {\\it angular luminance distributions} as a key feature in\ndiscriminating the material of the surface. The angle-space sampling in a\nmultiview satellite image sequence is an unstructured sampling of the\nunderlying reflectance function of the material. For real-world materials there\nis significant intra-class variation that can be managed by building a angular\nluminance network (AngLNet). This network combines angular reflectance cues\nfrom multiple images with spatial cues as input to fully convolutional networks\nfor material segmentation. We demonstrate the increased performance of AngLNet\nover prior state-of-the-art in material segmentation from satellite imagery.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 21:15:27 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Xue", "Jia", ""], ["Purri", "Matthew", ""], ["Dana", "Kristin", ""]]}, {"id": "2009.10858", "submitter": "Sonia Phene", "authors": "Joy Hsu, Sonia Phene, Akinori Mitani, Jieying Luo, Naama Hammel,\n  Jonathan Krause, Rory Sayres", "title": "Improving Medical Annotation Quality to Decrease Labeling Burden Using\n  Stratified Noisy Cross-Validation", "comments": null, "journal-ref": "ACM Conference on Health, Inference, and Learning, April 02-04,\n  2020, Toronto, Canada", "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning has become increasingly applied to medical imaging data,\nnoise in training labels has emerged as an important challenge. Variability in\ndiagnosis of medical images is well established; in addition, variability in\ntraining and attention to task among medical labelers may exacerbate this\nissue. Methods for identifying and mitigating the impact of low quality labels\nhave been studied, but are not well characterized in medical imaging tasks. For\ninstance, Noisy Cross-Validation splits the training data into halves, and has\nbeen shown to identify low-quality labels in computer vision tasks; but it has\nnot been applied to medical imaging tasks specifically. In this work we\nintroduce Stratified Noisy Cross-Validation (SNCV), an extension of noisy cross\nvalidation. SNCV can provide estimates of confidence in model predictions by\nassigning a quality score to each example; stratify labels to handle class\nimbalance; and identify likely low-quality labels to analyze the causes. We\nassess performance of SNCV on diagnosis of glaucoma suspect risk from retinal\nfundus photographs, a clinically important yet nuanced labeling task. Using\ntraining data from a previously-published deep learning model, we compute a\ncontinuous quality score (QS) for each training example. We relabel 1,277\nlow-QS examples using a trained glaucoma specialist; the new labels agree with\nthe SNCV prediction over the initial label >85% of the time, indicating that\nlow-QS examples mostly reflect labeler errors. We then quantify the impact of\ntraining with only high-QS labels, showing that strong model performance may be\nobtained with many fewer examples. By applying the method to randomly\nsub-sampled training dataset, we show that our method can reduce labelling\nburden by approximately 50% while achieving model performance non-inferior to\nusing the full dataset on multiple held-out test sets.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 23:32:59 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Hsu", "Joy", ""], ["Phene", "Sonia", ""], ["Mitani", "Akinori", ""], ["Luo", "Jieying", ""], ["Hammel", "Naama", ""], ["Krause", "Jonathan", ""], ["Sayres", "Rory", ""]]}, {"id": "2009.10868", "submitter": "Uehwan Kim", "authors": "Ue-Hwan Kim, Dongho Ka, Hwasoo Yeo, Jong-Hwan Kim", "title": "A Real-time Vision Framework for Pedestrian Behavior Recognition and\n  Intention Prediction at Intersections Using 3D Pose Estimation", "comments": "12 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimizing traffic accidents between vehicles and pedestrians is one of the\nprimary research goals in intelligent transportation systems. To achieve the\ngoal, pedestrian behavior recognition and prediction of pedestrian's crossing\nor not-crossing intention play a central role. Contemporary approaches do not\nguarantee satisfactory performance due to lack of generalization, the\nrequirement of manual data labeling, and high computational complexity. To\novercome these limitations, we propose a real-time vision framework for two\ntasks: pedestrian behavior recognition (100.53 FPS) and intention prediction\n(35.76 FPS). Our framework obtains satisfying generalization over multiple\nsites because of the proposed site-independent features. At the center of the\nfeature extraction lies 3D pose estimation. The 3D pose analysis enables robust\nand accurate recognition of pedestrian behaviors and prediction of intentions\nover multiple sites. The proposed vision framework realizes 89.3% accuracy in\nthe behavior recognition task on the TUD dataset without any training process\nand 91.28% accuracy in intention prediction on our dataset achieving new\nstate-of-the-art performance. To contribute to the corresponding research\ncommunity, we make our source codes public which are available at\nhttps://github.com/Uehwan/VisionForPedestrian\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 00:55:12 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Kim", "Ue-Hwan", ""], ["Ka", "Dongho", ""], ["Yeo", "Hwasoo", ""], ["Kim", "Jong-Hwan", ""]]}, {"id": "2009.10874", "submitter": "Xianbiao Qi", "authors": "Bingcong Li, Xin Tang, Xianbiao Qi, Yihao Chen, Rong Xiao", "title": "Hamming OCR: A Locality Sensitive Hashing Neural Network for Scene Text\n  Recognition", "comments": "9 Pages, 4 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, inspired by Transformer, self-attention-based scene text\nrecognition approaches have achieved outstanding performance. However, we find\nthat the size of model expands rapidly with the lexicon increasing.\nSpecifically, the number of parameters for softmax classification layer and\noutput embedding layer are proportional to the vocabulary size. It hinders the\ndevelopment of a lightweight text recognition model especially applied for\nChinese and multiple languages. Thus, we propose a lightweight scene text\nrecognition model named Hamming OCR. In this model, a novel Hamming classifier,\nwhich adopts locality sensitive hashing (LSH) algorithm to encode each\ncharacter, is proposed to replace the softmax regression and the generated LSH\ncode is directly employed to replace the output embedding. We also present a\nsimplified transformer decoder to reduce the number of parameters by removing\nthe feed-forward network and using cross-layer parameter sharing technique.\nCompared with traditional methods, the number of parameters in both\nclassification and embedding layers is independent on the size of vocabulary,\nwhich significantly reduces the storage requirement without loss of accuracy.\nExperimental results on several datasets, including four public benchmaks and a\nChinese text dataset synthesized by SynthText with more than 20,000 characters,\nshows that Hamming OCR achieves competitive results.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 01:20:19 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Li", "Bingcong", ""], ["Tang", "Xin", ""], ["Qi", "Xianbiao", ""], ["Chen", "Yihao", ""], ["Xiao", "Rong", ""]]}, {"id": "2009.10891", "submitter": "Pengju Zhang", "authors": "Pengju Zhang, Yihong Wu, Bingxi Liu", "title": "Leveraging Local and Global Descriptors in Parallel to Search\n  Correspondences for Visual Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual localization to compute 6DoF camera pose from a given image has wide\napplications such as in robotics, virtual reality, augmented reality, etc. Two\nkinds of descriptors are important for the visual localization. One is global\ndescriptors that extract the whole feature from each image. The other is local\ndescriptors that extract the local feature from each image patch usually\nenclosing a key point. More and more methods of the visual localization have\ntwo stages: at first to perform image retrieval by global descriptors and then\nfrom the retrieval feedback to make 2D-3D point correspondences by local\ndescriptors. The two stages are in serial for most of the methods. This simple\ncombination has not achieved superiority of fusing local and global\ndescriptors. The 3D points obtained from the retrieval feedback are as the\nnearest neighbor candidates of the 2D image points only by global descriptors.\nEach of the 2D image points is also called a query local feature when\nperforming the 2D-3D point correspondences. In this paper, we propose a novel\nparallel search framework, which leverages advantages of both local and global\ndescriptors to get nearest neighbor candidates of a query local feature.\nSpecifically, besides using deep learning based global descriptors, we also\nutilize local descriptors to construct random tree structures for obtaining\nnearest neighbor candidates of the query local feature. We propose a new\nprobabilistic model and a new deep learning based local descriptor when\nconstructing the random trees. A weighted Hamming regularization term to keep\ndiscriminativeness after binarization is given in the loss function for the\nproposed local descriptor. The loss function co-trains both real and binary\ndescriptors of which the results are integrated into the random trees.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 01:49:03 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Zhang", "Pengju", ""], ["Wu", "Yihong", ""], ["Liu", "Bingxi", ""]]}, {"id": "2009.10892", "submitter": "Ziqiang Shi", "authors": "Ziqiang Shi and Liu Liu and Zhongling Liu and Rujie Liu and Xiaoyu Mi\n  and and Kentaro Murase", "title": "HiCOMEX: Facial Action Unit Recognition Based on Hierarchy Intensity\n  Distribution and COMEX Relation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of facial action units (AUs) has been studied as it has the\ncompetition due to the wide-ranging applications thereof. In this paper, we\npropose a novel framework for the AU detection from a single input image by\ngrasping the \\textbf{c}o-\\textbf{o}ccurrence and \\textbf{m}utual\n\\textbf{ex}clusion (COMEX) as well as the intensity distribution among AUs. Our\nalgorithm uses facial landmarks to detect the features of local AUs. The\nfeatures are input to a bidirectional long short-term memory (BiLSTM) layer for\nlearning the intensity distribution. Afterwards, the new AU feature\ncontinuously passed through a self-attention encoding layer and a\ncontinuous-state modern Hopfield layer for learning the COMEX relationships.\nOur experiments on the challenging BP4D and DISFA benchmarks without any\nexternal data or pre-trained models yield F1-scores of 63.7\\% and 61.8\\%\nrespectively, which shows our proposed networks can lead to performance\nimprovement in the AU detection task.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 01:49:56 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 00:47:58 GMT"}, {"version": "v3", "created": "Thu, 29 Apr 2021 02:04:23 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Shi", "Ziqiang", ""], ["Liu", "Liu", ""], ["Liu", "Zhongling", ""], ["Liu", "Rujie", ""], ["Mi", "Xiaoyu", ""], ["Murase", "and Kentaro", ""]]}, {"id": "2009.10893", "submitter": "Najeeb Khan", "authors": "Najeeb Khan and Ian Stavness", "title": "Pruning Convolutional Filters using Batch Bridgeout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art computer vision models are rapidly increasing in capacity,\nwhere the number of parameters far exceeds the number required to fit the\ntraining set. This results in better optimization and generalization\nperformance. However, the huge size of contemporary models results in large\ninference costs and limits their use on resource-limited devices. In order to\nreduce inference costs, convolutional filters in trained neural networks could\nbe pruned to reduce the run-time memory and computational requirements during\ninference. However, severe post-training pruning results in degraded\nperformance if the training algorithm results in dense weight vectors. We\npropose the use of Batch Bridgeout, a sparsity inducing stochastic\nregularization scheme, to train neural networks so that they could be pruned\nefficiently with minimal degradation in performance. We evaluate the proposed\nmethod on common computer vision models VGGNet, ResNet, and Wide-ResNet on the\nCIFAR image classification task. For all the networks, experimental results\nshow that Batch Bridgeout trained networks achieve higher accuracy across a\nwide range of pruning intensities compared to Dropout and weight decay\nregularization.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 01:51:47 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Khan", "Najeeb", ""], ["Stavness", "Ian", ""]]}, {"id": "2009.10916", "submitter": "Lv Tang", "authors": "Lv Tang and Bo Li", "title": "CLASS: Cross-Level Attention and Supervision for Salient Objects\n  Detection", "comments": "Full version of ACCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection (SOD) is a fundamental computer vision task.\nRecently, with the revival of deep neural networks, SOD has made great\nprogresses. However, there still exist two thorny issues that cannot be well\naddressed by existing methods, indistinguishable regions and complex\nstructures. To address these two issues, in this paper we propose a novel deep\nnetwork for accurate SOD, named CLASS. First, in order to leverage the\ndifferent advantages of low-level and high-level features, we propose a novel\nnon-local cross-level attention (CLA), which can capture the long-range feature\ndependencies to enhance the distinction of complete salient object. Second, a\nnovel cross-level supervision (CLS) is designed to learn complementary context\nfor complex structures through pixel-level, region-level and object-level. Then\nthe fine structures and boundaries of salient objects can be well restored. In\nexperiments, with the proposed CLA and CLS, our CLASS net. consistently\noutperforms 13 state-of-the-art methods on five datasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 03:10:12 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 08:12:12 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Tang", "Lv", ""], ["Li", "Bo", ""]]}, {"id": "2009.10939", "submitter": "Yaniv Benny", "authors": "Maor Ivgi, Yaniv Benny, Avichai Ben-David, Jonathan Berant, and Lior\n  Wolf", "title": "Scene Graph to Image Generation with Contextualized Object Layout\n  Refinement", "comments": "To appear at IEEE International Conference in Image Processing (ICIP)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating images from scene graphs is a challenging task that attracted\nsubstantial interest recently. Prior works have approached this task by\ngenerating an intermediate layout description of the target image. However, the\nrepresentation of each object in the layout was generated independently, which\nresulted in high overlap, low coverage, and an overall blurry layout. We\npropose a novel method that alleviates these issues by generating the entire\nlayout description gradually to improve inter-object dependency. We empirically\nshow on the COCO-STUFF dataset that our approach improves the quality of both\nthe intermediate layout and the final image. Our approach improves the layout\ncoverage by almost 20 points and drops object overlap to negligible amounts.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 06:27:54 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 11:12:11 GMT"}, {"version": "v3", "created": "Sat, 17 Jul 2021 15:07:14 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ivgi", "Maor", ""], ["Benny", "Yaniv", ""], ["Ben-David", "Avichai", ""], ["Berant", "Jonathan", ""], ["Wolf", "Lior", ""]]}, {"id": "2009.10942", "submitter": "Ping Li PhD", "authors": "Ping Li, Qinghao Ye, Luming Zhang, Li Yuan, Xianghua Xu, Ling Shao", "title": "Exploring global diverse attention via pairwise temporal relation for\n  video summarization", "comments": "12 pages, 8 figures", "journal-ref": "Pattern Recognition, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summarization is an effective way to facilitate video searching and\nbrowsing. Most of existing systems employ encoder-decoder based recurrent\nneural networks, which fail to explicitly diversify the system-generated\nsummary frames while requiring intensive computations. In this paper, we\npropose an efficient convolutional neural network architecture for video\nSUMmarization via Global Diverse Attention called SUM-GDA, which adapts\nattention mechanism in a global perspective to consider pairwise temporal\nrelations of video frames. Particularly, the GDA module has two advantages: 1)\nit models the relations within paired frames as well as the relations among all\npairs, thus capturing the global attention across all frames of one video; 2)\nit reflects the importance of each frame to the whole video, leading to diverse\nattention on these frames. Thus, SUM-GDA is beneficial for generating diverse\nframes to form satisfactory video summary. Extensive experiments on three data\nsets, i.e., SumMe, TVSum, and VTW, have demonstrated that SUM-GDA and its\nextension outperform other competing state-of-the-art methods with remarkable\nimprovements. In addition, the proposed models can be run in parallel with\nsignificantly less computational costs, which helps the deployment in highly\ndemanding applications.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 06:29:09 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Li", "Ping", ""], ["Ye", "Qinghao", ""], ["Zhang", "Luming", ""], ["Yuan", "Li", ""], ["Xu", "Xianghua", ""], ["Shao", "Ling", ""]]}, {"id": "2009.10945", "submitter": "Zehan Zhang", "authors": "Zehan Zhang, Ming Zhang, Zhidong Liang, Xian Zhao, Ming Yang, Wenming\n  Tan, and ShiLiang Pu", "title": "MAFF-Net: Filter False Positive for 3D Vehicle Detection with\n  Multi-modal Adaptive Feature Fusion", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D vehicle detection based on multi-modal fusion is an important task of many\napplications such as autonomous driving. Although significant progress has been\nmade, we still observe two aspects that need to be further improvement: First,\nthe specific gain that camera images can bring to 3D detection is seldom\nexplored by previous works. Second, many fusion algorithms run slowly, which is\nessential for applications with high real-time requirements(autonomous\ndriving). To this end, we propose an end-to-end trainable single-stage\nmulti-modal feature adaptive network in this paper, which uses image\ninformation to effectively reduce false positive of 3D detection and has a fast\ndetection speed. A multi-modal adaptive feature fusion module based on channel\nattention mechanism is proposed to enable the network to adaptively use the\nfeature of each modal. Based on the above mechanism, two fusion technologies\nare proposed to adapt to different usage scenarios: PointAttentionFusion is\nsuitable for filtering simple false positive and faster; DenseAttentionFusion\nis suitable for filtering more difficult false positive and has better overall\nperformance. Experimental results on the KITTI dataset demonstrate significant\nimprovement in filtering false positive over the approach using only point\ncloud data. Furthermore, the proposed method can provide competitive results\nand has the fastest speed compared to the published state-of-the-art\nmulti-modal methods in the KITTI benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 06:31:59 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Zhang", "Zehan", ""], ["Zhang", "Ming", ""], ["Liang", "Zhidong", ""], ["Zhao", "Xian", ""], ["Yang", "Ming", ""], ["Tan", "Wenming", ""], ["Pu", "ShiLiang", ""]]}, {"id": "2009.10962", "submitter": "Keisuke Kanda", "authors": "Keisuke Kanda, Brian Kenji Iwana, Seiichi Uchida", "title": "What is the Reward for Handwriting? -- Handwriting Generation by\n  Imitation Learning", "comments": "Accepted at ICFHR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing the handwriting generation process is an important issue and has\nbeen tackled by various generation models, such as kinematics based models and\nstochastic models. In this study, we use a reinforcement learning (RL)\nframework to realize handwriting generation with the careful future planning\nability. In fact, the handwriting process of human beings is also supported by\ntheir future planning ability; for example, the ability is necessary to\ngenerate a closed trajectory like '0' because any shortsighted model, such as a\nMarkovian model, cannot generate it. For the algorithm, we employ generative\nadversarial imitation learning (GAIL). Typical RL algorithms require the manual\ndefinition of the reward function, which is very crucial to control the\ngeneration process. In contrast, GAIL trains the reward function along with the\nother modules of the framework. In other words, through GAIL, we can understand\nthe reward of the handwriting generation process from handwriting examples. Our\nexperimental results qualitatively and quantitatively show that the learned\nreward catches the trends in handwriting generation and thus GAIL is well\nsuited for the acquisition of handwriting behavior.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 07:04:08 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Kanda", "Keisuke", ""], ["Iwana", "Brian Kenji", ""], ["Uchida", "Seiichi", ""]]}, {"id": "2009.10978", "submitter": "Wonseok Lee", "authors": "Wonseok Lee, Hanbit Lee, Sang-goo Lee", "title": "Semantics-Preserving Adversarial Training", "comments": "Preprint. Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training is a defense technique that improves adversarial\nrobustness of a deep neural network (DNN) by including adversarial examples in\nthe training data. In this paper, we identify an overlooked problem of\nadversarial training in that these adversarial examples often have different\nsemantics than the original data, introducing unintended biases into the model.\nWe hypothesize that such non-semantics-preserving (and resultingly ambiguous)\nadversarial data harm the robustness of the target models. To mitigate such\nunintended semantic changes of adversarial examples, we propose\nsemantics-preserving adversarial training (SPAT) which encourages perturbation\non the pixels that are shared among all classes when generating adversarial\nexamples in the training stage. Experiment results show that SPAT improves\nadversarial robustness and achieves state-of-the-art results in CIFAR-10 and\nCIFAR-100.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 07:42:14 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Lee", "Wonseok", ""], ["Lee", "Hanbit", ""], ["Lee", "Sang-goo", ""]]}, {"id": "2009.10987", "submitter": "Jiabo He", "authors": "Jiabo He, Sarah Erfani, Sudanthi Wijewickrema, Stephen O'Leary,\n  Kotagiri Ramamohanarao", "title": "Learning Non-Unique Segmentation with Reward-Penalty Dice Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is one of the key problems in the field of computer\nvision, as it enables computer image understanding. However, most research and\napplications of semantic segmentation focus on addressing unique segmentation\nproblems, where there is only one gold standard segmentation result for every\ninput image. This may not be true in some problems, e.g., medical applications.\nWe may have non-unique segmentation annotations as different surgeons may\nperform successful surgeries for the same patient in slightly different ways.\nTo comprehensively learn non-unique segmentation tasks, we propose the\nreward-penalty Dice loss (RPDL) function as the optimization objective for deep\nconvolutional neural networks (DCNN). RPDL is capable of helping DCNN learn\nnon-unique segmentation by enhancing common regions and penalizing outside\nones. Experimental results show that RPDL improves the performance of DCNN\nmodels by up to 18.4% compared with other loss functions on our collected\nsurgical dataset.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 07:59:49 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["He", "Jiabo", ""], ["Erfani", "Sarah", ""], ["Wijewickrema", "Sudanthi", ""], ["O'Leary", "Stephen", ""], ["Ramamohanarao", "Kotagiri", ""]]}, {"id": "2009.11008", "submitter": "Duy Minh Ho Nguyen", "authors": "Duy M. H. Nguyen, Duy M. Nguyen, Huong Vu, Binh T. Nguyen, Fabrizio\n  Nunnari, Daniel Sonntag", "title": "An Attention Mechanism with Multiple Knowledge Sources for COVID-19\n  Detection from CT Images", "comments": "In AAAI 2021 Workshop: Trustworthy AI for Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Until now, Coronavirus SARS-CoV-2 has caused more than 850,000 deaths and\ninfected more than 27 million individuals in over 120 countries. Besides\nprincipal polymerase chain reaction (PCR) tests, automatically identifying\npositive samples based on computed tomography (CT) scans can present a\npromising option in the early diagnosis of COVID-19. Recently, there have been\nincreasing efforts to utilize deep networks for COVID-19 diagnosis based on CT\nscans. While these approaches mostly focus on introducing novel architectures,\ntransfer learning techniques, or construction large scale data, we propose a\nnovel strategy to improve the performance of several baselines by leveraging\nmultiple useful information sources relevant to doctors' judgments.\nSpecifically, infected regions and heat maps extracted from learned networks\nare integrated with the global image via an attention mechanism during the\nlearning process. This procedure not only makes our system more robust to noise\nbut also guides the network focusing on local lesion areas. Extensive\nexperiments illustrate the superior performance of our approach compared to\nrecent baselines. Furthermore, our learned network guidance presents an\nexplainable feature to doctors as we can understand the connection between\ninput and output in a grey-box model.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 09:05:24 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 01:31:50 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2020 16:22:57 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2020 15:16:06 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Nguyen", "Duy M. H.", ""], ["Nguyen", "Duy M.", ""], ["Vu", "Huong", ""], ["Nguyen", "Binh T.", ""], ["Nunnari", "Fabrizio", ""], ["Sonntag", "Daniel", ""]]}, {"id": "2009.11009", "submitter": "Gavriel Habib", "authors": "Gavriel Habib, Nahum Kiryati, Miri Sklair-Levy, Anat Shalmon, Osnat\n  Halshtok Neiman, Renata Faermann Weidenfeld, Yael Yagil, Eli Konen, Arnaldo\n  Mayer", "title": "Automatic Breast Lesion Classification by Joint Neural Analysis of\n  Mammography and Ultrasound", "comments": "10 pages including references, 8 figures, 1 table. Accepted to MICCAI\n  ML-CDS 2020 workshop (ML-CDS 2020/CLIP 2020, LNCS 12445 proceedings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammography and ultrasound are extensively used by radiologists as\ncomplementary modalities to achieve better performance in breast cancer\ndiagnosis. However, existing computer-aided diagnosis (CAD) systems for the\nbreast are generally based on a single modality. In this work, we propose a\ndeep-learning based method for classifying breast cancer lesions from their\nrespective mammography and ultrasound images. We present various approaches and\nshow a consistent improvement in performance when utilizing both modalities.\nThe proposed approach is based on a GoogleNet architecture, fine-tuned for our\ndata in two training steps. First, a distinct neural network is trained\nseparately for each modality, generating high-level features. Then, the\naggregated features originating from each modality are used to train a\nmultimodal network to provide the final classification. In quantitative\nexperiments, the proposed approach achieves an AUC of 0.94, outperforming\nstate-of-the-art models trained over a single modality. Moreover, it performs\nsimilarly to an average radiologist, surpassing two out of four radiologists\nparticipating in a reader study. The promising results suggest that the\nproposed method may become a valuable decision support tool for breast\nradiologists.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 09:08:24 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Habib", "Gavriel", ""], ["Kiryati", "Nahum", ""], ["Sklair-Levy", "Miri", ""], ["Shalmon", "Anat", ""], ["Neiman", "Osnat Halshtok", ""], ["Weidenfeld", "Renata Faermann", ""], ["Yagil", "Yael", ""], ["Konen", "Eli", ""], ["Mayer", "Arnaldo", ""]]}, {"id": "2009.11016", "submitter": "Cong Geng", "authors": "Cong Geng, Jia Wang, Li Chen, Zhiyong Gao", "title": "Generative Model without Prior Distribution Matching", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Autoencoder (VAE) and its variations are classic generative\nmodels by learning a low-dimensional latent representation to satisfy some\nprior distribution (e.g., Gaussian distribution). Their advantages over GAN are\nthat they can simultaneously generate high dimensional data and learn latent\nrepresentations to reconstruct the inputs. However, it has been observed that a\ntrade-off exists between reconstruction and generation since matching prior\ndistribution may destroy the geometric structure of data manifold. To mitigate\nthis problem, we propose to let the prior match the embedding distribution\nrather than imposing the latent variables to fit the prior. The embedding\ndistribution is trained using a simple regularized autoencoder architecture\nwhich preserves the geometric structure to the maximum. Then an adversarial\nstrategy is employed to achieve a latent mapping. We provide both theoretical\nand experimental support for the effectiveness of our method, which alleviates\nthe contradiction between topological properties' preserving of data manifold\nand distribution matching in latent space.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 09:33:24 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Geng", "Cong", ""], ["Wang", "Jia", ""], ["Chen", "Li", ""], ["Gao", "Zhiyong", ""]]}, {"id": "2009.11042", "submitter": "Sanghyuk Chun", "authors": "Song Park, Sanghyuk Chun, Junbum Cha, Bado Lee, Hyunjung Shim", "title": "Few-shot Font Generation with Localized Style Representations and\n  Factorization", "comments": "Accepted at AAAI 2021, 12 pages, 11 figures, the first two authors\n  contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic few-shot font generation is a practical and widely studied problem\nbecause manual designs are expensive and sensitive to the expertise of\ndesigners. Existing few-shot font generation methods aim to learn to\ndisentangle the style and content element from a few reference glyphs, and\nmainly focus on a universal style representation for each font style. However,\nsuch approach limits the model in representing diverse local styles, and thus\nmakes it unsuitable to the most complicated letter system, e.g., Chinese, whose\ncharacters consist of a varying number of components (often called \"radical\")\nwith a highly complex structure. In this paper, we propose a novel font\ngeneration method by learning localized styles, namely component-wise style\nrepresentations, instead of universal styles. The proposed style\nrepresentations enable us to synthesize complex local details in text designs.\nHowever, learning component-wise styles solely from reference glyphs is\ninfeasible in the few-shot font generation scenario, when a target script has a\nlarge number of components, e.g., over 200 for Chinese. To reduce the number of\nreference glyphs, we simplify component-wise styles by a product of component\nfactor and style factor, inspired by low-rank matrix factorization. Thanks to\nthe combination of strong representation and a compact factorization strategy,\nour method shows remarkably better few-shot font generation results (with only\n8 reference glyph images) than other state-of-the-arts, without utilizing\nstrong locality supervision, e.g., location of each component, skeleton, or\nstrokes. The source code is available at https://github.com/clovaai/lffont.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 10:33:01 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 07:04:49 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Park", "Song", ""], ["Chun", "Sanghyuk", ""], ["Cha", "Junbum", ""], ["Lee", "Bado", ""], ["Shim", "Hyunjung", ""]]}, {"id": "2009.11044", "submitter": "Dime Kostadinov", "authors": "Dimche Kostadinov and Davide Scaramuzza", "title": "Unsupervised Feature Learning for Event Data: Direct vs Inverse Problem\n  Formulation", "comments": null, "journal-ref": "IAPR IEEE/Computer Society International Conference on Pattern\n  Recognition (ICPR), Milan, 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-based cameras record an asynchronous stream of per-pixel brightness\nchanges. As such, they have numerous advantages over the standard frame-based\ncameras, including high temporal resolution, high dynamic range, and no motion\nblur. Due to the asynchronous nature, efficient learning of compact\nrepresentation for event data is challenging. While it remains not explored the\nextent to which the spatial and temporal event \"information\" is useful for\npattern recognition tasks. In this paper, we focus on single-layer\narchitectures. We analyze the performance of two general problem formulations:\nthe direct and the inverse, for unsupervised feature learning from local event\ndata (local volumes of events described in space-time). We identify and show\nthe main advantages of each approach. Theoretically, we analyze guarantees for\nan optimal solution, possibility for asynchronous, parallel parameter update,\nand the computational complexity. We present numerical experiments for object\nrecognition. We evaluate the solution under the direct and the inverse problem\nand give a comparison with the state-of-the-art methods. Our empirical results\nhighlight the advantages of both approaches for representation learning from\nevent data. We show improvements of up to 9 % in the recognition accuracy\ncompared to the state-of-the-art methods from the same class of methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 10:40:03 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 13:09:32 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Kostadinov", "Dimche", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "2009.11050", "submitter": "Alberto Sabater", "authors": "Alberto Sabater, Luis Montesano, Ana C. Murillo", "title": "Robust and efficient post-processing for video object detection", "comments": "Submitted to the International Conference on Intelligent Robots and\n  Systems, IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition in video is an important task for plenty of applications,\nincluding autonomous driving perception, surveillance tasks, wearable devices\nor IoT networks. Object recognition using video data is more challenging than\nusing still images due to blur, occlusions or rare object poses. Specific video\ndetectors with high computational cost or standard image detectors together\nwith a fast post-processing algorithm achieve the current state-of-the-art.\nThis work introduces a novel post-processing pipeline that overcomes some of\nthe limitations of previous post-processing methods by introducing a\nlearning-based similarity evaluation between detections across frames. Our\nmethod improves the results of state-of-the-art specific video detectors,\nspecially regarding fast moving objects, and presents low resource\nrequirements. And applied to efficient still image detectors, such as YOLO,\nprovides comparable results to much more computationally intensive detectors.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 10:47:24 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Sabater", "Alberto", ""], ["Montesano", "Luis", ""], ["Murillo", "Ana C.", ""]]}, {"id": "2009.11063", "submitter": "Michel Melo Silva", "authors": "Michel Melo Silva, Washington Luis Souza Ramos, Mario Fernando\n  Montenegro Campos, Erickson Rangel Nascimento", "title": "A Sparse Sampling-based framework for Semantic Fast-Forward of\n  First-Person Videos", "comments": "Accepted at the IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) 2020. arXiv admin note: text overlap with\n  arXiv:1802.08722", "journal-ref": null, "doi": "10.1109/TPAMI.2020.2983929", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technological advances in sensors have paved the way for digital cameras to\nbecome increasingly ubiquitous, which, in turn, led to the popularity of the\nself-recording culture. As a result, the amount of visual data on the Internet\nis moving in the opposite direction of the available time and patience of the\nusers. Thus, most of the uploaded videos are doomed to be forgotten and\nunwatched stashed away in some computer folder or website. In this paper, we\naddress the problem of creating smooth fast-forward videos without losing the\nrelevant content. We present a new adaptive frame selection formulated as a\nweighted minimum reconstruction problem. Using a smoothing frame transition and\nfilling visual gaps between segments, our approach accelerates first-person\nvideos emphasizing the relevant segments and avoids visual discontinuities.\nExperiments conducted on controlled videos and also on an unconstrained dataset\nof First-Person Videos (FPVs) show that, when creating fast-forward videos, our\nmethod is able to retain as much relevant information and smoothness as the\nstate-of-the-art techniques, but in less processing time.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 18:36:17 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Silva", "Michel Melo", ""], ["Ramos", "Washington Luis Souza", ""], ["Campos", "Mario Fernando Montenegro", ""], ["Nascimento", "Erickson Rangel", ""]]}, {"id": "2009.11072", "submitter": "Jia Xue", "authors": "Jia Xue, Hang Zhang, Ko Nishino, Kristin J. Dana", "title": "Differential Viewpoints for Ground Terrain Material Recognition", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI). arXiv admin note: substantial text overlap with arXiv:1612.02372", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI) 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computational surface modeling that underlies material recognition has\ntransitioned from reflectance modeling using in-lab controlled radiometric\nmeasurements to image-based representations based on internet-mined single-view\nimages captured in the scene. We take a middle-ground approach for material\nrecognition that takes advantage of both rich radiometric cues and flexible\nimage capture. A key concept is differential angular imaging, where small\nangular variations in image capture enables angular-gradient features for an\nenhanced appearance representation that improves recognition. We build a\nlarge-scale material database, Ground Terrain in Outdoor Scenes (GTOS)\ndatabase, to support ground terrain recognition for applications such as\nautonomous driving and robot navigation. The database consists of over 30,000\nimages covering 40 classes of outdoor ground terrain under varying weather and\nlighting conditions. We develop a novel approach for material recognition\ncalled texture-encoded angular network (TEAN) that combines deep encoding\npooling of RGB information and differential angular images for angular-gradient\nfeatures to fully leverage this large dataset. With this novel network\narchitecture, we extract characteristics of materials encoded in the angular\nand spatial gradients of their appearance. Our results show that TEAN achieves\nrecognition performance that surpasses single view performance and standard\n(non-differential/large-angle sampling) multiview performance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 02:57:28 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Xue", "Jia", ""], ["Zhang", "Hang", ""], ["Nishino", "Ko", ""], ["Dana", "Kristin J.", ""]]}, {"id": "2009.11080", "submitter": "Islem Rekik", "authors": "Megi Isallari and Islem Rekik", "title": "GSR-Net: Graph Super-Resolution Network for Predicting High-Resolution\n  from Low-Resolution Functional Brain Connectomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catchy but rigorous deep learning architectures were tailored for image\nsuper-resolution (SR), however, these fail to generalize to non-Euclidean data\nsuch as brain connectomes. Specifically, building generative models for\nsuper-resolving a low-resolution (LR) brain connectome at a higher resolution\n(HR) (i.e., adding new graph nodes/edges) remains unexplored although this\nwould circumvent the need for costly data collection and manual labelling of\nanatomical brain regions (i.e. parcellation). To fill this gap, we introduce\nGSR-Net (Graph Super-Resolution Network), the first super-resolution framework\noperating on graph-structured data that generates high-resolution brain graphs\nfrom low-resolution graphs. First, we adopt a U-Net like architecture based on\ngraph convolution, pooling and unpooling operations specific to non-Euclidean\ndata. However, unlike conventional U-Nets where graph nodes represent samples\nand node features are mapped to a low-dimensional space (encoding and decoding\nnode attributes or sample features), our GSR-Net operates directly on a single\nconnectome: a fully connected graph where conventionally, a node denotes a\nbrain region, nodes have no features, and edge weights denote brain\nconnectivity strength between two regions of interest (ROIs). In the absence of\noriginal node features, we initially assign identity feature vectors to each\nbrain ROI (node) and then leverage the learned local receptive fields to learn\nnode feature representations. Second, inspired by spectral theory, we break the\nsymmetry of the U-Net architecture by topping it up with a graph\nsuper-resolution (GSR) layer and two graph convolutional network layers to\npredict a HR graph while preserving the characteristics of the LR input. Our\nproposed GSR-Net framework outperformed its variants for predicting\nhigh-resolution brain functional connectomes from low-resolution connectomes.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 12:02:55 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Isallari", "Megi", ""], ["Rekik", "Islem", ""]]}, {"id": "2009.11084", "submitter": "Donald Dansereau", "authors": "Taihua Wang and Donald G. Dansereau", "title": "Multiplexed Illumination for Classifying Visually Similar Objects", "comments": "Submitted to Computer Vision and Image Understanding (CVIU)", "journal-ref": null, "doi": "10.1364/AO.414184", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distinguishing visually similar objects like forged/authentic bills and\nhealthy/unhealthy plants is beyond the capabilities of even the most\nsophisticated classifiers. We propose the use of multiplexed illumination to\nextend the range of objects that can be successfully classified. We construct a\ncompact RGB-IR light stage that images samples under different combinations of\nilluminant position and colour. We then develop a methodology for selecting\nillumination patterns and training a classifier using the resulting imagery. We\nuse the light stage to model and synthetically relight training samples, and\npropose a greedy pattern selection scheme that exploits this ability to train\nin simulation. We then apply the trained patterns to carry out fast\nclassification of new objects. We demonstrate the approach on visually similar\nartificial and real fruit samples, showing a marked improvement compared with\nfixed-illuminant approaches as well as a more conventional code selection\nscheme. This work allows fast classification of previously indistinguishable\nobjects, with potential applications in forgery detection, quality control in\nagriculture and manufacturing, and skin lesion classification.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 12:10:06 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Wang", "Taihua", ""], ["Dansereau", "Donald G.", ""]]}, {"id": "2009.11090", "submitter": "Amirhossein Bayat", "authors": "Hanwool Park, Amirhossein Bayat, Mohammad Sabokrou, Jan S. Kirschke,\n  Bjoern H. Menze", "title": "Robustification of Segmentation Models Against Adversarial Perturbations\n  In Medical Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel yet efficient defense framework for segmentation\nmodels against adversarial attacks in medical imaging. In contrary to the\ndefense methods against adversarial attacks for classification models which\nwidely are investigated, such defense methods for segmentation models has been\nless explored. Our proposed method can be used for any deep learning models\nwithout revising the target deep learning models, as well as can be independent\nof adversarial attacks. Our framework consists of a frequency domain converter,\na detector, and a reformer. The frequency domain converter helps the detector\ndetects adversarial examples by using a frame domain of an image. The reformer\nhelps target models to predict more precisely. We have experiments to\nempirically show that our proposed method has a better performance compared to\nthe existing defense method.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 12:18:05 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Park", "Hanwool", ""], ["Bayat", "Amirhossein", ""], ["Sabokrou", "Mohammad", ""], ["Kirschke", "Jan S.", ""], ["Menze", "Bjoern H.", ""]]}, {"id": "2009.11110", "submitter": "Islem Rekik", "authors": "Ahmet Serkan Goktas, Alaa Bessadok and Islem Rekik", "title": "Residual Embedding Similarity-Based Network Selection for Predicting\n  Brain Network Evolution Trajectory from a Single Observation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While existing predictive frameworks are able to handle Euclidean structured\ndata (i.e, brain images), they might fail to generalize to geometric\nnon-Euclidean data such as brain networks. Besides, these are rooted the sample\nselection step in using Euclidean or learned similarity measure between\nvectorized training and testing brain networks. Such sample connectomic\nrepresentation might include irrelevant and redundant features that could\nmislead the training sample selection step. Undoubtedly, this fails to exploit\nand preserve the topology of the brain connectome. To overcome this major\ndrawback, we propose Residual Embedding Similarity-Based Network selection\n(RESNets) for predicting brain network evolution trajectory from a single\ntimepoint. RESNets first learns a compact geometric embedding of each training\nand testing sample using adversarial connectome embedding network. This nicely\nreduces the high-dimensionality of brain networks while preserving their\ntopological properties via graph convolutional networks. Next, to compute the\nsimilarity between subjects, we introduce the concept of a connectional brain\ntemplate (CBT), a fixed network reference, where we further represent each\ntraining and testing network as a deviation from the reference CBT in the\nembedding space. As such, we select the most similar training subjects to the\ntesting subject at baseline by comparing their learned residual embeddings with\nrespect to the pre-defined CBT. Once the best training samples are selected at\nbaseline, we simply average their corresponding brain networks at follow-up\ntimepoints to predict the evolution trajectory of the testing network. Our\nexperiments on both healthy and disordered brain networks demonstrate the\nsuccess of our proposed method in comparison to RESNets ablated versions and\ntraditional approaches.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 12:40:04 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Goktas", "Ahmet Serkan", ""], ["Bessadok", "Alaa", ""], ["Rekik", "Islem", ""]]}, {"id": "2009.11118", "submitter": "Binh Nguyen Xuan", "authors": "Tuong Do, Binh X. Nguyen, Huy Tran, Erman Tjiputra, Quang D. Tran,\n  Thanh-Toan Do", "title": "Multiple interaction learning with question-type prior knowledge for\n  constraining answer search space in visual question answering", "comments": "Accepted in ECCV Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different approaches have been proposed to Visual Question Answering (VQA).\nHowever, few works are aware of the behaviors of varying joint modality methods\nover question type prior knowledge extracted from data in constraining answer\nsearch space, of which information gives a reliable cue to reason about answers\nfor questions asked in input images. In this paper, we propose a novel VQA\nmodel that utilizes the question-type prior information to improve VQA by\nleveraging the multiple interactions between different joint modality methods\nbased on their behaviors in answering questions from different types. The solid\nexperiments on two benchmark datasets, i.e., VQA 2.0 and TDIUC, indicate that\nthe proposed method yields the best performance with the most competitive\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 12:54:34 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Do", "Tuong", ""], ["Nguyen", "Binh X.", ""], ["Tran", "Huy", ""], ["Tjiputra", "Erman", ""], ["Tran", "Quang D.", ""], ["Do", "Thanh-Toan", ""]]}, {"id": "2009.11120", "submitter": "Anneke Meyer", "authors": "Anneke Meyer, Grzegorz Chlebus, Marko Rak, Daniel Schindele, Martin\n  Schostak, Bram van Ginneken, Andrea Schenk, Hans Meine, Horst K. Hahn,\n  Andreas Schreiber, Christian Hansen", "title": "Anisotropic 3D Multi-Stream CNN for Accurate Prostate Segmentation from\n  Multi-Planar MRI", "comments": "Accepted manuscript in Elsevier Computer Methods and Programs in\n  Biomedicine. Anneke Meyer and Grzegorz Chlebus contributed equally to this\n  work. Sourcecode available at\n  https://github.com/AnnekeMeyer/AnisotropicMultiStreamCNN. Data available at\n  https://doi.org/10.7937/TCIA.2019.DEG7ZG1U", "journal-ref": null, "doi": "10.1016/j.cmpb.2020.105821", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background and Objective: Accurate and reliable segmentation of the prostate\ngland in MR images can support the clinical assessment of prostate cancer, as\nwell as the planning and monitoring of focal and loco-regional therapeutic\ninterventions. Despite the availability of multi-planar MR scans due to\nstandardized protocols, the majority of segmentation approaches presented in\nthe literature consider the axial scans only. Methods: We propose an\nanisotropic 3D multi-stream CNN architecture, which processes additional scan\ndirections to produce a higher-resolution isotropic prostate segmentation. We\ninvestigate two variants of our architecture, which work on two (dual-plane)\nand three (triple-plane) image orientations, respectively. We compare them with\nthe standard baseline (single-plane) used in literature, i.e., plain axial\nsegmentation. To realize a fair comparison, we employ a hyperparameter\noptimization strategy to select optimal configurations for the individual\napproaches. Results: Training and evaluation on two datasets spanning multiple\nsites obtain statistical significant improvement over the plain axial\nsegmentation ($p<0.05$ on the Dice similarity coefficient). The improvement can\nbe observed especially at the base ($0.898$ single-plane vs. $0.906$\ntriple-plane) and apex ($0.888$ single-plane vs. $0.901$ dual-plane).\nConclusion: This study indicates that models employing two or three scan\ndirections are superior to plain axial segmentation. The knowledge of precise\nboundaries of the prostate is crucial for the conservation of risk structures.\nThus, the proposed models have the potential to improve the outcome of prostate\ncancer diagnosis and therapies.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 12:56:14 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 13:01:03 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Meyer", "Anneke", ""], ["Chlebus", "Grzegorz", ""], ["Rak", "Marko", ""], ["Schindele", "Daniel", ""], ["Schostak", "Martin", ""], ["van Ginneken", "Bram", ""], ["Schenk", "Andrea", ""], ["Meine", "Hans", ""], ["Hahn", "Horst K.", ""], ["Schreiber", "Andreas", ""], ["Hansen", "Christian", ""]]}, {"id": "2009.11150", "submitter": "Jihun Yi", "authors": "Jihun Yi, Eunji Kim, Siwon Kim, Sungroh Yoon", "title": "Information-Theoretic Visual Explanation for Black-Box Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we attempt to explain the prediction of any black-box\nclassifier from an information-theoretic perspective. For each input feature,\nwe compare the classifier outputs with and without that feature using two\ninformation-theoretic metrics. Accordingly, we obtain two attribution maps--an\ninformation gain (IG) map and a point-wise mutual information (PMI) map. IG map\nprovides a class-independent answer to \"How informative is each pixel?\", and\nPMI map offers a class-specific explanation of \"How much does each pixel\nsupport a specific class?\" Compared to existing methods, our method improves\nthe correctness of the attribution maps in terms of a quantitative metric. We\nalso provide a detailed analysis of an ImageNet classifier using the proposed\nmethod, and the code is available online.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 13:51:16 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 07:40:24 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Yi", "Jihun", ""], ["Kim", "Eunji", ""], ["Kim", "Siwon", ""], ["Yoon", "Sungroh", ""]]}, {"id": "2009.11154", "submitter": "Albert Mosella-Montoro", "authors": "Albert Mosella-Montoro, Javier Ruiz-Hidalgo", "title": "2D-3D Geometric Fusion Network using Multi-Neighbourhood Graph\n  Convolution for RGB-D Indoor Scene Classification", "comments": "Information Fusion 2021", "journal-ref": null, "doi": "10.1016/j.inffus.2021.05.002", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multi-modal fusion has been proved to help enhance the performance of scene\nclassification tasks. This paper presents a 2D-3D Fusion stage that combines 3D\nGeometric Features with 2D Texture Features obtained by 2D Convolutional Neural\nNetworks. To get a robust 3D Geometric embedding, a network that uses two novel\nlayers is proposed. The first layer, Multi-Neighbourhood Graph Convolution,\naims to learn a more robust geometric descriptor of the scene combining two\ndifferent neighbourhoods: one in the Euclidean space and the other in the\nFeature space. The second proposed layer, Nearest Voxel Pooling, improves the\nperformance of the well-known Voxel Pooling. Experimental results, using\nNYU-Depth-V2 and SUN RGB-D datasets, show that the proposed method outperforms\nthe current state-of-the-art in RGB-D indoor scene classification task.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 13:58:12 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 16:27:55 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 10:06:33 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Mosella-Montoro", "Albert", ""], ["Ruiz-Hidalgo", "Javier", ""]]}, {"id": "2009.11160", "submitter": "Junichiro Iwasawa", "authors": "Junichiro Iwasawa, Yuichiro Hirano and Yohei Sugawara", "title": "Label-Efficient Multi-Task Segmentation using Contrastive Learning", "comments": "Accepted to MICCAI BrainLes 2020 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.TO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Obtaining annotations for 3D medical images is expensive and time-consuming,\ndespite its importance for automating segmentation tasks. Although multi-task\nlearning is considered an effective method for training segmentation models\nusing small amounts of annotated data, a systematic understanding of various\nsubtasks is still lacking. In this study, we propose a multi-task segmentation\nmodel with a contrastive learning based subtask and compare its performance\nwith other multi-task models, varying the number of labeled data for training.\nWe further extend our model so that it can utilize unlabeled data through the\nregularization branch in a semi-supervised manner. We experimentally show that\nour proposed method outperforms other multi-task methods including the\nstate-of-the-art fully supervised model when the amount of annotated data is\nlimited.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 14:12:17 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Iwasawa", "Junichiro", ""], ["Hirano", "Yuichiro", ""], ["Sugawara", "Yohei", ""]]}, {"id": "2009.11166", "submitter": "Islem Rekik", "authors": "Zeynep Gurler, Ahmed Nebli and Islem Rekik", "title": "Foreseeing Brain Graph Evolution Over Time Using Deep Adversarial\n  Network Normalizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Foreseeing the brain evolution as a complex highly inter-connected system,\nwidely modeled as a graph, is crucial for mapping dynamic interactions between\ndifferent anatomical regions of interest (ROIs) in health and disease.\nInterestingly, brain graph evolution models remain almost absent in the\nliterature. Here we design an adversarial brain network normalizer for\nrepresenting each brain network as a transformation of a fixed centered\npopulation-driven connectional template. Such graph normalization with respect\nto a fixed reference paves the way for reliably identifying the most similar\ntraining samples (i.e., brain graphs) to the testing sample at baseline\ntimepoint. The testing evolution trajectory will be then spanned by the\nselected training graphs and their corresponding evolution trajectories. We\nbase our prediction framework on geometric deep learning which naturally\noperates on graphs and nicely preserves their topological properties.\nSpecifically, we propose the first graph-based Generative Adversarial Network\n(gGAN) that not only learns how to normalize brain graphs with respect to a\nfixed connectional brain template (CBT) (i.e., a brain template that\nselectively captures the most common features across a brain population) but\nalso learns a high-order representation of the brain graphs also called\nembeddings. We use these embeddings to compute the similarity between training\nand testing subjects which allows us to pick the closest training subjects at\nbaseline timepoint to predict the evolution of the testing brain graph over\ntime. A series of benchmarks against several comparison methods showed that our\nproposed method achieved the lowest brain disease evolution prediction error\nusing a single baseline timepoint. Our gGAN code is available at\nhttp://github.com/basiralab/gGAN.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 14:25:40 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Gurler", "Zeynep", ""], ["Nebli", "Ahmed", ""], ["Rekik", "Islem", ""]]}, {"id": "2009.11169", "submitter": "Jiawen Yao", "authors": "Jiawen Yao, Xinliang Zhu, Jitendra Jonnagaddala, Nicholas Hawkins,\n  Junzhou Huang", "title": "Whole Slide Images based Cancer Survival Prediction using Attention\n  Guided Deep Multiple Instance Learning Networks", "comments": "22 pages, 13 figures, published in Medical Image Analysis 65, 101789", "journal-ref": "Medical Image Analysis, Volume 65, October 2020, 101789", "doi": "10.1016/j.media.2020.101789", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional image-based survival prediction models rely on discriminative\npatch labeling which make those methods not scalable to extend to large\ndatasets. Recent studies have shown Multiple Instance Learning (MIL) framework\nis useful for histopathological images when no annotations are available in\nclassification task. Different to the current image-based survival models that\nlimit to key patches or clusters derived from Whole Slide Images (WSIs), we\npropose Deep Attention Multiple Instance Survival Learning (DeepAttnMISL) by\nintroducing both siamese MI-FCN and attention-based MIL pooling to efficiently\nlearn imaging features from the WSI and then aggregate WSI-level information to\npatient-level. Attention-based aggregation is more flexible and adaptive than\naggregation techniques in recent survival models. We evaluated our methods on\ntwo large cancer whole slide images datasets and our results suggest that the\nproposed approach is more effective and suitable for large datasets and has\nbetter interpretability in locating important patterns and features that\ncontribute to accurate cancer survival predictions. The proposed framework can\nalso be used to assess individual patient's risk and thus assisting in\ndelivering personalized medicine. Codes are available at\nhttps://github.com/uta-smile/DeepAttnMISL_MEDIA.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 14:31:15 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Yao", "Jiawen", ""], ["Zhu", "Xinliang", ""], ["Jonnagaddala", "Jitendra", ""], ["Hawkins", "Nicholas", ""], ["Huang", "Junzhou", ""]]}, {"id": "2009.11204", "submitter": "Radu Horaud P", "authors": "Sylvain Guy, St\\'ephane Lathuili\\`ere, Pablo Mesejo and Radu Horaud", "title": "Learning Visual Voice Activity Detection with an Automatically Annotated\n  Dataset", "comments": "International Conference on Pattern Recognition, Milan, Italy,\n  January 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual voice activity detection (V-VAD) uses visual features to predict\nwhether a person is speaking or not. V-VAD is useful whenever audio VAD (A-VAD)\nis inefficient either because the acoustic signal is difficult to analyze or\nbecause it is simply missing. We propose two deep architectures for V-VAD, one\nbased on facial landmarks and one based on optical flow. Moreover, available\ndatasets, used for learning and for testing V-VAD, lack content variability. We\nintroduce a novel methodology to automatically create and annotate very large\ndatasets in-the-wild -- WildVVAD -- based on combining A-VAD with face\ndetection and tracking. A thorough empirical evaluation shows the advantage of\ntraining the proposed deep V-VAD models with this dataset.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 15:12:24 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 15:08:12 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Guy", "Sylvain", ""], ["Lathuili\u00e8re", "St\u00e9phane", ""], ["Mesejo", "Pablo", ""], ["Horaud", "Radu", ""]]}, {"id": "2009.11232", "submitter": "Binjie Zhang", "authors": "Binjie Zhang, Yu Li, Chun Yuan, Dejing Xu, Pin Jiang, Ying Shan", "title": "A Simple Yet Effective Method for Video Temporal Grounding with\n  Cross-Modality Attention", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of language-guided video temporal grounding is to localize the\nparticular video clip corresponding to a query sentence in an untrimmed video.\nThough progress has been made continuously in this field, some issues still\nneed to be resolved. First, most of the existing methods rely on the\ncombination of multiple complicated modules to solve the task. Second, due to\nthe semantic gaps between the two different modalities, aligning the\ninformation at different granularities (local and global) between the video and\nthe language is significant, which is less addressed. Last, previous works do\nnot consider the inevitable annotation bias due to the ambiguities of action\nboundaries. To address these limitations, we propose a simple two-branch\nCross-Modality Attention (CMA) module with intuitive structure design, which\nalternatively modulates two modalities for better matching the information both\nlocally and globally. In addition, we introduce a new task-specific regression\nloss function, which improves the temporal grounding accuracy by alleviating\nthe impact of annotation bias. We conduct extensive experiments to validate our\nmethod, and the results show that just with this simple model, it can\noutperform the state of the arts on both Charades-STA and ActivityNet Captions\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 16:03:00 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Zhang", "Binjie", ""], ["Li", "Yu", ""], ["Yuan", "Chun", ""], ["Xu", "Dejing", ""], ["Jiang", "Pin", ""], ["Shan", "Ying", ""]]}, {"id": "2009.11250", "submitter": "Gaston Lenczner", "authors": "Gaston Lenczner, Adrien Chan-Hon-Tong, Nicola Luminari, Bertrand Le\n  Saux, Guy Le Besnerais", "title": "Interactive Learning for Semantic Segmentation in Earth Observation", "comments": "8 pages, 4 Figures, ECML-PKDD Workshop MACLEAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense pixel-wise classification maps output by deep neural networks are of\nextreme importance for scene understanding. However, these maps are often\npartially inaccurate due to a variety of possible factors. Therefore, we\npropose to interactively refine them within a framework named DISCA (Deep Image\nSegmentation with Continual Adaptation). It consists of continually adapting a\nneural network to a target image using an interactive learning process with\nsparse user annotations as ground-truth. We show through experiments on three\ndatasets using synthesized annotations the benefits of the approach, reaching\nan IoU improvement up to 4.7% for ten sampled clicks. Finally, we exhibit that\nour approach can be particularly rewarding when it is faced to additional\nissues such as domain adaptation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 16:57:28 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Lenczner", "Gaston", ""], ["Chan-Hon-Tong", "Adrien", ""], ["Luminari", "Nicola", ""], ["Saux", "Bertrand Le", ""], ["Besnerais", "Guy Le", ""]]}, {"id": "2009.11253", "submitter": "Henry Kvinge", "authors": "Henry Kvinge, Zachary New, Nico Courts, Jung H. Lee, Lauren A.\n  Phillips, Courtney D. Corley, Aaron Tuor, Andrew Avila, Nathan O. Hodas", "title": "Fuzzy Simplicial Networks: A Topology-Inspired Model to Improve Task\n  Generalization in Few-shot Learning", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV math.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown great success in settings with massive amounts of\ndata but has struggled when data is limited. Few-shot learning algorithms,\nwhich seek to address this limitation, are designed to generalize well to new\ntasks with limited data. Typically, models are evaluated on unseen classes and\ndatasets that are defined by the same fundamental task as they are trained for\n(e.g. category membership). One can also ask how well a model can generalize to\nfundamentally different tasks within a fixed dataset (for example: moving from\ncategory membership to tasks that involve detecting object orientation or\nquantity). To formalize this kind of shift we define a notion of \"independence\nof tasks\" and identify three new sets of labels for established computer vision\ndatasets that test a model's ability to generalize to tasks which draw on\northogonal attributes in the data. We use these datasets to investigate the\nfailure modes of metric-based few-shot models. Based on our findings, we\nintroduce a new few-shot model called Fuzzy Simplicial Networks (FSN) which\nleverages a construction from topology to more flexibly represent each class\nfrom limited data. In particular, FSN models can not only form multiple\nrepresentations for a given class but can also begin to capture the\nlow-dimensional structure which characterizes class manifolds in the encoded\nspace of deep networks. We show that FSN outperforms state-of-the-art models on\nthe challenging tasks we introduce in this paper while remaining competitive on\nstandard few-shot benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 17:01:09 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Kvinge", "Henry", ""], ["New", "Zachary", ""], ["Courts", "Nico", ""], ["Lee", "Jung H.", ""], ["Phillips", "Lauren A.", ""], ["Corley", "Courtney D.", ""], ["Tuor", "Aaron", ""], ["Avila", "Andrew", ""], ["Hodas", "Nathan O.", ""]]}, {"id": "2009.11262", "submitter": "Matthew Thorpe", "authors": "Oliver M. Crook, Mihai Cucuringu, Tim Hurst, Carola-Bibiane\n  Sch\\\"onlieb, Matthew Thorpe and Konstantinos C. Zygalakis", "title": "A Linear Transportation $\\mathrm{L}^p$ Distance for Pattern Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transportation $\\mathrm{L}^p$ distance, denoted $\\mathrm{TL}^p$, has been\nproposed as a generalisation of Wasserstein $\\mathrm{W}^p$ distances motivated\nby the property that it can be applied directly to colour or multi-channelled\nimages, as well as multivariate time-series without normalisation or mass\nconstraints. These distances, as with $\\mathrm{W}^p$, are powerful tools in\nmodelling data with spatial or temporal perturbations. However, their\ncomputational cost can make them infeasible to apply to even moderate pattern\nrecognition tasks. We propose linear versions of these distances and show that\nthe linear $\\mathrm{TL}^p$ distance significantly improves over the linear\n$\\mathrm{W}^p$ distance on signal processing tasks, whilst being several orders\nof magnitude faster to compute than the $\\mathrm{TL}^p$ distance.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 17:19:19 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Crook", "Oliver M.", ""], ["Cucuringu", "Mihai", ""], ["Hurst", "Tim", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""], ["Thorpe", "Matthew", ""], ["Zygalakis", "Konstantinos C.", ""]]}, {"id": "2009.11278", "submitter": "Jaemin Cho", "authors": "Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, Aniruddha\n  Kembhavi", "title": "X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal\n  Transformers", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mirroring the success of masked language models, vision-and-language\ncounterparts like ViLBERT, LXMERT and UNITER have achieved state of the art\nperformance on a variety of multimodal discriminative tasks like visual\nquestion answering and visual grounding. Recent work has also successfully\nadapted such models towards the generative task of image captioning. This begs\nthe question: Can these models go the other way and generate images from pieces\nof text? Our analysis of a popular representative from this model family -\nLXMERT - finds that it is unable to generate rich and semantically meaningful\nimagery with its current training setup. We introduce X-LXMERT, an extension to\nLXMERT with training refinements including: discretizing visual\nrepresentations, using uniform masking with a large range of masking ratios and\naligning the right pre-training datasets to the right objectives which enables\nit to paint. X-LXMERT's image generation capabilities rival state of the art\ngenerative models while its question answering and captioning abilities remains\ncomparable to LXMERT. Finally, we demonstrate the generality of these training\nrefinements by adding image generation capabilities into UNITER to produce\nX-UNITER.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 17:45:17 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Cho", "Jaemin", ""], ["Lu", "Jiasen", ""], ["Schwenk", "Dustin", ""], ["Hajishirzi", "Hannaneh", ""], ["Kembhavi", "Aniruddha", ""]]}, {"id": "2009.11279", "submitter": "Udit Bhatia", "authors": "Nidhin Harilal, Udit Bhatia, Mayank Singh", "title": "Augmented Convolutional LSTMs for Generation of High-Resolution Climate\n  Change Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projection of changes in extreme indices of climate variables such as\ntemperature and precipitation are critical to assess the potential impacts of\nclimate change on human-made and natural systems, including critical\ninfrastructures and ecosystems. While impact assessment and adaptation planning\nrely on high-resolution projections (typically in the order of a few\nkilometers), state-of-the-art Earth System Models (ESMs) are available at\nspatial resolutions of few hundreds of kilometers. Current solutions to obtain\nhigh-resolution projections of ESMs include downscaling approaches that\nconsider the information at a coarse-scale to make predictions at local scales.\nComplex and non-linear interdependence among local climate variables (e.g.,\ntemperature and precipitation) and large-scale predictors (e.g., pressure\nfields) motivate the use of neural network-based super-resolution\narchitectures. In this work, we present auxiliary variables informed\nspatio-temporal neural architecture for statistical downscaling. The current\nstudy performs daily downscaling of precipitation variable from an ESM output\nat 1.15 degrees (~115 km) to 0.25 degrees (25 km) over the world's most\nclimatically diversified country, India. We showcase significant improvement\ngain against three popular state-of-the-art baselines with a better ability to\npredict extreme events. To facilitate reproducible research, we make available\nall the codes, processed datasets, and trained models in the public domain.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 17:52:09 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Harilal", "Nidhin", ""], ["Bhatia", "Udit", ""], ["Singh", "Mayank", ""]]}, {"id": "2009.11327", "submitter": "Emmanuel Iarussi", "authors": "Emmanuel Iarussi, Felix Thomsen and Claudio Delrieux", "title": "Generative Modelling of 3D in-silico Spongiosa with Controllable\n  Micro-Structural Parameters", "comments": "Accepted for publication in MICCAI 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in vertebral bone micro-structure generally requires costly\nprocedures to obtain physical scans of real bone with a specific pathology\nunder study, since no methods are available yet to generate realistic bone\nstructures in-silico. Here we propose to apply recent advances in generative\nadversarial networks (GANs) to develop such a method. We adapted style-transfer\ntechniques, which have been largely used in other contexts, in order to\ntransfer style between image pairs while preserving its informational content.\nIn a first step, we trained a volumetric generative model in a progressive\nmanner using a Wasserstein objective and gradient penalty (PWGAN-GP) to create\npatches of realistic bone structure in-silico. The training set contained 7660\npurely spongeous bone samples from twelve human vertebrae (T12 or L1) with\nisotropic resolution of 164um and scanned with a high resolution peripheral\nquantitative CT (Scanco XCT). After training, we generated new samples with\ntailored micro-structure properties by optimizing a vector z in the learned\nlatent space. To solve this optimization problem, we formulated a\ndifferentiable goal function that leads to valid samples while compromising the\nappearance (content) with target 3D properties (style). Properties of the\nlearned latent space effectively matched the data distribution. Furthermore, we\nwere able to simulate the resulting bone structure after deterioration or\ntreatment effects of osteoporosis therapies based only on expected changes of\nmicro-structural parameters. Our method allows to generate a virtually infinite\nnumber of patches of realistic bone micro-structure, and thereby likely serves\nfor the development of bone-biomarkers and to simulate bone therapies in\nadvance.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 18:11:47 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Iarussi", "Emmanuel", ""], ["Thomsen", "Felix", ""], ["Delrieux", "Claudio", ""]]}, {"id": "2009.11342", "submitter": "Amir Shalev", "authors": "Amir Shalev (1,2), Omer Achrack (2), Brian Fulkerson, and Ben-Zion\n  Bobrovsky (1) ((1) Tel-Aviv-University, (2) Intel)", "title": "Insights on Evaluation of Camera Re-localization Using Relative Pose\n  Regression", "comments": "Accepted at ECCV 2020 joint workshop of UAVision and VisDrone", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of relative pose regression in visual relocalization.\nRecently, several promising approaches have emerged in this area. We claim that\neven though they demonstrate on the same datasets using the same split to train\nand test, a faithful comparison between them was not available since on\ncurrently used evaluation metric, some approaches might perform favorably,\nwhile in reality performing worse. We reveal a tradeoff between accuracy and\nthe 3D volume of the regressed subspace. We believe that unlike other\nrelocalization approaches, in the case of relative pose regression, the\nregressed subspace 3D volume is less dependent on the scene and more affect by\nthe method used to score the overlap, which determined how closely sampled\nviewpoints are. We propose three new metrics to remedy the issue mentioned\nabove. The proposed metrics incorporate statistics about the regression\nsubspace volume. We also propose a new pose regression network that serves as a\nnew baseline for this task. We compare the performance of our trained model on\nMicrosoft 7-Scenes and Cambridge Landmarks datasets both with the standard\nmetrics and the newly proposed metrics and adjust the overlap score to reveal\nthe tradeoff between the subspace and performance. The results show that the\nproposed metrics are more robust to different overlap threshold than the\nconventional approaches. Finally, we show that our network generalizes well,\nspecifically, training on a single scene leads to little loss of performance on\nthe other scenes.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 19:16:26 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Shalev", "Amir", "", "Tel-Aviv-University", "Intel"], ["Achrack", "Omer", "", "Intel"], ["Fulkerson", "Brian", "", "Tel-Aviv-University"], ["Bobrovsky", "Ben-Zion", "", "Tel-Aviv-University"]]}, {"id": "2009.11362", "submitter": "Renhao Wang", "authors": "Renhao Wang, Ashutosh Bhudia, Brandon Dos Remedios, Minnie Teng,\n  Raymond Ng", "title": "Dense Forecasting of Wildfire Smoke Particulate Matter Using Sparsity\n  Invariant Convolutional Neural Networks", "comments": "Submitted to the 2020 NeurIPS Workshop on Machine learning in Public\n  Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate forecasts of fine particulate matter (PM 2.5) from wildfire smoke\nare crucial to safeguarding cardiopulmonary public health. Existing forecasting\nsystems are trained on sparse and inaccurate ground truths, and do not take\nsufficient advantage of important spatial inductive biases. In this work, we\npresent a convolutional neural network which preserves sparsity invariance\nthroughout, and leverages multitask learning to perform dense forecasts of PM\n2.5values. We demonstrate that our model outperforms two existing smoke\nforecasting systems during the 2018 and 2019 wildfire season in British\nColumbia, Canada, predicting PM 2.5 at a grid resolution of 10 km, 24 hours in\nadvance with high fidelity. Most interestingly, our model also generalizes to\nmeaningful smoke dispersion patterns despite training with irregularly\ndistributed ground truth PM 2.5 values available in only 0.5% of grid cells.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 20:13:35 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Wang", "Renhao", ""], ["Bhudia", "Ashutosh", ""], ["Remedios", "Brandon Dos", ""], ["Teng", "Minnie", ""], ["Ng", "Raymond", ""]]}, {"id": "2009.11397", "submitter": "Matthias Rottmann", "authors": "Matthias Rottmann, Kira Maag, Mathis Peyron, Natasa Krejic and Hanno\n  Gottschalk", "title": "Detection of Iterative Adversarial Attacks via Counter Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have proven to be powerful tools for processing\nunstructured data. However for high-dimensional data, like images, they are\ninherently vulnerable to adversarial attacks. Small almost invisible\nperturbations added to the input can be used to fool DNNs. Various attacks,\nhardening methods and detection methods have been introduced in recent years.\nNotoriously, Carlini-Wagner (CW) type attacks computed by iterative\nminimization belong to those that are most difficult to detect. In this work we\noutline a mathematical proof that the CW attack can be used as a detector\nitself. That is, under certain assumptions and in the limit of attack\niterations this detector provides asymptotically optimal separation of original\nand attacked images. In numerical experiments, we experimentally validate this\nstatement and furthermore obtain AUROC values up to 99.73% on CIFAR10 and\nImageNet. This is in the upper part of the spectrum of current state-of-the-art\ndetection rates for CW attacks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 21:54:36 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 14:21:02 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Rottmann", "Matthias", ""], ["Maag", "Kira", ""], ["Peyron", "Mathis", ""], ["Krejic", "Natasa", ""], ["Gottschalk", "Hanno", ""]]}, {"id": "2009.11425", "submitter": "Shoudong Han", "authors": "Donghaisheng Liu, Shoudong Han, Yang Chen, Chenfei Xia, Jun Zhao", "title": "FTN: Foreground-Guided Texture-Focused Person Re-Identification", "comments": "9 pages,5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (Re-ID) is a challenging task as persons are often\nin different backgrounds. Most recent Re-ID methods treat the foreground and\nbackground information equally for person discriminative learning, but can\neasily lead to potential false alarm problems when different persons are in\nsimilar backgrounds or the same person is in different backgrounds. In this\npaper, we propose a Foreground-Guided Texture-Focused Network (FTN) for Re-ID,\nwhich can weaken the representation of unrelated background and highlight the\nattributes person-related in an end-to-end manner. FTN consists of a semantic\nencoder (S-Enc) and a compact foreground attention module (CFA) for Re-ID task,\nand a texture-focused decoder (TF-Dec) for reconstruction task. Particularly,\nwe build a foreground-guided semi-supervised learning strategy for TF-Dec\nbecause the reconstructed ground-truths are only the inputs of FTN weighted by\nthe Gaussian mask and the attention mask generated by CFA. Moreover, a new\ngradient loss is introduced to encourage the network to mine the texture\nconsistency between the inputs and the reconstructed outputs. Our FTN is\ncomputationally efficient and extensive experiments on three commonly used\ndatasets Market1501, CUHK03 and MSMT17 demonstrate that the proposed method\nperforms favorably against the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 00:44:05 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Liu", "Donghaisheng", ""], ["Han", "Shoudong", ""], ["Chen", "Yang", ""], ["Xia", "Chenfei", ""], ["Zhao", "Jun", ""]]}, {"id": "2009.11429", "submitter": "Xiaokang Liu", "authors": "Xiaokang Liu, Haijun Song", "title": "Automatic identification of fossils and abiotic grains during carbonate\n  microfacies analysis using deep convolutional neural networks", "comments": "This preprint has been accepted by Sedimentary Geology", "journal-ref": null, "doi": "10.1016/j.sedgeo.2020.105790", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Petrographic analysis based on microfacies identification in thin sections is\nwidely used in sedimentary environment interpretation and paleoecological\nreconstruction. Fossil recognition from microfacies is an essential procedure\nfor petrographers to complete this task. Distinguishing the morphological and\nmicrostructural diversity of skeletal fragments requires extensive prior\nknowledge of fossil morphotypes in microfacies and long training sessions under\nthe microscope. This requirement engenders certain challenges for\nsedimentologists and paleontologists, especially novices. However, a machine\nclassifier can help address this challenge. In this study, we collected a\nmicrofacies image dataset comprising both public data from 1,149 references and\nour own materials (including 30,815 images of 22 fossil and abiotic grain\ngroups). We employed a high-performance workstation to implement four classic\ndeep convolutional neural networks (DCNNs), which have proven to be highly\nefficient in computer vision over the last several years. Our framework uses a\ntransfer learning technique, which reuses the pre-trained parameters that are\ntrained on a larger ImageNet dataset as initialization for the network to\nachieve high accuracy with low computing costs. We obtained up to 95% of the\ntop one and 99% of the top three test accuracies in the Inception ResNet v2\narchitecture. The machine classifier exhibited 0.99 precision on minerals, such\nas dolomite and pyrite. Although it had some difficulty on samples having\nsimilar morphologies, such as the bivalve, brachiopod, and ostracod, it\nnevertheless obtained 0.88 precision. Our machine learning framework\ndemonstrated high accuracy with reproducibility and bias avoidance that was\ncomparable to those of human classifiers. Its application can thus eliminate\nmuch of the tedious, manually intensive efforts by human experts conducting\nroutine identification.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 00:58:48 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 02:04:02 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Liu", "Xiaokang", ""], ["Song", "Haijun", ""]]}, {"id": "2009.11433", "submitter": "Sayali Kulkarni", "authors": "Sayali Kulkarni, Tomer Gadot, Chen Luo, Tanya Birch, Eric Fegraus", "title": "Unifying data for fine-grained visual species classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Wildlife monitoring is crucial to nature conservation and has been done by\nmanual observations from motion-triggered camera traps deployed in the field.\nWidespread adoption of such in-situ sensors has resulted in unprecedented data\nvolumes being collected over the last decade. A significant challenge exists to\nprocess and reliably identify what is in these images efficiently. Advances in\ncomputer vision are poised to provide effective solutions with custom AI models\nbuilt to automatically identify images of interest and label the species in\nthem. Here we outline the data unification effort for the Wildlife Insights\nplatform from various conservation partners, and the challenges involved. Then\nwe present an initial deep convolutional neural network model, trained on 2.9M\nimages across 465 fine-grained species, with a goal to reduce the load on human\nexperts to classify species in images manually. The long-term goal is to enable\nscientists to make conservation recommendations from near real-time analysis of\nspecies abundance and population health.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 01:04:18 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Kulkarni", "Sayali", ""], ["Gadot", "Tomer", ""], ["Luo", "Chen", ""], ["Birch", "Tanya", ""], ["Fegraus", "Eric", ""]]}, {"id": "2009.11446", "submitter": "Muhammad Usman", "authors": "Taha Hasan Masood Siddique and Muhammad Usman", "title": "3D Object Localization Using 2D Estimates for Computer Vision\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A technique for object localization based on pose estimation and camera\ncalibration is presented. The 3-dimensional (3D) coordinates are estimated by\ncollecting multiple 2-dimensional (2D) images of the object and are utilized\nfor the calibration of the camera. The calibration steps involving a number of\nparameter calculation including intrinsic and extrinsic parameters for the\nremoval of lens distortion, computation of object's size and camera's position\ncalculation are discussed. A transformation strategy to estimate the 3D pose\nusing the 2D images is presented. The proposed method is implemented on MATLAB\nand validation experiments are carried out for both pose estimation and camera\ncalibration.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 01:50:24 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Siddique", "Taha Hasan Masood", ""], ["Usman", "Muhammad", ""]]}, {"id": "2009.11458", "submitter": "Ali Almadan", "authors": "Ali Almadan, Anoop Krishnan, Ajita Rattani", "title": "BWCFace: Open-set Face Recognition using Body-worn Camera", "comments": null, "journal-ref": "19th IEEE International Conference On Machine Learning And\n  Applications 2020 | Miami, Florida", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With computer vision reaching an inflection point in the past decade, face\nrecognition technology has become pervasive in policing, intelligence\ngathering, and consumer applications. Recently, face recognition technology has\nbeen deployed on bodyworn cameras to keep officers safe, enabling situational\nawareness and providing evidence for trial. However, limited academic research\nhas been conducted on this topic using traditional techniques on datasets with\nsmall sample size. This paper aims to bridge the gap in the state-of-the-art\nface recognition using bodyworn cameras (BWC). To this aim, the contribution of\nthis work is two-fold: (1) collection of a dataset called BWCFace consisting of\na total of 178K facial images of 132 subjects captured using the body-worn\ncamera in in-door and daylight conditions, and (2) open-set evaluation of the\nlatest deep-learning-based Convolutional Neural Network (CNN) architectures\ncombined with five different loss functions for face identification, on the\ncollected dataset. Experimental results on our BWCFace dataset suggest a\nmaximum of 33.89% Rank-1 accuracy obtained when facial features are extracted\nusing SENet-50 trained on a large scale VGGFace2 facial image dataset. However,\nperformance improved up to a maximum of 99.00% Rank-1 accuracy when pretrained\nCNN models are fine-tuned on a subset of identities in our BWCFace dataset.\nEquivalent performances were obtained across body-worn camera sensor models\nused in existing face datasets. The collected BWCFace dataset and the\npretrained/ fine-tuned algorithms are publicly available to promote further\nresearch and development in this area. A downloadable link of this dataset and\nthe algorithms is available by contacting the authors.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 02:45:29 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Almadan", "Ali", ""], ["Krishnan", "Anoop", ""], ["Rattani", "Ajita", ""]]}, {"id": "2009.11491", "submitter": "Anoop Krishnan Upendran Nair", "authors": "Anoop Krishnan, Ali Almadan, Ajita Rattani", "title": "Understanding Fairness of Gender Classification Algorithms Across\n  Gender-Race Groups", "comments": "19th IEEE International Conference On Machine Learning And\n  Applications 2020 | Miami, Florida", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated gender classification has important applications in many domains,\nsuch as demographic research, law enforcement, online advertising, as well as\nhuman-computer interaction. Recent research has questioned the fairness of this\ntechnology across gender and race. Specifically, the majority of the studies\nraised the concern of higher error rates of the face-based gender\nclassification system for darker-skinned people like African-American and for\nwomen. However, to date, the majority of existing studies were limited to\nAfrican-American and Caucasian only. The aim of this paper is to investigate\nthe differential performance of the gender classification algorithms across\ngender-race groups. To this aim, we investigate the impact of (a) architectural\ndifferences in the deep learning algorithms and (b) training set imbalance, as\na potential source of bias causing differential performance across gender and\nrace. Experimental investigations are conducted on two latest large-scale\npublicly available facial attribute datasets, namely, UTKFace and FairFace. The\nexperimental results suggested that the algorithms with architectural\ndifferences varied in performance with consistency towards specific gender-race\ngroups. For instance, for all the algorithms used, Black females (Black race in\ngeneral) always obtained the least accuracy rates. Middle Eastern males and\nLatino females obtained higher accuracy rates most of the time. Training set\nimbalance further widens the gap in the unequal accuracy rates across all\ngender-race groups. Further investigations using facial landmarks suggested\nthat facial morphological differences due to the bone structure influenced by\ngenetic and environmental factors could be the cause of the least performance\nof Black females and Black race, in general.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 04:56:10 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Krishnan", "Anoop", ""], ["Almadan", "Ali", ""], ["Rattani", "Ajita", ""]]}, {"id": "2009.11524", "submitter": "Islem Rekik", "authors": "Ahmed Nebli and Islem Rekik", "title": "Adversarial Brain Multiplex Prediction From a Single Network for\n  High-Order Connectional Gender-Specific Brain Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain connectivity networks, derived from magnetic resonance imaging (MRI),\nnon-invasively quantify the relationship in function, structure, and morphology\nbetween two brain regions of interest (ROIs) and give insights into\ngender-related connectional differences. However, to the best of our knowledge,\nstudies on gender differences in brain connectivity were limited to\ninvestigating pairwise (i.e., low-order) relationship ROIs, overlooking the\ncomplex high-order interconnectedness of the brain as a network. To address\nthis limitation, brain multiplexes have been introduced to model the\nrelationship between at least two different brain networks. However, this\ninhibits their application to datasets with single brain networks such as\nfunctional networks. To fill this gap, we propose the first work on predicting\nbrain multiplexes from a source network to investigate gender differences.\nRecently, generative adversarial networks (GANs) submerged the field of medical\ndata synthesis. However, although conventional GANs work well on images, they\ncannot handle brain networks due to their non-Euclidean topological structure.\nDifferently, in this paper, we tap into the nascent field of geometric-GANs\n(G-GAN) to design a deep multiplex prediction architecture comprising (i) a\ngeometric source to target network translator mimicking a U-Net architecture\nwith skip connections and (ii) a conditional discriminator which classifies\npredicted target intra-layers by conditioning on the multiplex source\nintra-layers. Such architecture simultaneously learns the latent source network\nrepresentation and the deep non-linear mapping from the source to target\nmultiplex intra-layers. Our experiments on a large dataset demonstrated that\npredicted multiplexes significantly boost gender classification accuracy\ncompared with source networks and identifies both low and high-order\ngender-specific multiplex connections.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 07:23:41 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Nebli", "Ahmed", ""], ["Rekik", "Islem", ""]]}, {"id": "2009.11528", "submitter": "Xin Lu", "authors": "Xin Lu, Quanquan Li, Buyu Li, Junjie Yan", "title": "MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object\n  Detection", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern object detection methods can be divided into one-stage approaches and\ntwo-stage ones. One-stage detectors are more efficient owing to straightforward\narchitectures, but the two-stage detectors still take the lead in accuracy.\nAlthough recent work try to improve the one-stage detectors by imitating the\nstructural design of the two-stage ones, the accuracy gap is still significant.\nIn this paper, we propose MimicDet, a novel and efficient framework to train a\none-stage detector by directly mimic the two-stage features, aiming to bridge\nthe accuracy gap between one-stage and two-stage detectors. Unlike conventional\nmimic methods, MimicDet has a shared backbone for one-stage and two-stage\ndetectors, then it branches into two heads which are well designed to have\ncompatible features for mimicking. Thus MimicDet can be end-to-end trained\nwithout the pre-train of the teacher network. And the cost does not increase\nmuch, which makes it practical to adopt large networks as backbones. We also\nmake several specialized designs such as dual-path mimicking and staggered\nfeature pyramid to facilitate the mimicking process. Experiments on the\nchallenging COCO detection benchmark demonstrate the effectiveness of MimicDet.\nIt achieves 46.1 mAP with ResNeXt-101 backbone on the COCO test-dev set, which\nsignificantly surpasses current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 07:36:58 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Lu", "Xin", ""], ["Li", "Quanquan", ""], ["Li", "Buyu", ""], ["Yan", "Junjie", ""]]}, {"id": "2009.11534", "submitter": "Islem Rekik", "authors": "Mustafa Saglam and Islem Rekik", "title": "Multi-Scale Profiling of Brain Multigraphs by Eigen-based\n  Cross-Diffusion and Heat Tracing for Brain State Profiling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The individual brain can be viewed as a highly-complex multigraph (i.e. a set\nof graphs also called connectomes), where each graph represents a unique\nconnectional view of pairwise brain region (node) relationships such as\nfunction or morphology. Due to its multifold complexity, understanding how\nbrain disorders alter not only a single view of the brain graph, but its\nmultigraph representation at the individual and population scales, remains one\nof the most challenging obstacles to profiling brain connectivity for\nultimately disentangling a wide spectrum of brain states (e.g., healthy vs.\ndisordered). In this work, while cross-pollinating the fields of spectral graph\ntheory and diffusion models, we unprecedentedly propose an eigen-based\ncross-diffusion strategy for multigraph brain integration, comparison, and\nprofiling. Specifically, we first devise a brain multigraph fusion model guided\nby eigenvector centrality to rely on most central nodes in the cross-diffusion\nprocess. Next, since the graph spectrum encodes its shape (or geometry) as if\none can hear the shape of the graph, for the first time, we profile the fused\nmultigraphs at several diffusion timescales by extracting the compact\nheat-trace signatures of their corresponding Laplacian matrices. Here, we\nreveal for the first time autistic and healthy profiles of morphological brain\nmultigraphs, derived from T1-w magnetic resonance imaging (MRI), and\ndemonstrate their discriminability in boosting the classification of unseen\nsamples in comparison with state-of-the-art methods. This study presents the\nfirst step towards hearing the shape of the brain multigraph that can be\nleveraged for profiling and disentangling comorbid neurological disorders,\nthereby advancing precision medicine.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 07:51:44 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Saglam", "Mustafa", ""], ["Rekik", "Islem", ""]]}, {"id": "2009.11551", "submitter": "Jie Liu", "authors": "Jie Liu, Jie Tang, Gangshan Wu", "title": "Residual Feature Distillation Network for Lightweight Image\n  Super-Resolution", "comments": "accepted by ECCV2020 AIM workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in single image super-resolution (SISR) explored the power of\nconvolutional neural network (CNN) to achieve a better performance. Despite the\ngreat success of CNN-based methods, it is not easy to apply these methods to\nedge devices due to the requirement of heavy computation. To solve this\nproblem, various fast and lightweight CNN models have been proposed. The\ninformation distillation network is one of the state-of-the-art methods, which\nadopts the channel splitting operation to extract distilled features. However,\nit is not clear enough how this operation helps in the design of efficient SISR\nmodels. In this paper, we propose the feature distillation connection (FDC)\nthat is functionally equivalent to the channel splitting operation while being\nmore lightweight and flexible. Thanks to FDC, we can rethink the information\nmulti-distillation network (IMDN) and propose a lightweight and accurate SISR\nmodel called residual feature distillation network (RFDN). RFDN uses multiple\nfeature distillation connections to learn more discriminative feature\nrepresentations. We also propose a shallow residual block (SRB) as the main\nbuilding block of RFDN so that the network can benefit most from residual\nlearning while still being lightweight enough. Extensive experimental results\nshow that the proposed RFDN achieve a better trade-off against the\nstate-of-the-art methods in terms of performance and model complexity.\nMoreover, we propose an enhanced RFDN (E-RFDN) and won the first place in the\nAIM 2020 efficient super-resolution challenge. Code will be available at\nhttps://github.com/njulj/RFDN.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 08:46:40 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Liu", "Jie", ""], ["Tang", "Jie", ""], ["Wu", "Gangshan", ""]]}, {"id": "2009.11553", "submitter": "Islem Rekik", "authors": "Alin Banka, Inis Buzi and Islem Rekik", "title": "Multi-View Brain HyperConnectome AutoEncoder For Brain State\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embedding is a powerful method to represent graph neurological data\n(e.g., brain connectomes) in a low dimensional space for brain connectivity\nmapping, prediction and classification. However, existing embedding algorithms\nhave two major limitations. First, they primarily focus on preserving\none-to-one topological relationships between nodes (i.e., regions of interest\n(ROIs) in a connectome), but they have mostly ignored many-to-many\nrelationships (i.e., set to set), which can be captured using a hyperconnectome\nstructure. Second, existing graph embedding techniques cannot be easily adapted\nto multi-view graph data with heterogeneous distributions. In this paper, while\ncross-pollinating adversarial deep learning with hypergraph theory, we aim to\njointly learn deep latent embeddings of subject0specific multi-view brain\ngraphs to eventually disentangle different brain states. First, we propose a\nnew simple strategy to build a hyperconnectome for each brain view based on\nnearest neighbour algorithm to preserve the connectivities across pairs of\nROIs. Second, we design a hyperconnectome autoencoder (HCAE) framework which\noperates directly on the multi-view hyperconnectomes based on hypergraph\nconvolutional layers to better capture the many-to-many relationships between\nbrain regions (i.e., nodes). For each subject, we further regularize the\nhypergraph autoencoding by adversarial regularization to align the distribution\nof the learned hyperconnectome embeddings with that of the input\nhyperconnectomes. We formalize our hyperconnectome embedding within a geometric\ndeep learning framework to optimize for a given subject, thereby designing an\nindividual-based learning framework. Our experiments showed that the learned\nembeddings by HCAE yield to better results for brain state classification\ncompared with other deep graph embedding methods methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 08:51:44 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Banka", "Alin", ""], ["Buzi", "Inis", ""], ["Rekik", "Islem", ""]]}, {"id": "2009.11562", "submitter": "Pengfei Xiong", "authors": "Jing Tan, Pengfei Xiong, Yuwen He, Kuntao Xiao, Zhengyi Lv", "title": "Local Context Attention for Salient Object Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object segmentation aims at distinguishing various salient objects\nfrom backgrounds. Despite the lack of semantic consistency, salient objects\noften have obvious texture and location characteristics in local area. Based on\nthis priori, we propose a novel Local Context Attention Network (LCANet) to\ngenerate locally reinforcement feature maps in a uniform representational\narchitecture. The proposed network introduces an Attentional Correlation Filter\n(ACF) module to generate explicit local attention by calculating the\ncorrelation feature map between coarse prediction and global context. Then it\nis expanded to a Local Context Block(LCB). Furthermore, an one-stage\ncoarse-to-fine structure is implemented based on LCB to adaptively enhance the\nlocal context description ability. Comprehensive experiments are conducted on\nseveral salient object segmentation datasets, demonstrating the superior\nperformance of the proposed LCANet against the state-of-the-art methods,\nespecially with 0.883 max F-score and 0.034 MAE on DUTS-TE dataset.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 09:20:06 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Tan", "Jing", ""], ["Xiong", "Pengfei", ""], ["He", "Yuwen", ""], ["Xiao", "Kuntao", ""], ["Lv", "Zhengyi", ""]]}, {"id": "2009.11577", "submitter": "Lea Berthomier", "authors": "L\\'ea Berthomier, Bruno Pradel and Lior Perez", "title": "Cloud Cover Nowcasting with Deep Learning", "comments": "6 pages, 11 figures", "journal-ref": "Proceedings of the 2020 Tenth International Conference on Image\n  Processing Theory, Tools and Applications (IPTA), IEEE, Paris, France, 9-12\n  November 2020", "doi": "10.1109/IPTA50016.2020.9286606", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowcasting is a field of meteorology which aims at forecasting weather on a\nshort term of up to a few hours. In the meteorology landscape, this field is\nrather specific as it requires particular techniques, such as data\nextrapolation, where conventional meteorology is generally based on physical\nmodeling. In this paper, we focus on cloud cover nowcasting, which has various\napplication areas such as satellite shots optimisation and photovoltaic energy\nproduction forecast.\n  Following recent deep learning successes on multiple imagery tasks, we\napplied deep convolutionnal neural networks on Meteosat satellite images for\ncloud cover nowcasting. We present the results of several architectures\nspecialized in image segmentation and time series prediction. We selected the\nbest models according to machine learning metrics as well as meteorological\nmetrics. All selected architectures showed significant improvements over\npersistence and the well-known U-Net surpasses AROME physical model.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 09:57:29 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 07:23:35 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 11:57:43 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Berthomier", "L\u00e9a", ""], ["Pradel", "Bruno", ""], ["Perez", "Lior", ""]]}, {"id": "2009.11663", "submitter": "Tooba Aamir Ms", "authors": "Tooba Aamir, Hai Dong and Athman Bouguettaya", "title": "Heuristics based Mosaic of Social-Sensor Services for Scene\n  Reconstruction", "comments": "13 pages, 8 figures and 1 table. This is an accepted paper and it is\n  going to appear in the Proceedings of the 2020 21st International Conference\n  on Web Information Systems Engineering (WISE 2020), Amsterdam and Leiden,\n  Netherlands. The proceedings of WISE 2020 will be published by Springer in\n  its Lecture Notes in Computer Science series", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a heuristics-based social-sensor cloud service selection and\ncomposition model to reconstruct mosaic scenes. The proposed approach leverages\ncrowdsourced social media images to create an image mosaic to reconstruct a\nscene at a designated location and an interval of time. The novel approach\nrelies on the set of features defined on the bases of the image metadata to\ndetermine the relevance and composability of services. Novel heuristics are\ndeveloped to filter out non-relevant services. Multiple machine learning\nstrategies are employed to produce smooth service composition resulting in a\nmosaic of relevant images indexed by geolocation and time. The preliminary\nanalytical results prove the feasibility of the proposed composition model.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 07:00:50 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Aamir", "Tooba", ""], ["Dong", "Hai", ""], ["Bouguettaya", "Athman", ""]]}, {"id": "2009.11676", "submitter": "Benedikt Hosp", "authors": "Benedikt Hosp, Florian Schultz, Oliver H\\\"oner, Enkelejda Kasneci", "title": "Eye Movement Feature Classification for Soccer Goalkeeper Expertise\n  Identification in Virtual Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latest research in expertise assessment of soccer players has affirmed\nthe importance of perceptual skills (especially for decision making) by\nfocusing either on high experimental control or on a realistic presentation. To\nassess the perceptual skills of athletes in an optimized manner, we captured\nomnidirectional in-field scenes and showed these to 12 expert, 10 intermediate\nand 13 novice soccer goalkeepers on virtual reality glasses. All scenes were\nshown from the same natural goalkeeper perspective and ended after the return\npass to the goalkeeper. Based on their gaze behavior we classified their\nexpertise with common machine learning techniques. This pilot study shows\npromising results for objective classification of goalkeepers expertise based\non their gaze behaviour and provided valuable insight to inform the design of\ntraining systems to enhance perceptual skills of athletes.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 12:18:41 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 17:22:41 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Hosp", "Benedikt", ""], ["Schultz", "Florian", ""], ["H\u00f6ner", "Oliver", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2009.11729", "submitter": "Quanshi Zhang", "authors": "Hao Zhang, Sen Li, Yinchao Ma, Mingjie Li, Yichen Xie, Quanshi Zhang", "title": "Interpreting and Boosting Dropout from a Game-Theoretic View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to understand and improve the utility of the dropout\noperation from the perspective of game-theoretic interactions. We prove that\ndropout can suppress the strength of interactions between input variables of\ndeep neural networks (DNNs). The theoretic proof is also verified by various\nexperiments. Furthermore, we find that such interactions were strongly related\nto the over-fitting problem in deep learning. Thus, the utility of dropout can\nbe regarded as decreasing interactions to alleviate the significance of\nover-fitting. Based on this understanding, we propose an interaction loss to\nfurther improve the utility of dropout. Experimental results have shown that\nthe interaction loss can effectively improve the utility of dropout and boost\nthe performance of DNNs.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 14:39:42 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 16:45:35 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 10:30:51 GMT"}, {"version": "v4", "created": "Tue, 16 Mar 2021 10:42:04 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Zhang", "Hao", ""], ["Li", "Sen", ""], ["Ma", "Yinchao", ""], ["Li", "Mingjie", ""], ["Xie", "Yichen", ""], ["Zhang", "Quanshi", ""]]}, {"id": "2009.11746", "submitter": "Xianbiao Qi", "authors": "Yihao Chen, Xin Tang, Xianbiao Qi, Chun-Guang Li, Rong Xiao", "title": "Learning Graph Normalization for Graph Neural Networks", "comments": "15 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have attracted considerable attention and have\nemerged as a new promising paradigm to process graph-structured data. GNNs are\nusually stacked to multiple layers and the node representations in each layer\nare computed through propagating and aggregating the neighboring node features\nwith respect to the graph. By stacking to multiple layers, GNNs are able to\ncapture the long-range dependencies among the data on the graph and thus bring\nperformance improvements. To train a GNN with multiple layers effectively, some\nnormalization techniques (e.g., node-wise normalization, batch-wise\nnormalization) are necessary. However, the normalization techniques for GNNs\nare highly task-relevant and different application tasks prefer to different\nnormalization techniques, which is hard to know in advance. To tackle this\ndeficiency, in this paper, we propose to learn graph normalization by\noptimizing a weighted combination of normalization techniques at four different\nlevels, including node-wise normalization, adjacency-wise normalization,\ngraph-wise normalization, and batch-wise normalization, in which the\nadjacency-wise normalization and the graph-wise normalization are newly\nproposed in this paper to take into account the local structure and the global\nstructure on the graph, respectively. By learning the optimal weights, we are\nable to automatically select a single best or a best combination of multiple\nnormalizations for a specific task. We conduct extensive experiments on\nbenchmark datasets for different tasks, including node classification, link\nprediction, graph classification and graph regression, and confirm that the\nlearned graph normalization leads to competitive results and that the learned\nweights suggest the appropriate normalization techniques for the specific task.\nSource code is released here https://github.com/cyh1112/GraphNormalization.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 15:16:43 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Chen", "Yihao", ""], ["Tang", "Xin", ""], ["Qi", "Xianbiao", ""], ["Li", "Chun-Guang", ""], ["Xiao", "Rong", ""]]}, {"id": "2009.11816", "submitter": "Lu Liu", "authors": "Lu Liu, Tianyi Zhou, Guodong Long, Jing Jiang, Chengqi Zhang", "title": "Attribute Propagation Network for Graph Zero-shot Learning", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of zero-shot learning (ZSL) is to train a model to classify samples\nof classes that were not seen during training. To address this challenging\ntask, most ZSL methods relate unseen test classes to seen(training) classes via\na pre-defined set of attributes that can describe all classes in the same\nsemantic space, so the knowledge learned on the training classes can be adapted\nto unseen classes. In this paper, we aim to optimize the attribute space for\nZSL by training a propagation mechanism to refine the semantic attributes of\neach class based on its neighbors and related classes on a graph of classes. We\nshow that the propagated attributes can produce classifiers for zero-shot\nclasses with significantly improved performance in different ZSL settings. The\ngraph of classes is usually free or very cheap to acquire such as WordNet or\nImageNet classes. When the graph is not provided, given pre-defined semantic\nembeddings of the classes, we can learn a mechanism to generate the graph in an\nend-to-end manner along with the propagation mechanism. However, this\ngraph-aided technique has not been well-explored in the literature. In this\npaper, we introduce the attribute propagation network (APNet), which is\ncomposed of 1) a graph propagation model generating attribute vector for each\nclass and 2) a parameterized nearest neighbor (NN) classifier categorizing an\nimage to the class with the nearest attribute vector to the image's embedding.\nFor better generalization over unseen classes, different from previous methods,\nwe adopt a meta-learning strategy to train the propagation mechanism and the\nsimilarity metric for the NN classifier on multiple sub-graphs, each associated\nwith a classification task over a subset of training classes. In experiments\nwith two zero-shot learning settings and five benchmark datasets, APNet\nachieves either compelling performance or new state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 16:53:40 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Liu", "Lu", ""], ["Zhou", "Tianyi", ""], ["Long", "Guodong", ""], ["Jiang", "Jing", ""], ["Zhang", "Chengqi", ""]]}, {"id": "2009.11839", "submitter": "Ekdeep Singh Lubana", "authors": "Ekdeep Singh Lubana and Robert P. Dick", "title": "A Gradient Flow Framework For Analyzing Network Pruning", "comments": "Accepted at ICLR, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent network pruning methods focus on pruning models early-on in training.\nTo estimate the impact of removing a parameter, these methods use importance\nmeasures that were originally designed to prune trained models. Despite lacking\njustification for their use early-on in training, such measures result in\nsurprisingly low accuracy loss. To better explain this behavior, we develop a\ngeneral framework that uses gradient flow to unify state-of-the-art importance\nmeasures through the norm of model parameters. We use this framework to\ndetermine the relationship between pruning measures and evolution of model\nparameters, establishing several results related to pruning models early-on in\ntraining: (i) magnitude-based pruning removes parameters that contribute least\nto reduction in loss, resulting in models that converge faster than\nmagnitude-agnostic methods; (ii) loss-preservation based pruning preserves\nfirst-order model evolution dynamics and is therefore appropriate for pruning\nminimally trained models; and (iii) gradient-norm based pruning affects\nsecond-order model evolution dynamics, such that increasing gradient norm via\npruning can produce poorly performing models. We validate our claims on several\nVGG-13, MobileNet-V1, and ResNet-56 models trained on CIFAR-10/CIFAR-100. Code\navailable at https://github.com/EkdeepSLubana/flowandprune.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 17:37:32 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 04:51:58 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 03:16:32 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Lubana", "Ekdeep Singh", ""], ["Dick", "Robert P.", ""]]}, {"id": "2009.11848", "submitter": "Keyulu Xu", "authors": "Keyulu Xu, Mozhi Zhang, Jingling Li, Simon S. Du, Ken-ichi\n  Kawarabayashi, Stefanie Jegelka", "title": "How Neural Networks Extrapolate: From Feedforward to Graph Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how neural networks trained by gradient descent extrapolate, i.e.,\nwhat they learn outside the support of the training distribution. Previous\nworks report mixed empirical results when extrapolating with neural networks:\nwhile feedforward neural networks, a.k.a. multilayer perceptrons (MLPs), do not\nextrapolate well in certain simple tasks, Graph Neural Networks (GNNs) --\nstructured networks with MLP modules -- have shown some success in more complex\ntasks. Working towards a theoretical explanation, we identify conditions under\nwhich MLPs and GNNs extrapolate well. First, we quantify the observation that\nReLU MLPs quickly converge to linear functions along any direction from the\norigin, which implies that ReLU MLPs do not extrapolate most nonlinear\nfunctions. But, they can provably learn a linear target function when the\ntraining distribution is sufficiently \"diverse\". Second, in connection to\nanalyzing the successes and limitations of GNNs, these results suggest a\nhypothesis for which we provide theoretical and empirical evidence: the success\nof GNNs in extrapolating algorithmic tasks to new data (e.g., larger graphs or\nedge weights) relies on encoding task-specific non-linearities in the\narchitecture or features. Our theoretical analysis builds on a connection of\nover-parameterized networks to the neural tangent kernel. Empirically, our\ntheory holds across different training settings.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 17:48:59 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 03:54:28 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 23:54:16 GMT"}, {"version": "v4", "created": "Sun, 21 Feb 2021 19:42:02 GMT"}, {"version": "v5", "created": "Tue, 2 Mar 2021 23:05:49 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Xu", "Keyulu", ""], ["Zhang", "Mozhi", ""], ["Li", "Jingling", ""], ["Du", "Simon S.", ""], ["Kawarabayashi", "Ken-ichi", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "2009.11850", "submitter": "Ashad Kabir", "authors": "Nihad Karim Chowdhury, Muhammad Ashad Kabir, Md. Muhtadir Rahman,\n  Noortaz Rezoana", "title": "ECOVNet: An Ensemble of Deep Convolutional Neural Networks Based on\n  EfficientNet to Detect COVID-19 From Chest X-rays", "comments": null, "journal-ref": "Peer J Computer Science, 2021", "doi": "10.7717/peerj-cs.551", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposed an ensemble of deep convolutional neural networks (CNN)\nbased on EfficientNet, named ECOVNet, to detect COVID-19 using a large chest\nX-ray data set. At first, the open-access large chest X-ray collection is\naugmented, and then ImageNet pre-trained weights for EfficientNet is\ntransferred with some customized fine-tuning top layers that are trained,\nfollowed by an ensemble of model snapshots to classify chest X-rays\ncorresponding to COVID-19, normal, and pneumonia. The predictions of the model\nsnapshots, which are created during a single training, are combined through two\nensemble strategies, i.e., hard ensemble and soft ensemble to ameliorate\nclassification performance and generalization in the related task of\nclassifying chest X-rays.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 17:53:17 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 01:24:48 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Chowdhury", "Nihad Karim", ""], ["Kabir", "Muhammad Ashad", ""], ["Rahman", "Md. Muhtadir", ""], ["Rezoana", "Noortaz", ""]]}, {"id": "2009.11859", "submitter": "Yue Wang", "authors": "Yue Wang and Alireza Fathi and Jiajun Wu and Thomas Funkhouser and\n  Justin Solomon", "title": "Multi-Frame to Single-Frame: Knowledge Distillation for 3D Object\n  Detection", "comments": "The Workshop on Perception for Autonomous Driving at ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common dilemma in 3D object detection for autonomous driving is that\nhigh-quality, dense point clouds are only available during training, but not\ntesting. We use knowledge distillation to bridge the gap between a model\ntrained on high-quality inputs at training time and another tested on\nlow-quality inputs at inference time. In particular, we design a two-stage\ntraining pipeline for point cloud object detection. First, we train an object\ndetection model on dense point clouds, which are generated from multiple frames\nusing extra information only available at training time. Then, we train the\nmodel's identical counterpart on sparse single-frame point clouds with\nconsistency regularization on features from both models. We show that this\nprocedure improves performance on low-quality data during testing, without\nadditional overhead.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 17:59:12 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Wang", "Yue", ""], ["Fathi", "Alireza", ""], ["Wu", "Jiajun", ""], ["Funkhouser", "Thomas", ""], ["Solomon", "Justin", ""]]}, {"id": "2009.11892", "submitter": "Xueli Xiao", "authors": "Xueli Xiao, Chunyan Ji, Thosini Bamunu Mudiyanselage, Yi Pan", "title": "PK-GCN: Prior Knowledge Assisted Image Classification using Graph\n  Convolution Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has gained great success in various classification tasks.\nTypically, deep learning models learn underlying features directly from data,\nand no underlying relationship between classes are included. Similarity between\nclasses can influence the performance of classification. In this article, we\npropose a method that incorporates class similarity knowledge into\nconvolutional neural networks models using a graph convolution layer. We\nevaluate our method on two benchmark image datasets: MNIST and CIFAR10, and\nanalyze the results on different data and model sizes. Experimental results\nshow that our model can improve classification accuracy, especially when the\namount of available data is small.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 18:31:35 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Xiao", "Xueli", ""], ["Ji", "Chunyan", ""], ["Mudiyanselage", "Thosini Bamunu", ""], ["Pan", "Yi", ""]]}, {"id": "2009.11924", "submitter": "Siwei Lyu", "authors": "Shu Hu, Yuezun Li, and Siwei Lyu", "title": "Exposing GAN-generated Faces Using Inconsistent Corneal Specular\n  Highlights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sophisticated generative adversary network (GAN) models are now able to\nsynthesize highly realistic human faces that are difficult to discern from real\nones visually. In this work, we show that GAN synthesized faces can be exposed\nwith the inconsistent corneal specular highlights between two eyes. The\ninconsistency is caused by the lack of physical/physiological constraints in\nthe GAN models. We show that such artifacts exist widely in high-quality GAN\nsynthesized faces and further describe an automatic method to extract and\ncompare corneal specular highlights from two eyes. Qualitative and quantitative\nevaluations of our method suggest its simplicity and effectiveness in\ndistinguishing GAN synthesized faces.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 19:43:16 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 19:28:14 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Hu", "Shu", ""], ["Li", "Yuezun", ""], ["Lyu", "Siwei", ""]]}, {"id": "2009.11929", "submitter": "Hieu Pham", "authors": "Lawrence Mosley and Hieu Pham and Yogesh Bansal and Eric Hare", "title": "Image-Based Sorghum Head Counting When You Only Look Once", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern trends in digital agriculture have seen a shift towards artificial\nintelligence for crop quality assessment and yield estimation. In this work, we\ndocument how a parameter tuned single-shot object detection algorithm can be\nused to identify and count sorghum head from aerial drone images. Our approach\ninvolves a novel exploratory analysis that identified key structural elements\nof the sorghum images and motivated the selection of parameter-tuned anchor\nboxes that contributed significantly to performance. These insights led to the\ndevelopment of a deep learning model that outperformed the baseline model and\nachieved an out-of-sample mean average precision of 0.95.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 19:50:08 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 16:01:16 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Mosley", "Lawrence", ""], ["Pham", "Hieu", ""], ["Bansal", "Yogesh", ""], ["Hare", "Eric", ""]]}, {"id": "2009.11931", "submitter": "Sina Akbarian", "authors": "Sina Akbarian, Tania Cawston, Laurent Moreno, Samir Patel, Vanessa\n  Allen, and Elham Dolatabadi", "title": "A Computer Vision Approach to Combat Lyme Disease", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lyme disease is an infectious disease transmitted to humans by a bite from an\ninfected Ixodes species (blacklegged ticks). It is one of the fastest growing\nvector-borne illness in North America and is expanding its geographic\nfootprint. Lyme disease treatment is time-sensitive, and can be cured by\nadministering an antibiotic (prophylaxis) to the patient within 72 hours after\na tick bite by the Ixodes species. However, the laboratory-based identification\nof each tick that might carry the bacteria is time-consuming and labour\nintensive and cannot meet the maximum turn-around-time of 72 hours for an\neffective treatment. Early identification of blacklegged ticks using computer\nvision technologies is a potential solution in promptly identifying a tick and\nadministering prophylaxis within a crucial window period. In this work, we\nbuild an automated detection tool that can differentiate blacklegged ticks from\nother ticks species using advanced deep learning and computer vision\napproaches. We demonstrate the classification of tick species using Convolution\nNeural Network (CNN) models, trained end-to-end from tick images directly.\nAdvanced knowledge transfer techniques within teacher-student learning\nframeworks are adopted to improve the performance of classification of tick\nspecies. Our best CNN model achieves 92% accuracy on test set. The tool can be\nintegrated with the geography of exposure to determine the risk of Lyme disease\ninfection and need for prophylaxis treatment.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 20:00:02 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Akbarian", "Sina", ""], ["Cawston", "Tania", ""], ["Moreno", "Laurent", ""], ["Patel", "Samir", ""], ["Allen", "Vanessa", ""], ["Dolatabadi", "Elham", ""]]}, {"id": "2009.11937", "submitter": "Yidan Qin", "authors": "Yidan Qin, Seyedshams Feyzabadi, Max Allan, Joel W. Burdick, Mahdi\n  Azizian", "title": "daVinciNet: Joint Prediction of Motion and Surgical State in\n  Robot-Assisted Surgery", "comments": "Accepted to IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a technique to concurrently and jointly predict the\nfuture trajectories of surgical instruments and the future state(s) of surgical\nsubtasks in robot-assisted surgeries (RAS) using multiple input sources. Such\npredictions are a necessary first step towards shared control and supervised\nautonomy of surgical subtasks. Minute-long surgical subtasks, such as suturing\nor ultrasound scanning, often have distinguishable tool kinematics and visual\nfeatures, and can be described as a series of fine-grained states with\ntransition schematics. We propose daVinciNet - an end-to-end dual-task model\nfor robot motion and surgical state predictions. daVinciNet performs concurrent\nend-effector trajectory and surgical state predictions using features extracted\nfrom multiple data streams, including robot kinematics, endoscopic vision, and\nsystem events. We evaluate our proposed model on an extended Robotic\nIntra-Operative Ultrasound (RIOUS+) imaging dataset collected on a da Vinci Xi\nsurgical system and the JHU-ISI Gesture and Skill Assessment Working Set\n(JIGSAWS). Our model achieves up to 93.85% short-term (0.5s) and 82.11%\nlong-term (2s) state prediction accuracy, as well as 1.07mm short-term and\n5.62mm long-term trajectory prediction error.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 20:28:06 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Qin", "Yidan", ""], ["Feyzabadi", "Seyedshams", ""], ["Allan", "Max", ""], ["Burdick", "Joel W.", ""], ["Azizian", "Mahdi", ""]]}, {"id": "2009.11939", "submitter": "Ali Karaali", "authors": "Ali Karaali, Naomi Harte, Claudio Rosito Jung", "title": "Deep Multi-Scale Feature Learning for Defocus Blur Estimation", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an edge-based defocus blur estimation method from a\nsingle defocused image. We first distinguish edges that lie at depth\ndiscontinuities (called depth edges, for which the blur estimate is ambiguous)\nfrom edges that lie at approximately constant depth regions (called pattern\nedges, for which the blur estimate is well-defined). Then, we estimate the\ndefocus blur amount at pattern edges only, and explore an interpolation scheme\nbased on guided filters that prevents data propagation across the detected\ndepth edges to obtain a dense blur map with well-defined object boundaries.\nBoth tasks (edge classification and blur estimation) are performed by deep\nconvolutional neural networks (CNNs) that share weights to learn meaningful\nlocal features from multi-scale patches centered at edge locations. Experiments\non naturally defocused images show that the proposed method presents\nqualitative and quantitative results that outperform state-of-the-art (SOTA)\nmethods, with a good compromise between running time and accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 20:36:40 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Karaali", "Ali", ""], ["Harte", "Naomi", ""], ["Jung", "Claudio Rosito", ""]]}, {"id": "2009.11948", "submitter": "Hao Zhang", "authors": "Hao Zhang, Xu Ma, Xianhong Zhao, Gonzalo R. Arce", "title": "Compressive spectral image classification using 3D coded convolutional\n  neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral image classification (HIC) is an active research topic in\nremote sensing. Hyperspectral images typically generate large data cubes posing\nbig challenges in data acquisition, storage, transmission and processing. To\novercome these limitations, this paper develops a novel deep learning HIC\napproach based on compressive measurements of coded-aperture snapshot spectral\nimagers (CASSI), without reconstructing the complete hyperspectral data cube. A\nnew kind of deep learning strategy, namely 3D coded convolutional neural\nnetwork (3D-CCNN) is proposed to efficiently solve for the classification\nproblem, where the hardware-based coded aperture is regarded as a pixel-wise\nconnected network layer. An end-to-end training method is developed to jointly\noptimize the network parameters and the coded apertures with periodic\nstructures. The accuracy of classification is effectively improved by\nexploiting the synergy between the deep learning network and coded apertures.\nThe superiority of the proposed method is assessed over the state-of-the-art\nHIC methods on several hyperspectral datasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 15:05:57 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 08:41:59 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 09:25:22 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhang", "Hao", ""], ["Ma", "Xu", ""], ["Zhao", "Xianhong", ""], ["Arce", "Gonzalo R.", ""]]}, {"id": "2009.11971", "submitter": "Bin Wu", "authors": "Bin Wu, Xue-Cheng Tai, and Talal Rahman", "title": "Multidimensional TV-Stokes for image processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A complete multidimential TV-Stokes model is proposed based on smoothing a\ngradient field in the first step and reconstruction of the multidimensional\nimage from the gradient field. It is the correct extension of the original two\ndimensional TV-Stokes to multidimensions. Numerical algorithm using the\nChambolle's semi-implicit dual formula is proposed. Numerical results applied\nto denoising 3D images and movies are presented. They show excellent\nperformance in avoiding the staircase effect, and preserving fine structures.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 22:21:27 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 21:16:30 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Wu", "Bin", ""], ["Tai", "Xue-Cheng", ""], ["Rahman", "Talal", ""]]}, {"id": "2009.11973", "submitter": "Bin Wu", "authors": "Bin Wu, Xue-Cheng Tai, and Talal Rahman", "title": "Alternating minimization for a single step TV-Stokes model for image\n  denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a fully coupled TV-Stokes model, and propose an algorithm\nbased on alternating minimization of the objective functional whose first\niteration is exactly the modified TV-Stokes model proposed earlier. The model\nis a generalization of the second order Total Generalized Variation model. A\nconvergence analysis is given.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 22:31:15 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 13:07:36 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Wu", "Bin", ""], ["Tai", "Xue-Cheng", ""], ["Rahman", "Talal", ""]]}, {"id": "2009.11975", "submitter": "Jingda Guo", "authors": "Jingda Guo, Dominic Carrillo, Sihai Tang, Qi Chen, Qing Yang, Song Fu,\n  Xi Wang, Nannan Wang, Paparao Palacharla", "title": "CoFF: Cooperative Spatial Feature Fusion for 3D Object Detection on\n  Autonomous Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce the amount of transmitted data, feature map based fusion is\nrecently proposed as a practical solution to cooperative 3D object detection by\nautonomous vehicles. The precision of object detection, however, may require\nsignificant improvement, especially for objects that are far away or occluded.\nTo address this critical issue for the safety of autonomous vehicles and human\nbeings, we propose a cooperative spatial feature fusion (CoFF) method for\nautonomous vehicles to effectively fuse feature maps for achieving a higher 3D\nobject detection performance. Specially, CoFF differentiates weights among\nfeature maps for a more guided fusion, based on how much new semantic\ninformation is provided by the received feature maps. It also enhances the\ninconspicuous features corresponding to far/occluded objects to improve their\ndetection precision. Experimental results show that CoFF achieves a significant\nimprovement in terms of both detection precision and effective detection range\nfor autonomous vehicles, compared to previous feature fusion solutions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 22:51:50 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Guo", "Jingda", ""], ["Carrillo", "Dominic", ""], ["Tang", "Sihai", ""], ["Chen", "Qi", ""], ["Yang", "Qing", ""], ["Fu", "Song", ""], ["Wang", "Xi", ""], ["Wang", "Nannan", ""], ["Palacharla", "Paparao", ""]]}, {"id": "2009.11976", "submitter": "Bin Wu", "authors": "Bin Wu, Leszek Marcinkowski, Xue-Cheng Tai, and Talal Rahman", "title": "Iterative regularization algorithms for image denoising with the\n  TV-Stokes model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a set of iterative regularization algorithms for the TV-Stokes\nmodel to restore images from noisy images with Gaussian noise. These are some\nextensions of the iterative regularization algorithm proposed for the classical\nRudin-Osher-Fatemi (ROF) model for image reconstruction, a single step model\ninvolving a scalar field smoothing, to the TV-Stokes model for image\nreconstruction, a two steps model involving a vector field smoothing in the\nfirst and a scalar field smoothing in the second. The iterative regularization\nalgorithms proposed here are Richardson's iteration like. We have experimental\nresults that show improvement over the original method in the quality of the\nrestored image. Convergence analysis and numerical experiments are presented.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 22:55:18 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Wu", "Bin", ""], ["Marcinkowski", "Leszek", ""], ["Tai", "Xue-Cheng", ""], ["Rahman", "Talal", ""]]}, {"id": "2009.11977", "submitter": "Fares Fourati", "authors": "Fares Fourati, Wided Souidene, Rabah Attia", "title": "An original framework for Wheat Head Detection using Deep,\n  Semi-supervised and Ensemble Learning within Global Wheat Head Detection\n  (GWHD) Dataset", "comments": "Canadian Journal of Remote Sensing (2021)", "journal-ref": null, "doi": "10.1080/07038992.2021.1906213", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an original object detection methodology applied to\nGlobal Wheat Head Detection (GWHD) Dataset. We have been through two major\narchitectures of object detection which are FasterRCNN and EfficientDet, in\norder to design a novel and robust wheat head detection model. We emphasize on\noptimizing the performance of our proposed final architectures. Furthermore, we\nhave been through an extensive exploratory data analysis and adapted best data\naugmentation techniques to our context. We use semi supervised learning to\nboost previous supervised models of object detection. Moreover, we put much\neffort on ensemble to achieve higher performance. Finally we use specific\npost-processing techniques to optimize our wheat head detection results. Our\nresults have been submitted to solve a research challenge launched on the GWHD\nDataset which is led by nine research institutes from seven countries. Our\nproposed method was ranked within the top 6% in the above mentioned challenge.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 22:58:40 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Fourati", "Fares", ""], ["Souidene", "Wided", ""], ["Attia", "Rabah", ""]]}, {"id": "2009.11988", "submitter": "Holger Roth", "authors": "Holger R Roth, Dong Yang, Ziyue Xu, Xiaosong Wang, Daguang Xu", "title": "Going to Extremes: Weakly Supervised Medical Image Segmentation", "comments": "13 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image annotation is a major hurdle for developing precise and robust\nmachine learning models. Annotation is expensive, time-consuming, and often\nrequires expert knowledge, particularly in the medical field. Here, we suggest\nusing minimal user interaction in the form of extreme point clicks to train a\nsegmentation model which, in effect, can be used to speed up medical image\nannotation. An initial segmentation is generated based on the extreme points\nutilizing the random walker algorithm. This initial segmentation is then used\nas a noisy supervision signal to train a fully convolutional network that can\nsegment the organ of interest, based on the provided user clicks. Through\nexperimentation on several medical imaging datasets, we show that the\npredictions of the network can be refined using several rounds of training with\nthe prediction from the same weakly annotated data. Further improvements are\nshown utilizing the clicked points within a custom-designed loss and attention\nmechanism. Our approach has the potential to speed up the process of generating\nnew training datasets for the development of new machine learning and deep\nlearning-based models for, but not exclusively, medical image analysis.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 00:28:10 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Roth", "Holger R", ""], ["Yang", "Dong", ""], ["Xu", "Ziyue", ""], ["Wang", "Xiaosong", ""], ["Xu", "Daguang", ""]]}, {"id": "2009.12007", "submitter": "Sayak Paul", "authors": "Souradip Chakraborty, Aritra Roy Gosthipaty, Sayak Paul", "title": "G-SimCLR : Self-Supervised Contrastive Learning with Guided Projection\n  via Pseudo Labelling", "comments": "Code available at this URL: https://github.com/ariG23498/G-SimCLR.\n  This paper is accepeted as a workshop paper at\n  https://fuzhenzhuang.github.io/DLKT2020/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the realms of computer vision, it is evident that deep neural networks\nperform better in a supervised setting with a large amount of labeled data. The\nrepresentations learned with supervision are not only of high quality but also\nhelps the model in enhancing its accuracy. However, the collection and\nannotation of a large dataset are costly and time-consuming. To avoid the same,\nthere has been a lot of research going on in the field of unsupervised visual\nrepresentation learning especially in a self-supervised setting. Amongst the\nrecent advancements in self-supervised methods for visual recognition, in\nSimCLR Chen et al. shows that good quality representations can indeed be\nlearned without explicit supervision. In SimCLR, the authors maximize the\nsimilarity of augmentations of the same image and minimize the similarity of\naugmentations of different images. A linear classifier trained with the\nrepresentations learned using this approach yields 76.5% top-1 accuracy on the\nImageNet ILSVRC-2012 dataset. In this work, we propose that, with the\nnormalized temperature-scaled cross-entropy (NT-Xent) loss function (as used in\nSimCLR), it is beneficial to not have images of the same category in the same\nbatch. In an unsupervised setting, the information of images pertaining to the\nsame category is missing. We use the latent space representation of a denoising\nautoencoder trained on the unlabeled dataset and cluster them with k-means to\nobtain pseudo labels. With this apriori information we batch images, where no\ntwo images from the same category are to be found. We report comparable\nperformance enhancements on the CIFAR10 dataset and a subset of the ImageNet\ndataset. We refer to our method as G-SimCLR.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 02:25:37 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Chakraborty", "Souradip", ""], ["Gosthipaty", "Aritra Roy", ""], ["Paul", "Sayak", ""]]}, {"id": "2009.12015", "submitter": "Essam Rashed", "authors": "Essam A. Rashed, Jose Gomez-Tames, Akimasa Hirata", "title": "Influence of segmentation accuracy in structural MR head scans on\n  electric field computation for TMS and tES", "comments": "Phys. Med. Biol", "journal-ref": "Physics in Medicine and Biology, 2021", "doi": "10.1088/1361-6560/abe223", "report-no": null, "categories": "physics.med-ph cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several diagnosis and therapy procedures based on electrostimulation\neffect, the internal physical quantity related to the stimulation is the\ninduced electric field. To estimate the induced electric field in an individual\nhuman model, the segmentation of anatomical imaging, such as (magnetic\nresonance image (MRI) scans, of the corresponding body parts into tissues is\nrequired. Then, electrical properties associated with different annotated\ntissues are assigned to the digital model to generate a volume conductor. An\nopen question is how segmentation accuracy of different tissues would influence\nthe distribution of the induced electric field. In this study, we applied\nparametric segmentation of different tissues to exploit the segmentation of\navailable MRI to generate different quality of head models using deep learning\nneural network architecture, named ForkNet. Then, the induced electric field\nare compared to assess the effect of model segmentation variations.\nComputational results indicate that the influence of segmentation error is\ntissue-dependent. In brain, sensitivity to segmentation accuracy is relatively\nhigh in cerebrospinal fluid (CSF), moderate in gray matter (GM) and low in\nwhite matter for transcranial magnetic stimulation (TMS) and transcranial\nelectrical stimulation (tES). A CSF segmentation accuracy reduction of 10% in\nterms of Dice coefficient (DC) lead to decrease up to 4% in normalized induced\nelectric field in both applications. However, a GM segmentation accuracy\nreduction of 5.6% DC leads to increase of normalized induced electric field up\nto 6%. Opposite trend of electric field variation was found between CSF and GM\nfor both TMS and tES. The finding obtained here would be useful to quantify\npotential uncertainty of computational results.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 03:38:24 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 00:54:14 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Rashed", "Essam A.", ""], ["Gomez-Tames", "Jose", ""], ["Hirata", "Akimasa", ""]]}, {"id": "2009.12021", "submitter": "Xudong Wang", "authors": "Xudong Wang and Stella X. Yu", "title": "Tied Block Convolution: Leaner and Better CNNs with Shared Thinner\n  Filters", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution is the main building block of convolutional neural networks\n(CNN). We observe that an optimized CNN often has highly correlated filters as\nthe number of channels increases with depth, reducing the expressive power of\nfeature representations. We propose Tied Block Convolution (TBC) that shares\nthe same thinner filters over equal blocks of channels and produces multiple\nresponses with a single filter. The concept of TBC can also be extended to\ngroup convolution and fully connected layers, and can be applied to various\nbackbone networks and attention modules. Our extensive experimentation on\nclassification, detection, instance segmentation, and attention demonstrates\nTBC's significant across-the-board gain over standard convolution and group\nconvolution. The proposed TiedSE attention module can even use 64 times fewer\nparameters than the SE module to achieve comparable performance. In particular,\nstandard CNNs often fail to accurately aggregate information in the presence of\nocclusion and result in multiple redundant partial object proposals. By sharing\nfilters across channels, TBC reduces correlation and can effectively handle\nhighly overlapping instances. TBC increases the average precision for object\ndetection on MS-COCO by 6% when the occlusion ratio is 80%. Our code will be\nreleased.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 03:58:40 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Wang", "Xudong", ""], ["Yu", "Stella X.", ""]]}, {"id": "2009.12027", "submitter": "Krishanu Sarker", "authors": "Krishanu Sarker, Xiulong Yang, Yang Li, Saeid Belkasim and Shihao Ji", "title": "A Unified Plug-and-Play Framework for Effective Data Denoising and\n  Robust Abstention", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of Deep Neural Networks (DNNs) highly depends on data quality.\nMoreover, predictive uncertainty makes high performing DNNs risky for\nreal-world deployment. In this paper, we aim to address these two issues by\nproposing a unified filtering framework leveraging underlying data density,\nthat can effectively denoise training data as well as avoid predicting\nuncertain test data points. Our proposed framework leverages underlying data\ndistribution to differentiate between noise and clean data samples without\nrequiring any modification to existing DNN architectures or loss functions.\nExtensive experiments on multiple image classification datasets and multiple\nCNN architectures demonstrate that our simple yet effective framework can\noutperform the state-of-the-art techniques in denoising training data and\nabstaining uncertain test data.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 04:18:08 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Sarker", "Krishanu", ""], ["Yang", "Xiulong", ""], ["Li", "Yang", ""], ["Belkasim", "Saeid", ""], ["Ji", "Shihao", ""]]}, {"id": "2009.12028", "submitter": "Jeremiah Deng", "authors": "Jinyong Hou, Xuejie Ding, Stephen Cranefield, Jeremiah D. Deng", "title": "Deep Adversarial Transition Learning using Cross-Grafted Generative\n  Stacks", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep domain adaptation methods used in computer vision have mainly\nfocused on learning discriminative and domain-invariant features across\ndifferent domains. In this paper, we present a novel \"deep adversarial\ntransition learning\" (DATL) framework that bridges the domain gap by projecting\nthe source and target domains into intermediate, transitional spaces through\nthe employment of adjustable, cross-grafted generative network stacks and\neffective adversarial learning between transitions. Specifically, we construct\nvariational auto-encoders (VAE) for the two domains, and form bidirectional\ntransitions by cross-grafting the VAEs' decoder stacks. Furthermore, generative\nadversarial networks (GAN) are employed for domain adaptation, mapping the\ntarget domain data to the known label space of the source domain. The overall\nadaptation process hence consists of three phases: feature representation\nlearning by VAEs, transitions generation, and transitions alignment by GANs.\nExperimental results demonstrate that our method outperforms the state-of-the\nart on a number of unsupervised domain adaptation benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 04:25:27 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Hou", "Jinyong", ""], ["Ding", "Xuejie", ""], ["Cranefield", "Stephen", ""], ["Deng", "Jeremiah D.", ""]]}, {"id": "2009.12053", "submitter": "Song Guo", "authors": "Song Guo", "title": "DPN: Detail-Preserving Network with High Resolution Representation for\n  Efficient Segmentation of Retinal Vessels", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal vessels are important biomarkers for many ophthalmological and\ncardiovascular diseases. It is of great significance to develop an accurate and\nfast vessel segmentation model for computer-aided diagnosis. Existing methods,\nsuch as U-Net follows the encoder-decoder pipeline, where detailed information\nis lost in the encoder in order to achieve a large field of view. Although\ndetailed information could be recovered in the decoder via multi-scale fusion,\nit still contains noise. In this paper, we propose a deep segmentation model,\ncalled detail-preserving network (DPN) for efficient vessel segmentation. To\npreserve detailed spatial information and learn structural information at the\nsame time, we designed the detail-preserving block (DP-Block). Further, we\nstacked eight DP-Blocks together to form the DPN. More importantly, there are\nno down-sampling operations among these blocks. As a result, the DPN could\nmaintain a high resolution during the processing, which is helpful to locate\nthe boundaries of thin vessels. To illustrate the effectiveness of our method,\nwe conducted experiments over three public datasets. Experimental results show,\ncompared to state-of-the-art methods, our method shows competitive/better\nperformance in terms of segmentation accuracy, segmentation speed,\nextensibility and the number of parameters. Specifically, 1) the AUC of our\nmethod ranks first/second/third on the STARE/CHASE_DB1/DRIVE datasets,\nrespectively. 2) Only one forward pass is required of our method to generate a\nvessel segmentation map, and the segmentation speed of our method is over\n20-160x faster than other methods on the DRIVE dataset. 3) We conducted\ncross-training experiments to demonstrate the extensibility of our method, and\nresults revealed that our method shows superior performance. 4) The number of\nparameters of our method is only around 96k, less then all comparison methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 06:38:18 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Guo", "Song", ""]]}, {"id": "2009.12063", "submitter": "Minsong Ki", "authors": "Minsong Ki, Youngjung Uh, Wonyoung Lee, Hyeran Byun", "title": "In-sample Contrastive Learning and Consistent Attention for Weakly\n  Supervised Object Localization", "comments": "To appear at ACCV2020 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised object localization (WSOL) aims to localize the target\nobject using only the image-level supervision. Recent methods encourage the\nmodel to activate feature maps over the entire object by dropping the most\ndiscriminative parts. However, they are likely to induce excessive extension to\nthe backgrounds which leads to over-estimated localization. In this paper, we\nconsider the background as an important cue that guides the feature activation\nto cover the sophisticated object region and propose contrastive attention\nloss. The loss promotes similarity between foreground and its dropped version,\nand, dissimilarity between the dropped version and background. Furthermore, we\npropose foreground consistency loss that penalizes earlier layers producing\nnoisy attention regarding the later layer as a reference to provide them with a\nsense of backgroundness. It guides the early layers to activate on objects\nrather than locally distinctive backgrounds so that their attentions to be\nsimilar to the later layer. For better optimizing the above losses, we use the\nnon-local attention blocks to replace channel-pooled attention leading to\nenhanced attention maps considering the spatial similarity. Last but not least,\nwe propose to drop background regions in addition to the most discriminative\nregion. Our method achieves state-of-theart performance on CUB-200-2011 and\nImageNet benchmark datasets regarding top-1 localization accuracy and\nMaxBoxAccV2, and we provide detailed analysis on our individual components. The\ncode will be publicly available online for reproducibility.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 07:24:46 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Ki", "Minsong", ""], ["Uh", "Youngjung", ""], ["Lee", "Wonyoung", ""], ["Byun", "Hyeran", ""]]}, {"id": "2009.12072", "submitter": "Pengxu Wei", "authors": "Pengxu Wei, Hannan Lu, Radu Timofte, Liang Lin, Wangmeng Zuo, Zhihong\n  Pan, Baopu Li, Teng Xi, Yanwen Fan, Gang Zhang, Jingtuo Liu, Junyu Han, Errui\n  Ding, Tangxin Xie, Liang Cao, Yan Zou, Yi Shen, Jialiang Zhang, Yu Jia,\n  Kaihua Cheng, Chenhuan Wu, Yue Lin, Cen Liu, Yunbo Peng, Xueyi Zou, Zhipeng\n  Luo, Yuehan Yao, Zhenyu Xu, Syed Waqas Zamir, Aditya Arora, Salman Khan,\n  Munawar Hayat, Fahad Shahbaz Khan, Keon-Hee Ahn, Jun-Hyuk Kim, Jun-Ho Choi,\n  Jong-Seok Lee, Tongtong Zhao, Shanshan Zhao, Yoseob Han, Byung-Hoon Kim,\n  JaeHyun Baek, Haoning Wu, Dejia Xu, Bo Zhou, Wei Guan, Xiaobo Li, Chen Ye,\n  Hao Li, Haoyu Zhong, Yukai Shi, Zhijing Yang, Xiaojun Yang, Haoyu Zhong, Xin\n  Li, Xin Jin, Yaojun Wu, Yingxue Pang, Sen Liu, Zhi-Song Liu, Li-Wen Wang,\n  Chu-Tak Li, Marie-Paule Cani, Wan-Chi Siu, Yuanbo Zhou, Rao Muhammad Umer,\n  Christian Micheloni, Xiaofeng Cong, Rajat Gupta, Keon-Hee Ahn, Jun-Hyuk Kim,\n  Jun-Ho Choi, Jong-Seok Lee, Feras Almasri, Thomas Vandamme, Olivier Debeir", "title": "AIM 2020 Challenge on Real Image Super-Resolution: Methods and Results", "comments": null, "journal-ref": "European Conference on Computer Vision Workshops, 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the real image Super-Resolution (SR) challenge that was\npart of the Advances in Image Manipulation (AIM) workshop, held in conjunction\nwith ECCV 2020. This challenge involves three tracks to super-resolve an input\nimage for $\\times$2, $\\times$3 and $\\times$4 scaling factors, respectively. The\ngoal is to attract more attention to realistic image degradation for the SR\ntask, which is much more complicated and challenging, and contributes to\nreal-world image super-resolution applications. 452 participants were\nregistered for three tracks in total, and 24 teams submitted their results.\nThey gauge the state-of-the-art approaches for real image SR in terms of PSNR\nand SSIM.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 07:42:55 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Wei", "Pengxu", ""], ["Lu", "Hannan", ""], ["Timofte", "Radu", ""], ["Lin", "Liang", ""], ["Zuo", "Wangmeng", ""], ["Pan", "Zhihong", ""], ["Li", "Baopu", ""], ["Xi", "Teng", ""], ["Fan", "Yanwen", ""], ["Zhang", "Gang", ""], ["Liu", "Jingtuo", ""], ["Han", "Junyu", ""], ["Ding", "Errui", ""], ["Xie", "Tangxin", ""], ["Cao", "Liang", ""], ["Zou", "Yan", ""], ["Shen", "Yi", ""], ["Zhang", "Jialiang", ""], ["Jia", "Yu", ""], ["Cheng", "Kaihua", ""], ["Wu", "Chenhuan", ""], ["Lin", "Yue", ""], ["Liu", "Cen", ""], ["Peng", "Yunbo", ""], ["Zou", "Xueyi", ""], ["Luo", "Zhipeng", ""], ["Yao", "Yuehan", ""], ["Xu", "Zhenyu", ""], ["Zamir", "Syed Waqas", ""], ["Arora", "Aditya", ""], ["Khan", "Salman", ""], ["Hayat", "Munawar", ""], ["Khan", "Fahad Shahbaz", ""], ["Ahn", "Keon-Hee", ""], ["Kim", "Jun-Hyuk", ""], ["Choi", "Jun-Ho", ""], ["Lee", "Jong-Seok", ""], ["Zhao", "Tongtong", ""], ["Zhao", "Shanshan", ""], ["Han", "Yoseob", ""], ["Kim", "Byung-Hoon", ""], ["Baek", "JaeHyun", ""], ["Wu", "Haoning", ""], ["Xu", "Dejia", ""], ["Zhou", "Bo", ""], ["Guan", "Wei", ""], ["Li", "Xiaobo", ""], ["Ye", "Chen", ""], ["Li", "Hao", ""], ["Zhong", "Haoyu", ""], ["Shi", "Yukai", ""], ["Yang", "Zhijing", ""], ["Yang", "Xiaojun", ""], ["Zhong", "Haoyu", ""], ["Li", "Xin", ""], ["Jin", "Xin", ""], ["Wu", "Yaojun", ""], ["Pang", "Yingxue", ""], ["Liu", "Sen", ""], ["Liu", "Zhi-Song", ""], ["Wang", "Li-Wen", ""], ["Li", "Chu-Tak", ""], ["Cani", "Marie-Paule", ""], ["Siu", "Wan-Chi", ""], ["Zhou", "Yuanbo", ""], ["Umer", "Rao Muhammad", ""], ["Micheloni", "Christian", ""], ["Cong", "Xiaofeng", ""], ["Gupta", "Rajat", ""], ["Ahn", "Keon-Hee", ""], ["Kim", "Jun-Hyuk", ""], ["Choi", "Jun-Ho", ""], ["Lee", "Jong-Seok", ""], ["Almasri", "Feras", ""], ["Vandamme", "Thomas", ""], ["Debeir", "Olivier", ""]]}, {"id": "2009.12088", "submitter": "Nicol\\`o Bonettini", "authors": "Sara Mandelli, Nicol\\`o Bonettini, Paolo Bestagini, Stefano Tubaro", "title": "Training CNNs in Presence of JPEG Compression: Multimedia Forensics vs\n  Computer Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have proved very accurate in multiple\ncomputer vision image classification tasks that required visual inspection in\nthe past (e.g., object recognition, face detection, etc.). Motivated by these\nastonishing results, researchers have also started using CNNs to cope with\nimage forensic problems (e.g., camera model identification, tampering\ndetection, etc.). However, in computer vision, image classification methods\ntypically rely on visual cues easily detectable by human eyes. Conversely,\nforensic solutions rely on almost invisible traces that are often very subtle\nand lie in the fine details of the image under analysis. For this reason,\ntraining a CNN to solve a forensic task requires some special care, as common\nprocessing operations (e.g., resampling, compression, etc.) can strongly hinder\nforensic traces. In this work, we focus on the effect that JPEG has on CNN\ntraining considering different computer vision and forensic image\nclassification problems. Specifically, we consider the issues that rise from\nJPEG compression and misalignment of the JPEG grid. We show that it is\nnecessary to consider these effects when generating a training dataset in order\nto properly train a forensic detector not losing generalization capability,\nwhereas it is almost possible to ignore these effects for computer vision\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 08:47:21 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Mandelli", "Sara", ""], ["Bonettini", "Nicol\u00f2", ""], ["Bestagini", "Paolo", ""], ["Tubaro", "Stefano", ""]]}, {"id": "2009.12111", "submitter": "Tung Le", "authors": "Hieu T. Nguyen, Tung T. Le, Thang V. Nguyen, Nhan T. Nguyen", "title": "Enhancing MRI Brain Tumor Segmentation with an Additional Classification\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain tumor segmentation plays an essential role in medical image analysis.\nIn recent studies, deep convolution neural networks (DCNNs) are extremely\npowerful to tackle tumor segmentation tasks. We propose in this paper a novel\ntraining method that enhances the segmentation results by adding an additional\nclassification branch to the network. The whole network was trained end-to-end\non the Multimodal Brain Tumor Segmentation Challenge (BraTS) 2020 training\ndataset. On the BraTS's validation set, it achieved an average Dice score of\n78.43%, 89.99%, and 84.22% respectively for the enhancing tumor, the whole\ntumor, and the tumor core.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 10:05:12 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 04:00:45 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Nguyen", "Hieu T.", ""], ["Le", "Tung T.", ""], ["Nguyen", "Thang V.", ""], ["Nguyen", "Nhan T.", ""]]}, {"id": "2009.12174", "submitter": "Micol Marchetti-Bowick", "authors": "Poornima Kaniarasu, Galen Clark Haynes, Micol Marchetti-Bowick", "title": "Goal-Directed Occupancy Prediction for Lane-Following Actors", "comments": "Published at ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the possible future behaviors of vehicles that drive on shared\nroads is a crucial task for safe autonomous driving. Many existing approaches\nto this problem strive to distill all possible vehicle behaviors into a\nsimplified set of high-level actions. However, these action categories do not\nsuffice to describe the full range of maneuvers possible in the complex road\nnetworks we encounter in the real world. To combat this deficiency, we propose\na new method that leverages the mapped road topology to reason over possible\ngoals and predict the future spatial occupancy of dynamic road actors. We show\nthat our approach is able to accurately predict future occupancy that remains\nconsistent with the mapped lane geometry and naturally captures multi-modality\nbased on the local scene context while also not suffering from the mode\ncollapse problem observed in prior work.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 20:44:59 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Kaniarasu", "Poornima", ""], ["Haynes", "Galen Clark", ""], ["Marchetti-Bowick", "Micol", ""]]}, {"id": "2009.12177", "submitter": "Baptiste Roziere", "authors": "Baptiste Roziere, Nathanal Carraz Rakotonirina, Vlad Hosu, Andry\n  Rasoanaivo, Hanhe Lin, Camille Couprie, Olivier Teytaud", "title": "Tarsier: Evolving Noise Injection in Super-Resolution GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution aims at increasing the resolution and level of detail within\nan image. The current state of the art in general single-image super-resolution\nis held by NESRGAN+, which injects a Gaussian noise after each residual layer\nat training time. In this paper, we harness evolutionary methods to improve\nNESRGAN+ by optimizing the noise injection at inference time. More precisely,\nwe use Diagonal CMA to optimize the injected noise according to a novel\ncriterion combining quality assessment and realism. Our results are validated\nby the PIRM perceptual score and a human study. Our method outperforms NESRGAN+\non several standard super-resolution datasets. More generally, our approach can\nbe used to optimize any method based on noise injection.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 12:29:16 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Roziere", "Baptiste", ""], ["Rakotonirina", "Nathanal Carraz", ""], ["Hosu", "Vlad", ""], ["Rasoanaivo", "Andry", ""], ["Lin", "Hanhe", ""], ["Couprie", "Camille", ""], ["Teytaud", "Olivier", ""]]}, {"id": "2009.12179", "submitter": "Chisom Ogbuanya", "authors": "Chisom Ezinne Ogbuanya", "title": "Improved Dimensionality Reduction of various Datasets using Novel\n  Multiplicative Factoring Principal Component Analysis (MPCA)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) is known to be the most widely applied\ndimensionality reduction approach. A lot of improvements have been done on the\ntraditional PCA, in order to obtain optimal results in the dimensionality\nreduction of various datasets. In this paper, we present an improvement to the\ntraditional PCA approach called Multiplicative factoring Principal Component\nAnalysis (MPCA). The advantage of MPCA over the traditional PCA is that a\npenalty is imposed on the occurrence space through a multiplier to make\nnegligible the effect of outliers in seeking out projections. Here we apply two\nmultiplier approaches, total distance and cosine similarity metrics. These two\napproaches can learn the relationship that exists between each of the data\npoints and the principal projections in the feature space. As a result of this,\nimproved low-rank projections are gotten through multiplying the data\niteratively to make negligible the effect of corrupt data in the training set.\nExperiments were carried out on YaleB, MNIST, AR, and Isolet datasets and the\nresults were compared to results gotten from some popular dimensionality\nreduction methods such as traditional PCA, RPCA-OM, and also some recently\npublished methods such as IFPCA-1 and IFPCA-2.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 12:30:15 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Ogbuanya", "Chisom Ezinne", ""]]}, {"id": "2009.12188", "submitter": "Veronica Vilaplana", "authors": "Laura Mora Ballestar and Veronica Vilaplana", "title": "Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation", "comments": "Pre-conference paper. Brain Tumor Segmentation (BraTS) Challenge 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automation of brain tumors in 3D magnetic resonance images (MRIs) is key to\nassess the diagnostic and treatment of the disease. In recent years,\nconvolutional neural networks (CNNs) have shown improved results in the task.\nHowever, high memory consumption is still a problem in 3D-CNNs. Moreover, most\nmethods do not include uncertainty information, which is specially critical in\nmedical diagnosis. This work proposes a 3D encoder-decoder architecture, based\non V-Net \\cite{vnet} which is trained with patching techniques to reduce memory\nconsumption and decrease the effect of unbalanced data. We also introduce\nvoxel-wise uncertainty, both epistemic and aleatoric using test-time dropout\nand data-augmentation respectively. Uncertainty maps can provide extra\ninformation to expert neurologists, useful for detecting when the model is not\nconfident on the provided segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 10:50:12 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Ballestar", "Laura Mora", ""], ["Vilaplana", "Veronica", ""]]}, {"id": "2009.12193", "submitter": "Xiaoqiong Huang", "authors": "Xiaoqiong Huang, Zejian Chen, Xin Yang, Zhendong Liu, Yuxin Zou,\n  Mingyuan Luo, Wufeng Xue, Dong Ni", "title": "Style-invariant Cardiac Image Segmentation with Test-time Augmentation", "comments": "Accepted by MICCAI STACOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep models often suffer from severe performance drop due to the appearance\nshift in the real clinical setting. Most of the existing learning-based methods\nrely on images from multiple sites/vendors or even corresponding labels.\nHowever, collecting enough unknown data to robustly model segmentation cannot\nalways hold since the complex appearance shift caused by imaging factors in\ndaily application. In this paper, we propose a novel style-invariant method for\ncardiac image segmentation. Based on the zero-shot style transfer to remove\nappearance shift and test-time augmentation to explore diverse underlying\nanatomy, our proposed method is effective in combating the appearance shift.\nOur contribution is three-fold. First, inspired by the spirit of universal\nstyle transfer, we develop a zero-shot stylization for content images to\ngenerate stylized images that appearance similarity to the style images.\nSecond, we build up a robust cardiac segmentation model based on the U-Net\nstructure. Our framework mainly consists of two networks during testing: the ST\nnetwork for removing appearance shift and the segmentation network. Third, we\ninvestigate test-time augmentation to explore transformed versions of the\nstylized image for prediction and the results are merged. Notably, our proposed\nframework is fully test-time adaptation. Experiment results demonstrate that\nour methods are promising and generic for generalizing deep segmentation\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 08:27:40 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Huang", "Xiaoqiong", ""], ["Chen", "Zejian", ""], ["Yang", "Xin", ""], ["Liu", "Zhendong", ""], ["Zou", "Yuxin", ""], ["Luo", "Mingyuan", ""], ["Xue", "Wufeng", ""], ["Ni", "Dong", ""]]}, {"id": "2009.12232", "submitter": "Zhangxuan Gu", "authors": "Zhangxuan Gu, Siyuan Zhou, Li Niu, Zihan Zhao, Liqing Zhang", "title": "From Pixel to Patch: Synthesize Context-aware Features for Zero-shot\n  Semantic Segmentation", "comments": "submitted to the TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning has been actively studied for image classification task to\nrelieve the burden of annotating image labels. Interestingly, semantic\nsegmentation task requires more labor-intensive pixel-wise annotation, but\nzero-shot semantic segmentation has only attracted limited research interest.\nThus, we focus on zero-shot semantic segmentation, which aims to segment unseen\nobjects with only category-level semantic representations provided for unseen\ncategories. In this paper, we propose a novel Context-aware feature Generation\nNetwork (CaGNet), which can synthesize context-aware pixel-wise visual features\nfor unseen categories based on category-level semantic representations and\npixel-wise contextual information. The synthesized features are used to\nfinetune the classifier to enable segmenting unseen objects. Furthermore, we\nextend pixel-wise feature generation and finetuning to patch-wise feature\ngeneration and finetuning, which additionally considers inter-pixel\nrelationship. Experimental results on Pascal-VOC, Pascal-Context, and\nCOCO-stuff show that our method significantly outperforms the existing\nzero-shot semantic segmentation methods. Code is available at\nhttps://github.com/bcmi/CaGNetv2-Zero-Shot-Semantic-Segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 13:26:30 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 05:17:03 GMT"}, {"version": "v3", "created": "Sat, 30 Jan 2021 12:21:13 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Gu", "Zhangxuan", ""], ["Zhou", "Siyuan", ""], ["Niu", "Li", ""], ["Zhao", "Zihan", ""], ["Zhang", "Liqing", ""]]}, {"id": "2009.12252", "submitter": "Joan Alexis Glaun\\`es", "authors": "Pierre-Louis Antonsanti, Thomas Benseghir, Vincent Jugnon, Joan\n  Glaun\\`es", "title": "Database Annotation with few Examples: An Atlas-based Framework using\n  Diffeomorphic Registration of 3D Trees", "comments": "Medical Image Computing and Computer Assisted Intervention (MICCAI)\n  conference, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic annotation of anatomical structures can help simplify workflow\nduring interventions in numerous clinical applications but usually involves a\nlarge amount of annotated data. The complexity of the labeling task, together\nwith the lack of representative data, slows down the development of robust\nsolutions. In this paper, we propose a solution requiring very few annotated\ncases to label 3D pelvic arterial trees of patients with benign prostatic\nhyperplasia. We take advantage of Large Deformation Diffeomorphic Metric\nMapping (LDDMM) to perform registration based on meaningful deformations from\nwhich we build an atlas. Branch pairing is then computed from the atlas to new\ncases using optimal transport to ensure one-to-one correspondence during the\nlabeling process. To tackle topological variations in the tree, which usually\ndegrades the performance of atlas-based techniques, we propose a simple\nbottom-up label assignment adapted to the pelvic anatomy. The proposed method\nachieves 97.6\\% labeling precision with only 5 cases for training, while in\ncomparison learning-based methods only reach 82.2\\% on such small training\nsets.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 14:10:52 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Antonsanti", "Pierre-Louis", ""], ["Benseghir", "Thomas", ""], ["Jugnon", "Vincent", ""], ["Glaun\u00e8s", "Joan", ""]]}, {"id": "2009.12276", "submitter": "Juncong Fei", "authors": "Juncong Fei, Wenbo Chen, Philipp Heidenreich, Sascha Wirges, Christoph\n  Stiller", "title": "SemanticVoxels: Sequential Fusion for 3D Pedestrian Detection using\n  LiDAR Point Cloud and Semantic Segmentation", "comments": "Accepted to present in the 2020 IEEE International Conference on\n  Multisensor Fusion and Integration (MFI 2020)", "journal-ref": null, "doi": "10.1109/MFI49285.2020.9235240", "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D pedestrian detection is a challenging task in automated driving because\npedestrians are relatively small, frequently occluded and easily confused with\nnarrow vertical objects. LiDAR and camera are two commonly used sensor\nmodalities for this task, which should provide complementary information.\nUnexpectedly, LiDAR-only detection methods tend to outperform multisensor\nfusion methods in public benchmarks. Recently, PointPainting has been presented\nto eliminate this performance drop by effectively fusing the output of a\nsemantic segmentation network instead of the raw image information. In this\npaper, we propose a generalization of PointPainting to be able to apply fusion\nat different levels. After the semantic augmentation of the point cloud, we\nencode raw point data in pillars to get geometric features and semantic point\ndata in voxels to get semantic features and fuse them in an effective way.\nExperimental results on the KITTI test set show that SemanticVoxels achieves\nstate-of-the-art performance in both 3D and bird's eye view pedestrian\ndetection benchmarks. In particular, our approach demonstrates its strength in\ndetecting challenging pedestrian cases and outperforms current state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 14:52:32 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Fei", "Juncong", ""], ["Chen", "Wenbo", ""], ["Heidenreich", "Philipp", ""], ["Wirges", "Sascha", ""], ["Stiller", "Christoph", ""]]}, {"id": "2009.12280", "submitter": "Raghavendra Selvan", "authors": "Raghavendra Selvan, Silas {\\O}rting, Erik B Dam", "title": "Locally orderless tensor networks for classifying two- and\n  three-dimensional medical images", "comments": "Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) (see https://melba-journal.org). Source code at\n  https://github.com/raghavian/LoTeNet_pytorch/", "journal-ref": "Journal of Machine Learning for Biomedical Imaging. 2021:5. pp\n  1-21. Special Issue: Medical Imaging with Deep Learning (MIDL) 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tensor networks are factorisations of high rank tensors into networks of\nlower rank tensors and have primarily been used to analyse quantum many-body\nproblems. Tensor networks have seen a recent surge of interest in relation to\nsupervised learning tasks with a focus on image classification. In this work,\nwe improve upon the matrix product state (MPS) tensor networks that can operate\non one-dimensional vectors to be useful for working with 2D and 3D medical\nimages. We treat small image regions as orderless, squeeze their spatial\ninformation into feature dimensions and then perform MPS operations on these\nlocally orderless regions. These local representations are then aggregated in a\nhierarchical manner to retain global structure. The proposed locally orderless\ntensor network (LoTeNet) is compared with relevant methods on three datasets.\nThe architecture of LoTeNet is fixed in all experiments and we show it requires\nlesser computational resources to attain performance on par or superior to the\ncompared methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 15:05:02 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 20:45:47 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Selvan", "Raghavendra", ""], ["\u00d8rting", "Silas", ""], ["Dam", "Erik B", ""]]}, {"id": "2009.12312", "submitter": "Simon Baeuerle", "authors": "Simon Baeuerle, Jonas Barth, Elton Renato Tavares de Menezes, Andreas\n  Steimer, Ralf Mikut", "title": "CAD2Real: Deep learning with domain randomization of CAD data for 3D\n  pose estimation of electronic control unit housings", "comments": "Proc. 30. Workshop Computational Intelligence, Berlin, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic control units (ECUs) are essential for many automobile components,\ne.g. engine, anti-lock braking system (ABS), steering and airbags. For some\nproducts, the 3D pose of each single ECU needs to be determined during series\nproduction. Deep learning approaches can not easily be applied to this problem,\nbecause labeled training data is not available in sufficient numbers. Thus, we\ntrain state-of-the-art artificial neural networks (ANNs) on purely synthetic\ntraining data, which is automatically created from a single CAD file. By\nrandomizing parameters during rendering of training images, we enable inference\non RGB images of a real sample part. In contrast to classic image processing\napproaches, this data-driven approach poses only few requirements regarding the\nmeasurement setup and transfers to related use cases with little development\neffort.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 16:08:16 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Baeuerle", "Simon", ""], ["Barth", "Jonas", ""], ["de Menezes", "Elton Renato Tavares", ""], ["Steimer", "Andreas", ""], ["Mikut", "Ralf", ""]]}, {"id": "2009.12313", "submitter": "Iacer Calixto", "authors": "Victor Milewski and Marie-Francine Moens and Iacer Calixto", "title": "Are scene graphs good enough to improve Image Captioning?", "comments": "Published at AACL-IJCNLP 2020. 12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many top-performing image captioning models rely solely on object features\ncomputed with an object detection model to generate image descriptions.\nHowever, recent studies propose to directly use scene graphs to introduce\ninformation about object relations into captioning, hoping to better describe\ninteractions between objects. In this work, we thoroughly investigate the use\nof scene graphs in image captioning. We empirically study whether using\nadditional scene graph encoders can lead to better image descriptions and\npropose a conditional graph attention network (C-GAT), where the image\ncaptioning decoder state is used to condition the graph updates. Finally, we\ndetermine to what extent noise in the predicted scene graphs influence caption\nquality. Overall, we find no significant difference between models that use\nscene graph features and models that only use object detection features across\ndifferent captioning metrics, which suggests that existing scene graph\ngeneration models are still too noisy to be useful in image captioning.\nMoreover, although the quality of predicted scene graphs is very low in\ngeneral, when using high quality scene graphs we obtain gains of up to 3.3\nCIDEr compared to a strong Bottom-Up Top-Down baseline. We open source code to\nreproduce all our experiments in\nhttps://github.com/iacercalixto/butd-image-captioning.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 16:09:08 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 17:55:55 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Milewski", "Victor", ""], ["Moens", "Marie-Francine", ""], ["Calixto", "Iacer", ""]]}, {"id": "2009.12318", "submitter": "John Wu", "authors": "John F. Wu and J. E. G. Peek", "title": "Predicting galaxy spectra from images with hybrid convolutional neural\n  networks", "comments": "5 pages, 2 figures, accepted to the Machine Learning and the Physical\n  Sciences workshop at NeurIPS 2020. Code available at\n  https://github.com/jwuphysics/predicting-spectra-from-images/", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Galaxies can be described by features of their optical spectra such as oxygen\nemission lines, or morphological features such as spiral arms. Although\nspectroscopy provides a rich description of the physical processes that govern\ngalaxy evolution, spectroscopic data are observationally expensive to obtain.\nFor the first time, we are able to robustly predict galaxy spectra directly\nfrom broad-band imaging. We present a powerful new approach using a hybrid\nconvolutional neural network with deconvolution instead of batch normalization;\nthis hybrid CNN outperforms other models in our tests. The learned mapping\nbetween galaxy imaging and spectra will be transformative for future wide-field\nsurveys, such as with the Vera C. Rubin Observatory and Nancy Grace Roman Space\nTelescope, by multiplying the scientific returns for spectroscopically-limited\ngalaxy samples.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 16:16:16 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 18:21:16 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wu", "John F.", ""], ["Peek", "J. E. G.", ""]]}, {"id": "2009.12339", "submitter": "Radu Tudor Ionescu", "authors": "Adrian Sandru, Georgian-Emilian Duta, Mariana-Iuliana Georgescu, Radu\n  Tudor Ionescu", "title": "SuPEr-SAM: Using the Supervision Signal from a Pose Estimator to Train a\n  Spatial Attention Module for Personal Protective Equipment Recognition", "comments": "Accepted at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep learning method to automatically detect personal protective\nequipment (PPE), such as helmets, surgical masks, reflective vests, boots and\nso on, in images of people. Typical approaches for PPE detection based on deep\nlearning are (i) to train an object detector for items such as those listed\nabove or (ii) to train a person detector and a classifier that takes the\nbounding boxes predicted by the detector and discriminates between people\nwearing and people not wearing the corresponding PPE items. We propose a novel\nand accurate approach that uses three components: a person detector, a body\npose estimator and a classifier. Our novelty consists in using the pose\nestimator only at training time, to improve the prediction performance of the\nclassifier. We modify the neural architecture of the classifier by adding a\nspatial attention mechanism, which is trained using supervision signal from the\npose estimator. In this way, the classifier learns to focus on PPE items, using\nknowledge from the pose estimator with almost no computational overhead during\ninference.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 16:58:18 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 12:59:03 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Sandru", "Adrian", ""], ["Duta", "Georgian-Emilian", ""], ["Georgescu", "Mariana-Iuliana", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "2009.12395", "submitter": "Mohammad Keshavarzi", "authors": "Mohammad Keshavarzi, Aakash Parikh, Xiyu Zhai, Melody Mao, Luisa\n  Caldas, Allen Y. Yang", "title": "SceneGen: Generative Contextual Scene Augmentation using Scene Graph\n  Priors", "comments": "19 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial computing experiences are constrained by the real-world surroundings\nof the user. In such experiences, augmenting virtual objects to existing scenes\nrequire a contextual approach, where geometrical conflicts are avoided, and\nfunctional and plausible relationships to other objects are maintained in the\ntarget environment. Yet, due to the complexity and diversity of user\nenvironments, automatically calculating ideal positions of virtual content that\nis adaptive to the context of the scene is considered a challenging task.\nMotivated by this problem, in this paper we introduce SceneGen, a generative\ncontextual augmentation framework that predicts virtual object positions and\norientations within existing scenes. SceneGen takes a semantically segmented\nscene as input, and outputs positional and orientational probability maps for\nplacing virtual content. We formulate a novel spatial Scene Graph\nrepresentation, which encapsulates explicit topological properties between\nobjects, object groups, and the room. We believe providing explicit and\nintuitive features plays an important role in informative content creation and\nuser interaction of spatial computing settings, a quality that is not captured\nin implicit models. We use kernel density estimation (KDE) to build a\nmultivariate conditional knowledge model trained using prior spatial Scene\nGraphs extracted from real-world 3D scanned data. To further capture\norientational properties, we develop a fast pose annotation tool to extend\ncurrent real-world datasets with orientational labels. Finally, to demonstrate\nour system in action, we develop an Augmented Reality application, in which\nobjects can be contextually augmented in real-time.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 18:36:27 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 17:06:05 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Keshavarzi", "Mohammad", ""], ["Parikh", "Aakash", ""], ["Zhai", "Xiyu", ""], ["Mao", "Melody", ""], ["Caldas", "Luisa", ""], ["Yang", "Allen Y.", ""]]}, {"id": "2009.12404", "submitter": "Yanpeng Zhao", "authors": "Yanpeng Zhao and Ivan Titov", "title": "Visually Grounded Compound PCFGs", "comments": "Accepted to EMNLP 2020. Our code is available at\n  https://github.com/zhaoyanpeng/vpcfg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Exploiting visual groundings for language understanding has recently been\ndrawing much attention. In this work, we study visually grounded grammar\ninduction and learn a constituency parser from both unlabeled text and its\nvisual groundings. Existing work on this task (Shi et al., 2019) optimizes a\nparser via Reinforce and derives the learning signal only from the alignment of\nimages and sentences. While their model is relatively accurate overall, its\nerror distribution is very uneven, with low performance on certain constituents\ntypes (e.g., 26.2% recall on verb phrases, VPs) and high on others (e.g., 79.6%\nrecall on noun phrases, NPs). This is not surprising as the learning signal is\nlikely insufficient for deriving all aspects of phrase-structure syntax and\ngradient estimates are noisy. We show that using an extension of probabilistic\ncontext-free grammar model we can do fully-differentiable end-to-end visually\ngrounded learning. Additionally, this enables us to complement the image-text\nalignment loss with a language modeling objective. On the MSCOCO test captions,\nour model establishes a new state of the art, outperforming its non-grounded\nversion and, thus, confirming the effectiveness of visual groundings in\nconstituency grammar induction. It also substantially outperforms the previous\ngrounded model, with largest improvements on more `abstract' categories (e.g.,\n+55.1% recall on VPs).\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 19:07:00 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zhao", "Yanpeng", ""], ["Titov", "Ivan", ""]]}, {"id": "2009.12419", "submitter": "Mikhail Romanov", "authors": "Mikhail Romanov, Nikolay Patatkin, Anna Vorontsova, Sergey Nikolenko,\n  Anton Konushin, Dmitry Senyushkin", "title": "Towards General Purpose Geometry-Preserving Single-View Depth Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-view depth estimation (SVDE) plays a crucial role in scene\nunderstanding for AR applications, 3D modeling, and robotics, providing the\ngeometry of a scene based on a single image. Recent works have shown that a\nsuccessful solution strongly relies on the diversity and volume of training\ndata. This data can be sourced from stereo movies and photos. However, they do\nnot provide geometrically complete depth maps (as disparities contain unknown\nshift value). Therefore, existing models trained on this data are not able to\nrecover correct 3D representations. Our work shows that a model trained on this\ndata along with conventional datasets can gain accuracy while predicting\ncorrect scene geometry. Surprisingly, only a small portion of geometrically\ncorrect depth maps are required to train a model that performs equally to a\nmodel trained on the full geometrically correct dataset. After that, we train\ncomputationally efficient models on a mixture of datasets using the proposed\nmethod. Through quantitative comparison on completely unseen datasets and\nqualitative comparison of 3D point clouds, we show that our model defines the\nnew state of the art in general-purpose SVDE.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 20:06:13 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 20:30:35 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Romanov", "Mikhail", ""], ["Patatkin", "Nikolay", ""], ["Vorontsova", "Anna", ""], ["Nikolenko", "Sergey", ""], ["Konushin", "Anton", ""], ["Senyushkin", "Dmitry", ""]]}, {"id": "2009.12433", "submitter": "Milad Abdollahzadeh", "authors": "Hamdollah Nasrollahi, Kamran Farajzadeh, Vahid Hosseini, Esmaeil\n  Zarezadeh, Milad Abdollahzadeh", "title": "Deep Artifact-Free Residual Network for Single Image Super-Resolution", "comments": "8 pages", "journal-ref": "SIViP 14, 407-415 (2020)", "doi": "10.1007/s11760-019-01569-3", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, convolutional neural networks have shown promising performance for\nsingle-image super-resolution. In this paper, we propose Deep Artifact-Free\nResidual (DAFR) network which uses the merits of both residual learning and\nusage of ground-truth image as target. Our framework uses a deep model to\nextract the high-frequency information which is necessary for high-quality\nimage reconstruction. We use a skip-connection to feed the low-resolution image\nto the network before the image reconstruction. In this way, we are able to use\nthe ground-truth images as target and avoid misleading the network due to\nartifacts in difference image. In order to extract clean high-frequency\ninformation, we train the network in two steps. The first step is a traditional\nresidual learning which uses the difference image as target. Then, the trained\nparameters of this step are transferred to the main training in the second\nstep. Our experimental results show that the proposed method achieves better\nquantitative and qualitative image quality compared to the existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 20:53:55 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Nasrollahi", "Hamdollah", ""], ["Farajzadeh", "Kamran", ""], ["Hosseini", "Vahid", ""], ["Zarezadeh", "Esmaeil", ""], ["Abdollahzadeh", "Milad", ""]]}, {"id": "2009.12434", "submitter": "G M Mashrur E Elahi", "authors": "G M Mashrur E Elahi, Yee-Hong Yang", "title": "Online Learnable Keyframe Extraction in Videos and its Application with\n  Semantic Word Vector in Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video processing has become a popular research direction in computer vision\ndue to its various applications such as video summarization, action\nrecognition, etc. Recently, deep learning-based methods have achieved\nimpressive results in action recognition. However, these methods need to\nprocess a full video sequence to recognize the action, even though most of\nthese frames are similar and non-essential to recognizing a particular action.\nAdditionally, these non-essential frames increase the computational cost and\ncan confuse a method in action recognition. Instead, the important frames\ncalled keyframes not only are helpful in the recognition of an action but also\ncan reduce the processing time of each video sequence for classification or in\nother applications, e.g. summarization. As well, current methods in video\nprocessing have not yet been demonstrated in an online fashion.\n  Motivated by the above, we propose an online learnable module for keyframe\nextraction. This module can be used to select key-shots in video and thus can\nbe applied to video summarization. The extracted keyframes can be used as input\nto any deep learning-based classification model to recognize action. We also\npropose a plugin module to use the semantic word vector as input along with\nkeyframes and a novel train/test strategy for the classification models. To our\nbest knowledge, this is the first time such an online module and train/test\nstrategy have been proposed.\n  The experimental results on many commonly used datasets in video\nsummarization and in action recognition have shown impressive results using the\nproposed module.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 20:54:46 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Elahi", "G M Mashrur E", ""], ["Yang", "Yee-Hong", ""]]}, {"id": "2009.12437", "submitter": "Vikash Gupta", "authors": "Vikash Gupta1 and Holger Roth and Varun Buch3 and Marcio A.B.C.\n  Rockenbach and Richard D White and Dong Yang and Olga Laur and Brian\n  Ghoshhajra and Ittai Dayan and Daguang Xu and Mona G. Flores and Barbaros\n  Selnur Erdal", "title": "Democratizing Artificial Intelligence in Healthcare: A Study of Model\n  Development Across Two Institutions Incorporating Transfer Learning", "comments": "8 pages, 5 figures, pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training of deep learning models typically requires extensive data, which\nare not readily available as large well-curated medical-image datasets for\ndevelopment of artificial intelligence (AI) models applied in Radiology.\nRecognizing the potential for transfer learning (TL) to allow a fully trained\nmodel from one institution to be fine-tuned by another institution using a much\nsmall local dataset, this report describes the challenges, methodology, and\nbenefits of TL within the context of developing an AI model for a basic\nuse-case, segmentation of Left Ventricular Myocardium (LVM) on images from\n4-dimensional coronary computed tomography angiography. Ultimately, our results\nfrom comparisons of LVM segmentation predicted by a model locally trained using\nrandom initialization, versus one training-enhanced by TL, showed that a\nuse-case model initiated by TL can be developed with sparse labels with\nacceptable performance. This process reduces the time required to build a new\nmodel in the clinical environment at a different institution.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 21:12:50 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Gupta1", "Vikash", ""], ["Roth", "Holger", ""], ["Buch3", "Varun", ""], ["Rockenbach", "Marcio A. B. C.", ""], ["White", "Richard D", ""], ["Yang", "Dong", ""], ["Laur", "Olga", ""], ["Ghoshhajra", "Brian", ""], ["Dayan", "Ittai", ""], ["Xu", "Daguang", ""], ["Flores", "Mona G.", ""], ["Erdal", "Barbaros Selnur", ""]]}, {"id": "2009.12461", "submitter": "Dong Huo", "authors": "Dong Huo, Yee-Hong Yang", "title": "Blind Image Super-Resolution with Spatial Context Hallucination", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolution neural networks (CNNs) play a critical role in single image\nsuper-resolution (SISR) since the amazing improvement of high performance\ncomputing. However, most of the super-resolution (SR) methods only focus on\nrecovering bicubic degradation. Reconstructing high-resolution (HR) images from\nrandomly blurred and noisy low-resolution (LR) images is still a challenging\nproblem. In this paper, we propose a novel Spatial Context Hallucination\nNetwork (SCHN) for blind super-resolution without knowing the degradation\nkernel. We find that when the blur kernel is unknown, separate deblurring and\nsuper-resolution could limit the performance because of the accumulation of\nerror. Thus, we integrate denoising, deblurring and super-resolution within one\nframework to avoid such a problem. We train our model on two high quality\ndatasets, DIV2K and Flickr2K. Our method performs better than state-of-the-art\nmethods when input images are corrupted with random blur and noise.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 22:36:07 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Huo", "Dong", ""], ["Yang", "Yee-Hong", ""]]}, {"id": "2009.12466", "submitter": "Mohamed Abdelkhalek", "authors": "Mohamed Abdelkhalek, Heba Aguib, Mohamed Moustafa, Khalil Elkhodary", "title": "Enhanced 3D Myocardial Strain Estimation from Multi-View 2D CMR Imaging", "comments": "This a preprint of original research work", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an enhanced 3D myocardial strain estimation\nprocedure, which combines complementary displacement information from multiple\norientations of a single imaging modality (untagged CMR SSFP images). To\nestimate myocardial strain across the left ventricle, we register the sets of\nshort-axis, four-chamber and two-chamber views via a 2D non-rigid registration\nalgorithm implemented in a commercial software (Segment, Medviso). We then\ncreate a series of interpolating functions for the three orthogonal directions\nof motion and use them to deform a tetrahedral mesh representation of a\npatient-specific left ventricle. Additionally, we correct for overestimation of\ndisplacement by introducing a weighting scheme that is based on displacement\nalong the long axis. The procedure was evaluated on the STACOM 2011 dataset\ncontaining CMR SSFP images for 16 healthy volunteers. We show increased\naccuracy in estimating the three strain components (radial, circumferential,\nlongitudinal) compared to reported results in the challenge, for the imaging\nmodality of interest (SSFP). Our peak strain estimates are also significantly\ncloser to reported measurements from studies of a larger cohort in the\nliterature and our own ground truth measurements using Segment Strain Analysis\nModule. Our proposed procedure provides a relatively fast and simple method to\nimprove 2D tracking results, with the added flexibility in either deforming a\nreconstructed mesh model from other image modalities or using the built-in CMR\nmesh reconstruction procedure. Our, proposed scheme presents a deforming\npatient-specific model of the left ventricle, using the commonest imaging\nmodality , routinely administered in clinical settings, without requiring\nadditional or specialized imaging protocols.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 22:47:50 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 02:58:47 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Abdelkhalek", "Mohamed", ""], ["Aguib", "Heba", ""], ["Moustafa", "Mohamed", ""], ["Elkhodary", "Khalil", ""]]}, {"id": "2009.12473", "submitter": "Deying Kong", "authors": "Deying Kong, Haoyu Ma, Xiaohui Xie", "title": "SIA-GCN: A Spatial Information Aware Graph Neural Network with 2D\n  Convolutions for Hand Pose Estimation", "comments": "31st British Machine Vision Conference (BMVC), oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) generalize neural networks from applications on\nregular structures to applications on arbitrary graphs, and have shown success\nin many application domains such as computer vision, social networks and\nchemistry. In this paper, we extend GNNs along two directions: a) allowing\nfeatures at each node to be represented by 2D spatial confidence maps instead\nof 1D vectors; and b) proposing an efficient operation to integrate information\nfrom neighboring nodes through 2D convolutions with different learnable kernels\nat each edge. The proposed SIA-GCN can efficiently extract spatial information\nfrom 2D maps at each node and propagate them through graph convolution. By\nassociating each edge with a designated convolution kernel, the SIA-GCN could\ncapture different spatial relationships for different pairs of neighboring\nnodes. We demonstrate the utility of SIA-GCN on the task of estimating hand\nkeypoints from single-frame images, where the nodes represent the 2D coordinate\nheatmaps of keypoints and the edges denote the kinetic relationships between\nkeypoints. Experiments on multiple datasets show that SIA-GCN provides a\nflexible and yet powerful framework to account for structural constraints\nbetween keypoints, and can achieve state-of-the-art performance on the task of\nhand pose estimation.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 23:23:09 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Kong", "Deying", ""], ["Ma", "Haoyu", ""], ["Xie", "Xiaohui", ""]]}, {"id": "2009.12478", "submitter": "Joshua Galita", "authors": "Sumeet Menon (1), Joshua Galita (1), David Chapman (1), Aryya\n  Gangopadhyay (1), Jayalakshmi Mangalagiri (1), Phuong Nguyen (1), Yaacov\n  Yesha (1), Yelena Yesha (1), Babak Saboury (1 and 2), Michael Morris (1, 2,\n  and 3) ((1) University of Maryland, Baltimore County, (2) National Institutes\n  of Health Clinical Center, (3) Networking Health)", "title": "Generating Realistic COVID19 X-rays with a Mean Teacher + Transfer\n  Learning GAN", "comments": "10 pages, 11 figures, 2 tables; Submitted to IEEE BigData 2020\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 is a novel infectious disease responsible for over 800K deaths\nworldwide as of August 2020. The need for rapid testing is a high priority and\nalternative testing strategies including X-ray image classification are a\npromising area of research. However, at present, public datasets for COVID19\nx-ray images have low data volumes, making it challenging to develop accurate\nimage classifiers. Several recent papers have made use of Generative\nAdversarial Networks (GANs) in order to increase the training data volumes. But\nrealistic synthetic COVID19 X-rays remain challenging to generate. We present a\nnovel Mean Teacher + Transfer GAN (MTT-GAN) that generates COVID19 chest X-ray\nimages of high quality. In order to create a more accurate GAN, we employ\ntransfer learning from the Kaggle Pneumonia X-Ray dataset, a highly relevant\ndata source orders of magnitude larger than public COVID19 datasets.\nFurthermore, we employ the Mean Teacher algorithm as a constraint to improve\nstability of training. Our qualitative analysis shows that the MTT-GAN\ngenerates X-ray images that are greatly superior to a baseline GAN and visually\ncomparable to real X-rays. Although board-certified radiologists can\ndistinguish MTT-GAN fakes from real COVID19 X-rays. Quantitative analysis shows\nthat MTT-GAN greatly improves the accuracy of both a binary COVID19 classifier\nas well as a multi-class Pneumonia classifier as compared to a baseline GAN.\nOur classification accuracy is favourable as compared to recently reported\nresults in the literature for similar binary and multi-class COVID19 screening\ntasks.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 00:05:06 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Menon", "Sumeet", "", "1 and 2"], ["Galita", "Joshua", "", "1 and 2"], ["Chapman", "David", "", "1 and 2"], ["Gangopadhyay", "Aryya", "", "1 and 2"], ["Mangalagiri", "Jayalakshmi", "", "1 and 2"], ["Nguyen", "Phuong", "", "1 and 2"], ["Yesha", "Yaacov", "", "1 and 2"], ["Yesha", "Yelena", "", "1 and 2"], ["Saboury", "Babak", "", "1 and 2"], ["Morris", "Michael", "", "1, 2,\n  and 3"]]}, {"id": "2009.12507", "submitter": "Tai-Xiang Jiang", "authors": "Tai-Xiang Jiang, Xi-Le Zhao, Hao Zhang, Michael K. Ng", "title": "Dictionary Learning with Low-rank Coding Coefficients for Tensor\n  Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel tensor learning and coding model for\nthird-order data completion. Our model is to learn a data-adaptive dictionary\nfrom the given observations, and determine the coding coefficients of\nthird-order tensor tubes. In the completion process, we minimize the\nlow-rankness of each tensor slice containing the coding coefficients. By\ncomparison with the traditional pre-defined transform basis, the advantages of\nthe proposed model are that (i) the dictionary can be learned based on the\ngiven data observations so that the basis can be more adaptively and accurately\nconstructed, and (ii) the low-rankness of the coding coefficients can allow the\nlinear combination of dictionary features more effectively. Also we develop a\nmulti-block proximal alternating minimization algorithm for solving such tensor\nlearning and coding model, and show that the sequence generated by the\nalgorithm can globally converge to a critical point. Extensive experimental\nresults for real data sets such as videos, hyperspectral images, and traffic\ndata are reported to demonstrate these advantages and show the performance of\nthe proposed tensor learning and coding method is significantly better than the\nother tensor completion methods in terms of several evaluation metrics.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 02:43:43 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 09:36:33 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Jiang", "Tai-Xiang", ""], ["Zhao", "Xi-Le", ""], ["Zhang", "Hao", ""], ["Ng", "Michael K.", ""]]}, {"id": "2009.12516", "submitter": "Rijun Liao", "authors": "Rijun Liao, Weizhi An, Shiqi Yu, Zhu Li, Yongzhen Huang", "title": "Dense-View GEIs Set: View Space Covering for Gait Recognition based on\n  Dense-View GAN", "comments": "Accepted for presentation at IJCB'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait recognition has proven to be effective for long-distance human\nrecognition. But view variance of gait features would change human appearance\ngreatly and reduce its performance. Most existing gait datasets usually collect\ndata with a dozen different angles, or even more few. Limited view angles would\nprevent learning better view invariant feature. It can further improve\nrobustness of gait recognition if we collect data with various angles at 1\ndegree interval. But it is time consuming and labor consuming to collect this\nkind of dataset. In this paper, we, therefore, introduce a Dense-View GEIs Set\n(DV-GEIs) to deal with the challenge of limited view angles. This set can cover\nthe whole view space, view angle from 0 degree to 180 degree with 1 degree\ninterval. In addition, Dense-View GAN (DV-GAN) is proposed to synthesize this\ndense view set. DV-GAN consists of Generator, Discriminator and Monitor, where\nMonitor is designed to preserve human identification and view information. The\nproposed method is evaluated on the CASIA-B and OU-ISIR dataset. The\nexperimental results show that DV-GEIs synthesized by DV-GAN is an effective\nway to learn better view invariant feature. We believe the idea of dense view\ngenerated samples will further improve the development of gait recognition.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 04:42:46 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Liao", "Rijun", ""], ["An", "Weizhi", ""], ["Yu", "Shiqi", ""], ["Li", "Zhu", ""], ["Huang", "Yongzhen", ""]]}, {"id": "2009.12518", "submitter": "Serban Stan", "authors": "Serban Stan, Mohammad Rostami", "title": "Unsupervised Model Adaptation for Continual Semantic Segmentation", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an algorithm for adapting a semantic segmentation model that is\ntrained using a labeled source domain to generalize well in an unlabeled target\ndomain. A similar problem has been studied extensively in the unsupervised\ndomain adaptation (UDA) literature, but existing UDA algorithms require access\nto both the source domain labeled data and the target domain unlabeled data for\ntraining a domain agnostic semantic segmentation model. Relaxing this\nconstraint enables a user to adapt pretrained models to generalize in a target\ndomain, without requiring access to source data. To this end, we learn a\nprototypical distribution for the source domain in an intermediate embedding\nspace. This distribution encodes the abstract knowledge that is learned from\nthe source domain. We then use this distribution for aligning the target domain\ndistribution with the source domain distribution in the embedding space. We\nprovide theoretical analysis and explain conditions under which our algorithm\nis effective. Experiments on benchmark adaptation task demonstrate our method\nachieves competitive performance even compared with joint UDA approaches.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 04:55:50 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 08:09:52 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Stan", "Serban", ""], ["Rostami", "Mohammad", ""]]}, {"id": "2009.12524", "submitter": "Zanyar Zohourianshahzadi Ph.D. Candidate", "authors": "Zanyar Zohourianshahzadi (UCCS) and Jugal Kumar Kalita (UCCS)", "title": "Neural Twins Talk", "comments": "Copyright 2020 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "Proceeding of 2020 IEEE International Conference on Humanized\n  Computing and Communication with Artificial Intelligence (HCCAI)", "doi": "10.1109/HCCAI49649.2020.00009", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by how the human brain employs more neural pathways when increasing\nthe focus on a subject, we introduce a novel twin cascaded attention model that\noutperforms a state-of-the-art image captioning model that was originally\nimplemented using one channel of attention for the visual grounding task.\nVisual grounding ensures the existence of words in the caption sentence that\nare grounded into a particular region in the input image. After a deep learning\nmodel is trained on visual grounding task, the model employs the learned\npatterns regarding the visual grounding and the order of objects in the caption\nsentences, when generating captions. We report the results of our experiments\nin three image captioning tasks on the COCO dataset. The results are reported\nusing standard image captioning metrics to show the improvements achieved by\nour model over the previous image captioning model. The results gathered from\nour experiments suggest that employing more parallel attention pathways in a\ndeep neural network leads to higher performance. Our implementation of NTT is\npublicly available at: https://github.com/zanyarz/NeuralTwinsTalk.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 06:58:58 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Zohourianshahzadi", "Zanyar", "", "UCCS"], ["Kalita", "Jugal Kumar", "", "UCCS"]]}, {"id": "2009.12537", "submitter": "Jing Jin", "authors": "Jing Jin and Junhui Hou and Zhiyu Zhu and Jie Chen and Sam Kwong", "title": "Deep Selective Combinatorial Embedding and Consistency Regularization\n  for Light Field Super-resolution", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field (LF) images acquired by hand-held devices usually suffer from low\nspatial resolution as the limited detector resolution has to be shared with the\nangular dimension. LF spatial super-resolution (SR) thus becomes an\nindispensable part of the LF camera processing pipeline. The\nhigh-dimensionality characteristic and complex geometrical structure of LF\nimages make the problem more challenging than traditional single-image SR. The\nperformance of existing methods is still limited as they fail to thoroughly\nexplore the coherence among LF sub-aperture images (SAIs) and are insufficient\nin accurately preserving the scene's parallax structure. To tackle this\nchallenge, we propose a novel learning-based LF spatial SR framework.\nSpecifically, each SAI of an LF image is first coarsely and individually\nsuper-resolved by exploring the complementary information among SAIs with\nselective combinatorial geometry embedding. To achieve efficient and effective\nselection of the complementary information, we propose two novel sub-modules\nconducted hierarchically: the patch selector provides an option of retrieving\nsimilar image patches based on offline disparity estimation to handle\nlarge-disparity correlations; and the SAI selector adaptively and flexibly\nselects the most informative SAIs to improve the embedding efficiency. To\npreserve the parallax structure among the reconstructed SAIs, we subsequently\nappend a consistency regularization network trained over a structure-aware loss\nfunction to refine the parallax relationships over the coarse estimation. In\naddition, we extend the proposed method to irregular LF data. To the best of\nour knowledge, this is the first learning-based SR method for irregular LF\ndata. Experimental results over both synthetic and real-world LF datasets\ndemonstrate the significant advantage of our approach over state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 08:34:37 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Jin", "Jing", ""], ["Hou", "Junhui", ""], ["Zhu", "Zhiyu", ""], ["Chen", "Jie", ""], ["Kwong", "Sam", ""]]}, {"id": "2009.12546", "submitter": "Alfred Sch\\\"ottl", "authors": "Alfred Sch\\\"ottl", "title": "A light-weight method to foster the (Grad)CAM interpretability and\n  explainability of classification networks", "comments": "2020 10th International Conference on Advanced Computer Information\n  Technologies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a light-weight method which allows to improve the explainability\nof localized classification networks. The method considers (Grad)CAM maps\nduring the training process by modification of the training loss and does not\nrequire additional structural elements. It is demonstrated that the (Grad)CAM\ninterpretability, as measured by several indicators, can be improved in this\nway. Since the method shall be applicable on embedded systems and on standard\ndeeper architectures, it essentially takes advantage of second order\nderivatives during the training and does not require additional model layers.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 09:15:28 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Sch\u00f6ttl", "Alfred", ""]]}, {"id": "2009.12547", "submitter": "Dong Zhang", "authors": "Dong Zhang, Hanwang Zhang, Jinhui Tang, Xiansheng Hua, Qianru Sun", "title": "Causal Intervention for Weakly-Supervised Semantic Segmentation", "comments": "Accepted as a NeurIPS 2020 oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a causal inference framework to improve Weakly-Supervised Semantic\nSegmentation (WSSS). Specifically, we aim to generate better pixel-level\npseudo-masks by using only image-level labels -- the most crucial step in WSSS.\nWe attribute the cause of the ambiguous boundaries of pseudo-masks to the\nconfounding context, e.g., the correct image-level classification of \"horse\"\nand \"person\" may be not only due to the recognition of each instance, but also\ntheir co-occurrence context, making the model inspection (e.g., CAM) hard to\ndistinguish between the boundaries. Inspired by this, we propose a structural\ncausal model to analyze the causalities among images, contexts, and class\nlabels. Based on it, we develop a new method: Context Adjustment (CONTA), to\nremove the confounding bias in image-level classification and thus provide\nbetter pseudo-masks as ground-truth for the subsequent segmentation model. On\nPASCAL VOC 2012 and MS-COCO, we show that CONTA boosts various popular WSSS\nmethods to new state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 09:26:29 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 04:20:09 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Zhang", "Dong", ""], ["Zhang", "Hanwang", ""], ["Tang", "Jinhui", ""], ["Hua", "Xiansheng", ""], ["Sun", "Qianru", ""]]}, {"id": "2009.12559", "submitter": "Wei Zhou", "authors": "Wei Zhou, Yukang Wang, Jiajia Chu, Jiehua Yang, Xiang Bai, Yongchao Xu", "title": "Affinity Space Adaptation for Semantic Segmentation Across Domains", "comments": "Accepted by IEEE TIP", "journal-ref": null, "doi": "10.1109/TIP.2020.3018221", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation with dense pixel-wise annotation has achieved excellent\nperformance thanks to deep learning. However, the generalization of semantic\nsegmentation in the wild remains challenging. In this paper, we address the\nproblem of unsupervised domain adaptation (UDA) in semantic segmentation.\nMotivated by the fact that source and target domain have invariant semantic\nstructures, we propose to exploit such invariance across domains by leveraging\nco-occurring patterns between pairwise pixels in the output of structured\nsemantic segmentation. This is different from most existing approaches that\nattempt to adapt domains based on individual pixel-wise information in image,\nfeature, or output level. Specifically, we perform domain adaptation on the\naffinity relationship between adjacent pixels termed affinity space of source\nand target domain. To this end, we develop two affinity space adaptation\nstrategies: affinity space cleaning and adversarial affinity space alignment.\nExtensive experiments demonstrate that the proposed method achieves superior\nperformance against some state-of-the-art methods on several challenging\nbenchmarks for semantic segmentation across domains. The code is available at\nhttps://github.com/idealwei/ASANet.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 10:28:11 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Zhou", "Wei", ""], ["Wang", "Yukang", ""], ["Chu", "Jiajia", ""], ["Yang", "Jiehua", ""], ["Bai", "Xiang", ""], ["Xu", "Yongchao", ""]]}, {"id": "2009.12569", "submitter": "Hongfeng You", "authors": "Hongfeng You, Long Yu, Shengwei Tian, Xiang Ma, Yan Xing and Xiaojie\n  Ma", "title": "DT-Net: A novel network based on multi-directional integrated\n  convolution and threshold convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since medical image data sets contain few samples and singular features,\nlesions are viewed as highly similar to other tissues. The traditional neural\nnetwork has a limited ability to learn features. Even if a host of feature maps\nis expanded to obtain more semantic information, the accuracy of segmenting the\nfinal medical image is slightly improved, and the features are excessively\nredundant. To solve the above problems, in this paper, we propose a novel\nend-to-end semantic segmentation algorithm, DT-Net, and use two new convolution\nstrategies to better achieve end-to-end semantic segmentation of medical\nimages. 1. In the feature mining and feature fusion stage, we construct a\nmulti-directional integrated convolution (MDIC). The core idea is to use the\nmulti-scale convolution to enhance the local multi-directional feature maps to\ngenerate enhanced feature maps and to mine the generated features that contain\nmore semantics without increasing the number of feature maps. 2. We also aim to\nfurther excavate and retain more meaningful deep features reduce a host of\nnoise features in the training process. Therefore, we propose a convolution\nthresholding strategy. The central idea is to set a threshold to eliminate a\nlarge number of redundant features and reduce computational complexity. Through\nthe two strategies proposed above, the algorithm proposed in this paper\nproduces state-of-the-art results on two public medical image datasets. We\nprove in detail that our proposed strategy plays an important role in feature\nmining and eliminating redundant features. Compared with the existing semantic\nsegmentation algorithms, our proposed algorithm has better robustness.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 11:12:06 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["You", "Hongfeng", ""], ["Yu", "Long", ""], ["Tian", "Shengwei", ""], ["Ma", "Xiang", ""], ["Xing", "Yan", ""], ["Ma", "Xiaojie", ""]]}, {"id": "2009.12575", "submitter": "Xiaowei Jia", "authors": "Xiaowei Jia, Jacob Zwart, Jeffrey Sadler, Alison Appling, Samantha\n  Oliver, Steven Markstrom, Jared Willard, Shaoming Xu, Michael Steinbach,\n  Jordan Read, and Vipin Kumar", "title": "Physics-Guided Recurrent Graph Networks for Predicting Flow and\n  Temperature in River Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a physics-guided machine learning approach that combines\nadvanced machine learning models and physics-based models to improve the\nprediction of water flow and temperature in river networks. We first build a\nrecurrent graph network model to capture the interactions among multiple\nsegments in the river network. Then we present a pre-training technique which\ntransfers knowledge from physics-based models to initialize the machine\nlearning model and learn the physics of streamflow and thermodynamics. We also\npropose a new loss function that balances the performance over different river\nsegments. We demonstrate the effectiveness of the proposed method in predicting\ntemperature and streamflow in a subset of the Delaware River Basin. In\nparticular, we show that the proposed method brings a 33\\%/14\\% improvement\nover the state-of-the-art physics-based model and 24\\%/14\\% over traditional\nmachine learning models (e.g., Long-Short Term Memory Neural Network) in\ntemperature/streamflow prediction using very sparse (0.1\\%) observation data\nfor training. The proposed method has also been shown to produce better\nperformance when generalized to different seasons or river segments with\ndifferent streamflow ranges.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 11:46:51 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 18:00:55 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Jia", "Xiaowei", ""], ["Zwart", "Jacob", ""], ["Sadler", "Jeffrey", ""], ["Appling", "Alison", ""], ["Oliver", "Samantha", ""], ["Markstrom", "Steven", ""], ["Willard", "Jared", ""], ["Xu", "Shaoming", ""], ["Steinbach", "Michael", ""], ["Read", "Jordan", ""], ["Kumar", "Vipin", ""]]}, {"id": "2009.12577", "submitter": "Mohamed Ali Souibgui", "authors": "Mohamed Ali Souibgui and Alicia Forn\\'es and Yousri Kessentini and\n  Crina Tudor", "title": "A Few-shot Learning Approach for Historical Ciphered Manuscript\n  Recognition", "comments": "Accepted in the 25th International Conference on Pattern Recognition\n  (ICPR2020), Milan, Italy 10 - 15 January 2021 (Camera Ready Version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoded (or ciphered) manuscripts are a special type of historical documents\nthat contain encrypted text. The automatic recognition of this kind of\ndocuments is challenging because: 1) the cipher alphabet changes from one\ndocument to another, 2) there is a lack of annotated corpus for training and 3)\ntouching symbols make the symbol segmentation difficult and complex. To\novercome these difficulties, we propose a novel method for handwritten ciphers\nrecognition based on few-shot object detection. Our method first detects all\nsymbols of a given alphabet in a line image, and then a decoding step maps the\nsymbol similarity scores to the final sequence of transcribed symbols. By\ntraining on synthetic data, we show that the proposed architecture is able to\nrecognize handwritten ciphers with unseen alphabets. In addition, if few\nlabeled pages with the same alphabet are used for fine tuning, our method\nsurpasses existing unsupervised and supervised HTR methods for ciphers\nrecognition.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 11:49:18 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Souibgui", "Mohamed Ali", ""], ["Forn\u00e9s", "Alicia", ""], ["Kessentini", "Yousri", ""], ["Tudor", "Crina", ""]]}, {"id": "2009.12596", "submitter": "Zixuan Xiao", "authors": "Zixuan Xiao, Wei Xue, and Ping Zhong", "title": "Few-shot Object Detection with Self-adaptive Attention Network for\n  Remote Sensing Images", "comments": "arXiv admin note: text overlap with arXiv:2009.01616", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In remote sensing field, there are many applications of object detection in\nrecent years, which demands a great number of labeled data. However, we may be\nfaced with some cases where only limited data are available. In this paper, we\nproposed a few-shot object detector which is designed for detecting novel\nobjects provided with only a few examples. Particularly, in order to fit the\nobject detection settings, our proposed few-shot detector concentrates on the\nrelations that lie in the level of objects instead of the full image with the\nassistance of Self-Adaptive Attention Network (SAAN). The SAAN can fully\nleverage the object-level relations through a relation GRU unit and\nsimultaneously attach attention on object features in a self-adaptive way\naccording to the object-level relations to avoid some situations where the\nadditional attention is useless or even detrimental. Eventually, the detection\nresults are produced from the features that are added with attention and thus\nare able to be detected simply. The experiments demonstrate the effectiveness\nof the proposed method in few-shot scenes.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 13:44:58 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Xiao", "Zixuan", ""], ["Xue", "Wei", ""], ["Zhong", "Ping", ""]]}, {"id": "2009.12597", "submitter": "Anwaar Ulhaq Dr", "authors": "Douglas P. S. Gomes, Anwaar Ulhaq, Manoranjan Paul, Michael J. Horry,\n  Subrata Chakraborty, Manas Saha, Tanmoy Debnath, D.M. Motiur Rahaman", "title": "Potential Features of ICU Admission in X-ray Images of COVID-19 Patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  X-ray images may present non-trivial features with predictive information of\npatients that develop severe symptoms of COVID-19. If true, this hypothesis may\nhave practical value in allocating resources to particular patients while using\na relatively inexpensive imaging technique. The difficulty of testing such a\nhypothesis comes from the need for large sets of labelled data, which need to\nbe well-annotated and should contemplate the post-imaging severity outcome.\nThis paper presents an original methodology for extracting semantic features\nthat correlate to severity from a data set with patient ICU admission labels\nthrough interpretable models. The methodology employs a neural network trained\nto recognise lung pathologies to extract the semantic features, which are then\nanalysed with low-complexity models to limit overfitting while increasing\ninterpretability. This analysis points out that only a few features explain\nmost of the variance between patients that developed severe symptoms. When\napplied to an unrelated larger data set with pathology-related clinical notes,\nthe method has shown to be capable of selecting images for the learned\nfeatures, which could translate some information about their common locations\nin the lung. Besides attesting separability on patients that eventually develop\nsevere symptoms, the proposed methods represent a statistical approach\nhighlighting the importance of features related to ICU admission that may have\nbeen only qualitatively reported. While handling limited data sets, notable\nmethodological aspects are adopted, such as presenting a state-of-the-art lung\nsegmentation network and the use of low-complexity models to avoid overfitting.\nThe code for methodology and experiments is also available.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 13:48:39 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 12:43:04 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Gomes", "Douglas P. S.", ""], ["Ulhaq", "Anwaar", ""], ["Paul", "Manoranjan", ""], ["Horry", "Michael J.", ""], ["Chakraborty", "Subrata", ""], ["Saha", "Manas", ""], ["Debnath", "Tanmoy", ""], ["Rahaman", "D. M. Motiur", ""]]}, {"id": "2009.12606", "submitter": "Chaozheng Wu", "authors": "Chaozheng Wu, Jian Chen, Qiaoyu Cao, Jianchi Zhang, Yunxin Tai, Lin\n  Sun, Kui Jia", "title": "Grasp Proposal Networks: An End-to-End Solution for Visual Learning of\n  Robotic Grasps", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning robotic grasps from visual observations is a promising yet\nchallenging task. Recent research shows its great potential by preparing and\nlearning from large-scale synthetic datasets. For the popular, 6\ndegree-of-freedom (6-DOF) grasp setting of parallel-jaw gripper, most of\nexisting methods take the strategy of heuristically sampling grasp candidates\nand then evaluating them using learned scoring functions. This strategy is\nlimited in terms of the conflict between sampling efficiency and coverage of\noptimal grasps. To this end, we propose in this work a novel, end-to-end\n\\emph{Grasp Proposal Network (GPNet)}, to predict a diverse set of 6-DOF grasps\nfor an unseen object observed from a single and unknown camera view. GPNet\nbuilds on a key design of grasp proposal module that defines \\emph{anchors of\ngrasp centers} at discrete but regular 3D grid corners, which is flexible to\nsupport either more precise or more diverse grasp predictions. To test GPNet,\nwe contribute a synthetic dataset of 6-DOF object grasps; evaluation is\nconducted using rule-based criteria, simulation test, and real test.\nComparative results show the advantage of our methods over existing ones.\nNotably, GPNet gains better simulation results via the specified coverage,\nwhich helps achieve a ready translation in real test. We will make our dataset\npublicly available.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 14:14:52 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Wu", "Chaozheng", ""], ["Chen", "Jian", ""], ["Cao", "Qiaoyu", ""], ["Zhang", "Jianchi", ""], ["Tai", "Yunxin", ""], ["Sun", "Lin", ""], ["Jia", "Kui", ""]]}, {"id": "2009.12610", "submitter": "YoungGon Kim", "authors": "Young-Gon Kim, Kyungsang Kim, Dufan Wu, Hui Ren, Won Young Tak, Soo\n  Young Park, Yu Rim Lee, Min Kyu Kang, Jung Gil Park, Byung Seok Kim, Woo Jin\n  Chung, Mannudeep K. Kalra, Quanzheng Li", "title": "Deep Learning-based Four-region Lung Segmentation in Chest Radiography\n  for COVID-19 Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose. Imaging plays an important role in assessing severity of COVID 19\npneumonia. However, semantic interpretation of chest radiography (CXR) findings\ndoes not include quantitative description of radiographic opacities. Most\ncurrent AI assisted CXR image analysis framework do not quantify for regional\nvariations of disease. To address these, we proposed a four region lung\nsegmentation method to assist accurate quantification of COVID 19 pneumonia.\nMethods. A segmentation model to separate left and right lung is firstly\napplied, and then a carina and left hilum detection network is used, which are\nthe clinical landmarks to separate the upper and lower lungs. To improve the\nsegmentation performance of COVID 19 images, ensemble strategy incorporating\nfive models is exploited. Using each region, we evaluated the clinical\nrelevance of the proposed method with the Radiographic Assessment of the\nQuality of Lung Edema (RALE). Results. The proposed ensemble strategy showed\ndice score of 0.900, which is significantly higher than conventional methods\n(0.854 0.889). Mean intensities of segmented four regions indicate positive\ncorrelation to the extent and density scores of pulmonary opacities under the\nRALE framework. Conclusion. A deep learning based model in CXR can accurately\nsegment and quantify regional distribution of pulmonary opacities in patients\nwith COVID 19 pneumonia.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 14:32:13 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Kim", "Young-Gon", ""], ["Kim", "Kyungsang", ""], ["Wu", "Dufan", ""], ["Ren", "Hui", ""], ["Tak", "Won Young", ""], ["Park", "Soo Young", ""], ["Lee", "Yu Rim", ""], ["Kang", "Min Kyu", ""], ["Park", "Jung Gil", ""], ["Kim", "Byung Seok", ""], ["Chung", "Woo Jin", ""], ["Kalra", "Mannudeep K.", ""], ["Li", "Quanzheng", ""]]}, {"id": "2009.12632", "submitter": "Mahmoud Afifi", "authors": "Mahmoud Afifi and Michael S. Brown", "title": "Interactive White Balancing for Camera-Rendered Images", "comments": "To appear in Color and Imaging Conference (CIC28), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  White balance (WB) is one of the first photo-finishing steps used to render a\ncaptured image to its final output. WB is applied to remove the color cast\ncaused by the scene's illumination. Interactive photo-editing software allows\nusers to manually select different regions in a photo as examples of the\nillumination for WB correction (e.g., clicking on achromatic objects). Such\ninteractive editing is possible only with images saved in a RAW image format.\nThis is because RAW images have no photo-rendering operations applied and\nphoto-editing software is able to apply WB and other photo-finishing procedures\nto render the final image. Interactively editing WB in camera-rendered images\nis significantly more challenging. This is because the camera hardware has\nalready applied WB to the image and subsequent nonlinear photo-processing\nroutines. These nonlinear rendering operations make it difficult to change the\nWB post-capture. The goal of this paper is to allow interactive WB manipulation\nof camera-rendered images. The proposed method is an extension of our recent\nwork \\cite{afifi2019color} that proposed a post-capture method for WB\ncorrection based on nonlinear color-mapping functions. Here, we introduce a new\nframework that links the nonlinear color-mapping functions directly to\nuser-selected colors to enable {\\it interactive} WB manipulation. This new\nframework is also more efficient in terms of memory and run-time (99\\%\nreduction in memory and 3$\\times$ speed-up). Lastly, we describe how our\nframework can leverage a simple illumination estimation method (i.e.,\ngray-world) to perform auto-WB correction that is on a par with the WB\ncorrection results in \\cite{afifi2019color}. The source code is publicly\navailable at https://github.com/mahmoudnafifi/Interactive_WB_correction.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 16:22:05 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Afifi", "Mahmoud", ""], ["Brown", "Michael S.", ""]]}, {"id": "2009.12648", "submitter": "Amitojdeep Singh", "authors": "Amitojdeep Singh, J. Jothi Balaji, Mohammed Abdul Rasheed,\n  Varadharajan Jayakumar, Rajiv Raman, Vasudevan Lakshminarayanan", "title": "Quantitative and Qualitative Evaluation of Explainable Deep Learning\n  Methods for Ophthalmic Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The lack of explanations for the decisions made by algorithms\nsuch as deep learning has hampered their acceptance by the clinical community\ndespite highly accurate results on multiple problems. Recently, attribution\nmethods have emerged for explaining deep learning models, and they have been\ntested on medical imaging problems. The performance of attribution methods is\ncompared on standard machine learning datasets and not on medical images. In\nthis study, we perform a comparative analysis to determine the most suitable\nexplainability method for retinal OCT diagnosis.\n  Methods: A commonly used deep learning model known as Inception v3 was\ntrained to diagnose 3 retinal diseases - choroidal neovascularization (CNV),\ndiabetic macular edema (DME), and drusen. The explanations from 13 different\nattribution methods were rated by a panel of 14 clinicians for clinical\nsignificance. Feedback was obtained from the clinicians regarding the current\nand future scope of such methods.\n  Results: An attribution method based on a Taylor series expansion, called\nDeep Taylor was rated the highest by clinicians with a median rating of 3.85/5.\nIt was followed by two other attribution methods, Guided backpropagation and\nSHAP (SHapley Additive exPlanations).\n  Conclusion: Explanations of deep learning models can make them more\ntransparent for clinical diagnosis. This study compared different explanations\nmethods in the context of retinal OCT diagnosis and found that the best\nperforming method may not be the one considered best for other deep learning\ntasks. Overall, there was a high degree of acceptance from the clinicians\nsurveyed in the study.\n  Keywords: explainable AI, deep learning, machine learning, image processing,\nOptical coherence tomography, retina, Diabetic macular edema, Choroidal\nNeovascularization, Drusen\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 17:17:08 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 20:13:37 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Singh", "Amitojdeep", ""], ["Balaji", "J. Jothi", ""], ["Rasheed", "Mohammed Abdul", ""], ["Jayakumar", "Varadharajan", ""], ["Raman", "Rajiv", ""], ["Lakshminarayanan", "Vasudevan", ""]]}, {"id": "2009.12664", "submitter": "Heng Zhang", "authors": "Heng Zhang, Elisa Fromont, S\\'ebastien Lefevre, Bruno Avignon", "title": "Multispectral Fusion for Object Detection with Cyclic Fuse-and-Refine\n  Blocks", "comments": "Accepted by ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multispectral images (e.g. visible and infrared) may be particularly useful\nwhen detecting objects with the same model in different environments (e.g.\nday/night outdoor scenes). To effectively use the different spectra, the main\ntechnical problem resides in the information fusion process. In this paper, we\npropose a new halfway feature fusion method for neural networks that leverages\nthe complementary/consistency balance existing in multispectral features by\nadding to the network architecture, a particular module that cyclically fuses\nand refines each spectral feature. We evaluate the effectiveness of our fusion\nmethod on two challenging multispectral datasets for object detection. Our\nresults show that implementing our Cyclic Fuse-and-Refine module in any network\nimproves the performance on both datasets compared to other state-of-the-art\nmultispectral object detection methods.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 18:39:05 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Zhang", "Heng", ""], ["Fromont", "Elisa", ""], ["Lefevre", "S\u00e9bastien", ""], ["Avignon", "Bruno", ""]]}, {"id": "2009.12674", "submitter": "Fares Abawi", "authors": "Matthias Kerzel (1), Fares Abawi (1), Manfred Eppe (1), Stefan Wermter\n  (1) ((1) University of Hamburg)", "title": "Enhancing a Neurocognitive Shared Visuomotor Model for Object\n  Identification, Localization, and Grasping With Learning From Auxiliary Tasks", "comments": "Matthias Kerzel and Fares Abawi contributed equally to this work", "journal-ref": null, "doi": "10.1109/TCDS.2020.3028460", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a follow-up study on our unified visuomotor neural model for the\nrobotic tasks of identifying, localizing, and grasping a target object in a\nscene with multiple objects. Our Retinanet-based model enables end-to-end\ntraining of visuomotor abilities in a biologically inspired developmental\napproach. In our initial implementation, a neural model was able to grasp\nselected objects from a planar surface. We embodied the model on the NICO\nhumanoid robot. In this follow-up study, we expand the task and the model to\nreaching for objects in a three-dimensional space with a novel dataset based on\naugmented reality and a simulation environment. We evaluate the influence of\ntraining with auxiliary tasks, i.e., if learning of the primary visuomotor task\nis supported by learning to classify and locate different objects. We show that\nthe proposed visuomotor model can learn to reach for objects in a\nthree-dimensional space. We analyze the results for biologically-plausible\nbiases based on object locations or properties. We show that the primary\nvisuomotor task can be successfully trained simultaneously with one of the two\nauxiliary tasks. This is enabled by a complex neurocognitive model with shared\nand task-specific components, similar to models found in biological systems.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 19:45:15 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Kerzel", "Matthias", "", "University of Hamburg"], ["Abawi", "Fares", "", "University of Hamburg"], ["Eppe", "Manfred", "", "University of Hamburg"], ["Wermter", "Stefan", "", "University of Hamburg"]]}, {"id": "2009.12678", "submitter": "Benjamin Busam", "authors": "Benjamin Busam and Hyun Jun Jung and Nassir Navab", "title": "I Like to Move It: 6D Pose Estimation as an Action Decision Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object pose estimation is an integral part of robot vision and AR. Previous\n6D pose retrieval pipelines treat the problem either as a regression task or\ndiscretize the pose space to classify. We change this paradigm and reformulate\nthe problem as an action decision process where an initial pose is updated in\nincremental discrete steps that sequentially move a virtual 3D rendering\ntowards the correct solution. A neural network estimates likely moves from a\nsingle RGB image iteratively and determines so an acceptable final pose. In\ncomparison to other approaches that train object-specific pose models, we learn\na decision process. This allows for a lightweight architecture while it\nnaturally generalizes to unseen objects. A coherent stop action for process\ntermination enables dynamic reduction of the computation cost if there are\ninsignificant changes in a video sequence. Instead of a static inference time,\nwe thereby automatically increase the runtime depending on the object motion.\nRobustness and accuracy of our action decision network are evaluated on Laval\nand YCB video scenes where we significantly improve the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 20:05:42 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 19:03:28 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Busam", "Benjamin", ""], ["Jung", "Hyun Jun", ""], ["Navab", "Nassir", ""]]}, {"id": "2009.12684", "submitter": "Jonathan Reiner", "authors": "Jonathan Reiner, Guy Azran, Gal Hyams", "title": "MicroAnalyzer: A Python Tool for Automated Bacterial Analysis with\n  Fluorescence Microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluorescence microscopy is a widely used method among cell biologists for\nstudying the localization and co-localization of fluorescent protein. For\nmicrobial cell biologists, these studies often include tedious and\ntime-consuming manual segmentation of bacteria and of the fluorescence clusters\nor working with multiple programs. Here, we present MicroAnalyzer - a tool that\nautomates these tasks by providing an end-to-end platform for microscope image\nanalysis. While such tools do exist, they are costly, black-boxed programs.\nMicroanalyzer offers an open-source alternative to these tools, allowing\nflexibility and expandability by advanced users. MicroAnalyzer provides\naccurate cell and fluorescence cluster segmentation based on state-of-the-art\ndeep-learning segmentation models, combined with ad-hoc post-processing and\nColicoords - an open-source cell image analysis tool for calculating general\ncell and fluorescence measurements. Using these methods, it performs better\nthan generic approaches since the dynamic nature of neural networks allows for\na quick adaptation to experiment restrictions and assumptions. Other existing\ntools do not consider experiment assumptions, nor do they provide fluorescence\ncluster detection without the need for any specialized equipment. The key goal\nof MicroAnalyzer is to automate the entire process of cell and fluorescence\nimage analysis \"from microscope to database\", meaning it does not require any\nfurther input from the researcher except for the initial deep-learning model\ntraining. In this fashion, it allows the researchers to concentrate on the\nbigger picture instead of granular, eye-straining labor\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 20:45:19 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Reiner", "Jonathan", ""], ["Azran", "Guy", ""], ["Hyams", "Gal", ""]]}, {"id": "2009.12698", "submitter": "Aysen Degerli", "authors": "Aysen Degerli, Mete Ahishali, Mehmet Yamac, Serkan Kiranyaz, Muhammad\n  E. H. Chowdhury, Khalid Hameed, Tahir Hamid, Rashid Mazhar, and Moncef\n  Gabbouj", "title": "COVID-19 Infection Map Generation and Detection from Chest X-Ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-aided diagnosis has become a necessity for accurate and immediate\ncoronavirus disease 2019 (COVID-19) detection to aid treatment and prevent the\nspread of the virus. Numerous studies have proposed to use Deep Learning\ntechniques for COVID-19 diagnosis. However, they have used very limited chest\nX-ray (CXR) image repositories for evaluation with a small number, a few\nhundreds, of COVID-19 samples. Moreover, these methods can neither localize nor\ngrade the severity of COVID-19 infection. For this purpose, recent studies\nproposed to explore the activation maps of deep networks. However, they remain\ninaccurate for localizing the actual infestation making them unreliable for\nclinical use. This study proposes a novel method for the joint localization,\nseverity grading, and detection of COVID-19 from CXR images by generating the\nso-called infection maps. To accomplish this, we have compiled the largest\ndataset with 119,316 CXR images including 2951 COVID-19 samples, where the\nannotation of the ground-truth segmentation masks is performed on CXRs by a\nnovel collaborative human-machine approach. Furthermore, we publicly release\nthe first CXR dataset with the ground-truth segmentation masks of the COVID-19\ninfected regions. A detailed set of experiments show that state-of-the-art\nsegmentation networks can learn to localize COVID-19 infection with an F1-score\nof 83.20%, which is significantly superior to the activation maps created by\nthe previous methods. Finally, the proposed approach achieved a COVID-19\ndetection performance with 94.96% sensitivity and 99.88% specificity.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 22:20:05 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 20:17:40 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Degerli", "Aysen", ""], ["Ahishali", "Mete", ""], ["Yamac", "Mehmet", ""], ["Kiranyaz", "Serkan", ""], ["Chowdhury", "Muhammad E. H.", ""], ["Hameed", "Khalid", ""], ["Hamid", "Tahir", ""], ["Mazhar", "Rashid", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2009.12743", "submitter": "Masaki Yamagata", "authors": "Masaki Yamagata, Hideaki Hayashi, and Seiichi Uchida", "title": "Handwriting Prediction Considering Inter-Class Bifurcation Structures", "comments": "Accepted at ICFHR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal prediction is a still difficult task due to the chaotic behavior,\nnon-Markovian characteristics, and non-stationary noise of temporal signals.\nHandwriting prediction is also challenging because of uncertainty arising from\ninter-class bifurcation structures, in addition to the above problems. For\nexample, the classes '0' and '6' are very similar in terms of their beginning\nparts; therefore it is nearly impossible to predict their subsequent parts from\nthe beginning part. In other words, '0' and '6' have a bifurcation structure\ndue to ambiguity between classes, and we cannot make a long-term prediction in\nthis context. In this paper, we propose a temporal prediction model that can\ndeal with this bifurcation structure. Specifically, the proposed model learns\nthe bifurcation structure explicitly as a Gaussian mixture model (GMM) for each\nclass as well as the posterior probability of the classes. The final result of\nprediction is represented as the weighted sum of GMMs using the class\nprobabilities as weights. When multiple classes have large weights, the model\ncan handle a bifurcation and thus avoid an inaccurate prediction. The proposed\nmodel is formulated as a neural network including long short-term memories and\nis thus trained in an end-to-end manner. The proposed model was evaluated on\nthe UNIPEN online handwritten character dataset, and the results show that the\nmodel can catch and deal with the bifurcation structures.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 05:13:46 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Yamagata", "Masaki", ""], ["Hayashi", "Hideaki", ""], ["Uchida", "Seiichi", ""]]}, {"id": "2009.12760", "submitter": "Qiegen Liu", "authors": "Zhuonan He, Yikun Zhang, Yu Guan, Shanzhou Niu, Yi Zhang, Yang Chen,\n  Qiegen Liu", "title": "Iterative Reconstruction for Low-Dose CT using Deep Gradient Priors of\n  Generative Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dose reduction in computed tomography (CT) is essential for decreasing\nradiation risk in clinical applications. Iterative reconstruction is one of the\nmost promising ways to compensate for the increased noise due to reduction of\nphoton flux. Rather than most existing prior-driven algorithms that benefit\nfrom manually designed prior functions or supervised learning schemes, in this\nwork we integrate the data-consistency as a conditional term into the iterative\ngenerative model for low-dose CT. At the stage of prior learning, the gradient\nof data density is directly learned from normal-dose CT images as a prior. Then\nat the iterative reconstruction stage, the stochastic gradient descent is\nemployed to update the trained prior with annealed and conditional schemes. The\ndistance between the reconstructed image and the manifold is minimized along\nwith data fidelity during reconstruction. Experimental comparisons demonstrated\nthe noise reduction and detail preservation abilities of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 06:36:39 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 06:05:03 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["He", "Zhuonan", ""], ["Zhang", "Yikun", ""], ["Guan", "Yu", ""], ["Niu", "Shanzhou", ""], ["Zhang", "Yi", ""], ["Chen", "Yang", ""], ["Liu", "Qiegen", ""]]}, {"id": "2009.12763", "submitter": "Tianyang Shi", "authors": "Yinglin Duan (1), Tianyang Shi (1), Zhengxia Zou (2), Jia Qin (1 and\n  3), Yifei Zhao (1), Yi Yuan (1), Jie Hou (1), Xiang Wen (1 and 3), Changjie\n  Fan (1) ((1) NetEase Fuxi AI Lab, (2) University of Michigan, Ann Arbor, (3)\n  Zhejiang University)", "title": "Semi-Supervised Learning for In-Game Expert-Level Music-to-Dance\n  Translation", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music-to-dance translation is a brand-new and powerful feature in recent\nrole-playing games. Players can now let their characters dance along with\nspecified music clips and even generate fan-made dance videos. Previous works\nof this topic consider music-to-dance as a supervised motion generation problem\nbased on time-series data. However, these methods suffer from limited training\ndata pairs and the degradation of movements. This paper provides a new\nperspective for this task where we re-formulate the translation problem as a\npiece-wise dance phrase retrieval problem based on the choreography theory.\nWith such a design, players are allowed to further edit the dance movements on\ntop of our generation while other regression based methods ignore such user\ninteractivity. Considering that the dance motion capture is an expensive and\ntime-consuming procedure which requires the assistance of professional dancers,\nwe train our method under a semi-supervised learning framework with a large\nunlabeled dataset (20x than labeled data) collected. A co-ascent mechanism is\nintroduced to improve the robustness of our network. Using this unlabeled\ndataset, we also introduce self-supervised pre-training so that the translator\ncan understand the melody, rhythm, and other components of music phrases. We\nshow that the pre-training significantly improves the translation accuracy than\nthat of training from scratch. Experimental results suggest that our method not\nonly generalizes well over various styles of music but also succeeds in\nexpert-level choreography for game players.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 07:08:04 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Duan", "Yinglin", "", "1 and\n  3"], ["Shi", "Tianyang", "", "1 and\n  3"], ["Zou", "Zhengxia", "", "1 and\n  3"], ["Qin", "Jia", "", "1 and\n  3"], ["Zhao", "Yifei", "", "1 and 3"], ["Yuan", "Yi", "", "1 and 3"], ["Hou", "Jie", "", "1 and 3"], ["Wen", "Xiang", "", "1 and 3"], ["Fan", "Changjie", ""]]}, {"id": "2009.12770", "submitter": "Deepak Gupta", "authors": "Deepak Gupta, Swati Suman, Asif Ekbal", "title": "Hierarchical Deep Multi-modal Network for Medical Visual Question\n  Answering", "comments": "Accepted for publication at Expert Systems with Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Visual Question Answering in Medical domain (VQA-Med) plays an important role\nin providing medical assistance to the end-users. These users are expected to\nraise either a straightforward question with a Yes/No answer or a challenging\nquestion that requires a detailed and descriptive answer. The existing\ntechniques in VQA-Med fail to distinguish between the different question types\nsometimes complicates the simpler problems, or over-simplifies the complicated\nones. It is certainly true that for different question types, several distinct\nsystems can lead to confusion and discomfort for the end-users. To address this\nissue, we propose a hierarchical deep multi-modal network that analyzes and\nclassifies end-user questions/queries and then incorporates a query-specific\napproach for answer prediction. We refer our proposed approach as Hierarchical\nQuestion Segregation based Visual Question Answering, in short HQS-VQA. Our\ncontributions are three-fold, viz. firstly, we propose a question segregation\n(QS) technique for VQAMed; secondly, we integrate the QS model to the\nhierarchical deep multi-modal neural network to generate proper answers to the\nqueries related to medical images; and thirdly, we study the impact of QS in\nMedical-VQA by comparing the performance of the proposed model with QS and a\nmodel without QS. We evaluate the performance of our proposed model on two\nbenchmark datasets, viz. RAD and CLEF18. Experimental results show that our\nproposed HQS-VQA technique outperforms the baseline models with significant\nmargins. We also conduct a detailed quantitative and qualitative analysis of\nthe obtained results and discover potential causes of errors and their\nsolutions.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 07:24:41 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Gupta", "Deepak", ""], ["Suman", "Swati", ""], ["Ekbal", "Asif", ""]]}, {"id": "2009.12796", "submitter": "Yanan Liu", "authors": "Yanan Liu, Laurie Bose, Colin Greatwood, Jianing Chen, Rui Fan, Thomas\n  Richardson, Stephen J. Carey, Piotr Dudek, Walterio Mayol-Cuevas", "title": "Agile Reactive Navigation for A Non-Holonomic Mobile Robot Using A Pixel\n  Processor Array", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an agile reactive navigation strategy for driving a\nnon-holonomic ground vehicle around a preset course of gates in a cluttered\nenvironment using a low-cost processor array sensor. This enables machine\nvision tasks to be performed directly upon the sensor's image plane, rather\nthan using a separate general-purpose computer. We demonstrate a small ground\nvehicle running through or avoiding multiple gates at high speed using minimal\ncomputational resources. To achieve this, target tracking algorithms are\ndeveloped for the Pixel Processing Array and captured images are then processed\ndirectly on the vision sensor acquiring target information for controlling the\nground vehicle. The algorithm can run at up to 2000 fps outdoors and 200fps at\nindoor illumination levels. Conducting image processing at the sensor level\navoids the bottleneck of image transfer encountered in conventional sensors.\nThe real-time performance of on-board image processing and robustness is\nvalidated through experiments. Experimental results demonstrate that the\nalgorithm's ability to enable a ground vehicle to navigate at an average speed\nof 2.20 m/s for passing through multiple gates and 3.88 m/s for a 'slalom' task\nin an environment featuring significant visual clutter.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 09:11:31 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Liu", "Yanan", ""], ["Bose", "Laurie", ""], ["Greatwood", "Colin", ""], ["Chen", "Jianing", ""], ["Fan", "Rui", ""], ["Richardson", "Thomas", ""], ["Carey", "Stephen J.", ""], ["Dudek", "Piotr", ""], ["Mayol-Cuevas", "Walterio", ""]]}, {"id": "2009.12798", "submitter": "Majed El Helou", "authors": "Majed El Helou, Ruofan Zhou, Sabine S\\\"usstrunk, Radu Timofte, Mahmoud\n  Afifi, Michael S. Brown, Kele Xu, Hengxing Cai, Yuzhong Liu, Li-Wen Wang,\n  Zhi-Song Liu, Chu-Tak Li, Sourya Dipta Das, Nisarg A. Shah, Akashdeep Jassal,\n  Tongtong Zhao, Shanshan Zhao, Sabari Nathan, M. Parisa Beham, R. Suganya,\n  Qing Wang, Zhongyun Hu, Xin Huang, Yaning Li, Maitreya Suin, Kuldeep Purohit,\n  A. N. Rajagopalan, Densen Puthussery, Hrishikesh P S, Melvin Kuriakose, Jiji\n  C V, Yu Zhu, Liping Dong, Zhuolong Jiang, Chenghua Li, Cong Leng, Jian Cheng", "title": "AIM 2020: Scene Relighting and Illumination Estimation Challenge", "comments": "ECCVW 2020. Data and more information on\n  https://github.com/majedelhelou/VIDIT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the AIM 2020 challenge on virtual image relighting and illumination\nestimation. This paper presents the novel VIDIT dataset used in the challenge\nand the different proposed solutions and final evaluation results over the 3\nchallenge tracks. The first track considered one-to-one relighting; the\nobjective was to relight an input photo of a scene with a different color\ntemperature and illuminant orientation (i.e., light source position). The goal\nof the second track was to estimate illumination settings, namely the color\ntemperature and orientation, from a given image. Lastly, the third track dealt\nwith any-to-any relighting, thus a generalization of the first track. The\ntarget color temperature and orientation, rather than being pre-determined, are\ninstead given by a guide image. Participants were allowed to make use of their\ntrack 1 and 2 solutions for track 3. The tracks had 94, 52, and 56 registered\nparticipants, respectively, leading to 20 confirmed submissions in the final\ncompetition stage.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 09:16:43 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Helou", "Majed El", ""], ["Zhou", "Ruofan", ""], ["S\u00fcsstrunk", "Sabine", ""], ["Timofte", "Radu", ""], ["Afifi", "Mahmoud", ""], ["Brown", "Michael S.", ""], ["Xu", "Kele", ""], ["Cai", "Hengxing", ""], ["Liu", "Yuzhong", ""], ["Wang", "Li-Wen", ""], ["Liu", "Zhi-Song", ""], ["Li", "Chu-Tak", ""], ["Das", "Sourya Dipta", ""], ["Shah", "Nisarg A.", ""], ["Jassal", "Akashdeep", ""], ["Zhao", "Tongtong", ""], ["Zhao", "Shanshan", ""], ["Nathan", "Sabari", ""], ["Beham", "M. Parisa", ""], ["Suganya", "R.", ""], ["Wang", "Qing", ""], ["Hu", "Zhongyun", ""], ["Huang", "Xin", ""], ["Li", "Yaning", ""], ["Suin", "Maitreya", ""], ["Purohit", "Kuldeep", ""], ["Rajagopalan", "A. N.", ""], ["Puthussery", "Densen", ""], ["S", "Hrishikesh P", ""], ["Kuriakose", "Melvin", ""], ["C", "Jiji", "V"], ["Zhu", "Yu", ""], ["Dong", "Liping", ""], ["Jiang", "Zhuolong", ""], ["Li", "Chenghua", ""], ["Leng", "Cong", ""], ["Cheng", "Jian", ""]]}, {"id": "2009.12829", "submitter": "Yufei Wang", "authors": "Haoliang Li, YuFei Wang, Renjie Wan, Shiqi Wang, Tie-Qiang Li, Alex C.\n  Kot", "title": "Domain Generalization for Medical Imaging Classification with\n  Linear-Dependency Regularization", "comments": "Accepted by NeurIPS, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, we have witnessed great progress in the field of medical imaging\nclassification by adopting deep neural networks. However, the recent advanced\nmodels still require accessing sufficiently large and representative datasets\nfor training, which is often unfeasible in clinically realistic environments.\nWhen trained on limited datasets, the deep neural network is lack of\ngeneralization capability, as the trained deep neural network on data within a\ncertain distribution (e.g. the data captured by a certain device vendor or\npatient population) may not be able to generalize to the data with another\ndistribution.\n  In this paper, we introduce a simple but effective approach to improve the\ngeneralization capability of deep neural networks in the field of medical\nimaging classification. Motivated by the observation that the domain\nvariability of the medical images is to some extent compact, we propose to\nlearn a representative feature space through variational encoding with a novel\nlinear-dependency regularization term to capture the shareable information\namong medical data collected from different domains. As a result, the trained\nneural network is expected to equip with better generalization capability to\nthe \"unseen\" medical data. Experimental results on two challenging medical\nimaging classification tasks indicate that our method can achieve better\ncross-domain generalization capability compared with state-of-the-art\nbaselines.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 12:30:30 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 03:38:27 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2020 10:28:49 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Li", "Haoliang", ""], ["Wang", "YuFei", ""], ["Wan", "Renjie", ""], ["Wang", "Shiqi", ""], ["Li", "Tie-Qiang", ""], ["Kot", "Alex C.", ""]]}, {"id": "2009.12836", "submitter": "Lei Huang", "authors": "Lei Huang, Jie Qin, Yi Zhou, Fan Zhu, Li Liu, Ling Shao", "title": "Normalization Techniques in Training DNNs: Methodology, Analysis and\n  Application", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalization techniques are essential for accelerating the training and\nimproving the generalization of deep neural networks (DNNs), and have\nsuccessfully been used in various applications. This paper reviews and comments\non the past, present and future of normalization methods in the context of DNN\ntraining. We provide a unified picture of the main motivation behind different\napproaches from the perspective of optimization, and present a taxonomy for\nunderstanding the similarities and differences between them. Specifically, we\ndecompose the pipeline of the most representative normalizing activation\nmethods into three components: the normalization area partitioning,\nnormalization operation and normalization representation recovery. In doing so,\nwe provide insight for designing new normalization technique. Finally, we\ndiscuss the current progress in understanding normalization methods, and\nprovide a comprehensive review of the applications of normalization for\nparticular tasks, in which it can effectively solve the key issues.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 13:06:52 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Huang", "Lei", ""], ["Qin", "Jie", ""], ["Zhou", "Yi", ""], ["Zhu", "Fan", ""], ["Liu", "Li", ""], ["Shao", "Ling", ""]]}, {"id": "2009.12840", "submitter": "Hyesong Choi", "authors": "Hyesong Choi, Hunsang Lee, Sunkyung Kim, Sunok Kim, Seungryong Kim,\n  Kwanghoon Sohn, Dongbo Min", "title": "Adaptive confidence thresholding for monocular depth estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised monocular depth estimation has become an appealing solution\nto the lack of ground truth labels, but its reconstruction loss often produces\nover-smoothed results across object boundaries and is incapable of handling\nocclusion explicitly. In this paper, we propose a new approach to leverage\npseudo ground truth depth maps of stereo images generated from pretrained\nstereo matching methods. The confidence map of the pseudo ground truth depth\nmap is estimated to mitigate performance degeneration by inaccurate pseudo\ndepth maps. To cope with the prediction error of the confidence map itself, we\nalso leverage the threshold network that learns the threshold dynamically\nconditioned on the pseudo depth maps. The pseudo depth labels filtered out by\nthe thresholded confidence map are used to supervise the monocular depth\nnetwork. Furthermore, we propose the probabilistic framework that refines the\nmonocular depth map with the help of its uncertainty map through the\npixel-adaptive convolution (PAC) layer. Experimental results demonstrate\nsuperior performance to state-of-the-art monocular depth estimation methods.\nLastly, we exhibit that the proposed threshold learning can also be used to\nimprove the performance of existing confidence estimation approaches.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 13:26:16 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 08:04:04 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Choi", "Hyesong", ""], ["Lee", "Hunsang", ""], ["Kim", "Sunkyung", ""], ["Kim", "Sunok", ""], ["Kim", "Seungryong", ""], ["Sohn", "Kwanghoon", ""], ["Min", "Dongbo", ""]]}, {"id": "2009.12875", "submitter": "Julian Busch", "authors": "Julian Busch, Evgeniy Faerman, Matthias Schubert and Thomas Seidl", "title": "Learning Self-Expression Metrics for Scalable and Inductive Subspace\n  Clustering", "comments": null, "journal-ref": "NeurIPS 2020 Workshop: Self-Supervised Learning - Theory and\n  Practice", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering has established itself as a state-of-the-art approach to\nclustering high-dimensional data. In particular, methods relying on the\nself-expressiveness property have recently proved especially successful.\nHowever, they suffer from two major shortcomings: First, a quadratic-size\ncoefficient matrix is learned directly, preventing these methods from scaling\nbeyond small datasets. Secondly, the trained models are transductive and thus\ncannot be used to cluster out-of-sample data unseen during training. Instead of\nlearning self-expression coefficients directly, we propose a novel metric\nlearning approach to learn instead a subspace affinity function using a siamese\nneural network architecture. Consequently, our model benefits from a constant\nnumber of parameters and a constant-size memory footprint, allowing it to scale\nto considerably larger datasets. In addition, we can formally show that out\nmodel is still able to exactly recover subspace clusters given an independence\nassumption. The siamese architecture in combination with a novel geometric\nclassifier further makes our model inductive, allowing it to cluster\nout-of-sample data. Additionally, non-linear clusters can be detected by simply\nadding an auto-encoder module to the architecture. The whole model can then be\ntrained end-to-end in a self-supervised manner. This work in progress reports\npromising preliminary results on the MNIST dataset. In the spirit of\nreproducible research, me make all code publicly available. In future work we\nplan to investigate several extensions of our model and to expand experimental\nevaluation.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 15:40:12 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 00:06:37 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Busch", "Julian", ""], ["Faerman", "Evgeniy", ""], ["Schubert", "Matthias", ""], ["Seidl", "Thomas", ""]]}, {"id": "2009.12877", "submitter": "Faruk Ahmed PhD", "authors": "Faruk Ahmed, Md Sultan Mahmud, Kazi Ashraf Moinuddin, Mohammed\n  Istiaque Hyder and Mohammed Yeasin", "title": "Virtual Experience to Real World Application: Sidewalk Obstacle\n  Avoidance Using Reinforcement Learning for Visually Impaired", "comments": "Journal, to be submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a path free from obstacles that poses minimal risk is critical for\nsafe navigation. People who are sighted and people who are visually impaired\nrequire navigation safety while walking on a sidewalk. In this research we\ndeveloped an assistive navigation on a sidewalk by integrating sensory inputs\nusing reinforcement learning. We trained a Sidewalk Obstacle Avoidance Agent\n(SOAA) through reinforcement learning in a simulated robotic environment. A\nSidewalk Obstacle Conversational Agent (SOCA) is built by training a natural\nlanguage conversation agent with real conversation data. The SOAA along with\nSOCA was integrated in a prototype device called augmented guide (AG).\nEmpirical analysis showed that this prototype improved the obstacle avoidance\nexperience about 5% from a base case of 81.29%\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 15:42:03 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Ahmed", "Faruk", ""], ["Mahmud", "Md Sultan", ""], ["Moinuddin", "Kazi Ashraf", ""], ["Hyder", "Mohammed Istiaque", ""], ["Yeasin", "Mohammed", ""]]}, {"id": "2009.12881", "submitter": "Aniruddha Mazumdar", "authors": "Aniruddha Mazumdar and Prabin Kumar Bora", "title": "Two-stream Encoder-Decoder Network for Localizing Image Forgeries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel two-stream encoder-decoder network, which\nutilizes both the high-level and the low-level image features for precisely\nlocalizing forged regions in a manipulated image. This is motivated from the\nfact that the forgery creation process generally introduces both the high-level\nartefacts (e.g. unnatural contrast) and the low-level artefacts (e.g. noise\ninconsistency) to the forged images. In the proposed two-stream network, one\nstream learns the low-level manipulation-related features in the encoder side\nby extracting noise residuals through a set of high-pass filters in the first\nlayer of the encoder network. In the second stream, the encoder learns the\nhigh-level image manipulation features from the input image RGB values. The\ncoarse feature maps of both the encoders are upsampled by their corresponding\ndecoder network to produce dense feature maps. The dense feature maps of the\ntwo streams are concatenated and fed to a final convolutional layer with\nsigmoidal activation to produce pixel-wise prediction. We have carried out\nexperimental analysis on multiple standard forensics datasets to evaluate the\nperformance of the proposed method. The experimental results show the efficacy\nof the proposed method with respect to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 15:49:17 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Mazumdar", "Aniruddha", ""], ["Bora", "Prabin Kumar", ""]]}, {"id": "2009.12894", "submitter": "Bryar Shareef", "authors": "Bryar Shareef, Alex Vakanski, Min Xian, Phoebe E. Freer", "title": "ESTAN: Enhanced Small Tumor-Aware Network for Breast Ultrasound Image\n  Segmentation", "comments": "9 pages, 4 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast tumor segmentation is a critical task in computer-aided diagnosis\n(CAD) systems for breast cancer detection because accurate tumor size, shape\nand location are important for further tumor quantification and classification.\nHowever, segmenting small tumors in ultrasound images is challenging, due to\nthe speckle noise, varying tumor shapes and sizes among patients, and the\nexistence of tumor-like image regions. Recently, deep learning-based approaches\nhave achieved great success for biomedical image analysis, but current\nstate-of-the-art approaches achieve poor performance for segmenting small\nbreast tumors. In this paper, we propose a novel deep neural network\narchitecture, namely Enhanced Small Tumor-Aware Network (ESTAN), to accurately\nand robustly segment breast tumors. ESTAN introduces two encoders to extract\nand fuse image context information at different scales and utilizes\nrow-column-wise kernels in the encoder to adapt to breast anatomy. We validate\nthe proposed approach and compare it to nine state-of-the-art approaches on\nthree public breast ultrasound datasets using seven quantitative metrics. The\nresults demonstrate that the proposed approach achieves the best overall\nperformance and outperforms all other approaches on small tumor segmentation.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 16:42:59 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Shareef", "Bryar", ""], ["Vakanski", "Alex", ""], ["Xian", "Min", ""], ["Freer", "Phoebe E.", ""]]}, {"id": "2009.12916", "submitter": "Micol Marchetti-Bowick", "authors": "Sumit Kumar, Yiming Gu, Jerrick Hoang, Galen Clark Haynes, Micol\n  Marchetti-Bowick", "title": "Interaction-Based Trajectory Prediction Over a Hybrid Traffic Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavior prediction of traffic actors is an essential component of any\nreal-world self-driving system. Actors' long-term behaviors tend to be governed\nby their interactions with other actors or traffic elements (traffic lights,\nstop signs) in the scene. To capture this highly complex structure of\ninteractions, we propose to use a hybrid graph whose nodes represent both the\ntraffic actors as well as the static and dynamic traffic elements present in\nthe scene. The different modes of temporal interaction (e.g., stopping and\ngoing) among actors and traffic elements are explicitly modeled by graph edges.\nThis explicit reasoning about discrete interaction types not only helps in\npredicting future motion, but also enhances the interpretability of the model,\nwhich is important for safety-critical applications such as autonomous driving.\nWe predict actors' trajectories and interaction types using a graph neural\nnetwork, which is trained in a semi-supervised manner. We show that our\nproposed model, TrafficGraphNet, achieves state-of-the-art trajectory\nprediction accuracy while maintaining a high level of interpretability.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 18:20:03 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Kumar", "Sumit", ""], ["Gu", "Yiming", ""], ["Hoang", "Jerrick", ""], ["Haynes", "Galen Clark", ""], ["Marchetti-Bowick", "Micol", ""]]}, {"id": "2009.12927", "submitter": "Ren Yang", "authors": "Yannick Str\\\"umpler, Ren Yang, Radu Timofte", "title": "Learning to Improve Image Compression without Changing the Standard\n  Decoder", "comments": "Accepted to ECCV AIM Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years we have witnessed an increasing interest in applying Deep\nNeural Networks (DNNs) to improve the rate-distortion performance in image\ncompression. However, the existing approaches either train a post-processing\nDNN on the decoder side, or propose learning for image compression in an\nend-to-end manner. This way, the trained DNNs are required in the decoder,\nleading to the incompatibility to the standard image decoders (e.g., JPEG) in\npersonal computers and mobiles. Therefore, we propose learning to improve the\nencoding performance with the standard decoder. In this paper, We work on JPEG\nas an example. Specifically, a frequency-domain pre-editing method is proposed\nto optimize the distribution of DCT coefficients, aiming at facilitating the\nJPEG compression. Moreover, we propose learning the JPEG quantization table\njointly with the pre-editing network. Most importantly, we do not modify the\nJPEG decoder and therefore our approach is applicable when viewing images with\nthe widely used standard JPEG decoder. The experiments validate that our\napproach successfully improves the rate-distortion performance of JPEG in terms\nof various quality metrics, such as PSNR, MS-SSIM and LPIPS. Visually, this\ntranslates to better overall color retention especially when strong compression\nis applied. The codes are available at\nhttps://github.com/YannickStruempler/LearnedJPEG.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 19:24:42 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 14:51:31 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 20:48:11 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Str\u00fcmpler", "Yannick", ""], ["Yang", "Ren", ""], ["Timofte", "Radu", ""]]}, {"id": "2009.12931", "submitter": "Tashin Ahmed", "authors": "Tashin Ahmed and Noor Hossain Nuri Sabab", "title": "Classification and understanding of cloud structures via satellite\n  images with EfficientUNet", "comments": "7 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Climate change has been a common interest and the forefront of crucial\npolitical discussion and decision-making for many years. Shallow clouds play a\nsignificant role in understanding the Earth's climate, but they are challenging\nto interpret and represent in a climate model. By classifying these cloud\nstructures, there is a better possibility of understanding the physical\nstructures of the clouds, which would improve the climate model generation,\nresulting in a better prediction of climate change or forecasting weather\nupdate. Clouds organise in many forms, which makes it challenging to build\ntraditional rule-based algorithms to separate cloud features. In this paper,\nclassification of cloud organization patterns was performed using a new\nscaled-up version of Convolutional Neural Network (CNN) named as EfficientNet\nas the encoder and UNet as decoder where they worked as feature extractor and\nreconstructor of fine grained feature map and was used as a classifier, which\nwill help experts to understand how clouds will shape the future climate. By\nusing a segmentation model in a classification task, it was shown that with a\ngood encoder alongside UNet, it is possible to obtain good performance from\nthis dataset. Dice coefficient has been used for the final evaluation metric,\nwhich gave the score of 66.26\\% and 66.02\\% for public and private (test set)\nleaderboard on Kaggle competition respectively.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 19:50:05 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 20:24:44 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 17:56:42 GMT"}, {"version": "v4", "created": "Fri, 9 Jul 2021 22:21:19 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ahmed", "Tashin", ""], ["Sabab", "Noor Hossain Nuri", ""]]}, {"id": "2009.12942", "submitter": "Georgios Takos", "authors": "Georgios Takos", "title": "A Survey on Deep Learning Methods for Semantic Image Segmentation in\n  Real-Time", "comments": "34 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image segmentation is one of fastest growing areas in computer\nvision with a variety of applications. In many areas, such as robotics and\nautonomous vehicles, semantic image segmentation is crucial, since it provides\nthe necessary context for actions to be taken based on a scene understanding at\nthe pixel level. Moreover, the success of medical diagnosis and treatment\nrelies on the extremely accurate understanding of the data under consideration\nand semantic image segmentation is one of the important tools in many cases.\nRecent developments in deep learning have provided a host of tools to tackle\nthis problem efficiently and with increased accuracy. This work provides a\ncomprehensive analysis of state-of-the-art deep learning architectures in image\nsegmentation and, more importantly, an extensive list of techniques to achieve\nfast inference and computational efficiency. The origins of these techniques as\nwell as their strengths and trade-offs are discussed with an in-depth analysis\nof their impact in the area. The best-performing architectures are summarized\nwith a list of methods used to achieve these state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 20:30:10 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Takos", "Georgios", ""]]}, {"id": "2009.12950", "submitter": "Trevor Bergstrom", "authors": "Trevor Bergstrom, Humphrey Shi", "title": "Human-Object Interaction Detection:A Quick Survey and Examination of\n  Methods", "comments": "Published at The 1st International Workshop On Human-Centric\n  Multimedia Analysis, at ACM Multimedia Conference 2020", "journal-ref": null, "doi": "10.1145/3422852.3423481", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-object interaction detection is a relatively new task in the world of\ncomputer vision and visual semantic information extraction. With the goal of\nmachines identifying interactions that humans perform on objects, there are\nmany real-world use cases for the research in this field. To our knowledge,\nthis is the first general survey of the state-of-the-art and milestone works in\nthis field. We provide a basic survey of the developments in the field of\nhuman-object interaction detection. Many works in this field use multi-stream\nconvolutional neural network architectures, which combine features from\nmultiple sources in the input image. Most commonly these are the humans and\nobjects in question, as well as the spatial quality of the two. As far as we\nare aware, there have not been in-depth studies performed that look into the\nperformance of each component individually. In order to provide insight to\nfuture researchers, we perform an individualized study that examines the\nperformance of each component of a multi-stream convolutional neural network\narchitecture for human-object interaction detection. Specifically, we examine\nthe HORCNN architecture as it is a foundational work in the field. In addition,\nwe provide an in-depth look at the HICO-DET dataset, a popular benchmark in the\nfield of human-object interaction detection. Code and papers can be found at\nhttps://github.com/SHI-Labs/Human-Object-Interaction-Detection.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 20:58:39 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Bergstrom", "Trevor", ""], ["Shi", "Humphrey", ""]]}, {"id": "2009.12967", "submitter": "Connor Daly", "authors": "Connor Daly", "title": "Recognition and Synthesis of Object Transport Motion", "comments": "46 pages, MEng thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning typically requires vast numbers of training examples in order\nto be used successfully. Conversely, motion capture data is often expensive to\ngenerate, requiring specialist equipment, along with actors to generate the\nprescribed motions, meaning that motion capture datasets tend to be relatively\nsmall. Motion capture data does however provide a rich source of information\nthat is becoming increasingly useful in a wide variety of applications, from\ngesture recognition in human-robot interaction, to data driven animation.\n  This project illustrates how deep convolutional networks can be used,\nalongside specialized data augmentation techniques, on a small motion capture\ndataset to learn detailed information from sequences of a specific type of\nmotion (object transport). The project shows how these same augmentation\ntechniques can be scaled up for use in the more complex task of motion\nsynthesis.\n  By exploring recent developments in the concept of Generative Adversarial\nModels (GANs), specifically the Wasserstein GAN, this project outlines a model\nthat is able to successfully generate lifelike object transportation motions,\nwith the generated samples displaying varying styles and transport strategies.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 22:13:26 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Daly", "Connor", ""]]}, {"id": "2009.12975", "submitter": "Liang Gou", "authors": "Liang Gou, Lincan Zou, Nanxiang Li, Michael Hofmann, Arvind Kumar\n  Shekar, Axel Wendt and Liu Ren", "title": "VATLD: A Visual Analytics System to Assess, Understand and Improve\n  Traffic Light Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic light detection is crucial for environment perception and\ndecision-making in autonomous driving. State-of-the-art detectors are built\nupon deep Convolutional Neural Networks (CNNs) and have exhibited promising\nperformance. However, one looming concern with CNN based detectors is how to\nthoroughly evaluate the performance of accuracy and robustness before they can\nbe deployed to autonomous vehicles. In this work, we propose a visual analytics\nsystem, VATLD, equipped with a disentangled representation learning and\nsemantic adversarial learning, to assess, understand, and improve the accuracy\nand robustness of traffic light detectors in autonomous driving applications.\nThe disentangled representation learning extracts data semantics to augment\nhuman cognition with human-friendly visual summarization, and the semantic\nadversarial learning efficiently exposes interpretable robustness risks and\nenables minimal human interaction for actionable insights. We also demonstrate\nthe effectiveness of various performance improvement strategies derived from\nactionable insights with our visual analytics system, VATLD, and illustrate\nsome practical implications for safety-critical applications in autonomous\ndriving.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 22:39:00 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Gou", "Liang", ""], ["Zou", "Lincan", ""], ["Li", "Nanxiang", ""], ["Hofmann", "Michael", ""], ["Shekar", "Arvind Kumar", ""], ["Wendt", "Axel", ""], ["Ren", "Liu", ""]]}, {"id": "2009.12987", "submitter": "Sanghyun Son", "authors": "Sanghyun Son, Jaerin Lee, Seungjun Nah, Radu Timofte, Kyoung Mu Lee", "title": "AIM 2020 Challenge on Video Temporal Super-Resolution", "comments": "Published in ECCV 2020 Workshop (Advances in Image Manipulation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos in the real-world contain various dynamics and motions that may look\nunnaturally discontinuous in time when the recordedframe rate is low. This\npaper reports the second AIM challenge on Video Temporal Super-Resolution\n(VTSR), a.k.a. frame interpolation, with a focus on the proposed solutions,\nresults, and analysis. From low-frame-rate (15 fps) videos, the challenge\nparticipants are required to submit higher-frame-rate (30 and 60 fps) sequences\nby estimating temporally intermediate frames. To simulate realistic and\nchallenging dynamics in the real-world, we employ the REDS_VTSR dataset derived\nfrom diverse videos captured in a hand-held camera for training and evaluation\npurposes. There have been 68 registered participants in the competition, and 5\nteams (one withdrawn) have competed in the final testing phase. The winning\nteam proposes the enhanced quadratic video interpolation method and achieves\nstate-of-the-art on the VTSR task.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 00:10:29 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Son", "Sanghyun", ""], ["Lee", "Jaerin", ""], ["Nah", "Seungjun", ""], ["Timofte", "Radu", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2009.12991", "submitter": "Kaihua Tang", "authors": "Kaihua Tang, Jianqiang Huang, Hanwang Zhang", "title": "Long-Tailed Classification by Keeping the Good and Removing the Bad\n  Momentum Causal Effect", "comments": "This paper is accepted by NeurIPS 2020. The code is available on\n  GitHub: https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the class size grows, maintaining a balanced dataset across many classes\nis challenging because the data are long-tailed in nature; it is even\nimpossible when the sample-of-interest co-exists with each other in one\ncollectable unit, e.g., multiple visual instances in one image. Therefore,\nlong-tailed classification is the key to deep learning at scale. However,\nexisting methods are mainly based on re-weighting/re-sampling heuristics that\nlack a fundamental theory. In this paper, we establish a causal inference\nframework, which not only unravels the whys of previous methods, but also\nderives a new principled solution. Specifically, our theory shows that the SGD\nmomentum is essentially a confounder in long-tailed classification. On one\nhand, it has a harmful causal effect that misleads the tail prediction biased\ntowards the head. On the other hand, its induced mediation also benefits the\nrepresentation learning and head prediction. Our framework elegantly\ndisentangles the paradoxical effects of the momentum, by pursuing the direct\ncausal effect caused by an input sample. In particular, we use causal\nintervention in training, and counterfactual reasoning in inference, to remove\nthe \"bad\" while keep the \"good\". We achieve new state-of-the-arts on three\nlong-tailed visual recognition benchmarks: Long-tailed CIFAR-10/-100,\nImageNet-LT for image classification and LVIS for instance segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 00:32:11 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 03:36:22 GMT"}, {"version": "v3", "created": "Sat, 3 Oct 2020 12:02:59 GMT"}, {"version": "v4", "created": "Thu, 11 Feb 2021 04:10:13 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Tang", "Kaihua", ""], ["Huang", "Jianqiang", ""], ["Zhang", "Hanwang", ""]]}, {"id": "2009.12994", "submitter": "Bin Wu", "authors": "Bin Wu, Xue-Cheng Tai, and Talal Rahman", "title": "Sparse-data based 3D surface reconstruction with vector matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three dimensional surface reconstruction based on two dimensional sparse\ninformation in the form of only a small number of level lines of the surface\nwith moderately complex structures, containing both structured and unstructured\ngeometries, is considered in this paper. A new model has been proposed which is\nbased on the idea of using normal vector matching combined with a first order\nand a second order total variation regularizers. A fast algorithm based on the\naugmented Lagrangian is also proposed. Numerical experiments are provided\nshowing the effectiveness of the model and the algorithm in reconstructing\nsurfaces with detailed features and complex structures for both synthetic and\nreal world digital maps.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 00:36:49 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Wu", "Bin", ""], ["Tai", "Xue-Cheng", ""], ["Rahman", "Talal", ""]]}, {"id": "2009.13000", "submitter": "Zhongqi Yue", "authors": "Zhongqi Yue and Hanwang Zhang and Qianru Sun and Xian-Sheng Hua", "title": "Interventional Few-Shot Learning", "comments": "Accepted by NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We uncover an ever-overlooked deficiency in the prevailing Few-Shot Learning\n(FSL) methods: the pre-trained knowledge is indeed a confounder that limits the\nperformance. This finding is rooted from our causal assumption: a Structural\nCausal Model (SCM) for the causalities among the pre-trained knowledge, sample\nfeatures, and labels. Thanks to it, we propose a novel FSL paradigm:\nInterventional Few-Shot Learning (IFSL). Specifically, we develop three\neffective IFSL algorithmic implementations based on the backdoor adjustment,\nwhich is essentially a causal intervention towards the SCM of many-shot\nlearning: the upper-bound of FSL in a causal view. It is worth noting that the\ncontribution of IFSL is orthogonal to existing fine-tuning and meta-learning\nbased FSL methods, hence IFSL can improve all of them, achieving a new\n1-/5-shot state-of-the-art on \\textit{mini}ImageNet, \\textit{tiered}ImageNet,\nand cross-domain CUB. Code is released at https://github.com/yue-zhongqi/ifsl.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 01:16:54 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 06:51:09 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Yue", "Zhongqi", ""], ["Zhang", "Hanwang", ""], ["Sun", "Qianru", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "2009.13008", "submitter": "Anjul Tyagi", "authors": "Anjul Tyagi, Cong Xie, Klaus Mueller", "title": "Visual Steering for One-Shot Deep Neural Network Synthesis", "comments": "9 pages, submitted to IEEE Transactions on Visualization and Computer\n  Graphics, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advancements in the area of deep learning have shown the effectiveness\nof very large neural networks in several applications. However, as these deep\nneural networks continue to grow in size, it becomes more and more difficult to\nconfigure their many parameters to obtain good results. Presently, analysts\nmust experiment with many different configurations and parameter settings,\nwhich is labor-intensive and time-consuming. On the other hand, the capacity of\nfully automated techniques for neural network architecture search is limited\nwithout the domain knowledge of human experts. To deal with the problem, we\nformulate the task of neural network architecture optimization as a graph space\nexploration, based on the one-shot architecture search technique. In this\napproach, a super-graph of all candidate architectures is trained in one-shot\nand the optimal neural network is identified as a sub-graph. In this paper, we\npresent a framework that allows analysts to effectively build the solution\nsub-graph space and guide the network search by injecting their domain\nknowledge. Starting with the network architecture space composed of basic\nneural network components, analysts are empowered to effectively select the\nmost promising components via our one-shot search scheme. Applying this\ntechnique in an iterative manner allows analysts to converge to the best\nperforming neural network architecture for a given application. During the\nexploration, analysts can use their domain knowledge aided by cues provided\nfrom a scatterplot visualization of the search space to edit different\ncomponents and guide the search for faster convergence. We designed our\ninterface in collaboration with several deep learning researchers and its final\neffectiveness is evaluated with a user study and two case studies.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 01:48:45 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Tyagi", "Anjul", ""], ["Xie", "Cong", ""], ["Mueller", "Klaus", ""]]}, {"id": "2009.13015", "submitter": "Heng Pan", "authors": "Heng Pan", "title": "Cloud Removal for Remote Sensing Imagery via Spatial Attention\n  Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical remote sensing imagery has been widely used in many fields due to its\nhigh resolution and stable geometric properties. However, remote sensing\nimagery is inevitably affected by climate, especially clouds. Removing the\ncloud in the high-resolution remote sensing satellite image is an indispensable\npre-processing step before analyzing it. For the sake of large-scale training\ndata, neural networks have been successful in many image processing tasks, but\nthe use of neural networks to remove cloud in remote sensing imagery is still\nrelatively small. We adopt generative adversarial network to solve this task\nand introduce the spatial attention mechanism into the remote sensing imagery\ncloud removal task, proposes a model named spatial attention generative\nadversarial network (SpA GAN), which imitates the human visual mechanism, and\nrecognizes and focuses the cloud area with local-to-global spatial attention,\nthereby enhancing the information recovery of these areas and generating\ncloudless images with better quality...\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 02:13:23 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 08:17:05 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Pan", "Heng", ""]]}, {"id": "2009.13019", "submitter": "Panwen Hu", "authors": "Panwen Hu, Jiazhen Liu and Rui Huang", "title": "Concentrated Multi-Grained Multi-Attention Network for Video Based\n  Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusion is still a severe problem in the video-based Re-IDentification\n(Re-ID) task, which has a great impact on the success rate. The attention\nmechanism has been proved to be helpful in solving the occlusion problem by a\nlarge number of existing methods. However, their attention mechanisms still\nlack the capability to extract sufficient discriminative information into the\nfinal representations from the videos. The single attention module scheme\nemployed by existing methods cannot exploit multi-scale spatial cues, and the\nattention of the single module will be dispersed by multiple salient parts of\nthe person. In this paper, we propose a Concentrated Multi-grained\nMulti-Attention Network (CMMANet) where two multi-attention modules are\ndesigned to extract multi-grained information through processing multi-scale\nintermediate features. Furthermore, multiple attention submodules in each\nmulti-attention module can automatically discover multiple discriminative\nregions of the video frames. To achieve this goal, we introduce a diversity\nloss to diversify the submodules in each multi-attention module, and a\nconcentration loss to integrate their attention responses so that each\nsubmodule can strongly focus on a specific meaningful part. The experimental\nresults show that the proposed approach outperforms the state-of-the-art\nmethods by large margins on multiple public datasets.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 02:18:06 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Hu", "Panwen", ""], ["Liu", "Jiazhen", ""], ["Huang", "Rui", ""]]}, {"id": "2009.13044", "submitter": "Yixing Xu", "authors": "Yixing Xu, Chang Xu, Xinghao Chen, Wei Zhang, Chunjing Xu, Yunhe Wang", "title": "Kernel Based Progressive Distillation for Adder Neural Networks", "comments": "Accepted by NeurIPS 2020, spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adder Neural Networks (ANNs) which only contain additions bring us a new way\nof developing deep neural networks with low energy consumption. Unfortunately,\nthere is an accuracy drop when replacing all convolution filters by adder\nfilters. The main reason here is the optimization difficulty of ANNs using\n$\\ell_1$-norm, in which the estimation of gradient in back propagation is\ninaccurate. In this paper, we present a novel method for further improving the\nperformance of ANNs without increasing the trainable parameters via a\nprogressive kernel based knowledge distillation (PKKD) method. A convolutional\nneural network (CNN) with the same architecture is simultaneously initialized\nand trained as a teacher network, features and weights of ANN and CNN will be\ntransformed to a new space to eliminate the accuracy drop. The similarity is\nconducted in a higher-dimensional space to disentangle the difference of their\ndistributions using a kernel based method. Finally, the desired ANN is learned\nbased on the information from both the ground-truth and teacher, progressively.\nThe effectiveness of the proposed method for learning ANN with higher\nperformance is then well-verified on several benchmarks. For instance, the\nANN-50 trained using the proposed PKKD method obtains a 76.8\\% top-1 accuracy\non ImageNet dataset, which is 0.6\\% higher than that of the ResNet-50.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 03:29:19 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 07:11:37 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 03:39:58 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Xu", "Yixing", ""], ["Xu", "Chang", ""], ["Chen", "Xinghao", ""], ["Zhang", "Wei", ""], ["Xu", "Chunjing", ""], ["Wang", "Yunhe", ""]]}, {"id": "2009.13049", "submitter": "Chaoxing Huang", "authors": "Chaoxing Huang", "title": "Event-based Action Recognition Using Timestamp Image Encoding Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event camera is an asynchronous, high frequency vision sensor with low power\nconsumption, which is suitable for human action recognition task. It is vital\nto encode the spatial-temporal information of event data properly and use\nstandard computer vision tool to learn from the data. In this work, we propose\na timestamp image encoding 2D network, which takes the encoded spatial-temporal\nimages of the event data as input and output the action label. Experiment\nresults show that our method can achieve the same level of performance as those\nRGB-based benchmarks on real world action recognition, and also achieve the\nSOTA result on gesture recognition.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 03:48:14 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Huang", "Chaoxing", ""]]}, {"id": "2009.13055", "submitter": "Mingbao Lin", "authors": "Mingbao Lin, Rongrong Ji, Zihan Xu, Baochang Zhang, Yan Wang, Yongjian\n  Wu, Feiyue Huang, Chia-Wen Lin", "title": "Rotated Binary Neural Network", "comments": "Accepted by NeurIPS2020 (The 34th Conference on Neural Information\n  Processing Systems)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary Neural Network (BNN) shows its predominance in reducing the complexity\nof deep neural networks. However, it suffers severe performance degradation.\nOne of the major impediments is the large quantization error between the\nfull-precision weight vector and its binary vector. Previous works focus on\ncompensating for the norm gap while leaving the angular bias hardly touched. In\nthis paper, for the first time, we explore the influence of angular bias on the\nquantization error and then introduce a Rotated Binary Neural Network (RBNN),\nwhich considers the angle alignment between the full-precision weight vector\nand its binarized version. At the beginning of each training epoch, we propose\nto rotate the full-precision weight vector to its binary vector to reduce the\nangular bias. To avoid the high complexity of learning a large rotation matrix,\nwe further introduce a bi-rotation formulation that learns two smaller rotation\nmatrices. In the training stage, we devise an adjustable rotated weight vector\nfor binarization to escape the potential local optimum. Our rotation leads to\naround 50% weight flips which maximize the information gain. Finally, we\npropose a training-aware approximation of the sign function for the gradient\nbackward. Experiments on CIFAR-10 and ImageNet demonstrate the superiorities of\nRBNN over many state-of-the-arts. Our source code, experimental settings,\ntraining logs and binary models are available at\nhttps://github.com/lmbxmu/RBNN.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 04:22:26 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 15:10:00 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 09:06:18 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Lin", "Mingbao", ""], ["Ji", "Rongrong", ""], ["Xu", "Zihan", ""], ["Zhang", "Baochang", ""], ["Wang", "Yan", ""], ["Wu", "Yongjian", ""], ["Huang", "Feiyue", ""], ["Lin", "Chia-Wen", ""]]}, {"id": "2009.13075", "submitter": "Rajeev Yasarla", "authors": "Rajeev Yasarla, V.A. Sindagi, V.M. Patel", "title": "Semi-Supervised Image Deraining using Gaussian Processes", "comments": "arXiv admin note: substantial text overlap with 2006.05580", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent CNN-based methods for image deraining have achieved excellent\nperformance in terms of reconstruction error as well as visual quality.\nHowever, these methods are limited in the sense that they can be trained only\non fully labeled data. Due to various challenges in obtaining real world\nfully-labeled image deraining datasets, existing methods are trained only on\nsynthetically generated data and hence, generalize poorly to real-world images.\nThe use of real-world data in training image deraining networks is relatively\nless explored in the literature. We propose a Gaussian Process-based\nsemi-supervised learning framework which enables the network in learning to\nderain using synthetic dataset while generalizing better using unlabeled\nreal-world images. More specifically, we model the latent space vectors of\nunlabeled data using Gaussian Processes, which is then used to compute\npseudo-ground-truth for supervising the network on unlabeled data. Through\nextensive experiments and ablations on several challenging datasets (such as\nRain800, Rain200L and DDN-SIRR), we show that the proposed method is able to\neffectively leverage unlabeled data thereby resulting in significantly better\nperformance as compared to labeled-only training. Additionally, we demonstrate\nthat using unlabeled real-world images in the proposed GP-based framework\nresults\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 17:16:16 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Yasarla", "Rajeev", ""], ["Sindagi", "V. A.", ""], ["Patel", "V. M.", ""]]}, {"id": "2009.13077", "submitter": "Boyu Wang", "authors": "Boyu Wang, Huidong Liu, Dimitris Samaras, Minh Hoai", "title": "Distribution Matching for Crowd Counting", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In crowd counting, each training image contains multiple people, where each\nperson is annotated by a dot. Existing crowd counting methods need to use a\nGaussian to smooth each annotated dot or to estimate the likelihood of every\npixel given the annotated point. In this paper, we show that imposing Gaussians\nto annotations hurts generalization performance. Instead, we propose to use\nDistribution Matching for crowd COUNTing (DM-Count). In DM-Count, we use\nOptimal Transport (OT) to measure the similarity between the normalized\npredicted density map and the normalized ground truth density map. To stabilize\nOT computation, we include a Total Variation loss in our model. We show that\nthe generalization error bound of DM-Count is tighter than that of the Gaussian\nsmoothed methods. In terms of Mean Absolute Error, DM-Count outperforms the\nprevious state-of-the-art methods by a large margin on two large-scale counting\ndatasets, UCF-QNRF and NWPU, and achieves the state-of-the-art results on the\nShanghaiTech and UCF-CC50 datasets. DM-Count reduced the error of the\nstate-of-the-art published result by approximately 16%. Code is available at\nhttps://github.com/cvlab-stonybrook/DM-Count.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 04:57:23 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 23:53:54 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Wang", "Boyu", ""], ["Liu", "Huidong", ""], ["Samaras", "Dimitris", ""], ["Hoai", "Minh", ""]]}, {"id": "2009.13087", "submitter": "Yinxiao Li", "authors": "Yinxiao Li and Zhichao Lu and Xuehan Xiong and Jonathan Huang", "title": "PERF-Net: Pose Empowered RGB-Flow Net", "comments": "9 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, many works in the video action recognition literature have\nshown that two stream models (combining spatial and temporal input streams) are\nnecessary for achieving state of the art performance. In this paper we show the\nbenefits of including yet another stream based on human pose estimated from\neach frame -- specifically by rendering pose on input RGB frames. At first\nblush, this additional stream may seem redundant given that human pose is fully\ndetermined by RGB pixel values -- however we show (perhaps surprisingly) that\nthis simple and flexible addition can provide complementary gains. Using this\ninsight, we then propose a new model, which we dub PERF-Net (short for Pose\nEmpowered RGB-Flow Net), which combines this new pose stream with the standard\nRGB and flow based input streams via distillation techniques and show that our\nmodel outperforms the state-of-the-art by a large margin in a number of human\naction recognition datasets while not requiring flow or pose to be explicitly\ncomputed at inference time.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 06:06:51 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Li", "Yinxiao", ""], ["Lu", "Zhichao", ""], ["Xiong", "Xuehan", ""], ["Huang", "Jonathan", ""]]}, {"id": "2009.13108", "submitter": "Maolin Wang", "authors": "Maolin Wang, Seyedramin Rasoulinezhad, Philip H.W. Leong, Hayden K.H.\n  So", "title": "NITI: Training Integer Neural Networks Using Integer-only Arithmetic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While integer arithmetic has been widely adopted for improved performance in\ndeep quantized neural network inference, training remains a task primarily\nexecuted using floating point arithmetic. This is because both high dynamic\nrange and numerical accuracy are central to the success of most modern training\nalgorithms. However, due to its potential for computational, storage and energy\nadvantages in hardware accelerators, neural network training methods that can\nbe implemented with low precision integer-only arithmetic remains an active\nresearch challenge. In this paper, we present NITI, an efficient deep neural\nnetwork training framework that stores all parameters and intermediate values\nas integers, and computes exclusively with integer arithmetic. A pseudo\nstochastic rounding scheme that eliminates the need for external random number\ngeneration is proposed to facilitate conversion from wider intermediate results\nto low precision storage. Furthermore, a cross-entropy loss backpropagation\nscheme computed with integer-only arithmetic is proposed. A proof-of-concept\nopen-source software implementation of NITI that utilizes native 8-bit integer\noperations in modern GPUs to achieve end-to-end training is presented. When\ncompared with an equivalent training setup implemented with floating point\nstorage and arithmetic, NITI achieves negligible accuracy degradation on the\nMNIST and CIFAR10 datasets using 8-bit integer storage and computation. On\nImageNet, 16-bit integers are needed for weight accumulation with an 8-bit\ndatapath. This achieves training results comparable to all-floating-point\nimplementations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 07:41:36 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Wang", "Maolin", ""], ["Rasoulinezhad", "Seyedramin", ""], ["Leong", "Philip H. W.", ""], ["So", "Hayden K. H.", ""]]}, {"id": "2009.13112", "submitter": "Jiannan Xiang", "authors": "Jiannan Xiang, Xin Eric Wang, William Yang Wang", "title": "Learning to Stop: A Simple yet Effective Approach to Urban\n  Vision-Language Navigation", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-and-Language Navigation (VLN) is a natural language grounding task\nwhere an agent learns to follow language instructions and navigate to specified\ndestinations in real-world environments. A key challenge is to recognize and\nstop at the correct location, especially for complicated outdoor environments.\nExisting methods treat the STOP action equally as other actions, which results\nin undesirable behaviors that the agent often fails to stop at the destination\neven though it might be on the right path. Therefore, we propose Learning to\nStop (L2Stop), a simple yet effective policy module that differentiates STOP\nand other actions. Our approach achieves the new state of the art on a\nchallenging urban VLN dataset Touchdown, outperforming the baseline by 6.89%\n(absolute improvement) on Success weighted by Edit Distance (SED).\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 07:44:46 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 01:59:26 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 05:41:06 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Xiang", "Jiannan", ""], ["Wang", "Xin Eric", ""], ["Wang", "William Yang", ""]]}, {"id": "2009.13118", "submitter": "Jianqi Ma", "authors": "Jianqi Ma", "title": "RRPN++: Guidance Towards More Accurate Scene Text Detection", "comments": "Tech report, code will be released", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RRPN is among the outstanding scene text detection approaches, but the\nmanually-designed anchor and coarse proposal refinement make the performance\nstill far from perfection. In this paper, we propose RRPN++ to exploit the\npotential of RRPN-based model by several improvements. Based on RRPN, we\npropose the Anchor-free Pyramid Proposal Networks (APPN) to generate\nfirst-stage proposals, which adopts the anchor-free design to reduce proposal\nnumber and accelerate the inference speed. In our second stage, both the\ndetection branch and the recognition branch are incorporated to perform\nmulti-task learning. In inference stage, the detection branch outputs the\nproposal refinement and the recognition branch predicts the transcript of the\nrefined text region. Further, the recognition branch also helps rescore the\nproposals and eliminate the false positive proposals by the jointing filtering\nstrategy. With these enhancements, we boost the detection results by $6\\%$ of\nF-measure in ICDAR2015 compared to RRPN. Experiments conducted on other\nbenchmarks also illustrate the superior performance and efficiency of our\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 08:00:35 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Ma", "Jianqi", ""]]}, {"id": "2009.13120", "submitter": "Risheng Wang", "authors": "Tao Lei, Risheng Wang, Yong Wan, Bingtao Zhang, Hongying Meng and\n  Asoke K. Nandi", "title": "Medical Image Segmentation Using Deep Learning: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been widely used for medical image segmentation and a large\nnumber of papers has been presented recording the success of deep learning in\nthe field. In this paper, we present a comprehensive thematic survey on medical\nimage segmentation using deep learning techniques. This paper makes two\noriginal contributions. Firstly, compared to traditional surveys that directly\ndivide literatures of deep learning on medical image segmentation into many\ngroups and introduce literatures in detail for each group, we classify\ncurrently popular literatures according to a multi-level structure from coarse\nto fine. Secondly, this paper focuses on supervised and weakly supervised\nlearning approaches, without including unsupervised approaches since they have\nbeen introduced in many old surveys and they are not popular currently. For\nsupervised learning approaches, we analyze literatures in three aspects: the\nselection of backbone networks, the design of network blocks, and the\nimprovement of loss functions. For weakly supervised learning approaches, we\ninvestigate literature according to data augmentation, transfer learning, and\ninteractive segmentation, separately. Compared to existing surveys, this survey\nclassifies the literatures very differently from before and is more convenient\nfor readers to understand the relevant rationale and will guide them to think\nof appropriate improvements in medical image segmentation based on deep\nlearning approaches.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 08:05:02 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 03:17:09 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Lei", "Tao", ""], ["Wang", "Risheng", ""], ["Wan", "Yong", ""], ["Zhang", "Bingtao", ""], ["Meng", "Hongying", ""], ["Nandi", "Asoke K.", ""]]}, {"id": "2009.13134", "submitter": "Yuanfei Huang", "authors": "Yuanfei Huang, Jie Li, Xinbo Gao, Yanting Hu, Wen Lu", "title": "Interpretable Detail-Fidelity Attention Network for Single Image\n  Super-Resolution", "comments": "14 pages, submitted to IEEE Transactions, codes are available at\n  https://github.com/YuanfeiHuang/DeFiAN", "journal-ref": null, "doi": "10.1109/TIP.2021.3050856", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefiting from the strong capabilities of deep CNNs for feature\nrepresentation and nonlinear mapping, deep-learning-based methods have achieved\nexcellent performance in single image super-resolution. However, most existing\nSR methods depend on the high capacity of networks which is initially designed\nfor visual recognition, and rarely consider the initial intention of\nsuper-resolution for detail fidelity. Aiming at pursuing this intention, there\nare two challenging issues to be solved: (1) learning appropriate operators\nwhich is adaptive to the diverse characteristics of smoothes and details; (2)\nimproving the ability of model to preserve the low-frequency smoothes and\nreconstruct the high-frequency details. To solve them, we propose a purposeful\nand interpretable detail-fidelity attention network to progressively process\nthese smoothes and details in divide-and-conquer manner, which is a novel and\nspecific prospect of image super-resolution for the purpose on improving the\ndetail fidelity, instead of blindly designing or employing the deep CNNs\narchitectures for merely feature representation in local receptive fields.\nParticularly, we propose a Hessian filtering for interpretable feature\nrepresentation which is high-profile for detail inference, a dilated\nencoder-decoder and a distribution alignment cell to improve the inferred\nHessian features in morphological manner and statistical manner respectively.\nExtensive experiments demonstrate that the proposed methods achieve superior\nperformances over the state-of-the-art methods quantitatively and\nqualitatively. Code is available at https://github.com/YuanfeiHuang/DeFiAN.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 08:31:23 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Huang", "Yuanfei", ""], ["Li", "Jie", ""], ["Gao", "Xinbo", ""], ["Hu", "Yanting", ""], ["Lu", "Wen", ""]]}, {"id": "2009.13144", "submitter": "Mirsalar Kamari Mr.", "authors": "Mirsalar Kamari and Oguz Gunes", "title": "Segmentation and Analysis of a Sketched Truss Frame Using Morphological\n  Image Processing Techniques", "comments": null, "journal-ref": "Conference: (ICCACS) International Conference On Civil\n  Engineering, Architecture and Cityscape, July 2016, Istanbul, Turkey", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Development of computational tools to analyze and assess the building\ncapacities has had a major impact in civil engineering. The interaction with\nthe structural software packages is becoming easier and the modeling tools are\nbecoming smarter by automating the users role during their interaction with the\nsoftware. One of the difficulties and the most time consuming steps involved in\nthe structural modeling is defining the geometry of the structure to provide\nthe analysis. This paper is dedicated to the development of a methodology to\nautomate analysis of a hand sketched or computer generated truss frame drawn on\na piece of paper. First, we focus on the segmentation methodologies for hand\nsketched truss components using the morphological image processing techniques,\nand then we provide a real time analysis of the truss. We visualize and augment\nthe results on the input image to facilitate the public understanding of the\ntruss geometry and internal forces. MATLAB is used as the programming language\nfor the image processing purposes, and the truss is analyzed using Sap2000 API\nto integrate with MATLAB to provide a convenient structural analysis. This\npaper highlights the potential of the automation of the structural analysis\nusing image processing to quickly assess the efficiency of structural systems.\nFurther development of this framework is likely to revolutionize the way that\nstructures are modeled and analyzed.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 08:50:18 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Kamari", "Mirsalar", ""], ["Gunes", "Oguz", ""]]}, {"id": "2009.13146", "submitter": "William Agnew", "authors": "William Agnew, Christopher Xie, Aaron Walsman, Octavian Murad, Caelen\n  Wang, Pedro Domingos, Siddhartha Srinivasa", "title": "Amodal 3D Reconstruction for Robotic Manipulation via Stability and\n  Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based 3D object reconstruction enables single- or few-shot\nestimation of 3D object models. For robotics, this holds the potential to allow\nmodel-based methods to rapidly adapt to novel objects and scenes. Existing 3D\nreconstruction techniques optimize for visual reconstruction fidelity,\ntypically measured by chamfer distance or voxel IOU. We find that when applied\nto realistic, cluttered robotics environments, these systems produce\nreconstructions with low physical realism, resulting in poor task performance\nwhen used for model-based control. We propose ARM, an amodal 3D reconstruction\nsystem that introduces (1) a stability prior over object shapes, (2) a\nconnectivity prior, and (3) a multi-channel input representation that allows\nfor reasoning over relationships between groups of objects. By using these\npriors over the physical properties of objects, our system improves\nreconstruction quality not just by standard visual metrics, but also\nperformance of model-based control on a variety of robotics manipulation tasks\nin challenging, cluttered environments. Code is available at\ngithub.com/wagnew3/ARM.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 08:52:54 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Agnew", "William", ""], ["Xie", "Christopher", ""], ["Walsman", "Aaron", ""], ["Murad", "Octavian", ""], ["Wang", "Caelen", ""], ["Domingos", "Pedro", ""], ["Srinivasa", "Siddhartha", ""]]}, {"id": "2009.13148", "submitter": "Chen Shen", "authors": "Pochuan Wang, Chen Shen, Holger R. Roth, Dong Yang, Daguang Xu,\n  Masahiro Oda, Kazunari Misawa, Po-Ting Chen, Kao-Lang Liu, Wei-Chih Liao,\n  Weichung Wang, Kensaku Mori", "title": "Automated Pancreas Segmentation Using Multi-institutional Collaborative\n  Deep Learning", "comments": "Accepted by MICCAI DCL Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of deep learning-based methods strongly relies on the number\nof datasets used for training. Many efforts have been made to increase the data\nin the medical image analysis field. However, unlike photography images, it is\nhard to generate centralized databases to collect medical images because of\nnumerous technical, legal, and privacy issues. In this work, we study the use\nof federated learning between two institutions in a real-world setting to\ncollaboratively train a model without sharing the raw data across national\nboundaries. We quantitatively compare the segmentation models obtained with\nfederated learning and local training alone. Our experimental results show that\nfederated learning models have higher generalizability than standalone\ntraining.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 08:54:10 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Wang", "Pochuan", ""], ["Shen", "Chen", ""], ["Roth", "Holger R.", ""], ["Yang", "Dong", ""], ["Xu", "Daguang", ""], ["Oda", "Masahiro", ""], ["Misawa", "Kazunari", ""], ["Chen", "Po-Ting", ""], ["Liu", "Kao-Lang", ""], ["Liao", "Wei-Chih", ""], ["Wang", "Weichung", ""], ["Mori", "Kensaku", ""]]}, {"id": "2009.13158", "submitter": "Taimur Hassan", "authors": "Taimur Hassan and Samet Akcay and Mohammed Bennamoun and Salman Khan\n  and Naoufel Werghi", "title": "Trainable Structure Tensors for Autonomous Baggage Threat Detection\n  Under Extreme Occlusion", "comments": "ACCV-2020 Camera Ready, Source Code:\n  https://github.com/taimurhassan/TST", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting baggage threats is one of the most difficult tasks, even for expert\nofficers. Many researchers have developed computer-aided screening systems to\nrecognize these threats from the baggage X-ray scans. However, all of these\nframeworks are limited in identifying the contraband items under extreme\nocclusion. This paper presents a novel instance segmentation framework that\nutilizes trainable structure tensors to highlight the contours of the occluded\nand cluttered contraband items (by scanning multiple predominant orientations),\nwhile simultaneously suppressing the irrelevant baggage content. The proposed\nframework has been extensively tested on four publicly available X-ray datasets\nwhere it outperforms the state-of-the-art frameworks in terms of mean average\nprecision scores. Furthermore, to the best of our knowledge, it is the only\nframework that has been validated on combined grayscale and colored scans\nobtained from four different types of X-ray scanners.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 09:12:10 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 07:26:46 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Hassan", "Taimur", ""], ["Akcay", "Samet", ""], ["Bennamoun", "Mohammed", ""], ["Khan", "Salman", ""], ["Werghi", "Naoufel", ""]]}, {"id": "2009.13167", "submitter": "Qian Li", "authors": "Qian Li, Nan Guo, Xiaochun Ye, Dongrui Fan, and Zhimin Tang", "title": "Video Face Recognition System: RetinaFace-mnet-faster and Secondary\n  Search", "comments": "Accepted by FICC(Future of Information and Communication Conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition is widely used in the scene. However, different visual\nenvironments require different methods, and face recognition has a difficulty\nin complex environments. Therefore, this paper mainly experiments complex faces\nin the video. First, we design an image pre-processing module for fuzzy scene\nor under-exposed faces to enhance images. Our experimental results demonstrate\nthat effective images pre-processing improves the accuracy of 0.11%, 0.2% and\n1.4% on LFW, WIDER FACE and our datasets, respectively. Second, we propose\nRetinacFace-mnet-faster for detection and a confidence threshold specification\nfor face recognition, reducing the lost rate. Our experimental results show\nthat our RetinaFace-mnet-faster for 640*480 resolution on the Tesla P40 and\nsingle-thread improve speed of 16.7% and 70.2%, respectively. Finally, we\ndesign secondary search mechanism with HNSW to improve performance. Ours is\nsuitable for large-scale datasets, and experimental results show that our\nmethod is 82% faster than the violent retrieval for the single-frame detection.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 09:31:38 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 01:47:49 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Li", "Qian", ""], ["Guo", "Nan", ""], ["Ye", "Xiaochun", ""], ["Fan", "Dongrui", ""], ["Tang", "Zhimin", ""]]}, {"id": "2009.13217", "submitter": "Islem Rekik", "authors": "Ahmed Nebli, Ugur Ali Kaplan and Islem Rekik", "title": "Deep EvoGraphNet Architecture For Time-Dependent Brain Graph Data\n  Synthesis From a Single Timepoint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning how to predict the brain connectome (i.e. graph) development and\naging is of paramount importance for charting the future of within-disorder and\ncross-disorder landscape of brain dysconnectivity evolution. Indeed, predicting\nthe longitudinal (i.e., time-dependent ) brain dysconnectivity as it emerges\nand evolves over time from a single timepoint can help design personalized\ntreatments for disordered patients in a very early stage. Despite its\nsignificance, evolution models of the brain graph are largely overlooked in the\nliterature. Here, we propose EvoGraphNet, the first end-to-end geometric deep\nlearning-powered graph-generative adversarial network (gGAN) for predicting\ntime-dependent brain graph evolution from a single timepoint. Our EvoGraphNet\narchitecture cascades a set of time-dependent gGANs, where each gGAN\ncommunicates its predicted brain graphs at a particular timepoint to train the\nnext gGAN in the cascade at follow-up timepoint. Therefore, we obtain each next\npredicted timepoint by setting the output of each generator as the input of its\nsuccessor which enables us to predict a given number of timepoints using only\none single timepoint in an end- to-end fashion. At each timepoint, to better\nalign the distribution of the predicted brain graphs with that of the\nground-truth graphs, we further integrate an auxiliary Kullback-Leibler\ndivergence loss function. To capture time-dependency between two consecutive\nobservations, we impose an l1 loss to minimize the sparse distance between two\nserialized brain graphs. A series of benchmarks against variants and ablated\nversions of our EvoGraphNet showed that we can achieve the lowest brain graph\nevolution prediction error using a single baseline timepoint. Our EvoGraphNet\ncode is available at http://github.com/basiralab/EvoGraphNet.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 11:10:38 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Nebli", "Ahmed", ""], ["Kaplan", "Ugur Ali", ""], ["Rekik", "Islem", ""]]}, {"id": "2009.13239", "submitter": "Joan Puigcerver", "authors": "Joan Puigcerver, Carlos Riquelme, Basil Mustafa, Cedric Renggli,\n  Andr\\'e Susano Pinto, Sylvain Gelly, Daniel Keysers, Neil Houlsby", "title": "Scalable Transfer Learning with Expert Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer of pre-trained representations can improve sample efficiency and\nreduce computational requirements for new tasks. However, representations used\nfor transfer are usually generic, and are not tailored to a particular\ndistribution of downstream tasks. We explore the use of expert representations\nfor transfer with a simple, yet effective, strategy. We train a diverse set of\nexperts by exploiting existing label structures, and use cheap-to-compute\nperformance proxies to select the relevant expert for each target task. This\nstrategy scales the process of transferring to new tasks, since it does not\nrevisit the pre-training data during transfer. Accordingly, it requires little\nextra compute per target task, and results in a speed-up of 2-3 orders of\nmagnitude compared to competing approaches. Further, we provide an\nadapter-based architecture able to compress many experts into a single model.\nWe evaluate our approach on two different data sources and demonstrate that it\noutperforms baselines on over 20 diverse vision tasks in both cases.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 12:07:10 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Puigcerver", "Joan", ""], ["Riquelme", "Carlos", ""], ["Mustafa", "Basil", ""], ["Renggli", "Cedric", ""], ["Pinto", "Andr\u00e9 Susano", ""], ["Gelly", "Sylvain", ""], ["Keysers", "Daniel", ""], ["Houlsby", "Neil", ""]]}, {"id": "2009.13240", "submitter": "Rui Xu", "authors": "Rui Xu, Minghao Guo, Jiaqi Wang, Xiaoxiao Li, Bolei Zhou, Chen Change\n  Loy", "title": "Texture Memory-Augmented Deep Patch-Based Image Inpainting", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patch-based methods and deep networks have been employed to tackle image\ninpainting problem, with their own strengths and weaknesses. Patch-based\nmethods are capable of restoring a missing region with high-quality texture\nthrough searching nearest neighbor patches from the unmasked regions. However,\nthese methods bring problematic contents when recovering large missing regions.\nDeep networks, on the other hand, show promising results in completing large\nregions. Nonetheless, the results often lack faithful and sharp details that\nresemble the surrounding area. By bringing together the best of both paradigms,\nwe propose a new deep inpainting framework where texture generation is guided\nby a texture memory of patch samples extracted from unmasked regions. The\nframework has a novel design that allows texture memory retrieval to be trained\nend-to-end with the deep inpainting network. In addition, we introduce a patch\ndistribution loss to encourage high-quality patch synthesis. The proposed\nmethod shows superior performance both qualitatively and quantitatively on\nthree challenging image benchmarks, i.e., Places, CelebA-HQ, and Paris\nStreet-View datasets.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 12:09:08 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Xu", "Rui", ""], ["Guo", "Minghao", ""], ["Wang", "Jiaqi", ""], ["Li", "Xiaoxiao", ""], ["Zhou", "Bolei", ""], ["Loy", "Chen Change", ""]]}, {"id": "2009.13276", "submitter": "Mariella Dreissig", "authors": "Mariella Dreissig, Mohamed Hedi Baccour, Tim Schaeck, Enkelejda\n  Kasneci", "title": "Driver Drowsiness Classification Based on Eye Blink and Head Movement\n  Features Using the k-NN Algorithm", "comments": "accepted paper at IEEE Symposium on Computational Intelligence in\n  Feature Analysis, Selection and Learning in Image and Pattern Recognition as\n  part of the 2020 IEEE Symposium Series on Computational Intelligence (SSCI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern advanced driver-assistance systems analyze the driving performance to\ngather information about the driver's state. Such systems are able, for\nexample, to detect signs of drowsiness by evaluating the steering or lane\nkeeping behavior and to alert the driver when the drowsiness state reaches a\ncritical level. However, these kinds of systems have no access to direct cues\nabout the driver's state. Hence, the aim of this work is to extend the driver\ndrowsiness detection in vehicles using signals of a driver monitoring camera.\nFor this purpose, 35 features related to the driver's eye blinking behavior and\nhead movements are extracted in driving simulator experiments. Based on that\nlarge dataset, we developed and evaluated a feature selection method based on\nthe k-Nearest Neighbor algorithm for the driver's state classification. A\nconcluding analysis of the best performing feature sets yields valuable\ninsights about the influence of drowsiness on the driver's blink behavior and\nhead movements. These findings will help in the future development of robust\nand reliable driver drowsiness monitoring systems to prevent fatigue-induced\naccidents.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 12:37:38 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Dreissig", "Mariella", ""], ["Baccour", "Mohamed Hedi", ""], ["Schaeck", "Tim", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2009.13278", "submitter": "Arijit Mallick", "authors": "Arijit Mallick, J\\\"org St\\\"uckler, Hendrik Lensch", "title": "Learning to Adapt Multi-View Stereo by Self-Supervision", "comments": "19 pages, including supplementary, accepted and presented in BMVC\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D scene reconstruction from multiple views is an important classical problem\nin computer vision. Deep learning based approaches have recently demonstrated\nimpressive reconstruction results. When training such models, self-supervised\nmethods are favourable since they do not rely on ground truth data which would\nbe needed for supervised training and is often difficult to obtain. Moreover,\nlearned multi-view stereo reconstruction is prone to environment changes and\nshould robustly generalise to different domains. We propose an adaptive\nlearning approach for multi-view stereo which trains a deep neural network for\nimproved adaptability to new target domains. We use model-agnostic\nmeta-learning (MAML) to train base parameters which, in turn, are adapted for\nmulti-view stereo on new domains through self-supervised training. Our\nevaluations demonstrate that the proposed adaptation method is effective in\nlearning self-supervised multi-view stereo reconstruction in new domains.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 12:42:36 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Mallick", "Arijit", ""], ["St\u00fcckler", "J\u00f6rg", ""], ["Lensch", "Hendrik", ""]]}, {"id": "2009.13289", "submitter": "Xi'an Li", "authors": "Xi-An Li, Lei Zhang, Li-Yan Wang, Jian Lu", "title": "Multi-scale Receptive Fields Graph Attention Network for Point Cloud\n  Classification", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the implication of point cloud is still challenging to achieve\nthe goal of classification or segmentation due to the irregular and sparse\nstructure of point cloud. As we have known, PointNet architecture as a\nground-breaking work for point cloud which can learn efficiently shape features\ndirectly on unordered 3D point cloud and have achieved favorable performance.\nHowever, this model fail to consider the fine-grained semantic information of\nlocal structure for point cloud. Afterwards, many valuable works are proposed\nto enhance the performance of PointNet by means of semantic features of local\npatch for point cloud. In this paper, a multi-scale receptive fields graph\nattention network (named after MRFGAT) for point cloud classification is\nproposed. By focusing on the local fine features of point cloud and applying\nmulti attention modules based on channel affinity, the learned feature map for\nour network can well capture the abundant features information of point cloud.\nThe proposed MRFGAT architecture is tested on ModelNet10 and ModelNet40\ndatasets, and results show it achieves state-of-the-art performance in shape\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 13:01:28 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Li", "Xi-An", ""], ["Zhang", "Lei", ""], ["Wang", "Li-Yan", ""], ["Lu", "Jian", ""]]}, {"id": "2009.13290", "submitter": "Hang Du", "authors": "Hang Du, Hailin Shi, Dan Zeng, Xiaoping Zhang, and Tao Mei", "title": "The Elements of End-to-end Deep Face Recognition: A Survey of Recent\n  Advances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition is one of the most fundamental and long-standing topics in\ncomputer vision community. With the recent developments of deep convolutional\nneural networks and large-scale datasets, deep face recognition has made\nremarkable progress and been widely used in the real-world applications. Given\na natural image or video frame as input, an end-to-end deep face recognition\nsystem outputs the face feature for recognition. To achieve this, the whole\nsystem is generally built with three key elements: face detection, face\nalignment, and face representation. The face detection locates faces in the\nimage or frame. Then, the face alignment is proceeded to calibrate the faces to\na canonical view and crop them to a normalized pixel size. Finally, in the\nstage of face representation, the discriminative features are extracted from\nthe preprocessed faces for recognition. All of the three elements are fulfilled\nby deep convolutional neural networks. In this paper, we present a\ncomprehensive survey about the recent advances of every element of the\nend-to-end deep face recognition, since the thriving deep learning techniques\nhave greatly improved the capability of them. To start with, we introduce an\noverview of the end-to-end deep face recognition, which, as mentioned above,\nincludes face detection, face alignment, and face representation. Then, we\nreview the deep learning based advances of each element, respectively, covering\nmany aspects such as the up-to-date algorithm designs, evaluation metrics,\ndatasets, performance comparison, existing challenges, and promising directions\nfor future research. We hope this survey could bring helpful thoughts to one\nfor better understanding of the big picture of end-to-end face recognition and\ndeeper exploration in a systematic way.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 13:02:17 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 08:54:13 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Du", "Hang", ""], ["Shi", "Hailin", ""], ["Zeng", "Dan", ""], ["Zhang", "Xiaoping", ""], ["Mei", "Tao", ""]]}, {"id": "2009.13302", "submitter": "Josimar Chire Saire", "authors": "Josimar Chire, Esteban Wilfredo Vilca Zuniga", "title": "Characterization of Covid-19 Dataset using Complex Networks and Image\n  Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to explore the structure of pattern behind covid-19 dataset.\nThe dataset includes medical images with positive and negative cases. A sample\nof 100 sample is chosen, 50 per each class. An histogram frequency is\ncalculated to get features using statistical measurements, besides a feature\nextraction using Grey Level Co-Occurrence Matrix (GLCM). Using both features\nare build Complex Networks respectively to analyze the adjacency matrices and\ncheck the presence of patterns. Initial experiments introduces the evidence of\nhidden patterns in the dataset for each class, which are visible using Complex\nNetworks representation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 20:35:31 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Chire", "Josimar", ""], ["Zuniga", "Esteban Wilfredo Vilca", ""]]}, {"id": "2009.13304", "submitter": "Lucie Leveque", "authors": "Lucie L\\'ev\\^eque (UNIV GUSTAVE EIFFEL), Ji Yang, Xiaohan Yang,\n  Pengfei Guo, Kenneth Dasalla, Leida Li, Yingying Wu, Hantao Liu", "title": "Cuid: A new study of perceived image quality and its subjective\n  assessment", "comments": null, "journal-ref": "27th IEEE International Conference on Image Processing (ICIP), Oct\n  2020, Abu Dhabi, United Arab Emirates", "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on image quality assessment (IQA) remains limited mainly due to our\nincomplete knowledge about human visual perception. Existing IQA algorithms\nhave been designed or trained with insufficient subjective data with a small\ndegree of stimulus variability. This has led to challenges for those algorithms\nto handle complexity and diversity of real-world digital content. Perceptual\nevidence from human subjects serves as a grounding for the development of\nadvanced IQA algorithms. It is thus critical to acquire reliable subjective\ndata with controlled perception experiments that faithfully reflect human\nbehavioural responses to distortions in visual signals. In this paper, we\npresent a new study of image quality perception where subjective ratings were\ncollected in a controlled lab environment. We investigate how quality\nperception is affected by a combination of different categories of images and\ndifferent types and levels of distortions. The database will be made publicly\navailable to facilitate calibration and validation of IQA algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 13:14:45 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["L\u00e9v\u00eaque", "Lucie", "", "UNIV GUSTAVE EIFFEL"], ["Yang", "Ji", ""], ["Yang", "Xiaohan", ""], ["Guo", "Pengfei", ""], ["Dasalla", "Kenneth", ""], ["Li", "Leida", ""], ["Wu", "Yingying", ""], ["Liu", "Hantao", ""]]}, {"id": "2009.13311", "submitter": "Olivier Teytaud", "authors": "Baptiste Roziere and Fabien Teytaud and Vlad Hosu and Hanhe Lin and\n  Jeremy Rapin and Mariia Zameshina and Olivier Teytaud", "title": "EvolGAN: Evolutionary Generative Adversarial Networks", "comments": "accepted ACCV oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to use a quality estimator and evolutionary methods to search the\nlatent space of generative adversarial networks trained on small, difficult\ndatasets, or both. The new method leads to the generation of significantly\nhigher quality images while preserving the original generator's diversity.\nHuman raters preferred an image from the new version with frequency 83.7pc for\nCats, 74pc for FashionGen, 70.4pc for Horses, and 69.2pc for Artworks, and\nminor improvements for the already excellent GANs for faces. This approach\napplies to any quality scorer and GAN generator.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 13:31:13 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Roziere", "Baptiste", ""], ["Teytaud", "Fabien", ""], ["Hosu", "Vlad", ""], ["Lin", "Hanhe", ""], ["Rapin", "Jeremy", ""], ["Zameshina", "Mariia", ""], ["Teytaud", "Olivier", ""]]}, {"id": "2009.13318", "submitter": "Conor Horgan", "authors": "Conor C. Horgan, Magnus Jensen, Anika Nagelkerke, Jean-Phillipe\n  St-Pierre, Tom Vercauteren, Molly M. Stevens, Mads S. Bergholt", "title": "High-throughput molecular imaging via deep learning enabled Raman\n  spectroscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Raman spectroscopy enables non-destructive, label-free imaging with\nunprecedented molecular contrast but is limited by slow data acquisition,\nlargely preventing high-throughput imaging applications. Here, we present a\ncomprehensive framework for higher-throughput molecular imaging via deep\nlearning enabled Raman spectroscopy, termed DeepeR, trained on a large dataset\nof hyperspectral Raman images, with over 1.5 million spectra (400 hours of\nacquisition) in total. We firstly perform denoising and reconstruction of low\nsignal-to-noise ratio Raman molecular signatures via deep learning, with a 9x\nimprovement in mean squared error over state-of-the-art Raman filtering\nmethods. Next, we develop a neural network for robust 2-4x super-resolution of\nhyperspectral Raman images that preserves molecular cellular information.\nCombining these approaches, we achieve Raman imaging speed-ups of up to 160x,\nenabling high resolution, high signal-to-noise ratio cellular imaging in under\none minute. Finally, transfer learning is applied to extend DeepeR from cell to\ntissue-scale imaging. DeepeR provides a foundation that will enable a host of\nhigher-throughput Raman spectroscopy and molecular imaging applications across\nbiomedicine.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 13:40:14 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Horgan", "Conor C.", ""], ["Jensen", "Magnus", ""], ["Nagelkerke", "Anika", ""], ["St-Pierre", "Jean-Phillipe", ""], ["Vercauteren", "Tom", ""], ["Stevens", "Molly M.", ""], ["Bergholt", "Mads S.", ""]]}, {"id": "2009.13323", "submitter": "Philippe Burlina", "authors": "Philippe M. Burlina, William Paul, Phil A. Mathew, Neil J. Joshi,\n  Alison W. Rebman, John N. Aucott", "title": "AI Progress in Skin Lesion Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine progress in the use of AI for detecting skin lesions, with\nparticular emphasis on the erythema migrans rash of acute Lyme disease, and\nother lesions, such as those from conditions like herpes zoster (shingles),\ntinea corporis, erythema multiforme, cellulitis, insect bites, or tick bites.\nWe discuss important challenges for these applications, in particular the\nproblems of AI bias regarding the lack of skin images in dark skinned\nindividuals, being able to accurately detect, delineate, and segment lesions or\nregions of interest compared to normal skin in images, and low shot learning\n(addressing classification with a paucity of training images). Solving these\nproblems ranges from being highly desirable requirements -- e.g. for\ndelineation, which may be useful to disambiguate between similar types of\nlesions, and perform improved diagnostics -- or required, as is the case for AI\nde-biasing, to allow for the deployment of fair AI techniques in the clinic for\nskin lesion analysis. For the problem of low shot learning in particular, we\nreport skin analysis algorithms that gracefully degrade and still perform well\nat low shots, when compared to baseline algorithms: when using a little as 10\ntraining exemplars per class, the baseline DL algorithm performance\nsignificantly degrades, with accuracy of 56.41%, close to chance, whereas the\nbest performing low shot algorithm yields an accuracy of 85.26%.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 13:44:50 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 16:58:15 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Burlina", "Philippe M.", ""], ["Paul", "William", ""], ["Mathew", "Phil A.", ""], ["Joshi", "Neil J.", ""], ["Rebman", "Alison W.", ""], ["Aucott", "John N.", ""]]}, {"id": "2009.13331", "submitter": "He Huang", "authors": "He Huang, Shunta Saito, Yuta Kikuchi, Eiichi Matsumoto, Wei Tang,\n  Philip S. Yu", "title": "Addressing Class Imbalance in Scene Graph Parsing by Learning to\n  Contrast and Score", "comments": "ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scene graph parsing aims to detect objects in an image scene and recognize\ntheir relations. Recent approaches have achieved high average scores on some\npopular benchmarks, but fail in detecting rare relations, as the highly\nlong-tailed distribution of data biases the learning towards frequent labels.\nMotivated by the fact that detecting these rare relations can be critical in\nreal-world applications, this paper introduces a novel integrated framework of\nclassification and ranking to resolve the class imbalance problem in scene\ngraph parsing. Specifically, we design a new Contrasting Cross-Entropy loss,\nwhich promotes the detection of rare relations by suppressing incorrect\nfrequent ones. Furthermore, we propose a novel scoring module, termed as\nScorer, which learns to rank the relations based on the image features and\nrelation features to improve the recall of predictions. Our framework is simple\nand effective, and can be incorporated into current scene graph models.\nExperimental results show that the proposed approach improves the current\nstate-of-the-art methods, with a clear advantage of detecting rare relations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 13:57:59 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 13:06:45 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Huang", "He", ""], ["Saito", "Shunta", ""], ["Kikuchi", "Yuta", ""], ["Matsumoto", "Eiichi", ""], ["Tang", "Wei", ""], ["Yu", "Philip S.", ""]]}, {"id": "2009.13333", "submitter": "Lei Huang", "authors": "Lei Huang, Yi Zhou, Li Liu, Fan Zhu, Ling Shao", "title": "Group Whitening: Balancing Learning Efficiency and Representational\n  Capacity", "comments": "V4: camera version of CVPR 2021. Code available at:\n  https://github.com/huangleiBuaa/GroupWhitening", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch normalization (BN) is an important technique commonly incorporated into\ndeep learning models to perform standardization within mini-batches. The merits\nof BN in improving a model's learning efficiency can be further amplified by\napplying whitening, while its drawbacks in estimating population statistics for\ninference can be avoided through group normalization (GN). This paper proposes\ngroup whitening (GW), which exploits the advantages of the whitening operation\nand avoids the disadvantages of normalization within mini-batches. In addition,\nwe analyze the constraints imposed on features by normalization, and show how\nthe batch size (group number) affects the performance of batch (group)\nnormalized networks, from the perspective of model's representational capacity.\nThis analysis provides theoretical guidance for applying GW in practice.\nFinally, we apply the proposed GW to ResNet and ResNeXt architectures and\nconduct experiments on the ImageNet and COCO benchmarks. Results show that GW\nconsistently improves the performance of different architectures, with absolute\ngains of $1.02\\%$ $\\sim$ $1.49\\%$ in top-1 accuracy on ImageNet and $1.82\\%$\n$\\sim$ $3.21\\%$ in bounding box AP on COCO.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 14:00:07 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 12:29:51 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 09:46:16 GMT"}, {"version": "v4", "created": "Tue, 6 Apr 2021 04:17:27 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Huang", "Lei", ""], ["Zhou", "Yi", ""], ["Liu", "Li", ""], ["Zhu", "Fan", ""], ["Shao", "Ling", ""]]}, {"id": "2009.13339", "submitter": "Abhishek Sharma", "authors": "Abhishek Sharma and Maks Ovsjanikov", "title": "Weakly Supervised Deep Functional Map for Shape Matching", "comments": "Accepted to appear in proceedings of Neurips 2020. Code available at:\n  \\url{https://github.com/Not-IITian/Weakly-supervised-Functional-map}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of deep functional maps have been proposed recently, from fully\nsupervised to totally unsupervised, with a range of loss functions as well as\ndifferent regularization terms. However, it is still not clear what are minimum\ningredients of a deep functional map pipeline and whether such ingredients\nunify or generalize all recent work on deep functional maps. We show\nempirically minimum components for obtaining state of the art results with\ndifferent loss functions, supervised as well as unsupervised. Furthermore, we\npropose a novel framework designed for both full-to-full as well as partial to\nfull shape matching that achieves state of the art results on several benchmark\ndatasets outperforming even the fully supervised methods by a significant\nmargin. Our code is publicly available at\nhttps://github.com/Not-IITian/Weakly-supervised-Functional-map\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 14:06:46 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Sharma", "Abhishek", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "2009.13342", "submitter": "Naiyu Gao", "authors": "Naiyu Gao, Yanhu Shan, Xin Zhao, Kaiqi Huang", "title": "Learning Category- and Instance-Aware Pixel Embedding for Fast Panoptic\n  Segmentation", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3090522", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoptic segmentation (PS) is a complex scene understanding task that\nrequires providing high-quality segmentation for both thing objects and stuff\nregions. Previous methods handle these two classes with semantic and instance\nsegmentation modules separately, following with heuristic fusion or additional\nmodules to resolve the conflicts between the two outputs. This work simplifies\nthis pipeline of PS by consistently modeling the two classes with a novel PS\nframework, which extends a detection model with an extra module to predict\ncategory- and instance-aware pixel embedding (CIAE). CIAE is a novel pixel-wise\nembedding feature that encodes both semantic-classification and\ninstance-distinction information. At the inference process, PS results are\nsimply derived by assigning each pixel to a detected instance or a stuff class\naccording to the learned embedding. Our method not only demonstrates fast\ninference speed but also the first one-stage method to achieve comparable\nperformance to two-stage methods on the challenging COCO benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 14:07:50 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 01:13:09 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Gao", "Naiyu", ""], ["Shan", "Yanhu", ""], ["Zhao", "Xin", ""], ["Huang", "Kaiqi", ""]]}, {"id": "2009.13364", "submitter": "Haifeng Li", "authors": "Haifeng Li, Zhenqi Cui, Zhiqing Zhu, Li Chen, Jiawei Zhu, Haozhe\n  Huang, Chao Tao", "title": "RS-MetaNet: Deep meta metric learning for few-shot remote sensing scene\n  classification", "comments": "13 pages, 11 figures", "journal-ref": "IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, 2020", "doi": "10.1109/TGRS.2020.3027387", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a modern deep neural network on massive labeled samples is the main\nparadigm in solving the scene classification problem for remote sensing, but\nlearning from only a few data points remains a challenge. Existing methods for\nfew-shot remote sensing scene classification are performed in a sample-level\nmanner, resulting in easy overfitting of learned features to individual samples\nand inadequate generalization of learned category segmentation surfaces. To\nsolve this problem, learning should be organized at the task level rather than\nthe sample level. Learning on tasks sampled from a task family can help tune\nlearning algorithms to perform well on new tasks sampled in that family.\nTherefore, we propose a simple but effective method, called RS-MetaNet, to\nresolve the issues related to few-shot remote sensing scene classification in\nthe real world. On the one hand, RS-MetaNet raises the level of learning from\nthe sample to the task by organizing training in a meta way, and it learns to\nlearn a metric space that can well classify remote sensing scenes from a series\nof tasks. We also propose a new loss function, called Balance Loss, which\nmaximizes the generalization ability of the model to new samples by maximizing\nthe distance between different categories, providing the scenes in different\ncategories with better linear segmentation planes while ensuring model fit. The\nexperimental results on three open and challenging remote sensing datasets,\nUCMerced\\_LandUse, NWPU-RESISC45, and Aerial Image Data, demonstrate that our\nproposed RS-MetaNet method achieves state-of-the-art results in cases where\nthere are only 1-20 labeled samples.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 14:34:15 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Li", "Haifeng", ""], ["Cui", "Zhenqi", ""], ["Zhu", "Zhiqing", ""], ["Chen", "Li", ""], ["Zhu", "Jiawei", ""], ["Huang", "Haozhe", ""], ["Tao", "Chao", ""]]}, {"id": "2009.13395", "submitter": "Satoshi Takahashi", "authors": "Satoshi Takahashi, Keiko Yamaguchi, and Asuka Watanabe", "title": "CAT STREET: Chronicle Archive of Tokyo Street-fashion", "comments": "19 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The analysis of daily-life fashion trends can provide us a profound\nunderstanding of our societies and cultures. However, no appropriate digital\narchive exists that includes images illustrating what people wore in their\ndaily lives over an extended period. In this study, we propose a new fashion\nimage archive, Chronicle Archive of Tokyo Street-fashion (CAT STREET), to shed\nlight on daily-life fashion trends. CAT STREET includes images showing what\npeople wore in their daily lives during 1970--2017, and these images contain\ntimestamps and street location annotations. This novel database combined with\nmachine learning enables us to observe daily-life fashion trends over a long\nterm and analyze them quantitatively. To evaluate the potential of our proposed\napproach with the novel database, we corroborated the rules-of-thumb of two\nfashion trend phenomena that have been observed and discussed qualitatively in\nprevious studies. Through these empirical analyses, we verified that our\napproach to quantify fashion trends can help in exploring unsolved research\nquestions. We also demonstrate CAT STREET's potential to find new standpoints\nto promote the understanding of societies and cultures through fashion embedded\nin consumers' daily lives.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 15:16:45 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 13:54:35 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Takahashi", "Satoshi", ""], ["Yamaguchi", "Keiko", ""], ["Watanabe", "Asuka", ""]]}, {"id": "2009.13420", "submitter": "K K Thyagharajan", "authors": "S.D. Lalitha, K.K. Thyagharajan", "title": "A Study on Lip Localization Techniques used for Lip reading from a Video", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper some of the different techniques used to localize the lips from\nthe face are discussed and compared along with its processing steps. Lip\nlocalization is the basic step needed to read the lips for extracting visual\ninformation from the video input. The techniques could be applied on asymmetric\nlips and also on the mouth with visible teeth, tongue & mouth with moustache.\nIn the process of Lip reading the following steps are generally used. They are,\ninitially locating lips in the first frame of the video input, then tracking\nthe lips in the following frames using the resulting pixel points of initial\nstep and at last converting the tracked lip model to its corresponding matched\nletter to give the visual information. A new proposal is also initiated from\nthe discussed techniques. The lip reading is useful in Automatic Speech\nRecognition when the audio is absent or present low with or without noise in\nthe communication systems. Human Computer communication also will require\nspeech recognition.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 15:36:35 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Lalitha", "S. D.", ""], ["Thyagharajan", "K. K.", ""]]}, {"id": "2009.13436", "submitter": "Davide Nitti", "authors": "Etienne Perot, Pierre de Tournemire, Davide Nitti, Jonathan Masci,\n  Amos Sironi", "title": "Learning to Detect Objects with a 1 Megapixel Event Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras encode visual information with high temporal precision, low\ndata-rate, and high-dynamic range. Thanks to these characteristics, event\ncameras are particularly suited for scenarios with high motion, challenging\nlighting conditions and requiring low latency. However, due to the novelty of\nthe field, the performance of event-based systems on many vision tasks is still\nlower compared to conventional frame-based solutions. The main reasons for this\nperformance gap are: the lower spatial resolution of event sensors, compared to\nframe cameras; the lack of large-scale training datasets; the absence of well\nestablished deep learning architectures for event-based processing. In this\npaper, we address all these problems in the context of an event-based object\ndetection task. First, we publicly release the first high-resolution\nlarge-scale dataset for object detection. The dataset contains more than 14\nhours recordings of a 1 megapixel event camera, in automotive scenarios,\ntogether with 25M bounding boxes of cars, pedestrians, and two-wheelers,\nlabeled at high frequency. Second, we introduce a novel recurrent architecture\nfor event-based detection and a temporal consistency loss for better-behaved\ntraining. The ability to compactly represent the sequence of events into the\ninternal memory of the model is essential to achieve high accuracy. Our model\noutperforms by a large margin feed-forward event-based architectures. Moreover,\nour method does not require any reconstruction of intensity images from events,\nshowing that training directly from raw events is possible, more efficient, and\nmore accurate than passing through an intermediate intensity image. Experiments\non the dataset introduced in this work, for which events and gray level images\nare available, show performance on par with that of highly tuned and studied\nframe-based detectors.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 16:03:59 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 15:41:24 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Perot", "Etienne", ""], ["de Tournemire", "Pierre", ""], ["Nitti", "Davide", ""], ["Masci", "Jonathan", ""], ["Sironi", "Amos", ""]]}, {"id": "2009.13443", "submitter": "Mahmoud Yassien Shams El Den", "authors": "Amira. A. Elsonbaty and Mahmoud Shams", "title": "The Smart Parking Management System", "comments": "12 pages, 15 figures", "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 12, No 4, August 2020", "doi": "10.5121/ijcsit.2020.12405", "report-no": null, "categories": "cs.CY cs.AR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With growing, Car parking increases with the number of car users. With the\nincreased use of smartphones and their applications, users prefer mobile\nphone-based solutions. This paper proposes the Smart Parking Management System\n(SPMS) that depends on Arduino parts, Android applications, and based on IoT.\nThis gave the client the ability to check available parking spaces and reserve\na parking spot. IR sensors are utilized to know if a car park space is allowed.\nIts area data are transmitted using the WI-FI module to the server and are\nrecovered by the mobile application which offers many options attractively and\nwith no cost to users and lets the user check reservation details. With IoT\ntechnology, the smart parking system can be connected wirelessly to easily\ntrack available locations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 16:08:10 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Elsonbaty", "Amira. A.", ""], ["Shams", "Mahmoud", ""]]}, {"id": "2009.13450", "submitter": "Mahmoud Yassien Shams El Den", "authors": "Mahmoud Shams, Amira. A. Elsonbaty, Wael. Z. ElSawy", "title": "Arabic Handwritten Character Recognition based on Convolution Neural\n  Networks and Support Vector Machine", "comments": "6 pages, 3 figures, 7 tables", "journal-ref": null, "doi": "10.14569/IJACSA.2020.0110819", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recognition of Arabic characters is essential for natural language processing\nand computer vision fields. The need to recognize and classify the handwritten\nArabic letters and characters are essentially required. In this paper, we\npresent an algorithm for recognizing Arabic letters and characters based on\nusing deep convolution neural networks (DCNN) and support vector machine (SVM).\nThis paper addresses the problem of recognizing the Arabic handwritten\ncharacters by determining the similarity between the input templates and the\npre-stored templates using both fully connected DCNN and dropout SVM.\nFurthermore, this paper determines the correct classification rate (CRR)\ndepends on the accuracy of the corrected classified templates, of the\nrecognized handwritten Arabic characters. Moreover, we determine the error\nclassification rate (ECR). The experimental results of this work indicate the\nability of the proposed algorithm to recognize, identify, and verify the input\nhandwritten Arabic characters. Furthermore, the proposed system determines\nsimilar Arabic characters using a clustering algorithm based on the K-means\nclustering approach to handle the problem of multi-stroke in Arabic characters.\nThe comparative evaluation is stated and the system accuracy reached 95.07% CRR\nwith 4.93% ECR compared with the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 16:18:52 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Shams", "Mahmoud", ""], ["Elsonbaty", "Amira. A.", ""], ["ElSawy", "Wael. Z.", ""]]}, {"id": "2009.13454", "submitter": "Mubariz Zaffar", "authors": "Mihnea-Alexandru Tomit\\u{a}, Mubariz Zaffar, Michael Milford, Klaus\n  McDonald-Maier and Shoaib Ehsan", "title": "ConvSequential-SLAM: A Sequence-based, Training-less Visual Place\n  Recognition Technique for Changing Environments", "comments": "10 pages, currently under-review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Place Recognition (VPR) is the ability to correctly recall a\npreviously visited place under changing viewpoints and appearances. A large\nnumber of handcrafted and deep-learning-based VPR techniques exist, where the\nformer suffer from appearance changes and the latter have significant\ncomputational needs. In this paper, we present a new handcrafted VPR technique\nthat achieves state-of-the-art place matching performance under challenging\nconditions. Our technique combines the best of 2 existing trainingless VPR\ntechniques, SeqSLAM and CoHOG, which are each robust to conditional and\nviewpoint changes, respectively. This blend, namely ConvSequential-SLAM,\nutilises sequential information and block-normalisation to handle appearance\nchanges, while using regional-convolutional matching to achieve\nviewpoint-invariance. We analyse content-overlap in-between query frames to\nfind a minimum sequence length, while also re-using the image entropy\ninformation for environment-based sequence length tuning. State-of-the-art\nperformance is reported in contrast to 8 contemporary VPR techniques on 4\npublic datasets. Qualitative insights and an ablation study on sequence length\nare also provided.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 16:31:29 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Tomit\u0103", "Mihnea-Alexandru", ""], ["Zaffar", "Mubariz", ""], ["Milford", "Michael", ""], ["McDonald-Maier", "Klaus", ""], ["Ehsan", "Shoaib", ""]]}, {"id": "2009.13460", "submitter": "Vinay Kumar", "authors": "Jasmine Kaur and Vinay Kumar", "title": "A complete character recognition and transliteration technique for\n  Devanagari script", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transliteration involves transformation of one script to another based on\nphonetic similarities between the characters of two distinctive scripts. In\nthis paper, we present a novel technique for automatic transliteration of\nDevanagari script using character recognition. One of the first tasks performed\nto isolate the constituent characters is segmentation. Line segmentation\nmethodology in this manuscript discusses the case of overlapping lines.\nCharacter segmentation algorithm is designed to segment conjuncts and separate\nshadow characters. Presented shadow character segmentation scheme employs\nconnected component method to isolate the character, keeping the constituent\ncharacters intact. Statistical features namely different order moments like\narea, variance, skewness and kurtosis along with structural features of\ncharacters are employed in two phase recognition process. After recognition,\nconstituent Devanagari characters are mapped to corresponding roman alphabets\nin way that resulting roman alphabets have similar pronunciation to source\ncharacters.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 16:43:18 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Kaur", "Jasmine", ""], ["Kumar", "Vinay", ""]]}, {"id": "2009.13501", "submitter": "Koushik Biswas", "authors": "Koushik Biswas, Sandeep Kumar, Shilpak Banerjee, Ashish Kumar Pandey", "title": "EIS -- a family of activation functions combining Exponential, ISRU, and\n  Softplus", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activation functions play a pivotal role in the function learning using\nneural networks. The non-linearity in the learned function is achieved by\nrepeated use of the activation function. Over the years, numerous activation\nfunctions have been proposed to improve accuracy in several tasks. Basic\nfunctions like ReLU, Sigmoid, Tanh, or Softplus have been favorite among the\ndeep learning community because of their simplicity. In recent years, several\nnovel activation functions arising from these basic functions have been\nproposed, which have improved accuracy in some challenging datasets. We propose\na five hyper-parameters family of activation functions, namely EIS, defined as,\n\\[ \\frac{x(\\ln(1+e^x))^\\alpha}{\\sqrt{\\beta+\\gamma x^2}+\\delta e^{-\\theta x}}.\n\\] We show examples of activation functions from the EIS family which\noutperform widely used activation functions on some well known datasets and\nmodels. For example, $\\frac{x\\ln(1+e^x)}{x+1.16e^{-x}}$ beats ReLU by 0.89\\% in\nDenseNet-169, 0.24\\% in Inception V3 in CIFAR100 dataset while 1.13\\% in\nInception V3, 0.13\\% in DenseNet-169, 0.94\\% in SimpleNet model in CIFAR10\ndataset. Also, $\\frac{x\\ln(1+e^x)}{\\sqrt{1+x^2}}$ beats ReLU by 1.68\\% in\nDenseNet-169, 0.30\\% in Inception V3 in CIFAR100 dataset while 1.0\\% in\nInception V3, 0.15\\% in DenseNet-169, 1.13\\% in SimpleNet model in CIFAR10\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 17:48:24 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 15:51:12 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Biswas", "Koushik", ""], ["Kumar", "Sandeep", ""], ["Banerjee", "Shilpak", ""], ["Pandey", "Ashish Kumar", ""]]}, {"id": "2009.13504", "submitter": "Peiyuan Liao", "authors": "Peiyuan Liao, Han Zhao, Keyulu Xu, Tommi Jaakkola, Geoffrey Gordon,\n  Stefanie Jegelka, Ruslan Salakhutdinov", "title": "Information Obfuscation of Graph Neural Networks", "comments": "ICML 2021; Code is available at https://github.com/liaopeiyuan/GAL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the advent of Graph Neural Networks (GNNs) has greatly improved node\nand graph representation learning in many applications, the neighborhood\naggregation scheme exposes additional vulnerabilities to adversaries seeking to\nextract node-level information about sensitive attributes. In this paper, we\nstudy the problem of protecting sensitive attributes by information obfuscation\nwhen learning with graph structured data. We propose a framework to locally\nfilter out pre-determined sensitive attributes via adversarial training with\nthe total variation and the Wasserstein distance. Our method creates a strong\ndefense against inference attacks, while only suffering small loss in task\nperformance. Theoretically, we analyze the effectiveness of our framework\nagainst a worst-case adversary, and characterize an inherent trade-off between\nmaximizing predictive accuracy and minimizing information leakage. Experiments\nacross multiple datasets from recommender systems, knowledge graphs and quantum\nchemistry demonstrate that the proposed approach provides a robust defense\nacross various graph structures and tasks, while producing competitive GNN\nencoders for downstream tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 17:55:04 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 14:34:52 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 16:27:46 GMT"}, {"version": "v4", "created": "Sun, 9 May 2021 06:25:27 GMT"}, {"version": "v5", "created": "Sun, 13 Jun 2021 05:35:04 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Liao", "Peiyuan", ""], ["Zhao", "Han", ""], ["Xu", "Keyulu", ""], ["Jaakkola", "Tommi", ""], ["Gordon", "Geoffrey", ""], ["Jegelka", "Stefanie", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "2009.13509", "submitter": "Daniel Wu", "authors": "Daniel J Wu, Andrew C Yang, Vinay U Prabhu", "title": "Afro-MNIST: Synthetic generation of MNIST-style datasets for\n  low-resource languages", "comments": "10 pages, 11 figures, presented as a workshop paper at Practical\n  Machine Learning for Developing Countries @ ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Afro-MNIST, a set of synthetic MNIST-style datasets for four\northographies used in Afro-Asiatic and Niger-Congo languages: Ge`ez (Ethiopic),\nVai, Osmanya, and N'Ko. These datasets serve as \"drop-in\" replacements for\nMNIST. We also describe and open-source a method for synthetic MNIST-style\ndataset generation from single examples of each digit. These datasets can be\nfound at https://github.com/Daniel-Wu/AfroMNIST. We hope that MNIST-style\ndatasets will be developed for other numeral systems, and that these datasets\nvitalize machine learning education in underrepresented nations in the research\ncommunity.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 17:57:40 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Wu", "Daniel J", ""], ["Yang", "Andrew C", ""], ["Prabhu", "Vinay U", ""]]}, {"id": "2009.13583", "submitter": "Chuanbo Wang", "authors": "Chuanbo Wang, Ye Guo, Wei Chen, Zeyun Yu", "title": "Fully Automatic Intervertebral Disc Segmentation Using Multimodal 3D\n  U-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intervertebral discs (IVDs), as small joints lying between adjacent\nvertebrae, have played an important role in pressure buffering and tissue\nprotection. The fully-automatic localization and segmentation of IVDs have been\ndiscussed in the literature for many years since they are crucial to spine\ndisease diagnosis and provide quantitative parameters in the treatment.\nTraditionally hand-crafted features are derived based on image intensities and\nshape priors to localize and segment IVDs. With the advance of deep learning,\nvarious neural network models have gained great success in image analysis\nincluding the recognition of intervertebral discs. Particularly, U-Net stands\nout among other approaches due to its outstanding performance on biomedical\nimages with a relatively small set of training data. This paper proposes a\nnovel convolutional framework based on 3D U-Net to segment IVDs from\nmulti-modality MRI images. We first localize the centers of intervertebral\ndiscs in each spine sample and then train the network based on the cropped\nsmall volumes centered at the localized intervertebral discs. A detailed\ncomprehensive analysis of the results using various combinations of\nmulti-modalities is presented. Furthermore, experiments conducted on 2D and 3D\nU-Nets with augmented and non-augmented datasets are demonstrated and compared\nin terms of Dice coefficient and Hausdorff distance. Our method has proved to\nbe effective with a mean segmentation Dice coefficient of 89.0% and a standard\ndeviation of 1.4%.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 18:58:24 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Wang", "Chuanbo", ""], ["Guo", "Ye", ""], ["Chen", "Wei", ""], ["Yu", "Zeyun", ""]]}, {"id": "2009.13592", "submitter": "Kemal Oksuz", "authors": "Kemal Oksuz and Baris Can Cam and Emre Akbas and Sinan Kalkan", "title": "A Ranking-based, Balanced Loss Function Unifying Classification and\n  Localisation in Object Detection", "comments": "NeurIPS 2020 spotlight paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose average Localisation-Recall-Precision (aLRP), a unified, bounded,\nbalanced and ranking-based loss function for both classification and\nlocalisation tasks in object detection. aLRP extends the\nLocalisation-Recall-Precision (LRP) performance metric (Oksuz et al., 2018)\ninspired from how Average Precision (AP) Loss extends precision to a\nranking-based loss function for classification (Chen et al., 2020). aLRP has\nthe following distinct advantages: (i) aLRP is the first ranking-based loss\nfunction for both classification and localisation tasks. (ii) Thanks to using\nranking for both tasks, aLRP naturally enforces high-quality localisation for\nhigh-precision classification. (iii) aLRP provides provable balance between\npositives and negatives. (iv) Compared to on average $\\sim$6 hyperparameters in\nthe loss functions of state-of-the-art detectors, aLRP Loss has only one\nhyperparameter, which we did not tune in practice. On the COCO dataset, aLRP\nLoss improves its ranking-based predecessor, AP Loss, up to around $5$ AP\npoints, achieves $48.9$ AP without test time augmentation and outperforms all\none-stage detectors. Code available at: https://github.com/kemaloksuz/aLRPLoss .\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 19:24:51 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 07:20:56 GMT"}, {"version": "v3", "created": "Wed, 30 Dec 2020 10:38:52 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2021 08:25:43 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Oksuz", "Kemal", ""], ["Cam", "Baris Can", ""], ["Akbas", "Emre", ""], ["Kalkan", "Sinan", ""]]}, {"id": "2009.13615", "submitter": "Milad Abdollahzadeh", "authors": "Milad Abdollahzadeh, Touba Malekzadeh, Hadi Seyedarabi", "title": "Multi-focus Image Fusion for Visual Sensor Networks", "comments": "5 pages", "journal-ref": null, "doi": "10.1109/IranianCEE.2016.7585790", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image fusion in visual sensor networks (VSNs) aims to combine information\nfrom multiple images of the same scene in order to transform a single image\nwith more information. Image fusion methods based on discrete cosine transform\n(DCT) are less complex and time-saving in DCT based standards of image and\nvideo which makes them more suitable for VSN applications. In this paper, an\nefficient algorithm for the fusion of multi-focus images in the DCT domain is\nproposed. The Sum of modified laplacian (SML) of corresponding blocks of source\nimages is used as a contrast criterion and blocks with the larger value of SML\nare absorbed to output images. The experimental results on several images show\nthe improvement of the proposed algorithm in terms of both subjective and\nobjective quality of fused image relative to other DCT based techniques.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 20:39:35 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 03:01:04 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 18:04:32 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Abdollahzadeh", "Milad", ""], ["Malekzadeh", "Touba", ""], ["Seyedarabi", "Hadi", ""]]}, {"id": "2009.13627", "submitter": "Xiaoran Zhang", "authors": "Xiaoran Zhang and Michelle Noga and David Glynn Martin and Kumaradevan\n  Punithakumar", "title": "Fully Automated Left Atrium Segmentation from Anatomical Cine Long-axis\n  MRI Sequences using Deep Convolutional Neural Network with Unscented Kalman\n  Filter", "comments": "Accepted by Medical Image Analysis 2020", "journal-ref": null, "doi": "10.1016/j.media.2020.101916", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes a fully automated approach for the left atrial\nsegmentation from routine cine long-axis cardiac magnetic resonance image\nsequences using deep convolutional neural networks and Bayesian filtering. The\nproposed approach consists of a classification network that automatically\ndetects the type of long-axis sequence and three different convolutional neural\nnetwork models followed by unscented Kalman filtering (UKF) that delineates the\nleft atrium. Instead of training and predicting all long-axis sequence types\ntogether, the proposed approach first identifies the image sequence type as to\n2, 3 and 4 chamber views, and then performs prediction based on neural nets\ntrained for that particular sequence type. The datasets were acquired\nretrospectively and ground truth manual segmentation was provided by an expert\nradiologist. In addition to neural net based classification and segmentation,\nanother neural net is trained and utilized to select image sequences for\nfurther processing using UKF to impose temporal consistency over cardiac cycle.\nA cyclic dynamic model with time-varying angular frequency is introduced in UKF\nto characterize the variations in cardiac motion during image scanning. The\nproposed approach was trained and evaluated separately with varying amount of\ntraining data with images acquired from 20, 40, 60 and 80 patients. Evaluations\nover 1515 images with equal number of images from each chamber group acquired\nfrom an additional 20 patients demonstrated that the proposed model\noutperformed state-of-the-art and yielded a mean Dice coefficient value of\n94.1%, 93.7% and 90.1% for 2, 3 and 4-chamber sequences, respectively, when\ntrained with datasets from 80 patients.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 21:06:35 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 18:47:25 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Zhang", "Xiaoran", ""], ["Noga", "Michelle", ""], ["Martin", "David Glynn", ""], ["Punithakumar", "Kumaradevan", ""]]}, {"id": "2009.13634", "submitter": "Zeyu Fu", "authors": "Zeyu Fu, Yang Sun, Xiangyu Zhang, Scott Stainton, Shaun Barney, Jeffry\n  Hogg, William Innes and Satnam Dlay", "title": "MPG-Net: Multi-Prediction Guided Network for Segmentation of Retinal\n  Layers in OCT Images", "comments": "EUSIPCO2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical coherence tomography (OCT) is a commonly-used method of extracting\nhigh resolution retinal information. Moreover there is an increasing demand for\nthe automated retinal layer segmentation which facilitates the retinal disease\ndiagnosis. In this paper, we propose a novel multiprediction guided attention\nnetwork (MPG-Net) for automated retinal layer segmentation in OCT images. The\nproposed method consists of two major steps to strengthen the discriminative\npower of a U-shape Fully convolutional network (FCN) for reliable automated\nsegmentation. Firstly, the feature refinement module which adaptively\nre-weights the feature channels is exploited in the encoder to capture more\ninformative features and discard information in irrelevant regions.\nFurthermore, we propose a multi-prediction guided attention mechanism which\nprovides pixel-wise semantic prediction guidance to better recover the\nsegmentation mask at each scale. This mechanism which transforms the deep\nsupervision to supervised attention is able to guide feature aggregation with\nmore semantic information between intermediate layers. Experiments on the\npublicly available Duke OCT dataset confirm the effectiveness of the proposed\nmethod as well as an improved performance over other state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 21:22:22 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Fu", "Zeyu", ""], ["Sun", "Yang", ""], ["Zhang", "Xiangyu", ""], ["Stainton", "Scott", ""], ["Barney", "Shaun", ""], ["Hogg", "Jeffry", ""], ["Innes", "William", ""], ["Dlay", "Satnam", ""]]}, {"id": "2009.13635", "submitter": "Zeyu Fu", "authors": "Zeyu Fu, Jianbo Jiao, Michael Suttie, J. Alison Noble", "title": "Cross-Task Representation Learning for Anatomical Landmark Detection", "comments": "MICCAI-MLMI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there is an increasing demand for automatically detecting\nanatomical landmarks which provide rich structural information to facilitate\nsubsequent medical image analysis. Current methods related to this task often\nleverage the power of deep neural networks, while a major challenge in fine\ntuning such models in medical applications arises from insufficient number of\nlabeled samples. To address this, we propose to regularize the knowledge\ntransfer across source and target tasks through cross-task representation\nlearning. The proposed method is demonstrated for extracting facial anatomical\nlandmarks which facilitate the diagnosis of fetal alcohol syndrome. The source\nand target tasks in this work are face recognition and landmark detection,\nrespectively. The main idea of the proposed method is to retain the feature\nrepresentations of the source model on the target task data, and to leverage\nthem as an additional source of supervisory signals for regularizing the target\nmodel learning, thereby improving its performance under limited training\nsamples. Concretely, we present two approaches for the proposed representation\nlearning by constraining either final or intermediate model features on the\ntarget model. Experimental results on a clinical face image dataset demonstrate\nthat the proposed approach works well with few labeled data, and outperforms\nother compared approaches.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 21:22:49 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Fu", "Zeyu", ""], ["Jiao", "Jianbo", ""], ["Suttie", "Michael", ""], ["Noble", "J. Alison", ""]]}, {"id": "2009.13664", "submitter": "Eugene Tam", "authors": "Eugene Tam, Shenfei Jiang, Paul Duan, Shawn Meng, Yue Pang, Cayden\n  Huang, Yi Han, Jacke Xie, Yuanjun Cui, Jinsong Yu, Minggui Lu", "title": "Breaking the Memory Wall for AI Chip with a New Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in deep learning have led to the widespread adoption of\nartificial intelligence (AI) in applications such as computer vision and\nnatural language processing. As neural networks become deeper and larger, AI\nmodeling demands outstrip the capabilities of conventional chip architectures.\nMemory bandwidth falls behind processing power. Energy consumption comes to\ndominate the total cost of ownership. Currently, memory capacity is\ninsufficient to support the most advanced NLP models. In this work, we present\na 3D AI chip, called Sunrise, with near-memory computing architecture to\naddress these three challenges. This distributed, near-memory computing\narchitecture allows us to tear down the performance-limiting memory wall with\nan abundance of data bandwidth. We achieve the same level of energy efficiency\non 40nm technology as competing chips on 7nm technology. By moving to similar\ntechnologies as other AI chips, we project to achieve more than ten times the\nenergy efficiency, seven times the performance of the current state-of-the-art\nchips, and twenty times of memory capacity as compared with the best chip in\neach benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 22:34:10 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Tam", "Eugene", ""], ["Jiang", "Shenfei", ""], ["Duan", "Paul", ""], ["Meng", "Shawn", ""], ["Pang", "Yue", ""], ["Huang", "Cayden", ""], ["Han", "Yi", ""], ["Xie", "Jacke", ""], ["Cui", "Yuanjun", ""], ["Yu", "Jinsong", ""], ["Lu", "Minggui", ""]]}, {"id": "2009.13682", "submitter": "Xiaowei Hu", "authors": "Xiaowei Hu, Xi Yin, Kevin Lin, Lijuan Wang, Lei Zhang, Jianfeng Gao,\n  Zicheng Liu", "title": "VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is highly desirable yet challenging to generate image captions that can\ndescribe novel objects which are unseen in caption-labeled training data, a\ncapability that is evaluated in the novel object captioning challenge (nocaps).\nIn this challenge, no additional image-caption training data, other thanCOCO\nCaptions, is allowed for model training. Thus, conventional Vision-Language\nPre-training (VLP) methods cannot be applied. This paper presents VIsual\nVOcabulary pretraining (VIVO) that performs pre-training in the absence of\ncaption annotations. By breaking the dependency of paired image-caption\ntraining data in VLP, VIVO can leverage large amounts of paired image-tag data\nto learn a visual vocabulary. This is done by pre-training a multi-layer\nTransformer model that learns to align image-level tags with their\ncorresponding image region features. To address the unordered nature of image\ntags, VIVO uses a Hungarian matching loss with masked tag prediction to conduct\npre-training. We validate the effectiveness of VIVO by fine-tuning the\npre-trained model for image captioning. In addition, we perform an analysis of\nthe visual-text alignment inferred by our model. The results show that our\nmodel can not only generate fluent image captions that describe novel objects,\nbut also identify the locations of these objects. Our single model has achieved\nnew state-of-the-art results on nocaps and surpassed the human CIDEr score.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 23:20:02 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 20:01:10 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Hu", "Xiaowei", ""], ["Yin", "Xi", ""], ["Lin", "Kevin", ""], ["Wang", "Lijuan", ""], ["Zhang", "Lei", ""], ["Gao", "Jianfeng", ""], ["Liu", "Zicheng", ""]]}, {"id": "2009.13684", "submitter": "Douglas Meneghetti", "authors": "Douglas De Rizzo Meneghetti, Thiago Pedro Donadon Homem, Jonas\n  Henrique Renolfi de Oliveira, Isaac Jesus da Silva, Danilo Hernani Perico,\n  Reinaldo Augusto da Costa Bianchi", "title": "Detecting soccer balls with reduced neural networks: a comparison of\n  multiple architectures under constrained hardware scenarios", "comments": "11-page version of a ~24-page version published in the Journal of\n  Intelligent & Robotics Systems", "journal-ref": null, "doi": "10.1007/s10846-021-01336-y", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection techniques that achieve state-of-the-art detection accuracy\nemploy convolutional neural networks, implemented to have optimal performance\nin graphics processing units. Some hardware systems, such as mobile robots,\noperate under constrained hardware situations, but still benefit from object\ndetection capabilities. Multiple network models have been proposed, achieving\ncomparable accuracy with reduced architectures and leaner operations. Motivated\nby the need to create an object detection system for a soccer team of mobile\nrobots, this work provides a comparative study of recent proposals of neural\nnetworks targeted towards constrained hardware environments, in the specific\ntask of soccer ball detection. We train multiple open implementations of\nMobileNetV2 and MobileNetV3 models with different underlying architectures, as\nwell as YOLOv3, TinyYOLOv3, YOLOv4 and TinyYOLOv4 in an annotated image data\nset captured using a mobile robot. We then report their mean average precision\non a test data set and their inference times in videos of different\nresolutions, under constrained and unconstrained hardware configurations.\nResults show that MobileNetV3 models have a good trade-off between mAP and\ninference time in constrained scenarios only, while MobileNetV2 with high width\nmultipliers are appropriate for server-side inference. YOLO models in their\nofficial implementations are not suitable for inference in CPUs.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 23:26:25 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 12:15:09 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Meneghetti", "Douglas De Rizzo", ""], ["Homem", "Thiago Pedro Donadon", ""], ["de Oliveira", "Jonas Henrique Renolfi", ""], ["da Silva", "Isaac Jesus", ""], ["Perico", "Danilo Hernani", ""], ["Bianchi", "Reinaldo Augusto da Costa", ""]]}, {"id": "2009.13696", "submitter": "Yuteng Zhu", "authors": "Yuteng Zhu and Graham D. Finlayson", "title": "Mathematical derivation for Vora-Value based filter design method:\n  Gradient and Hessian", "comments": "6 pages, correct typos and improve the mathematical proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the detailed mathematical derivation of the\ngradient and Hessian matrix for the Vora-Value based colorimetric filter\noptimization. We make a full recapitulation of the steps involved in\ndifferentiating the objective function and reveal the positive-definite Hessian\nmatrix when a positive regularizer is applied. This paper serves as a\nsupplementary material for our paper in the colorimetric filter design theory.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 00:14:56 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 16:49:05 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Zhu", "Yuteng", ""], ["Finlayson", "Graham D.", ""]]}, {"id": "2009.13698", "submitter": "Jerry Wei", "authors": "Jerry Wei, Arief Suriawinata, Bing Ren, Xiaoying Liu, Mikhail\n  Lisovsky, Louis Vaickus, Charles Brown, Michael Baker, Mustafa Nasir-Moin,\n  Naofumi Tomita, Lorenzo Torresani, Jason Wei and Saeed Hassanpour", "title": "Learn like a Pathologist: Curriculum Learning by Annotator Agreement for\n  Histopathology Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying curriculum learning requires both a range of difficulty in data and\na method for determining the difficulty of examples. In many tasks, however,\nsatisfying these requirements can be a formidable challenge. In this paper, we\ncontend that histopathology image classification is a compelling use case for\ncurriculum learning. Based on the nature of histopathology images, a range of\ndifficulty inherently exists among examples, and, since medical datasets are\noften labeled by multiple annotators, annotator agreement can be used as a\nnatural proxy for the difficulty of a given example. Hence, we propose a simple\ncurriculum learning method that trains on progressively-harder images as\ndetermined by annotator agreement. We evaluate our hypothesis on the\nchallenging and clinically-important task of colorectal polyp classification.\nWhereas vanilla training achieves an AUC of 83.7% for this task, a model\ntrained with our proposed curriculum learning approach achieves an AUC of\n88.2%, an improvement of 4.5%. Our work aims to inspire researchers to think\nmore creatively and rigorously when choosing contexts for applying curriculum\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 00:25:21 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Wei", "Jerry", ""], ["Suriawinata", "Arief", ""], ["Ren", "Bing", ""], ["Liu", "Xiaoying", ""], ["Lisovsky", "Mikhail", ""], ["Vaickus", "Louis", ""], ["Brown", "Charles", ""], ["Baker", "Michael", ""], ["Nasir-Moin", "Mustafa", ""], ["Tomita", "Naofumi", ""], ["Torresani", "Lorenzo", ""], ["Wei", "Jason", ""], ["Hassanpour", "Saeed", ""]]}, {"id": "2009.13704", "submitter": "Franco Matzkin", "authors": "Franco Matzkin, Virginia Newcombe, Ben Glocker, Enzo Ferrante", "title": "Cranial Implant Design via Virtual Craniectomy with Shape Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Cranial implant design is a challenging task, whose accuracy is crucial in\nthe context of cranioplasty procedures. This task is usually performed manually\nby experts using computer-assisted design software. In this work, we propose\nand evaluate alternative automatic deep learning models for cranial implant\nreconstruction from CT images. The models are trained and evaluated using the\ndatabase released by the AutoImplant challenge, and compared to a baseline\nimplemented by the organizers. We employ a simulated virtual craniectomy to\ntrain our models using complete skulls, and compare two different approaches\ntrained with this procedure. The first one is a direct estimation method based\non the UNet architecture. The second method incorporates shape priors to\nincrease the robustness when dealing with out-of-distribution implant shapes.\nOur direct estimation method outperforms the baselines provided by the\norganizers, while the model with shape priors shows superior performance when\ndealing with out-of-distribution cases. Overall, our methods show promising\nresults in the difficult task of cranial implant design.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 00:35:44 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Matzkin", "Franco", ""], ["Newcombe", "Virginia", ""], ["Glocker", "Ben", ""], ["Ferrante", "Enzo", ""]]}, {"id": "2009.13714", "submitter": "Pu Zhao", "authors": "Pu Zhao, Sijia Liu, Parikshit Ram, Songtao Lu, Yuguang Yao, Djallel\n  Bouneffouf, Xue Lin", "title": "Learned Fine-Tuner for Incongruous Few-Shot Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Model-agnostic meta-learning (MAML) effectively meta-learns an initialization\nof model parameters for few-shot learning where all learning problems share the\nsame format of model parameters -- congruous meta-learning. However, there are\nfew-shot learning scenarios, such as adversarial attack design, where different\nyet related few-shot learning problems may not share any optimizee variables,\nnecessitating incongruous meta-learning. We extend MAML to this setting -- a\nLearned Fine Tuner (LFT) is used to replace hand-designed optimizers (such as\nSGD) for the task-specific fine-tuning. Here, MAML instead meta-learns the\nparameters of this LFT across incongruous tasks leveraging the\nlearning-to-optimize (L2O) framework such that models fine-tuned with LFT (even\nfrom random initializations) adapt quickly to new tasks. As novel\ncontributions, we show that the use of LFT within MAML (i) offers the\ncapability to tackle few-shot learning tasks by meta-learning across\nincongruous yet related problems and (ii) can efficiently work with first-order\nand derivative-free few-shot learning problems. Theoretically, we quantify the\ndifference between LFT (for MAML) and L2O. Empirically, we demonstrate the\neffectiveness of LFT through a novel application of generating universal\nadversarial attacks across different image sources and sizes in the few-shot\nlearning regime.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 01:23:20 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 00:47:28 GMT"}, {"version": "v3", "created": "Mon, 10 May 2021 17:42:57 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhao", "Pu", ""], ["Liu", "Sijia", ""], ["Ram", "Parikshit", ""], ["Lu", "Songtao", ""], ["Yao", "Yuguang", ""], ["Bouneffouf", "Djallel", ""], ["Lin", "Xue", ""]]}, {"id": "2009.13716", "submitter": "Qing Tian", "authors": "Qing Tian, Tal Arbel, James J. Clark", "title": "Grow-Push-Prune: aligning deep discriminants for effective structural\n  network compression", "comments": "title changed, unimportant figures/tables moved to the appendix,\n  typos fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of today's popular deep architectures are hand-engineered to be\ngeneralists. However, this design procedure usually leads to massive redundant,\nuseless, or even harmful features for specific tasks. Unnecessarily high\ncomplexities render deep nets impractical for many real-world applications,\nespecially those without powerful GPU support. In this paper, we attempt to\nderive task-dependent compact models from a deep discriminant analysis\nperspective. We propose an iterative and proactive approach for classification\ntasks which alternates between (1) a pushing step, with an objective to\nsimultaneously maximize class separation, penalize co-variances, and push deep\ndiscriminants into alignment with a compact set of neurons, and (2) a pruning\nstep, which discards less useful or even interfering neurons. Deconvolution is\nadopted to reverse 'unimportant' filters' effects and recover useful\ncontributing sources. A simple network growing strategy based on the basic\nInception module is proposed for challenging tasks requiring larger capacity\nthan what the base net can offer. Experiments on the MNIST, CIFAR10, and\nImageNet datasets demonstrate our approach's efficacy. On ImageNet, by pushing\nand pruning our grown Inception-88 model, we achieve more accurate models than\nInception nets generated during growing, residual nets, and popular compact\nnets at similar sizes. We also show that our grown Inception nets (without\nhard-coded dimension alignment) clearly outperform residual nets of similar\ncomplexities.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 01:29:23 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 16:44:41 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Tian", "Qing", ""], ["Arbel", "Tal", ""], ["Clark", "James J.", ""]]}, {"id": "2009.13721", "submitter": "Yixin Li", "authors": "Yixin Li, Chen Li, Xiaoyan Li, Kai Wang, Md Mamunur Rahaman, Changhao\n  Sun, Hao Chen, Xinran Wu, Hong Zhang, Qian Wang", "title": "A Comprehensive Review for MRF and CRF Approaches in Pathology Image\n  Analysis", "comments": "Arch Computat Methods Eng (2021)", "journal-ref": null, "doi": "10.1007/s11831-021-09591-w", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pathology image analysis is an essential procedure for clinical diagnosis of\nmany diseases. To boost the accuracy and objectivity of detection, nowadays, an\nincreasing number of computer-aided diagnosis (CAD) system is proposed. Among\nthese methods, random field models play an indispensable role in improving the\nanalysis performance. In this review, we present a comprehensive overview of\npathology image analysis based on the markov random fields (MRFs) and\nconditional random fields (CRFs), which are two popular random field models.\nFirstly, we introduce the background of two random fields and pathology images.\nSecondly, we summarize the basic mathematical knowledge of MRFs and CRFs from\nmodelling to optimization. Then, a thorough review of the recent research on\nthe MRFs and CRFs of pathology images analysis is presented. Finally, we\ninvestigate the popular methodologies in the related works and discuss the\nmethod migration among CAD field.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 01:46:22 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 13:35:46 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 12:40:21 GMT"}, {"version": "v4", "created": "Sun, 9 May 2021 02:42:16 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Li", "Yixin", ""], ["Li", "Chen", ""], ["Li", "Xiaoyan", ""], ["Wang", "Kai", ""], ["Rahaman", "Md Mamunur", ""], ["Sun", "Changhao", ""], ["Chen", "Hao", ""], ["Wu", "Xinran", ""], ["Zhang", "Hong", ""], ["Wang", "Qian", ""]]}, {"id": "2009.13723", "submitter": "Zhao Zhiyuan", "authors": "Zhiyuan Zhao, Tao Han, Junyu Gao, Qi Wang, Xuelong Li", "title": "A Flow Base Bi-path Network for Cross-scene Video Crowd Understanding in\n  Aerial View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drones shooting can be applied in dynamic traffic monitoring, object\ndetecting and tracking, and other vision tasks. The variability of the shooting\nlocation adds some intractable challenges to these missions, such as varying\nscale, unstable exposure, and scene migration. In this paper, we strive to\ntackle the above challenges and automatically understand the crowd from the\nvisual data collected from drones. First, to alleviate the background noise\ngenerated in cross-scene testing, a double-stream crowd counting model is\nproposed, which extracts optical flow and frame difference information as an\nadditional branch. Besides, to improve the model's generalization ability at\ndifferent scales and time, we randomly combine a variety of data transformation\nmethods to simulate some unseen environments. To tackle the crowd density\nestimation problem under extreme dark environments, we introduce synthetic data\ngenerated by game Grand Theft Auto V(GTAV). Experiment results show the\neffectiveness of the virtual data. Our method wins the challenge with a mean\nabsolute error (MAE) of 12.70. Moreover, a comprehensive ablation study is\nconducted to explore each component's contribution.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 01:48:24 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Zhao", "Zhiyuan", ""], ["Han", "Tao", ""], ["Gao", "Junyu", ""], ["Wang", "Qi", ""], ["Li", "Xuelong", ""]]}, {"id": "2009.13727", "submitter": "Fredrik Westling", "authors": "Fredrik Westling, Dr James Underwood, Dr Mitch Bryson", "title": "Graph-based methods for analyzing orchard tree structure using noisy\n  point cloud data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitisation of fruit trees using LiDAR enables analysis which can be used to\nbetter growing practices to improve yield. Sophisticated analysis requires\ngeometric and semantic understanding of the data, including the ability to\ndiscern individual trees as well as identifying leafy and structural matter.\nExtraction of this information should be rapid, as should data capture, so that\nentire orchards can be processed, but existing methods for classification and\nsegmentation rely on high-quality data or additional data sources like cameras.\nWe present a method for analysis of LiDAR data specifically for individual tree\nlocation, segmentation and matter classification, which can operate on\nlow-quality data captured by handheld or mobile LiDAR. Our methods for tree\nlocation and segmentation improved on existing methods with an F1 score of\n0.774 and a v-measure of 0.915 respectively, while trunk matter classification\nperformed poorly in absolute terms with an average F1 score of 0.490 on real\ndata, though consistently outperformed existing methods and displayed a\nsignificantly shorter runtime.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 02:07:30 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 01:18:06 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Westling", "Fredrik", ""], ["Underwood", "Dr James", ""], ["Bryson", "Dr Mitch", ""]]}, {"id": "2009.13735", "submitter": "Yangbin Chen", "authors": "Yangbin Chen, Yun Ma, Tom Ko, Jianping Wang, Qing Li", "title": "MetaMix: Improved Meta-Learning with Interpolation-based Consistency\n  Regularization", "comments": "8 pages, 3 figures, 3 tables. Accepted by 25th International\n  Conference on Pattern Recognition (ICPR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-Agnostic Meta-Learning (MAML) and its variants are popular few-shot\nclassification methods. They train an initializer across a variety of sampled\nlearning tasks (also known as episodes) such that the initialized model can\nadapt quickly to new tasks. However, current MAML-based algorithms have\nlimitations in forming generalizable decision boundaries. In this paper, we\npropose an approach called MetaMix. It generates virtual feature-target pairs\nwithin each episode to regularize the backbone models. MetaMix can be\nintegrated with any of the MAML-based algorithms and learn the decision\nboundaries generalizing better to new tasks. Experiments on the mini-ImageNet,\nCUB, and FC100 datasets show that MetaMix improves the performance of\nMAML-based algorithms and achieves state-of-the-art result when integrated with\nMeta-Transfer Learning.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 02:44:13 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 05:36:55 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chen", "Yangbin", ""], ["Ma", "Yun", ""], ["Ko", "Tom", ""], ["Wang", "Jianping", ""], ["Li", "Qing", ""]]}, {"id": "2009.13743", "submitter": "Leonardo Ramos Thomas", "authors": "Leonardo Ramos, Bernardo Morales", "title": "SwiftFace: Real-Time Face Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision is a field of artificial intelligence that trains computers\nto interpret the visual world in a way similar to that of humans. Due to the\nrapid advancements in technology and the increasing availability of\nsufficiently large training datasets, the topics within computer vision have\nexperienced a steep growth in the last decade. Among them, one of the most\npromising fields is face detection. Being used daily in a wide variety of\nfields; from mobile apps and augmented reality for entertainment purposes, to\nsocial studies and security cameras; designing high-performance models for face\ndetection is crucial. On top of that, with the aforementioned growth in face\ndetection technologies, precision and accuracy are no longer the only relevant\nfactors: for real-time face detection, speed of detection is essential.\nSwiftFace is a novel deep learning model created solely to be a fast face\ndetection model. By focusing only on detecting faces, SwiftFace performs 30%\nfaster than current state-of-the-art face detection models. Code available at\nhttps://github.com/leo7r/swiftface\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 03:09:29 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Ramos", "Leonardo", ""], ["Morales", "Bernardo", ""]]}, {"id": "2009.13755", "submitter": "Hang Zhang", "authors": "Hang Zhang, Jinwei Zhang, Rongguang Wang, Qihao Zhang, Susan A.\n  Gauthier, Pascal Spincemaille, Thanh D. Nguyen, Yi Wang", "title": "Geometric Loss for Deep Multiple Sclerosis lesion Segmentation", "comments": "5 pages, three figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple sclerosis (MS) lesions occupy a small fraction of the brain volume,\nand are heterogeneous with regards to shape, size and locations, which poses a\ngreat challenge for training deep learning based segmentation models. We\nproposed a new geometric loss formula to address the data imbalance and exploit\nthe geometric property of MS lesions. We showed that traditional region-based\nand boundary-aware loss functions can be associated with the formula. We\nfurther develop and instantiate two loss functions containing first- and\nsecond-order geometric information of lesion regions to enforce regularization\non optimizing deep segmentation models. Experimental results on two MS lesion\ndatasets with different scales, acquisition protocols and resolutions\ndemonstrated the superiority of our proposed methods compared to other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 03:49:28 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Zhang", "Hang", ""], ["Zhang", "Jinwei", ""], ["Wang", "Rongguang", ""], ["Zhang", "Qihao", ""], ["Gauthier", "Susan A.", ""], ["Spincemaille", "Pascal", ""], ["Nguyen", "Thanh D.", ""], ["Wang", "Yi", ""]]}, {"id": "2009.13782", "submitter": "Ganesh Samarth Chamarahalli Arunkumar", "authors": "Ganesh Samarth, Sheetal Ojha, Nikhil Pareek", "title": "Knowledge Fusion Transformers for Video Action Recognition", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Knowledge Fusion Transformers for video action classification.\nWe present a self-attention based feature enhancer to fuse action knowledge in\n3D inception based spatio-temporal context of the video clip intended to be\nclassified. We show, how using only one stream networks and with little or, no\npretraining can pave the way for a performance close to the current\nstate-of-the-art. Additionally, we present how different self-attention\narchitectures used at different levels of the network can be blended-in to\nenhance feature representation. Our architecture is trained and evaluated on\nUCF-101 and Charades dataset, where it is competitive with the state of the\nart. It also exceeds by a large gap from single stream networks with no to less\npretraining.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 05:13:45 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 03:53:44 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Samarth", "Ganesh", ""], ["Ojha", "Sheetal", ""], ["Pareek", "Nikhil", ""]]}, {"id": "2009.13792", "submitter": "K K Thyagharajan", "authors": "S. D. Lalitha, K. K. Thyagharajan", "title": "Micro-Facial Expression Recognition in Video Based on Optimal\n  Convolutional Neural Network (MFEOCNN) Algorithm", "comments": "19 pages, 10 figures, \"for published version see\n  https://www.ijeat.org/wp-content/uploads/papers/v9i1/A9802109119.pdf\"", "journal-ref": null, "doi": "10.35940/ijeat.A9802.109119", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression is a standout amongst the most imperative features of human\nemotion recognition. For demonstrating the emotional states facial expressions\nare utilized by the people. In any case, recognition of facial expressions has\npersisted a testing and intriguing issue with regards to PC vision. Recognizing\nthe Micro-Facial expression in video sequence is the main objective of the\nproposed approach. For efficient recognition, the proposed method utilizes the\noptimal convolution neural network. Here the proposed method considering the\ninput dataset is the CK+ dataset. At first, by means of Adaptive median\nfiltering preprocessing is performed in the input image. From the preprocessed\noutput, the extracted features are Geometric features, Histogram of Oriented\nGradients features and Local binary pattern features. The novelty of the\nproposed method is, with the help of Modified Lion Optimization (MLO)\nalgorithm, the optimal features are selected from the extracted features. In a\nshorter computational time, it has the benefits of rapidly focalizing and\neffectively acknowledging with the aim of getting an overall arrangement or\nidea. Finally, the recognition is done by Convolution Neural network (CNN).\nThen the performance of the proposed MFEOCNN method is analysed in terms of\nfalse measures and recognition accuracy. This kind of emotion recognition is\nmainly used in medicine, marketing, E-learning, entertainment, law and\nmonitoring. From the simulation, we know that the proposed approach achieves\nmaximum recognition accuracy of 99.2% with minimum Mean Absolute Error (MAE)\nvalue. These results are compared with the existing for MicroFacial Expression\nBased Deep-Rooted Learning (MFEDRL), Convolutional Neural Network with Lion\nOptimization (CNN+LO) and Convolutional Neural Network (CNN) without\noptimization. The simulation of the proposed method is done in the working\nplatform of MATLAB.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 05:56:26 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Lalitha", "S. D.", ""], ["Thyagharajan", "K. K.", ""]]}, {"id": "2009.13793", "submitter": "Fabrizio Patuzzo", "authors": "Fabrizio Patuzzo", "title": "A comparison of classical and variational autoencoders for anomaly\n  detection", "comments": null, "journal-ref": null, "doi": null, "report-no": "IDSIA-2020-10.1", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes and compares a classical and a variational autoencoder in\nthe context of anomaly detection. To better understand their architecture and\nfunctioning, describe their properties and compare their performance, it\nexplores how they address a simple problem: reconstructing a line with a slope.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 05:58:31 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Patuzzo", "Fabrizio", ""]]}, {"id": "2009.13799", "submitter": "Dongchao Wen", "authors": "Junjie Liu, Dongchao Wen, Deyu Wang, Wei Tao, Tse-Wei Chen, Kinya Osa,\n  Masami Kato", "title": "BAMSProd: A Step towards Generalizing the Adaptive Optimization Methods\n  to Deep Binary Model", "comments": "10 pages, 4 figures, 2 tables", "journal-ref": "2020 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent methods have significantly reduced the performance degradation of\nBinary Neural Networks (BNNs), but guaranteeing the effective and efficient\ntraining of BNNs is an unsolved problem. The main reason is that the estimated\ngradients produced by the Straight-Through-Estimator (STE) mismatches with the\ngradients of the real derivatives. In this paper, we provide an explicit convex\noptimization example where training the BNNs with the traditionally adaptive\noptimization methods still faces the risk of non-convergence, and identify that\nconstraining the range of gradients is critical for optimizing the deep binary\nmodel to avoid highly suboptimal solutions. For solving above issues, we\npropose a BAMSProd algorithm with a key observation that the convergence\nproperty of optimizing deep binary model is strongly related to the\nquantization errors. In brief, it employs an adaptive range constraint via an\nerrors measurement for smoothing the gradients transition while follows the\nexponential moving strategy from AMSGrad to avoid errors accumulation during\nthe optimization. The experiments verify the corollary of theoretical\nconvergence analysis, and further demonstrate that our optimization method can\nspeed up the convergence about 1:2x and boost the performance of BNNs to a\nsignificant level than the specific binary optimizer about 3:7%, even in a\nhighly non-convex optimization problem.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 06:12:32 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Liu", "Junjie", ""], ["Wen", "Dongchao", ""], ["Wang", "Deyu", ""], ["Tao", "Wei", ""], ["Chen", "Tse-Wei", ""], ["Osa", "Kinya", ""], ["Kato", "Masami", ""]]}, {"id": "2009.13803", "submitter": "Qingbei Guo", "authors": "Qingbei Guo and Xiao-Jun Wu and Josef Kittler and Zhiquan Feng", "title": "Self-grouping Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although group convolution operators are increasingly used in deep\nconvolutional neural networks to improve the computational efficiency and to\nreduce the number of parameters, most existing methods construct their group\nconvolution architectures by a predefined partitioning of the filters of each\nconvolutional layer into multiple regular filter groups with an equal spatial\ngroup size and data-independence, which prevents a full exploitation of their\npotential. To tackle this issue, we propose a novel method of designing\nself-grouping convolutional neural networks, called SG-CNN, in which the\nfilters of each convolutional layer group themselves based on the similarity of\ntheir importance vectors. Concretely, for each filter, we first evaluate the\nimportance value of their input channels to identify the importance vectors,\nand then group these vectors by clustering. Using the resulting\n\\emph{data-dependent} centroids, we prune the less important connections, which\nimplicitly minimizes the accuracy loss of the pruning, thus yielding a set of\n\\emph{diverse} group convolution filters. Subsequently, we develop two\nfine-tuning schemes, i.e. (1) both local and global fine-tuning and (2) global\nonly fine-tuning, which experimentally deliver comparable results, to recover\nthe recognition capacity of the pruned network. Comprehensive experiments\ncarried out on the CIFAR-10/100 and ImageNet datasets demonstrate that our\nself-grouping convolution method adapts to various state-of-the-art CNN\narchitectures, such as ResNet and DenseNet, and delivers superior performance\nin terms of compression ratio, speedup and recognition accuracy. We demonstrate\nthe ability of SG-CNN to generalise by transfer learning, including domain\nadaption and object detection, showing competitive results. Our source code is\navailable at https://github.com/QingbeiGuo/SG-CNN.git.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 06:24:32 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Guo", "Qingbei", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""], ["Feng", "Zhiquan", ""]]}, {"id": "2009.13824", "submitter": "Laura Doerr", "authors": "Laura D\\\"orr, Felix Brandt, Martin Pouls, Alexander Naumann", "title": "An Image Processing Pipeline for Automated Packaging Structure\n  Recognition", "comments": "To be published in: \"Forum Bildverarbeitung 2020\", KIT Scientific\n  Publishing, Karlsruhe", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dispatching and receiving logistics goods, as well as transportation itself,\ninvolve a high amount of manual efforts. The transported goods, including their\npackaging and labeling, need to be double-checked, verified or recognized at\nmany supply chain network points. These processes hold automation potentials,\nwhich we aim to exploit using computer vision techniques. More precisely, we\npropose a cognitive system for the fully automated recognition of packaging\nstructures for standardized logistics shipments based on single RGB images. Our\ncontribution contains descriptions of a suitable system design and its\nevaluation on relevant real-world data. Further, we discuss our algorithmic\nchoices.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 07:26:08 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["D\u00f6rr", "Laura", ""], ["Brandt", "Felix", ""], ["Pouls", "Martin", ""], ["Naumann", "Alexander", ""]]}, {"id": "2009.13829", "submitter": "Ting-Yun Chang", "authors": "Ting-Yun Chang and Chi-Jen Lu", "title": "TinyGAN: Distilling BigGAN for Conditional Image Generation", "comments": "accepted by ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GANs) have become a powerful approach for\ngenerative image modeling. However, GANs are notorious for their training\ninstability, especially on large-scale, complex datasets. While the recent work\nof BigGAN has significantly improved the quality of image generation on\nImageNet, it requires a huge model, making it hard to deploy on\nresource-constrained devices. To reduce the model size, we propose a black-box\nknowledge distillation framework for compressing GANs, which highlights a\nstable and efficient training process. Given BigGAN as the teacher network, we\nmanage to train a much smaller student network to mimic its functionality,\nachieving competitive performance on Inception and FID scores with the\ngenerator having $16\\times$ fewer parameters.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 07:33:49 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Chang", "Ting-Yun", ""], ["Lu", "Chi-Jen", ""]]}, {"id": "2009.13836", "submitter": "Abon Chaudhuri", "authors": "Theban Stanley, Nihar Vanjara, Yanxin Pan, Ekaterina Pirogova, Swagata\n  Chakraborty, Abon Chaudhuri", "title": "SIR: Similar Image Retrieval for Product Search in E-Commerce", "comments": "Accepted in 13th International Conference on Similarity Search and\n  Applications, SISAP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a similar image retrieval (SIR) platform that is used to quickly\ndiscover visually similar products in a catalog of millions. Given the size,\ndiversity, and dynamism of our catalog, product search poses many challenges.\nIt can be addressed by building supervised models to tagging product images\nwith labels representing themes and later retrieving them by labels. This\napproach suffices for common and perennial themes like \"white shirt\" or\n\"lifestyle image of TV\". It does not work for new themes such as\n\"e-cigarettes\", hard-to-define ones such as \"image with a promotional badge\",\nor the ones with short relevance span such as \"Halloween costumes\". SIR is\nideal for such cases because it allows us to search by an example, not a\npre-defined theme. We describe the steps - embedding computation, encoding, and\nindexing - that power the approximate nearest neighbor search back-end. We also\nhighlight two applications of SIR. The first one is related to the detection of\nproducts with various types of potentially objectionable themes. This\napplication is run with a sense of urgency, hence the typical time frame to\ntrain and bootstrap a model is not permitted. Also, these themes are often\nshort-lived based on current trends, hence spending resources to build a\nlasting model is not justified. The second application is a variant item\ndetection system where SIR helps discover visual variants that are hard to find\nthrough text search. We analyze the performance of SIR in the context of these\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 07:53:03 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Stanley", "Theban", ""], ["Vanjara", "Nihar", ""], ["Pan", "Yanxin", ""], ["Pirogova", "Ekaterina", ""], ["Chakraborty", "Swagata", ""], ["Chaudhuri", "Abon", ""]]}, {"id": "2009.13839", "submitter": "Saurabh Gupta", "authors": "Saurabh Gupta, Arun Balaji Buduru, Ponnurangam Kumaraguru", "title": "imdpGAN: Generating Private and Specific Data with Generative\n  Adversarial Networks", "comments": "9 pages, 7 figures, Accepted at IEEE TPS'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Network (GAN) and its variants have shown promising\nresults in generating synthetic data. However, the issues with GANs are: (i)\nthe learning happens around the training samples and the model often ends up\nremembering them, consequently, compromising the privacy of individual samples\n- this becomes a major concern when GANs are applied to training data including\npersonally identifiable information, (ii) the randomness in generated data -\nthere is no control over the specificity of generated samples. To address these\nissues, we propose imdpGAN - an information maximizing differentially private\nGenerative Adversarial Network. It is an end-to-end framework that\nsimultaneously achieves privacy protection and learns latent representations.\nWith experiments on MNIST dataset, we show that imdpGAN preserves the privacy\nof the individual data point, and learns latent codes to control the\nspecificity of the generated samples. We perform binary classification on digit\npairs to show the utility versus privacy trade-off. The classification accuracy\ndecreases as we increase privacy levels in the framework. We also\nexperimentally show that the training process of imdpGAN is stable but\nexperience a 10-fold time increase as compared with other GAN frameworks.\nFinally, we extend imdpGAN framework to CelebA dataset to show how the privacy\nand learned representations can be used to control the specificity of the\noutput.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 08:03:32 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Gupta", "Saurabh", ""], ["Buduru", "Arun Balaji", ""], ["Kumaraguru", "Ponnurangam", ""]]}, {"id": "2009.13851", "submitter": "M Usman Maqbool Bhutta", "authors": "M Usman Maqbool Bhutta, Manohar Kuse, Rui Fan, Yanan Liu, Ming Liu", "title": "Loop-box: Multi-Agent Direct SLAM Triggered by Single Loop Closure for\n  Large-Scale Mapping", "comments": "Material related to this work is available at\n  https://usmanmaqbool.github.io/loop-box", "journal-ref": "IEEE Transactions on Cybernetics, 2020", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a multi-agent framework for real-time large-scale\n3D reconstruction applications. In SLAM, researchers usually build and update a\n3D map after applying non-linear pose graph optimization techniques. Moreover,\nmany multi-agent systems are prevalently using odometry information from\nadditional sensors. These methods generally involve intensive computer vision\nalgorithms and are tightly coupled with various sensors. We develop a generic\nmethod for the keychallenging scenarios in multi-agent 3D mapping based on\ndifferent camera systems. The proposed framework performs actively in terms of\nlocalizing each agent after the first loop closure between them. It is shown\nthat the proposed system only uses monocular cameras to yield real-time\nmulti-agent large-scale localization and 3D global mapping. Based on the\ninitial matching, our system can calculate the optimal scale difference between\nmultiple 3D maps and then estimate an accurate relative pose transformation for\nlarge-scale global mapping.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 08:26:43 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Bhutta", "M Usman Maqbool", ""], ["Kuse", "Manohar", ""], ["Fan", "Rui", ""], ["Liu", "Yanan", ""], ["Liu", "Ming", ""]]}, {"id": "2009.13852", "submitter": "Seamus Anderson", "authors": "Seamus Anderson, Martin Towner, Phil Bland, Christopher Haikings,\n  William Volante, Eleanor Sansom, Hadrien Devillepoix, Patrick Shober,\n  Benjamin Hartig, Martin Cupak, Trent Jansen-Sturgeon, Robert Howie, Gretchen\n  Benedix, Geoff Deacon", "title": "Machine Learning for Semi-Automated Meteorite Recovery", "comments": "15 pages, 3 figures, 2 tables", "journal-ref": null, "doi": "10.1111/maps.13593", "report-no": null, "categories": "astro-ph.EP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel methodology for recovering meteorite falls observed and\nconstrained by fireball networks, using drones and machine learning algorithms.\nThis approach uses images of the local terrain for a given fall site to train\nan artificial neural network, designed to detect meteorite candidates. We have\nfield tested our methodology to show a meteorite detection rate between 75-97%,\nwhile also providing an efficient mechanism to eliminate false-positives. Our\ntests at a number of locations within Western Australia also showcase the\nability for this training scheme to generalize a model to learn localized\nterrain features. Our model-training approach was also able to correctly\nidentify 3 meteorites in their native fall sites, that were found using\ntraditional searching techniques. Our methodology will be used to recover\nmeteorite falls in a wide range of locations within globe-spanning fireball\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 08:27:41 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Anderson", "Seamus", ""], ["Towner", "Martin", ""], ["Bland", "Phil", ""], ["Haikings", "Christopher", ""], ["Volante", "William", ""], ["Sansom", "Eleanor", ""], ["Devillepoix", "Hadrien", ""], ["Shober", "Patrick", ""], ["Hartig", "Benjamin", ""], ["Cupak", "Martin", ""], ["Jansen-Sturgeon", "Trent", ""], ["Howie", "Robert", ""], ["Benedix", "Gretchen", ""], ["Deacon", "Geoff", ""]]}, {"id": "2009.13856", "submitter": "Maayan Shuvi", "authors": "Maayan Shuvi, Noa Fish, Kfir Aberman, Ariel Shamir, Daniel Cohen-Or", "title": "Neural Alignment for Face De-pixelization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple method to reconstruct a high-resolution video from a\nface-video, where the identity of a person is obscured by pixelization. This\nconcealment method is popular because the viewer can still perceive a human\nface figure and the overall head motion. However, we show in our experiments\nthat a fairly good approximation of the original video can be reconstructed in\na way that compromises anonymity. Our system exploits the simultaneous\nsimilarity and small disparity between close-by video frames depicting a human\nface, and employs a spatial transformation component that learns the alignment\nbetween the pixelated frames. Each frame, supported by its aligned surrounding\nframes, is first encoded, then decoded to a higher resolution. Reconstruction\nand perceptual losses promote adherence to the ground-truth, and an adversarial\nloss assists in maintaining domain faithfulness. There is no need for explicit\ntemporal coherency loss as it is maintained implicitly by the alignment of\nneighboring frames and reconstruction. Although simple, our framework\nsynthesizes high-quality face reconstructions, demonstrating that given the\nstatistical prior of a human face, multiple aligned pixelated frames contain\nsufficient information to reconstruct a high-quality approximation of the\noriginal signal.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 08:29:15 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Shuvi", "Maayan", ""], ["Fish", "Noa", ""], ["Aberman", "Kfir", ""], ["Shamir", "Ariel", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2009.13862", "submitter": "Jiuniu Wang", "authors": "Wenjia Xu, Jiuniu Wang, Yang Wang, Guangluan Xu, Wei Dai, Yirong Wu", "title": "Where is the Model Looking At?--Concentrate and Explain the Network\n  Attention", "comments": null, "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, vol. 14, no.\n  3, pp. 506-516, March 2020", "doi": "10.1109/JSTSP.2020.2987729", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification models have achieved satisfactory performance on many\ndatasets, sometimes even better than human. However, The model attention is\nunclear since the lack of interpretability. This paper investigates the\nfidelity and interpretability of model attention. We propose an Explainable\nAttribute-based Multi-task (EAT) framework to concentrate the model attention\non the discriminative image area and make the attention interpretable. We\nintroduce attributes prediction to the multi-task learning network, helping the\nnetwork to concentrate attention on the foreground objects. We generate\nattribute-based textual explanations for the network and ground the attributes\non the image to show visual explanations. The multi-model explanation can not\nonly improve user trust but also help to find the weakness of network and\ndataset. Our framework can be generalized to any basic model. We perform\nexperiments on three datasets and five basic models. Results indicate that the\nEAT framework can give multi-modal explanations that interpret the network\ndecision. The performance of several recognition approaches is improved by\nguiding network attention.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 08:36:18 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Xu", "Wenjia", ""], ["Wang", "Jiuniu", ""], ["Wang", "Yang", ""], ["Xu", "Guangluan", ""], ["Dai", "Wei", ""], ["Wu", "Yirong", ""]]}, {"id": "2009.13885", "submitter": "Sachihiro Youoku", "authors": "Sachihiro Youoku, Yuushi Toyoda, Takahisa Yamamoto, Junya Saito,\n  Ryosuke Kawamura, Xiaoyu Mi and Kentaro Murase", "title": "A Multi-term and Multi-task Analyzing Framework for Affective Analysis\n  in-the-wild", "comments": "5 pages with 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human affective recognition is an important factor in human-computer\ninteraction. However, the method development with in-the-wild data is not yet\naccurate enough for practical usage. In this paper, we introduce the affective\nrecognition method focusing on valence-arousal (VA) and expression (EXP) that\nwas submitted to the Affective Behavior Analysis in-the-wild (ABAW) 2020\nContest. Since we considered that affective behaviors have many observable\nfeatures that have their own time frames, we introduced multiple optimized time\nwindows (short-term, middle-term, and long-term) into our analyzing framework\nfor extracting feature parameters from video data. Moreover, multiple modality\ndata are used, including action units, head poses, gaze, posture, and ResNet 50\nor Efficient NET features, and are optimized during the extraction of these\nfeatures. Then, we generated affective recognition models for each time window\nand ensembled these models together. Also, we fussed the valence, arousal, and\nexpression models together to enable the multi-task learning, considering the\nfact that the basic psychological states behind facial expressions are closely\nrelated to each another. In the validation set, our model achieved a\nvalence-arousal score of 0.498 and a facial expression score of 0.471. These\nverification results reveal that our proposed framework can improve estimation\naccuracy and robustness effectively.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 09:24:29 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 13:04:04 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Youoku", "Sachihiro", ""], ["Toyoda", "Yuushi", ""], ["Yamamoto", "Takahisa", ""], ["Saito", "Junya", ""], ["Kawamura", "Ryosuke", ""], ["Mi", "Xiaoyu", ""], ["Murase", "Kentaro", ""]]}, {"id": "2009.13898", "submitter": "Xin Tian", "authors": "Xin Tian, Ke Xu, Xin Yang, Baocai Yin, Rynson W.H. Lau", "title": "Weakly-supervised Salient Instance Detection", "comments": "BMVC 2020, best student paper runner-up", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing salient instance detection (SID) methods typically learn from\npixel-level annotated datasets. In this paper, we present the first\nweakly-supervised approach to the SID problem. Although weak supervision has\nbeen considered in general saliency detection, it is mainly based on using\nclass labels for object localization. However, it is non-trivial to use only\nclass labels to learn instance-aware saliency information, as salient instances\nwith high semantic affinities may not be easily separated by the labels. We\nnote that subitizing information provides an instant judgement on the number of\nsalient items, which naturally relates to detecting salient instances and may\nhelp separate instances of the same class while grouping different parts of the\nsame instance. Inspired by this insight, we propose to use class and subitizing\nlabels as weak supervision for the SID problem. We propose a novel\nweakly-supervised network with three branches: a Saliency Detection Branch\nleveraging class consistency information to locate candidate objects; a\nBoundary Detection Branch exploiting class discrepancy information to delineate\nobject boundaries; and a Centroid Detection Branch using subitizing information\nto detect salient instance centroids. This complementary information is further\nfused to produce salient instance maps. We conduct extensive experiments to\ndemonstrate that the proposed method plays favorably against carefully designed\nbaseline methods adapted from related tasks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 09:47:23 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Tian", "Xin", ""], ["Xu", "Ke", ""], ["Yang", "Xin", ""], ["Yin", "Baocai", ""], ["Lau", "Rynson W. H.", ""]]}, {"id": "2009.13935", "submitter": "Gencer Sumbul", "authors": "Hichame Yessou, Gencer Sumbul, Beg\\\"um Demir", "title": "A Comparative Study of Deep Learning Loss Functions for Multi-Label\n  Remote Sensing Image Classification", "comments": "Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS) 2020. For code visit:\n  https://gitlab.tubit.tu-berlin.de/rsim/RS-MLC-Losses", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes and compares different deep learning loss functions in\nthe framework of multi-label remote sensing (RS) image scene classification\nproblems. We consider seven loss functions: 1) cross-entropy loss; 2) focal\nloss; 3) weighted cross-entropy loss; 4) Hamming loss; 5) Huber loss; 6)\nranking loss; and 7) sparseMax loss. All the considered loss functions are\nanalyzed for the first time in RS. After a theoretical analysis, an\nexperimental analysis is carried out to compare the considered loss functions\nin terms of their: 1) overall accuracy; 2) class imbalance awareness (for which\nthe number of samples associated to each class significantly varies); 3)\nconvexibility and differentiability; and 4) learning efficiency (i.e.,\nconvergence speed). On the basis of our analysis, some guidelines are derived\nfor a proper selection of a loss function in multi-label RS scene\nclassification problems.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 11:35:26 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Yessou", "Hichame", ""], ["Sumbul", "Gencer", ""], ["Demir", "Beg\u00fcm", ""]]}, {"id": "2009.13939", "submitter": "Diogo Pernes", "authors": "Diogo Pernes and Jaime S. Cardoso", "title": "Tackling unsupervised multi-source domain adaptation with optimism and\n  consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been known for a while that the problem of multi-source domain\nadaptation can be regarded as a single source domain adaptation task where the\nsource domain corresponds to a mixture of the original source domains.\nNonetheless, how to adjust the mixture distribution weights remains an open\nquestion. Moreover, most existing work on this topic focuses only on minimizing\nthe error on the source domains and achieving domain-invariant representations,\nwhich is insufficient to ensure low error on the target domain. In this work,\nwe present a novel framework that addresses both problems and beats the current\nstate of the art by using a mildly optimistic objective function and\nconsistency regularization on the target samples.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 11:55:14 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Pernes", "Diogo", ""], ["Cardoso", "Jaime S.", ""]]}, {"id": "2009.13940", "submitter": "Cristian Cioflan", "authors": "Cristian Cioflan, Radu Timofte", "title": "MS-RANAS: Multi-Scale Resource-Aware Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) has proved effective in offering\noutperforming alternatives to handcrafted neural networks. In this paper we\nanalyse the benefits of NAS for image classification tasks under strict\ncomputational constraints. Our aim is to automate the design of highly\nefficient deep neural networks, capable of offering fast and accurate\npredictions and that could be deployed on a low-memory, low-power\nsystem-on-chip. The task thus becomes a three-party trade-off between accuracy,\ncomputational complexity, and memory requirements. To address this concern, we\npropose Multi-Scale Resource-Aware Neural Architecture Search (MS-RANAS). We\nemploy a one-shot architecture search approach in order to obtain a reduced\nsearch cost and we focus on an anytime prediction setting. Through the usage of\nmultiple-scaled features and early classifiers, we achieved state-of-the-art\nresults in terms of accuracy-speed trade-off.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 11:56:01 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Cioflan", "Cristian", ""], ["Timofte", "Radu", ""]]}, {"id": "2009.13953", "submitter": "Shivaank Agarwal", "authors": "Shivaank Agarwal, Ravindra Gudi, Paresh Saxena", "title": "One-Shot learning based classification for segregation of plastic waste", "comments": "Accepted in The International Conference on Digital Image Computing:\n  Techniques and Applications, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of segregating recyclable waste is fairly daunting for many\ncountries. This article presents an approach for image based classification of\nplastic waste using one-shot learning techniques. The proposed approach\nexploits discriminative features generated via the siamese and triplet loss\nconvolutional neural networks to help differentiate between 5 types of plastic\nwaste based on their resin codes. The approach achieves an accuracy of 99.74%\non the WaDaBa Database\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 12:16:50 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Agarwal", "Shivaank", ""], ["Gudi", "Ravindra", ""], ["Saxena", "Paresh", ""]]}, {"id": "2009.13954", "submitter": "Shixian Wen", "authors": "Shixian Wen, Amanda Rios, Yunhao Ge, Laurent Itti", "title": "Beneficial Perturbation Network for designing general adaptive\n  artificial intelligence systems", "comments": "Accepted at IEEE Transactions on Neural Networks and Learning Systems\n  Keyword: Adaptive artificial intelligence system , Switch modes , Beneficial\n  perturbations , Continual learning , Adversarial examples", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems 2021", "doi": "10.1109/TNNLS.2021.3054423", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human brain is the gold standard of adaptive learning. It not only can\nlearn and benefit from experience, but also can adapt to new situations. In\ncontrast, deep neural networks only learn one sophisticated but fixed mapping\nfrom inputs to outputs. This limits their applicability to more dynamic\nsituations, where input to output mapping may change with different contexts. A\nsalient example is continual learning - learning new independent tasks\nsequentially without forgetting previous tasks. Continual learning of multiple\ntasks in artificial neural networks using gradient descent leads to\ncatastrophic forgetting, whereby a previously learned mapping of an old task is\nerased when learning new mappings for new tasks. Here, we propose a new\nbiologically plausible type of deep neural network with extra, out-of-network,\ntask-dependent biasing units to accommodate these dynamic situations. This\nallows, for the first time, a single network to learn potentially unlimited\nparallel input to output mappings, and to switch on the fly between them at\nruntime. Biasing units are programmed by leveraging beneficial perturbations\n(opposite to well-known adversarial perturbations) for each task. Beneficial\nperturbations for a given task bias the network toward that task, essentially\nswitching the network into a different mode to process that task. This largely\neliminates catastrophic interference between tasks. Our approach is\nmemory-efficient and parameter-efficient, can accommodate many tasks, and\nachieves state-of-the-art performance across different tasks and domains.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 01:28:10 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 02:12:37 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Wen", "Shixian", ""], ["Rios", "Amanda", ""], ["Ge", "Yunhao", ""], ["Itti", "Laurent", ""]]}, {"id": "2009.13957", "submitter": "Jinting Wu", "authors": "Jinting Wu, Yujia Zhang and Xiaoguang Zhao", "title": "A Prototype-Based Generalized Zero-Shot Learning Framework for Hand\n  Gesture Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand gesture recognition plays a significant role in human-computer\ninteraction for understanding various human gestures and their intent. However,\nmost prior works can only recognize gestures of limited labeled classes and\nfail to adapt to new categories. The task of Generalized Zero-Shot Learning\n(GZSL) for hand gesture recognition aims to address the above issue by\nleveraging semantic representations and detecting both seen and unseen class\nsamples. In this paper, we propose an end-to-end prototype-based GZSL framework\nfor hand gesture recognition which consists of two branches. The first branch\nis a prototype-based detector that learns gesture representations and\ndetermines whether an input sample belongs to a seen or unseen category. The\nsecond branch is a zero-shot label predictor which takes the features of unseen\nclasses as input and outputs predictions through a learned mapping mechanism\nbetween the feature and the semantic space. We further establish a hand gesture\ndataset that specifically targets this GZSL task, and comprehensive experiments\non this dataset demonstrate the effectiveness of our proposed approach on\nrecognizing both seen and unseen gestures.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 12:18:35 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Wu", "Jinting", ""], ["Zhang", "Yujia", ""], ["Zhao", "Xiaoguang", ""]]}, {"id": "2009.13986", "submitter": "Ulugbek Kamilov", "authors": "Weijie Gan, Yu Sun, Cihat Eldeniz, Jiaming Liu, Hongyu An, and Ulugbek\n  S. Kamilov", "title": "Deep Image Reconstruction using Unregistered Measurements without\n  Groundtruth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key limitations in conventional deep learning based image\nreconstruction is the need for registered pairs of training images containing a\nset of high-quality groundtruth images. This paper addresses this limitation by\nproposing a novel unsupervised deep registration-augmented reconstruction\nmethod (U-Dream) for training deep neural nets to reconstruct high-quality\nimages by directly mapping pairs of unregistered and artifact-corrupted images.\nThe ability of U-Dream to circumvent the need for accurately registered data\nmakes it widely applicable to many biomedical image reconstruction tasks. We\nvalidate it in accelerated magnetic resonance imaging (MRI) by training an\nimage reconstruction model directly on pairs of undersampled measurements from\nimages that have undergone nonrigid deformations.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 13:15:45 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Gan", "Weijie", ""], ["Sun", "Yu", ""], ["Eldeniz", "Cihat", ""], ["Liu", "Jiaming", ""], ["An", "Hongyu", ""], ["Kamilov", "Ulugbek S.", ""]]}, {"id": "2009.13990", "submitter": "Yeachan Park", "authors": "Yeachan Park, Myeongho Jeon, Junho Lee and Myungjoo Kang", "title": "MARA-Net: Single Image Deraining Network with Multi-level connections\n  and Adaptive Regional Attentions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Removing rain streaks from single images is an important problem in various\ncomputer vision tasks because rain streaks can degrade outdoor images and\nreduce their visibility. While recent convolutional neural network-based\nderaining models have succeeded in capturing rain streaks effectively,\ndifficulties in recovering the details in rain-free images still remain. In\nthis paper, we present a multi-level connection and adaptive regional attention\nnetwork (MARA-Net) to properly restore the original background textures in\nrainy images. The first main idea is a multi-level connection design that\nrepeatedly connects multi-level features of the encoder network to the decoder\nnetwork. Multi-level connections encourage the decoding process to use the\nfeature information of all levels. Channel attention is considered in\nmulti-level connections to learn which level of features is important in the\ndecoding process of the current level. The second main idea is a wide regional\nnon-local block (WRNL). As rain streaks primarily exhibit a vertical\ndistribution, we divide the grid of the image into horizontally-wide patches\nand apply a non-local operation to each region to explore the rich rain-free\nbackground information. Experimental results on both synthetic and real-world\nrainy datasets demonstrate that the proposed model significantly outperforms\nexisting state-of-the-art models. Furthermore, the results of the joint\nderaining and segmentation experiment prove that our model contributes\neffectively to other vision tasks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 13:21:31 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 08:49:30 GMT"}, {"version": "v3", "created": "Wed, 23 Dec 2020 06:52:40 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Park", "Yeachan", ""], ["Jeon", "Myeongho", ""], ["Lee", "Junho", ""], ["Kang", "Myungjoo", ""]]}, {"id": "2009.14001", "submitter": "Antoine Pirovano", "authors": "Antoine Pirovano and Hippolyte Heuberger and Sylvain Berlemont and\n  Sa\\\"id Ladjal and Isabelle Bloch", "title": "Improving Interpretability for Computer-aided Diagnosis tools on Whole\n  Slide Imaging with Multiple Instance Learning and Gradient-based Explanations", "comments": "8 pages (references excluded), 3 figures, presented in iMIMIC\n  Workshop at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods are widely used for medical applications to assist\nmedical doctors in their daily routines. While performances reach expert's\nlevel, interpretability (highlight how and what a trained model learned and why\nit makes a specific decision) is the next important challenge that deep\nlearning methods need to answer to be fully integrated in the medical field. In\nthis paper, we address the question of interpretability in the context of whole\nslide images (WSI) classification. We formalize the design of WSI\nclassification architectures and propose a piece-wise interpretability\napproach, relying on gradient-based methods, feature visualization and multiple\ninstance learning context. We aim at explaining how the decision is made based\non tile level scoring, how these tile scores are decided and which features are\nused and relevant for the task. After training two WSI classification\narchitectures on Camelyon-16 WSI dataset, highlighting discriminative features\nlearned, and validating our approach with pathologists, we propose a novel\nmanner of computing interpretability slide-level heat-maps, based on the\nextracted features, that improves tile-level classification performances by\nmore than 29% for AUC.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 13:39:27 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Pirovano", "Antoine", ""], ["Heuberger", "Hippolyte", ""], ["Berlemont", "Sylvain", ""], ["Ladjal", "Sa\u00efd", ""], ["Bloch", "Isabelle", ""]]}, {"id": "2009.14005", "submitter": "Sk Aziz Ali", "authors": "Sk Aziz Ali and Kerem Kahraman and Christian Theobalt and Didier\n  Stricker and Vladislav Golyanik", "title": "Fast Gravitational Approach for Rigid Point Set Registration with\n  Ordinary Differential Equations", "comments": "18 pages, 18 figures and two tables", "journal-ref": "IEEE Access, vol. 9, pp. 79060-79079, 2021", "doi": "10.1109/ACCESS.2021.3084505", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a new physics-based method for rigid point set\nalignment called Fast Gravitational Approach (FGA). In FGA, the source and\ntarget point sets are interpreted as rigid particle swarms with masses\ninteracting in a globally multiply-linked manner while moving in a simulated\ngravitational force field. The optimal alignment is obtained by explicit\nmodeling of forces acting on the particles as well as their velocities and\ndisplacements with second-order ordinary differential equations of motion.\nAdditional alignment cues (point-based or geometric features, and other\nboundary conditions) can be integrated into FGA through particle masses. We\npropose a smooth-particle mass function for point mass initialization, which\nimproves robustness to noise and structural discontinuities. To avoid\nprohibitive quadratic complexity of all-to-all point interactions, we adapt a\nBarnes-Hut tree for accelerated force computation and achieve quasilinear\ncomputational complexity. We show that the new method class has characteristics\nnot found in previous alignment methods such as efficient handling of partial\noverlaps, inhomogeneous point sampling densities, and coping with large point\nclouds with reduced runtime compared to the state of the art. Experiments show\nthat our method performs on par with or outperforms all compared competing\nnon-deep-learning-based and general-purpose techniques (which do not assume the\navailability of training data and a scene prior) in resolving transformations\nfor LiDAR data and gains state-of-the-art accuracy and speed when coping with\ndifferent types of data disturbances.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 15:05:39 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 15:12:05 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ali", "Sk Aziz", ""], ["Kahraman", "Kerem", ""], ["Theobalt", "Christian", ""], ["Stricker", "Didier", ""], ["Golyanik", "Vladislav", ""]]}, {"id": "2009.14082", "submitter": "Yimian Dai", "authors": "Yimian Dai and Fabian Gieseke and Stefan Oehmcke and Yiquan Wu and\n  Kobus Barnard", "title": "Attentional Feature Fusion", "comments": "Accepted by WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature fusion, the combination of features from different layers or\nbranches, is an omnipresent part of modern network architectures. It is often\nimplemented via simple operations, such as summation or concatenation, but this\nmight not be the best choice. In this work, we propose a uniform and general\nscheme, namely attentional feature fusion, which is applicable for most common\nscenarios, including feature fusion induced by short and long skip connections\nas well as within Inception layers. To better fuse features of inconsistent\nsemantics and scales, we propose a multi-scale channel attention module, which\naddresses issues that arise when fusing features given at different scales. We\nalso demonstrate that the initial integration of feature maps can become a\nbottleneck and that this issue can be alleviated by adding another level of\nattention, which we refer to as iterative attentional feature fusion. With\nfewer layers or parameters, our models outperform state-of-the-art networks on\nboth CIFAR-100 and ImageNet datasets, which suggests that more sophisticated\nattention mechanisms for feature fusion hold great potential to consistently\nyield better results compared to their direct counterparts. Our codes and\ntrained models are available online.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 15:10:18 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 17:41:20 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Dai", "Yimian", ""], ["Gieseke", "Fabian", ""], ["Oehmcke", "Stefan", ""], ["Wu", "Yiquan", ""], ["Barnard", "Kobus", ""]]}, {"id": "2009.14085", "submitter": "Heng Zhang", "authors": "Heng Zhang, Elisa Fromont, S\\'ebastien Lefevre, Bruno Avignon", "title": "Localize to Classify and Classify to Localize: Mutual Guidance in Object\n  Detection", "comments": "Accepted by ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most deep learning object detectors are based on the anchor mechanism and\nresort to the Intersection over Union (IoU) between predefined anchor boxes and\nground truth boxes to evaluate the matching quality between anchors and\nobjects. In this paper, we question this use of IoU and propose a new anchor\nmatching criterion guided, during the training phase, by the optimization of\nboth the localization and the classification tasks: the predictions related to\none task are used to dynamically assign sample anchors and improve the model on\nthe other task, and vice versa. Despite the simplicity of the proposed method,\nour experiments with different state-of-the-art deep learning architectures on\nPASCAL VOC and MS COCO datasets demonstrate the effectiveness and generality of\nour Mutual Guidance strategy.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 15:15:26 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Zhang", "Heng", ""], ["Fromont", "Elisa", ""], ["Lefevre", "S\u00e9bastien", ""], ["Avignon", "Bruno", ""]]}, {"id": "2009.14110", "submitter": "Meixu Chen", "authors": "Meixu Chen, Todd Goodall, Anjul Patney, and Alan C. Bovik", "title": "Learning to Compress Videos without Computing Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new deep learning video compression architecture\nthat does not require motion estimation, which is the most expensive element of\nmodern hybrid video compression codecs like H.264 and HEVC. Our framework\nexploits the regularities inherent to video motion, which we capture by using\ndisplaced frame differences as video representations to train the neural\nnetwork. In addition, we propose a new space-time reconstruction network based\non both an LSTM model and a UNet model, which we call LSTM-UNet. The combined\nnetwork is able to efficiently capture both temporal and spatial video\ninformation, making it highly amenable for our purposes. The new video\ncompression framework has three components: a Displacement Calculation Unit\n(DCU), a Displacement Compression Network (DCN), and a Frame Reconstruction\nNetwork (FRN), all of which are jointly optimized against a single perceptual\nloss function. The DCU removes the need for motion estimation found in hybrid\ncodecs, and is less expensive. In the DCN, an RNN-based network is utilized to\ncompress displaced frame differences as well as retain temporal information\nbetween frames. The LSTM-UNet is used in the FRN to learn space time\ndifferential representations of videos. Our experimental results show that our\ncompression model, which we call the MOtionless VIdeo Codec (MOVI-Codec),\nlearns how to efficiently compress videos without computing motion. Our\nexperiments show that MOVI-Codec outperforms the video coding standard H.264\nand exceeds the performance of the modern global standard HEVC codec as\nmeasured by MS-SSIM, especially on higher resolution videos.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 15:49:25 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 01:11:07 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Chen", "Meixu", ""], ["Goodall", "Todd", ""], ["Patney", "Anjul", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2009.14115", "submitter": "Yutong Bai", "authors": "Yutong Bai, Angtian Wang, Adam Kortylewski, Alan Yuille", "title": "CoKe: Localized Contrastive Learning for Robust Keypoint Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's most popular approaches to keypoint detection involve very complex\nnetwork architectures that aim to learn holistic representations of all\nkeypoints. In this work, we take a step back and ask: Can we simply learn a\nlocal keypoint representation from the output of a standard backbone\narchitecture? This will help make the network simpler and more robust,\nparticularly if large parts of the object are occluded. We demonstrate that\nthis is possible by looking at the problem from the perspective of\nrepresentation learning. Specifically, the keypoint kernels need to be chosen\nto optimize three types of distances in the feature space: Features of the same\nkeypoint should be similar to each other, while differing from those of other\nkeypoints, and also being distinct from features from the background clutter.\nWe formulate this optimization process within a framework, which we call CoKe,\nwhich includes supervised contrastive learning. CoKe needs to make several\napproximations to enable representation learning process on large datasets. In\nparticular, we introduce a clutter bank to approximate non-keypoint features,\nand a momentum update to compute the keypoint representation while training the\nfeature extractor. Our experiments show that CoKe achieves state-of-the-art\nresults compared to approaches that jointly represent all keypoints\nholistically (Stacked Hourglass Networks, MSS-Net) as well as to approaches\nthat are supervised by detailed 3D object geometry (StarMap). Moreover, CoKe is\nrobust and performs exceptionally well when objects are partially occluded and\nsignificantly outperforms related work on a range of diverse datasets\n(PASCAL3D+, MPII, ObjectNet3D).\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 16:00:43 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 01:32:46 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 16:22:35 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Bai", "Yutong", ""], ["Wang", "Angtian", ""], ["Kortylewski", "Adam", ""], ["Yuille", "Alan", ""]]}, {"id": "2009.14119", "submitter": "Tal Ridnik", "authors": "Emanuel Ben-Baruch, Tal Ridnik, Nadav Zamir, Asaf Noy, Itamar\n  Friedman, Matan Protter, Lihi Zelnik-Manor", "title": "Asymmetric Loss For Multi-Label Classification", "comments": "Accepted to ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a typical multi-label setting, a picture contains on average few positive\nlabels, and many negative ones. This positive-negative imbalance dominates the\noptimization process, and can lead to under-emphasizing gradients from positive\nlabels during training, resulting in poor accuracy. In this paper, we introduce\na novel asymmetric loss (\"ASL\"), which operates differently on positive and\nnegative samples. The loss enables to dynamically down-weights and\nhard-thresholds easy negative samples, while also discarding possibly\nmislabeled samples. We demonstrate how ASL can balance the probabilities of\ndifferent samples, and how this balancing is translated to better mAP scores.\nWith ASL, we reach state-of-the-art results on multiple popular multi-label\ndatasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate\nASL applicability for other tasks, such as single-label classification and\nobject detection. ASL is effective, easy to implement, and does not increase\nthe training time or complexity.\n  Implementation is available at: https://github.com/Alibaba-MIIL/ASL.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 16:08:19 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 14:50:30 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 06:54:56 GMT"}, {"version": "v4", "created": "Thu, 29 Jul 2021 15:02:43 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Ben-Baruch", "Emanuel", ""], ["Ridnik", "Tal", ""], ["Zamir", "Nadav", ""], ["Noy", "Asaf", ""], ["Friedman", "Itamar", ""], ["Protter", "Matan", ""], ["Zelnik-Manor", "Lihi", ""]]}, {"id": "2009.14139", "submitter": "O\\u{g}ulcan \\\"Ozdemir", "authors": "\\c{C}a\\u{g}r{\\i} G\\\"ok\\c{c}e and O\\u{g}ulcan \\\"Ozdemir and Ahmet Alp\n  K{\\i}nd{\\i}ro\\u{g}lu and Lale Akarun", "title": "Score-level Multi Cue Fusion for Sign Language Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign Languages are expressed through hand and upper body gestures as well as\nfacial expressions. Therefore, Sign Language Recognition (SLR) needs to focus\non all such cues. Previous work uses hand-crafted mechanisms or network\naggregation to extract the different cue features, to increase SLR performance.\nThis is slow and involves complicated architectures. We propose a more\nstraightforward approach that focuses on training separate cue models\nspecializing on the dominant hand, hands, face, and upper body regions. We\ncompare the performance of 3D Convolutional Neural Network (CNN) models\nspecializing in these regions, combine them through score-level fusion, and use\nthe weighted alternative. Our experimental results have shown the effectiveness\nof mixed convolutional models. Their fusion yields up to 19% accuracy\nimprovement over the baseline using the full upper body. Furthermore, we\ninclude a discussion for fusion settings, which can help future work on Sign\nLanguage Translation (SLT).\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 16:32:51 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["G\u00f6k\u00e7e", "\u00c7a\u011fr\u0131", ""], ["\u00d6zdemir", "O\u011fulcan", ""], ["K\u0131nd\u0131ro\u011flu", "Ahmet Alp", ""], ["Akarun", "Lale", ""]]}, {"id": "2009.14146", "submitter": "Jessie James Suarez", "authors": "Jessie James P. Suarez, Prospero C. Naval Jr", "title": "A Survey on Deep Learning Techniques for Video Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in videos is a problem that has been studied for more than\na decade. This area has piqued the interest of researchers due to its wide\napplicability. Because of this, there has been a wide array of approaches that\nhave been proposed throughout the years and these approaches range from\nstatistical-based approaches to machine learning-based approaches. Numerous\nsurveys have already been conducted on this area but this paper focuses on\nproviding an overview on the recent advances in the field of anomaly detection\nusing Deep Learning. Deep Learning has been applied successfully in many fields\nof artificial intelligence such as computer vision, natural language processing\nand more. This survey, however, focuses on how Deep Learning has improved and\nprovided more insights to the area of video anomaly detection. This paper\nprovides a categorization of the different Deep Learning approaches with\nrespect to their objectives. Additionally, it also discusses the commonly used\ndatasets along with the common evaluation metrics. Afterwards, a discussion\nsynthesizing all of the recent approaches is made to provide direction and\npossible areas for future research.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 16:40:46 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Suarez", "Jessie James P.", ""], ["Naval", "Prospero C.", "Jr"]]}, {"id": "2009.14162", "submitter": "Akin Caliskan", "authors": "Akin Caliskan, Armin Mustafa, Evren Imre, Adrian Hilton", "title": "Multi-View Consistency Loss for Improved Single-Image 3D Reconstruction\n  of Clothed People", "comments": "Accepted to Asian Conference on Computer Vision 2020 (ACCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method to improve the accuracy of the 3D reconstruction of\nclothed human shape from a single image. Recent work has introduced volumetric,\nimplicit and model-based shape learning frameworks for reconstruction of\nobjects and people from one or more images. However, the accuracy and\ncompleteness for reconstruction of clothed people is limited due to the large\nvariation in shape resulting from clothing, hair, body size, pose and camera\nviewpoint. This paper introduces two advances to overcome this limitation:\nfirstly a new synthetic dataset of realistic clothed people, 3DVH; and\nsecondly, a novel multiple-view loss function for training of monocular\nvolumetric shape estimation, which is demonstrated to significantly improve\ngeneralisation and reconstruction accuracy. The 3DVH dataset of realistic\nclothed 3D human models rendered with diverse natural backgrounds is\ndemonstrated to allows transfer to reconstruction from real images of people.\nComprehensive comparative performance evaluation on both synthetic and real\nimages of people demonstrates that the proposed method significantly\noutperforms the previous state-of-the-art learning-based single image 3D human\nshape estimation approaches achieving significant improvement of reconstruction\naccuracy, completeness, and quality. An ablation study shows that this is due\nto both the proposed multiple-view training and the new 3DVH dataset. The code\nand the dataset can be found at the project website:\nhttps://akincaliskan3d.github.io/MV3DH/.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 17:18:00 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Caliskan", "Akin", ""], ["Mustafa", "Armin", ""], ["Imre", "Evren", ""], ["Hilton", "Adrian", ""]]}, {"id": "2009.14178", "submitter": "Joris Gu\\'erin", "authors": "Joris Guerin, Anne Magaly de Paula Canuto and Luiz Marcos Garcia\n  Goncalves", "title": "Robust Detection of Objects under Periodic Motion with Gaussian Process\n  Filtering", "comments": "8 pages, 11 figures, 1 table Accepted as a full paper at ICMLA 2020\n  (19th IEEE International Conference On Machine Learning And Applications)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object Detection (OD) is an important task in Computer Vision with many\npractical applications. For some use cases, OD must be done on videos, where\nthe object of interest has a periodic motion. In this paper, we formalize the\nproblem of periodic OD, which consists in improving the performance of an OD\nmodel in the specific case where the object of interest is repeating similar\nspatio-temporal trajectories with respect to the video frames. The proposed\napproach is based on training a Gaussian Process to model the periodic motion,\nand use it to filter out the erroneous predictions of the OD model. By\nsimulating various OD models and periodic trajectories, we demonstrate that\nthis filtering approach, which is entirely data-driven, improves the detection\nperformance by a large margin.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 17:45:21 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Guerin", "Joris", ""], ["Canuto", "Anne Magaly de Paula", ""], ["Goncalves", "Luiz Marcos Garcia", ""]]}, {"id": "2009.14193", "submitter": "Anastasios Angelopoulos", "authors": "Anastasios Angelopoulos, Stephen Bates, Jitendra Malik, Michael I.\n  Jordan", "title": "Uncertainty Sets for Image Classifiers using Conformal Prediction", "comments": "ICLR 2021 Spotlight, https://openreview.net/forum?id=eNdiU_DbM9 .\n  Project website available at\n  https://people.eecs.berkeley.edu/~angelopoulos/blog/posts/conformal-classification/\n  . Codebase available at\n  https://github.com/aangelopoulos/conformal_classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional image classifiers can achieve high predictive accuracy, but\nquantifying their uncertainty remains an unresolved challenge, hindering their\ndeployment in consequential settings. Existing uncertainty quantification\ntechniques, such as Platt scaling, attempt to calibrate the network's\nprobability estimates, but they do not have formal guarantees. We present an\nalgorithm that modifies any classifier to output a predictive set containing\nthe true label with a user-specified probability, such as 90%. The algorithm is\nsimple and fast like Platt scaling, but provides a formal finite-sample\ncoverage guarantee for every model and dataset. Our method modifies an existing\nconformal prediction algorithm to give more stable predictive sets by\nregularizing the small scores of unlikely classes after Platt scaling. In\nexperiments on both Imagenet and Imagenet-V2 with ResNet-152 and other\nclassifiers, our scheme outperforms existing approaches, achieving coverage\nwith sets that are often factors of 5 to 10 smaller than a stand-alone Platt\nscaling baseline.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 17:58:04 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 18:59:13 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 01:50:26 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Angelopoulos", "Anastasios", ""], ["Bates", "Stephen", ""], ["Malik", "Jitendra", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2009.14194", "submitter": "Emmanuel Dufourq Dr", "authors": "Emmanuel Dufourq, Bruce A. Bassett", "title": "Deep Evolution for Facial Emotion Recognition", "comments": "Conference of the South African Institute of Computer Scientists and\n  Information Technologists 2020", "journal-ref": null, "doi": "10.1145/3410886.3410892", "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep facial expression recognition faces two challenges that both stem from\nthe large number of trainable parameters: long training times and a lack of\ninterpretability. We propose a novel method based on evolutionary algorithms,\nthat deals with both challenges by massively reducing the number of trainable\nparameters, whilst simultaneously retaining classification performance, and in\nsome cases achieving superior performance. We are robustly able to reduce the\nnumber of parameters on average by 95% (e.g. from 2M to 100k parameters) with\nno loss in classification accuracy. The algorithm learns to choose small\npatches from the image, relative to the nose, which carry the most important\ninformation about emotion, and which coincide with typical human choices of\nimportant features. Our work implements a novel form attention and shows that\nevolutionary algorithms are a valuable addition to machine learning in the deep\nlearning era, both for reducing the number of parameters for facial expression\nrecognition and for providing interpretable features that can help reduce bias.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 17:58:09 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 13:21:18 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Dufourq", "Emmanuel", ""], ["Bassett", "Bruce A.", ""]]}, {"id": "2009.14233", "submitter": "Yujiang Wang", "authors": "Pingchuan Ma, Yujiang Wang, Jie Shen, Stavros Petridis, Maja Pantic", "title": "Lip-reading with Densely Connected Temporal Convolutional Networks", "comments": "WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present the Densely Connected Temporal Convolutional Network\n(DC-TCN) for lip-reading of isolated words. Although Temporal Convolutional\nNetworks (TCN) have recently demonstrated great potential in many vision tasks,\nits receptive fields are not dense enough to model the complex temporal\ndynamics in lip-reading scenarios. To address this problem, we introduce dense\nconnections into the network to capture more robust temporal features.\nMoreover, our approach utilises the Squeeze-and-Excitation block, a\nlight-weight attention mechanism, to further enhance the model's classification\npower. Without bells and whistles, our DC-TCN method has achieved 88.36%\naccuracy on the Lip Reading in the Wild (LRW) dataset and 43.65% on the\nLRW-1000 dataset, which has surpassed all the baseline methods and is the new\nstate-of-the-art on both datasets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 18:08:15 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 20:15:49 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Ma", "Pingchuan", ""], ["Wang", "Yujiang", ""], ["Shen", "Jie", ""], ["Petridis", "Stavros", ""], ["Pantic", "Maja", ""]]}, {"id": "2009.14244", "submitter": "Benyamin Ghojogh", "authors": "Parisa Abdolrahim Poorheravi, Benyamin Ghojogh, Vincent Gaudet, Fakhri\n  Karray, Mark Crowley", "title": "Acceleration of Large Margin Metric Learning for Nearest Neighbor\n  Classification Using Triplet Mining and Stratified Sampling", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning is one of the techniques in manifold learning with the goal\nof finding a projection subspace for increasing and decreasing the inter- and\nintra-class variances, respectively. Some of the metric learning methods are\nbased on triplet learning with anchor-positive-negative triplets. Large margin\nmetric learning for nearest neighbor classification is one of the fundamental\nmethods to do this. Recently, Siamese networks have been introduced with the\ntriplet loss. Many triplet mining methods have been developed for Siamese\nnetworks; however, these techniques have not been applied on the triplets of\nlarge margin metric learning for nearest neighbor classification. In this work,\ninspired by the mining methods for Siamese networks, we propose several triplet\nmining techniques for large margin metric learning. Moreover, a hierarchical\napproach is proposed, for acceleration and scalability of optimization, where\ntriplets are selected by stratified sampling in hierarchical hyper-spheres. We\nanalyze the proposed methods on three publicly available datasets, i.e., Fisher\nIris, ORL faces, and MNIST datasets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 18:24:34 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Poorheravi", "Parisa Abdolrahim", ""], ["Ghojogh", "Benyamin", ""], ["Gaudet", "Vincent", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "2009.14259", "submitter": "Peter Jansen", "authors": "Peter A. Jansen", "title": "Visually-Grounded Planning without Vision: Language Models Infer\n  Detailed Plans from High-level Instructions", "comments": "Accepted to Findings of EMNLP. V2: corrected typo Table 1; margins\n  Table 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recently proposed ALFRED challenge task aims for a virtual robotic agent\nto complete complex multi-step everyday tasks in a virtual home environment\nfrom high-level natural language directives, such as \"put a hot piece of bread\non a plate\". Currently, the best-performing models are able to complete less\nthan 5% of these tasks successfully. In this work we focus on modeling the\ntranslation problem of converting natural language directives into detailed\nmulti-step sequences of actions that accomplish those goals in the virtual\nenvironment. We empirically demonstrate that it is possible to generate gold\nmulti-step plans from language directives alone without any visual input in 26%\nof unseen cases. When a small amount of visual information is incorporated,\nnamely the starting location in the virtual environment, our best-performing\nGPT-2 model successfully generates gold command sequences in 58% of cases. Our\nresults suggest that contextualized language models may provide strong visual\nsemantic planning modules for grounded virtual agents.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 18:52:39 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 19:16:00 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Jansen", "Peter A.", ""]]}, {"id": "2009.14260", "submitter": "Freddy Lecue", "authors": "Nicholas Halliwell, Freddy Lecue", "title": "Trustworthy Convolutional Neural Networks: A Gradient Penalized-based\n  Approach", "comments": "13pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are commonly used for image\nclassification. Saliency methods are examples of approaches that can be used to\ninterpret CNNs post hoc, identifying the most relevant pixels for a prediction\nfollowing the gradients flow. Even though CNNs can correctly classify images,\nthe underlying saliency maps could be erroneous in many cases. This can result\nin skepticism as to the validity of the model or its interpretation. We propose\na novel approach for training trustworthy CNNs by penalizing parameter choices\nthat result in inaccurate saliency maps generated during training. We add a\npenalty term for inaccurate saliency maps produced when the predicted label is\ncorrect, a penalty term for accurate saliency maps produced when the predicted\nlabel is incorrect, and a regularization term penalizing overly confident\nsaliency maps. Experiments show increased classification performance, user\nengagement, and trust.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 18:56:40 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Halliwell", "Nicholas", ""], ["Lecue", "Freddy", ""]]}, {"id": "2009.14265", "submitter": "Samreen Anjum", "authors": "Samreen Anjum, Chi Lin, Danna Gurari", "title": "CrowdMOT: Crowdsourcing Strategies for Tracking Multiple Objects in\n  Videos", "comments": "CSCW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is a valuable approach for tracking objects in videos in a more\nscalable manner than possible with domain experts. However, existing frameworks\ndo not produce high quality results with non-expert crowdworkers, especially\nfor scenarios where objects split. To address this shortcoming, we introduce a\ncrowdsourcing platform called CrowdMOT, and investigate two micro-task design\ndecisions: (1) whether to decompose the task so that each worker is in charge\nof annotating all objects in a sub-segment of the video versus annotating a\nsingle object across the entire video, and (2) whether to show annotations from\nprevious workers to the next individuals working on the task. We conduct\nexperiments on a diversity of videos which show both familiar objects (aka -\npeople) and unfamiliar objects (aka - cells). Our results highlight strategies\nfor efficiently collecting higher quality annotations than observed when using\nstrategies employed by today's state-of-art crowdsourcing system.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 19:12:21 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Anjum", "Samreen", ""], ["Lin", "Chi", ""], ["Gurari", "Danna", ""]]}, {"id": "2009.14303", "submitter": "Elias Nehme", "authors": "Elias Nehme, Boris Ferdman, Lucien E. Weiss, Tal Naor, Daniel\n  Freedman, Tomer Michaeli, Yoav Shechtman", "title": "Learning an optimal PSF-pair for ultra-dense 3D localization microscopy", "comments": "20 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.bio-ph physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-standing challenge in multiple-particle-tracking is the accurate and\nprecise 3D localization of individual particles at close proximity. One\nestablished approach for snapshot 3D imaging is point-spread-function (PSF)\nengineering, in which the PSF is modified to encode the axial information.\nHowever, engineered PSFs are challenging to localize at high densities due to\nlateral PSF overlaps. Here we suggest using multiple PSFs simultaneously to\nhelp overcome this challenge, and investigate the problem of engineering\nmultiple PSFs for dense 3D localization. We implement our approach using a\nbifurcated optical system that modifies two separate PSFs, and design the PSFs\nusing three different approaches including end-to-end learning. We demonstrate\nour approach experimentally by volumetric imaging of fluorescently labelled\ntelomeres in cells.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 20:54:52 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Nehme", "Elias", ""], ["Ferdman", "Boris", ""], ["Weiss", "Lucien E.", ""], ["Naor", "Tal", ""], ["Freedman", "Daniel", ""], ["Michaeli", "Tomer", ""], ["Shechtman", "Yoav", ""]]}, {"id": "2009.14326", "submitter": "B Debnath", "authors": "B Debnath, M O'brien, S Kumar, A Behera", "title": "Attention-Driven Body Pose Encoding for Human Activity Recognition", "comments": "This paper has been accepted for publication at the IAPR\n  IEEE/Computer Society International Conference on Pattern Recognition (ICPR),\n  Milan, 2021", "journal-ref": "IAPR IEEE/Computer Society International Conference on Pattern\n  Recognition (ICPR), Milan, 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a novel attention-based body pose encoding for human\nactivity recognition that presents a enriched representation of body-pose that\nis learned. The enriched data complements the 3D body joint position data and\nimproves model performance. In this paper, we propose a novel approach that\nlearns enhanced feature representations from a given sequence of 3D body\njoints. To achieve this encoding, the approach exploits 1) a spatial stream\nwhich encodes the spatial relationship between various body joints at each time\npoint to learn spatial structure involving the spatial distribution of\ndifferent body joints 2) a temporal stream that learns the temporal variation\nof individual body joints over the entire sequence duration to present a\ntemporally enhanced representation. Afterwards, these two pose streams are\nfused with a multi-head attention mechanism. % adapted from neural machine\ntranslation. We also capture the contextual information from the RGB video\nstream using a Inception-ResNet-V2 model combined with a multi-head attention\nand a bidirectional Long Short-Term Memory (LSTM) network. %Moreover, we whose\nperformance is enhanced through the multi-head attention mechanism. Finally,\nthe RGB video stream is combined with the fused body pose stream to give a\nnovel end-to-end deep model for effective human activity recognition.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 22:17:17 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 17:53:46 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Debnath", "B", ""], ["O'brien", "M", ""], ["Kumar", "S", ""], ["Behera", "A", ""]]}, {"id": "2009.14343", "submitter": "Abhishek Sharma", "authors": "Abhishek Sharma and Maks Ovsjanikov", "title": "Geometric Matrix Completion: A Functional View", "comments": "Accepted at GRL workshop, ICML'20. Code:\n  \\url{https://github.com/Not-IITian/functional-matrix-completion}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a totally functional view of geometric matrix completion problem.\nDifferently from existing work, we propose a novel regularization inspired from\nthe functional map literature that is more interpretable and theoretically\nsound. On synthetic tasks with strong underlying geometric structure, our\nframework outperforms state of the art by a huge margin (two order of\nmagnitude) demonstrating the potential of our approach. On real datasets, we\nachieve state-of-the-art results at a fraction of the computational effort of\nprevious methods. Our code is publicly available at\nhttps://github.com/Not-IITian/functional-matrix-completion\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 23:23:04 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Sharma", "Abhishek", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "2009.14352", "submitter": "Xiangxi Shi", "authors": "Xiangxi Shi, Xu Yang, Jiuxiang Gu, Shafiq Joty, and Jianfei Cai", "title": "Finding It at Another Side: A Viewpoint-Adapted Matching Encoder for\n  Change Captioning", "comments": null, "journal-ref": "ECCV2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change Captioning is a task that aims to describe the difference between\nimages with natural language. Most existing methods treat this problem as a\ndifference judgment without the existence of distractors, such as viewpoint\nchanges. However, in practice, viewpoint changes happen often and can overwhelm\nthe semantic difference to be described. In this paper, we propose a novel\nvisual encoder to explicitly distinguish viewpoint changes from semantic\nchanges in the change captioning task. Moreover, we further simulate the\nattention preference of humans and propose a novel reinforcement learning\nprocess to fine-tune the attention directly with language evaluation rewards.\nExtensive experimental results show that our method outperforms the\nstate-of-the-art approaches by a large margin in both Spot-the-Diff and\nCLEVR-Change datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 00:13:49 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Shi", "Xiangxi", ""], ["Yang", "Xu", ""], ["Gu", "Jiuxiang", ""], ["Joty", "Shafiq", ""], ["Cai", "Jianfei", ""]]}, {"id": "2009.14376", "submitter": "Ahmadreza Mosallanezhad", "authors": "Ahmadreza Mosallanezhad and Yasin N. Silva and Michelle V. Mancenido\n  and Huan Liu", "title": "Toward Privacy and Utility Preserving Image Representation", "comments": "Accepted as a working paper in SBP-BRiMS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face images are rich data items that are useful and can easily be collected\nin many applications, such as in 1-to-1 face verification tasks in the domain\nof security and surveillance systems. Multiple methods have been proposed to\nprotect an individual's privacy by perturbing the images to remove traces of\nidentifiable information, such as gender or race. However, significantly less\nattention has been given to the problem of protecting images while maintaining\noptimal task utility. In this paper, we study the novel problem of creating\nprivacy-preserving image representations with respect to a given utility task\nby proposing a principled framework called the Adversarial Image Anonymizer\n(AIA). AIA first creates an image representation using a generative model, then\nenhances the learned image representations using adversarial learning to\npreserve privacy and utility for a given task. Experiments were conducted on a\npublicly available data set to demonstrate the effectiveness of AIA as a\nprivacy-preserving mechanism for face images.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 01:25:00 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 16:27:59 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Mosallanezhad", "Ahmadreza", ""], ["Silva", "Yasin N.", ""], ["Mancenido", "Michelle V.", ""], ["Liu", "Huan", ""]]}, {"id": "2009.14385", "submitter": "Alexander Wong", "authors": "Alexander Wong, Mahmoud Famouri, and Mohammad Javad Shafiee", "title": "AttendNets: Tiny Deep Image Recognition Neural Networks for the Edge via\n  Visual Attention Condensers", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While significant advances in deep learning has resulted in state-of-the-art\nperformance across a large number of complex visual perception tasks, the\nwidespread deployment of deep neural networks for TinyML applications involving\non-device, low-power image recognition remains a big challenge given the\ncomplexity of deep neural networks. In this study, we introduce AttendNets,\nlow-precision, highly compact deep neural networks tailored for on-device image\nrecognition. More specifically, AttendNets possess deep self-attention\narchitectures based on visual attention condensers, which extends on the\nrecently introduced stand-alone attention condensers to improve spatial-channel\nselective attention. Furthermore, AttendNets have unique machine-designed\nmacroarchitecture and microarchitecture designs achieved via a machine-driven\ndesign exploration strategy. Experimental results on ImageNet$_{50}$ benchmark\ndataset for the task of on-device image recognition showed that AttendNets have\nsignificantly lower architectural and computational complexity when compared to\nseveral deep neural networks in research literature designed for efficiency\nwhile achieving highest accuracies (with the smallest AttendNet achieving\n$\\sim$7.2% higher accuracy, while requiring $\\sim$3$\\times$ fewer multiply-add\noperations, $\\sim$4.17$\\times$ fewer parameters, and $\\sim$16.7$\\times$ lower\nweight memory requirements than MobileNet-V1). Based on these promising\nresults, AttendNets illustrate the effectiveness of visual attention condensers\nas building blocks for enabling various on-device visual perception tasks for\nTinyML applications.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 01:53:17 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Wong", "Alexander", ""], ["Famouri", "Mahmoud", ""], ["Shafiee", "Mohammad Javad", ""]]}, {"id": "2009.14405", "submitter": "Yiqing Huang", "authors": "Yiqing Huang, Jiansheng Chen", "title": "Teacher-Critical Training Strategies for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing image captioning models are usually trained by cross-entropy (XE)\nloss and reinforcement learning (RL), which set ground-truth words as hard\ntargets and force the captioning model to learn from them. However, the widely\nadopted training strategies suffer from misalignment in XE training and\ninappropriate reward assignment in RL training. To tackle these problems, we\nintroduce a teacher model that serves as a bridge between the ground-truth\ncaption and the caption model by generating some easier-to-learn word proposals\nas soft targets. The teacher model is constructed by incorporating the\nground-truth image attributes into the baseline caption model. To effectively\nlearn from the teacher model, we propose Teacher-Critical Training Strategies\n(TCTS) for both XE and RL training to facilitate better learning processes for\nthe caption model. Experimental evaluations of several widely adopted caption\nmodels on the benchmark MSCOCO dataset show the proposed TCTS comprehensively\nenhances most evaluation metrics, especially the Bleu and Rouge-L scores, in\nboth training stages. TCTS is able to achieve to-date the best published single\nmodel Bleu-4 and Rouge-L performances of 40.2% and 59.4% on the MSCOCO Karpathy\ntest split. Our codes and pre-trained models will be open-sourced.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 03:15:12 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Huang", "Yiqing", ""], ["Chen", "Jiansheng", ""]]}, {"id": "2009.14406", "submitter": "Jing Li", "authors": "Chu-ran Wang, Jing Li, Fandong Zhang, Xinwei Sun, Hao Dong, Yizhou Yu,\n  Yizhou Wang", "title": "Bilateral Asymmetry Guided Counterfactual Generating Network for\n  Mammogram Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammogram benign or malignant classification with only image-level labels is\nchallenging due to the absence of lesion annotations. Motivated by the\nsymmetric prior that the lesions on one side of breasts rarely appear in the\ncorresponding areas on the other side, given a diseased image, we can explore a\ncounterfactual problem that how would the features have behaved if there were\nno lesions in the image, so as to identify the lesion areas. We derive a new\ntheoretical result for counterfactual generation based on the symmetric prior.\nBy building a causal model that entails such a prior for bilateral images, we\nobtain two optimization goals for counterfactual generation, which can be\naccomplished via our newly proposed counterfactual generative network. Our\nproposed model is mainly composed of Generator Adversarial Network and a\n\\emph{prediction feedback mechanism}, they are optimized jointly and prompt\neach other. Specifically, the former can further improve the classification\nperformance by generating counterfactual features to calculate lesion areas. On\nthe other hand, the latter helps counterfactual generation by the supervision\nof classification loss. The utility of our method and the effectiveness of each\nmodule in our model can be verified by state-of-the-art performance on INBreast\nand an in-house dataset and ablation studies.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 03:15:30 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Wang", "Chu-ran", ""], ["Li", "Jing", ""], ["Zhang", "Fandong", ""], ["Sun", "Xinwei", ""], ["Dong", "Hao", ""], ["Yu", "Yizhou", ""], ["Wang", "Yizhou", ""]]}, {"id": "2009.14410", "submitter": "Meng Fanxu", "authors": "Fanxu Meng, Hao Cheng, Ke Li, Huixiang Luo, Xiaowei Guo, Guangming Lu,\n  Xing Sun", "title": "Pruning Filter in Filter", "comments": "Accepted by NeurIPS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning has become a very powerful and effective technique to compress and\naccelerate modern neural networks. Existing pruning methods can be grouped into\ntwo categories: filter pruning (FP) and weight pruning (WP). FP wins at\nhardware compatibility but loses at the compression ratio compared with WP. To\nconverge the strength of both methods, we propose to prune the filter in the\nfilter. Specifically, we treat a filter $F \\in \\mathbb{R}^{C\\times K\\times K}$\nas $K \\times K$ stripes, i.e., $1\\times 1$ filters $\\in \\mathbb{R}^{C}$, then\nby pruning the stripes instead of the whole filter, we can achieve finer\ngranularity than traditional FP while being hardware friendly. We term our\nmethod as SWP (\\emph{Stripe-Wise Pruning}). SWP is implemented by introducing a\nnovel learnable matrix called Filter Skeleton, whose values reflect the shape\nof each filter. As some recent work has shown that the pruned architecture is\nmore crucial than the inherited important weights, we argue that the\narchitecture of a single filter, i.e., the shape, also matters. Through\nextensive experiments, we demonstrate that SWP is more effective compared to\nthe previous FP-based methods and achieves the state-of-art pruning ratio on\nCIFAR-10 and ImageNet datasets without obvious accuracy drop. Code is available\nat https://github.com/fxmeng/Pruning-Filter-in-Filter\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 03:35:16 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 15:47:13 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2020 08:35:21 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Meng", "Fanxu", ""], ["Cheng", "Hao", ""], ["Li", "Ke", ""], ["Luo", "Huixiang", ""], ["Guo", "Xiaowei", ""], ["Lu", "Guangming", ""], ["Sun", "Xing", ""]]}, {"id": "2009.14411", "submitter": "Viresh Ranjan", "authors": "Viresh Ranjan, Boyu Wang, Mubarak Shah, Minh Hoai", "title": "Uncertainty Estimation and Sample Selection for Crowd Counting", "comments": "ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for image-based crowd counting, one that can predict a\ncrowd density map together with the uncertainty values pertaining to the\npredicted density map. To obtain prediction uncertainty, we model the crowd\ndensity values using Gaussian distributions and develop a convolutional neural\nnetwork architecture to predict these distributions. A key advantage of our\nmethod over existing crowd counting methods is its ability to quantify the\nuncertainty of its predictions. We illustrate the benefits of knowing the\nprediction uncertainty by developing a method to reduce the human annotation\neffort needed to adapt counting networks to a new domain. We present sample\nselection strategies which make use of the density and uncertainty of\npredictions from the networks trained on one domain to select the informative\nimages from a target domain of interest to acquire human annotation. We show\nthat our sample selection strategy drastically reduces the amount of labeled\ndata from the target domain needed to adapt a counting network trained on a\nsource domain to the target domain. Empirically, the networks trained on\nUCF-QNRF dataset can be adapted to surpass the performance of the previous\nstate-of-the-art results on NWPU dataset and Shanghaitech dataset using only\n17$\\%$ of the labeled training samples from the target domain.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 03:40:07 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 18:41:49 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ranjan", "Viresh", ""], ["Wang", "Boyu", ""], ["Shah", "Mubarak", ""], ["Hoai", "Minh", ""]]}, {"id": "2009.14416", "submitter": "Qi Qian", "authors": "Qi Qian, Hao Li, Juhua Hu", "title": "Efficient Kernel Transfer in Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation is an effective way for model compression in deep\nlearning. Given a large model (i.e., teacher model), it aims to improve the\nperformance of a compact model (i.e., student model) by transferring the\ninformation from the teacher. An essential challenge in knowledge distillation\nis to identify the appropriate information to transfer. In early works, only\nthe final output of the teacher model is used as the soft label to help the\ntraining of student models. Recently, the information from intermediate layers\nis also adopted for better distillation. In this work, we aim to optimize the\nprocess of knowledge distillation from the perspective of kernel matrix. The\noutput of each layer in a neural network can be considered as a new feature\nspace generated by applying a kernel function on original images. Hence, we\npropose to transfer the corresponding kernel matrix (i.e., Gram matrix) from\nteacher models to student models for distillation. However, the size of the\nwhole kernel matrix is quadratic to the number of examples. To improve the\nefficiency, we decompose the original kernel matrix with Nystr{\\\"{o}}m method\nand then transfer the partial matrix obtained with landmark points, whose size\nis linear in the number of examples. More importantly, our theoretical analysis\nshows that the difference between the original kernel matrices of teacher and\nstudent can be well bounded by that of their corresponding partial matrices.\nFinally, a new strategy of generating appropriate landmark points is proposed\nfor better distillation. The empirical study on benchmark data sets\ndemonstrates the effectiveness of the proposed algorithm. Code will be\nreleased.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 04:03:09 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Qian", "Qi", ""], ["Li", "Hao", ""], ["Hu", "Juhua", ""]]}, {"id": "2009.14420", "submitter": "Bin Zhang", "authors": "Bin Zhang, Shengjie Zhao, Rongqing Zhang", "title": "Towards Adaptive Semantic Segmentation by Progressive Feature Refinement", "comments": null, "journal-ref": null, "doi": "10.1109/ICIP40778.2020.9190829", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the fundamental tasks in computer vision, semantic segmentation\nplays an important role in real world applications. Although numerous deep\nlearning models have made notable progress on several mainstream datasets with\nthe rapid development of convolutional networks, they still encounter various\nchallenges in practical scenarios. Unsupervised adaptive semantic segmentation\naims to obtain a robust classifier trained with source domain data, which is\nable to maintain stable performance when deployed to a target domain with\ndifferent data distribution. In this paper, we propose an innovative\nprogressive feature refinement framework, along with domain adversarial\nlearning to boost the transferability of segmentation networks. Specifically,\nwe firstly align the multi-stage intermediate feature maps of source and target\ndomain images, and then a domain classifier is adopted to discriminate the\nsegmentation output. As a result, the segmentation models trained with source\ndomain images can be transferred to a target domain without significant\nperformance degradation. Experimental results verify the efficiency of our\nproposed method compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 04:17:48 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Zhang", "Bin", ""], ["Zhao", "Shengjie", ""], ["Zhang", "Rongqing", ""]]}, {"id": "2009.14440", "submitter": "Darshan Gera", "authors": "Darshan Gera and S Balasubramanian", "title": "Affect Expression Behaviour Analysis in the Wild using Spatio-Channel\n  Attention and Complementary Context Information", "comments": "arXiv admin note: text overlap with arXiv:2007.10298 (ABAW2020\n  challenge test set results added)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression recognition(FER) in the wild is crucial for building\nreliable human-computer interactive systems. However, current FER systems fail\nto perform well under various natural and un-controlled conditions. This report\npresents attention based framework used in our submission to expression\nrecognition track of the Affective Behaviour Analysis in-the-wild (ABAW) 2020\ncompetition. Spatial-channel attention net(SCAN) is used to extract local and\nglobal attentive features without seeking any information from landmark\ndetectors. SCAN is complemented by a complementary context information(CCI)\nbranch which uses efficient channel attention(ECA) to enhance the relevance of\nfeatures. The performance of the model is validated on challenging Aff-Wild2\ndataset for categorical expression classification.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 12:26:15 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 06:24:19 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Gera", "Darshan", ""], ["Balasubramanian", "S", ""]]}, {"id": "2009.14448", "submitter": "Jayaraman J. Thiagarajan", "authors": "Bindya Venkatesh and Jayaraman J. Thiagarajan", "title": "Ask-n-Learn: Active Learning via Reliable Gradient Representations for\n  Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep predictive models rely on human supervision in the form of labeled\ntraining data. Obtaining large amounts of annotated training data can be\nexpensive and time consuming, and this becomes a critical bottleneck while\nbuilding such models in practice. In such scenarios, active learning (AL)\nstrategies are used to achieve faster convergence in terms of labeling efforts.\nExisting active learning employ a variety of heuristics based on uncertainty\nand diversity to select query samples. Despite their wide-spread use, in\npractice, their performance is limited by a number of factors including\nnon-calibrated uncertainties, insufficient trade-off between data exploration\nand exploitation, presence of confirmation bias etc. In order to address these\nchallenges, we propose Ask-n-Learn, an active learning approach based on\ngradient embeddings obtained using the pesudo-labels estimated in each\niteration of the algorithm. More importantly, we advocate the use of prediction\ncalibration to obtain reliable gradient embeddings, and propose a data\naugmentation strategy to alleviate the effects of confirmation bias during\npseudo-labeling. Through empirical studies on benchmark image classification\ntasks (CIFAR-10, SVHN, Fashion-MNIST, MNIST), we demonstrate significant\nimprovements over state-of-the-art baselines, including the recently proposed\nBADGE algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 05:19:56 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Venkatesh", "Bindya", ""], ["Thiagarajan", "Jayaraman J.", ""]]}, {"id": "2009.14468", "submitter": "Hui Zeng", "authors": "Hui Zeng, Jianrui Cai, Lida Li, Zisheng Cao, Lei Zhang", "title": "Learning Image-adaptive 3D Lookup Tables for High Performance Photo\n  Enhancement in Real-time", "comments": "High quality adaptive photo enhancement in real-time (<2ms for 4K\n  resolution images)! Accepted by IEEE Transactions on Pattern Analysis and\n  Machine Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2020.3026740", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the increasing popularity of learning based\nmethods to enhance the color and tone of photos. However, many existing photo\nenhancement methods either deliver unsatisfactory results or consume too much\ncomputational and memory resources, hindering their application to\nhigh-resolution images (usually with more than 12 megapixels) in practice. In\nthis paper, we learn image-adaptive 3-dimensional lookup tables (3D LUTs) to\nachieve fast and robust photo enhancement. 3D LUTs are widely used for\nmanipulating color and tone of photos, but they are usually manually tuned and\nfixed in camera imaging pipeline or photo editing tools. We, for the first time\nto our best knowledge, propose to learn 3D LUTs from annotated data using\npairwise or unpaired learning. More importantly, our learned 3D LUT is\nimage-adaptive for flexible photo enhancement. We learn multiple basis 3D LUTs\nand a small convolutional neural network (CNN) simultaneously in an end-to-end\nmanner. The small CNN works on the down-sampled version of the input image to\npredict content-dependent weights to fuse the multiple basis 3D LUTs into an\nimage-adaptive one, which is employed to transform the color and tone of source\nimages efficiently. Our model contains less than 600K parameters and takes less\nthan 2 ms to process an image of 4K resolution using one Titan RTX GPU. While\nbeing highly efficient, our model also outperforms the state-of-the-art photo\nenhancement methods by a large margin in terms of PSNR, SSIM and a color\ndifference metric on two publically available benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 06:34:57 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Zeng", "Hui", ""], ["Cai", "Jianrui", ""], ["Li", "Lida", ""], ["Cao", "Zisheng", ""], ["Zhang", "Lei", ""]]}, {"id": "2009.14487", "submitter": "Arash Akbarinia", "authors": "Arash Akbarinia, Raquel Gil-Rodr\\'iguez, Alban Flachot and Matteo\n  Toscani", "title": "The Utility of Decorrelating Colour Spaces in Vector Quantised\n  Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector quantised variational autoencoders (VQ-VAE) are characterised by three\nmain components: 1) encoding visual data, 2) assigning $k$ different vectors in\nthe so-called embedding space, and 3) decoding the learnt features. While\nimages are often represented in RGB colour space, the specific organisation of\ncolours in other spaces also offer interesting features, e.g. CIE L*a*b*\ndecorrelates chromaticity into opponent axes. In this article, we propose\ncolour space conversion, a simple quasi-unsupervised task, to enforce a network\nlearning structured representations. To this end, we trained several instances\nof VQ-VAE whose input is an image in one colour space, and its output in\nanother, e.g. from RGB to CIE L*a*b* (in total five colour spaces were\nconsidered). We examined the finite embedding space of trained networks in\norder to disentangle the colour representation in VQ-VAE models. Our analysis\nsuggests that certain vectors encode hue and others luminance information. We\nfurther evaluated the quality of reconstructed images at low-level using\npixel-wise colour metrics, and at high-level by inputting them to image\nclassification and scene segmentation networks. We conducted experiments in\nthree benchmark datasets: ImageNet, COCO and CelebA. Our results show, with\nrespect to the baseline network (whose input and output are RGB), colour\nconversion to decorrelated spaces obtains 1-2 Delta-E lower colour difference\nand 5-10% higher classification accuracy. We also observed that the learnt\nembedding space is easier to interpret in colour opponent models.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 07:44:01 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Akbarinia", "Arash", ""], ["Gil-Rodr\u00edguez", "Raquel", ""], ["Flachot", "Alban", ""], ["Toscani", "Matteo", ""]]}, {"id": "2009.14524", "submitter": "Deniz Beker", "authors": "Deniz Beker, Hiroharu Kato, Mihai Adrian Morariu, Takahiro Ando, Toru\n  Matsuoka, Wadim Kehl, Adrien Gaidon", "title": "Monocular Differentiable Rendering for Self-Supervised 3D Object\n  Detection", "comments": "20 pages, Supplementary material included, Published in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection from monocular images is an ill-posed problem due to the\nprojective entanglement of depth and scale. To overcome this ambiguity, we\npresent a novel self-supervised method for textured 3D shape reconstruction and\npose estimation of rigid objects with the help of strong shape priors and 2D\ninstance masks. Our method predicts the 3D location and meshes of each object\nin an image using differentiable rendering and a self-supervised objective\nderived from a pretrained monocular depth estimation network. We use the KITTI\n3D object detection dataset to evaluate the accuracy of the method. Experiments\ndemonstrate that we can effectively use noisy monocular depth and\ndifferentiable rendering as an alternative to expensive 3D ground-truth labels\nor LiDAR information.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 09:21:43 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Beker", "Deniz", ""], ["Kato", "Hiroharu", ""], ["Morariu", "Mihai Adrian", ""], ["Ando", "Takahiro", ""], ["Matsuoka", "Toru", ""], ["Kehl", "Wadim", ""], ["Gaidon", "Adrien", ""]]}, {"id": "2009.14530", "submitter": "Yimian Dai", "authors": "Yimian Dai and Yiquan Wu and Fei Zhou and Kobus Barnard", "title": "Asymmetric Contextual Modulation for Infrared Small Target Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-frame infrared small target detection remains a challenge not only due\nto the scarcity of intrinsic target characteristics but also because of lacking\na public dataset. In this paper, we first contribute an open dataset with\nhigh-quality annotations to advance the research in this field. We also propose\nan asymmetric contextual modulation module specially designed for detecting\ninfrared small targets. To better highlight small targets, besides a top-down\nglobal contextual feedback, we supplement a bottom-up modulation pathway based\non point-wise channel attention for exchanging high-level semantics and subtle\nlow-level details. We report ablation studies and comparisons to\nstate-of-the-art methods, where we find that our approach performs\nsignificantly better. Our dataset and code are available online.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 09:30:08 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Dai", "Yimian", ""], ["Wu", "Yiquan", ""], ["Zhou", "Fei", ""], ["Barnard", "Kobus", ""]]}, {"id": "2009.14545", "submitter": "Nikolai Huckle", "authors": "Nikolai Huckle and Noa Garcia and Yuta Nakashima", "title": "Demographic Influences on Contemporary Art with Unsupervised Style\n  Embeddings", "comments": "To be published in Proceedings of the European Conference in Computer\n  Vision Workshops 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational art analysis has, through its reliance on classification tasks,\nprioritised historical datasets in which the artworks are already well sorted\nwith the necessary annotations. Art produced today, on the other hand, is\nnumerous and easily accessible, through the internet and social networks that\nare used by professional and amateur artists alike to display their work.\nAlthough this art, yet unsorted in terms of style and genre, is less suited for\nsupervised analysis, the data sources come with novel information that may help\nframe the visual content in equally novel ways. As a first step in this\ndirection, we present contempArt, a multi-modal dataset of exclusively\ncontemporary artworks. contempArt is a collection of paintings and drawings, a\ndetailed graph network based on social connections on Instagram and additional\nsocio-demographic information; all attached to 442 artists at the beginning of\ntheir career. We evaluate three methods suited for generating unsupervised\nstyle embeddings of images and correlate them with the remaining data. We find\nno connections between visual style on the one hand and social proximity,\ngender, and nationality on the other.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 10:13:18 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 10:34:15 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Huckle", "Nikolai", ""], ["Garcia", "Noa", ""], ["Nakashima", "Yuta", ""]]}, {"id": "2009.14547", "submitter": "Yingxue Pang", "authors": "Yingxue Pang, Xin Li, Xin Jin, Yaojun Wu, Jianzhao Liu, Sen Liu, and\n  Zhibo Chen", "title": "FAN: Frequency Aggregation Network for Real Image Super-resolution", "comments": "14 pages, 7 figures, presented as a workshop paper at AIM 2020\n  Challenge @ ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution (SISR) aims to recover the high-resolution (HR)\nimage from its low-resolution (LR) input image. With the development of deep\nlearning, SISR has achieved great progress. However, It is still a challenge to\nrestore the real-world LR image with complicated authentic degradations.\nTherefore, we propose FAN, a frequency aggregation network, to address the\nreal-world image super-resolu-tion problem. Specifically, we extract different\nfrequencies of the LR image and pass them to a channel attention-grouped\nresidual dense network (CA-GRDB) individually to output corresponding feature\nmaps. And then aggregating these residual dense feature maps adaptively to\nrecover the HR image with enhanced details and textures. We conduct extensive\nexperiments quantitatively and qualitatively to verify that our FAN performs\nwell on the real image super-resolution task of AIM 2020 challenge. According\nto the released final results, our team SR-IM achieves the fourth place on the\nX4 track with PSNR of 31.1735 and SSIM of 0.8728.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 10:18:41 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Pang", "Yingxue", ""], ["Li", "Xin", ""], ["Jin", "Xin", ""], ["Wu", "Yaojun", ""], ["Liu", "Jianzhao", ""], ["Liu", "Sen", ""], ["Chen", "Zhibo", ""]]}, {"id": "2009.14558", "submitter": "Achiya Jerbi", "authors": "Achiya Jerbi, Roei Herzig, Jonathan Berant, Gal Chechik, Amir\n  Globerson", "title": "Learning Object Detection from Captions via Textual Scene Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a fundamental task in computer vision, requiring large\nannotated datasets that are difficult to collect, as annotators need to label\nobjects and their bounding boxes. Thus, it is a significant challenge to use\ncheaper forms of supervision effectively. Recent work has begun to explore\nimage captions as a source for weak supervision, but to date, in the context of\nobject detection, captions have only been used to infer the categories of the\nobjects in the image. In this work, we argue that captions contain much richer\ninformation about the image, including attributes of objects and their\nrelations. Namely, the text represents a scene of the image, as described\nrecently in the literature. We present a method that uses the attributes in\nthis \"textual scene graph\" to train object detectors. We empirically\ndemonstrate that the resulting model achieves state-of-the-art results on\nseveral challenging object detection datasets, outperforming recent approaches.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 10:59:20 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Jerbi", "Achiya", ""], ["Herzig", "Roei", ""], ["Berant", "Jonathan", ""], ["Chechik", "Gal", ""], ["Globerson", "Amir", ""]]}, {"id": "2009.14563", "submitter": "Namhyuk Ahn", "authors": "Sijin Kim, Namhyuk Ahn, Kyung-Ah Sohn", "title": "Restoring Spatially-Heterogeneous Distortions using Mixture of Experts\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning-based methods have been successfully applied\nto the image distortion restoration tasks. However, scenarios that assume a\nsingle distortion only may not be suitable for many real-world applications. To\ndeal with such cases, some studies have proposed sequentially combined\ndistortions datasets. Viewing in a different point of combining, we introduce a\nspatially-heterogeneous distortion dataset in which multiple corruptions are\napplied to the different locations of each image. In addition, we also propose\na mixture of experts network to effectively restore a multi-distortion image.\nMotivated by the multi-task learning, we design our network to have multiple\npaths that learn both common and distortion-specific representations. Our model\nis effective for restoring real-world distortions and we experimentally verify\nthat our method outperforms other models designed to manage both single\ndistortion and multiple distortions.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 11:06:38 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Kim", "Sijin", ""], ["Ahn", "Namhyuk", ""], ["Sohn", "Kyung-Ah", ""]]}, {"id": "2009.14570", "submitter": "Karl Hoffmann", "authors": "Karl B. Hoffmann and Ivo F. Sbalzarini", "title": "A robustness measure for singular point and index estimation in\n  discretized orientation and vector fields", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": "10.1002/pamm.202000261", "report-no": null, "categories": "cs.CV cond-mat.soft math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of singular points or topological defects in discretized\nvector fields occurs in diverse areas ranging from the polarization of the\ncosmic microwave background to liquid crystals to fingerprint recognition and\nbio-medical imaging. Due to their discrete nature, defects and their\ntopological charge cannot depend continuously on each single vector, but they\ndiscontinuously change as soon as a vector changes by more than a threshold.\nConsidering this threshold of admissible change at the level of vectors, we\ndevelop a robustness measure for discrete defect estimators. Here, we compare\ndifferent template paths for defect estimation in discretized vector or\norientation fields. Sampling prototypical vector field patterns around defects\nshows that the robustness increases with the length of template path, but less\nso in the presence of noise on the vectors. We therefore find an optimal\ntrade-off between resolution and robustness against noise for relatively small\ntemplates, except for the \"single pixel\" defect analysis, which cannot exclude\nzero robustness. The presented robustness measure paves the way for uncertainty\nquantification of defects in discretized vector fields.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 11:21:19 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Hoffmann", "Karl B.", ""], ["Sbalzarini", "Ivo F.", ""]]}, {"id": "2009.14623", "submitter": "Arash Mohammadi", "authors": "Parnian Afshar, Shahin Heidarian, Nastaran Enshaei, Farnoosh\n  Naderkhani, Moezedin Javad Rafiee, Anastasia Oikonomou, Faranak Babaki Fard,\n  Kaveh Samimi, Konstantinos N. Plataniotis, Arash Mohammadi", "title": "COVID-CT-MD: COVID-19 Computed Tomography (CT) Scan Dataset Applicable\n  in Machine Learning and Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel Coronavirus (COVID-19) has drastically overwhelmed more than 200\ncountries affecting millions and claiming almost 1 million lives, since its\nemergence in late 2019. This highly contagious disease can easily spread, and\nif not controlled in a timely fashion, can rapidly incapacitate healthcare\nsystems. The current standard diagnosis method, the Reverse Transcription\nPolymerase Chain Reaction (RT- PCR), is time consuming, and subject to low\nsensitivity. Chest Radiograph (CXR), the first imaging modality to be used, is\nreadily available and gives immediate results. However, it has notoriously\nlower sensitivity than Computed Tomography (CT), which can be used efficiently\nto complement other diagnostic methods. This paper introduces a new COVID-19 CT\nscan dataset, referred to as COVID-CT-MD, consisting of not only COVID-19\ncases, but also healthy and subjects infected by Community Acquired Pneumonia\n(CAP). COVID-CT-MD dataset, which is accompanied with lobe-level, slice-level\nand patient-level labels, has the potential to facilitate the COVID-19\nresearch, in particular COVID-CT-MD can assist in development of advanced\nMachine Learning (ML) and Deep Neural Network (DNN) based solutions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 20:42:07 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Afshar", "Parnian", ""], ["Heidarian", "Shahin", ""], ["Enshaei", "Nastaran", ""], ["Naderkhani", "Farnoosh", ""], ["Rafiee", "Moezedin Javad", ""], ["Oikonomou", "Anastasia", ""], ["Fard", "Faranak Babaki", ""], ["Samimi", "Kaveh", ""], ["Plataniotis", "Konstantinos N.", ""], ["Mohammadi", "Arash", ""]]}, {"id": "2009.14635", "submitter": "Kourosh Meshgi", "authors": "Kourosh Meshgi, Maryam Sadat Mirzaei", "title": "Adversarial Semi-Supervised Multi-Domain Tracking", "comments": "Accepted for ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks for multi-domain learning empowers an effective combination\nof information from different domains by sharing and co-learning the\nparameters. In visual tracking, the emerging features in shared layers of a\nmulti-domain tracker, trained on various sequences, are crucial for tracking in\nunseen videos. Yet, in a fully shared architecture, some of the emerging\nfeatures are useful only in a specific domain, reducing the generalization of\nthe learned feature representation. We propose a semi-supervised learning\nscheme to separate domain-invariant and domain-specific features using\nadversarial learning, to encourage mutual exclusion between them, and to\nleverage self-supervised learning for enhancing the shared features using the\nunlabeled reservoir. By employing these features and training dedicated layers\nfor each sequence, we build a tracker that performs exceptionally on different\ntypes of videos.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 12:47:28 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Meshgi", "Kourosh", ""], ["Mirzaei", "Maryam Sadat", ""]]}, {"id": "2009.14639", "submitter": "Okan K\\\"op\\\"ukl\\\"u", "authors": "Okan K\\\"op\\\"ukl\\\"u, Stefan H\\\"ormann, Fabian Herzog, Hakan Cevikalp,\n  Gerhard Rigoll", "title": "Dissected 3D CNNs: Temporal Skip Connections for Efficient Online Video\n  Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks with 3D kernels (3D CNNs) currently achieve\nstate-of-the-art results in video recognition tasks due to their supremacy in\nextracting spatiotemporal features within video frames. There have been many\nsuccessful 3D CNN architectures surpassing the state-of-the-art results\nsuccessively. However, nearly all of them are designed to operate offline\ncreating several serious handicaps during online operation. Firstly,\nconventional 3D CNNs are not dynamic since their output features represent the\ncomplete input clip instead of the most recent frame in the clip. Secondly,\nthey are not temporal resolution-preserving due to their inherent temporal\ndownsampling. Lastly, 3D CNNs are constrained to be used with fixed temporal\ninput size limiting their flexibility. In order to address these drawbacks, we\npropose dissected 3D CNNs, where the intermediate volumes of the network are\ndissected and propagated over depth (time) dimension for future calculations,\nsubstantially reducing the number of computations at online operation. For\naction classification, the dissected version of ResNet models performs 74-90%\nfewer computations at online operation while achieving $\\sim$5% better\nclassification accuracy on the Kinetics-600 dataset than conventional 3D ResNet\nmodels. Moreover, the advantages of dissected 3D CNNs are demonstrated by\ndeploying our approach onto several vision tasks, which consistently improved\nthe performance.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 12:48:52 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["K\u00f6p\u00fckl\u00fc", "Okan", ""], ["H\u00f6rmann", "Stefan", ""], ["Herzog", "Fabian", ""], ["Cevikalp", "Hakan", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "2009.14660", "submitter": "Okan K\\\"op\\\"ukl\\\"u", "authors": "Okan K\\\"op\\\"ukl\\\"u, Jiapeng Zheng, Hang Xu, Gerhard Rigoll", "title": "Driver Anomaly Detection: A Dataset and Contrastive Learning Approach", "comments": "Accepted to IEEE Winter Conference on Applications of Computer Vision\n  (WACV 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distracted drivers are more likely to fail to anticipate hazards, which\nresult in car accidents. Therefore, detecting anomalies in drivers' actions\n(i.e., any action deviating from normal driving) contains the utmost importance\nto reduce driver-related accidents. However, there are unbounded many anomalous\nactions that a driver can do while driving, which leads to an 'open set\nrecognition' problem. Accordingly, instead of recognizing a set of anomalous\nactions that are commonly defined by previous dataset providers, in this work,\nwe propose a contrastive learning approach to learn a metric to differentiate\nnormal driving from anomalous driving. For this task, we introduce a new\nvideo-based benchmark, the Driver Anomaly Detection (DAD) dataset, which\ncontains normal driving videos together with a set of anomalous actions in its\ntraining set. In the test set of the DAD dataset, there are unseen anomalous\nactions that still need to be winnowed out from normal driving. Our method\nreaches 0.9673 AUC on the test set, demonstrating the effectiveness of the\ncontrastive learning approach on the anomaly detection task. Our dataset, codes\nand pre-trained models are publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 13:23:21 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 15:00:06 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["K\u00f6p\u00fckl\u00fc", "Okan", ""], ["Zheng", "Jiapeng", ""], ["Xu", "Hang", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "2009.14661", "submitter": "Tong Yu", "authors": "Tong Yu, Nicolas Padoy", "title": "Encode the Unseen: Predictive Video Hashing for Scalable Mid-Stream\n  Retrieval", "comments": "Accepted at ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper tackles a new problem in computer vision: mid-stream\nvideo-to-video retrieval. This task, which consists in searching a database for\ncontent similar to a video right as it is playing, e.g. from a live stream,\nexhibits challenging characteristics. Only the beginning part of the video is\navailable as query and new frames are constantly added as the video plays out.\nTo perform retrieval in this demanding situation, we propose an approach based\non a binary encoder that is both predictive and incremental in order to (1)\naccount for the missing video content at query time and (2) keep up with\nrepeated, continuously evolving queries throughout the streaming. In\nparticular, we present the first hashing framework that infers the unseen\nfuture content of a currently playing video. Experiments on FCVID and\nActivityNet demonstrate the feasibility of this task. Our approach also yields\na significant mAP@20 performance increase compared to a baseline adapted from\nthe literature for this task, for instance 7.4% (2.6%) increase at 20% (50%) of\nelapsed runtime on FCVID using bitcodes of size 192 bits.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 13:25:59 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 13:11:34 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Yu", "Tong", ""], ["Padoy", "Nicolas", ""]]}, {"id": "2009.14677", "submitter": "Karsten W\\\"ullems", "authors": "Karsten W\\\"ullems, Tim W. Nattkemper", "title": "SoRC -- Evaluation of Computational Molecular Co-Localization Analysis\n  in Mass Spectrometry Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.QM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational analysis of Mass Spectrometry Imaging (MSI) data aims at\nthe identification of interesting mass co-localizations and the visualization\nof their lateral distribution in the sample, usually a tissue cross section.\nBut as the morphological structure of tissues and the different kinds of mass\nco-localization naturally show a huge diversity, the selection and tuning of\nthe computational method is a time-consuming effort. In this work we address\nthe special problem of computationally grouping mass channel images according\nto their similarities in their lateral distribution patterns. Such an analysis\nis driven by the idea, that groups of molecules that feature a similar\ndistribution pattern may have a functional relation. But the selection of the\nsimilarity function and other parameters is often done by a time-consuming and\nunsatsifactory trial and error. We propose a new flexible workflow scheme\ncalled SoRC (sum of ranked cluster indices) for automating this tuning step and\nmaking it much more efficient. We test SoRC using three different data sets\nacquired from the lab for three different kinds of samples (barley seed, mouse\nbladder tissue, human PXE skin). We show, that SORC can be applied to score and\nvisualize the results obtained with the applied methods in short time without\ntoo much effort. In our application example, the SoRC results for the three\ndata sets reveal that a) some well-known similarity functions are suited to\nachieve good results for all three data sets and b) for the MSI data featuring\na higher degree of irregularity improved results can be achieved by applying\nnon-standard similarity functions. The SoRC scores computed with our approach\nindicate that an automated testing and scoring of different methods for mass\nchannel image grouping can improve the final outcome of a study by finally\nselecting the methods of the highest scores.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 10:24:41 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["W\u00fcllems", "Karsten", ""], ["Nattkemper", "Tim W.", ""]]}, {"id": "2009.14684", "submitter": "Ricardo Sanchez-Matilla", "authors": "Ricardo Sanchez-Matilla and Andrea Cavallaro", "title": "Benchmark for Anonymous Video Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating the number of people exposed to digital signage is important to\nhelp measuring the return on investment of digital out-of-home advertisement.\nHowever, while audience measurement solutions are of increasing interest, no\ncommonly accepted benchmark exists to evaluate their performance. In this\npaper, we propose the first benchmark for digital out-of-home audience\nmeasurement that evaluates the tasks of audience localization and counting, and\naudience demographics. The benchmark is composed of a novel video dataset\ncaptured in multiple indoor and outdoor locations and a set of performance\nmeasures. Using the benchmark, we present an in-depth comparison of eight\nopen-source algorithms on four hardware platforms with GPU and CPU-optimized\ninferences and of two commercial off-the-shelf solutions for localization,\ncount, age, and gender estimation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 14:01:16 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Sanchez-Matilla", "Ricardo", ""], ["Cavallaro", "Andrea", ""]]}, {"id": "2009.14701", "submitter": "Alexander Wong", "authors": "Andrew Hryniowski, Xiao Yu Wang, and Alexander Wong", "title": "Where Does Trust Break Down? A Quantitative Trust Analysis of Deep\n  Neural Networks via Trust Matrix and Conditional Trust Densities", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advances and successes in deep learning in recent years have led to\nconsiderable efforts and investments into its widespread ubiquitous adoption\nfor a wide variety of applications, ranging from personal assistants and\nintelligent navigation to search and product recommendation in e-commerce. With\nthis tremendous rise in deep learning adoption comes questions about the\ntrustworthiness of the deep neural networks that power these applications.\nMotivated to answer such questions, there has been a very recent interest in\ntrust quantification. In this work, we introduce the concept of trust matrix, a\nnovel trust quantification strategy that leverages the recently introduced\nquestion-answer trust metric by Wong et al. to provide deeper, more detailed\ninsights into where trust breaks down for a given deep neural network given a\nset of questions. More specifically, a trust matrix defines the expected\nquestion-answer trust for a given actor-oracle answer scenario, allowing one to\nquickly spot areas of low trust that needs to be addressed to improve the\ntrustworthiness of a deep neural network. The proposed trust matrix is simple\nto calculate, humanly interpretable, and to the best of the authors' knowledge\nis the first to study trust at the actor-oracle answer level. We further extend\nthe concept of trust densities with the notion of conditional trust densities.\nWe experimentally leverage trust matrices to study several well-known deep\nneural network architectures for image recognition, and further study the trust\ndensity and conditional trust densities for an interesting actor-oracle answer\nscenario. The results illustrate that trust matrices, along with conditional\ntrust densities, can be useful tools in addition to the existing suite of trust\nquantification metrics for guiding practitioners and regulators in creating and\ncertifying deep learning solutions for trusted operation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 14:33:43 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Hryniowski", "Andrew", ""], ["Wang", "Xiao Yu", ""], ["Wong", "Alexander", ""]]}, {"id": "2009.14711", "submitter": "Mel Vecerik", "authors": "Mel Vecerik, Jean-Baptiste Regli, Oleg Sushkov, David Barker, Rugile\n  Pevceviciute, Thomas Roth\\\"orl, Christopher Schuster, Raia Hadsell, Lourdes\n  Agapito, Jonathan Scholz", "title": "S3K: Self-Supervised Semantic Keypoints for Robotic Manipulation via\n  Multi-View Consistency", "comments": "11 pages, supplementary material available at:\n  https://sites.google.com/view/2020-s3k/home", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robot's ability to act is fundamentally constrained by what it can\nperceive. Many existing approaches to visual representation learning utilize\ngeneral-purpose training criteria, e.g. image reconstruction, smoothness in\nlatent space, or usefulness for control, or else make use of large datasets\nannotated with specific features (bounding boxes, segmentations, etc.).\nHowever, both approaches often struggle to capture the fine-detail required for\nprecision tasks on specific objects, e.g. grasping and mating a plug and\nsocket. We argue that these difficulties arise from a lack of geometric\nstructure in these models. In this work we advocate semantic 3D keypoints as a\nvisual representation, and present a semi-supervised training objective that\ncan allow instance or category-level keypoints to be trained to 1-5\nmillimeter-accuracy with minimal supervision. Furthermore, unlike local\ntexture-based approaches, our model integrates contextual information from a\nlarge area and is therefore robust to occlusion, noise, and lack of discernible\ntexture. We demonstrate that this ability to locate semantic keypoints enables\nhigh level scripting of human understandable behaviours. Finally we show that\nthese keypoints provide a good way to define reward functions for reinforcement\nlearning and are a good representation for training agents.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 14:44:54 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 10:42:41 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Vecerik", "Mel", ""], ["Regli", "Jean-Baptiste", ""], ["Sushkov", "Oleg", ""], ["Barker", "David", ""], ["Pevceviciute", "Rugile", ""], ["Roth\u00f6rl", "Thomas", ""], ["Schuster", "Christopher", ""], ["Hadsell", "Raia", ""], ["Agapito", "Lourdes", ""], ["Scholz", "Jonathan", ""]]}, {"id": "2009.14712", "submitter": "Mathis Hoffmann", "authors": "Mathis Hoffmann, Claudia Buerhop-Lutz, Luca Reeb, Tobias Pickel, Thilo\n  Winkler, Bernd Doll, Tobias W\\\"urfl, Ian Marius Peters, Christoph Brabec,\n  Andreas Maier and Vincent Christlein", "title": "Deep Learning-based Pipeline for Module Power Prediction from EL\n  Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated inspection plays an important role in monitoring large-scale\nphotovoltaic power plants. Commonly, electroluminescense measurements are used\nto identify various types of defects on solar modules but have not been used to\ndetermine the power of a module. However, knowledge of the power at maximum\npower point is important as well, since drops in the power of a single module\ncan affect the performance of an entire string. By now, this is commonly\ndetermined by measurements that require to discontact or even dismount the\nmodule, rendering a regular inspection of individual modules infeasible. In\nthis work, we bridge the gap between electroluminescense measurements and the\npower determination of a module. We compile a large dataset of 719\nelectroluminescense measurementsof modules at various stages of degradation,\nespecially cell cracks and fractures, and the corresponding power at maximum\npower point. Here,we focus on inactive regions and cracks as the predominant\ntype of defect. We set up a baseline regression model to predict the power from\nelectroluminescense measurements with a mean absolute error of 9.0+/-3.7$W_P$\n(4.0+/-8.4%). Then, we show that deep-learning can be used to train a model\nthat performs significantly better (7.3+/-2.7$W_P$ or 3.2+/-6.5%) and propose a\nvariant of class activation maps to obtain the per cell power loss, as\npredicted by the model. With this work, we aim to open a new research topic.\nTherefore, we publicly release the dataset, the code and trained models to\nempower other researchers to compare against our results. Finally, we present a\nthorough evaluation of certain boundary conditions like the dataset size and an\nautomated preprocessing pipeline for on-site measurements showing multiple\nmodules at once.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 14:46:47 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 10:25:54 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Hoffmann", "Mathis", ""], ["Buerhop-Lutz", "Claudia", ""], ["Reeb", "Luca", ""], ["Pickel", "Tobias", ""], ["Winkler", "Thilo", ""], ["Doll", "Bernd", ""], ["W\u00fcrfl", "Tobias", ""], ["Peters", "Ian Marius", ""], ["Brabec", "Christoph", ""], ["Maier", "Andreas", ""], ["Christlein", "Vincent", ""]]}, {"id": "2009.14721", "submitter": "Mohamed Abbas Hedjazi", "authors": "Mohamed Abbas Hedjazi, Yakup Genc", "title": "Efficient texture-aware multi-GAN for image inpainting", "comments": "25 pages, 15 figures, 11 tables", "journal-ref": "Knowledge-Based Systems, Volume 217, 6 April 2021, 106789", "doi": "10.1016/j.knosys.2021.106789", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent GAN-based (Generative adversarial networks) inpainting methods show\nremarkable improvements and generate plausible images using multi-stage\nnetworks or Contextual Attention Modules (CAM). However, these techniques\nincrease the model complexity limiting their application in low-resource\nenvironments. Furthermore, they fail in generating high-resolution images with\nrealistic texture details due to the GAN stability problem. Motivated by these\nobservations, we propose a multi-GAN architecture improving both the\nperformance and rendering efficiency. Our training schema optimizes the\nparameters of four progressive efficient generators and discriminators in an\nend-to-end manner. Filling in low-resolution images is less challenging for\nGANs due to the small dimensional space. Meanwhile, it guides higher resolution\ngenerators to learn the global structure consistency of the image. To constrain\nthe inpainting task and ensure fine-grained textures, we adopt an LBP-based\nloss function to minimize the difference between the generated and the ground\ntruth textures. We conduct our experiments on Places2 and CelebHQ datasets.\nQualitative and quantitative results show that the proposed method not only\nperforms favorably against state-of-the-art algorithms but also speeds up the\ninference time.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 14:58:03 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 15:19:43 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Hedjazi", "Mohamed Abbas", ""], ["Genc", "Yakup", ""]]}, {"id": "2009.14737", "submitter": "Keyu Tian", "authors": "Keyu Tian, Chen Lin, Ming Sun, Luping Zhou, Junjie Yan, Wanli Ouyang", "title": "Improving Auto-Augment via Augmentation-Wise Weight Sharing", "comments": "Accepted to NeurIPS 2020 (Poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent progress on automatically searching augmentation policies has\nboosted the performance substantially for various tasks. A key component of\nautomatic augmentation search is the evaluation process for a particular\naugmentation policy, which is utilized to return reward and usually runs\nthousands of times. A plain evaluation process, which includes full model\ntraining and validation, would be time-consuming. To achieve efficiency, many\nchoose to sacrifice evaluation reliability for speed. In this paper, we dive\ninto the dynamics of augmented training of the model. This inspires us to\ndesign a powerful and efficient proxy task based on the Augmentation-Wise\nWeight Sharing (AWS) to form a fast yet accurate evaluation process in an\nelegant way. Comprehensive analysis verifies the superiority of this approach\nin terms of effectiveness and efficiency. The augmentation policies found by\nour method achieve superior accuracies compared with existing auto-augmentation\nsearch methods. On CIFAR-10, we achieve a top-1 error rate of 1.24%, which is\ncurrently the best performing single model without extra training data. On\nImageNet, we get a top-1 error rate of 20.36% for ResNet-50, which leads to\n3.34% absolute error rate reduction over the baseline augmentation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 15:23:12 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 15:12:47 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Tian", "Keyu", ""], ["Lin", "Chen", ""], ["Sun", "Ming", ""], ["Zhou", "Luping", ""], ["Yan", "Junjie", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2009.14743", "submitter": "Miao Jin", "authors": "Zhiqian You, Tingting Yang, Miao Jin", "title": "Multi-channel Deep 3D Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has been of great importance in many applications as a\nbiometric for its throughput, convenience, and non-invasiveness. Recent\nadvancements in deep Convolutional Neural Network (CNN) architectures have\nboosted significantly the performance of face recognition based on\ntwo-dimensional (2D) facial texture images and outperformed the previous state\nof the art using conventional methods. However, the accuracy of 2D face\nrecognition is still challenged by the change of pose, illumination, make-up,\nand expression. On the other hand, the geometric information contained in\nthree-dimensional (3D) face data has the potential to overcome the fundamental\nlimitations of 2D face data.\n  We propose a multi-Channel deep 3D face network for face recognition based on\n3D face data. We compute the geometric information of a 3D face based on its\npiecewise-linear triangular mesh structure and then conformally flatten\ngeometric information along with the color from 3D to 2D plane to leverage the\nstate-of-the-art deep CNN architectures. We modify the input layer of the\nnetwork to take images with nine channels instead of three only such that more\ngeometric information can be explicitly fed to it. We pre-train the network\nusing images from the VGG-Face \\cite{Parkhi2015} and then fine-tune it with the\ngenerated multi-channel face images. The face recognition accuracy of the\nmulti-Channel deep 3D face network has achieved 98.6. The experimental results\nalso clearly show that the network performs much better when a 9-channel image\nis flattened to plane based on the conformal map compared with the orthographic\nprojection.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 15:29:05 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["You", "Zhiqian", ""], ["Yang", "Tingting", ""], ["Jin", "Miao", ""]]}, {"id": "2009.14754", "submitter": "Hanbin Son", "authors": "Hanbin Son, Taeoh Kim, Hyeongmin Lee, Sangyoun Lee", "title": "Enhanced Standard Compatible Image Compression Framework based on\n  Auxiliary Codec Networks", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enhance image compression performance, recent deep neural network-based\nresearch can be divided into three categories: a learnable codec, a\npostprocessing network, and a compact representation network. The learnable\ncodec has been designed for an end-to-end learning beyond the conventional\ncompression modules. The postprocessing network increases the quality of\ndecoded images using an example-based learning. The compact representation\nnetwork is learned to reduce the capacity of an input image to reduce the\nbitrate while keeping the quality of the decoded image. However, these\napproaches are not compatible with the existing codecs or not optimal to\nincrease the coding efficiency. Specifically, it is difficult to achieve\noptimal learning in the previous studies using the compact representation\nnetwork, due to the inaccurate consideration of the codecs. In this paper, we\npropose a novel standard compatible image compression framework based on\nAuxiliary Codec Networks (ACNs). ACNs are designed to imitate image degradation\noperations of the existing codec, which delivers more accurate gradients to the\ncompact representation network. Therefore, the compact representation and the\npostprocessing networks can be learned effectively and optimally. We\ndemonstrate that our proposed framework based on JPEG and High Efficiency Video\nCoding (HEVC) standard substantially outperforms existing image compression\nalgorithms in a standard compatible manner.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 15:42:06 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Son", "Hanbin", ""], ["Kim", "Taeoh", ""], ["Lee", "Hyeongmin", ""], ["Lee", "Sangyoun", ""]]}, {"id": "2009.14757", "submitter": "Zhenzhen Wang", "authors": "Zhenzhen Wang, Chunyan Xu, Yap-Peng Tan and Junsong Yuan", "title": "Attention-Aware Noisy Label Learning for Image Classification", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) learned on large-scale labeled\nsamples have achieved remarkable progress in computer vision, such as\nimage/video classification. The cheapest way to obtain a large body of labeled\nvisual data is to crawl from websites with user-supplied labels, such as\nFlickr. However, these samples often tend to contain incorrect labels (i.e.\nnoisy labels), which will significantly degrade the network performance. In\nthis paper, the attention-aware noisy label learning approach ($A^2NL$) is\nproposed to improve the discriminative capability of the network trained on\ndatasets with potential label noise. Specifically, a Noise-Attention model,\nwhich contains multiple noise-specific units, is designed to better capture\nnoisy information. Each unit is expected to learn a specific noisy distribution\nfor a subset of images so that different disturbances are more precisely\nmodeled. Furthermore, a recursive learning process is introduced to strengthen\nthe learning ability of the attention network by taking advantage of the\nlearned high-level knowledge. To fully evaluate the proposed method, we conduct\nexperiments from two aspects: manually flipped label noise on large-scale image\nclassification datasets, including CIFAR-10, SVHN; and real-world label noise\non an online crawled clothing dataset with multiple attributes. The superior\nresults over state-of-the-art methods validate the effectiveness of our\nproposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 15:45:36 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Wang", "Zhenzhen", ""], ["Xu", "Chunyan", ""], ["Tan", "Yap-Peng", ""], ["Yuan", "Junsong", ""]]}, {"id": "2009.14776", "submitter": "Ting Yao", "authors": "Qi Cai and Yu Wang and Yingwei Pan and Ting Yao and Tao Mei", "title": "Joint Contrastive Learning with Infinite Possibilities", "comments": "NeurIPS 2020 Spotlight; Code is publicly available at:\n  https://github.com/caiqi/Joint-Contrastive-Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores useful modifications of the recent development in\ncontrastive learning via novel probabilistic modeling. We derive a particular\nform of contrastive loss named Joint Contrastive Learning (JCL). JCL implicitly\ninvolves the simultaneous learning of an infinite number of query-key pairs,\nwhich poses tighter constraints when searching for invariant features. We\nderive an upper bound on this formulation that allows analytical solutions in\nan end-to-end training manner. While JCL is practically effective in numerous\ncomputer vision applications, we also theoretically unveil the certain\nmechanisms that govern the behavior of JCL. We demonstrate that the proposed\nformulation harbors an innate agency that strongly favors similarity within\neach instance-specific class, and therefore remains advantageous when searching\nfor discriminative features among distinct instances. We evaluate these\nproposals on multiple benchmarks, demonstrating considerable improvements over\nexisting algorithms. Code is publicly available at:\nhttps://github.com/caiqi/Joint-Contrastive-Learning.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 16:24:21 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 13:27:10 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Cai", "Qi", ""], ["Wang", "Yu", ""], ["Pan", "Yingwei", ""], ["Yao", "Ting", ""], ["Mei", "Tao", ""]]}, {"id": "2009.14788", "submitter": "Matteo Ronchetti", "authors": "Matteo Ronchetti", "title": "TorchRadon: Fast Differentiable Routines for Computed Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work presents TorchRadon -- an open source CUDA library which contains a\nset of differentiable routines for solving computed tomography (CT)\nreconstruction problems. The library is designed to help researchers working on\nCT problems to combine deep learning and model-based approaches. The package is\ndeveloped as a PyTorch extension and can be seamlessly integrated into existing\ndeep learning training code. Compared to the existing Astra Toolbox, TorchRadon\nis up to 125 faster. The operators implemented by TorchRadon allow the\ncomputation of gradients using PyTorch backward(), and can therefore be easily\ninserted inside existing neural networks architectures. Because of its speed\nand GPU support, TorchRadon can also be effectively used as a fast backend for\nthe implementation of iterative algorithms. This paper presents the main\nfunctionalities of the library, compares results with existing libraries and\nprovides examples of usage.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 09:20:22 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Ronchetti", "Matteo", ""]]}, {"id": "2009.14798", "submitter": "Rumeysa Bodur", "authors": "Rumeysa Bodur, Binod Bhattarai, Tae-Kyun Kim", "title": "3D Dense Geometry-Guided Facial Expression Synthesis by Adversarial\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manipulating facial expressions is a challenging task due to fine-grained\nshape changes produced by facial muscles and the lack of input-output pairs for\nsupervised learning. Unlike previous methods using Generative Adversarial\nNetworks (GAN), which rely on cycle-consistency loss or sparse geometry\n(landmarks) loss for expression synthesis, we propose a novel GAN framework to\nexploit 3D dense (depth and surface normals) information for expression\nmanipulation. However, a large-scale dataset containing RGB images with\nexpression annotations and their corresponding depth maps is not available. To\nthis end, we propose to use an off-the-shelf state-of-the-art 3D reconstruction\nmodel to estimate the depth and create a large-scale RGB-Depth dataset after a\nmanual data clean-up process. We utilise this dataset to minimise the novel\ndepth consistency loss via adversarial learning (note we do not have ground\ntruth depth maps for generated face images) and the depth categorical loss of\nsynthetic data on the discriminator. In addition, to improve the generalisation\nand lower the bias of the depth parameters, we propose to use a novel\nconfidence regulariser on the discriminator side of the framework. We\nextensively performed both quantitative and qualitative evaluations on two\npublicly available challenging facial expression benchmarks: AffectNet and\nRaFD. Our experiments demonstrate that the proposed method outperforms the\ncompetitive baseline and existing arts by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 17:12:35 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Bodur", "Rumeysa", ""], ["Bhattarai", "Binod", ""], ["Kim", "Tae-Kyun", ""]]}]