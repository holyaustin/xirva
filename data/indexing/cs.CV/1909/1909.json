[{"id": "1909.00032", "submitter": "Shingo Kagami", "authors": "Shingo Kagami and Koichi Hashimoto", "title": "Animated Stickies: Fast Video Projection Mapping onto a Markerless Plane\n  through a Direct Closed-Loop Alignment", "comments": "To appear in IEEE Transactions on Visualization and Computer Graphics\n  (ISMAR 2019 Special Issue)", "journal-ref": null, "doi": "10.1109/TVCG.2019.2932248", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fast projection mapping method for moving image content\nprojected onto a markerless planar surface using a low-latency Digital\nMicromirror Device (DMD) projector. By adopting a closed-loop alignment\napproach, in which not only the surface texture but also the projected image is\ntracked by a camera, the proposed method is free from a calibration or position\nadjustment between the camera and projector. We designed fiducial patterns to\nbe inserted into a fast flapping sequence of binary frames of the DMD\nprojector, which allows the simultaneous tracking of the surface texture and a\nfiducial geometry separate from a single image captured by the camera. The\nproposed method implemented on a CPU runs at 400 fps and enables arbitrary\nvideo contents to be \"stuck\" onto a variety of textured surfaces.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 18:37:19 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Kagami", "Shingo", ""], ["Hashimoto", "Koichi", ""]]}, {"id": "1909.00052", "submitter": "Amey Agrawal", "authors": "Amey Agrawal, Rohit Karlupia", "title": "Learning Digital Circuits: A Journey Through Weight Invariant\n  Self-Pruning Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, in the paper \"Weight Agnostic Neural Networks\" Gaier & Ha utilized\narchitecture search to find networks where the topology completely encodes the\nknowledge. However, architecture search in topology space is expensive. We use\nthe existing framework of binarized networks to find performant topologies by\nconstraining the weights to be either, zero or one. We show that such\ntopologies achieve performance similar to standard networks while pruning more\nthan 99% weights. We further demonstrate that these topologies can perform\ntasks using constant weights without any explicit tuning. Finally, we discover\nthat in our setup each neuron acts like a NOR gate, virtually learning a\ndigital circuit. We demonstrate the efficacy of our approach on computer vision\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 20:07:39 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 18:42:12 GMT"}, {"version": "v3", "created": "Sun, 3 May 2020 22:23:59 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Agrawal", "Amey", ""], ["Karlupia", "Rohit", ""]]}, {"id": "1909.00073", "submitter": "Ricardo Borsoi", "authors": "Ricardo Augusto Borsoi", "title": "Robust Online Video Super-Resolution Using an Efficient Alternating\n  Projections Scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video super-resolution reconstruction (SRR) algorithms attempt to reconstruct\nhigh-resolution (HR) video sequences from low-resolution observations. Although\nrecent progress in video SRR has significantly improved the quality of the\nreconstructed HR sequences, it remains challenging to design SRR algorithms\nthat achieve good quality and robustness at a small computational complexity,\nbeing thus suitable for online applications. In this paper, we propose a new\nadaptive video SRR algorithm that achieves state-of-the-art performance at a\nvery small computational cost. Using a nonlinear cost function constructed\nconsidering characteristics of typical innovation outliers in natural image\nsequences and an edge-preserving regularization strategy, we achieve\nstate-of-the-art reconstructed image quality and robustness. This cost function\nis optimized using a specific alternating projections strategy over non-convex\nsets that is able to converge in a very few iterations. An accurate and very\nefficient approximation for the projection operations is also obtained using\ntools from multidimensional multirate signal processing. This solves the slow\nconvergence issue of stochastic gradient-based methods while keeping a small\ncomputational complexity. Simulation results with both synthetic and real image\nsequences show that the performance of the proposed algorithm is similar or\nbetter than state-of-the-art SRR algorithms, while requiring only a small\nfraction of their computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 21:15:58 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 02:31:28 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Borsoi", "Ricardo Augusto", ""]]}, {"id": "1909.00089", "submitter": "Ali Pour Yazdanpanah", "authors": "Ali Pour Yazdanpanah, Onur Afacan, Simon K. Warfield", "title": "Deep Plug-and-Play Prior for Parallel MRI Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast data acquisition in Magnetic Resonance Imaging (MRI) is vastly in demand\nand scan time directly depends on the number of acquired k-space samples.\nConventional MRI reconstruction methods for fast MRI acquisition mostly relied\non different regularizers which represent analytical models of sparsity.\nHowever, recent data-driven methods based on deep learning has resulted in\npromising improvements in image reconstruction algorithms. In this paper, we\npropose a deep plug-and-play prior framework for parallel MRI reconstruction\nproblems which utilize a deep neural network (DNN) as an advanced denoiser\nwithin an iterative method. This, in turn, enables rapid acquisition of MR\nimages with improved image quality. The proposed method was compared with the\nreconstructions using the clinical gold standard GRAPPA method. Our results\nwith undersampled data demonstrate that our method can deliver considerably\nhigher quality images at high acceleration factors in comparison to clinical\ngold standard method for MRI reconstructions. Our proposed reconstruction\nenables an increase in acceleration factor, and a reduction in acquisition time\nwhile maintaining high image quality.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 23:16:08 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 22:12:51 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Yazdanpanah", "Ali Pour", ""], ["Afacan", "Onur", ""], ["Warfield", "Simon K.", ""]]}, {"id": "1909.00112", "submitter": "Kaiwei Zeng", "authors": "Kaiwei Zeng", "title": "Energy Clustering for Unsupervised Person Re-identification", "comments": "Accepted by Image and Vision Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the high cost of data annotation in supervised learning for person\nre-identification (Re-ID) methods, unsupervised learning becomes more\nattractive in the real world. The Bottom-up Clustering (BUC) approach based on\nhierarchical clustering serves as one promising unsupervised clustering method.\nOne key factor of BUC is the distance measurement strategy. Ideally, the\ndistance measurement should consider both inter-cluster and intra-cluster\ndistance of all samples. However, BUC uses the minimum distance, only considers\na pair of the nearest sample between two clusters and ignores the diversity of\nother samples in clusters. To solve this problem, we propose to use the energy\ndistance to evaluate both the inter-cluster and intra-cluster distance in\nhierarchical clustering(E-cluster), and use the sum of squares of\ndeviations(SSD) as a regularization term to further balance the diversity and\nsimilarity of energy distance evaluation. We evaluate our method on large scale\nre-ID datasets, including Market-1501, DukeMTMC-reID and MARS. Extensive\nexperiments show that our method obtains significant improvements over the\nstate-of-the-art unsupervised methods, and even better than some transfer\nlearning methods.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 02:52:28 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 10:00:20 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2020 07:29:14 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Zeng", "Kaiwei", ""]]}, {"id": "1909.00114", "submitter": "Ziming Zhang", "authors": "Xenju Xu, Guanghui Wang, Alan Sullivan, Ziming Zhang", "title": "Towards Learning Affine-Invariant Representations via Data-Efficient\n  CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose integrating a priori knowledge into both design and\ntraining of convolutional neural networks (CNNs) to learn object\nrepresentations that are invariant to affine transformations (i.e.,\ntranslation, scale, rotation). Accordingly we propose a novel multi-scale\nmaxout CNN and train it end-to-end with a novel rotation-invariant regularizer.\nThis regularizer aims to enforce the weights in each 2D spatial filter to\napproximate circular patterns. In this way, we manage to handle affine\ntransformations in training using convolution, multi-scale maxout, and circular\nfilters. Empirically we demonstrate that such knowledge can significantly\nimprove the data-efficiency as well as generalization and robustness of learned\nmodels. For instance, on the Traffic Sign data set and trained with only 10\nimages per class, our method can achieve 84.15% that outperforms the\nstate-of-the-art by 29.80% in terms of test accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 03:11:59 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Xu", "Xenju", ""], ["Wang", "Guanghui", ""], ["Sullivan", "Alan", ""], ["Zhang", "Ziming", ""]]}, {"id": "1909.00119", "submitter": "Tairan Chen", "authors": "Tairan Chen, Zirui Li, Yiting He, Zewen Xu, Zhe Yan, Huiqian Li", "title": "From perception to control: an autonomous driving system for a formula\n  student driverless car", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper introduces the autonomous system of the \"Smart Shark II\" which won\nthe Formula Student Autonomous China (FSAC) Competition in 2018. In this\ncompetition, an autonomous racecar is required to complete autonomously two\nlaps of unknown track. In this paper, the author presents the self-driving\nsoftware structure of this racecar which ensure high vehicle speed and safety.\nThe key components ensure a stable driving of the racecar, LiDAR-based and\nVision-based cone detection provide a redundant perception; the EKF-based\nlocalization offers high accuracy and high frequency state estimation;\nperception results are accumulated in time and space by occupancy grid map.\nAfter getting the trajectory, a model predictive control algorithm is used to\noptimize in both longitudinal and lateral control of the racecar. Finally, the\nperformance of an experiment based on real-world data is shown.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 03:56:47 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Chen", "Tairan", ""], ["Li", "Zirui", ""], ["He", "Yiting", ""], ["Xu", "Zewen", ""], ["Yan", "Zhe", ""], ["Li", "Huiqian", ""]]}, {"id": "1909.00121", "submitter": "Haoran Chen", "authors": "Haoran Chen, Ke Lin, Alexander Maye, Jianming Li and Xiaolin Hu", "title": "A Semantics-Assisted Video Captioning Model Trained with Scheduled\n  Sampling", "comments": "11 pages", "journal-ref": "Front. Robot. AI 7:475767 (2020)", "doi": "10.3389/frobt.2020.475767", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the features of a video, recurrent neural networks can be used to\nautomatically generate a caption for the video. Existing methods for video\ncaptioning have at least three limitations. First, semantic information has\nbeen widely applied to boost the performance of video captioning models, but\nexisting networks often fail to provide meaningful semantic features. Second,\nthe Teacher Forcing algorithm is often utilized to optimize video captioning\nmodels, but during training and inference, different strategies are applied to\nguide word generation, leading to poor performance. Third, current video\ncaptioning models are prone to generate relatively short captions that express\nvideo contents inappropriately. Toward resolving these three problems, we\nsuggest three corresponding improvements. First of all, we propose a metric to\ncompare the quality of semantic features, and utilize appropriate features as\ninput for a semantic detection network (SDN) with adequate complexity in order\nto generate meaningful semantic features for videos. Then, we apply a scheduled\nsampling strategy that gradually transfers the training phase from a\nteacher-guided manner toward a more self-teaching manner. Finally, the ordinary\nlogarithm probability loss function is leveraged by sentence length so that the\ninclination of generating short sentences is alleviated. Our model achieves\nbetter results than previous models on the YouTube2Text dataset and is\ncompetitive with the previous best model on the MSR-VTT dataset.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 04:01:38 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 16:01:47 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 08:01:29 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Chen", "Haoran", ""], ["Lin", "Ke", ""], ["Maye", "Alexander", ""], ["Li", "Jianming", ""], ["Hu", "Xiaolin", ""]]}, {"id": "1909.00122", "submitter": "Shen Yan", "authors": "Shen Yan, Biyi Fang, Faen Zhang, Yu Zheng, Xiao Zeng, Hui Xu, Mi Zhang", "title": "HM-NAS: Efficient Neural Architecture Search via Hierarchical Masking", "comments": "9 pages, 6 figures, 6 tables. Nominated for ICCV 2019 Neural\n  Architects Workshop Best Paper Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of automatic methods, often referred to as Neural Architecture Search\n(NAS), in designing neural network architectures has recently drawn\nconsiderable attention. In this work, we present an efficient NAS approach,\nnamed HM- NAS, that generalizes existing weight sharing based NAS approaches.\nExisting weight sharing based NAS approaches still adopt hand-designed\nheuristics to generate architecture candidates. As a consequence, the space of\narchitecture candidates is constrained in a subset of all possible\narchitectures, making the architecture search results sub-optimal. HM-NAS\naddresses this limitation via two innovations. First, HM-NAS incorporates a\nmulti-level architecture encoding scheme to enable searching for more flexible\nnetwork architectures. Second, it discards the hand-designed heuristics and\nincorporates a hierarchical masking scheme that automatically learns and\ndetermines the optimal architecture. Compared to state-of-the-art weight\nsharing based approaches, HM-NAS is able to achieve better architecture search\nperformance and competitive model evaluation accuracy. Without the constraint\nimposed by the hand-designed heuristics, our searched networks contain more\nflexible and meaningful architectures that existing weight sharing based NAS\napproaches are not able to discover.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 04:02:16 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2019 08:33:25 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Yan", "Shen", ""], ["Fang", "Biyi", ""], ["Zhang", "Faen", ""], ["Zheng", "Yu", ""], ["Zeng", "Xiao", ""], ["Xu", "Hui", ""], ["Zhang", "Mi", ""]]}, {"id": "1909.00125", "submitter": "Cem Sazara", "authors": "Cem Sazara, Mecit Cetin, Khan M. Iftekharuddin", "title": "Detecting floodwater on roadways from image data with handcrafted\n  features and deep transfer learning", "comments": "Accepted at IEEE-ITSC 2019: The 22nd IEEE International Conference on\n  Intelligent Transportation Systems, Auckland, NZ, October 27-30, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting roadway segments inundated due to floodwater has important\napplications for vehicle routing and traffic management decisions. This paper\nproposes a set of algorithms to automatically detect floodwater that may be\npresent in an image captured by mobile phones or other types of optical\ncameras. For this purpose, image classification and flood area segmentation\nmethods are developed. For the classification task, we used Local Binary\nPatterns (LBP), Histogram of Oriented Gradients (HOG) and pre-trained deep\nneural network (VGG-16) as feature extractors and trained logistic regression,\nk-nearest neighbors, and decision tree classifiers on the extracted features.\nPre-trained VGG-16 network with logistic regression classifier outperformed all\nother methods. For the flood area segmentation task, we investigated superpixel\nbased methods and Fully Convolutional Neural Network (FCN). Similar to the\nclassification task, we trained logistic regression and k-nearest neighbors\nclassifiers on the superpixel areas and compared that with an end-to-end\ntrained FCN. Conditional Random Fields (CRF) method was applied after both\nsegmentation methods to post-process coarse segmentation results. FCN offered\nthe highest scores in all metrics; it was followed by superpixel-based logistic\nregression and then superpixel-based KNN.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 04:35:56 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Sazara", "Cem", ""], ["Cetin", "Mecit", ""], ["Iftekharuddin", "Khan M.", ""]]}, {"id": "1909.00133", "submitter": "Gong Cheng", "authors": "Ke Li, Gang Wan, Gong Cheng, Liqiu Meng, Junwei Han", "title": "Object Detection in Optical Remote Sensing Images: A Survey and A New\n  Benchmark", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, 159: 296-307,\n  2020", "doi": "10.1016/j.isprsjprs.2019.11.023", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Substantial efforts have been devoted more recently to presenting various\nmethods for object detection in optical remote sensing images. However, the\ncurrent survey of datasets and deep learning based methods for object detection\nin optical remote sensing images is not adequate. Moreover, most of the\nexisting datasets have some shortcomings, for example, the numbers of images\nand object categories are small scale, and the image diversity and variations\nare insufficient. These limitations greatly affect the development of deep\nlearning based object detection methods. In the paper, we provide a\ncomprehensive review of the recent deep learning based object detection\nprogress in both the computer vision and earth observation communities. Then,\nwe propose a large-scale, publicly available benchmark for object DetectIon in\nOptical Remote sensing images, which we name as DIOR. The dataset contains\n23463 images and 192472 instances, covering 20 object classes. The proposed\nDIOR dataset 1) is large-scale on the object categories, on the object instance\nnumber, and on the total image number; 2) has a large range of object size\nvariations, not only in terms of spatial resolutions, but also in the aspect of\ninter- and intra-class size variability across objects; 3) holds big variations\nas the images are obtained with different imaging conditions, weathers,\nseasons, and image quality; and 4) has high inter-class similarity and\nintra-class diversity. The proposed benchmark can help the researchers to\ndevelop and validate their data-driven methods. Finally, we evaluate several\nstate-of-the-art approaches on our DIOR dataset to establish a baseline for\nfuture research.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 05:36:15 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 02:13:30 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Li", "Ke", ""], ["Wan", "Gang", ""], ["Cheng", "Gong", ""], ["Meng", "Liqiu", ""], ["Han", "Junwei", ""]]}, {"id": "1909.00134", "submitter": "Kaihong Wang", "authors": "Kaihong Wang and Mona Jalal and Sankara Jefferson and Yi Zheng and\n  Elaine O. Nsoesie and Margrit Betke", "title": "Scraping Social Media Photos Posted in Kenya and Elsewhere to Detect and\n  Analyze Food Types", "comments": "Another version of the paper was submitted to the ACM International\n  Conference on Multimedia (ACMMM2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring population-level changes in diet could be useful for education and\nfor implementing interventions to improve health. Research has shown that data\nfrom social media sources can be used for monitoring dietary behavior. We\npropose a scrape-by-location methodology to create food image datasets from\nInstagram posts. We used it to collect 3.56 million images over a period of 20\ndays in March 2019. We also propose a scrape-by-keywords methodology and used\nit to scrape ~30,000 images and their captions of 38 Kenyan food types. We\npublish two datasets of 104,000 and 8,174 image/caption pairs, respectively.\nWith the first dataset, Kenya104K, we train a Kenyan Food Classifier, called\nKenyanFC, to distinguish Kenyan food from non-food images posted in Kenya. We\nused the second dataset, KenyanFood13, to train a classifier KenyanFTR, short\nfor Kenyan Food Type Recognizer, to recognize 13 popular food types in Kenya.\nThe KenyanFTR is a multimodal deep neural network that can identify 13 types of\nKenyan foods using both images and their corresponding captions. Experiments\nshow that the average top-1 accuracy of KenyanFC is 99% over 10,400 tested\nInstagram images and of KenyanFTR is 81% over 8,174 tested data points.\nAblation studies show that three of the 13 food types are particularly\ndifficult to categorize based on image content only and that adding analysis of\ncaptions to the image analysis yields a classifier that is 9 percent points\nmore accurate than a classifier that relies only on images. Our food trend\nanalysis revealed that cakes and roasted meats were the most popular foods in\nphotographs on Instagram in Kenya in March 2019.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 05:37:13 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Wang", "Kaihong", ""], ["Jalal", "Mona", ""], ["Jefferson", "Sankara", ""], ["Zheng", "Yi", ""], ["Nsoesie", "Elaine O.", ""], ["Betke", "Margrit", ""]]}, {"id": "1909.00166", "submitter": "Maryam Asadi", "authors": "Reza Azad, Maryam Asadi-Aghbolaghi, Mahmood Fathy, Sergio Escalera", "title": "Bi-Directional ConvLSTM U-Net with Densley Connected Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning-based networks have achieved state-of-the-art\nperformance in medical image segmentation. Among the existing networks, U-Net\nhas been successfully applied on medical image segmentation. In this paper, we\npropose an extension of U-Net, Bi-directional ConvLSTM U-Net with Densely\nconnected convolutions (BCDU-Net), for medical image segmentation, in which we\ntake full advantages of U-Net, bi-directional ConvLSTM (BConvLSTM) and the\nmechanism of dense convolutions. Instead of a simple concatenation in the skip\nconnection of U-Net, we employ BConvLSTM to combine the feature maps extracted\nfrom the corresponding encoding path and the previous decoding up-convolutional\nlayer in a non-linear way. To strengthen feature propagation and encourage\nfeature reuse, we use densely connected convolutions in the last convolutional\nlayer of the encoding path. Finally, we can accelerate the convergence speed of\nthe proposed network by employing batch normalization (BN). The proposed model\nis evaluated on three datasets of: retinal blood vessel segmentation, skin\nlesion segmentation, and lung nodule segmentation, achieving state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 08:29:31 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Azad", "Reza", ""], ["Asadi-Aghbolaghi", "Maryam", ""], ["Fathy", "Mahmood", ""], ["Escalera", "Sergio", ""]]}, {"id": "1909.00169", "submitter": "Kemal Oksuz", "authors": "Kemal Oksuz, Baris Can Cam, Sinan Kalkan, Emre Akbas", "title": "Imbalance Problems in Object Detection: A Review", "comments": "Accepted to IEEE TPAMI; currently in press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a comprehensive review of the imbalance problems in\nobject detection. To analyze the problems in a systematic manner, we introduce\na problem-based taxonomy. Following this taxonomy, we discuss each problem in\ndepth and present a unifying yet critical perspective on the solutions in the\nliterature. In addition, we identify major open issues regarding the existing\nimbalance problems as well as imbalance problems that have not been discussed\nbefore. Moreover, in order to keep our review up to date, we provide an\naccompanying webpage which catalogs papers addressing imbalance problems,\naccording to our problem-based taxonomy. Researchers can track newer studies on\nthis webpage available at:\nhttps://github.com/kemaloksuz/ObjectDetectionImbalance .\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 08:55:28 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 22:00:15 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 17:54:54 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Oksuz", "Kemal", ""], ["Cam", "Baris Can", ""], ["Kalkan", "Sinan", ""], ["Akbas", "Emre", ""]]}, {"id": "1909.00179", "submitter": "Henghui Ding", "authors": "Henghui Ding, Xudong Jiang, Ai Qun Liu, Nadia Magnenat Thalmann, Gang\n  Wang", "title": "Boundary-Aware Feature Propagation for Scene Segmentation", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the challenging issue of scene segmentation. To\nincrease the feature similarity of the same object while keeping the feature\ndiscrimination of different objects, we explore to propagate information\nthroughout the image under the control of objects' boundaries. To this end, we\nfirst propose to learn the boundary as an additional semantic class to enable\nthe network to be aware of the boundary layout. Then, we propose unidirectional\nacyclic graphs (UAGs) to model the function of undirected cyclic graphs (UCGs),\nwhich structurize the image via building graphic pixel-by-pixel connections, in\nan efficient and effective way. Furthermore, we propose a boundary-aware\nfeature propagation (BFP) module to harvest and propagate the local features\nwithin their regions isolated by the learned boundaries in the UAG-structured\nimage. The proposed BFP is capable of splitting the feature propagation into a\nset of semantic groups via building strong connections among the same segment\nregion but weak connections between different segment regions. Without bells\nand whistles, our approach achieves new state-of-the-art segmentation\nperformance on three challenging semantic segmentation datasets, i.e.,\nPASCAL-Context, CamVid, and Cityscapes.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 09:56:09 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Ding", "Henghui", ""], ["Jiang", "Xudong", ""], ["Liu", "Ai Qun", ""], ["Thalmann", "Nadia Magnenat", ""], ["Wang", "Gang", ""]]}, {"id": "1909.00182", "submitter": "Aojun Zhou", "authors": "Zhuoran Yu, Aojun Zhou, Yukun Ma, Yudian Li, Xiaohan Zhang, Ping Luo", "title": "Scale Calibrated Training: Improving Generalization of Deep Networks via\n  Scale-Specific Normalization", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard convolutional neural networks(CNNs) require consistent image\nresolutions in both training and testing phase. However, in practice, testing\nwith smaller image sizes is necessary for fast inference. We show that\ntrivially evaluating low-resolution images on networks trained with\nhigh-resolution images results in a catastrophic accuracy drop in standard CNN\narchitectures. We propose a novel training regime called Scale calibrated\nTraining(SCT) which allows networks to learn from various scales of input\nsimultaneously. By taking advantages of SCT, single network can provide decent\naccuracy at test time in response to multiple test scales. In our analysis, we\nsurprisingly find that vanilla batch normalization can lead to sub-optimal\nperformance in SCT. Therefore, a novel normalization scheme called\nScale-Specific Batch Normalization is equipped to SCT in replacement of batch\nnormalization. Experiment results show that SCT improves accuracy of single\nResnet-50 on ImageNet by 1.7% and 11.5% accuracy when testing on image sizes of\n224 and 128 respectively.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 10:01:37 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 15:09:01 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Yu", "Zhuoran", ""], ["Zhou", "Aojun", ""], ["Ma", "Yukun", ""], ["Li", "Yudian", ""], ["Zhang", "Xiaohan", ""], ["Luo", "Ping", ""]]}, {"id": "1909.00186", "submitter": "Xin Yang", "authors": "Xu Wang, Xin Yang, Haoran Dou, Shengli Li, Pheng-Ann Heng, Dong Ni", "title": "Joint Segmentation and Landmark Localization of Fetal Femur in\n  Ultrasound Volumes", "comments": "Accepted by IEEE-EMBS International Conference on Biomedical and\n  Health Informatics (BHI), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric ultrasound has great potentials in promoting prenatal\nexaminations. Automated solutions are highly desired to efficiently and\neffectively analyze the massive volumes. Segmentation and landmark localization\nare two key techniques in making the quantitative evaluation of prenatal\nultrasound volumes available in clinic. However, both tasks are non-trivial\nwhen considering the poor image quality, boundary ambiguity and anatomical\nvariations in volumetric ultrasound. In this paper, we propose an effective\nframework for simultaneous segmentation and landmark localization in prenatal\nultrasound volumes. The proposed framework has two branches where informative\ncues of segmentation and landmark localization can be propagated\nbidirectionally to benefit both tasks. As landmark localization tends to suffer\nfrom false positives, we propose a distance based loss to suppress the noise\nand thus enhance the localization map and in turn the segmentation. Finally, we\nfurther leverage an adversarial module to emphasize the correspondence between\nsegmentation and landmark localization. Extensively validated on a volumetric\nultrasound dataset of fetal femur, our proposed framework proves to be a\npromising solution to facilitate the interpretation of prenatal ultrasound\nvolumes.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 10:37:56 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Wang", "Xu", ""], ["Yang", "Xin", ""], ["Dou", "Haoran", ""], ["Li", "Shengli", ""], ["Heng", "Pheng-Ann", ""], ["Ni", "Dong", ""]]}, {"id": "1909.00206", "submitter": "Yunqiang Li", "authors": "Yunqiang Li, Wenjie Pei, Yufei zha and Jan van Gemert", "title": "Push for Quantization: Deep Fisher Hashing", "comments": "BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current massive datasets demand light-weight access for analysis. Discrete\nhashing methods are thus beneficial because they map high-dimensional data to\ncompact binary codes that are efficient to store and process, while preserving\nsemantic similarity. To optimize powerful deep learning methods for image\nhashing, gradient-based methods are required. Binary codes, however, are\ndiscrete and thus have no continuous derivatives. Relaxing the problem by\nsolving it in a continuous space and then quantizing the solution is not\nguaranteed to yield separable binary codes. The quantization needs to be\nincluded in the optimization. In this paper we push for quantization: We\noptimize maximum class separability in the binary space. We introduce a margin\non distances between dissimilar image pairs as measured in the binary space. In\naddition to pair-wise distances, we draw inspiration from Fisher's Linear\nDiscriminant Analysis (Fisher LDA) to maximize the binary distances between\nclasses and at the same time minimize the binary distance of images within the\nsame class. Experiments on CIFAR-10, NUS-WIDE and ImageNet100 demonstrate\ncompact codes comparing favorably to the current state of the art.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 12:18:43 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Li", "Yunqiang", ""], ["Pei", "Wenjie", ""], ["zha", "Yufei", ""], ["van Gemert", "Jan", ""]]}, {"id": "1909.00211", "submitter": "Vikas Ahuja", "authors": "Vikas Ahuja and Vijay Kumar Neeluru", "title": "Robust BGA Void Detection Using Multi Directional Scan Algorithms", "comments": "arXiv admin note: substantial text overlap with arXiv:1907.04222", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The life time of electronic circuits board are impacted by the voids present\nin soldering balls. The quality inspection of solder balls by detecting and\nmeasuring the void is important to improve the board yield issues in electronic\ncircuits. In general, the inspection is carried out manually, based on 2D or 3D\nX-ray images. For high quality inspection, it is difficult to detect and\nmeasure voids accurately with high repeatability through the manual inspection\nand it is time consuming process. In need of high quality and fast inspection,\nvarious approaches were proposed for void detection. But, lacks in robustness\nin dealing with various challenges like vias, reflections from the plating or\nvias, inconsistent lighting, noise, void-like artefacts, various void shapes,\nlow resolution images and scalability to various devices. Robust BGA void\ndetection becomes quite difficult problem, especially if the image size is very\nsmall (say, around 40x40) and with low contrast between void and the BGA\nbackground (say around 7 intensity levels on a scale of 255). In this work, we\npropose novel approach for void detection based on the multi directional\nscanning. The proposed approach is able to segment the voids for low resolution\nimages and can be easily scaled to various electronic manufacturing products.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 13:09:55 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Ahuja", "Vikas", ""], ["Neeluru", "Vijay Kumar", ""]]}, {"id": "1909.00229", "submitter": "Huan Qi", "authors": "Huan Qi, Sally Collins, J. Alison Noble", "title": "UPI-Net: Semantic Contour Detection in Placental Ultrasound", "comments": "9 pages, 8 figures, accepted at Visual Recognition for Medical Images\n  (VRMI), ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic contour detection is a challenging problem that is often met in\nmedical imaging, of which placental image analysis is a particular example. In\nthis paper, we investigate utero-placental interface (UPI) detection in 2D\nplacental ultrasound images by formulating it as a semantic contour detection\nproblem. As opposed to natural images, placental ultrasound images contain\nspecific anatomical structures thus have unique geometry. We argue it would be\nbeneficial for UPI detectors to incorporate global context modelling in order\nto reduce unwanted false positive UPI predictions. Our approach, namely\nUPI-Net, aims to capture long-range dependencies in placenta geometry through\nlightweight global context modelling and effective multi-scale feature\naggregation. We perform a subject-level 10-fold nested cross-validation on a\nplacental ultrasound database (4,871 images with labelled UPI from 49 scans).\nExperimental results demonstrate that, without introducing considerable\ncomputational overhead, UPI-Net yields the highest performance in terms of\nstandard contour detection metrics, compared to other competitive benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 15:39:12 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 16:27:51 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Qi", "Huan", ""], ["Collins", "Sally", ""], ["Noble", "J. Alison", ""]]}, {"id": "1909.00239", "submitter": "Mingfei Gao", "authors": "Mingfei Gao, Larry S. Davis, Richard Socher, Caiming Xiong", "title": "WSLLN: Weakly Supervised Natural Language Localization Networks", "comments": "accepted by EMNLP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose weakly supervised language localization networks (WSLLN) to detect\nevents in long, untrimmed videos given language queries. To learn the\ncorrespondence between visual segments and texts, most previous methods require\ntemporal coordinates (start and end times) of events for training, which leads\nto high costs of annotation. WSLLN relieves the annotation burden by training\nwith only video-sentence pairs without accessing to temporal locations of\nevents. With a simple end-to-end structure, WSLLN measures segment-text\nconsistency and conducts segment selection (conditioned on the text)\nsimultaneously. Results from both are merged and optimized as a video-sentence\nmatching problem. Experiments on ActivityNet Captions and DiDeMo demonstrate\nthat WSLLN achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 16:30:28 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Gao", "Mingfei", ""], ["Davis", "Larry S.", ""], ["Socher", "Richard", ""], ["Xiong", "Caiming", ""]]}, {"id": "1909.00240", "submitter": "Muhammad Usman Ghani", "authors": "Muhammad Usman Ghani and W. Clem Karl", "title": "Integrating Data and Image Domain Deep Learning for Limited Angle\n  Tomography using Consensus Equilibrium", "comments": "Accepted for publication in proceedings of IEEE ICCV Workshop on\n  Learning for Computational Imaging (ICCVW-LCI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computed Tomography (CT) is a non-invasive imaging modality with applications\nranging from healthcare to security. It reconstructs cross-sectional images of\nan object using a collection of projection data collected at different angles.\nConventional methods, such as FBP, require that the projection data be\nuniformly acquired over the complete angular range. In some applications, it is\nnot possible to acquire such data. Security is one such domain where\nnon-rotational scanning configurations are being developed which violate the\ncomplete data assumption. Conventional methods produce images from such data\nthat are filled with artifacts. The recent success of deep learning (DL)\nmethods has inspired researchers to post-process these artifact laden images\nusing deep neural networks (DNNs). This approach has seen limited success on\nreal CT problems. Another approach has been to pre-process the incomplete data\nusing DNNs aiming to avoid the creation of artifacts altogether. Due to\nimperfections in the learning process, this approach can still leave\nperceptible residual artifacts. In this work, we aim to combine the power of\ndeep learning in both the data and image domains through a two-step process\nbased on the consensus equilibrium (CE) framework. Specifically, we use\nconditional generative adversarial networks (cGANs) in both the data and the\nimage domain for enhanced performance and efficient computation and combine\nthem through a consensus process. We demonstrate the effectiveness of our\napproach on a real security CT dataset for a challenging 90 degree\nlimited-angle problem. The same framework can be applied to other limited data\nproblems arising in applications such as electron microscopy, non-destructive\nevaluation, and medical imaging.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 16:46:12 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Ghani", "Muhammad Usman", ""], ["Karl", "W. Clem", ""]]}, {"id": "1909.00270", "submitter": "Shima Rafiei", "authors": "Safiyeh Rezaei, Ali Emami, Hamidreza Zarrabi, Shima Rafiei, Kayvan\n  Najarian, Nader Karimi, Shadrokh Samavi, S.M.Reza Soroushmehr", "title": "Gland Segmentation in Histopathology Images Using Deep Networks and\n  Handcrafted Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histopathology images contain essential information for medical diagnosis and\nprognosis of cancerous disease. Segmentation of glands in histopathology images\nis a primary step for analysis and diagnosis of an unhealthy patient. Due to\nthe widespread application and the great success of deep neural networks in\nintelligent medical diagnosis and histopathology, we propose a modified version\nof LinkNet for gland segmentation and recognition of malignant cases. We show\nthat using specific handcrafted features such as invariant local binary pattern\ndrastically improves the system performance. The experimental results\ndemonstrate the competency of the proposed system against state-of-the-art\nmethods. We achieved the best results in testing on section B images of the\nWarwick-QU dataset and obtained comparable results on section A images.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 19:42:12 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Rezaei", "Safiyeh", ""], ["Emami", "Ali", ""], ["Zarrabi", "Hamidreza", ""], ["Rafiei", "Shima", ""], ["Najarian", "Kayvan", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""], ["Soroushmehr", "S. M. Reza", ""]]}, {"id": "1909.00273", "submitter": "Shima Rafiei", "authors": "Zahra Sobhaninia, Shima Rafiei, Ali Emami, Nader Karimi, Kayvan\n  Najarian, Shadrokh Samavi, S.M.Reza Soroushmehr", "title": "Fetal Ultrasound Image Segmentation for Measuring Biometric Parameters\n  Using Multi-Task Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound imaging is a standard examination during pregnancy that can be\nused for measuring specific biometric parameters towards prenatal diagnosis and\nestimating gestational age. Fetal head circumference (HC) is one of the\nsignificant factors to determine the fetus growth and health. In this paper, a\nmulti-task deep convolutional neural network is proposed for automatic\nsegmentation and estimation of HC ellipse by minimizing a compound cost\nfunction composed of segmentation dice score and MSE of ellipse parameters.\nExperimental results on fetus ultrasound dataset in different trimesters of\npregnancy show that the segmentation results and the extracted HC match well\nwith the radiologist annotations. The obtained dice scores of the fetal head\nsegmentation and the accuracy of HC evaluations are comparable to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 19:43:31 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Sobhaninia", "Zahra", ""], ["Rafiei", "Shima", ""], ["Emami", "Ali", ""], ["Karimi", "Nader", ""], ["Najarian", "Kayvan", ""], ["Samavi", "Shadrokh", ""], ["Soroushmehr", "S. M. Reza", ""]]}, {"id": "1909.00276", "submitter": "Robert Holland", "authors": "Robert Holland, Uday Patel, Phillip Lung, Elisa Chotzoglou and\n  Bernhard Kainz", "title": "Automatic Detection of Bowel Disease with Residual Networks", "comments": "Accepted to PRIME-MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crohn's disease, one of two inflammatory bowel diseases (IBD), affects\n200,000 people in the UK alone, or roughly one in every 500. We explore the\nfeasibility of deep learning algorithms for identification of terminal ileal\nCrohn's disease in Magnetic Resonance Enterography images on a small dataset.\nWe show that they provide comparable performance to the current clinical\nstandard, the MaRIA score, while requiring only a fraction of the preparation\nand inference time. Moreover, bowels are subject to high variation between\nindividuals due to the complex and free-moving anatomy. Thus we also explore\nthe effect of difficulty of the classification at hand on performance. Finally,\nwe employ soft attention mechanisms to amplify salient local features and add\ninterpretability.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 19:51:23 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Holland", "Robert", ""], ["Patel", "Uday", ""], ["Lung", "Phillip", ""], ["Chotzoglou", "Elisa", ""], ["Kainz", "Bernhard", ""]]}, {"id": "1909.00292", "submitter": "Murari Mandal", "authors": "Murari Mandal, Manal Shah, Prashant Meena, Santosh Kumar Vipparthi", "title": "SSSDET: Simple Short and Shallow Network for Resource Efficient Vehicle\n  Detection in Aerial Scenes", "comments": "International Conference on Image Processing (ICIP) 2019, Taipei,\n  Taiwan", "journal-ref": null, "doi": "10.1109/ICIP.2019.8803262", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of small-sized targets is of paramount importance in many aerial\nvision-based applications. The commonly deployed low cost unmanned aerial\nvehicles (UAVs) for aerial scene analysis are highly resource constrained in\nnature. In this paper we propose a simple short and shallow network (SSSDet) to\nrobustly detect and classify small-sized vehicles in aerial scenes. The\nproposed SSSDet is up to 4x faster, requires 4.4x less FLOPs, has 30x less\nparameters, requires 31x less memory space and provides better accuracy in\ncomparison to existing state-of-the-art detectors. Thus, it is more suitable\nfor hardware implementation in real-time applications. We also created a new\nairborne image dataset (ABD) by annotating 1396 new objects in 79 aerial images\nfor our experiments. The effectiveness of the proposed method is validated on\nthe existing VEDAI, DLR-3K, DOTA and Combined dataset. The SSSDet outperforms\nstate-of-the-art detectors in term of accuracy, speed, compute and memory\nefficiency.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 22:00:07 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Mandal", "Murari", ""], ["Shah", "Manal", ""], ["Meena", "Prashant", ""], ["Vipparthi", "Santosh Kumar", ""]]}, {"id": "1909.00295", "submitter": "Bryan (Ning) Xia", "authors": "Bryan (Ning) Xia, Yuan Gong, Yizhe Zhang, Christian Poellabauer", "title": "Second-order Non-local Attention Networks for Person Re-identification", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent efforts have shown promising results for person re-identification by\ndesigning part-based architectures to allow a neural network to learn\ndiscriminative representations from semantically coherent parts. Some efforts\nuse soft attention to reallocate distant outliers to their most similar parts,\nwhile others adjust part granularity to incorporate more distant positions for\nlearning the relationships. Others seek to generalize part-based methods by\nintroducing a dropout mechanism on consecutive regions of the feature map to\nenhance distant region relationships. However, only few prior efforts model the\ndistant or non-local positions of the feature map directly for the person re-ID\ntask. In this paper, we propose a novel attention mechanism to directly model\nlong-range relationships via second-order feature statistics. When combined\nwith a generalized DropBlock module, our method performs equally to or better\nthan state-of-the-art results for mainstream person re-identification datasets,\nincluding Market1501, CUHK03, and DukeMTMC-reID.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 22:50:42 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Bryan", "", "", "Ning"], ["Xia", "", ""], ["Gong", "Yuan", ""], ["Zhang", "Yizhe", ""], ["Poellabauer", "Christian", ""]]}, {"id": "1909.00300", "submitter": "Sahar Abdelnabi", "authors": "Sahar Abdelnabi and Katharina Krombholz and Mario Fritz", "title": "VisualPhishNet: Zero-Day Phishing Website Detection by Visual Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phishing websites are still a major threat in today's Internet ecosystem.\nDespite numerous previous efforts, similarity-based detection methods do not\noffer sufficient protection for the trusted websites - in particular against\nunseen phishing pages. This paper contributes VisualPhishNet, a new\nsimilarity-based phishing detection framework, based on a triplet Convolutional\nNeural Network (CNN). VisualPhishNet learns profiles for websites in order to\ndetect phishing websites by a similarity metric that can generalize to pages\nwith new visual appearances. We furthermore present VisualPhish, the largest\ndataset to date that facilitates visual phishing detection in an ecologically\nvalid manner. We show that our method outperforms previous visual similarity\nphishing detection approaches by a large margin while being robust against a\nrange of evasion attacks.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 00:55:10 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 20:39:38 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 16:22:37 GMT"}, {"version": "v4", "created": "Sun, 5 Jul 2020 15:24:44 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Abdelnabi", "Sahar", ""], ["Krombholz", "Katharina", ""], ["Fritz", "Mario", ""]]}, {"id": "1909.00302", "submitter": "Akshay Gadi Patil", "authors": "Akshay Gadi Patil, Omri Ben-Eliezer, Or Perel, Hadar Averbuch-Elor", "title": "READ: Recursive Autoencoders for Document Layout Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layout is a fundamental component of any graphic design. Creating large\nvarieties of plausible document layouts can be a tedious task, requiring\nnumerous constraints to be satisfied, including local ones relating different\nsemantic elements and global constraints on the general appearance and spacing.\nIn this paper, we present a novel framework, coined READ, for REcursive\nAutoencoders for Document layout generation, to generate plausible 2D layouts\nof documents in large quantities and varieties. First, we devise an exploratory\nrecursive method to extract a structural decomposition of a single document.\nLeveraging a dataset of documents annotated with labeled bounding boxes, our\nrecursive neural network learns to map the structural representation, given in\nthe form of a simple hierarchy, to a compact code, the space of which is\napproximated by a Gaussian distribution. Novel hierarchies can be sampled from\nthis space, obtaining new document layouts. Moreover, we introduce a\ncombinatorial metric to measure structural similarity among document layouts.\nWe deploy it to show that our method is able to generate highly variable and\nrealistic layouts. We further demonstrate the utility of our generated layouts\nin the context of standard detection tasks on documents, showing that detection\nperformance improves when the training data is augmented with generated\ndocuments whose layouts are produced by READ.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 01:58:31 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 17:43:41 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2019 22:38:52 GMT"}, {"version": "v4", "created": "Thu, 16 Apr 2020 23:26:17 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Patil", "Akshay Gadi", ""], ["Ben-Eliezer", "Omri", ""], ["Perel", "Or", ""], ["Averbuch-Elor", "Hadar", ""]]}, {"id": "1909.00318", "submitter": "Guizhong Liu", "authors": "Weiqiang Li, Jiatong Mu, Guizhong Liu", "title": "Multiple Object Tracking with Motion and Appearance Cues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to better video quality and higher frame rate, the performance of\nmultiple object tracking issues has been greatly improved in recent years.\nHowever, in real application scenarios, camera motion and noisy per frame\ndetection results degrade the performance of trackers significantly. High-speed\nand high-quality multiple object trackers are still in urgent demand. In this\npaper, we propose a new multiple object tracker following the popular\ntracking-by-detection scheme. We tackle the camera motion problem with an\noptical flow network and utilize an auxiliary tracker to deal with the missing\ndetection problem. Besides, we use both the appearance and motion information\nto improve the matching quality. The experimental results on the VisDrone-MOT\ndataset show that our approach can improve the performance of multiple object\ntracking significantly while achieving a high efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 03:54:40 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Li", "Weiqiang", ""], ["Mu", "Jiatong", ""], ["Liu", "Guizhong", ""]]}, {"id": "1909.00319", "submitter": "Guizhong Liu", "authors": "Han Wu, Xueyuan Yang, Yong Yang, Guizhong Liu", "title": "Flow Guided Short-term Trackers with Cascade Detection for Long-term\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object tracking has been studied for decades, but most of the existing works\nare focused on the short-term tracking. For a long sequence, the object is\noften fully occluded or out of view for a long time, and existing short-term\nobject tracking algorithms often lose the target, and it is difficult to\nre-catch the target even if it reappears again. In this paper a novel long-term\nobject tracking algorithm flow_MDNet_RPN is proposed, in which a tracking\nresult judgement module and a detection module are added to the short-term\nobject tracking algorithm. Experiments show that the proposed long-term\ntracking algorithm is effective to the problem of target disappearance.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 04:06:42 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Wu", "Han", ""], ["Yang", "Xueyuan", ""], ["Yang", "Yong", ""], ["Liu", "Guizhong", ""]]}, {"id": "1909.00321", "submitter": "Junyi Pan", "authors": "Junyi Pan, Xiaoguang Han, Weikai Chen, Jiapeng Tang, Kui Jia", "title": "Deep Mesh Reconstruction from Single RGB Images via Topology\n  Modification Networks", "comments": "10 pages, 11 figures, to be presented at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing the 3D mesh of a general object from a single image is now\npossible thanks to the latest advances of deep learning technologies. However,\ndue to the nontrivial difficulty of generating a feasible mesh structure, the\nstate-of-the-art approaches often simplify the problem by learning the\ndisplacements of a template mesh that deforms it to the target surface. Though\nreconstructing a 3D shape with complex topology can be achieved by deforming\nmultiple mesh patches, it remains difficult to stitch the results to ensure a\nhigh meshing quality. In this paper, we present an end-to-end single-view mesh\nreconstruction framework that is able to generate high-quality meshes with\ncomplex topologies from a single genus-0 template mesh. The key to our approach\nis a novel progressive shaping framework that alternates between mesh\ndeformation and topology modification. While a deformation network predicts the\nper-vertex translations that reduce the gap between the reconstructed mesh and\nthe ground truth, a novel topology modification network is employed to prune\nthe error-prone faces, enabling the evolution of topology. By iterating over\nthe two procedures, one can progressively modify the mesh topology while\nachieving higher reconstruction accuracy. Moreover, a boundary refinement\nnetwork is designed to refine the boundary conditions to further improve the\nvisual quality of the reconstructed mesh. Extensive experiments demonstrate\nthat our approach outperforms the current state-of-the-art methods both\nqualitatively and quantitatively, especially for the shapes with complex\ntopologies.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 04:17:41 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Pan", "Junyi", ""], ["Han", "Xiaoguang", ""], ["Chen", "Weikai", ""], ["Tang", "Jiapeng", ""], ["Jia", "Kui", ""]]}, {"id": "1909.00331", "submitter": "Sripad Krishna Devalla", "authors": "Tan Hung Pham, Sripad Krishna Devalla, Aloysius Ang, Soh Zhi Da,\n  Alexandre H. Thiery, Craig Boote, Ching-Yu Cheng, Victor Koh, and Michael J.\n  A. Girard", "title": "Deep Learning Algorithms to Isolate and Quantify the Structures of the\n  Anterior Segment in Optical Coherence Tomography Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate isolation and quantification of intraocular dimensions in the\nanterior segment (AS) of the eye using optical coherence tomography (OCT)\nimages is important in the diagnosis and treatment of many eye diseases,\nespecially angle closure glaucoma. In this study, we developed a deep\nconvolutional neural network (DCNN) for the localization of the scleral spur,\nand the segmentation of anterior segment structures (iris, corneo-sclera shell,\nanterior chamber). With limited training data, the DCNN was able to detect the\nscleral spur on unseen ASOCT images as accurately as an experienced\nophthalmologist; and simultaneously isolated the anterior segment structures\nwith a Dice coefficient of 95.7%. We then automatically extracted eight\nclinically relevant ASOCT parameters and proposed an automated quality check\nprocess that asserts the reliability of these parameters. When combined with an\nOCT machine capable of imaging multiple radial sections, the algorithms can\nprovide a more complete objective assessment. This is an essential step toward\nproviding a robust automated framework for reliable quantification of ASOCT\nscans, for applications in the diagnosis and management of angle closure\nglaucoma.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 06:27:05 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Pham", "Tan Hung", ""], ["Devalla", "Sripad Krishna", ""], ["Ang", "Aloysius", ""], ["Da", "Soh Zhi", ""], ["Thiery", "Alexandre H.", ""], ["Boote", "Craig", ""], ["Cheng", "Ching-Yu", ""], ["Koh", "Victor", ""], ["Girard", "Michael J. A.", ""]]}, {"id": "1909.00350", "submitter": "Alessandro Betti", "authors": "Alessandro Betti, Marco Gori, Stefano Melacci", "title": "Learning Visual Features Under Motion Invariance", "comments": "73 pages, 9 figures. arXiv admin note: substantial text overlap with\n  arXiv:1801.07110", "journal-ref": "Neural Networks 126 (2020), pp. 275--299", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are continuously exposed to a stream of visual data with a natural\ntemporal structure. However, most successful computer vision algorithms work at\nimage level, completely discarding the precious information carried by motion.\nIn this paper, we claim that processing visual streams naturally leads to\nformulate the motion invariance principle, which enables the construction of a\nnew theory of learning that originates from variational principles, just like\nin physics. Such principled approach is well suited for a discussion on a\nnumber of interesting questions that arise in vision, and it offers a\nwell-posed computational scheme for the discovery of convolutional filters over\nthe retina. Differently from traditional convolutional networks, which need\nmassive supervision, the proposed theory offers a truly new scenario for the\nunsupervised processing of video signals, where features are extracted in a\nmulti-layer architecture with motion invariance. While the theory enables the\nimplementation of novel computer vision systems, it also sheds light on the\nrole of information-based principles to drive possible biological solutions.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 08:20:44 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 14:02:59 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2020 10:00:44 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Betti", "Alessandro", ""], ["Gori", "Marco", ""], ["Melacci", "Stefano", ""]]}, {"id": "1909.00384", "submitter": "Hyunjung Kwak", "authors": "Gloria Hyunjung Kwak and Pan Hui", "title": "DeepHealth: Review and challenges of artificial intelligence in health\n  informatics", "comments": "42 pages, 19 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence has provided us with an exploration of a whole new\nresearch era. As more data and better computational power become available, the\napproach is being implemented in various fields. The demand for it in health\ninformatics is also increasing, and we can expect to see the potential benefits\nof its applications in healthcare. It can help clinicians diagnose disease,\nidentify drug effects for each patient, understand the relationship between\ngenotypes and phenotypes, explore new phenotypes or treatment recommendations,\nand predict infectious disease outbreaks with high accuracy. In contrast to\ntraditional models, recent artificial intelligence approaches do not require\ndomain-specific data pre-processing, and it is expected that it will ultimately\nchange life in the future. Despite its notable advantages, there are some key\nchallenges on data (high dimensionality, heterogeneity, time dependency,\nsparsity, irregularity, lack of label, bias) and model (reliability,\ninterpretability, feasibility, security, scalability) for practical use. This\narticle presents a comprehensive review of research applying artificial\nintelligence in health informatics, focusing on the last seven years in the\nfields of medical imaging, electronic health records, genomics, sensing, and\nonline communication health, as well as challenges and promising directions for\nfuture research. We highlight ongoing popular approaches' research and identify\nseveral challenges in building models.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 11:54:38 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 05:54:41 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Kwak", "Gloria Hyunjung", ""], ["Hui", "Pan", ""]]}, {"id": "1909.00390", "submitter": "Philip May", "authors": "Philip May", "title": "Improved Image Augmentation for Convolutional Neural Networks by Copyout\n  and CopyPairing", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image augmentation is a widely used technique to improve the performance of\nconvolutional neural networks (CNNs). In common image shifting, cropping,\nflipping, shearing and rotating are used for augmentation. But there are more\nadvanced techniques like Cutout and SamplePairing. In this work we present two\nimprovements of the state-of-the-art Cutout and SamplePairing techniques. Our\nnew method called Copyout takes a square patch of another random training image\nand copies it onto a random location of each image used for training. The\nsecond technique we discovered is called CopyPairing. It combines Copyout and\nSamplePairing for further augmentation and even better performance. We apply\ndifferent experiments with these augmentation techniques on the CIFAR-10\ndataset to evaluate and compare them under different configurations. In our\nexperiments we show that Copyout reduces the test error rate by 8.18% compared\nwith Cutout and 4.27% compared with SamplePairing. CopyPairing reduces the test\nerror rate by 11.97% compared with Cutout and 8.21% compared with\nSamplePairing. Copyout and CopyPairing implementations are available at\nhttps://github.com/t-systems-on-site-services-gmbh/coocop.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 12:59:09 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 05:26:07 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["May", "Philip", ""]]}, {"id": "1909.00392", "submitter": "Tae Ha Park", "authors": "Tae Ha Park, Sumant Sharma, Simone D'Amico", "title": "Towards Robust Learning-Based Pose Estimation of Noncooperative\n  Spacecraft", "comments": "Presented at 2019 AAS/AIAA Astrodynamics Specialist Conference", "journal-ref": null, "doi": null, "report-no": "AAS 19-840", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel Convolutional Neural Network (CNN) architecture\nand a training procedure to enable robust and accurate pose estimation of a\nnoncooperative spacecraft. First, a new CNN architecture is introduced that has\nscored a fourth place in the recent Pose Estimation Challenge hosted by\nStanford's Space Rendezvous Laboratory (SLAB) and the Advanced Concepts Team\n(ACT) of the European Space Agency (ESA). The proposed architecture first\ndetects the object by regressing a 2D bounding box, then a separate network\nregresses the 2D locations of the known surface keypoints from an image of the\ntarget cropped around the detected Region-of-Interest (RoI). In a single-image\npose estimation problem, the extracted 2D keypoints can be used in conjunction\nwith corresponding 3D model coordinates to compute relative pose via the\nPerspective-n-Point (PnP) problem. These keypoint locations have known\ncorrespondences to those in the 3D model, since the CNN is trained to predict\nthe corners in a pre-defined order, allowing for bypassing the computationally\nexpensive feature matching processes. This work also introduces and explores\nthe texture randomization to train a CNN for spaceborne applications.\nSpecifically, Neural Style Transfer (NST) is applied to randomize the texture\nof the spacecraft in synthetically rendered images. It is shown that using the\ntexture-randomized images of spacecraft for training improves the network's\nperformance on spaceborne images without exposure to them during training. It\nis also shown that when using the texture-randomized spacecraft images during\ntraining, regressing 3D bounding box corners leads to better performance on\nspaceborne images than regressing surface keypoints, as NST inevitably distorts\nthe spacecraft's geometric features to which the surface keypoints have closer\nrelation.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 13:22:19 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Park", "Tae Ha", ""], ["Sharma", "Sumant", ""], ["D'Amico", "Simone", ""]]}, {"id": "1909.00475", "submitter": "Guha Balakrishnan", "authors": "Guha Balakrishnan, Adrian V. Dalca, Amy Zhao, John V. Guttag, Fredo\n  Durand, William T. Freeman", "title": "Visual Deprojection: Probabilistic Recovery of Collapsed Dimensions", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce visual deprojection: the task of recovering an image or video\nthat has been collapsed along a dimension. Projections arise in various\ncontexts, such as long-exposure photography, where a dynamic scene is collapsed\nin time to produce a motion-blurred image, and corner cameras, where reflected\nlight from a scene is collapsed along a spatial dimension because of an edge\noccluder to yield a 1D video. Deprojection is ill-posed-- often there are many\nplausible solutions for a given input. We first propose a probabilistic model\ncapturing the ambiguity of the task. We then present a variational inference\nstrategy using convolutional neural networks as functional approximators.\nSampling from the inference network at test time yields plausible candidates\nfrom the distribution of original signals that are consistent with a given\ninput projection. We evaluate the method on several datasets for both spatial\nand temporal deprojection tasks. We first demonstrate the method can recover\nhuman gait videos and face images from spatial projections, and then show that\nit can recover videos of moving digits from dramatically motion-blurred images\nobtained via temporal projection.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 21:34:28 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Balakrishnan", "Guha", ""], ["Dalca", "Adrian V.", ""], ["Zhao", "Amy", ""], ["Guttag", "John V.", ""], ["Durand", "Fredo", ""], ["Freeman", "William T.", ""]]}, {"id": "1909.00482", "submitter": "Mario Amrehn", "authors": "Mario Amrehn, Stefan Steidl, Reinier Kortekaas, Maddalena Strumia,\n  Markus Weingarten, Markus Kowarschik, Andreas Maier", "title": "A Semi-Automated Usability Evaluation Framework for Interactive Image\n  Segmentation Systems", "comments": "Accepted as research article at the International Journal of\n  Biomedical Imaging, Hindawi", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For complex segmentation tasks, the achievable accuracy of fully automated\nsystems is inherently limited. Specifically, when a precise segmentation result\nis desired for a small amount of given data sets, semi-automatic methods\nexhibit a clear benefit for the user. The optimization of human computer\ninteraction (HCI) is an essential part of interactive image segmentation.\nNevertheless, publications introducing novel interactive segmentation systems\n(ISS) often lack an objective comparison of HCI aspects. It is demonstrated,\nthat even when the underlying segmentation algorithm is the same throughout\ninteractive prototypes, their user experience may vary substantially. As a\nresult, users prefer simple interfaces as well as a considerable degree of\nfreedom to control each iterative step of the segmentation. In this article, an\nobjective method for the comparison of ISS is proposed, based on extensive user\nstudies. A summative qualitative content analysis is conducted via abstraction\nof visual and verbal feedback given by the participants. A direct assessment of\nthe segmentation system is executed by the users via the system usability scale\n(SUS) and AttrakDiff-2 questionnaires. Furthermore, an approximation of the\nfindings regarding usability aspects in those studies is introduced, conducted\nsolely from the system-measurable user actions during their usage of\ninteractive segmentation prototypes. The prediction of all questionnaire\nresults has an average relative error of 8.9%, which is close to the expected\nprecision of the questionnaire results themselves. This automated evaluation\nscheme may significantly reduce the resources necessary to investigate each\nvariation of a prototype's user interface (UI) features and segmentation\nmethodologies.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 22:33:06 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Amrehn", "Mario", ""], ["Steidl", "Stefan", ""], ["Kortekaas", "Reinier", ""], ["Strumia", "Maddalena", ""], ["Weingarten", "Markus", ""], ["Kowarschik", "Markus", ""], ["Maier", "Andreas", ""]]}, {"id": "1909.00523", "submitter": "Zhao Zhang", "authors": "Zhao Zhang, Yan Zhang, Sheng Li, Guangcan Liu, Dan Zeng, Shuicheng Yan\n  and Meng Wang", "title": "Flexible Auto-weighted Local-coordinate Concept Factorization: A Robust\n  Framework for Unsupervised Clustering", "comments": "Accepted by IEEE Transactions on Knowledge and Data Engineering (IEEE\n  TKDE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept Factorization (CF) and its variants may produce inaccurate\nrepresentation and clustering results due to the sensitivity to noise, hard\nconstraint on the reconstruction error and pre-obtained approximate\nsimilarities. To improve the representation ability, a novel unsupervised\nRobust Flexible Auto-weighted Local-coordinate Concept Factorization (RFA-LCF)\nframework is proposed for clustering high-dimensional data. Specifically,\nRFA-LCF integrates the robust flexible CF by clean data space recovery, robust\nsparse local-coordinate coding and adaptive weighting into a unified model.\nRFA-LCF improves the representations by enhancing the robustness of CF to noise\nand errors, providing a flexible constraint on the reconstruction error and\noptimizing the locality jointly. For robust learning, RFA-LCF clearly learns a\nsparse projection to recover the underlying clean data space, and then the\nflexible CF is performed in the projected feature space. RFA-LCF also uses a\nL2,1-norm based flexible residue to encode the mismatch between the recovered\ndata and its reconstruction, and uses the robust sparse local-coordinate coding\nto represent data using a few nearby basis concepts. For auto-weighting,\nRFA-LCF jointly preserves the manifold structures in the basis concept space\nand new coordinate space in an adaptive manner by minimizing the reconstruction\nerrors on clean data, anchor points and coordinates. By updating the\nlocal-coordinate preserving data, basis concepts and new coordinates\nalternately, the representation abilities can be potentially improved.\nExtensive results on public databases show that RFA-LCF delivers the\nstate-of-the-art clustering results compared with other related methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 03:16:01 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zhang", "Zhao", ""], ["Zhang", "Yan", ""], ["Li", "Sheng", ""], ["Liu", "Guangcan", ""], ["Zeng", "Dan", ""], ["Yan", "Shuicheng", ""], ["Wang", "Meng", ""]]}, {"id": "1909.00532", "submitter": "Yuanyou Xu", "authors": "Yuanyou Xu, Kaiwei Wang, Kailun Yang, Dongming Sun and Jia Fu", "title": "Semantic Segmentation of Panoramic Images Using a Synthetic Dataset", "comments": "15 pages, 12 figures, SPIE Security + Defence International Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoramic images have advantages in information capacity and scene stability\ndue to their large field of view (FoV). In this paper, we propose a method to\nsynthesize a new dataset of panoramic image. We managed to stitch the images\ntaken from different directions into panoramic images, together with their\nlabeled images, to yield the panoramic semantic segmentation dataset\ndenominated as SYNTHIA-PANO. For the purpose of finding out the effect of using\npanoramic images as training dataset, we designed and performed a comprehensive\nset of experiments. Experimental results show that using panoramic images as\ntraining data is beneficial to the segmentation result. In addition, it has\nbeen shown that by using panoramic images with a 180 degree FoV as training\ndata the model has better performance. Furthermore, the model trained with\npanoramic images also has a better capacity to resist the image distortion.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 04:10:34 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Xu", "Yuanyou", ""], ["Wang", "Kaiwei", ""], ["Yang", "Kailun", ""], ["Sun", "Dongming", ""], ["Fu", "Jia", ""]]}, {"id": "1909.00548", "submitter": "Woong Bae", "authors": "Woong Bae, Seungho Lee, Yeha Lee, Beomhee Park, Minki Chung, Kyu-Hwan\n  Jung", "title": "Resource Optimized Neural Architecture Search for 3D Medical Image\n  Segmentation", "comments": "MICCAI(International Conference on Medical Image Computing and\n  Computer Assisted Intervention) 2019 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS), a framework which automates the task of\ndesigning neural networks, has recently been actively studied in the field of\ndeep learning. However, there are only a few NAS methods suitable for 3D\nmedical image segmentation. Medical 3D images are generally very large; thus it\nis difficult to apply previous NAS methods due to their GPU computational\nburden and long training time. We propose the resource-optimized neural\narchitecture search method which can be applied to 3D medical segmentation\ntasks in a short training time (1.39 days for 1GB dataset) using a small amount\nof computation power (one RTX 2080Ti, 10.8GB GPU memory). Excellent performance\ncan also be achieved without retraining(fine-tuning) which is essential in most\nNAS methods. These advantages can be achieved by using a reinforcement\nlearning-based controller with parameter sharing and focusing on the optimal\nsearch space configuration of macro search rather than micro search. Our\nexperiments demonstrate that the proposed NAS method outperforms manually\ndesigned networks with state-of-the-art performance in 3D medical image\nsegmentation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 05:08:25 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Bae", "Woong", ""], ["Lee", "Seungho", ""], ["Lee", "Yeha", ""], ["Park", "Beomhee", ""], ["Chung", "Minki", ""], ["Jung", "Kyu-Hwan", ""]]}, {"id": "1909.00589", "submitter": "Jaehoon Choi", "authors": "Jaehoon Choi, Taekyung Kim, Changick Kim", "title": "Self-Ensembling with GAN-based Data Augmentation for Domain Adaptation\n  in Semantic Segmentation", "comments": "Accepted to International Conference on Computer Vision (ICCV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning-based semantic segmentation methods have an intrinsic\nlimitation that training a model requires a large amount of data with\npixel-level annotations. To address this challenging issue, many researchers\ngive attention to unsupervised domain adaptation for semantic segmentation.\nUnsupervised domain adaptation seeks to adapt the model trained on the source\ndomain to the target domain. In this paper, we introduce a self-ensembling\ntechnique, one of the successful methods for domain adaptation in\nclassification. However, applying self-ensembling to semantic segmentation is\nvery difficult because heavily-tuned manual data augmentation used in\nself-ensembling is not useful to reduce the large domain gap in the semantic\nsegmentation. To overcome this limitation, we propose a novel framework\nconsisting of two components, which are complementary to each other. First, we\npresent a data augmentation method based on Generative Adversarial Networks\n(GANs), which is computationally efficient and effective to facilitate domain\nalignment. Given those augmented images, we apply self-ensembling to enhance\nthe performance of the segmentation network on the target domain. The proposed\nmethod outperforms state-of-the-art semantic segmentation methods on\nunsupervised domain adaptation benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 08:15:16 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Choi", "Jaehoon", ""], ["Kim", "Taekyung", ""], ["Kim", "Changick", ""]]}, {"id": "1909.00597", "submitter": "Seunghyeon Kim", "authors": "Seunghyeon Kim, Jaehoon Choi, Taekyung Kim, Changick Kim", "title": "Self-Training and Adversarial Background Regularization for Unsupervised\n  Domain Adaptive One-Stage Object Detection", "comments": "ICCV 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning-based object detectors have shown remarkable improvements.\nHowever, supervised learning-based methods perform poorly when the train data\nand the test data have different distributions. To address the issue, domain\nadaptation transfers knowledge from the label-sufficient domain (source domain)\nto the label-scarce domain (target domain). Self-training is one of the\npowerful ways to achieve domain adaptation since it helps class-wise domain\nadaptation. Unfortunately, a naive approach that utilizes pseudo-labels as\nground-truth degenerates the performance due to incorrect pseudo-labels. In\nthis paper, we introduce a weak self-training (WST) method and adversarial\nbackground score regularization (BSR) for domain adaptive one-stage object\ndetection. WST diminishes the adverse effects of inaccurate pseudo-labels to\nstabilize the learning procedure. BSR helps the network extract discriminative\nfeatures for target backgrounds to reduce the domain shift. Two components are\ncomplementary to each other as BSR enhances discrimination between foregrounds\nand backgrounds, whereas WST strengthen class-wise discrimination. Experimental\nresults show that our approach effectively improves the performance of the\none-stage object detection in unsupervised domain adaptation setting.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 08:37:27 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Kim", "Seunghyeon", ""], ["Choi", "Jaehoon", ""], ["Kim", "Taekyung", ""], ["Kim", "Changick", ""]]}, {"id": "1909.00617", "submitter": "Walid Abdullah Al", "authors": "Walid Abdullah Al, Il Dong Yun, Kyong Joon Lee", "title": "Reinforcement Learning-based Automatic Diagnosis of Acute Appendicitis\n  in Abdominal CT", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acute appendicitis characterized by a painful inflammation of the vermiform\nappendix is one of the most common surgical emergencies. Localizing the\nappendix is challenging due to its unclear anatomy amidst the complex\ncolon-structure as observed in the conventional CT views, resulting in a\ntime-consuming diagnosis. End-to-end learning of a convolutional neural network\n(CNN) is also not likely to be useful because of the negligible size of the\nappendix compared with the abdominal CT volume. With no prior computational\napproaches to the best of our knowledge, we propose the first computerized\nautomation for acute appendicitis diagnosis. In our approach, we utilize a\nreinforcement learning agent deployed in the lower abdominal region to obtain\nthe appendix location first to reduce the search space for diagnosis. Then, we\nobtain the classification scores (i.e., the likelihood of acute appendicitis)\nfor the local neighborhood around the localized position, using a CNN trained\nonly on a small appendix patch per volume. From the spatial representation of\nthe resultant scores, we finally define a region of low-entropy (RLE) to choose\nthe optimal diagnosis score, which helps improve the classification accuracy\nshowing robustness even under high appendix localization error cases. In our\nexperiment with 319 abdominal CT volumes, the proposed RLE-based decision with\nprior localization showed significant improvement over the standard CNN-based\ndiagnosis approaches.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 09:19:33 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Al", "Walid Abdullah", ""], ["Yun", "Il Dong", ""], ["Lee", "Kyong Joon", ""]]}, {"id": "1909.00632", "submitter": "Yu Liu", "authors": "Yu Liu, Guanglu Song, Manyuan Zhang, Jihao Liu, Yucong Zhou, Junjie\n  Yan", "title": "Towards Flops-constrained Face Recognition", "comments": "ICCV2019 LFR workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale face recognition is challenging especially when the computational\nbudget is limited. Given a \\textit{flops} upper bound, the key is to find the\noptimal neural network architecture and optimization method. In this article,\nwe briefly introduce the solutions of team 'trojans' for the ICCV19 -\nLightweight Face Recognition Challenge~\\cite{lfr}. The challenge requires each\nsubmission to be one single model with the computational budget no higher than\n30 GFlops. We introduce a searched network architecture `Efficient PolyFace'\nbased on the Flops constraint, a novel loss function `ArcNegFace', a novel\nframe aggregation method `QAN++', together with a bag of useful tricks in our\nimplementation (augmentations, regular face, label smoothing, anchor\nfinetuning, etc.). Our basic model, `Efficient PolyFace', takes 28.25 Gflops\nfor the `deepglint-large' image-based track, and the `PolyFace+QAN++' solution\ntakes 24.12 Gflops for the `iQiyi-large' video-based track. These two solutions\nachieve 94.198\\% @ 1e-8 and 72.981\\% @ 1e-4 in the two tracks respectively,\nwhich are the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 09:42:52 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Liu", "Yu", ""], ["Song", "Guanglu", ""], ["Zhang", "Manyuan", ""], ["Liu", "Jihao", ""], ["Zhou", "Yucong", ""], ["Yan", "Junjie", ""]]}, {"id": "1909.00640", "submitter": "Hongdong Zheng", "authors": "Hongdong Zheng, Yalong Bai, Wei Zhang, Tao Mei", "title": "Relationship-Aware Spatial Perception Fusion for Realistic Scene Layout\n  Generation", "comments": "There is a serious mistake", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The significant progress on Generative Adversarial Networks (GANs) have made\nit possible to generate surprisingly realistic images for single object based\non natural language descriptions. However, controlled generation of images for\nmultiple entities with explicit interactions is still difficult to achieve due\nto the scene layout generation heavily suffer from the diversity object scaling\nand spatial locations. In this paper, we proposed a novel framework for\ngenerating realistic image layout from textual scene graphs. In our framework,\na spatial constraint module is designed to fit reasonable scaling and spatial\nlayout of object pairs with considering relationship between them. Moreover, a\ncontextual fusion module is introduced for fusing pair-wise spatial information\nin terms of object dependency in scene graph. By using these two modules, our\nproposed framework tends to generate more commonsense layout which is helpful\nfor realistic image generation. Experimental results including quantitative\nresults, qualitative results and user studies on two different scene graph\ndatasets demonstrate our proposed framework's ability to generate complex and\nlogical layout with multiple objects from scene graph.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 09:55:29 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 16:59:08 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Zheng", "Hongdong", ""], ["Bai", "Yalong", ""], ["Zhang", "Wei", ""], ["Mei", "Tao", ""]]}, {"id": "1909.00676", "submitter": "David Haldimann", "authors": "David Haldimann, Hermann Blum, Roland Siegwart, Cesar Cadena", "title": "This is not what I imagined: Error Detection for Semantic Segmentation\n  through Visual Dissimilarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a remarkable progress in the accuracy of semantic segmentation\ndue to the capabilities of deep learning. Unfortunately, these methods are not\nable to generalize much further than the distribution of their training data\nand fail to handle out-of-distribution classes appropriately. This limits the\napplicability to autonomous or safety critical systems. We propose a novel\nmethod leveraging generative models to detect wrongly segmented or\nout-of-distribution instances. Conditioned on the predicted semantic\nsegmentation, an RGB image is generated. We then learn a dissimilarity metric\nthat compares the generated image with the original input and detects\ninconsistencies introduced by the semantic segmentation. We present test cases\nfor outlier and misclassification detection and evaluate our method\nqualitatively and quantitatively on multiple datasets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 11:38:43 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Haldimann", "David", ""], ["Blum", "Hermann", ""], ["Siegwart", "Roland", ""], ["Cadena", "Cesar", ""]]}, {"id": "1909.00691", "submitter": "Avelino Javer", "authors": "Avelino Javer, Jens Rittscher", "title": "Semantic filtering through deep source separation on microscopy images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By their very nature microscopy images of cells and tissues consist of a\nlimited number of object types or components. In contrast to most natural\nscenes, the composition is known a priori. Decomposing biological images into\nsemantically meaningful objects and layers is the aim of this paper. Building\non recent approaches to image de-noising we present a framework that achieves\nstate-of-the-art segmentation results requiring little or no manual\nannotations. Here, synthetic images generated by adding cell crops are\nsufficient to train the model. Extensive experiments on cellular images, a\nhistology data set, and small animal videos demonstrate that our approach\ngeneralizes to a broad range of experimental settings. As the proposed\nmethodology does not require densely labelled training images and is capable of\nresolving the partially overlapping objects it holds the promise of being of\nuse in a number of different applications.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 12:39:34 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Javer", "Avelino", ""], ["Rittscher", "Jens", ""]]}, {"id": "1909.00700", "submitter": "Zili Liu", "authors": "Zili Liu, Tu Zheng, Guodong Xu, Zheng Yang, Haifeng Liu, Deng Cai", "title": "Training-Time-Friendly Network for Real-Time Object Detection", "comments": "Accepted to AAAI2020 (8 pages, 3 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern object detectors can rarely achieve short training time, fast\ninference speed, and high accuracy at the same time. To strike a balance among\nthem, we propose the Training-Time-Friendly Network (TTFNet). In this work, we\nstart with light-head, single-stage, and anchor-free designs, which enable fast\ninference speed. Then, we focus on shortening training time. We notice that\nencoding more training samples from annotated boxes plays a similar role as\nincreasing batch size, which helps enlarge the learning rate and accelerate the\ntraining process. To this end, we introduce a novel approach using Gaussian\nkernels to encode training samples. Besides, we design the initiative sample\nweights for better information utilization. Experiments on MS COCO show that\nour TTFNet has great advantages in balancing training time, inference speed,\nand accuracy. It has reduced training time by more than seven times compared to\nprevious real-time detectors while maintaining state-of-the-art performances.\nIn addition, our super-fast version of TTFNet-18 and TTFNet-53 can outperform\nSSD300 and YOLOv3 by less than one-tenth of their training time, respectively.\nThe code has been made available at\n\\url{https://github.com/ZJULearning/ttfnet}.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 12:59:18 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 11:01:52 GMT"}, {"version": "v3", "created": "Sun, 24 Nov 2019 08:08:22 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Liu", "Zili", ""], ["Zheng", "Tu", ""], ["Xu", "Guodong", ""], ["Yang", "Zheng", ""], ["Liu", "Haifeng", ""], ["Cai", "Deng", ""]]}, {"id": "1909.00703", "submitter": "Denys Rozumnyi", "authors": "Denys Rozumnyi, Ian Cherabier, Marc Pollefeys, Martin R. Oswald", "title": "Learned Semantic Multi-Sensor Depth Map Fusion", "comments": "11 pages, 7 figures, 2 tables, accepted for the 2nd Workshop on 3D\n  Reconstruction in the Wild (3DRW2019) in conjunction with ICCV2019", "journal-ref": "2019 IEEE/CVF International Conference on Computer Vision Workshop\n  (ICCVW)", "doi": "10.1109/ICCVW.2019.00264", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric depth map fusion based on truncated signed distance functions has\nbecome a standard method and is used in many 3D reconstruction pipelines. In\nthis paper, we are generalizing this classic method in multiple ways: 1)\nSemantics: Semantic information enriches the scene representation and is\nincorporated into the fusion process. 2) Multi-Sensor: Depth information can\noriginate from different sensors or algorithms with very different noise and\noutlier statistics which are considered during data fusion. 3) Scene denoising\nand completion: Sensors can fail to recover depth for certain materials and\nlight conditions, or data is missing due to occlusions. Our method denoises the\ngeometry, closes holes and computes a watertight surface for every semantic\nclass. 4) Learning: We propose a neural network reconstruction method that\nunifies all these properties within a single powerful framework. Our method\nlearns sensor or algorithm properties jointly with semantic depth fusion and\nscene completion and can also be used as an expert system, e.g. to unify the\nstrengths of various photometric stereo algorithms. Our approach is the first\nto unify all these properties. Experimental evaluations on both synthetic and\nreal data sets demonstrate clear improvements.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 13:15:24 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Rozumnyi", "Denys", ""], ["Cherabier", "Ian", ""], ["Pollefeys", "Marc", ""], ["Oswald", "Martin R.", ""]]}, {"id": "1909.00713", "submitter": "Alexander Velizhev", "authors": "Danila Rukhovich, Daniel Mouritzen, Ralf Kaestner, Martin Rufli,\n  Alexander Velizhev", "title": "Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of scale estimation in monocular SLAM by\nestimating absolute distances between camera centers of consecutive image\nframes. These estimates would improve the overall performance of classical (not\ndeep) SLAM systems and allow metric feature locations to be recovered from a\nsingle monocular camera. We propose several network architectures that lead to\nan improvement of scale estimation accuracy over the state of the art. In\naddition, we exploit a possibility to train the neural network only with\nsynthetic data derived from a computer graphics simulator. Our key insight is\nthat, using only synthetic training inputs, we can achieve similar scale\nestimation accuracy as that obtained from real data. This fact indicates that\nfully annotated simulated data is a viable alternative to existing\ndeep-learning-based SLAM systems trained on real (unlabeled) data. Our\nexperiments with unsupervised domain adaptation also show that the difference\nin visual appearance between simulated and real data does not affect scale\nestimation results. Our method operates with low-resolution images (0.03MP),\nwhich makes it practical for real-time SLAM applications with a monocular\ncamera.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 13:35:01 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Rukhovich", "Danila", ""], ["Mouritzen", "Daniel", ""], ["Kaestner", "Ralf", ""], ["Rufli", "Martin", ""], ["Velizhev", "Alexander", ""]]}, {"id": "1909.00733", "submitter": "Benjamin Naujoks", "authors": "Benjamin Naujoks and Patrick Burger and Hans-Joachim Wuensche", "title": "Combining Deep Learning and Model-Based Methods for Robust Real-Time\n  Semantic Landmark Detection", "comments": "In 22nd International Conference on Information Fusion (FUSION), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to abstract features, significant objects, so-called landmarks, are\na more natural means for vehicle localization and navigation, especially in\nchallenging unstructured environments. The major challenge is to recognize\nlandmarks in various lighting conditions and changing environment (growing\nvegetation) while only having few training samples available. We propose a new\nmethod which leverages Deep Learning as well as model-based methods to overcome\nthe need of a large data set. Using RGB images and light detection and ranging\n(LiDAR) point clouds, our approach combines state-of-the-art classification\nresults of Convolutional Neural Networks (CNN), with robust model-based methods\nby taking prior knowledge of previous time steps into account. Evaluations on a\nchallenging real-wold scenario, with trees and bushes as landmarks, show\npromising results over pure learning-based state-of-the-art 3D detectors, while\nbeing significant faster.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 14:28:45 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Naujoks", "Benjamin", ""], ["Burger", "Patrick", ""], ["Wuensche", "Hans-Joachim", ""]]}, {"id": "1909.00735", "submitter": "Gianmarco Santini", "authors": "Gianmarco Santini, No\\'emie Moreau, Mathieu Rubeaux", "title": "Kidney tumor segmentation using an ensembling multi-stage deep learning\n  approach. A contribution to the KiTS19 challenge", "comments": "11 pages, 4 figures, submitted to MICCAI 2019 - KiTS Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Precise characterization of the kidney and kidney tumor characteristics is of\noutmost importance in the context of kidney cancer treatment, especially for\nnephron sparing surgery which requires a precise localization of the tissues to\nbe removed. The need for accurate and automatic delineation tools is at the\norigin of the KiTS19 challenge. It aims at accelerating the research and\ndevelopment in this field to aid prognosis and treatment planning by providing\na characterized dataset of 300 CT scans to be segmented. To address the\nchallenge, we proposed an automatic, multi-stage, 2.5D deep learning-based\nsegmentation approach based on Residual UNet framework. An ensembling operation\nis added at the end to combine prediction results from previous stages reducing\nthe variance between single models. Our neural network segmentation algorithm\nreaches a mean Dice score of 0.96 and 0.74 for kidney and kidney tumors,\nrespectively on 90 unseen test cases. The results obtained are promising and\ncould be improved by incorporating prior knowledge about the benign cysts that\nregularly lower the tumor segmentation results.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 14:35:32 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Santini", "Gianmarco", ""], ["Moreau", "No\u00e9mie", ""], ["Rubeaux", "Mathieu", ""]]}, {"id": "1909.00741", "submitter": "Sreyasi Nag Chowdhury", "authors": "Sreyasi Nag Chowdhury, Niket Tandon, Hakan Ferhatosmanoglu, Gerhard\n  Weikum", "title": "VISIR: Visual and Semantic Image Label Refinement", "comments": "Published in WSDM 2018", "journal-ref": "ACM ISBN 978-1-4503-5581-0/18/02 2018", "doi": "10.1145/3159652.3159693", "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The social media explosion has populated the Internet with a wealth of\nimages. There are two existing paradigms for image retrieval: 1) content-based\nimage retrieval (CBIR), which has traditionally used visual features for\nsimilarity search (e.g., SIFT features), and 2) tag-based image retrieval\n(TBIR), which has relied on user tagging (e.g., Flickr tags). CBIR now gains\nsemantic expressiveness by advances in deep-learning-based detection of visual\nlabels. TBIR benefits from query-and-click logs to automatically infer more\ninformative labels. However, learning-based tagging still yields noisy labels\nand is restricted to concrete objects, missing out on generalizations and\nabstractions. Click-based tagging is limited to terms that appear in the\ntextual context of an image or in queries that lead to a click. This paper\naddresses the above limitations by semantically refining and expanding the\nlabels suggested by learning-based object detection. We consider the semantic\ncoherence between the labels for different objects, leverage lexical and\ncommonsense knowledge, and cast the label assignment into a constrained\noptimization problem solved by an integer linear program. Experiments show that\nour method, called VISIR, improves the quality of the state-of-the-art visual\nlabeling tools like LSDA and YOLO.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 14:41:44 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Chowdhury", "Sreyasi Nag", ""], ["Tandon", "Niket", ""], ["Ferhatosmanoglu", "Hakan", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1909.00781", "submitter": "Umberto Michieli", "authors": "Umberto Michieli, Matteo Biasetton, Gianluca Agresti, Pietro Zanuttigh", "title": "Adversarial Learning and Self-Teaching Techniques for Domain Adaptation\n  in Semantic Segmentation", "comments": "Accepted at IEEE Transactions on Intelligent Vehicles (T-IV) 10\n  pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have been widely used in autonomous driving systems\nfor the semantic understanding of urban scenes. However, they need a huge\namount of labeled data for training, which is difficult and expensive to\nacquire. A recently proposed workaround is to train deep networks using\nsynthetic data, but the domain shift between real world and synthetic\nrepresentations limits the performance. In this work, a novel Unsupervised\nDomain Adaptation (UDA) strategy is introduced to solve this issue. The\nproposed learning strategy is driven by three components: a standard supervised\nlearning loss on labeled synthetic data; an adversarial learning module that\nexploits both labeled synthetic data and unlabeled real data; finally, a\nself-teaching strategy applied to unlabeled data. The last component exploits a\nregion growing framework guided by the segmentation confidence. Furthermore, we\nweighted this component on the basis of the class frequencies to enhance the\nperformance on less common classes. Experimental results prove the\neffectiveness of the proposed strategy in adapting a segmentation network\ntrained on synthetic datasets, like GTA5 and SYNTHIA, to real world datasets\nlike Cityscapes and Mapillary.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 16:05:05 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 15:46:24 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Michieli", "Umberto", ""], ["Biasetton", "Matteo", ""], ["Agresti", "Gianluca", ""], ["Zanuttigh", "Pietro", ""]]}, {"id": "1909.00794", "submitter": "Zhanghui Kuang", "authors": "Youjiang Xu, Jiaqi Duan, Zhanghui Kuang, Xiaoyu Yue, Hongbin Sun, Yue\n  Guan, and Wayne Zhang", "title": "Geometry Normalization Networks for Accurate Scene Text Detection", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large geometry (e.g., orientation) variances are the key challenges in the\nscene text detection. In this work, we first conduct experiments to investigate\nthe capacity of networks for learning geometry variances on detecting scene\ntexts, and find that networks can handle only limited text geometry variances.\nThen, we put forward a novel Geometry Normalization Module (GNM) with multiple\nbranches, each of which is composed of one Scale Normalization Unit and one\nOrientation Normalization Unit, to normalize each text instance to one desired\ncanonical geometry range through at least one branch. The GNM is general and\nreadily plugged into existing convolutional neural network based text detectors\nto construct end-to-end Geometry Normalization Networks (GNNets). Moreover, we\npropose a geometry-aware training scheme to effectively train the GNNets by\nsampling and augmenting text instances from a uniform geometry variance\ndistribution. Finally, experiments on popular benchmarks of ICDAR 2015 and\nICDAR 2017 MLT validate that our method outperforms all the state-of-the-art\napproaches remarkably by obtaining one-forward test F-scores of 88.52 and 74.54\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 16:47:05 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Xu", "Youjiang", ""], ["Duan", "Jiaqi", ""], ["Kuang", "Zhanghui", ""], ["Yue", "Xiaoyu", ""], ["Sun", "Hongbin", ""], ["Guan", "Yue", ""], ["Zhang", "Wayne", ""]]}, {"id": "1909.00798", "submitter": "Joseph Antony A", "authors": "Rama Sai Mamidala, Uday Uthkota, Mahamkali Bhavani Shankar, A. Joseph\n  Antony and A. V. Narasimhadhan", "title": "Dynamic Approach for Lane Detection using Google Street View and CNN", "comments": "Preprint: To be published in the proceedings of IEEE TENCON 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Lane detection algorithms have been the key enablers for a fully-assistive\nand autonomous navigation systems. In this paper, a novel and pragmatic\napproach for lane detection is proposed using a convolutional neural network\n(CNN) model based on SegNet encoder-decoder architecture. The encoder block\nrenders low-resolution feature maps of the input and the decoder block provides\npixel-wise classification from the feature maps. The proposed model has been\ntrained over 2000 image data-set and tested against their corresponding\nground-truth provided in the data-set for evaluation. To enable real-time\nnavigation, we extend our model's predictions interfacing it with the existing\nGoogle APIs evaluating the metrics of the model tuning the hyper-parameters.\nThe novelty of this approach lies in the integration of existing segNet\narchitecture with google APIs. This interface makes it handy for assistive\nrobotic systems. The observed results show that the proposed method is robust\nunder challenging occlusion conditions due to pre-processing involved and gives\nsuperior performance when compared to the existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 17:10:01 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Mamidala", "Rama Sai", ""], ["Uthkota", "Uday", ""], ["Shankar", "Mahamkali Bhavani", ""], ["Antony", "A. Joseph", ""], ["Narasimhadhan", "A. V.", ""]]}, {"id": "1909.00799", "submitter": "Steven Grosz Mr.", "authors": "Steven A. Grosz, Joshua J. Engelsma, Nicholas G. Paulter Jr. and Anil\n  K. Jain", "title": "White-Box Evaluation of Fingerprint Matchers: Robustness to Minutiae\n  Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prevailing evaluations of fingerprint recognition systems have been performed\nas end-to-end black-box tests of fingerprint identification or authentication\naccuracy. However, performance of the end-to-end system is subject to errors\narising in any of its constituent modules, including: fingerprint scanning,\npreprocessing, feature extraction, and matching. Conversely, white-box\nevaluations provide a more granular evaluation by studying the individual\nsub-components of a system. While a few studies have conducted stand-alone\nevaluations of the fingerprint reader and feature extraction modules of\nfingerprint recognition systems, little work has been devoted towards white-box\nevaluations of the fingerprint matching module. We report results of a\ncontrolled, white-box evaluation of one open-source and two\ncommercial-off-the-shelf (COTS) minutiae-based matchers in terms of their\nrobustness against controlled perturbations (random noise and non-linear\ndistortions) introduced into the input minutiae feature sets. Our white-box\nevaluations reveal that the performance of fingerprint minutiae matchers are\nmore susceptible to non-linear distortion and missing minutiae than spurious\nminutiae and small positional displacements of the minutiae locations.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 17:11:17 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 17:16:16 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 19:20:00 GMT"}, {"version": "v4", "created": "Sun, 12 Apr 2020 13:07:33 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Grosz", "Steven A.", ""], ["Engelsma", "Joshua J.", ""], ["Paulter", "Nicholas G.", "Jr."], ["Jain", "Anil K.", ""]]}, {"id": "1909.00823", "submitter": "Md Nafee Al Islam", "authors": "Md Nafee Al Islam and Siamul Karim Khan", "title": "HishabNet: Detection, Localization and Calculation of Handwritten\n  Bengali Mathematical Expressions", "comments": "6 pages, 5 figures, This paper is under review in \"22nd International\n  Conference on Computer and Information Technology (ICCIT), 2019\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, recognition of handwritten Bengali letters and digits have captured\na lot of attention among the researchers of the AI community. In this work, we\npropose a Convolutional Neural Network (CNN) based object detection model which\ncan recognize and evaluate handwritten Bengali mathematical expressions. This\nmethod is able to detect multiple Bengali digits and operators and locate their\npositions in the image. With that information, it is able to construct numbers\nfrom series of digits and perform mathematical operations on them. For the\nobject detection task, the state-of-the-art YOLOv3 algorithm was utilized. For\ntraining and evaluating the model, we have engineered a new dataset 'Hishab'\nwhich is the first Bengali handwritten digits dataset intended for object\ndetection. The model achieved an overall validation mean average precision\n(mAP) of 98.6%. Also, the classification accuracy of the feature extractor\nbackbone CNN used in our model was tested on two publicly available Bengali\nhandwritten digits datasets: NumtaDB and CMATERdb. The backbone CNN achieved a\ntest set accuracy of 99.6252% on NumtaDB and 99.0833% on CMATERdb.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 18:28:14 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Islam", "Md Nafee Al", ""], ["Khan", "Siamul Karim", ""]]}, {"id": "1909.00848", "submitter": "Pedro H. Bugatti", "authors": "Pedro H. Bugatti, Priscila T. M. Saito, Larry S. Davis", "title": "HiCoRe: Visual Hierarchical Context-Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning about images/objects and their hierarchical interactions is a key\nconcept for the next generation of computer vision approaches. Here we present\na new framework to deal with it through a visual hierarchical context-based\nreasoning. Current reasoning methods use the fine-grained labels from images'\nobjects and their interactions to predict labels to new objects. Our framework\nmodifies this current information flow. It goes beyond and is independent of\nthe fine-grained labels from the objects to define the image context. It takes\ninto account the hierarchical interactions between different abstraction levels\n(i.e. taxonomy) of information in the images and their bounding-boxes. Besides\nthese connections, it considers their intrinsic characteristics. To do so, we\nbuild and apply graphs to graph convolution networks with convolutional neural\nnetworks. We show a strong effectiveness over widely used convolutional neural\nnetworks, reaching a gain 3 times greater on well-known image datasets. We\nevaluate the capability and the behavior of our framework under different\nscenarios, considering distinct (superclass, subclass and hierarchical)\ngranularity levels. We also explore attention mechanisms through graph\nattention networks and pre-processing methods considering dimensionality\nexpansion and/or reduction of the features' representations. Further analyses\nare performed comparing supervised and semi-supervised approaches.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 19:57:05 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Bugatti", "Pedro H.", ""], ["Saito", "Priscila T. M.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1909.00866", "submitter": "Shiva Azimi", "authors": "Shiva Azimi and Tapan K. Gandhi", "title": "Performance comparison of 3D correspondence grouping algorithm for 3D\n  plant point clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Plant Phenomics can be used to monitor the health and the growth of plants.\nComputer vision applications like stereo reconstruction, image retrieval,\nobject tracking, and object recognition play an important role in imaging based\nplant phenotyping. This paper offers a comparative evaluation of some popular\n3D correspondence grouping algorithms, motivated by the important role that\nthey can play in tasks such as model creation, plant recognition and\nidentifying plant parts. Another contribution of this paper is the extension of\n2D maximum likelihood matching to 3D Maximum Likelihood Estimation Sample\nConsensus (MLEASAC). MLESAC is efficient and is computationally less intense\nthan 3D random sample consensus (RANSAC). We test these algorithms on 3D point\nclouds of plants along with two standard benchmarks addressing shape retrieval\nand point cloud registration scenarios. The performance is evaluated in terms\nof precision and recall.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 21:02:00 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Azimi", "Shiva", ""], ["Gandhi", "Tapan K.", ""]]}, {"id": "1909.00883", "submitter": "David Smith", "authors": "David Smith, Matthew Loper, Xiaochen Hu, Paris Mavroidis, Javier\n  Romero", "title": "FACSIMILE: Fast and Accurate Scans From an Image in Less Than a Second", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current methods for body shape estimation either lack detail or require many\nimages. They are usually architecturally complex and computationally expensive.\nWe propose FACSIMILE (FAX), a method that estimates a detailed body from a\nsingle photo, lowering the bar for creating virtual representations of humans.\nOur approach is easy to implement and fast to execute, making it easily\ndeployable. FAX uses an image-translation network which recovers geometry at\nthe original resolution of the image. Counterintuitively, the main loss which\ndrives FAX is on per-pixel surface normals instead of per-pixel depth, making\nit possible to estimate detailed body geometry without any depth supervision.\nWe evaluate our approach both qualitatively and quantitatively, and compare\nwith a state-of-the-art method.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 22:39:05 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Smith", "David", ""], ["Loper", "Matthew", ""], ["Hu", "Xiaochen", ""], ["Mavroidis", "Paris", ""], ["Romero", "Javier", ""]]}, {"id": "1909.00889", "submitter": "Xiangyu Yue", "authors": "Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto\n  Sangiovanni-Vincentelli, Kurt Keutzer, Boqing Gong", "title": "Domain Randomization and Pyramid Consistency: Simulation-to-Real\n  Generalization without Accessing Target Domain Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to harness the potential of simulation for the semantic\nsegmentation of real-world self-driving scenes in a domain generalization\nfashion. The segmentation network is trained without any data of target domains\nand tested on the unseen target domains. To this end, we propose a new approach\nof domain randomization and pyramid consistency to learn a model with high\ngeneralizability. First, we propose to randomize the synthetic images with the\nstyles of real images in terms of visual appearances using auxiliary datasets,\nin order to effectively learn domain-invariant representations. Second, we\nfurther enforce pyramid consistency across different \"stylized\" images and\nwithin an image, in order to learn domain-invariant and scale-invariant\nfeatures, respectively. Extensive experiments are conducted on the\ngeneralization from GTA and SYNTHIA to Cityscapes, BDDS and Mapillary; and our\nmethod achieves superior results over the state-of-the-art techniques.\nRemarkably, our generalization results are on par with or even better than\nthose obtained by state-of-the-art simulation-to-real domain adaptation\nmethods, which access the target domain data at training time.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 23:33:47 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Yue", "Xiangyu", ""], ["Zhang", "Yang", ""], ["Zhao", "Sicheng", ""], ["Sangiovanni-Vincentelli", "Alberto", ""], ["Keutzer", "Kurt", ""], ["Gong", "Boqing", ""]]}, {"id": "1909.00900", "submitter": "Chengzhi Mao", "authors": "Chengzhi Mao, Ziyuan Zhong, Junfeng Yang, Carl Vondrick, Baishakhi Ray", "title": "Metric Learning for Adversarial Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks are well-known to be fragile to adversarial attacks. We conduct\nan empirical analysis of deep representations under the state-of-the-art attack\nmethod called PGD, and find that the attack causes the internal representation\nto shift closer to the \"false\" class. Motivated by this observation, we propose\nto regularize the representation space under attack with metric learning to\nproduce more robust classifiers. By carefully sampling examples for metric\nlearning, our learned representation not only increases robustness, but also\ndetects previously unseen adversarial samples. Quantitative experiments show\nimprovement of robustness accuracy by up to 4% and detection efficiency by up\nto 6% according to Area Under Curve score over prior work. The code of our work\nis available at\nhttps://github.com/columbia/Metric_Learning_Adversarial_Robustness.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 00:39:40 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 00:43:15 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Mao", "Chengzhi", ""], ["Zhong", "Ziyuan", ""], ["Yang", "Junfeng", ""], ["Vondrick", "Carl", ""], ["Ray", "Baishakhi", ""]]}, {"id": "1909.00903", "submitter": "Jing Dong", "authors": "Jing Dong, Zhaoyang Lv", "title": "miniSAM: A Flexible Factor Graph Non-linear Least Squares Optimization\n  Framework", "comments": "Accepted in IROS 2019 PPNIV workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in computer vision and robotics can be phrased as non-linear\nleast squares optimization problems represented by factor graphs, for example,\nsimultaneous localization and mapping (SLAM), structure from motion (SfM),\nmotion planning, and control. We have developed an open-source C++/Python\nframework miniSAM, for solving such factor graph based least squares problems.\nCompared to most existing frameworks for least squares solvers, miniSAM has (1)\nfull Python/NumPy API, which enables more agile development and easy binding\nwith existing Python projects, and (2) a wide list of sparse linear solvers,\nincluding CUDA enabled sparse linear solvers. Our benchmarking results shows\nminiSAM offers comparable performances on various types of problems, with more\nflexible and smoother development experience.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 00:51:29 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Dong", "Jing", ""], ["Lv", "Zhaoyang", ""]]}, {"id": "1909.00906", "submitter": "Yuyin Zhou", "authors": "Yuyin Zhou, Yingwei Li, Zhishuai Zhang, Yan Wang, Angtian Wang, Elliot\n  Fishman, Alan Yuille, Seyoun Park", "title": "Hyper-Pairing Network for Multi-Phase Pancreatic Ductal Adenocarcinoma\n  Segmentation", "comments": "To appear in MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pancreatic ductal adenocarcinoma (PDAC) is one of the most lethal cancers\nwith an overall five-year survival rate of 8%. Due to subtle texture changes of\nPDAC, pancreatic dual-phase imaging is recommended for better diagnosis of\npancreatic disease. In this study, we aim at enhancing PDAC automatic\nsegmentation by integrating multi-phase information (i.e., arterial phase and\nvenous phase). To this end, we present Hyper-Pairing Network (HPN), a 3D fully\nconvolution neural network which effectively integrates information from\ndifferent phases. The proposed approach consists of a dual path network where\nthe two parallel streams are interconnected with hyper-connections for\nintensive information exchange. Additionally, a pairing loss is added to\nencourage the commonality between high-level feature representations of\ndifferent phases. Compared to prior arts which use single phase data, HPN\nreports a significant improvement up to 7.73% (from 56.21% to 63.94%) in terms\nof DSC.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 00:55:37 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zhou", "Yuyin", ""], ["Li", "Yingwei", ""], ["Zhang", "Zhishuai", ""], ["Wang", "Yan", ""], ["Wang", "Angtian", ""], ["Fishman", "Elliot", ""], ["Yuille", "Alan", ""], ["Park", "Seyoun", ""]]}, {"id": "1909.00915", "submitter": "Theerasit Issaranon", "authors": "Theerasit Issaranon, Chuhang Zou, David Forsyth", "title": "Counterfactual Depth from a Single RGB Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method that predicts, from a single RGB image, a depth map that\ndescribes the scene when a masked object is removed - we call this\n\"counterfactual depth\" that models hidden scene geometry together with the\nobservations. Our method works for the same reason that scene completion works:\nthe spatial structure of objects is simple. But we offer a much higher\nresolution representation of space than current scene completion methods, as we\noperate at pixel-level precision and do not rely on a voxel representation.\nFurthermore, we do not require RGBD inputs. Our method uses a standard\nencoder-decoder architecture, and with a decoder modified to accept an object\nmask. We describe a small evaluation dataset that we have collected, which\nallows inference about what factors affect reconstruction most strongly. Using\nthis dataset, we show that our depth predictions for masked objects are better\nthan other baselines.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 01:50:17 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Issaranon", "Theerasit", ""], ["Zou", "Chuhang", ""], ["Forsyth", "David", ""]]}, {"id": "1909.00948", "submitter": "Ping Chao", "authors": "Ping Chao, Chao-Yang Kao, Yu-Shan Ruan, Chien-Hsiang Huang, Youn-Long\n  Lin", "title": "HarDNet: A Low Memory Traffic Network", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art neural network architectures such as ResNet, MobileNet, and\nDenseNet have achieved outstanding accuracy over low MACs and small model size\ncounterparts. However, these metrics might not be accurate for predicting the\ninference time. We suggest that memory traffic for accessing intermediate\nfeature maps can be a factor dominating the inference latency, especially in\nsuch tasks as real-time object detection and semantic segmentation of\nhigh-resolution video. We propose a Harmonic Densely Connected Network to\nachieve high efficiency in terms of both low MACs and memory traffic. The new\nnetwork achieves 35%, 36%, 30%, 32%, and 45% inference time reduction compared\nwith FC-DenseNet-103, DenseNet-264, ResNet-50, ResNet-152, and SSD-VGG,\nrespectively. We use tools including Nvidia profiler and ARM Scale-Sim to\nmeasure the memory traffic and verify that the inference latency is indeed\nproportional to the memory traffic consumption and the proposed network\nconsumes low memory traffic. We conclude that one should take memory traffic\ninto consideration when designing neural network architectures for\nhigh-resolution applications at the edge.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 04:34:18 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Chao", "Ping", ""], ["Kao", "Chao-Yang", ""], ["Ruan", "Yu-Shan", ""], ["Huang", "Chien-Hsiang", ""], ["Lin", "Youn-Long", ""]]}, {"id": "1909.00968", "submitter": "Chaohao Xie", "authors": "Chaohao Xie, Shaohui Liu, Chao Li, Ming-Ming Cheng, Wangmeng Zuo, Xiao\n  Liu, Shilei Wen, Errui Ding", "title": "Image Inpainting with Learnable Bidirectional Attention Maps", "comments": "18 pages, 16 figures, 2 tables, main paper + supplementary material\n  (Accepted at ICCV, 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most convolutional network (CNN)-based inpainting methods adopt standard\nconvolution to indistinguishably treat valid pixels and holes, making them\nlimited in handling irregular holes and more likely to generate inpainting\nresults with color discrepancy and blurriness. Partial convolution has been\nsuggested to address this issue, but it adopts handcrafted feature\nre-normalization, and only considers forward mask-updating. In this paper, we\npresent a learnable attention map module for learning feature renormalization\nand mask-updating in an end-to-end manner, which is effective in adapting to\nirregular holes and propagation of convolution layers. Furthermore, learnable\nreverse attention maps are introduced to allow the decoder of U-Net to\nconcentrate on filling in irregular holes instead of reconstructing both holes\nand known regions, resulting in our learnable bidirectional attention maps.\nQualitative and quantitative experiments show that our method performs\nfavorably against state-of-the-arts in generating sharper, more coherent and\nvisually plausible inpainting results. The source code and pre-trained models\nwill be available.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 06:18:40 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 07:18:14 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 12:55:14 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Xie", "Chaohao", ""], ["Liu", "Shaohui", ""], ["Li", "Chao", ""], ["Cheng", "Ming-Ming", ""], ["Zuo", "Wangmeng", ""], ["Liu", "Xiao", ""], ["Wen", "Shilei", ""], ["Ding", "Errui", ""]]}, {"id": "1909.00997", "submitter": "Pritha Ganguly", "authors": "Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra and Pratyush Kumar", "title": "PlotQA: Reasoning over Scientific Plots", "comments": "This is an extension of our previous arxiv paper \"Data Interpretation\n  over Plots\" and it is to be presented at WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing synthetic datasets (FigureQA, DVQA) for reasoning over plots do not\ncontain variability in data labels, real-valued data, or complex reasoning\nquestions. Consequently, proposed models for these datasets do not fully\naddress the challenge of reasoning over plots. In particular, they assume that\nthe answer comes either from a small fixed size vocabulary or from a bounding\nbox within the image. However, in practice, this is an unrealistic assumption\nbecause many questions require reasoning and thus have real-valued answers\nwhich appear neither in a small fixed size vocabulary nor in the image. In this\nwork, we aim to bridge this gap between existing datasets and real-world plots.\nSpecifically, we propose PlotQA with 28.9 million question-answer pairs over\n224,377 plots on data from real-world sources and questions based on\ncrowd-sourced question templates. Further, 80.76% of the out-of-vocabulary\n(OOV) questions in PlotQA have answers that are not in a fixed vocabulary.\nAnalysis of existing models on PlotQA reveals that they cannot deal with OOV\nquestions: their overall accuracy on our dataset is in single digits. This is\nnot surprising given that these models were not designed for such questions. As\na step towards a more holistic model which can address fixed vocabulary as well\nas OOV questions, we propose a hybrid approach: Specific questions are answered\nby choosing the answer from a fixed vocabulary or by extracting it from a\npredicted bounding box in the plot, while other questions are answered with a\ntable question-answering engine which is fed with a structured table generated\nby detecting visual elements from the image. On the existing DVQA dataset, our\nmodel has an accuracy of 58%, significantly improving on the highest reported\naccuracy of 46%. On PlotQA, our model has an accuracy of 22.52%, which is\nsignificantly better than state of the art models.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 08:23:51 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 08:45:36 GMT"}, {"version": "v3", "created": "Sat, 1 Feb 2020 06:56:30 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Methani", "Nitesh", ""], ["Ganguly", "Pritha", ""], ["Khapra", "Mitesh M.", ""], ["Kumar", "Pratyush", ""]]}, {"id": "1909.01025", "submitter": "Guizhong Liu", "authors": "Zhou Lingtao, Fang Jiaojiao, Liu Guizhong", "title": "Object Viewpoint Classification Based 3D Bounding Box Estimation for\n  Autonomous Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection is one of the most important tasks for the perception\nsystems of autonomous vehicles. With the significant success in the field of 2D\nobject detection, several monocular image based 3D object detection algorithms\nhave been proposed based on advanced 2D object detectors and the geometric\nconstraints between the 2D and 3D bounding boxes. In this paper, we propose a\nnovel method for determining the configuration of the 2D-3D geometric\nconstraints which is based on the well-known 2D-3D two stage object detection\nframework. First, we discrete viewpoints in which the camera shots the object\ninto 16 categories with respect to the observation relationship between camera\nand objects. Second, we design a viewpoint classifier by integrated a new\nsub-branch into the existing multi-branches CNN. Then, the configuration of\ngeometric constraint between the 2D and 3D bounding boxes can be determined\naccording to the output of this classifier. Extensive experiments on the KITTI\ndataset show that, our method not only improves the computational efficiency,\nbut also increases the overall precision of the model, especially to the\norientation angle estimation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 09:48:36 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Lingtao", "Zhou", ""], ["Jiaojiao", "Fang", ""], ["Guizhong", "Liu", ""]]}, {"id": "1909.01026", "submitter": "Guoqing Li", "authors": "Guoqing Li and Meng Zhang and Qianru Zhang and Ziyang Chen and Wenzhao\n  Liu and Jiaojie Li and Xuzhao Shen and Jianjun Li and Zhenyu Zhu and Chau\n  Yuen", "title": "PSDNet and DPDNet: Efficient channel expansion,\n  Depthwise-Pointwise-Depthwise Inverted Bottleneck Block", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-time applications, the deployment of deep neural networks is\nconstrained by high computational cost and efficient lightweight neural\nnetworks are widely concerned. In this paper, we propose that depthwise\nconvolution (DWC) is used to expand the number of channels in a bottleneck\nblock, which is more efficient than 1 x 1 convolution. The proposed\nPointwise-Standard-Depthwise network (PSDNet) based on channel expansion with\nDWC has fewer number of parameters, less computational cost and higher accuracy\nthan corresponding ResNet on CIFAR datasets. To design more efficient\nlightweight concolutional neural netwok, Depthwise-Pointwise-Depthwise inverted\nbottleneck block (DPD block) is proposed and DPDNet is designed by stacking DPD\nblock. Meanwhile, the number of parameters of DPDNet is only about 60% of that\nof MobileNetV2 for networks with the same number of layers, but can achieve\napproximated accuracy. Additionally, two hyperparameters of DPDNet can make the\ntrade-off between accuracy and computational cost, which makes DPDNet suitable\nfor diverse tasks. Furthermore, we find the networks with more DWC layers\noutperform the networks with more 1x1 convolution layers, which indicates that\nextracting spatial information is more important than combining channel\ninformation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 09:53:31 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 08:53:02 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Li", "Guoqing", ""], ["Zhang", "Meng", ""], ["Zhang", "Qianru", ""], ["Chen", "Ziyang", ""], ["Liu", "Wenzhao", ""], ["Li", "Jiaojie", ""], ["Shen", "Xuzhao", ""], ["Li", "Jianjun", ""], ["Zhu", "Zhenyu", ""], ["Yuen", "Chau", ""]]}, {"id": "1909.01028", "submitter": "Guizhong Liu", "authors": "Lingtao Zhou, Jiaojiao Fang, Guizhong Liu", "title": "Unsupervised Video Depth Estimation Based on Ego-motion and Disparity\n  Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning based depth estimation methods have received more and\nmore attention as they do not need vast quantities of densely labeled data for\ntraining which are touch to acquire. In this paper, we propose a novel\nunsupervised monocular video depth estimation method in natural scenes by\ntaking advantage of the state-of-the-art method of Zhou et al. which jointly\nestimates depth and camera motion. Our method advances beyond the baseline\nmethod by three aspects: 1) we add an additional signal as supervision to the\nbaseline method by incorporating left-right binocular images reconstruction\nloss based on the estimated disparities, thus the left frame can be\nreconstructed by the temporal frames and right frames of stereo vision; 2) the\nnetwork is trained by jointly using two kinds of view syntheses loss and\nleft-right disparity consistency regularization to estimate depth and pose\nsimultaneously; 3) we use the edge aware smooth L2 regularization to smooth the\ndepth map while preserving the contour of the target. Extensive experiments on\nthe KITTI autonomous driving dataset and Make3D dataset indicate the\nsuperiority of our algorithm in training efficiency. We can achieve competitive\nresults with the baseline by only 3/5 times training data. The experimental\nresults also show that our method even outperforms the classical supervised\nmethods that using either ground truth depth or given pose for training.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 09:55:56 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zhou", "Lingtao", ""], ["Fang", "Jiaojiao", ""], ["Liu", "Guizhong", ""]]}, {"id": "1909.01040", "submitter": "Koustav Ghosal", "authors": "Koustav Ghosal, Mukta Prasad, Aljosa Smolic", "title": "A Geometry-Sensitive Approach for Photographic Style Classification", "comments": "Irish Machine Vision and Image Processing Conference, Belfast, 2018", "journal-ref": "Irish Pattern Recognition and Classication Society (iprcs.org)\n  2018", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photographs are characterized by different compositional attributes like the\nRule of Thirds, depth of field, vanishing-lines etc. The presence or absence of\none or more of these attributes contributes to the overall artistic value of an\nimage. In this work, we analyze the ability of deep learning based methods to\nlearn such photographic style attributes. We observe that although a standard\nCNN learns the texture and appearance based features reasonably well, its\nunderstanding of global and geometric features is limited by two factors.\nFirst, the data-augmentation strategies (cropping, warping, etc.) distort the\ncomposition of a photograph and affect the performance. Secondly, the CNN\nfeatures, in principle, are translation-invariant and appearance-dependent. But\nsome geometric properties important for aesthetics, e.g. the Rule of Thirds\n(RoT), are position-dependent and appearance-invariant. Therefore, we propose a\nnovel input representation which is geometry-sensitive, position-cognizant and\nappearance-invariant. We further introduce a two-column CNN architecture that\nperforms better than the state-of-the-art (SoA) in photographic style\nclassification. From our results, we observe that the proposed network learns\nboth the geometric and appearance-based attributes better than the SoA.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 10:22:40 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Ghosal", "Koustav", ""], ["Prasad", "Mukta", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1909.01051", "submitter": "Pedro M. Esperan\\c{c}a", "authors": "Fabio Maria Carlucci and Pedro M Esperan\\c{c}a and Marco Singh and\n  Victor Gabillon and Antoine Yang and Hang Xu and Zewei Chen and Jun Wang", "title": "MANAS: Multi-Agent Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Neural Architecture Search (NAS) problem is typically formulated as a\ngraph search problem where the goal is to learn the optimal operations over\nedges in order to maximise a graph-level global objective. Due to the large\narchitecture parameter space, efficiency is a key bottleneck preventing NAS\nfrom its practical use. In this paper, we address the issue by framing NAS as a\nmulti-agent problem where agents control a subset of the network and coordinate\nto reach optimal architectures. We provide two distinct lightweight\nimplementations, with reduced memory requirements (1/8th of state-of-the-art),\nand performances above those of much more computationally expensive methods.\nTheoretically, we demonstrate vanishing regrets of the form O(sqrt(T)), with T\nbeing the total number of rounds. Finally, aware that random search is an,\noften ignored, effective baseline we perform additional experiments on 3\nalternative datasets and 2 network configurations, and achieve favourable\nresults in comparison.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 10:36:37 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 12:36:50 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 11:37:52 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Carlucci", "Fabio Maria", ""], ["Esperan\u00e7a", "Pedro M", ""], ["Singh", "Marco", ""], ["Gabillon", "Victor", ""], ["Yang", "Antoine", ""], ["Xu", "Hang", ""], ["Chen", "Zewei", ""], ["Wang", "Jun", ""]]}, {"id": "1909.01056", "submitter": "Koustav Ghosal", "authors": "Xu Zheng, Tejo Chalasani, Koustav Ghosal, Sebastian Lutz, Aljosa\n  Smolic", "title": "STaDA: Style Transfer as Data Augmentation", "comments": "14th International Conference on Computer Vision Theory and\n  Applications, 2019", "journal-ref": null, "doi": "10.5220/0007353401070114", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of training deep Convolutional Neural Networks (CNNs) heavily\ndepends on a significant amount of labelled data. Recent research has found\nthat neural style transfer algorithms can apply the artistic style of one image\nto another image without changing the latter's high-level semantic content,\nwhich makes it feasible to employ neural style transfer as a data augmentation\nmethod to add more variation to the training dataset. The contribution of this\npaper is a thorough evaluation of the effectiveness of the neural style\ntransfer as a data augmentation method for image classification tasks. We\nexplore the state-of-the-art neural style transfer algorithms and apply them as\na data augmentation method on Caltech 101 and Caltech 256 dataset, where we\nfound around 2% improvement from 83% to 85% of the image classification\naccuracy with VGG16, compared with traditional data augmentation strategies. We\nalso combine this new method with conventional data augmentation approaches to\nfurther improve the performance of image classification. This work shows the\npotential of neural style transfer in computer vision field, such as helping us\nto reduce the difficulty of collecting sufficient labelled data and improve the\nperformance of generic image-based deep learning algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 10:48:42 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zheng", "Xu", ""], ["Chalasani", "Tejo", ""], ["Ghosal", "Koustav", ""], ["Lutz", "Sebastian", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1909.01058", "submitter": "Bharti Munjal", "authors": "Bharti Munjal, Fabio Galasso, Sikandar Amin", "title": "Knowledge Distillation for End-to-End Person Search", "comments": "The British Machine Vision conference (BMVC), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce knowledge distillation for end-to-end person search. End-to-End\nmethods are the current state-of-the-art for person search that solve both\ndetection and re-identification jointly. These approaches for joint\noptimization show their largest drop in performance due to a sub-optimal\ndetector.\n  We propose two distinct approaches for extra supervision of end-to-end person\nsearch methods in a teacher-student setting. The first is adopted from\nstate-of-the-art knowledge distillation in object detection. We employ this to\nsupervise the detector of our person search model at various levels using a\nspecialized detector. The second approach is new, simple and yet considerably\nmore effective. This distills knowledge from a teacher re-identification\ntechnique via a pre-computed look-up table of ID features. It relaxes the\nlearning of identification features and allows the student to focus on the\ndetection task. This procedure not only helps fixing the sub-optimal detector\ntraining in the joint optimization and simultaneously improving the person\nsearch, but also closes the performance gap between the teacher and the student\nfor model compression in this case. Overall, we demonstrate significant\nimprovements for two recent state-of-the-art methods using our proposed\nknowledge distillation approach on two benchmark datasets. Moreover, on the\nmodel compression task our approach brings the performance of smaller models on\npar with the larger models.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 10:53:17 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 18:31:40 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Munjal", "Bharti", ""], ["Galasso", "Fabio", ""], ["Amin", "Sikandar", ""]]}, {"id": "1909.01064", "submitter": "Tianyang Shi", "authors": "Tianyang Shi (1), Yi Yuan (1), Changjie Fan (1), Zhengxia Zou (2),\n  Zhenwei Shi (3), Yong Liu (4) ((1) NetEase Fuxi AI Lab, (2) University of\n  Michigan, (3) Beihang University, (4) Zhejiang University)", "title": "Face-to-Parameter Translation for Game Character Auto-Creation", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Character customization system is an important component in Role-Playing\nGames (RPGs), where players are allowed to edit the facial appearance of their\nin-game characters with their own preferences rather than using default\ntemplates. This paper proposes a method for automatically creating in-game\ncharacters of players according to an input face photo. We formulate the above\n\"artistic creation\" process under a facial similarity measurement and parameter\nsearching paradigm by solving an optimization problem over a large set of\nphysically meaningful facial parameters. To effectively minimize the distance\nbetween the created face and the real one, two loss functions, i.e. a\n\"discriminative loss\" and a \"facial content loss\", are specifically designed.\nAs the rendering process of a game engine is not differentiable, a generative\nnetwork is further introduced as an \"imitator\" to imitate the physical behavior\nof the game engine so that the proposed method can be implemented under a\nneural style transfer framework and the parameters can be optimized by gradient\ndescent. Experimental results demonstrate that our method achieves a high\ndegree of generation similarity between the input face photo and the created\nin-game character in terms of both global appearance and local details. Our\nmethod has been deployed in a new game last year and has now been used by\nplayers over 1 million times.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 11:05:19 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Shi", "Tianyang", ""], ["Yuan", "Yi", ""], ["Fan", "Changjie", ""], ["Zou", "Zhengxia", ""], ["Shi", "Zhenwei", ""], ["Liu", "Yong", ""]]}, {"id": "1909.01068", "submitter": "Yanning Zhou", "authors": "Yanning Zhou, Simon Graham, Navid Alemi Koohbanani, Muhammad Shaban,\n  Pheng-Ann Heng, Nasir Rajpoot", "title": "CGC-Net: Cell Graph Convolutional Network for Grading of Colorectal\n  Cancer Histology Images", "comments": "Accepted in ICCVW 2019 (Visual Recognition for Medical Images)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorectal cancer (CRC) grading is typically carried out by assessing the\ndegree of gland formation within histology images. To do this, it is important\nto consider the overall tissue micro-environment by assessing the cell-level\ninformation along with the morphology of the gland. However, current automated\nmethods for CRC grading typically utilise small image patches and therefore\nfail to incorporate the entire tissue micro-architecture for grading purposes.\nTo overcome the challenges of CRC grading, we present a novel cell-graph\nconvolutional neural network (CGC-Net) that converts each large histology image\ninto a graph, where each node is represented by a nucleus within the original\nimage and cellular interactions are denoted as edges between these nodes\naccording to node similarity. The CGC-Net utilises nuclear appearance features\nin addition to the spatial location of nodes to further boost the performance\nof the algorithm. To enable nodes to fuse multi-scale information, we introduce\nAdaptive GraphSage, which is a graph convolution technique that combines\nmulti-level features in a data-driven way. Furthermore, to deal with redundancy\nin the graph, we propose a sampling technique that removes nodes in areas of\ndense nuclear activity. We show that modeling the image as a graph enables us\nto effectively consider a much larger image (around 16$\\times$ larger) than\ntraditional patch-based approaches and model the complex structure of the\ntissue micro-environment. We construct cell graphs with an average of over\n3,000 nodes on a large CRC histology image dataset and report state-of-the-art\nresults as compared to recent patch-based as well as contextual patch-based\ntechniques, demonstrating the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 11:25:00 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zhou", "Yanning", ""], ["Graham", "Simon", ""], ["Koohbanani", "Navid Alemi", ""], ["Shaban", "Muhammad", ""], ["Heng", "Pheng-Ann", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "1909.01098", "submitter": "C\\'ecilia Ostertag", "authors": "Cecilia Ostertag, Marie Beurton-Aimar, Thierry Urruty", "title": "3DSiameseNet to Analyze Brain MRI", "comments": "published in ICPRS 2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of the cognitive evolution of a person susceptible to develop a\nneurodegenerative disorder is crucial to provide an appropriate treatment as\nsoon as possible. In this paper we propose a 3D siamese network designed to\nextract features from whole-brain 3D MRI images. We show that it is possible to\nextract meaningful features using convolution layers, reducing the need of\nclassical image processing operations such as segmentation or pre-computing\nfeatures such as cortical thickness. To lead this study we used the Alzheimer's\nDisease Neuroimaging Initiative (ADNI), a public data base of 3D MRI brain\nimages. A set of 247 subjects has been extracted, all of the subjects having 2\nimages in a range of 12 months. In order to measure the evolution of the\npatients states we have compared these 2 images. Our work has been inspired at\nthe beginning by an article of Bhagwat et al. in 2018, who have proposed a\nsiamese network to predict the status of patients but without any convolutional\nlayers and reducing the MRI images to a vector of features extracted from\npredefined ROIs. We show that our network achieves an accuracy of 90\\% in the\nclassification of cognitively declining VS stable patients. This result has\nbeen obtained without the help of a cognitive score and with a small number of\npatients comparing to the current datasets size claimed in deep learning\ndomain.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 11:52:25 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Ostertag", "Cecilia", ""], ["Beurton-Aimar", "Marie", ""], ["Urruty", "Thierry", ""]]}, {"id": "1909.01106", "submitter": "Yida Wang", "authors": "Yida Wang, David Joseph Tan, Nassir Navab, Federico Tombari", "title": "ForkNet: Multi-branch Volumetric Semantic Completion from a Single Depth\n  Image", "comments": "Accepted in International Conference on Computer Vision 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel model for 3D semantic completion from a single depth\nimage, based on a single encoder and three separate generators used to\nreconstruct different geometric and semantic representations of the original\nand completed scene, all sharing the same latent space. To transfer information\nbetween the geometric and semantic branches of the network, we introduce paths\nbetween them concatenating features at corresponding network layers. Motivated\nby the limited amount of training samples from real scenes, an interesting\nattribute of our architecture is the capacity to supplement the existing\ndataset by generating a new training dataset with high quality, realistic\nscenes that even includes occlusion and real noise. We build the new dataset by\nsampling the features directly from latent space which generates a pair of\npartial volumetric surface and completed volumetric semantic surface. Moreover,\nwe utilize multiple discriminators to increase the accuracy and realism of the\nreconstructions. We demonstrate the benefits of our approach on standard\nbenchmarks for the two most common completion tasks: semantic 3D scene\ncompletion and 3D object completion.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 12:04:39 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Wang", "Yida", ""], ["Tan", "David Joseph", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "1909.01122", "submitter": "Orest Kupyn", "authors": "Orest Kupyn, Dmitry Pranchuk", "title": "Fast and Efficient Model for Real-Time Tiger Detection In The Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The highest accuracy object detectors to date are based either on a two-stage\napproach such as Fast R-CNN or one-stage detectors such as Retina-Net or SSD\nwith deep and complex backbones. In this paper we present TigerNet - simple yet\nefficient FPN based network architecture for Amur Tiger Detection in the wild.\nThe model has 600k parameters, requires 0.071 GFLOPs per image and can run on\nthe edge devices (smart cameras) in near real time. In addition, we introduce a\ntwo-stage semi-supervised learning via pseudo-labelling learning approach to\ndistill the knowledge from the larger networks. For ATRW-ICCV 2019 tiger\ndetection sub-challenge, based on public leaderboard score, our approach shows\nsuperior performance in comparison to other methods.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 12:47:23 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Kupyn", "Orest", ""], ["Pranchuk", "Dmitry", ""]]}, {"id": "1909.01127", "submitter": "Guanxiong Luo", "authors": "GuanXiong Luo, Na Zhao, Wenhao Jiang, Peng Cao", "title": "MRI Reconstruction Using Deep Bayesian Estimation", "comments": null, "journal-ref": null, "doi": "10.1002/mrm.28274", "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To develop a deep learning-based Bayesian inference for MRI\nreconstruction. Methods: We modeled the MRI reconstruction problem with Bayes's\ntheorem, following the recently proposed PixelCNN++ method. The image\nreconstruction from incomplete k-space measurement was obtained by maximizing\nthe posterior possibility. A generative network was utilized as the image\nprior, which was computationally tractable, and the k-space data fidelity was\nenforced by using an equality constraint. The stochastic backpropagation was\nutilized to calculate the descent gradient in the process of maximum a\nposterior, and a projected subgradient method was used to impose the equality\nconstraint. In contrast to the other deep learning reconstruction methods, the\nproposed one used the likelihood of prior as the training loss and the\nobjective function in reconstruction to improve the image quality. Results: The\nproposed method showed an improved performance in preserving image details and\nreducing aliasing artifacts, compared with GRAPPA, $\\ell_1$-ESPRiT, and MODL, a\nstate-of-the-art deep learning reconstruction method. The proposed method\ngenerally achieved more than 5 dB peak signal-to-noise ratio improvement for\ncompressed sensing and parallel imaging reconstructions compared with the other\nmethods. Conclusion: The Bayesian inference significantly improved the\nreconstruction performance, compared with the conventional $\\ell_1$-sparsity\nprior in compressed sensing reconstruction tasks. More importantly, the\nproposed reconstruction framework can be generalized for most MRI\nreconstruction scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 12:54:58 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 22:59:53 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Luo", "GuanXiong", ""], ["Zhao", "Na", ""], ["Jiang", "Wenhao", ""], ["Cao", "Peng", ""]]}, {"id": "1909.01140", "submitter": "Mikael Brudfors", "authors": "Mikael Brudfors, Yael Balbastre, Parashkev Nachev, John Ashburner", "title": "A Tool for Super-Resolving Multimodal Clinical MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a tool for resolution recovery in multimodal clinical magnetic\nresonance imaging (MRI). Such images exhibit great variability, both biological\nand instrumental. This variability makes automated processing with neuroimaging\nanalysis software very challenging. This leaves intelligence extractable only\nfrom large-scale analyses of clinical data untapped, and impedes the\nintroduction of automated predictive systems in clinical care. The tool\npresented in this paper enables such processing, via inference in a generative\nmodel of thick-sliced, multi-contrast MR scans. All model parameters are\nestimated from the observed data, without the need for manual tuning. The\nmodel-driven nature of the approach means that no type of training is needed\nfor applicability to the diversity of MR contrasts present in a clinical\ncontext. We show on simulated data that the proposed approach outperforms\nconventional model-based techniques, and on a large hospital dataset of\nmultimodal MRIs that the tool can successfully super-resolve very thick-sliced\nimages. The implementation is available from\nhttps://github.com/brudfors/spm_superres.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 13:03:32 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Brudfors", "Mikael", ""], ["Balbastre", "Yael", ""], ["Nachev", "Parashkev", ""], ["Ashburner", "John", ""]]}, {"id": "1909.01161", "submitter": "Yaqi Xie", "authors": "Yaqi Xie, Ziwei Xu, Mohan S. Kankanhalli, Kuldeep S. Meel, Harold Soh", "title": "Embedding Symbolic Knowledge into Deep Networks", "comments": "*Equal contribution; Accepted at conference Neural Information\n  Processing Systems (NeurIPS), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we aim to leverage prior symbolic knowledge to improve the\nperformance of deep models. We propose a graph embedding network that projects\npropositional formulae (and assignments) onto a manifold via an augmented Graph\nConvolutional Network (GCN). To generate semantically-faithful embeddings, we\ndevelop techniques to recognize node heterogeneity, and semantic regularization\nthat incorporate structural constraints into the embedding. Experiments show\nthat our approach improves the performance of models trained to perform\nentailment checking and visual relation prediction. Interestingly, we observe a\nconnection between the tractability of the propositional theory representation\nand the ease of embedding. Future exploration of this connection may elucidate\nthe relationship between knowledge compilation and vector representation\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 13:23:25 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 12:38:40 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 10:26:34 GMT"}, {"version": "v4", "created": "Tue, 29 Oct 2019 10:53:02 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Xie", "Yaqi", ""], ["Xu", "Ziwei", ""], ["Kankanhalli", "Mohan S.", ""], ["Meel", "Kuldeep S.", ""], ["Soh", "Harold", ""]]}, {"id": "1909.01182", "submitter": "V\\'ictor Manuel Campello", "authors": "V\\'ictor M. Campello, Carlos Mart\\'in-Isla, Cristian Izquierdo,\n  Steffen E. Petersen, Miguel A. Gonz\\'alez Ballester, Karim Lekadir", "title": "Combining Multi-Sequence and Synthetic Images for Improved Segmentation\n  of Late Gadolinium Enhancement Cardiac MRI", "comments": "10 pages, Accepted to MS-CMRSeg Challenge (STACOM 2019), reference\n  added and affiliations updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of the cardiac boundaries in late gadolinium\nenhancement magnetic resonance images (LGE-MRI) is a fundamental step for\naccurate quantification of scar tissue. However, while there are many solutions\nfor automatic cardiac segmentation of cine images, the presence of scar tissue\ncan make the correct delineation of the myocardium in LGE-MRI challenging even\nfor human experts. As part of the Multi-Sequence Cardiac MR Segmentation\nChallenge, we propose a solution for LGE-MRI segmentation based on two\ncomponents. First, a generative adversarial network is trained for the task of\nmodality-to-modality translation between cine and LGE-MRI sequences to obtain\nextra synthetic images for both modalities. Second, a deep learning model is\ntrained for segmentation with different combinations of original, augmented and\nsynthetic sequences. Our results based on three magnetic resonance sequences\n(LGE, bSSFP and T2) from 45 different patients show that the multi-sequence\nmodel training integrating synthetic images and data augmentation improves in\nthe segmentation over conventional training with real datasets. In conclusion,\nthe accuracy of the segmentation of LGE-MRI images can be improved by using\ncomplementary information provided by non-contrast MRI sequences.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 13:50:46 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 14:06:44 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Campello", "V\u00edctor M.", ""], ["Mart\u00edn-Isla", "Carlos", ""], ["Izquierdo", "Cristian", ""], ["Petersen", "Steffen E.", ""], ["Ballester", "Miguel A. Gonz\u00e1lez", ""], ["Lekadir", "Karim", ""]]}, {"id": "1909.01193", "submitter": "Spyridon Thermos", "authors": "Vladimiros Sterzentsenko, Leonidas Saroglou, Anargyros Chatzitofis,\n  Spyridon Thermos, Nikolaos Zioulis, Alexandros Doumanoglou, Dimitrios\n  Zarpalas, Petros Daras", "title": "Self-Supervised Deep Depth Denoising", "comments": "18 pages, 15 figures, ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth perception is considered an invaluable source of information for\nvarious vision tasks. However, depth maps acquired using consumer-level sensors\nstill suffer from non-negligible noise. This fact has recently motivated\nresearchers to exploit traditional filters, as well as the deep learning\nparadigm, in order to suppress the aforementioned non-uniform noise, while\npreserving geometric details. Despite the effort, deep depth denoising is still\nan open challenge mainly due to the lack of clean data that could be used as\nground truth. In this paper, we propose a fully convolutional deep autoencoder\nthat learns to denoise depth maps, surpassing the lack of ground truth data.\nSpecifically, the proposed autoencoder exploits multiple views of the same\nscene from different points of view in order to learn to suppress noise in a\nself-supervised end-to-end manner using depth and color information during\ntraining, yet only depth during inference. To enforce selfsupervision, we\nleverage a differentiable rendering technique to exploit photometric\nsupervision, which is further regularized using geometric and surface priors.\nAs the proposed approach relies on raw data acquisition, a large RGB-D corpus\nis collected using Intel RealSense sensors. Complementary to a quantitative\nevaluation, we demonstrate the effectiveness of the proposed self-supervised\ndenoising approach on established 3D reconstruction applications. Code is\navalable at https://github.com/VCL3D/DeepDepthDenoising\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 14:08:32 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 09:18:14 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Sterzentsenko", "Vladimiros", ""], ["Saroglou", "Leonidas", ""], ["Chatzitofis", "Anargyros", ""], ["Thermos", "Spyridon", ""], ["Zioulis", "Nikolaos", ""], ["Doumanoglou", "Alexandros", ""], ["Zarpalas", "Dimitrios", ""], ["Daras", "Petros", ""]]}, {"id": "1909.01203", "submitter": "Chunyu Wang", "authors": "Haibo Qiu, Chunyu Wang, Jingdong Wang, Naiyan Wang, Wenjun Zeng", "title": "Cross View Fusion for 3D Human Pose Estimation", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to recover absolute 3D human poses from multi-view\nimages by incorporating multi-view geometric priors in our model. It consists\nof two separate steps: (1) estimating the 2D poses in multi-view images and (2)\nrecovering the 3D poses from the multi-view 2D poses. First, we introduce a\ncross-view fusion scheme into CNN to jointly estimate 2D poses for multiple\nviews. Consequently, the 2D pose estimation for each view already benefits from\nother views. Second, we present a recursive Pictorial Structure Model to\nrecover the 3D pose from the multi-view 2D poses. It gradually improves the\naccuracy of 3D pose with affordable computational cost. We test our method on\ntwo public datasets H36M and Total Capture. The Mean Per Joint Position Errors\non the two datasets are 26mm and 29mm, which outperforms the state-of-the-arts\nremarkably (26mm vs 52mm, 29mm vs 35mm). Our code is released at\n\\url{https://github.com/microsoft/multiview-human-pose-estimation-pytorch}.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 14:15:30 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Qiu", "Haibo", ""], ["Wang", "Chunyu", ""], ["Wang", "Jingdong", ""], ["Wang", "Naiyan", ""], ["Zeng", "Wenjun", ""]]}, {"id": "1909.01205", "submitter": "Bram Wallace", "authors": "Bram Wallace, Bharath Hariharan", "title": "Few-Shot Generalization for Single-Image 3D Reconstruction via Priors", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on single-view 3D reconstruction shows impressive results, but\nhas been restricted to a few fixed categories where extensive training data is\navailable. The problem of generalizing these models to new classes with limited\ntraining data is largely open. To address this problem, we present a new model\narchitecture that reframes single-view 3D reconstruction as learnt, category\nagnostic refinement of a provided, category-specific prior. The provided prior\nshape for a novel class can be obtained from as few as one 3D shape from this\nclass. Our model can start reconstructing objects from the novel class using\nthis prior without seeing any training image for this class and without any\nretraining. Our model outperforms category-agnostic baselines and remains\ncompetitive with more sophisticated baselines that finetune on the novel\ncategories. Additionally, our network is capable of improving the\nreconstruction given multiple views despite not being trained on task of\nmulti-view reconstruction.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 14:18:42 GMT"}], "update_date": "2019-09-15", "authors_parsed": [["Wallace", "Bram", ""], ["Hariharan", "Bharath", ""]]}, {"id": "1909.01206", "submitter": "Amogh Gudi", "authors": "Amogh Gudi, Marian Bittner, Roelof Lochmans, Jan van Gemert", "title": "Efficient Real-Time Camera Based Estimation of Heart Rate and Its\n  Variability", "comments": "International Conference on Computer Vision (ICCV) Workshop on\n  Computer Vision for Physiological Measurement (CVPM) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote photo-plethysmography (rPPG) uses a remotely placed camera to\nestimating a person's heart rate (HR). Similar to how heart rate can provide\nuseful information about a person's vital signs, insights about the underlying\nphysio/psychological conditions can be obtained from heart rate variability\n(HRV). HRV is a measure of the fine fluctuations in the intervals between heart\nbeats. However, this measure requires temporally locating heart beats with a\nhigh degree of precision. We introduce a refined and efficient real-time rPPG\npipeline with novel filtering and motion suppression that not only estimates\nheart rate more accurately, but also extracts the pulse waveform to time heart\nbeats and measure heart rate variability. This method requires no rPPG specific\ntraining and is able to operate in real-time. We validate our method on a\nself-recorded dataset under an idealized lab setting, and show state-of-the-art\nresults on two public dataset with realistic conditions (VicarPPG and PURE).\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 14:21:11 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Gudi", "Amogh", ""], ["Bittner", "Marian", ""], ["Lochmans", "Roelof", ""], ["van Gemert", "Jan", ""]]}, {"id": "1909.01207", "submitter": "Nikolaos Zioulis Mr.", "authors": "Vladimiros Sterzentsenko, Antonis Karakottas, Alexandros Papachristou,\n  Nikolaos Zioulis, Alexandros Doumanoglou, Dimitrios Zarpalas, Petros Daras", "title": "A Low-Cost, Flexible and Portable Volumetric Capturing System", "comments": "System available at https://github.com/VCL3D/VolumetricCapture", "journal-ref": null, "doi": "10.1109/SITIS.2018.00038", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view capture systems are complex systems to engineer. They require\ntechnical knowledge to install and intricate processes to setup related mainly\nto the sensors' spatial alignment (i.e. external calibration). However, with\nthe ongoing developments in new production methods, we are now at a position\nwhere the production of high quality realistic 3D assets is possible even with\ncommodity sensors. Nonetheless, the capturing systems developed with these\nmethods are heavily intertwined with the methods themselves, relying on custom\nsolutions and seldom - if not at all - publicly available. In light of this, we\ndesign, develop and publicly offer a multi-view capture system based on the\nlatest RGB-D sensor technology. For our system, we develop a portable and\neasy-to-use external calibration method that greatly reduces the effort and\nknowledge required, as well as simplify the overall process.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 14:23:24 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Sterzentsenko", "Vladimiros", ""], ["Karakottas", "Antonis", ""], ["Papachristou", "Alexandros", ""], ["Zioulis", "Nikolaos", ""], ["Doumanoglou", "Alexandros", ""], ["Zarpalas", "Dimitrios", ""], ["Daras", "Petros", ""]]}, {"id": "1909.01218", "submitter": "Maximilian M\\\"uller-Eberstein", "authors": "Maximilian M\\\"uller-Eberstein and Nanne van Noord", "title": "Translating Visual Art into Music", "comments": "Accepted for ICCV 2019 Workshop on Fashion, Art and Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Synesthetic Variational Autoencoder (SynVAE) introduced in this research\nis able to learn a consistent mapping between visual and auditive sensory\nmodalities in the absence of paired datasets. A quantitative evaluation on\nMNIST as well as the Behance Artistic Media dataset (BAM) shows that SynVAE is\ncapable of retaining sufficient information content during the translation\nwhile maintaining cross-modal latent space consistency. In a qualitative\nevaluation trial, human evaluators were furthermore able to match musical\nsamples with the images which generated them with accuracies of up to 73%.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 14:36:19 GMT"}], "update_date": "2019-09-15", "authors_parsed": [["M\u00fcller-Eberstein", "Maximilian", ""], ["van Noord", "Nanne", ""]]}, {"id": "1909.01228", "submitter": "Md Nafee Al Islam", "authors": "Md Nafee Al Islam, Tanzil Bin Hassan, Siamul Karim Khan", "title": "A CNN-based approach to classify cricket bowlers based on their bowling\n  actions", "comments": "5 pages, 7 figures, The paper is under review in \"IEEE International\n  Conference on Robotics, Automation, Artificial-Intelligence and\n  Internet-of-Things, 2019\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advances in hardware technologies and deep learning techniques, it\nhas become feasible to apply these techniques in diverse fields. Convolutional\nNeural Network (CNN), an architecture from the field of deep learning, has\nrevolutionized Computer Vision. Sports is one of the avenues in which the use\nof computer vision is thriving. Cricket is a complex game consisting of\ndifferent types of shots, bowling actions and many other activities. Every\nbowler, in a game of cricket, bowls with a different bowling action. We\nleverage this point to identify different bowlers. In this paper, we have\nproposed a CNN model to identify eighteen different cricket bowlers based on\ntheir bowling actions using transfer learning. Additionally, we have created a\ncompletely new dataset containing 8100 images of these eighteen bowlers to\ntrain the proposed framework and evaluate its performance. We have used the\nVGG16 model pre-trained with the ImageNet dataset and added a few layers on top\nof it to build our model. After trying out different strategies, we found that\nfreezing the weights for the first 14 layers of the network and training the\nrest of the layers works best. Our approach achieves an overall average\naccuracy of 93.3% on the test set and converges to a very low cross-entropy\nloss.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 14:45:40 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Islam", "Md Nafee Al", ""], ["Hassan", "Tanzil Bin", ""], ["Khan", "Siamul Karim", ""]]}, {"id": "1909.01258", "submitter": "Damian Campo", "authors": "Vahid Bastani, Damian Campo, Lucio Marcenaro, Carlo S. Regazzoni", "title": "Online Pedestrian Group Walking Event Detection Using Spectral Analysis\n  of Motion Similarity Graph", "comments": "Published in: 2015 12th IEEE International Conference on Advanced\n  Video and Signal Based Surveillance (AVSS)", "journal-ref": null, "doi": "10.1109/AVSS.2015.7301744", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for online identification of group of moving objects in the video is\nproposed in this paper. This method at each frame identifies group of tracked\nobjects with similar local instantaneous motion pattern using spectral\nclustering on motion similarity graph. Then, the output of the algorithm is\nused to detect the event of more than two object moving together as required by\nPETS2015 challenge. The performance of the algorithm is evaluated on the\nPETS2015 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 15:49:47 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Bastani", "Vahid", ""], ["Campo", "Damian", ""], ["Marcenaro", "Lucio", ""], ["Regazzoni", "Carlo S.", ""]]}, {"id": "1909.01285", "submitter": "Kevin Zhang", "authors": "Kevin Alex Zhang, Lei Xu, Alfredo Cuesta-Infante, Kalyan\n  Veeramachaneni", "title": "Robust Invisible Video Watermarking with Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of video watermarking is to embed a message within a video file in a\nway such that it minimally impacts the viewing experience but can be recovered\neven if the video is redistributed and modified, allowing media producers to\nassert ownership over their content. This paper presents RivaGAN, a novel\narchitecture for robust video watermarking which features a custom\nattention-based mechanism for embedding arbitrary data as well as two\nindependent adversarial networks which critique the video quality and optimize\nfor robustness. Using this technique, we are able to achieve state-of-the-art\nresults in deep learning-based video watermarking and produce watermarked\nvideos which have minimal visual distortion and are robust against common video\nprocessing operations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 16:28:40 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zhang", "Kevin Alex", ""], ["Xu", "Lei", ""], ["Cuesta-Infante", "Alfredo", ""], ["Veeramachaneni", "Kalyan", ""]]}, {"id": "1909.01341", "submitter": "Jing Jin", "authors": "Jing Jin and Junhui Hou and Jie Chen and Huanqiang Zeng and Sam Kwong\n  and Jingyi Yu", "title": "Deep Coarse-to-fine Dense Light Field Reconstruction with Flexible\n  Sampling and Geometry-aware Fusion", "comments": "17 pages, 11 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A densely-sampled light field (LF) is highly desirable in various\napplications, such as 3-D reconstruction, post-capture refocusing and virtual\nreality. However, it is costly to acquire such data. Although many\ncomputational methods have been proposed to reconstruct a densely-sampled LF\nfrom a sparsely-sampled one, they still suffer from either low reconstruction\nquality, low computational efficiency, or the restriction on the regularity of\nthe sampling pattern. To this end, we propose a novel learning-based method,\nwhich accepts sparsely-sampled LFs with irregular structures, and produces\ndensely-sampled LFs with arbitrary angular resolution accurately and\nefficiently. We also propose a simple yet effective method for optimizing the\nsampling pattern. Our proposed method, an end-to-end trainable network,\nreconstructs a densely-sampled LF in a coarse-to-fine manner. Specifically, the\ncoarse sub-aperture image (SAI) synthesis module first explores the scene\ngeometry from an unstructured sparsely-sampled LF and leverages it to\nindependently synthesize novel SAIs, in which a confidence-based blending\nstrategy is proposed to fuse the information from different input SAIs, giving\nan intermediate densely-sampled LF. Then, the efficient LF refinement module\nlearns the angular relationship within the intermediate result to recover the\nLF parallax structure. Comprehensive experimental evaluations demonstrate the\nsuperiority of our method on both real-world and synthetic LF images when\ncompared with state-of-the-art methods. In addition, we illustrate the benefits\nand advantages of the proposed approach when applied in various LF-based\napplications, including image-based rendering and depth estimation enhancement.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 05:16:21 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 05:28:03 GMT"}, {"version": "v3", "created": "Sat, 26 Sep 2020 10:33:15 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Jin", "Jing", ""], ["Hou", "Junhui", ""], ["Chen", "Jie", ""], ["Zeng", "Huanqiang", ""], ["Kwong", "Sam", ""], ["Yu", "Jingyi", ""]]}, {"id": "1909.01394", "submitter": "Luyao Shi", "authors": "Luyao Shi, John A. Onofrey, Enette Mae Revilla, Takuya Toyonaga, David\n  Menard, Jo-seph Ankrah, Richard E. Carson, Chi Liu, Yihuan Lu", "title": "A Novel Loss Function Incorporating Imaging Acquisition Physics for PET\n  Attenuation Map Generation using Deep Learning", "comments": "Accepted at MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In PET/CT imaging, CT is used for PET attenuation correction (AC). Mismatch\nbetween CT and PET due to patient body motion results in AC artifacts. In\naddition, artifact caused by metal, beam-hardening and count-starving in CT\nitself also introduces inaccurate AC for PET. Maximum likelihood reconstruction\nof activity and attenuation (MLAA) was proposed to solve those issues by\nsimultaneously reconstructing tracer activity ($\\lambda$-MLAA) and attenuation\nmap ($\\mu$-MLAA) based on the PET raw data only. However, $\\mu$-MLAA suffers\nfrom high noise and $\\lambda$-MLAA suffers from large bias as compared to the\nreconstruction using the CT-based attenuation map ($\\mu$-CT). Recently, a\nconvolutional neural network (CNN) was applied to predict the CT attenuation\nmap ($\\mu$-CNN) from $\\lambda$-MLAA and $\\mu$-MLAA, in which an image-domain\nloss (IM-loss) function between the $\\mu$-CNN and the ground truth $\\mu$-CT was\nused. However, IM-loss does not directly measure the AC errors according to the\nPET attenuation physics, where the line-integral projection of the attenuation\nmap ($\\mu$) along the path of the two annihilation events, instead of the $\\mu$\nitself, is used for AC. Therefore, a network trained with the IM-loss may yield\nsuboptimal performance in the $\\mu$ generation. Here, we propose a novel\nline-integral projection loss (LIP-loss) function that incorporates the PET\nattenuation physics for $\\mu$ generation. Eighty training and twenty testing\ndatasets of whole-body 18F-FDG PET and paired ground truth $\\mu$-CT were used.\nQuantitative evaluations showed that the model trained with the additional\nLIP-loss was able to significantly outperform the model trained solely based on\nthe IM-loss function.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 18:40:52 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Shi", "Luyao", ""], ["Onofrey", "John A.", ""], ["Revilla", "Enette Mae", ""], ["Toyonaga", "Takuya", ""], ["Menard", "David", ""], ["Ankrah", "Jo-seph", ""], ["Carson", "Richard E.", ""], ["Liu", "Chi", ""], ["Lu", "Yihuan", ""]]}, {"id": "1909.01412", "submitter": "Youshan Zhang", "authors": "Youshan Zhang, Jiarui Xing, Miaomiao Zhang", "title": "Mixture Probabilistic Principal Geodesic Analysis", "comments": "Seventh MICCAI Workshop on Mathematical Foundations of Computational\n  Anatomy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction on Riemannian manifolds is challenging due to the\ncomplex nonlinear data structures. While probabilistic principal geodesic\nanalysis~(PPGA) has been proposed to generalize conventional principal\ncomponent analysis (PCA) onto manifolds, its effectiveness is limited to data\nwith a single modality. In this paper, we present a novel Gaussian latent\nvariable model that provides a unique way to integrate multiple PGA models into\na maximum-likelihood framework. This leads to a well-defined mixture model of\nprobabilistic principal geodesic analysis (MPPGA) on sub-populations, where\nparameters of the principal subspaces are automatically estimated by employing\nan Expectation Maximization algorithm. We further develop a mixture Bayesian\nPGA (MBPGA) model that automatically reduces data dimensionality by suppressing\nirrelevant principal geodesics. We demonstrate the advantages of our model in\nthe contexts of clustering and statistical shape analysis, using synthetic\nsphere data, real corpus callosum, and mandible data from human brain magnetic\nresonance~(MR) and CT images.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 19:22:57 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 08:23:42 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Zhang", "Youshan", ""], ["Xing", "Jiarui", ""], ["Zhang", "Miaomiao", ""]]}, {"id": "1909.01417", "submitter": "Prerana Mukherjee", "authors": "Anupama Ray, Siddharth Kumar, Rutvik Reddy, Prerana Mukherjee, Ritu\n  Garg", "title": "Multi-level Attention network using text, audio and video for Depression\n  Prediction", "comments": "in Proceedings of the 9th International Workshop on Audio/Visual\n  Emotion Challenge, AVEC 2019, ACM Multimedia Workshop, Nice, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depression has been the leading cause of mental-health illness worldwide.\nMajor depressive disorder (MDD), is a common mental health disorder that\naffects both psychologically as well as physically which could lead to loss of\nlives. Due to the lack of diagnostic tests and subjectivity involved in\ndetecting depression, there is a growing interest in using behavioural cues to\nautomate depression diagnosis and stage prediction. The absence of labelled\nbehavioural datasets for such problems and the huge amount of variations\npossible in behaviour makes the problem more challenging. This paper presents a\nnovel multi-level attention based network for multi-modal depression prediction\nthat fuses features from audio, video and text modalities while learning the\nintra and inter modality relevance. The multi-level attention reinforces\noverall learning by selecting the most influential features within each\nmodality for the decision making. We perform exhaustive experimentation to\ncreate different regression models for audio, video and text modalities.\nSeveral fusions models with different configurations are constructed to\nunderstand the impact of each feature and modality. We outperform the current\nbaseline by 17.52% in terms of root mean squared error.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 19:40:38 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Ray", "Anupama", ""], ["Kumar", "Siddharth", ""], ["Reddy", "Rutvik", ""], ["Mukherjee", "Prerana", ""], ["Garg", "Ritu", ""]]}, {"id": "1909.01456", "submitter": "Paul Rosen", "authors": "Junyi Tu, Paul Rosen", "title": "Topologically-Guided Color Image Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enhancement is an important step in post-processing digital images for\npersonal use, in medical imaging, and for object recognition. Most existing\nmanual techniques rely on region selection, similarity, and/or thresholding for\nediting, never really considering the topological structure of the image. In\nthis paper, we leverage the contour tree to extract a hierarchical\nrepresentation of the topology of an image. We propose 4 topology-aware\ntransfer functions for editing features of the image using local topological\nproperties, instead of global image properties. Finally, we evaluate our\napproach with grayscale and color images.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 21:15:24 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Tu", "Junyi", ""], ["Rosen", "Paul", ""]]}, {"id": "1909.01497", "submitter": "Chen Zhao", "authors": "Chen Zhao, Jiaqi Yang, Ke Xian, Zhiguo Cao, Xin Li", "title": "Iterative Clustering with Game-Theoretic Matching for Robust\n  Multi-consistency Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Matching corresponding features between two images is a fundamental task to\ncomputer vision with numerous applications in object recognition, robotics, and\n3D reconstruction. Current state of the art in image feature matching has\nfocused on establishing a single consistency in static scenes; by contrast,\nfinding multiple consistencies in dynamic scenes has been under-researched. In\nthis paper, we present an end-to-end optimization framework named \"iterative\nclustering with Game-Theoretic Matching\" (ic-GTM) for robust multi-consistency\ncorrespondence. The key idea is to formulate multi-consistency matching as a\ngeneralized clustering problem for an image pair. In our formulation, several\nlocal matching games are simultaneously carried out in different corresponding\nblock pairs under the guidance of a novel payoff function consisting of both\ngeometric and descriptive compatibility; the global matching results are\nfurther iteratively refined by clustering and thresholding with respect to a\npayoff matrix. We also propose three new metrics for evaluating the performance\nof multi-consistency image feature matching. Extensive experimental results\nhave shown that the proposed framework significantly outperforms previous\nstate-of-the-art approaches on both singleconsistency and multi-consistency\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 23:20:34 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Zhao", "Chen", ""], ["Yang", "Jiaqi", ""], ["Xian", "Ke", ""], ["Cao", "Zhiguo", ""], ["Li", "Xin", ""]]}, {"id": "1909.01498", "submitter": "Avinash Kori", "authors": "Parth Natekar, Avinash Kori, Ganapathy Krishnamurthi", "title": "Demystifying Brain Tumour Segmentation Networks: Interpretability and\n  Uncertainty Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The accurate automatic segmentation of gliomas and its intra-tumoral\nstructures is important not only for treatment planning but also for follow-up\nevaluations. Several methods based on 2D and 3D Deep Neural Networks (DNN) have\nbeen developed to segment brain tumors and to classify different categories of\ntumors from different MRI modalities. However, these networks are often\nblack-box models and do not provide any evidence regarding the process they\ntake to perform this task. Increasing transparency and interpretability of such\ndeep learning techniques are necessary for the complete integration of such\nmethods into medical practice. In this paper, we explore various techniques to\nexplain the functional organization of brain tumor segmentation models and to\nextract visualizations of internal concepts to understand how these networks\nachieve highly accurate tumor segmentations. We use the BraTS 2018 dataset to\ntrain three different networks with standard architectures and outline\nsimilarities and differences in the process that these networks take to segment\nbrain tumors. We show that brain tumor segmentation networks learn certain\nhuman-understandable disentangled concepts on a filter level. We also show that\nthey take a top-down or hierarchical approach to localizing the different parts\nof the tumor. We then extract visualizations of some internal feature maps and\nalso provide a measure of uncertainty with regards to the outputs of the models\nto give additional qualitative evidence about the predictions of these\nnetworks. We believe that the emergence of such human-understandable\norganization and concepts might aid in the acceptance and integration of such\nmethods in medical diagnosis.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 23:53:11 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 06:29:21 GMT"}, {"version": "v3", "created": "Sat, 25 Jan 2020 03:04:49 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Natekar", "Parth", ""], ["Kori", "Avinash", ""], ["Krishnamurthi", "Ganapathy", ""]]}, {"id": "1909.01507", "submitter": "Siyuan Huang", "authors": "Yixin Chen, Siyuan Huang, Tao Yuan, Siyuan Qi, Yixin Zhu, Song-Chun\n  Zhu", "title": "Holistic++ Scene Understanding: Single-view 3D Holistic Scene Parsing\n  and Human Pose Estimation with Human-Object Interaction and Physical\n  Commonsense", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new 3D holistic++ scene understanding problem, which jointly\ntackles two tasks from a single-view image: (i) holistic scene parsing and\nreconstruction---3D estimations of object bounding boxes, camera pose, and room\nlayout, and (ii) 3D human pose estimation. The intuition behind is to leverage\nthe coupled nature of these two tasks to improve the granularity and\nperformance of scene understanding. We propose to exploit two critical and\nessential connections between these two tasks: (i) human-object interaction\n(HOI) to model the fine-grained relations between agents and objects in the\nscene, and (ii) physical commonsense to model the physical plausibility of the\nreconstructed scene. The optimal configuration of the 3D scene, represented by\na parse graph, is inferred using Markov chain Monte Carlo (MCMC), which\nefficiently traverses through the non-differentiable joint solution space.\nExperimental results demonstrate that the proposed algorithm significantly\nimproves the performance of the two tasks on three datasets, showing an\nimproved generalization ability.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 00:42:20 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Chen", "Yixin", ""], ["Huang", "Siyuan", ""], ["Yuan", "Tao", ""], ["Qi", "Siyuan", ""], ["Zhu", "Yixin", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1909.01520", "submitter": "Tyler Hayes", "authors": "Tyler L. Hayes and Christopher Kanan", "title": "Lifelong Machine Learning with Deep Streaming Linear Discriminant\n  Analysis", "comments": "To appear in the IEEE Conference on Computer Vision and Pattern\n  Recognition Workshop (CVPR-W) on Continual Learning in Computer Vision\n  (CLVision) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When an agent acquires new information, ideally it would immediately be\ncapable of using that information to understand its environment. This is not\npossible using conventional deep neural networks, which suffer from\ncatastrophic forgetting when they are incrementally updated, with new knowledge\noverwriting established representations. A variety of approaches have been\ndeveloped that attempt to mitigate catastrophic forgetting in the incremental\nbatch learning scenario, where a model learns from a series of large\ncollections of labeled samples. However, in this setting, inference is only\npossible after a batch has been accumulated, which prohibits many applications.\nAn alternative paradigm is online learning in a single pass through the\ntraining dataset on a resource constrained budget, which is known as streaming\nlearning. Streaming learning has been much less studied in the deep learning\ncommunity. In streaming learning, an agent learns instances one-by-one and can\nbe tested at any time, rather than only after learning a large batch. Here, we\nrevisit streaming linear discriminant analysis, which has been widely used in\nthe data mining research community. By combining streaming linear discriminant\nanalysis with deep learning, we are able to outperform both incremental batch\nlearning and streaming learning algorithms on both ImageNet ILSVRC-2012 and\nCORe50, a dataset that involves learning to classify from temporally ordered\nsamples.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 02:13:22 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 17:17:47 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 16:28:51 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Hayes", "Tyler L.", ""], ["Kanan", "Christopher", ""]]}, {"id": "1909.01524", "submitter": "Dakai Jin", "authors": "Dakai Jin, Dazhou Guo, Tsung-Ying Ho, Adam P. Harrison, Jing Xiao,\n  Chen-kan Tseng, and Le Lu", "title": "Accurate Esophageal Gross Tumor Volume Segmentation in PET/CT using\n  Two-Stream Chained 3D Deep Network Fusion", "comments": "MICCAI 2019 (early accept and oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gross tumor volume (GTV) segmentation is a critical step in esophageal cancer\nradiotherapy treatment planning. Inconsistencies across oncologists and\nprohibitive labor costs motivate automated approaches for this task. However,\nleading approaches are only applied to radiotherapy computed tomography (RTCT)\nimages taken prior to treatment. This limits the performance as RTCT suffers\nfrom low contrast between the esophagus, tumor, and surrounding tissues. In\nthis paper, we aim to exploit both RTCT and positron emission tomography (PET)\nimaging modalities to facilitate more accurate GTV segmentation. By utilizing\nPET, we emulate medical professionals who frequently delineate GTV boundaries\nthrough observation of the RTCT images obtained after prescribing radiotherapy\nand PET/CT images acquired earlier for cancer staging. To take advantage of\nboth modalities, we present a two-stream chained segmentation approach that\neffectively fuses the CT and PET modalities via early and late 3D\ndeep-network-based fusion. Furthermore, to effect the fusion and segmentation\nwe propose a simple yet effective progressive semantically nested network\n(PSNN) model that outperforms more complicated models. Extensive 5-fold\ncross-validation on 110 esophageal cancer patients, the largest analysis to\ndate, demonstrates that both the proposed two-stream chained segmentation\npipeline and the PSNN model can significantly improve the quantitative\nperformance over the previous state-of-the-art work by 11% in absolute Dice\nscore (DSC) (from 0.654 to 0.764) and, at the same time, reducing the Hausdorff\ndistance from 129 mm to 47 mm.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 02:24:58 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 02:59:32 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Jin", "Dakai", ""], ["Guo", "Dazhou", ""], ["Ho", "Tsung-Ying", ""], ["Harrison", "Adam P.", ""], ["Xiao", "Jing", ""], ["Tseng", "Chen-kan", ""], ["Lu", "Le", ""]]}, {"id": "1909.01526", "submitter": "Dakai Jin", "authors": "Dakai Jin, Dazhou Guo, Tsung-Ying Ho, Adam P. Harrison, Jing Xiao,\n  Chen-kan Tseng, and Le Lu", "title": "Deep Esophageal Clinical Target Volume Delineation using Encoded 3D\n  Spatial Context of Tumors, Lymph Nodes, and Organs At Risk", "comments": "MICCAI 2019 (early accept)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical target volume (CTV) delineation from radiotherapy computed\ntomography (RTCT) images is used to define the treatment areas containing the\ngross tumor volume (GTV) and/or sub-clinical malignant disease for radiotherapy\n(RT). High intra- and inter-user variability makes this a particularly\ndifficult task for esophageal cancer. This motivates automated solutions, which\nis the aim of our work. Because CTV delineation is highly context-dependent--it\nmust encompass the GTV and regional lymph nodes (LNs) while also avoiding\nexcessive exposure to the organs at risk (OARs)--we formulate it as a deep\ncontextual appearance-based problem using encoded spatial contexts of these\nanatomical structures. This allows the deep network to better learn from and\nemulate the margin- and appearance-based delineation performed by human\nphysicians. Additionally, we develop domain-specific data augmentation to\ninject robustness to our system. Finally, we show that a simple 3D progressive\nholistically nested network (PHNN), which avoids computationally heavy decoding\npaths while still aggregating features at different levels of context, can\noutperform more complicated networks. Cross-validated experiments on a dataset\nof 135 esophageal cancer patients demonstrate that our encoded spatial context\napproach can produce concrete performance improvements, with an average Dice\nscore of 83.9% and an average surface distance of 4.2 mm, representing\nimprovements of 3.8% and 2.4 mm, respectively, over the state-of-the-art\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 02:40:12 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 03:04:02 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Jin", "Dakai", ""], ["Guo", "Dazhou", ""], ["Ho", "Tsung-Ying", ""], ["Harrison", "Adam P.", ""], ["Xiao", "Jing", ""], ["Tseng", "Chen-kan", ""], ["Lu", "Le", ""]]}, {"id": "1909.01532", "submitter": "Xin Zhong", "authors": "Yucong Shen, Xin Zhong, Frank Y. Shih", "title": "Deep Morphological Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical morphology is a theory and technique to collect features like\ngeometric and topological structures in digital images. Given a target image,\ndetermining suitable morphological operations and structuring elements is a\ncumbersome and time-consuming task. In this paper, a morphological neural\nnetwork is proposed to address this problem. Serving as a nonlinear feature\nextracting layer in deep learning frameworks, the efficiency of the proposed\nmorphological layer is confirmed analytically and empirically. With a known\ntarget, a single-filter morphological layer learns the structuring element\ncorrectly, and an adaptive layer can automatically select appropriate\nmorphological operations. For practical applications, the proposed\nmorphological neural networks are tested on several classification datasets\nrelated to shape or geometric image features, and the experimental results have\nconfirmed the high computational efficiency and high accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 03:03:20 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Shen", "Yucong", ""], ["Zhong", "Xin", ""], ["Shih", "Frank Y.", ""]]}, {"id": "1909.01542", "submitter": "Yang Li", "authors": "Yang Li, Jianhe Yuan, Zhiqun Zhao, Hao Sun, Zhihai He", "title": "Snowball: Iterative Model Evolution and Confident Sample Discovery for\n  Semi-Supervised Learning on Very Small Labeled Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop a joint sample discovery and iterative model\nevolution method for semi-supervised learning on very small labeled training\nsets. We propose a master-teacher-student model framework to provide\nmulti-layer guidance during the model evolution process with multiple\niterations and generations. The teacher model is constructed by performing an\nexponential moving average of the student models obtained from past training\nsteps. The master network combines the knowledge of the student and teacher\nmodels with additional access to newly discovered samples. The master and\nteacher models are then used to guide the training of the student network by\nenforcing the consistence between their predictions of unlabeled samples and\nevolve all models when more and more samples are discovered. Our extensive\nexperiments demonstrate that the discovering confident samples from the\nunlabeled dataset, once coupled with the above master-teacher-student network\nevolution, can significantly improve the overall semi-supervised learning\nperformance. For example, on the CIFAR-10 dataset, with a very small set of 250\nlabeled samples, our method achieves an error rate of 11.81 %, more than 38 %\nlower than the state-of-the-art method Mean-Teacher (49.91 %).\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 03:41:27 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Li", "Yang", ""], ["Yuan", "Jianhe", ""], ["Zhao", "Zhiqun", ""], ["Sun", "Hao", ""], ["He", "Zhihai", ""]]}, {"id": "1909.01547", "submitter": "Prerana Mukherjee", "authors": "Ajit Jadhav, Prerana Mukherjee, Vinay Kaushik, Brejesh Lall", "title": "Aerial multi-object tracking by detection using deep association\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot a research is focused on object detection and it has achieved\nsignificant advances with deep learning techniques in recent years. Inspite of\nthe existing research, these algorithms are not usually optimal for dealing\nwith sequences or images captured by drone-based platforms, due to various\nchallenges such as view point change, scales, density of object distribution\nand occlusion. In this paper, we develop a model for detection of objects in\ndrone images using the VisDrone2019 DET dataset. Using the RetinaNet model as\nour base, we modify the anchor scales to better handle the detection of dense\ndistribution and small size of the objects. We explicitly model the channel\ninterdependencies by using \"Squeeze-and-Excitation\" (SE) blocks that adaptively\nrecalibrates channel-wise feature responses. This helps to bring significant\nimprovements in performance at a slight additional computational cost. Using\nthis architecture for object detection, we build a custom DeepSORT network for\nobject detection on the VisDrone2019 MOT dataset by training a custom Deep\nAssociation network for the algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 03:52:37 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Jadhav", "Ajit", ""], ["Mukherjee", "Prerana", ""], ["Kaushik", "Vinay", ""], ["Lall", "Brejesh", ""]]}, {"id": "1909.01585", "submitter": "Guillaume Noyel", "authors": "Guillaume Noyel (IPRI, SIGPH@iPRI), Michel Jourlin (LHC, IPRI)", "title": "Functional Asplund metrics for pattern matching, robust to variable\n  lighting conditions", "comments": null, "journal-ref": "Image Analysis and Stereology, International Society for\n  Stereology, 2020, 39 (2), pp.53--71", "doi": "10.5566/ias.2292", "report-no": null, "categories": "cs.CV cs.NA eess.SP math.FA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a complete framework to process images captured\nunder uncontrolled lighting and especially under low lighting. By taking\nadvantage of the Logarithmic Image Processing (LIP) context, we study two novel\nfunctional metrics: i) the LIP-multiplicative Asplund metric which is robust to\nobject absorption variations and ii) the LIP-additive Asplund metric which is\nrobust to variations of source intensity or camera exposure-time. We introduce\nrobust to noise versions of these metrics. We demonstrate that the maps of\ntheir corresponding distances between an image and a reference template are\nlinked to Mathematical Morphology. This facilitates their implementation. We\nassess them in various situations with different lightings and movement.\nResults show that those maps of distances are robust to lighting variations.\nImportantly, they are efficient to detect patterns in low-contrast images with\na template acquired under a different lighting.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 07:26:03 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 07:17:58 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Noyel", "Guillaume", "", "IPRI, SIGPH@iPRI"], ["Jourlin", "Michel", "", "LHC, IPRI"]]}, {"id": "1909.01595", "submitter": "Tomer Cohen", "authors": "Tomer Cohen and Lior Wolf", "title": "Bidirectional One-Shot Unsupervised Domain Mapping", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of mapping between a domain $A$, in which there is a\nsingle training sample and a domain $B$, for which we have a richer training\nset. The method we present is able to perform this mapping in both directions.\nFor example, we can transfer all MNIST images to the visual domain captured by\na single SVHN image and transform the SVHN image to the domain of the MNIST\nimages. Our method is based on employing one encoder and one decoder for each\ndomain, without utilizing weight sharing. The autoencoder of the single sample\ndomain is trained to match both this sample and the latent space of domain $B$.\nOur results demonstrate convincing mapping between domains, where either the\nsource or the target domain are defined by a single sample, far surpassing\nexisting solutions. Our code is made publicly available at\nhttps://github.com/tomercohen11/BiOST\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 07:49:18 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Cohen", "Tomer", ""], ["Wolf", "Lior", ""]]}, {"id": "1909.01616", "submitter": "Naiyu Gao", "authors": "Naiyu Gao, Yanhu Shan, Yupei Wang, Xin Zhao, Yinan Yu, Ming Yang,\n  Kaiqi Huang", "title": "SSAP: Single-Shot Instance Segmentation With Affinity Pyramid", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, proposal-free instance segmentation has received increasing\nattention due to its concise and efficient pipeline. Generally, proposal-free\nmethods generate instance-agnostic semantic segmentation labels and\ninstance-aware features to group pixels into different object instances.\nHowever, previous methods mostly employ separate modules for these two\nsub-tasks and require multiple passes for inference. We argue that treating\nthese two sub-tasks separately is suboptimal. In fact, employing multiple\nseparate modules significantly reduces the potential for application. The\nmutual benefits between the two complementary sub-tasks are also unexplored. To\nthis end, this work proposes a single-shot proposal-free instance segmentation\nmethod that requires only one single pass for prediction. Our method is based\non a pixel-pair affinity pyramid, which computes the probability that two\npixels belong to the same instance in a hierarchical manner. The affinity\npyramid can also be jointly learned with the semantic class labeling and\nachieve mutual benefits. Moreover, incorporating with the learned affinity\npyramid, a novel cascaded graph partition module is presented to sequentially\ngenerate instances from coarse to fine. Unlike previous time-consuming graph\npartition methods, this module achieves $5\\times$ speedup and 9% relative\nimprovement on Average-Precision (AP). Our approach achieves state-of-the-art\nresults on the challenging Cityscapes dataset.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 08:33:07 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Gao", "Naiyu", ""], ["Shan", "Yanhu", ""], ["Wang", "Yupei", ""], ["Zhao", "Xin", ""], ["Yu", "Yinan", ""], ["Yang", "Ming", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1909.01643", "submitter": "Xin Kong", "authors": "Xin Kong, Guangyao Zhai, Baoquan Zhong, Yong Liu", "title": "PASS3D: Precise and Accelerated Semantic Segmentation for 3D Point Cloud", "comments": "This paper has been accepted by IROS-2019", "journal-ref": null, "doi": "10.1109/IROS40897.2019.8968296", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose PASS3D to achieve point-wise semantic segmentation\nfor 3D point cloud. Our framework combines the efficiency of traditional\ngeometric methods with robustness of deep learning methods, consisting of two\nstages: At stage-1, our accelerated cluster proposal algorithm will generate\nrefined cluster proposals by segmenting point clouds without ground, capable of\ngenerating less redundant proposals with higher recall in an extremely short\ntime; stage-2 we will amplify and further process these proposals by a neural\nnetwork to estimate semantic label for each point and meanwhile propose a novel\ndata augmentation method to enhance the network's recognition capability for\nall categories especially for non-rigid objects. Evaluated on KITTI raw\ndataset, PASS3D stands out against the state-of-the-art on some results, making\nitself competent to 3D perception in autonomous driving system. Our source code\nwill be open-sourced. A video demonstration is available at\nhttps://www.youtube.com/watch?v=cukEqDuP_Qw.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 09:26:28 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 09:42:09 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Kong", "Xin", ""], ["Zhai", "Guangyao", ""], ["Zhong", "Baoquan", ""], ["Liu", "Yong", ""]]}, {"id": "1909.01647", "submitter": "Raabid Hussain", "authors": "Raabid Hussain (ImVia), Alain Lalande (Le2i), Kibrom Berihu Girum\n  (Le2i), Caroline Guigou (Le2i), Alexis Bozorg Grayeli (Le2i)", "title": "3D landmark detection for augmented reality based otologic procedures", "comments": null, "journal-ref": "Surgetica, Jun 2019, Rennes, France", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ear consists of the smallest bones in the human body and does not contain\nsignificant amount of distinct landmark points that may be used to register a\npreoperative CT-scan with the surgical video in an augmented reality framework.\nLearning based algorithms may be used to help the surgeons to identify landmark\npoints. This paper presents a convolutional neural network approach to landmark\ndetection in preoperative ear CT images and then discusses an augmented reality\nsystem that can be used to visualize the cochlear axis on an otologic surgical\nvideo.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 09:31:22 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Hussain", "Raabid", "", "ImVia"], ["Lalande", "Alain", "", "Le2i"], ["Girum", "Kibrom Berihu", "", "Le2i"], ["Guigou", "Caroline", "", "Le2i"], ["Grayeli", "Alexis Bozorg", "", "Le2i"]]}, {"id": "1909.01671", "submitter": "Hal Ccsd", "authors": "Nicolas Audebert (OBELIX), Alexandre Boulch, Bertrand Le Saux,\n  S\\'ebastien Lef\\`evre (OBELIX)", "title": "Distance transform regression for spatially-aware deep semantic\n  segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding visual scenes relies more and more on dense pixel-wise\nclassification obtained via deep fully convolutional neural networks. However,\ndue to the nature of the networks, predictions often suffer from blurry\nboundaries and ill-segmented shapes, fueling the need for post-processing. This\nwork introduces a new semantic segmentation regularization based on the\nregression of a distance transform. After computing the distance transform on\nthe label masks, we train a FCN in a multi-task setting in both discrete and\ncontinuous spaces by learning jointly classification and distance regression.\nThis requires almost no modification of the network structure and adds a very\nlow overhead to the training process. Learning to approximate the distance\ntransform back-propagates spatial cues that implicitly regularizes the\nsegmentation. We validate this technique with several architectures on various\ndatasets, and we show significant improvements compared to competitive\nbaselines.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 10:05:08 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Audebert", "Nicolas", "", "OBELIX"], ["Boulch", "Alexandre", "", "OBELIX"], ["Saux", "Bertrand Le", "", "OBELIX"], ["Lef\u00e8vre", "S\u00e9bastien", "", "OBELIX"]]}, {"id": "1909.01700", "submitter": "Chengzhu Yu", "authors": "Chengzhu Yu, Heng Lu, Na Hu, Meng Yu, Chao Weng, Kun Xu, Peng Liu,\n  Deyi Tuo, Shiyin Kang, Guangzhi Lei, Dan Su, Dong Yu", "title": "DurIAN: Duration Informed Attention Network For Multimodal Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a generic and robust multimodal synthesis system\nthat produces highly natural speech and facial expression simultaneously. The\nkey component of this system is the Duration Informed Attention Network\n(DurIAN), an autoregressive model in which the alignments between the input\ntext and the output acoustic features are inferred from a duration model. This\nis different from the end-to-end attention mechanism used, and accounts for\nvarious unavoidable artifacts, in existing end-to-end speech synthesis systems\nsuch as Tacotron. Furthermore, DurIAN can be used to generate high quality\nfacial expression which can be synchronized with generated speech with/without\nparallel speech and face data. To improve the efficiency of speech generation,\nwe also propose a multi-band parallel generation strategy on top of the WaveRNN\nmodel. The proposed Multi-band WaveRNN effectively reduces the total\ncomputational complexity from 9.8 to 5.5 GFLOPS, and is able to generate audio\nthat is 6 times faster than real time on a single CPU core. We show that DurIAN\ncould generate highly natural speech that is on par with current state of the\nart end-to-end systems, while at the same time avoid word skipping/repeating\nerrors in those systems. Finally, a simple yet effective approach for\nfine-grained control of expressiveness of speech and facial expression is\nintroduced.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 11:35:48 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 22:35:53 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Yu", "Chengzhu", ""], ["Lu", "Heng", ""], ["Hu", "Na", ""], ["Yu", "Meng", ""], ["Weng", "Chao", ""], ["Xu", "Kun", ""], ["Liu", "Peng", ""], ["Tuo", "Deyi", ""], ["Kang", "Shiyin", ""], ["Lei", "Guangzhi", ""], ["Su", "Dan", ""], ["Yu", "Dong", ""]]}, {"id": "1909.01754", "submitter": "Rayson Laroca", "authors": "Rayson Laroca, Luiz A. Zanlorensi, Gabriel R. Gon\\c{c}alves, Eduardo\n  Todt, William Robson Schwartz, David Menotti", "title": "An Efficient and Layout-Independent Automatic License Plate Recognition\n  System Based on the YOLO detector", "comments": null, "journal-ref": "IET Intelligent Transport Systems, vol. 15, no. 4, pp. 483-503,\n  2021", "doi": "10.1049/itr2.12030", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient and layout-independent Automatic License\nPlate Recognition (ALPR) system based on the state-of-the-art YOLO object\ndetector that contains a unified approach for license plate (LP) detection and\nlayout classification to improve the recognition results using post-processing\nrules. The system is conceived by evaluating and optimizing different models,\naiming at achieving the best speed/accuracy trade-off at each stage. The\nnetworks are trained using images from several datasets, with the addition of\nvarious data augmentation techniques, so that they are robust under different\nconditions. The proposed system achieved an average end-to-end recognition rate\nof 96.9% across eight public datasets (from five different regions) used in the\nexperiments, outperforming both previous works and commercial systems in the\nChineseLP, OpenALPR-EU, SSIG-SegPlate and UFPR-ALPR datasets. In the other\ndatasets, the proposed approach achieved competitive results to those attained\nby the baselines. Our system also achieved impressive frames per second (FPS)\nrates on a high-end GPU, being able to perform in real time even when there are\nfour vehicles in the scene. An additional contribution is that we manually\nlabeled 38,351 bounding boxes on 6,239 images from public datasets and made the\nannotations publicly available to the research community.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 12:51:51 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 16:24:18 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2021 15:21:24 GMT"}, {"version": "v4", "created": "Tue, 9 Mar 2021 15:36:30 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Laroca", "Rayson", ""], ["Zanlorensi", "Luiz A.", ""], ["Gon\u00e7alves", "Gabriel R.", ""], ["Todt", "Eduardo", ""], ["Schwartz", "William Robson", ""], ["Menotti", "David", ""]]}, {"id": "1909.01763", "submitter": "Yin Zhao", "authors": "Jie Zhang, Yin Zhao, Longjun Cai, Chaoping Tu, Wu Wei", "title": "Video Affective Effects Prediction with Multi-modal Fusion and Shot-Long\n  Temporal Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the emotional impact of videos using machine learning is a\nchallenging task considering the varieties of modalities, the complicated\ntemporal contex of the video as well as the time dependency of the emotional\nstates. Feature extraction, multi-modal fusion and temporal context fusion are\ncrucial stages for predicting valence and arousal values in the emotional\nimpact, but have not been successfully exploited. In this paper, we propose a\ncomprehensive framework with novel designs of modal structure and multi-modal\nfusion strategy. We select the most suitable modalities for valence and arousal\ntasks respectively and each modal feature is extracted using the\nmodality-specific pre-trained deep model on large generic dataset.\nTwo-time-scale structures, one for the intra-clip and the other for the\ninter-clip, are proposed to capture the temporal dependency of video content\nand emotion states. To combine the complementary information from multiple\nmodalities, an effective and efficient residual-based progressive training\nstrategy is proposed. Each modality is step-wisely combined into the\nmulti-modal model, responsible for completing the missing parts of features.\nWith all those improvements above, our proposed prediction framework achieves\nbetter performance on the LIRIS-ACCEDE dataset with a large margin compared to\nthe state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 07:22:20 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Zhang", "Jie", ""], ["Zhao", "Yin", ""], ["Cai", "Longjun", ""], ["Tu", "Chaoping", ""], ["Wei", "Wu", ""]]}, {"id": "1909.01802", "submitter": "Chiara Ravazzi", "authors": "Diego Valsesia, Sophie Marie Fosson, Chiara Ravazzi, Tiziano Bianchi,\n  Enrico Magli", "title": "Analysis of SparseHash: an efficient embedding of set-similarity via\n  sparse projections", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Embeddings provide compact representations of signals in order to perform\nefficient inference in a wide variety of tasks. In particular, random\nprojections are common tools to construct Euclidean distance-preserving\nembeddings, while hashing techniques are extensively used to embed\nset-similarity metrics, such as the Jaccard coefficient. In this letter, we\ntheoretically prove that a class of random projections based on sparse\nmatrices, called SparseHash, can preserve the Jaccard coefficient between the\nsupports of sparse signals, which can be used to estimate set similarities.\nMoreover, besides the analysis, we provide an efficient implementation and we\ntest the performance in several numerical experiments, both on synthetic and\nreal datasets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 15:41:54 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Valsesia", "Diego", ""], ["Fosson", "Sophie Marie", ""], ["Ravazzi", "Chiara", ""], ["Bianchi", "Tiziano", ""], ["Magli", "Enrico", ""]]}, {"id": "1909.01804", "submitter": "Zhanghan Ke", "authors": "Zhanghan Ke and Daoye Wang and Qiong Yan and Jimmy Ren and Rynson W.H.\n  Lau", "title": "Dual Student: Breaking the Limits of the Teacher in Semi-supervised\n  Learning", "comments": "International Conference in Computer Vision 2019 (ICCV 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, consistency-based methods have achieved state-of-the-art results in\nsemi-supervised learning (SSL). These methods always involve two roles, an\nexplicit or implicit teacher model and a student model, and penalize\npredictions under different perturbations by a consistency constraint. However,\nthe weights of these two roles are tightly coupled since the teacher is\nessentially an exponential moving average (EMA) of the student. In this work,\nwe show that the coupled EMA teacher causes a performance bottleneck. To\naddress this problem, we introduce Dual Student, which replaces the teacher\nwith another student. We also define a novel concept, stable sample, following\nwhich a stabilization constraint is designed for our structure to be trainable.\nFurther, we discuss two variants of our method, which produce even higher\nperformance. Extensive experiments show that our method improves the\nclassification performance significantly on several main SSL benchmarks.\nSpecifically, it reduces the error rate of the 13-layer CNN from 16.84% to\n12.39% on CIFAR-10 with 1k labels and from 34.10% to 31.56% on CIFAR-100 with\n10k labels. In addition, our method also achieves a clear improvement in domain\nadaptation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 17:32:11 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Ke", "Zhanghan", ""], ["Wang", "Daoye", ""], ["Yan", "Qiong", ""], ["Ren", "Jimmy", ""], ["Lau", "Rynson W. H.", ""]]}, {"id": "1909.01815", "submitter": "Bernhard Egger", "authors": "Bernhard Egger, William A. P. Smith, Ayush Tewari, Stefanie Wuhrer,\n  Michael Zollhoefer, Thabo Beeler, Florian Bernard, Timo Bolkart, Adam\n  Kortylewski, Sami Romdhani, Christian Theobalt, Volker Blanz, Thomas Vetter", "title": "3D Morphable Face Models -- Past, Present and Future", "comments": "ACM Transactions on Graphics (TOG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a detailed survey of 3D Morphable Face Models over\nthe 20 years since they were first proposed. The challenges in building and\napplying these models, namely capture, modeling, image formation, and image\nanalysis, are still active research topics, and we review the state-of-the-art\nin each of these areas. We also look ahead, identifying unsolved challenges,\nproposing directions for future research and highlighting the broad range of\ncurrent and future applications.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 16:49:53 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 13:56:31 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Egger", "Bernhard", ""], ["Smith", "William A. P.", ""], ["Tewari", "Ayush", ""], ["Wuhrer", "Stefanie", ""], ["Zollhoefer", "Michael", ""], ["Beeler", "Thabo", ""], ["Bernard", "Florian", ""], ["Bolkart", "Timo", ""], ["Kortylewski", "Adam", ""], ["Romdhani", "Sami", ""], ["Theobalt", "Christian", ""], ["Blanz", "Volker", ""], ["Vetter", "Thomas", ""]]}, {"id": "1909.01818", "submitter": "Xiaoli Liu", "authors": "Xiaoli Liu, Jianqin Yin, Huaping Liu, Yilong Yin", "title": "PISEP^2: Pseudo Image Sequence Evolution based 3D Pose Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose prediction is to predict future poses given a window of previous poses.\nIn this paper, we propose a new problem that predicts poses using 3D joint\ncoordinate sequences. Different from the traditional pose prediction based on\nMocap frames, this problem is convenient to use in real applications due to its\nsimple sensors to capture data. We also present a new framework, PISEP^2\n(Pseudo Image Sequence Evolution based 3D Pose Prediction), to address this new\nproblem. Specifically, a skeletal representation is proposed by transforming\nthe joint coordinate sequence into an image sequence, which can model the\ndifferent correlations of different joints. With this image based skeletal\nrepresentation, we model the pose prediction as the evolution of image\nsequence. Moreover, a novel inference network is proposed to predict all future\nposes in one step by decoupling the decoders in a non-recursive manner.\nCompared with the recursive sequence to sequence model, we can improve the\ncomputational efficiency and avoid error accumulation significantly. Extensive\nexperiments are carried out on two benchmark datasets (e.g. G3D and FNTU). The\nproposed method achieves the state-of-the-art performance on both datasets,\nwhich demonstrates the effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 14:10:49 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Liu", "Xiaoli", ""], ["Yin", "Jianqin", ""], ["Liu", "Huaping", ""], ["Yin", "Yilong", ""]]}, {"id": "1909.01840", "submitter": "Bin Yan", "authors": "Bin Yan, Haojie Zhao, Dong Wang, Huchuan Lu and Xiaoyun Yang", "title": "'Skimming-Perusal' Tracking: A Framework for Real-Time and Robust\n  Long-term Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with traditional short-term tracking, long-term tracking poses more\nchallenges and is much closer to realistic applications. However, few works\nhave been done and their performance have also been limited. In this work, we\npresent a novel robust and real-time long-term tracking framework based on the\nproposed skimming and perusal modules. The perusal module consists of an\neffective bounding box regressor to generate a series of candidate proposals\nand a robust target verifier to infer the optimal candidate with its confidence\nscore. Based on this score, our tracker determines whether the tracked object\nbeing present or absent, and then chooses the tracking strategies of local\nsearch or global search respectively in the next frame. To speed up the\nimage-wide global search, a novel skimming module is designed to efficiently\nchoose the most possible regions from a large number of sliding windows.\nNumerous experimental results on the VOT-2018 long-term and OxUvA long-term\nbenchmarks demonstrate that the proposed method achieves the best performance\nand runs in real-time. The source codes are available at\nhttps://github.com/iiau-tracker/SPLT.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 14:34:24 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Yan", "Bin", ""], ["Zhao", "Haojie", ""], ["Wang", "Dong", ""], ["Lu", "Huchuan", ""], ["Yang", "Xiaoyun", ""]]}, {"id": "1909.01860", "submitter": "Shiv Ram Dubey", "authors": "Yash Srivastava, Vaishnav Murali, Shiv Ram Dubey, Snehasis Mukherjee", "title": "Visual Question Answering using Deep Learning: A Survey and Performance\n  Analysis", "comments": "Accepted in Fifth IAPR International Conference on Computer Vision\n  and Image Processing (CVIP), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Visual Question Answering (VQA) task combines challenges for processing\ndata with both Visual and Linguistic processing, to answer basic `common sense'\nquestions about given images. Given an image and a question in natural\nlanguage, the VQA system tries to find the correct answer to it using visual\nelements of the image and inference gathered from textual questions. In this\nsurvey, we cover and discuss the recent datasets released in the VQA domain\ndealing with various types of question-formats and robustness of the\nmachine-learning models. Next, we discuss about new deep learning models that\nhave shown promising results over the VQA datasets. At the end, we present and\ndiscuss some of the results computed by us over the vanilla VQA model, Stacked\nAttention Network and the VQA Challenge 2017 winner model. We also provide the\ndetailed analysis along with the challenges and future research directions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 07:03:03 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 01:11:29 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Srivastava", "Yash", ""], ["Murali", "Vaishnav", ""], ["Dubey", "Shiv Ram", ""], ["Mukherjee", "Snehasis", ""]]}, {"id": "1909.01861", "submitter": "Hui Zhu", "authors": "Hui Zhu, Zhulin An, Chuanguang Yang, Xiaolong Hu, Kaiqiang Xu, Yongjun\n  Xu", "title": "Rethinking the Number of Channels for the Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latest algorithms for automatic neural architecture search perform remarkable\nbut few of them can effectively design the number of channels for convolutional\nneural networks and consume less computational efforts. In this paper, we\npropose a method for efficient automatic architecture search which is special\nto the widths of networks instead of the connections of neural architecture.\nOur method, functionally incremental search based on function-preserving, will\nexplore the number of channels rapidly while controlling the number of\nparameters of the target network. On CIFAR-10 and CIFAR-100 classification, our\nmethod using minimal computational resources (0.4~1.3 GPU-days) can discover\nmore efficient rules of the widths of networks to improve the accuracy by about\n0.5% on CIFAR-10 and a~2.33% on CIFAR-100 with fewer number of parameters. In\nparticular, our method is suitable for exploring the number of channels of\nalmost any convolutional neural network rapidly.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 15:09:22 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Zhu", "Hui", ""], ["An", "Zhulin", ""], ["Yang", "Chuanguang", ""], ["Hu", "Xiaolong", ""], ["Xu", "Kaiqiang", ""], ["Xu", "Yongjun", ""]]}, {"id": "1909.01867", "submitter": "Guizhong Liu", "authors": "Jiaojiao Fang, Lingtao Zhou, Guizhong Liu", "title": "3D Bounding Box Estimation for Autonomous Vehicles by Cascaded Geometric\n  Constraints and Depurated 2D Detections Using 3D Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection is one of the most important tasks in 3D vision\nperceptual system of autonomous vehicles. In this paper, we propose a novel two\nstage 3D object detection method aimed at get the optimal solution of object\nlocation in 3D space based on regressing two additional 3D object properties by\na deep convolutional neural network and combined with cascaded geometric\nconstraints between the 2D and 3D boxes. First, we modify the existing 3D\nproperties regressing network by adding two additional components, viewpoints\nclassification and the center projection of the 3D bounding box s bottom face.\nSecond, we use the predicted center projection combined with similar triangle\nconstraint to acquire an initial 3D bounding box by a closed-form solution.\nThen, the location predicted by previous step is used as the initial value of\nthe over-determined equations constructed by 2D and 3D boxes fitting constraint\nwith the configuration determined with the classified viewpoint. Finally, we\nuse the recovered physical world information by the 3D detections to filter out\nthe false detection and false alarm in 2D detections. We compare our method\nwith the state-of-the-arts on the KITTI dataset show that although conceptually\nsimple, our method outperforms more complex and computational expensive methods\nnot only by improving the overall precision of 3D detections, but also\nincreasing the orientation estimation precision. Furthermore our method can\ndeal with the truncated objects to some extent and remove the false alarm and\nfalse detections in both 2D and 3D detections.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 11:50:17 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Fang", "Jiaojiao", ""], ["Zhou", "Lingtao", ""], ["Liu", "Guizhong", ""]]}, {"id": "1909.01871", "submitter": "Khanh Nguyen", "authors": "Khanh Nguyen and Hal Daum\\'e III", "title": "Help, Anna! Visual Navigation with Natural Multimodal Assistance via\n  Retrospective Curiosity-Encouraging Imitation Learning", "comments": "In EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile agents that can leverage help from humans can potentially accomplish\nmore complex tasks than they could entirely on their own. We develop \"Help,\nAnna!\" (HANNA), an interactive photo-realistic simulator in which an agent\nfulfills object-finding tasks by requesting and interpreting natural\nlanguage-and-vision assistance. An agent solving tasks in a HANNA environment\ncan leverage simulated human assistants, called ANNA (Automatic Natural\nNavigation Assistants), which, upon request, provide natural language and\nvisual instructions to direct the agent towards the goals. To address the HANNA\nproblem, we develop a memory-augmented neural agent that hierarchically models\nmultiple levels of decision-making, and an imitation learning algorithm that\nteaches the agent to avoid repeating past mistakes while simultaneously\npredicting its own chances of making future progress. Empirically, our approach\nis able to ask for help more effectively than competitive baselines and, thus,\nattains higher task success rate on both previously seen and previously unseen\nenvironments. We publicly release code and data at\nhttps://github.com/khanhptnk/hanna . A video demo is available at\nhttps://youtu.be/18P94aaaLKg .\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 15:20:01 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 16:16:31 GMT"}, {"version": "v3", "created": "Sun, 15 Sep 2019 03:30:13 GMT"}, {"version": "v4", "created": "Tue, 8 Oct 2019 16:08:40 GMT"}, {"version": "v5", "created": "Mon, 21 Oct 2019 07:12:17 GMT"}, {"version": "v6", "created": "Fri, 22 Nov 2019 16:11:17 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Nguyen", "Khanh", ""], ["Daum\u00e9", "Hal", "III"]]}, {"id": "1909.01887", "submitter": "Davide Barbieri", "authors": "Davide Barbieri, Carlos Cabrelli, Eugenio Hern\\'andez, Ursula Molter", "title": "Optimal translational-rotational invariant dictionaries for images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NA math.FA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the construction of a set of square matrices whose translates and\nrotates provide a Parseval frame that is optimal for approximating a given\ndataset of images. Our approach is based on abstract harmonic analysis\ntechniques. Optimality is considered with respect to the quadratic error of\napproximation of the images in the dataset with their projection onto a linear\nsubspace that is invariant under translations and rotations. In addition, we\nprovide an elementary and fully self-contained proof of optimality, and the\nnumerical results from datasets of natural images.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 15:34:04 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Barbieri", "Davide", ""], ["Cabrelli", "Carlos", ""], ["Hern\u00e1ndez", "Eugenio", ""], ["Molter", "Ursula", ""]]}, {"id": "1909.01891", "submitter": "Carole Sudre", "authors": "Carole H. Sudre, Beatriz Gomez Anson, Silvia Ingala, Chris D. Lane,\n  Daniel Jimenez, Lukas Haider, Thomas Varsavsky, Ryutaro Tanno, Lorna Smith,\n  S\\'ebastien Ourselin, Rolf H. J\\\"ager, M. Jorge Cardoso", "title": "Let's agree to disagree: learning highly debatable multirater labelling", "comments": "Accepted at MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification and differentiation of small pathological objects may greatly\nvary among human raters due to differences in training, expertise and their\nconsistency over time. In a radiological setting, objects commonly have high\nwithin-class appearance variability whilst sharing certain characteristics\nacross different classes, making their distinction even more difficult. As an\nexample, markers of cerebral small vessel disease, such as enlarged\nperivascular spaces (EPVS) and lacunes, can be very varied in their appearance\nwhile exhibiting high inter-class similarity, making this task highly\nchallenging for human raters. In this work, we investigate joint models of\nindividual rater behaviour and multirater consensus in a deep learning setting,\nand apply it to a brain lesion object-detection task. Results show that jointly\nmodelling both individual and consensus estimates leads to significant\nimprovements in performance when compared to directly predicting consensus\nlabels, while also allowing the characterization of human-rater consistency.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 15:40:14 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Sudre", "Carole H.", ""], ["Anson", "Beatriz Gomez", ""], ["Ingala", "Silvia", ""], ["Lane", "Chris D.", ""], ["Jimenez", "Daniel", ""], ["Haider", "Lukas", ""], ["Varsavsky", "Thomas", ""], ["Tanno", "Ryutaro", ""], ["Smith", "Lorna", ""], ["Ourselin", "S\u00e9bastien", ""], ["J\u00e4ger", "Rolf H.", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "1909.01939", "submitter": "Pengfei Zhang", "authors": "Pengfei Zhang, Jianru Xue, Cuiling Lan, Wenjun Zeng, Zhanning Gao,\n  Nanning Zheng", "title": "EleAtt-RNN: Adding Attentiveness to Neurons in Recurrent Neural Networks", "comments": "IEEE Transactions on Image Processing (Accept). arXiv admin note:\n  substantial text overlap with arXiv:1807.04445", "journal-ref": null, "doi": "10.1109/TIP.2019.2937724", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are capable of modeling temporal\ndependencies of complex sequential data. In general, current available\nstructures of RNNs tend to concentrate on controlling the contributions of\ncurrent and previous information. However, the exploration of different\nimportance levels of different elements within an input vector is always\nignored. We propose a simple yet effective Element-wise-Attention Gate\n(EleAttG), which can be easily added to an RNN block (e.g. all RNN neurons in\nan RNN layer), to empower the RNN neurons to have attentiveness capability. For\nan RNN block, an EleAttG is used for adaptively modulating the input by\nassigning different levels of importance, i.e., attention, to each\nelement/dimension of the input. We refer to an RNN block equipped with an\nEleAttG as an EleAtt-RNN block. Instead of modulating the input as a whole, the\nEleAttG modulates the input at fine granularity, i.e., element-wise, and the\nmodulation is content adaptive. The proposed EleAttG, as an additional\nfundamental unit, is general and can be applied to any RNN structures, e.g.,\nstandard RNN, Long Short-Term Memory (LSTM), or Gated Recurrent Unit (GRU). We\ndemonstrate the effectiveness of the proposed EleAtt-RNN by applying it to\ndifferent tasks including the action recognition, from both skeleton-based data\nand RGB videos, gesture recognition, and sequential MNIST classification.\nExperiments show that adding attentiveness through EleAttGs to RNN blocks\nsignificantly improves the power of RNNs.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 08:15:09 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Zhang", "Pengfei", ""], ["Xue", "Jianru", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Gao", "Zhanning", ""], ["Zheng", "Nanning", ""]]}, {"id": "1909.01940", "submitter": "Eduardo Pooch", "authors": "Eduardo H. P. Pooch, Pedro L. Ballester, Rodrigo C. Barros", "title": "Can we trust deep learning models diagnosis? The impact of domain shift\n  in chest radiograph classification", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning models become more widespread, their ability to handle\nunseen data and generalize for any scenario is yet to be challenged. In medical\nimaging, there is a high heterogeneity of distributions among images based on\nthe equipment that generates them and their parametrization. This heterogeneity\ntriggers a common issue in machine learning called domain shift, which\nrepresents the difference between the training data distribution and the\ndistribution of where a model is employed. A high domain shift tends to\nimplicate in a poor generalization performance from the models. In this work,\nwe evaluate the extent of domain shift on four of the largest datasets of chest\nradiographs. We show how training and testing with different datasets (e.g.,\ntraining in ChestX-ray14 and testing in CheXpert) drastically affects model\nperformance, posing a big question over the reliability of deep learning models\ntrained on public datasets. We also show that models trained on CheXpert and\nMIMIC-CXR generalize better to other datasets.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 14:03:55 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 22:50:10 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Pooch", "Eduardo H. P.", ""], ["Ballester", "Pedro L.", ""], ["Barros", "Rodrigo C.", ""]]}, {"id": "1909.01955", "submitter": "Xavier Soria Poma", "authors": "Xavier Soria, Edgar Riba and Angel D. Sappa", "title": "Dense Extreme Inception Network: Towards a Robust CNN Model for Edge\n  Detection", "comments": "WACV2020 Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes a Deep Learning based edge detector, which is inspired on\nboth HED (Holistically-Nested Edge Detection) and Xception networks. The\nproposed approach generates thin edge-maps that are plausible for human eyes;\nit can be used in any edge detection task without previous training or fine\ntuning process. As a second contribution, a large dataset with carefully\nannotated edges has been generated. This dataset has been used for training the\nproposed approach as well the state-of-the-art algorithms for comparisons.\nQuantitative and qualitative evaluations have been performed on different\nbenchmarks showing improvements with the proposed method when F-measure of ODS\nand OIS are considered.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 17:27:46 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 00:08:24 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Soria", "Xavier", ""], ["Riba", "Edgar", ""], ["Sappa", "Angel D.", ""]]}, {"id": "1909.01960", "submitter": "Kristofer Schlachter", "authors": "Kristofer Schlachter, Connor DeFanti, Sebastian Herscher, Ken Perlin,\n  Jonathan Tompson", "title": "Beyond Photo Realism for Domain Adaptation from Synthetic Data", "comments": "Originally submitted for publication in November 2017", "journal-ref": null, "doi": null, "report-no": "01", "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As synthetic imagery is used more frequently in training deep models, it is\nimportant to understand how different synthesis techniques impact the\nperformance of such models. In this work, we perform a thorough evaluation of\nthe effectiveness of several different synthesis techniques and their impact on\nthe complexity of classifier domain adaptation to the \"real\" underlying data\ndistribution that they seek to replicate. In addition, we propose a novel\nlearned synthesis technique to better train classifier models than\nstate-of-the-art offline graphical methods, while using significantly less\ncomputational resources. We accomplish this by learning a generative model to\nperform shading of synthetic geometry conditioned on a \"g-buffer\"\nrepresentation of the scene to render, as well as a low sample Monte Carlo\nrendered image. The major contributions are (i) a dataset that allows\ncomparison of real and synthetic versions of the same scene, (ii) an augmented\ndata representation that boosts the stability of learning and improves the\ndatasets accuracy, (iii) three different partially differentiable rendering\ntechniques where lighting, denoising and shading are learned, and (iv) we\nimprove a state of the art generative adversarial network (GAN) approach by\nusing an ensemble of trained models to generate datasets that approach the\nperformance of training on real data and surpass the performance of the full\nglobal illumination rendering.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 17:38:05 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Schlachter", "Kristofer", ""], ["DeFanti", "Connor", ""], ["Herscher", "Sebastian", ""], ["Perlin", "Ken", ""], ["Tompson", "Jonathan", ""]]}, {"id": "1909.01963", "submitter": "Aman Shrivastava", "authors": "Aman Shrivastava, Will Adorno, Yash Sharma, Lubaina Ehsan, S. Asad\n  Ali, Sean R. Moore, Beatrice C. Amadi, Paul Kelly, Sana Syed, Donald E. Brown", "title": "Self-Attentive Adversarial Stain Normalization", "comments": "Accepted at AIDP (ICPR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hematoxylin and Eosin (H&E) stained Whole Slide Images (WSIs) are utilized\nfor biopsy visualization-based diagnostic and prognostic assessment of\ndiseases. Variation in the H&E staining process across different lab sites can\nlead to significant variations in biopsy image appearance. These variations\nintroduce an undesirable bias when the slides are examined by pathologists or\nused for training deep learning models. To reduce this bias, slides need to be\ntranslated to a common domain of stain appearance before analysis. We propose a\nSelf-Attentive Adversarial Stain Normalization (SAASN) approach for the\nnormalization of multiple stain appearances to a common domain. This\nunsupervised generative adversarial approach includes self-attention mechanism\nfor synthesizing images with finer detail while preserving the structural\nconsistency of the biopsy features during translation. SAASN demonstrates\nconsistent and superior performance compared to other popular stain\nnormalization techniques on H&E stained duodenal biopsy image data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 17:41:19 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 00:52:02 GMT"}, {"version": "v3", "created": "Sun, 22 Nov 2020 22:50:24 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Shrivastava", "Aman", ""], ["Adorno", "Will", ""], ["Sharma", "Yash", ""], ["Ehsan", "Lubaina", ""], ["Ali", "S. Asad", ""], ["Moore", "Sean R.", ""], ["Amadi", "Beatrice C.", ""], ["Kelly", "Paul", ""], ["Syed", "Sana", ""], ["Brown", "Donald E.", ""]]}, {"id": "1909.01966", "submitter": "Olivier Verdier", "authors": "Camille Pouchol and Olivier Verdier", "title": "The ML-EM algorithm in continuum: sparse measure solutions", "comments": null, "journal-ref": "Inverse Problems, Vol 36 (3), (2020)", "doi": "10.1088/1361-6420/ab6d55", "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear inverse problems $A \\mu = \\delta$ with Poisson noise and non-negative\nunknown $\\mu \\geq 0$ are ubiquitous in applications, for instance in Positron\nEmission Tomography (PET) in medical imaging. The associated maximum likelihood\nproblem is routinely solved using an expectation-maximisation algorithm\n(ML-EM). This typically results in images which look spiky, even with early\nstopping. We give an explanation for this phenomenon. We first regard the image\n$\\mu$ as a measure. We prove that if the measurements $\\delta$ are not in the\ncone $\\{A \\mu, \\mu \\geq 0\\}$, which is typical of short exposure times,\nlikelihood maximisers as well as ML-EM cluster points must be sparse, i.e.,\ntypically a sum of point masses. On the other hand, in the long exposure\nregime, we prove that cluster points of ML-EM will be measures without singular\npart. Finally, we provide concentration bounds for the probability to be in the\nsparse case.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 17:47:42 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 21:03:36 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Pouchol", "Camille", ""], ["Verdier", "Olivier", ""]]}, {"id": "1909.01976", "submitter": "Muhammad Kamran Janjua", "authors": "Shah Nawaz, Muhammad Kamran Janjua, Ignazio Gallo, Arif Mahmood,\n  Alessandro Calefati, and Faisal Shafait", "title": "Do Cross Modal Systems Leverage Semantic Relationships?", "comments": "Accepted to cross modal learning in real world in conjunction with\n  ICCV 2019. arXiv admin note: text overlap with arXiv:1807.07364", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current cross-modal retrieval systems are evaluated using R@K measure which\ndoes not leverage semantic relationships rather strictly follows the manually\nmarked image text query pairs. Therefore, current systems do not generalize\nwell for the unseen data in the wild. To handle this, we propose a new measure,\nSemanticMap, to evaluate the performance of cross-modal systems. Our proposed\nmeasure evaluates the semantic similarity between the image and text\nrepresentations in the latent embedding space. We also propose a novel\ncross-modal retrieval system using a single stream network for bidirectional\nretrieval. The proposed system is based on a deep neural network trained using\nextended center loss, minimizing the distance of image and text descriptions in\nthe latent space from the class centers. In our system, the text descriptions\nare also encoded as images which enabled us to use a single stream network for\nboth text and images. To the best of our knowledge, our work is the first of\nits kind in terms of employing a single stream network for cross-modal\nretrieval systems. The proposed system is evaluated on two publicly available\ndatasets including MSCOCO and Flickr30K and has shown comparable results to the\ncurrent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 18:33:38 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Nawaz", "Shah", ""], ["Janjua", "Muhammad Kamran", ""], ["Gallo", "Ignazio", ""], ["Mahmood", "Arif", ""], ["Calefati", "Alessandro", ""], ["Shafait", "Faisal", ""]]}, {"id": "1909.02040", "submitter": "Ulugbek Kamilov", "authors": "Zihui Wu, Yu Sun, Jiaming Liu, and Ulugbek S. Kamilov", "title": "Online Regularization by Denoising with Applications to Phase Retrieval", "comments": "Accepted ICCVW 2019 (LCI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization by denoising (RED) is a powerful framework for solving imaging\ninverse problems. Most RED algorithms are iterative batch procedures, which\nlimits their applicability to very large datasets. In this paper, we address\nthis limitation by introducing a novel online RED (On-RED) algorithm, which\nprocesses a small subset of the data at a time. We establish the theoretical\nconvergence of On-RED in convex settings and empirically discuss its\neffectiveness in non-convex ones by illustrating its applicability to phase\nretrieval. Our results suggest that On-RED is an effective alternative to the\ntraditional RED algorithms when dealing with large datasets.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 18:29:10 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Wu", "Zihui", ""], ["Sun", "Yu", ""], ["Liu", "Jiaming", ""], ["Kamilov", "Ulugbek S.", ""]]}, {"id": "1909.02050", "submitter": "Ming Jiang", "authors": "Ming Jiang, Qiuyuan Huang, Lei Zhang, Xin Wang, Pengchuan Zhang, Zhe\n  Gan, Jana Diesner, Jianfeng Gao", "title": "TIGEr: Text-to-Image Grounding for Image Caption Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new metric called TIGEr for the automatic evaluation of\nimage captioning systems. Popular metrics, such as BLEU and CIDEr, are based\nsolely on text matching between reference captions and machine-generated\ncaptions, potentially leading to biased evaluations because references may not\nfully cover the image content and natural language is inherently ambiguous.\nBuilding upon a machine-learned text-image grounding model, TIGEr allows to\nevaluate caption quality not only based on how well a caption represents image\ncontent, but also on how well machine-generated captions match human-generated\ncaptions. Our empirical tests show that TIGEr has a higher consistency with\nhuman judgments than alternative existing metrics. We also comprehensively\nassess the metric's effectiveness in caption evaluation by measuring the\ncorrelation between human judgments and metric scores.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 18:43:04 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Jiang", "Ming", ""], ["Huang", "Qiuyuan", ""], ["Zhang", "Lei", ""], ["Wang", "Xin", ""], ["Zhang", "Pengchuan", ""], ["Gan", "Zhe", ""], ["Diesner", "Jana", ""], ["Gao", "Jianfeng", ""]]}, {"id": "1909.02062", "submitter": "Basel Alyafi", "authors": "Basel Alyafi, Oliver Diaz, Robert Marti", "title": "DCGANs for Realistic Breast Mass Augmentation in X-ray Mammography", "comments": "4 pages, 4 figures, SPIE Medical Imaging 2020 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of breast cancer has a major contribution to curability, and\nusing mammographic images, this can be achieved non-invasively. Supervised deep\nlearning, the dominant CADe tool currently, has played a great role in object\ndetection in computer vision, but it suffers from a limiting property: the need\nof a large amount of labelled data. This becomes stricter when it comes to\nmedical datasets which require high-cost and time-consuming annotations.\nFurthermore, medical datasets are usually imbalanced, a condition that often\nhinders classifiers performance. The aim of this paper is to learn the\ndistribution of the minority class to synthesise new samples in order to\nimprove lesion detection in mammography. Deep Convolutional Generative\nAdversarial Networks (DCGANs) can efficiently generate breast masses. They are\ntrained on increasing-size subsets of one mammographic dataset and used to\ngenerate diverse and realistic breast masses. The effect of including the\ngenerated images and/or applying horizontal and vertical flipping is tested in\nan environment where a 1:10 imbalanced dataset of masses and normal tissue\npatches is classified by a fully-convolutional network. A maximum of ~ 0:09\nimprovement of F1 score is reported by using DCGANs along with flipping\naugmentation over using the original images. We show that DCGANs can be used\nfor synthesising photo-realistic breast mass patches with considerable\ndiversity. It is demonstrated that appending synthetic images in this\nenvironment, along with flipping, outperforms the traditional augmentation\nmethod of flipping solely, offering faster improvements as a function of the\ntraining set size.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 19:09:49 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Alyafi", "Basel", ""], ["Diaz", "Oliver", ""], ["Marti", "Robert", ""]]}, {"id": "1909.02068", "submitter": "Ran Xu", "authors": "Ran Xu, Rakesh Kumar, Pengcheng Wang, Peter Bai, Ganga Meghanath,\n  Somali Chaterji, Subrata Mitra, Saurabh Bagchi", "title": "ApproxNet: Content and Contention-Aware Video Analytics System for\n  Embedded Clients", "comments": "This paper has been accepted to appear in ACM Transactions on Sensor\n  Networks in 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos take a lot of time to transport over the network, hence running\nanalytics on the live video on embedded or mobile devices has become an\nimportant system driver. Considering that such devices, e.g., surveillance\ncameras or AR/VR gadgets, are resource constrained, creating lightweight deep\nneural networks (DNNs) for embedded devices is crucial. None of the current\napproximation techniques for object classification DNNs can adapt to changing\nruntime conditions, e.g., changes in resource availability on the device, the\ncontent characteristics, or requirements from the user. In this paper, we\nintroduce ApproxNet, a video object classification system for embedded or\nmobile clients. It enables novel dynamic approximation techniques to achieve\ndesired inference latency and accuracy trade-off under changing runtime\nconditions. It achieves this by enabling two approximation knobs within a\nsingle DNN model, rather than creating and maintaining an ensemble of models\n(e.g., MCDNN [MobiSys-16]. We show that ApproxNet can adapt seamlessly at\nruntime to these changes, provides low and stable latency for the image and\nvideo frame classification problems, and show the improvement in accuracy and\nlatency over ResNet [CVPR-16], MCDNN [MobiSys-16], MobileNets [Google-17],\nNestDNN [MobiCom-18], and MSDNet [ICLR-18].\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 19:29:41 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 17:09:05 GMT"}, {"version": "v3", "created": "Tue, 19 May 2020 05:04:48 GMT"}, {"version": "v4", "created": "Wed, 23 Sep 2020 05:47:34 GMT"}, {"version": "v5", "created": "Wed, 14 Jul 2021 20:22:04 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Xu", "Ran", ""], ["Kumar", "Rakesh", ""], ["Wang", "Pengcheng", ""], ["Bai", "Peter", ""], ["Meghanath", "Ganga", ""], ["Chaterji", "Somali", ""], ["Mitra", "Subrata", ""], ["Bagchi", "Saurabh", ""]]}, {"id": "1909.02072", "submitter": "Tianlang Chen", "authors": "Tianlang Chen, Zhaowen Wang, Ning Xu, Hailin Jin, Jiebo Luo", "title": "Large-scale Tag-based Font Retrieval with Generative Feature Learning", "comments": "accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Font selection is one of the most important steps in a design workflow.\nTraditional methods rely on ordered lists which require significant domain\nknowledge and are often difficult to use even for trained professionals. In\nthis paper, we address the problem of large-scale tag-based font retrieval\nwhich aims to bring semantics to the font selection process and enable people\nwithout expert knowledge to use fonts effectively. We collect a large-scale\nfont tagging dataset of high-quality professional fonts. The dataset contains\nnearly 20,000 fonts, 2,000 tags, and hundreds of thousands of font-tag\nrelations. We propose a novel generative feature learning algorithm that\nleverages the unique characteristics of fonts. The key idea is that font images\nare synthetic and can therefore be controlled by the learning algorithm. We\ndesign an integrated rendering and learning process so that the visual feature\nfrom one image can be used to reconstruct another image with different text.\nThe resulting feature captures important font design details while is robust to\nnuisance factors such as text. We propose a novel attention mechanism to\nre-weight the visual feature for joint visual-text modeling. We combine the\nfeature and the attention mechanism in a novel recognition-retrieval model.\nExperimental results show that our method significantly outperforms the\nstate-of-the-art for the important problem of large-scale tag-based font\nretrieval.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 19:49:58 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 15:57:07 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Chen", "Tianlang", ""], ["Wang", "Zhaowen", ""], ["Xu", "Ning", ""], ["Jin", "Hailin", ""], ["Luo", "Jiebo", ""]]}, {"id": "1909.02077", "submitter": "Yirui Wang", "authors": "Yirui Wang, Le Lu, Chi-Tung Cheng, Dakai Jin, Adam P. Harrison, Jing\n  Xiao, Chien-Hung Liao, Shun Miao", "title": "Weakly Supervised Universal Fracture Detection in Pelvic X-rays", "comments": "MICCAI 2019 (early accept)", "journal-ref": null, "doi": "10.1007/978-3-030-32226-7_51", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hip and pelvic fractures are serious injuries with life-threatening\ncomplications. However, diagnostic errors of fractures in pelvic X-rays (PXRs)\nare very common, driving the demand for computer-aided diagnosis (CAD)\nsolutions. A major challenge lies in the fact that fractures are localized\npatterns that require localized analyses. Unfortunately, the PXRs residing in\nhospital picture archiving and communication system do not typically specify\nregion of interests. In this paper, we propose a two-stage hip and pelvic\nfracture detection method that executes localized fracture classification using\nweakly supervised ROI mining. The first stage uses a large capacity\nfully-convolutional network, i.e., deep with high levels of abstraction, in a\nmultiple instance learning setting to automatically mine probable true positive\nand definite hard negative ROIs from the whole PXR in the training data. The\nsecond stage trains a smaller capacity model, i.e., shallower and more\ngeneralizable, with the mined ROIs to perform localized analyses to classify\nfractures. During inference, our method detects hip and pelvic fractures in one\npass by chaining the probability outputs of the two stages together. We\nevaluate our method on 4 410 PXRs, reporting an area under the ROC curve value\nof 0.975, the highest among state-of-the-art fracture detection methods.\nMoreover, we show that our two-stage approach can perform comparably to human\nphysicians (even outperforming emergency physicians and surgeons), in a\npreliminary reader study of 23 readers.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 20:04:50 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Wang", "Yirui", ""], ["Lu", "Le", ""], ["Cheng", "Chi-Tung", ""], ["Jin", "Dakai", ""], ["Harrison", "Adam P.", ""], ["Xiao", "Jing", ""], ["Liao", "Chien-Hung", ""], ["Miao", "Shun", ""]]}, {"id": "1909.02097", "submitter": "Soravit Changpinyo", "authors": "Soravit Changpinyo, Bo Pang, Piyush Sharma, Radu Soricut", "title": "Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic\n  Labels Improve Image Captioning and Visual Question Answering", "comments": "The 2019 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection plays an important role in current solutions to vision and\nlanguage tasks like image captioning and visual question answering. However,\npopular models like Faster R-CNN rely on a costly process of annotating\nground-truths for both the bounding boxes and their corresponding semantic\nlabels, making it less amenable as a primitive task for transfer learning. In\nthis paper, we examine the effect of decoupling box proposal and featurization\nfor down-stream tasks. The key insight is that this allows us to leverage a\nlarge amount of labeled annotations that were previously unavailable for\nstandard object detection benchmarks. Empirically, we demonstrate that this\nleads to effective transfer learning and improved image captioning and visual\nquestion answering models, as measured on publicly available benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 20:37:30 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Changpinyo", "Soravit", ""], ["Pang", "Bo", ""], ["Sharma", "Piyush", ""], ["Soricut", "Radu", ""]]}, {"id": "1909.02116", "submitter": "Jiayuan Mao", "authors": "Jiayuan Mao and Xiuming Zhang and Yikai Li and William T. Freeman and\n  Joshua B. Tenenbaum and Jiajun Wu", "title": "Program-Guided Image Manipulators", "comments": "ICCV 2019. First two authors contributed equally. Project page:\n  http://pgim.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are capable of building holistic representations for images at various\nlevels, from local objects, to pairwise relations, to global structures. The\ninterpretation of structures involves reasoning over repetition and symmetry of\nthe objects in the image. In this paper, we present the Program-Guided Image\nManipulator (PG-IM), inducing neuro-symbolic program-like representations to\nrepresent and manipulate images. Given an image, PG-IM detects repeated\npatterns, induces symbolic programs, and manipulates the image using a neural\nnetwork that is guided by the program. PG-IM learns from a single image,\nexploiting its internal statistics. Despite trained only on image inpainting,\nPG-IM is directly capable of extrapolation and regularity editing in a unified\nframework. Extensive experiments show that PG-IM achieves superior performance\non all the tasks.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 21:10:47 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Mao", "Jiayuan", ""], ["Zhang", "Xiuming", ""], ["Li", "Yikai", ""], ["Freeman", "William T.", ""], ["Tenenbaum", "Joshua B.", ""], ["Wu", "Jiajun", ""]]}, {"id": "1909.02129", "submitter": "Jialiang Zhao", "authors": "Jialiang Zhao, Jacky Liang, and Oliver Kroemer", "title": "Towards Precise Robotic Grasping by Probabilistic Post-grasp\n  Displacement Estimation", "comments": "Submitted and accepted to 12th Conference on Field and Service\n  Robotics (FSR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise robotic grasping is important for many industrial applications, such\nas assembly and palletizing, where the location of the object needs to be\ncontrolled and known. However, achieving precise grasps is challenging due to\nnoise in sensing and control, as well as unknown object properties. We propose\na method to plan robotic grasps that are both robust and precise by training\ntwo convolutional neural networks - one to predict the robustness of a grasp\nand another to predict a distribution of post-grasp object displacements. Our\nnetworks are trained with depth images in simulation on a dataset of over 1000\nindustrial parts and were successfully deployed on a real robot without having\nto be further fine-tuned. The proposed displacement estimator achieves a mean\nprediction errors of 0.68cm and 3.42deg on novel objects in real world\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 21:50:03 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Zhao", "Jialiang", ""], ["Liang", "Jacky", ""], ["Kroemer", "Oliver", ""]]}, {"id": "1909.02144", "submitter": "Siyuan Huang", "authors": "Lifeng Fan, Wenguan Wang, Siyuan Huang, Xinyu Tang, Song-Chun Zhu", "title": "Understanding Human Gaze Communication by Spatio-Temporal Graph\n  Reasoning", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses a new problem of understanding human gaze communication\nin social videos from both atomic-level and event-level, which is significant\nfor studying human social interactions. To tackle this novel and challenging\nproblem, we contribute a large-scale video dataset, VACATION, which covers\ndiverse daily social scenes and gaze communication behaviors with complete\nannotations of objects and human faces, human attention, and communication\nstructures and labels in both atomic-level and event-level. Together with\nVACATION, we propose a spatio-temporal graph neural network to explicitly\nrepresent the diverse gaze interactions in the social scenes and to infer\natomic-level gaze communication by message passing. We further propose an event\nnetwork with encoder-decoder structure to predict the event-level gaze\ncommunication. Our experiments demonstrate that the proposed model improves\nvarious baselines significantly in predicting the atomic-level and event-level\ngaze\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 22:50:33 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Fan", "Lifeng", ""], ["Wang", "Wenguan", ""], ["Huang", "Siyuan", ""], ["Tang", "Xinyu", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1909.02157", "submitter": "Richard Jiang", "authors": "Gary Storey, Ahmed Bouridane, Richard Jiang and Chang-tsun Li", "title": "Atypical Facial Landmark Localisation with Stacked Hourglass Networks: A\n  Study on 3D Facial Modelling for Medical Diagnosis", "comments": "In press, 2019", "journal-ref": "Deep Biometrics, Springer Book, 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While facial biometrics has been widely used for identification purpose, it\nhas recently been researched as medical biometrics for a range of diseases. In\nthis chapter, we investigate the facial landmark detection for atypical 3D\nfacial modelling in facial palsy cases, while potentially such modelling can\nassist the medical diagnosis using atypical facial features. In our work, a\nstudy of landmarks localisation methods such as stacked hourglass networks is\nconducted and evaluated to ascertain their accuracy when presented with unseen\natypical faces. The evaluation highlights that the state-of-the-art stacked\nhourglass architecture outperforms other traditional methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 00:08:28 GMT"}], "update_date": "2019-09-15", "authors_parsed": [["Storey", "Gary", ""], ["Bouridane", "Ahmed", ""], ["Jiang", "Richard", ""], ["Li", "Chang-tsun", ""]]}, {"id": "1909.02165", "submitter": "Nilesh Pandey", "authors": "Nilesh Pandey and Andreas Savakis", "title": "Poly-GAN: Multi-Conditioned GAN for Fashion Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Poly-GAN, a novel conditional GAN architecture that is motivated\nby Fashion Synthesis, an application where garments are automatically placed on\nimages of human models at an arbitrary pose. Poly-GAN allows conditioning on\nmultiple inputs and is suitable for many tasks, including image alignment,\nimage stitching, and inpainting. Existing methods have a similar pipeline where\nthree different networks are used to first align garments with the human pose,\nthen perform stitching of the aligned garment and finally refine the results.\nPoly-GAN is the first instance where a common architecture is used to perform\nall three tasks. Our novel architecture enforces the conditions at all layers\nof the encoder and utilizes skip connections from the coarse layers of the\nencoder to the respective layers of the decoder. Poly-GAN is able to perform a\nspatial transformation of the garment based on the RGB skeleton of the model at\nan arbitrary pose. Additionally, Poly-GAN can perform image stitching,\nregardless of the garment orientation, and inpainting on the garment mask when\nit contains irregular holes. Our system achieves state-of-the-art quantitative\nresults on Structural Similarity Index metric and Inception Score metric using\nthe DeepFashion dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 00:29:39 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Pandey", "Nilesh", ""], ["Savakis", "Andreas", ""]]}, {"id": "1909.02168", "submitter": "Yiwei Lu", "authors": "Yiwei Lu, Mahesh Kumar Krishna Reddy, Seyed shahabeddin Nabavi and\n  Yang Wang", "title": "Future Frame Prediction Using Convolutional VRNN for Anomaly Detection", "comments": "Accepted to AVSS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in videos aims at reporting anything that does not conform\nthe normal behaviour or distribution. However, due to the sparsity of abnormal\nvideo clips in real life, collecting annotated data for supervised learning is\nexceptionally cumbersome. Inspired by the practicability of generative models\nfor semi-supervised learning, we propose a novel sequential generative model\nbased on variational autoencoder (VAE) for future frame prediction with\nconvolutional LSTM (ConvLSTM). To the best of our knowledge, this is the first\nwork that considers temporal information in future frame prediction based\nanomaly detection framework from the model perspective. Our experiments\ndemonstrate that our approach is superior to the state-of-the-art methods on\nthree benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 00:34:33 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 21:48:03 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Lu", "Yiwei", ""], ["Reddy", "Mahesh Kumar Krishna", ""], ["Nabavi", "Seyed shahabeddin", ""], ["Wang", "Yang", ""]]}, {"id": "1909.02201", "submitter": "Dong-Jin Kim", "authors": "Dong-Jin Kim, Jinsoo Choi, Tae-Hyun Oh, In So Kweon", "title": "Image Captioning with Very Scarce Supervised Data: Adversarial\n  Semi-Supervised Learning Approach", "comments": "EMNLP 2019. Project page :\n  https://sites.google.com/view/emnlp19scarcecaption", "journal-ref": null, "doi": "10.18653/v1/D19-1208", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing an organized dataset comprised of a large number of images and\nseveral captions for each image is a laborious task, which requires vast human\neffort. On the other hand, collecting a large number of images and sentences\nseparately may be immensely easier. In this paper, we develop a novel\ndata-efficient semi-supervised framework for training an image captioning\nmodel. We leverage massive unpaired image and caption data by learning to\nassociate them. To this end, our proposed semi-supervised learning method\nassigns pseudo-labels to unpaired samples via Generative Adversarial Networks\nto learn the joint distribution of image and caption. To evaluate, we construct\nscarcely-paired COCO dataset, a modified version of MS COCO caption dataset.\nThe empirical results show the effectiveness of our method compared to several\nstrong baselines, especially when the amount of the paired samples are scarce.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 04:16:48 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 07:01:02 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Kim", "Dong-Jin", ""], ["Choi", "Jinsoo", ""], ["Oh", "Tae-Hyun", ""], ["Kweon", "In So", ""]]}, {"id": "1909.02211", "submitter": "Helge Rhodin", "authors": "Didier Bieler, Semih G\\\"unel, Pascal Fua and Helge Rhodin", "title": "Gravity as a Reference for Estimating a Person's Height from Video", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the metric height of a person from monocular imagery without\nadditional assumptions is ill-posed. Existing solutions either require manual\ncalibration of ground plane and camera geometry, special cameras, or reference\nobjects of known size. We focus on motion cues and exploit gravity on earth as\nan omnipresent reference 'object' to translate acceleration, and subsequently\nheight, measured in image-pixels to values in meters. We require videos of\nmotion as input, where gravity is the only external force. This limitation is\ndifferent to those of existing solutions that recover a person's height and,\ntherefore, our method opens up new application fields. We show theoretically\nand empirically that a simple motion trajectory analysis suffices to translate\nfrom pixel measurements to the person's metric height, reaching a MAE of up to\n3.9 cm on jumping motions, and that this works without camera and ground plane\ncalibration.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 05:03:46 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 06:07:08 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Bieler", "Didier", ""], ["G\u00fcnel", "Semih", ""], ["Fua", "Pascal", ""], ["Rhodin", "Helge", ""]]}, {"id": "1909.02214", "submitter": "Bohan Zhuang", "authors": "Yifan Liu, Bohan Zhuang, Chunhua Shen, Hao Chen, Wei Yin", "title": "Auxiliary Learning for Deep Multi-task Learning", "comments": "First two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) is an efficient solution to solve multiple tasks\nsimultaneously in order to get better speed and performance than handling each\nsingle-task in turn. The most current methods can be categorized as either: (i)\nhard parameter sharing where a subset of the parameters is shared among tasks\nwhile other parameters are task-specific; or (ii) soft parameter sharing where\nall parameters are task-specific but they are jointly regularized. Both methods\nsuffer from limitations: the shared hidden layers of the former are difficult\nto optimize due to the competing objectives while the complexity of the latter\ngrows linearly with the increasing number of tasks. To mitigate those\ndrawbacks, this paper proposes an alternative, where we explicitly construct an\nauxiliary module to mimic the soft parameter sharing for assisting the\noptimization of the hard parameter sharing layers in the training phase. In\nparticular, the auxiliary module takes the outputs of the shared hidden layers\nas inputs and is supervised by the auxiliary task loss. During training, the\nauxiliary module is jointly optimized with the MTL network, serving as a\nregularization by introducing an inductive bias to the shared layers. In the\ntesting phase, only the original MTL network is kept. Thus our method avoids\nthe limitation of both categories. We evaluate the proposed auxiliary module on\npixel-wise prediction tasks, including semantic segmentation, depth estimation,\nand surface normal prediction with different network structures. The extensive\nexperiments over various settings verify the effectiveness of our methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 05:29:15 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 01:46:55 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Liu", "Yifan", ""], ["Zhuang", "Bohan", ""], ["Shen", "Chunhua", ""], ["Chen", "Hao", ""], ["Yin", "Wei", ""]]}, {"id": "1909.02215", "submitter": "Baris Gecer", "authors": "Baris Gecer, Alexander Lattas, Stylianos Ploumpis, Jiankang Deng,\n  Athanasios Papaioannou, Stylianos Moschoglou, Stefanos Zafeiriou", "title": "Synthesizing Coupled 3D Face Modalities by Trunk-Branch Generative\n  Adversarial Networks", "comments": "Check project page: https://github.com/barisgecer/TBGAN for the full\n  resolution results and the accompanying video", "journal-ref": null, "doi": "10.1007/978-3-030-58526-6_25", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generating realistic 3D faces is of high importance for computer graphics and\ncomputer vision applications. Generally, research on 3D face generation\nrevolves around linear statistical models of the facial surface. Nevertheless,\nthese models cannot represent faithfully either the facial texture or the\nnormals of the face, which are very crucial for photo-realistic face synthesis.\nRecently, it was demonstrated that Generative Adversarial Networks (GANs) can\nbe used for generating high-quality textures of faces. Nevertheless, the\ngeneration process either omits the geometry and normals, or independent\nprocesses are used to produce 3D shape information. In this paper, we present\nthe first methodology that generates high-quality texture, shape, and normals\njointly, which can be used for photo-realistic synthesis. To do so, we propose\na novel GAN that can generate data from different modalities while exploiting\ntheir correlations. Furthermore, we demonstrate how we can condition the\ngeneration on the expression and create faces with various facial expressions.\nThe qualitative results shown in this paper are compressed due to size\nlimitations, full-resolution results and the accompanying video can be found in\nthe supplementary documents. The code and models are available at the project\npage: https://github.com/barisgecer/TBGAN.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 05:33:50 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 18:25:34 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 01:12:57 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Gecer", "Baris", ""], ["Lattas", "Alexander", ""], ["Ploumpis", "Stylianos", ""], ["Deng", "Jiankang", ""], ["Papaioannou", "Athanasios", ""], ["Moschoglou", "Stylianos", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1909.02217", "submitter": "Ming Jiang", "authors": "Ming Jiang, Junjie Hu, Qiuyuan Huang, Lei Zhang, Jana Diesner,\n  Jianfeng Gao", "title": "REO-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image\n  Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular metrics used for evaluating image captioning systems, such as BLEU\nand CIDEr, provide a single score to gauge the system's overall effectiveness.\nThis score is often not informative enough to indicate what specific errors are\nmade by a given system. In this study, we present a fine-grained evaluation\nmethod REO for automatically measuring the performance of image captioning\nsystems. REO assesses the quality of captions from three perspectives: 1)\nRelevance to the ground truth, 2) Extraness of the content that is irrelevant\nto the ground truth, and 3) Omission of the elements in the images and human\nreferences. Experiments on three benchmark datasets demonstrate that our method\nachieves a higher consistency with human judgments and provides more intuitive\nevaluation results than alternative metrics.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 05:44:46 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Jiang", "Ming", ""], ["Hu", "Junjie", ""], ["Huang", "Qiuyuan", ""], ["Zhang", "Lei", ""], ["Diesner", "Jana", ""], ["Gao", "Jianfeng", ""]]}, {"id": "1909.02218", "submitter": "Hongyang Xue", "authors": "Hongyang Xue, Wenqing Chu, Zhou Zhao, Deng Cai", "title": "A Better Way to Attend: Attention with Trees for Video Question\n  Answering", "comments": "12 pages", "journal-ref": "IEEE Transactions on Image Processing ( Volume: 27 , Issue: 11 ,\n  Nov. 2018 )", "doi": "10.1109/TIP.2018.2859820", "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new attention model for video question answering. The main idea\nof the attention models is to locate on the most informative parts of the\nvisual data. The attention mechanisms are quite popular these days. However,\nmost existing visual attention mechanisms regard the question as a whole. They\nignore the word-level semantics where each word can have different attentions\nand some words need no attention. Neither do they consider the semantic\nstructure of the sentences. Although the Extended Soft Attention (E-SA) model\nfor video question answering leverages the word-level attention, it performs\npoorly on long question sentences. In this paper, we propose the heterogeneous\ntree-structured memory network (HTreeMN) for video question answering. Our\nproposed approach is based upon the syntax parse trees of the question\nsentences. The HTreeMN treats the words differently where the \\textit{visual}\nwords are processed with an attention module and the \\textit{verbal} ones not.\nIt also utilizes the semantic structure of the sentences by combining the\nneighbors based on the recursive structure of the parse trees. The\nunderstandings of the words and the videos are propagated and merged from\nleaves to the root. Furthermore, we build a hierarchical attention mechanism to\ndistill the attended features. We evaluate our approach on two datasets. The\nexperimental results show the superiority of our HTreeMN model over the other\nattention models especially on complex questions. Our code is available on\ngithub.\n  Our code is available at https://github.com/ZJULearning/TreeAttention\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 05:48:51 GMT"}], "update_date": "2019-09-15", "authors_parsed": [["Xue", "Hongyang", ""], ["Chu", "Wenqing", ""], ["Zhao", "Zhou", ""], ["Cai", "Deng", ""]]}, {"id": "1909.02221", "submitter": "Mehrdad Shoeiby", "authors": "Mehrdad Shoeiby, Lars Petersson, Mohammad Ali Armin, Sadegh\n  Aliakbarian, Antonio Robles-Kelly", "title": "Super-resolved Chromatic Mapping of Snapshot Mosaic Image Sensors via a\n  Texture Sensitive Residual Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel method to simultaneously super-resolve and\ncolour-predict images acquired by snapshot mosaic sensors. These sensors allow\nfor spectral images to be acquired using low-power, small form factor,\nsolid-state CMOS sensors that can operate at video frame rates without the need\nfor complex optical setups. Despite their desirable traits, their main drawback\nstems from the fact that the spatial resolution of the imagery acquired by\nthese sensors is low. Moreover, chromatic mapping in snapshot mosaic sensors is\nnot straightforward since the bands delivered by the sensor tend to be narrow\nand unevenly distributed across the range in which they operate. We tackle this\ndrawback as applied to chromatic mapping by using a residual channel attention\nnetwork equipped with a texture sensitive block. Our method significantly\noutperforms the traditional approach of interpolating the image and,\nafterwards, applying a colour matching function. This work establishes\nstate-of-the-art in this domain while also making available to the research\ncommunity a dataset containing 296 registered stereo multi-spectral/RGB images\npairs.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 06:05:31 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Shoeiby", "Mehrdad", ""], ["Petersson", "Lars", ""], ["Armin", "Mohammad Ali", ""], ["Aliakbarian", "Sadegh", ""], ["Robles-Kelly", "Antonio", ""]]}, {"id": "1909.02225", "submitter": "Junran Peng", "authors": "Junran Peng, Ming Sun, Zhaoxiang Zhang, Tieniu Tan, Junjie Yan", "title": "POD: Practical Object Detection with Scale-Sensitive Network", "comments": "arXiv admin note: text overlap with arXiv:1901.06563 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scale-sensitive object detection remains a challenging task, where most of\nthe existing methods could not learn it explicitly and are not robust to scale\nvariance. In addition, the most existing methods are less efficient during\ntraining or slow during inference, which are not friendly to real-time\napplications. In this paper, we propose a practical object detection method\nwith scale-sensitive network.Our method first predicts a global continuous\nscale ,which is shared by all position, for each convolution filter of each\nnetwork stage. To effectively learn the scale, we average the spatial features\nand distill the scale from channels. For fast-deployment, we propose a scale\ndecomposition method that transfers the robust fractional scale into\ncombination of fixed integral scales for each convolution filter, which\nexploits the dilated convolution. We demonstrate it on one-stage and two-stage\nalgorithms under different configurations. For practical applications, training\nof our method is of efficiency and simplicity which gets rid of complex data\nsampling or optimize strategy. During test-ing, the proposed method requires no\nextra operation and is very supportive of hardware acceleration like TensorRT\nand TVM. On the COCO test-dev, our model could achieve a 41.5 mAP on one-stage\ndetector and 42.1 mAP on two-stage detectors based on ResNet-101, outperforming\nbase-lines by 2.4 and 2.1 respectively without extra FLOPS.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 06:24:50 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Peng", "Junran", ""], ["Sun", "Ming", ""], ["Zhang", "Zhaoxiang", ""], ["Tan", "Tieniu", ""], ["Yan", "Junjie", ""]]}, {"id": "1909.02236", "submitter": "Zhichen Zhao", "authors": "Zhichen Zhao, Bowen Zhang, Yuning Jiang, Li Xu, Lei Li, Wei-Ying Ma", "title": "Effective Domain Knowledge Transfer with Soft Fine-tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks require numerous data for training. Considering\nthe difficulties in data collection and labeling in some specific tasks,\nexisting approaches generally use models pre-trained on a large source domain\n(e.g. ImageNet), and then fine-tune them on these tasks. However, the datasets\nfrom source domain are simply discarded in the fine-tuning process. We argue\nthat the source datasets could be better utilized and benefit fine-tuning. This\npaper firstly introduces the concept of general discrimination to describe\nability of a network to distinguish untrained patterns, and then experimentally\ndemonstrates that general discrimination could potentially enhance the total\ndiscrimination ability on target domain. Furthermore, we propose a novel and\nlight-weighted method, namely soft fine-tuning. Unlike traditional fine-tuning\nwhich directly replaces optimization objective by a loss function on the target\ndomain, soft fine-tuning effectively keeps general discrimination by holding\nthe previous loss and removes it softly. By doing so, soft fine-tuning improves\nthe robustness of the network to data bias, and meanwhile accelerates the\nconvergence. We evaluate our approach on several visual recognition tasks.\nExtensive experimental results support that soft fine-tuning provides\nconsistent improvement on all evaluated tasks, and outperforms the\nstate-of-the-art significantly. Codes will be made available to the public.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 07:10:55 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Zhao", "Zhichen", ""], ["Zhang", "Bowen", ""], ["Jiang", "Yuning", ""], ["Xu", "Li", ""], ["Li", "Lei", ""], ["Ma", "Wei-Ying", ""]]}, {"id": "1909.02240", "submitter": "Yiming Wu", "authors": "Yiming Wu, Omar El Farouk Bourahla, Xi Li, Fei Wu, Qi Tian, and Xue\n  Zhou", "title": "Adaptive Graph Representation Learning for Video Person\n  Re-identification", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": "10.1109/TIP.2020.3001693", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the remarkable progress of applying deep learning\nmodels in video person re-identification (Re-ID). A key factor for video person\nRe-ID is to effectively construct discriminative and robust video feature\nrepresentations for many complicated situations. Part-based approaches employ\nspatial and temporal attention to extract representative local features. While\ncorrelations between parts are ignored in the previous methods, to leverage the\nrelations of different parts, we propose an innovative adaptive graph\nrepresentation learning scheme for video person Re-ID, which enables the\ncontextual interactions between relevant regional features. Specifically, we\nexploit the pose alignment connection and the feature affinity connection to\nconstruct an adaptive structure-aware adjacency graph, which models the\nintrinsic relations between graph nodes. We perform feature propagation on the\nadjacency graph to refine regional features iteratively, and the neighbor\nnodes' information is taken into account for part feature representation. To\nlearn compact and discriminative representations, we further propose a novel\ntemporal resolution-aware regularization, which enforces the consistency among\ndifferent temporal resolutions for the same identities. We conduct extensive\nevaluations on four benchmarks, i.e. iLIDS-VID, PRID2011, MARS, and\nDukeMTMC-VideoReID, experimental results achieve the competitive performance\nwhich demonstrates the effectiveness of our proposed method. The code is\navailable at https://github.com/weleen/AGRL.pytorch.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 07:18:06 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 02:12:40 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Wu", "Yiming", ""], ["Bourahla", "Omar El Farouk", ""], ["Li", "Xi", ""], ["Wu", "Fei", ""], ["Tian", "Qi", ""], ["Zhou", "Xue", ""]]}, {"id": "1909.02244", "submitter": "Xiujun Li", "authors": "Xiujun Li and Chunyuan Li and Qiaolin Xia and Yonatan Bisk and Asli\n  Celikyilmaz and Jianfeng Gao and Noah Smith and Yejin Choi", "title": "Robust Navigation with Language Pretraining and Stochastic Sampling", "comments": "8 pages, 4 figures, EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Core to the vision-and-language navigation (VLN) challenge is building robust\ninstruction representations and action decoding schemes, which can generalize\nwell to previously unseen instructions and environments. In this paper, we\nreport two simple but highly effective methods to address these challenges and\nlead to a new state-of-the-art performance. First, we adapt large-scale\npretrained language models to learn text representations that generalize better\nto previously unseen instructions. Second, we propose a stochastic sampling\nscheme to reduce the considerable gap between the expert actions in training\nand sampled actions in test, so that the agent can learn to correct its own\nmistakes during long sequential action decoding. Combining the two techniques,\nwe achieve a new state of the art on the Room-to-Room benchmark with 6%\nabsolute gain over the previous best result (47% -> 53%) on the Success Rate\nweighted by Path Length metric.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 07:31:58 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Li", "Xiujun", ""], ["Li", "Chunyuan", ""], ["Xia", "Qiaolin", ""], ["Bisk", "Yonatan", ""], ["Celikyilmaz", "Asli", ""], ["Gao", "Jianfeng", ""], ["Smith", "Noah", ""], ["Choi", "Yejin", ""]]}, {"id": "1909.02293", "submitter": "Junran Peng", "authors": "Junran Peng, Ming Sun, Zhaoxiang Zhang, Tieniu Tan, Junjie Yan", "title": "Efficient Neural Architecture Transformation Searchin Channel-Level for\n  Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Neural Architecture Search has achieved great success in\nlarge-scale image classification. In contrast, there have been limited works\nfocusing on architecture search for object detection, mainly because the costly\nImageNet pre-training is always required for detectors. Training from scratch,\nas a substitute, demands more epochs to converge and brings no computation\nsaving. To overcome this obstacle, we introduce a practical neural architecture\ntransformation search(NATS)algorithm for object detection in this paper.\nInstead of searching and constructing an entire network, NATS explores the\narchitecture space on the base of existing network and reusing its weights. We\npropose a novel neural architecture search strategy in channel-level instead of\npath-level and devise a search space specially targeting at object detection.\nWith the combination of these two designs, an architecture transformation\nscheme could be discovered to adapt a network designed for image classification\nto task of object detection. Since our method is gradient-based and only\nsearches for a transformation scheme, the weights of models pretrained\ninImageNet could be utilized in both searching and retraining stage, which\nmakes the whole process very efficient. The transformed network requires no\nextra parameters and FLOPs, and is friendly to hardware optimization, which is\npractical to use in real-time application. In experiments, we demonstrate the\neffectiveness of NATSon networks like ResNet and ResNeXt. Our transformed\nnetworks, combined with various detection frameworks, achieve significant\nimprovements on the COCO dataset while keeping fast.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 10:05:57 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Peng", "Junran", ""], ["Sun", "Ming", ""], ["Zhang", "Zhaoxiang", ""], ["Tan", "Tieniu", ""], ["Yan", "Junjie", ""]]}, {"id": "1909.02294", "submitter": "Dawid Mieloch", "authors": "Dawid Mieloch, Olgierd Stankiewicz and Marek Doma\\'nski", "title": "Depth Map Estimation for Free-Viewpoint Television", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2019.2963487", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a new method of depth estimation dedicated for\nfree-viewpoint television (FTV). The estimation is performed for segments and\nthus their size can be used to control a trade-off between the quality of depth\nmaps and the processing time of their estimation. The proposed algorithm can\ntake as its input multiple arbitrarily positioned views which are\nsimultaneously used to produce multiple inter view consistent output depth\nmaps. The presented depth estimation method uses novel parallelization and\ntemporal consistency enhancement methods that significantly reduce the\nprocessing time of depth estimation. An experimental assessment of the\nproposals has been performed, based on the analysis of virtual view quality in\nFTV. The results show that the proposed method provides an improvement of the\ndepth map quality over the state of-the-art method, simultaneously reducing the\ncomplexity of depth estimation. The consistency of depth maps, which is crucial\nfor the quality of the synthesized video and thus the quality of experience of\nnavigating through a 3D scene, is also vastly improved.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 10:06:28 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Mieloch", "Dawid", ""], ["Stankiewicz", "Olgierd", ""], ["Doma\u0144ski", "Marek", ""]]}, {"id": "1909.02301", "submitter": "Yonghyun Kim", "authors": "Yonghyun Kim, Bong-Nam Kang, Daijin Kim", "title": "Detector With Focus: Normalizing Gradient In Image Pyramid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An image pyramid can extend many object detection algorithms to solve\ndetection on multiple scales. However, interpolation during the resampling\nprocess of an image pyramid causes gradient variation, which is the difference\nof the gradients between the original image and the scaled images. Our key\ninsight is that the increased variance of gradients makes the classifiers have\ndifficulty in correctly assigning categories. We prove the existence of the\ngradient variation by formulating the ratio of gradient expectations between an\noriginal image and scaled images, then propose a simple and novel gradient\nnormalization method to eliminate the effect of this variation. The proposed\nnormalization method reduce the variance in an image pyramid and allow the\nclassifier to focus on a smaller coverage. We show the improvement in three\ndifferent visual recognition problems: pedestrian detection, pose estimation,\nand object detection. The method is generally applicable to many vision\nalgorithms based on an image pyramid with gradients.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 10:18:55 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Kim", "Yonghyun", ""], ["Kang", "Bong-Nam", ""], ["Kim", "Daijin", ""]]}, {"id": "1909.02321", "submitter": "Nantheera Anantrasirichai", "authors": "N. Anantrasirichai and J. Biggs and F. Albino and D. Bull", "title": "The application of Convolutional Neural Networks to Detect Slow,\n  Sustained Deformation in InSAR Timeseries", "comments": null, "journal-ref": null, "doi": "10.1029/2019GL084993", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated systems for detecting deformation in satellite InSAR imagery could\nbe used to develop a global monitoring system for volcanic and urban\nenvironments. Here we explore the limits of a CNN for detecting slow, sustained\ndeformations in wrapped interferograms. Using synthetic data, we estimate a\ndetection threshold of 3.9cm for deformation signals alone, and 6.3cm when\natmospheric artefacts are considered. Over-wrapping reduces this to 1.8cm and\n5.0cm respectively as more fringes are generated without altering SNR. We test\nthe approach on timeseries of cumulative deformation from Campi Flegrei and\nDallol, where over-wrapping improves classication performance by up to 15%. We\npropose a mean-filtering method for combining results of different wrap\nparameters to flag deformation. At Campi Flegrei, deformation of 8.5cm/yr was\ndetected after 60days and at Dallol, deformation of 3.5cm/yr was detected after\n310 days. This corresponds to cumulative displacements of 3 cm and 4 cm\nconsistent with estimates based on synthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 11:09:28 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Anantrasirichai", "N.", ""], ["Biggs", "J.", ""], ["Albino", "F.", ""], ["Bull", "D.", ""]]}, {"id": "1909.02344", "submitter": "Xueying Shi", "authors": "Xueying Shi, Qi Dou, Cheng Xue, Jing Qin, Hao Chen, Pheng-Ann Heng", "title": "An Active Learning Approach for Reducing Annotation Cost in Skin Lesion\n  Analysis", "comments": "Accepted by MIML2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated skin lesion analysis is very crucial in clinical practice, as skin\ncancer is among the most common human malignancy. Existing approaches with deep\nlearning have achieved remarkable performance on this challenging task,\nhowever, heavily relying on large-scale labelled datasets. In this paper, we\npresent a novel active learning framework for cost-effective skin lesion\nanalysis. The goal is to effectively select and utilize much fewer labelled\nsamples, while the network can still achieve state-of-the-art performance. Our\nsample selection criteria complementarily consider both informativeness and\nrepresentativeness, derived from decoupled aspects of measuring model certainty\nand covering sample diversity. To make wise use of the selected samples, we\nfurther design a simple yet effective strategy to aggregate intra-class images\nin pixel space, as a new form of data augmentation. We validate our proposed\nmethod on data of ISIC 2017 Skin Lesion Classification Challenge for two tasks.\nUsing only up to 50% of samples, our approach can achieve state-of-the-art\nperformances on both tasks, which are comparable or exceeding the accuracies\nwith full-data training, and outperform other well-known active learning\nmethods by a large margin.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 12:00:01 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Shi", "Xueying", ""], ["Dou", "Qi", ""], ["Xue", "Cheng", ""], ["Qin", "Jing", ""], ["Chen", "Hao", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1909.02358", "submitter": "Wei Zhou", "authors": "Wei Zhou, Likun Shi, Zhibo Chen", "title": "Tensor Oriented No-Reference Light Field Image Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field image (LFI) quality assessment is becoming more and more\nimportant, which helps to better guide the acquisition, processing and\napplication of immersive media. However, due to the inherent high dimensional\ncharacteristics of LFI, the LFI quality assessment turns into a\nmulti-dimensional problem that requires consideration of the quality\ndegradation in both spatial and angular dimensions. Therefore, we propose a\nnovel Tensor oriented No-reference Light Field image Quality evaluator\n(Tensor-NLFQ) based on tensor theory. Specifically, since the LFI is regarded\nas a low-rank 4D tensor, the principle components of four oriented sub-aperture\nview stacks are obtained via Tucker decomposition. Then, the Principal\nComponent Spatial Characteristic (PCSC) is designed to measure the\nspatial-dimensional quality of LFI considering its global naturalness and local\nfrequency properties. Finally, the Tensor Angular Variation Index (TAVI) is\nproposed to measure angular consistency quality by analyzing the structural\nsimilarity distribution between the first principal component and each view in\nthe view stack. Extensive experimental results on four publicly available LFI\nquality databases demonstrate that the proposed Tensor-NLFQ model outperforms\nstate-of-the-art 2D, 3D, multi-view, and LFI quality assessment algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 12:27:44 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Zhou", "Wei", ""], ["Shi", "Likun", ""], ["Chen", "Zhibo", ""]]}, {"id": "1909.02406", "submitter": "Hafez Farazi", "authors": "Anna Kukleva, Mohammad Asif Khan, Hafez Farazi, and Sven Behnke", "title": "Utilizing Temporal Information in Deep Convolutional Network for\n  Efficient Soccer Ball Detection and Tracking", "comments": "23rd RoboCup International Symposium, Sydney, Australia, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soccer ball detection is identified as one of the critical challenges in the\nRoboCup competition. It requires an efficient vision system capable of handling\nthe task of detection with high precision and recall and providing robust and\nlow inference time. In this work, we present a novel convolutional neural\nnetwork (CNN) approach to detect the soccer ball in an image sequence. In\ncontrast to the existing methods where only the current frame or an image is\nused for the detection, we make use of the history of frames. Using history\nallows to efficiently track the ball in situations where the ball disappears or\ngets partially occluded in some of the frames. Our approach exploits\nspatio-temporal correlation and detects the ball based on the trajectory of its\nmovements. We present our results with three convolutional methods, namely\ntemporal convolutional networks (TCN), ConvLSTM, and ConvGRU. We first solve\nthe detection task for an image using fully convolutional encoder-decoder\narchitecture, and later, we use it as an input to our temporal models and\njointly learn the detection task in sequences of images. We evaluate all our\nexperiments on a novel dataset prepared as a part of this work. Furthermore, we\npresent empirical results to support the effectiveness of using the history of\nthe ball in challenging scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 13:40:27 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 10:41:39 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Kukleva", "Anna", ""], ["Khan", "Mohammad Asif", ""], ["Farazi", "Hafez", ""], ["Behnke", "Sven", ""]]}, {"id": "1909.02410", "submitter": "Alejandro Lopez-Cifuentes", "authors": "Alejandro L\\'opez-Cifuentes, Marcos Escudero-Vi\\~nolo, Jes\\'us\n  Besc\\'os, \\'Alvaro Garc\\'ia-Mart\\'in", "title": "Semantic-Aware Scene Recognition", "comments": "Paper submitted for publication to Elsevier Pattern Recognition\n  journal", "journal-ref": "Pattern Recognition Volume 102, June 2020, 107256", "doi": "10.1016/j.patcog.2020.107256", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene recognition is currently one of the top-challenging research fields in\ncomputer vision. This may be due to the ambiguity between classes: images of\nseveral scene classes may share similar objects, which causes confusion among\nthem. The problem is aggravated when images of a particular scene class are\nnotably different. Convolutional Neural Networks (CNNs) have significantly\nboosted performance in scene recognition, albeit it is still far below from\nother recognition tasks (e.g., object or image recognition). In this paper, we\ndescribe a novel approach for scene recognition based on an end-to-end\nmulti-modal CNN that combines image and context information by means of an\nattention module. Context information, in the shape of semantic segmentation,\nis used to gate features extracted from the RGB image by leveraging on\ninformation encoded in the semantic representation: the set of scene objects\nand stuff, and their relative locations. This gating process reinforces the\nlearning of indicative scene content and enhances scene disambiguation by\nrefocusing the receptive fields of the CNN towards them. Experimental results\non four publicly available datasets show that the proposed approach outperforms\nevery other state-of-the-art method while significantly reducing the number of\nnetwork parameters. All the code and data used along this paper is available at\nhttps://github.com/vpulab/Semantic-Aware-Scene-Recognition\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 13:44:03 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 10:44:22 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 16:03:52 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["L\u00f3pez-Cifuentes", "Alejandro", ""], ["Escudero-Vi\u00f1olo", "Marcos", ""], ["Besc\u00f3s", "Jes\u00fas", ""], ["Garc\u00eda-Mart\u00edn", "\u00c1lvaro", ""]]}, {"id": "1909.02436", "submitter": "Alfred Laugros", "authors": "Alfred Laugros, Alice Caplier, Matthieu Ospici", "title": "Are Adversarial Robustness and Common Perturbation Robustness\n  Independent Attributes ?", "comments": "To appear in ICCV Workshop on Real-World Recognition from Low-Quality\n  Images and Videos (RLQ) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Networks have been shown to be sensitive to common perturbations such\nas blur, Gaussian noise, rotations, etc. They are also vulnerable to some\nartificial malicious corruptions called adversarial examples. The adversarial\nexamples study has recently become very popular and it sometimes even reduces\nthe term \"adversarial robustness\" to the term \"robustness\". Yet, we do not know\nto what extent the adversarial robustness is related to the global robustness.\nSimilarly, we do not know if a robustness to various common perturbations such\nas translations or contrast losses for instance, could help with adversarial\ncorruptions. We intend to study the links between the robustnesses of neural\nnetworks to both perturbations. With our experiments, we provide one of the\nfirst benchmark designed to estimate the robustness of neural networks to\ncommon perturbations. We show that increasing the robustness to carefully\nselected common perturbations, can make neural networks more robust to unseen\ncommon perturbations. We also prove that adversarial robustness and robustness\nto common perturbations are independent. Our results make us believe that\nneural network robustness should be addressed in a broader sense.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 14:36:23 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 08:14:58 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Laugros", "Alfred", ""], ["Caplier", "Alice", ""], ["Ospici", "Matthieu", ""]]}, {"id": "1909.02466", "submitter": "Fang Wan", "authors": "Xiaosong Zhang, Fang Wan, Chang Liu, Rongrong Ji, Qixiang Ye", "title": "FreeAnchor: Learning to Match Anchors for Visual Object Detection", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern CNN-based object detectors assign anchors for ground-truth objects\nunder the restriction of object-anchor Intersection-over-Unit (IoU). In this\nstudy, we propose a learning-to-match approach to break IoU restriction,\nallowing objects to match anchors in a flexible manner. Our approach, referred\nto as FreeAnchor, updates hand-crafted anchor assignment to \"free\" anchor\nmatching by formulating detector training as a maximum likelihood estimation\n(MLE) procedure. FreeAnchor targets at learning features which best explain a\nclass of objects in terms of both classification and localization. FreeAnchor\nis implemented by optimizing detection customized likelihood and can be fused\nwith CNN-based detectors in a plug-and-play manner. Experiments on COCO\ndemonstrate that FreeAnchor consistently outperforms their counterparts with\nsignificant margins.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 14:57:53 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 05:10:29 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Zhang", "Xiaosong", ""], ["Wan", "Fang", ""], ["Liu", "Chang", ""], ["Ji", "Rongrong", ""], ["Ye", "Qixiang", ""]]}, {"id": "1909.02468", "submitter": "Vladislav Golyanik", "authors": "Vladislav Golyanik and Andr\\'e Jonas and Didier Stricker and Christian\n  Theobalt", "title": "Intrinsic Dynamic Shape Prior for Fast, Sequential and Dense Non-Rigid\n  Structure from Motion with Detection of Temporally-Disjoint Rigidity", "comments": "16 pages, 13 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While dense non-rigid structure from motion (NRSfM) has been extensively\nstudied from the perspective of the reconstructability problem over the recent\nyears, almost no attempts have been undertaken to bring it into the practical\nrealm. The reasons for the slow dissemination are the severe ill-posedness,\nhigh sensitivity to motion and deformation cues and the difficulty to obtain\nreliable point tracks in the vast majority of practical scenarios. To fill this\ngap, we propose a hybrid approach that extracts prior shape knowledge from an\ninput sequence with NRSfM and uses it as a dynamic shape prior for sequential\nsurface recovery in scenarios with recurrence. Our Dynamic Shape Prior\nReconstruction (DSPR) method can be combined with existing dense NRSfM\ntechniques while its energy functional is optimised with stochastic gradient\ndescent at real-time rates for new incoming point tracks. The proposed\nversatile framework with a new core NRSfM approach outperforms several other\nmethods in the ability to handle inaccurate and noisy point tracks, provided we\nhave access to a representative (in terms of the deformation variety) image\nsequence. Comprehensive experiments highlight convergence properties and the\naccuracy of DSPR under different disturbing effects. We also perform a joint\nstudy of tracking and reconstruction and show applications to shape compression\nand heart reconstruction under occlusions. We achieve state-of-the-art metrics\n(accuracy and compression ratios) in different scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 14:58:55 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 18:37:45 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Golyanik", "Vladislav", ""], ["Jonas", "Andr\u00e9", ""], ["Stricker", "Didier", ""], ["Theobalt", "Christian", ""]]}, {"id": "1909.02477", "submitter": "Dechun Wang", "authors": "Dechun Wang, Ning Zhang, Xinzi Sun, Pengfei Zhang, Chenxi Zhang, Yu\n  Cao, Benyuan Liu", "title": "AFP-Net: Realtime Anchor-Free Polyp Detection in Colonoscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorectal cancer (CRC) is a common and lethal disease. Globally, CRC is the\nthird most commonly diagnosed cancer in males and the second in females. For\ncolorectal cancer, the best screening test available is the colonoscopy. During\na colonoscopic procedure, a tiny camera at the tip of the endoscope generates a\nvideo of the internal mucosa of the colon. The video data are displayed on a\nmonitor for the physician to examine the lining of the entire colon and check\nfor colorectal polyps. Detection and removal of colorectal polyps are\nassociated with a reduction in mortality from colorectal cancer. However, the\nmiss rate of polyp detection during colonoscopy procedure is often high even\nfor very experienced physicians. The reason lies in the high variation of polyp\nin terms of shape, size, textural, color and illumination. Though challenging,\nwith the great advances in object detection techniques, automated polyp\ndetection still demonstrates a great potential in reducing the false negative\nrate while maintaining a high precision. In this paper, we propose a novel\nanchor free polyp detector that can localize polyps without using predefined\nanchor boxes. To further strengthen the model, we leverage a Context\nEnhancement Module and Cosine Ground truth Projection. Our approach can respond\nin real time while achieving state-of-the-art performance with 99.36% precision\nand 96.44% recall.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 15:23:34 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 18:13:04 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 17:47:55 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Wang", "Dechun", ""], ["Zhang", "Ning", ""], ["Sun", "Xinzi", ""], ["Zhang", "Pengfei", ""], ["Zhang", "Chenxi", ""], ["Cao", "Yu", ""], ["Liu", "Benyuan", ""]]}, {"id": "1909.02489", "submitter": "Wei Wei", "authors": "Wei Wei, Ling Cheng, Xianling Mao, Guangyou Zhou, and Feida Zhu", "title": "Stack-VS: Stacked Visual-Semantic Attention for Image Caption Generation", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, automatic image caption generation has been an important focus of\nthe work on multimodal translation task. Existing approaches can be roughly\ncategorized into two classes, i.e., top-down and bottom-up, the former\ntransfers the image information (called as visual-level feature) directly into\na caption, and the later uses the extracted words (called as semanticlevel\nattribute) to generate a description. However, previous methods either are\ntypically based one-stage decoder or partially utilize part of visual-level or\nsemantic-level information for image caption generation. In this paper, we\naddress the problem and propose an innovative multi-stage architecture (called\nas Stack-VS) for rich fine-gained image caption generation, via combining\nbottom-up and top-down attention models to effectively handle both visual-level\nand semantic-level information of an input image. Specifically, we also propose\na novel well-designed stack decoder model, which is constituted by a sequence\nof decoder cells, each of which contains two LSTM-layers work interactively to\nre-optimize attention weights on both visual-level feature vectors and\nsemantic-level attribute embeddings for generating a fine-gained image caption.\nExtensive experiments on the popular benchmark dataset MSCOCO show the\nsignificant improvements on different evaluation metrics, i.e., the\nimprovements on BLEU-4/CIDEr/SPICE scores are 0.372, 1.226 and 0.216,\nrespectively, as compared to the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 15:41:53 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Wei", "Wei", ""], ["Cheng", "Ling", ""], ["Mao", "Xianling", ""], ["Zhou", "Guangyou", ""], ["Zhu", "Feida", ""]]}, {"id": "1909.02511", "submitter": "Bo Zhou", "authors": "Bo Zhou, Adam P. Harrison, Jiawen Yao, Chi-Tung Cheng, Jing Xiao,\n  Chien-Hung Liao, Le Lu", "title": "CT Data Curation for Liver Patients: Phase Recognition in Dynamic\n  Contrast-Enhanced CT", "comments": "11 pages, accepted by 2019 MICCAI - Medical Image Learning with Less\n  Labels and Imperfect Data Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the demand for more descriptive machine learning models grows within\nmedical imaging, bottlenecks due to data paucity will exacerbate. Thus,\ncollecting enough large-scale data will require automated tools to harvest\ndata/label pairs from messy and real-world datasets, such as hospital PACS.\nThis is the focus of our work, where we present a principled data curation tool\nto extract multi-phase CT liver studies and identify each scan's phase from a\nreal-world and heterogenous hospital PACS dataset. Emulating a typical\ndeployment scenario, we first obtain a set of noisy labels from our\ninstitutional partners that are text mined using simple rules from DICOM tags.\nWe train a deep learning system, using a customized and streamlined 3D SE\narchitecture, to identify non-contrast, arterial, venous, and delay phase\ndynamic CT liver scans, filtering out anything else, including other types of\nliver contrast studies. To exploit as much training data as possible, we also\nintroduce an aggregated cross entropy loss that can learn from scans only\nidentified as \"contrast\". Extensive experiments on a dataset of 43K scans of\n7680 patient imaging studies demonstrate that our 3DSE architecture, armed with\nour aggregated loss, can achieve a mean F1 of 0.977 and can correctly harvest\nup to 92.7% of studies, which significantly outperforms the text-mined and\nstandard-loss approach, and also outperforms other, and more complex, model\narchitectures.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 16:31:40 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 21:48:31 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Zhou", "Bo", ""], ["Harrison", "Adam P.", ""], ["Yao", "Jiawen", ""], ["Cheng", "Chi-Tung", ""], ["Xiao", "Jing", ""], ["Liao", "Chien-Hung", ""], ["Lu", "Le", ""]]}, {"id": "1909.02518", "submitter": "Christian Richardt", "authors": "Hyeongwoo Kim, Mohamed Elgharib, Michael Zollh\\\"ofer, Hans-Peter\n  Seidel, Thabo Beeler, Christian Richardt, Christian Theobalt", "title": "Neural Style-Preserving Visual Dubbing", "comments": "SIGGRAPH Asia 2019", "journal-ref": null, "doi": "10.1145/3355089.3356500", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dubbing is a technique for translating video content from one language to\nanother. However, state-of-the-art visual dubbing techniques directly copy\nfacial expressions from source to target actors without considering\nidentity-specific idiosyncrasies such as a unique type of smile. We present a\nstyle-preserving visual dubbing approach from single video inputs, which\nmaintains the signature style of target actors when modifying facial\nexpressions, including mouth motions, to match foreign languages. At the heart\nof our approach is the concept of motion style, in particular for facial\nexpressions, i.e., the person-specific expression change that is yet another\nessential factor beyond visual accuracy in face editing applications. Our\nmethod is based on a recurrent generative adversarial network that captures the\nspatiotemporal co-activation of facial expressions, and enables generating and\nmodifying the facial expressions of the target actor while preserving their\nstyle. We train our model with unsynchronized source and target videos in an\nunsupervised manner using cycle-consistency and mouth expression losses, and\nsynthesize photorealistic video frames using a layered neural face renderer.\nOur approach generates temporally coherent results, and handles dynamic\nbackgrounds. Our results show that our dubbing approach maintains the\nidiosyncratic style of the target actor better than previous approaches, even\nfor widely differing source and target actors.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 16:41:20 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 10:53:40 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Kim", "Hyeongwoo", ""], ["Elgharib", "Mohamed", ""], ["Zollh\u00f6fer", "Michael", ""], ["Seidel", "Hans-Peter", ""], ["Beeler", "Thabo", ""], ["Richardt", "Christian", ""], ["Theobalt", "Christian", ""]]}, {"id": "1909.02533", "submitter": "David Novotn\\'y", "authors": "David Novotny, Nikhila Ravi, Benjamin Graham, Natalia Neverova, Andrea\n  Vedaldi", "title": "C3DPO: Canonical 3D Pose Networks for Non-Rigid Structure From Motion", "comments": "Added a link to the source code into the abstract", "journal-ref": "IEEE/CVF International Conference on Computer Vision 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose C3DPO, a method for extracting 3D models of deformable objects\nfrom 2D keypoint annotations in unconstrained images. We do so by learning a\ndeep network that reconstructs a 3D object from a single view at a time,\naccounting for partial occlusions, and explicitly factoring the effects of\nviewpoint changes and object deformations. In order to achieve this\nfactorization, we introduce a novel regularization technique. We first show\nthat the factorization is successful if, and only if, there exists a certain\ncanonicalization function of the reconstructed shapes. Then, we learn the\ncanonicalization function together with the reconstruction one, which\nconstrains the result to be consistent. We demonstrate state-of-the-art\nreconstruction results for methods that do not use ground-truth 3D supervision\nfor a number of benchmarks, including Up3D and PASCAL3D+. Source code has been\nmade available at https://github.com/facebookresearch/c3dpo_nrsfm.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 17:16:15 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 16:39:18 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Novotny", "David", ""], ["Ravi", "Nikhila", ""], ["Graham", "Benjamin", ""], ["Neverova", "Natalia", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1909.02543", "submitter": "Md Sirajus Salekin", "authors": "Md Sirajus Salekin, Ghada Zamzmi, Rahul Paul, Dmitry Goldgof,\n  Rangachar Kasturi, Thao Ho, Yu Sun", "title": "Harnessing the Power of Deep Learning Methods in Healthcare: Neonatal\n  Pain Assessment from Crying Sound", "comments": "Accepted to IEEE HI-POCT 2019", "journal-ref": null, "doi": "10.1109/HI-POCT45284.2019.8962827", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neonatal pain assessment in clinical environments is challenging as it is\ndiscontinuous and biased. Facial/body occlusion can occur in such settings due\nto clinical condition, developmental delays, prone position, or other external\nfactors. In such cases, crying sound can be used to effectively assess neonatal\npain. In this paper, we investigate the use of a novel CNN architecture (N-CNN)\nalong with other CNN architectures (VGG16 and ResNet50) for assessing pain from\ncrying sounds of neonates. The experimental results demonstrate that using our\nnovel N-CNN for assessing pain from the sounds of neonates has a strong\nclinical potential and provides a viable alternative to the current assessment\npractice.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 17:33:45 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Salekin", "Md Sirajus", ""], ["Zamzmi", "Ghada", ""], ["Paul", "Rahul", ""], ["Goldgof", "Dmitry", ""], ["Kasturi", "Rangachar", ""], ["Ho", "Thao", ""], ["Sun", "Yu", ""]]}, {"id": "1909.02548", "submitter": "Mihir Chauhan", "authors": "Mihir Chauhan, Mohammad Abuzar Shaikh and Sargur N. Srihari", "title": "Explanation based Handwriting Verification", "comments": "Presented at BMVC 2019: Workshop on Interpretable and Explainable\n  Machine Vision, Cardiff, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning system have drawback that their output is not accompanied with\nex-planation. In a domain such as forensic handwriting verification it is\nessential to provideexplanation to jurors. The goal of handwriting verification\nis to find a measure of confi-dence whether the given handwritten samples are\nwritten by the same or different writer.We propose a method to generate\nexplanations for the confidence provided by convolu-tional neural network (CNN)\nwhich maps the input image to 15 annotations (features)provided by experts. Our\nsystem comprises of: (1) Feature learning network (FLN),a differentiable\nsystem, (2) Inference module for providing explanations. Furthermore,inference\nmodule provides two types of explanations: (a) Based on cosine\nsimilaritybetween categorical probabilities of each feature, (b) Based on\nLog-Likelihood Ratio(LLR) using directed probabilistic graphical model. We\nperform experiments using acombination of feature learning network (FLN) and\neach inference module. We evaluateour system using XAI-AND dataset, containing\n13700 handwritten samples and 15 cor-responding expert examined features for\neach sample. The dataset is released for publicuse and the methods can be\nextended to provide explanations on other verification taskslike face\nverification and bio-medical comparison. This dataset can serve as the basis\nand benchmark for future research in explanation based handwriting\nverification. The code is available on github.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 19:50:07 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Chauhan", "Mihir", ""], ["Shaikh", "Mohammad Abuzar", ""], ["Srihari", "Sargur N.", ""]]}, {"id": "1909.02562", "submitter": "Houssem Ben Braiek", "authors": "Houssem Ben Braiek and Foutse Khomh", "title": "TFCheck : A TensorFlow Library for Detecting Training Issues in Neural\n  Network Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing inclusion of Machine Learning (ML) models in safety critical\nsystems like autonomous cars have led to the development of multiple\nmodel-based ML testing techniques. One common denominator of these testing\ntechniques is their assumption that training programs are adequate and\nbug-free. These techniques only focus on assessing the performance of the\nconstructed model using manually labeled data or automatically generated data.\nHowever, their assumptions about the training program are not always true as\ntraining programs can contain inconsistencies and bugs. In this paper, we\nexamine training issues in ML programs and propose a catalog of verification\nroutines that can be used to detect the identified issues, automatically. We\nimplemented the routines in a Tensorflow-based library named TFCheck. Using\nTFCheck, practitioners can detect the aforementioned issues automatically. To\nassess the effectiveness of TFCheck, we conducted a case study with real-world,\nmutants, and synthetic training programs. Results show that TFCheck can\nsuccessfully detect training issues in ML code implementations.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 13:21:22 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Braiek", "Houssem Ben", ""], ["Khomh", "Foutse", ""]]}, {"id": "1909.02563", "submitter": "Houssem Ben Braiek", "authors": "Houssem Ben Braiek and Foutse khomh", "title": "DeepEvolution: A Search-Based Testing Approach for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing inclusion of Deep Learning (DL) models in safety-critical\nsystems such as autonomous vehicles have led to the development of multiple\nmodel-based DL testing techniques. One common denominator of these testing\ntechniques is the automated generation of test cases, e.g., new inputs\ntransformed from the original training data with the aim to optimize some test\nadequacy criteria. So far, the effectiveness of these approaches has been\nhindered by their reliance on random fuzzing or transformations that do not\nalways produce test cases with a good diversity. To overcome these limitations,\nwe propose, DeepEvolution, a novel search-based approach for testing DL models\nthat relies on metaheuristics to ensure a maximum diversity in generated test\ncases. We assess the effectiveness of DeepEvolution in testing computer-vision\nDL models and found that it significantly increases the neuronal coverage of\ngenerated test cases. Moreover, using DeepEvolution, we could successfully find\nseveral corner-case behaviors. Finally, DeepEvolution outperformed Tensorfuzz\n(a coverage-guided fuzzing tool developed at Google Brain) in detecting latent\ndefects introduced during the quantization of the models. These results suggest\nthat search-based approaches can help build effective testing tools for DL\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 13:42:08 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Braiek", "Houssem Ben", ""], ["khomh", "Foutse", ""]]}, {"id": "1909.02576", "submitter": "Brian Davis", "authors": "Brian Davis, Bryan Morse, Scott Cohen, Brian Price, Chris Tensmeyer", "title": "Deep Visual Template-Free Form Parsing", "comments": "Accepted at ICDAR 2019. Updated results with average of repeated\n  experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic, template-free extraction of information from form images is\nchallenging due to the variety of form layouts. This is even more challenging\nfor historical forms due to noise and degradation. A crucial part of the\nextraction process is associating input text with pre-printed labels. We\npresent a learned, template-free solution to detecting pre-printed text and\ninput text/handwriting and predicting pair-wise relationships between them.\nWhile previous approaches to this problem have been focused on clean images and\nclear layouts, we show our approach is effective in the domain of noisy,\ndegraded, and varied form images. We introduce a new dataset of historical form\nimages (late 1800s, early 1900s) for training and validating our approach. Our\nmethod uses a convolutional network to detect pre-printed text and input text\nlines. We pool features from the detection network to classify possible\nrelationships in a language-agnostic way. We show that our proposed pairing\nmethod outperforms heuristic rules and that visual features are critical to\nobtaining high accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 18:00:09 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 21:58:56 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Davis", "Brian", ""], ["Morse", "Bryan", ""], ["Cohen", "Scott", ""], ["Price", "Brian", ""], ["Tensmeyer", "Chris", ""]]}, {"id": "1909.02620", "submitter": "Ozan Ciga", "authors": "Ozan Ciga, Jianan Chen, Anne Martel", "title": "Multi-layer Domain Adaptation for Deep Convolutional Networks", "comments": "To be presented on Domain Adaptation and Representation Transfer 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their success in many computer vision tasks, convolutional networks\ntend to require large amounts of labeled data to achieve generalization.\nFurthermore, the performance is not guaranteed on a sample from an unseen\ndomain at test time, if the network was not exposed to similar samples from\nthat domain at training time. This hinders the adoption of these techniques in\nclinical setting where the imaging data is scarce, and where the intra- and\ninter-domain variance of the data can be substantial. We propose a domain\nadaptation technique that is especially suitable for deep networks to alleviate\nthis requirement of labeled data. Our method utilizes gradient reversal layers\nand Squeezeand-Excite modules to stabilize the training in deep networks. The\nproposed method was applied to publicly available histopathology and chest\nX-ray databases and achieved superior performance to existing state-of-the-art\nnetworks with and without domain adaptation. Depending on the application, our\nmethod can improve multi-class classification accuracy by 5-20% compared to\nDANN introduced in (Ganin, 2014).\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 20:24:49 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Ciga", "Ozan", ""], ["Chen", "Jianan", ""], ["Martel", "Anne", ""]]}, {"id": "1909.02641", "submitter": "Jinsoo Choi", "authors": "Jinsoo Choi, In So Kweon", "title": "Deep Iterative Frame Interpolation for Full-frame Video Stabilization", "comments": "Accepted to ACM Transactions on Graphics, To be presented at SIGGRAPH\n  Asia 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video stabilization is a fundamental and important technique for higher\nquality videos. Prior works have extensively explored video stabilization, but\nmost of them involve cropping of the frame boundaries and introduce moderate\nlevels of distortion. We present a novel deep approach to video stabilization\nwhich can generate video frames without cropping and low distortion. The\nproposed framework utilizes frame interpolation techniques to generate in\nbetween frames, leading to reduced inter-frame jitter. Once applied in an\niterative fashion, the stabilization effect becomes stronger. A major advantage\nis that our framework is end-to-end trainable in an unsupervised manner. In\naddition, our method is able to run in near real-time (15 fps). To the best of\nour knowledge, this is the first work to propose an unsupervised deep approach\nto full-frame video stabilization. We show the advantages of our method through\nquantitative and qualitative evaluations comparing to the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 21:38:45 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Choi", "Jinsoo", ""], ["Kweon", "In So", ""]]}, {"id": "1909.02642", "submitter": "Anne Martel", "authors": "Linde S. Hesse, Grey Kuling, Mitko Veta, Anne L. Martel", "title": "Intensity augmentation for domain transfer of whole breast segmentation\n  in MRI", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of the breast from the chest wall is an important first step\nin the analysis of breast magnetic resonance images. 3D U-nets have been shown\nto obtain high segmentation accuracy and appear to generalize well when trained\non one scanner type and tested on another scanner, provided that a very similar\nT1-weighted MR protocol is used. There has, however, been little work\naddressing the problem of domain adaptation when image intensities or patient\norientation differ markedly between the training set and an unseen test set. To\novercome the domain shift we propose to apply extensive intensity augmentation\nin addition to geometric augmentation during training. We explored both style\ntransfer and a novel intensity remapping approach as intensity augmentation\nstrategies. For our experiments, we trained a 3D U-net on T1-weighted scans and\ntested on T2-weighted scans. By applying intensity augmentation we increased\nsegmentation performance from a DSC of 0.71 to 0.90. This performance is very\nclose to the baseline performance of training and testing on T2-weighted scans\n(0.92). Furthermore, we applied our network to an independent test set made up\nof publicly available scans acquired using a T1-weighted TWIST sequence and a\ndifferent coil configuration. On this dataset we obtained a performance of\n0.89, close to the inter-observer variability of the ground truth segmentations\n(0.92). Our results show that using intensity augmentation in addition to\ngeometric augmentation is a suitable method to overcome the intensity domain\nshift and we expect it to be useful for a wide range of segmentation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 21:40:02 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Hesse", "Linde S.", ""], ["Kuling", "Grey", ""], ["Veta", "Mitko", ""], ["Martel", "Anne L.", ""]]}, {"id": "1909.02651", "submitter": "Henghui Ding", "authors": "Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu and Gang Wang", "title": "Semantic Correlation Promoted Shape-Variant Context for Segmentation", "comments": "CVPR 2019, Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context is essential for semantic segmentation. Due to the diverse shapes of\nobjects and their complex layout in various scene images, the spatial scales\nand shapes of contexts for different objects have very large variation. It is\nthus ineffective or inefficient to aggregate various context information from a\npredefined fixed region. In this work, we propose to generate a scale- and\nshape-variant semantic mask for each pixel to confine its contextual region. To\nthis end, we first propose a novel paired convolution to infer the semantic\ncorrelation of the pair and based on that to generate a shape mask. Using the\ninferred spatial scope of the contextual region, we propose a shape-variant\nconvolution, of which the receptive field is controlled by the shape mask that\nvaries with the appearance of input. In this way, the proposed network\naggregates the context information of a pixel from its semantic-correlated\nregion instead of a predefined fixed region. Furthermore, this work also\nproposes a labeling denoising model to reduce wrong predictions caused by the\nnoisy low-level features. Without bells and whistles, the proposed segmentation\nnetwork achieves new state-of-the-arts consistently on the six public\nsegmentation datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 22:09:41 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Ding", "Henghui", ""], ["Jiang", "Xudong", ""], ["Shuai", "Bing", ""], ["Liu", "Ai Qun", ""], ["Wang", "Gang", ""]]}, {"id": "1909.02680", "submitter": "Amir Erfan Eshratifar", "authors": "Amir Erfan Eshratifar, David Eigen, Michael Gormish, Massoud Pedram", "title": "Coarse2Fine: A Two-stage Training Method for Fine-grained Visual\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small inter-class and large intra-class variations are the main challenges in\nfine-grained visual classification. Objects from different classes share\nvisually similar structures and objects in the same class can have different\nposes and viewpoints. Therefore, the proper extraction of discriminative local\nfeatures (e.g. bird's beak or car's headlight) is crucial. Most of the recent\nsuccesses on this problem are based upon the attention models which can\nlocalize and attend the local discriminative objects parts. In this work, we\npropose a training method for visual attention networks, Coarse2Fine, which\ncreates a differentiable path from the input space to the attended feature\nmaps. Coarse2Fine learns an inverse mapping function from the attended feature\nmaps to the informative regions in the raw image, which will guide the\nattention maps to better attend the fine-grained features. We show Coarse2Fine\nand orthogonal initialization of the attention weights can surpass the\nstate-of-the-art accuracies on common fine-grained classification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 00:09:17 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Eshratifar", "Amir Erfan", ""], ["Eigen", "David", ""], ["Gormish", "Michael", ""], ["Pedram", "Massoud", ""]]}, {"id": "1909.02701", "submitter": "Kunpeng Li", "authors": "Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, Yun Fu", "title": "Visual Semantic Reasoning for Image-Text Matching", "comments": "Accepted to ICCV 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-text matching has been a hot research topic bridging the vision and\nlanguage areas. It remains challenging because the current representation of\nimage usually lacks global semantic concepts as in its corresponding text\ncaption. To address this issue, we propose a simple and interpretable reasoning\nmodel to generate visual representation that captures key objects and semantic\nconcepts of a scene. Specifically, we first build up connections between image\nregions and perform reasoning with Graph Convolutional Networks to generate\nfeatures with semantic relationships. Then, we propose to use the gate and\nmemory mechanism to perform global semantic reasoning on these\nrelationship-enhanced features, select the discriminative information and\ngradually generate the representation for the whole scene. Experiments validate\nthat our method achieves a new state-of-the-art for the image-text matching on\nMS-COCO and Flickr30K datasets. It outperforms the current best method by 6.8%\nrelatively for image retrieval and 4.8% relatively for caption retrieval on\nMS-COCO (Recall@1 using 1K test set). On Flickr30K, our model improves image\nretrieval by 12.6% relatively and caption retrieval by 5.8% relatively\n(Recall@1). Our code is available at https://github.com/KunpengLi1994/VSRN.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 03:23:01 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Li", "Kunpeng", ""], ["Zhang", "Yulun", ""], ["Li", "Kai", ""], ["Li", "Yuanyuan", ""], ["Fu", "Yun", ""]]}, {"id": "1909.02710", "submitter": "Dmitry Konovalov", "authors": "Dmitry A. Konovalov, Alzayat Saleh, Dina B. Efremova, Jose A.\n  Domingos, Dean R. Jerry", "title": "Automatic Weight Estimation of Harvested Fish from Images", "comments": "Accepted for IEEE Digital Image Computing: Techniques and\n  Applications, 2019 (DICTA 2019), 2-4 December 2019 in Perth, Australia,\n  http://dicta2019.dictaconference.org/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximately 2,500 weights and corresponding images of harvested Lates\ncalcarifer (Asian seabass or barramundi) were collected at three different\nlocations in Queensland, Australia. Two instances of the LinkNet-34\nsegmentation Convolutional Neural Network (CNN) were trained. The first one was\ntrained on 200 manually segmented fish masks with excluded fins and tails. The\nsecond was trained on 100 whole-fish masks. The two CNNs were applied to the\nrest of the images and yielded automatically segmented masks. The one-factor\nand two-factor simple mathematical weight-from-area models were fitted on 1072\narea-weight pairs from the first two locations, where area values were\nextracted from the automatically segmented masks. When applied to 1,400 test\nimages (from the third location), the one-factor whole-fish mask model achieved\nthe best mean absolute percentage error (MAPE), MAPE=4.36%. Direct\nweight-from-image regression CNNs were also trained, where the no-fins based\nCNN performed best on the test images with MAPE=4.28%.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 04:51:40 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Konovalov", "Dmitry A.", ""], ["Saleh", "Alzayat", ""], ["Efremova", "Dina B.", ""], ["Domingos", "Jose A.", ""], ["Jerry", "Dean R.", ""]]}, {"id": "1909.02729", "submitter": "Guneet Singh Dhillon", "authors": "Guneet S. Dhillon, Pratik Chaudhari, Avinash Ravichandran, Stefano\n  Soatto", "title": "A Baseline for Few-Shot Image Classification", "comments": null, "journal-ref": "International Conference on Learning Representations (ICLR), 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning a deep network trained with the standard cross-entropy loss is a\nstrong baseline for few-shot learning. When fine-tuned transductively, this\noutperforms the current state-of-the-art on standard datasets such as\nMini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100 with the same\nhyper-parameters. The simplicity of this approach enables us to demonstrate the\nfirst few-shot learning results on the ImageNet-21k dataset. We find that using\na large number of meta-training classes results in high few-shot accuracies\neven for a large number of few-shot classes. We do not advocate our approach as\nthe solution for few-shot learning, but simply use the results to highlight\nlimitations of current benchmarks and few-shot protocols. We perform extensive\nstudies on benchmark datasets to propose a metric that quantifies the\n\"hardness\" of a few-shot episode. This metric can be used to report the\nperformance of few-shot algorithms in a more systematic way.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 06:14:03 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 02:08:39 GMT"}, {"version": "v3", "created": "Sun, 1 Mar 2020 03:19:17 GMT"}, {"version": "v4", "created": "Mon, 23 Mar 2020 01:37:04 GMT"}, {"version": "v5", "created": "Wed, 21 Oct 2020 21:13:04 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Dhillon", "Guneet S.", ""], ["Chaudhari", "Pratik", ""], ["Ravichandran", "Avinash", ""], ["Soatto", "Stefano", ""]]}, {"id": "1909.02742", "submitter": "Shaofeng Li", "authors": "Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, Xinpeng\n  Zhang", "title": "Invisible Backdoor Attacks on Deep Neural Networks via Steganography and\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been proven vulnerable to backdoor attacks,\nwhere hidden features (patterns) trained to a normal model, which is only\nactivated by some specific input (called triggers), trick the model into\nproducing unexpected behavior. In this paper, we create covert and scattered\ntriggers for backdoor attacks, invisible backdoors, where triggers can fool\nboth DNN models and human inspection. We apply our invisible backdoors through\ntwo state-of-the-art methods of embedding triggers for backdoor attacks. The\nfirst approach on Badnets embeds the trigger into DNNs through steganography.\nThe second approach of a trojan attack uses two types of additional\nregularization terms to generate the triggers with irregular shape and size. We\nuse the Attack Success Rate and Functionality to measure the performance of our\nattacks. We introduce two novel definitions of invisibility for human\nperception; one is conceptualized by the Perceptual Adversarial Similarity\nScore (PASS) and the other is Learned Perceptual Image Patch Similarity\n(LPIPS). We show that the proposed invisible backdoors can be fairly effective\nacross various DNN models as well as four datasets MNIST, CIFAR-10, CIFAR-100,\nand GTSRB, by measuring their attack success rates for the adversary,\nfunctionality for the normal users, and invisibility scores for the\nadministrators. We finally argue that the proposed invisible backdoor attacks\ncan effectively thwart the state-of-the-art trojan backdoor detection\napproaches, such as Neural Cleanse and TABOR.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 07:11:26 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 03:17:24 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2020 04:14:46 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Li", "Shaofeng", ""], ["Xue", "Minhui", ""], ["Zhao", "Benjamin Zi Hao", ""], ["Zhu", "Haojin", ""], ["Zhang", "Xinpeng", ""]]}, {"id": "1909.02747", "submitter": "Takehisa Yamakita", "authors": "Takehisa Yamakita", "title": "Eelgrass beds and oyster farming at a lagoon before and after the Great\n  East Japan Earthquake 2011: potential to apply deep learning at a coastal\n  area", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a small number of case studies of automatic land cover\nclassification on the coastal area. Here, I test extraction of seagrass beds,\nsandy area, oyster farming rafts at Mangoku-ura Lagoon, Miyagi, Japan by\ncomparing manual tracing, simple image segmentation, and image transformation\nusing deep learning. The result was used to extract the changes before and\nafter the earthquake and tsunami. The output resolution was best in the image\ntransformation method, which showed more than 69% accuracy for vegetation\nclassification by an assessment using random points on independent test data.\nThe distribution of oyster farming rafts was detected by the segmentation\nmodel. Assessment of the change before and after the earthquake by the manual\ntracing and image transformation result revealed increase of sand area and\ndecrease of the vegetation. By the segmentation model only the decrease of the\noyster farming was detected. These results demonstrate the potential to extract\nthe spatial pattern of these elements after an earthquake and tsunami. Index\nTerms: Great East Japan Earthquake of 2011, Land use land cover (LULC),\nZosteracea seagrass, cultured oyster, deep learning, Mangoku Bay\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 07:34:58 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Yamakita", "Takehisa", ""]]}, {"id": "1909.02749", "submitter": "Kevin Shih", "authors": "Kevin J. Shih, Aysegul Dundar, Animesh Garg, Robert Pottorf, Andrew\n  Tao, Bryan Catanzaro", "title": "Video Interpolation and Prediction with Unsupervised Landmarks", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction and interpolation for long-range video data involves the complex\ntask of modeling motion trajectories for each visible object, occlusions and\ndis-occlusions, as well as appearance changes due to viewpoint and lighting.\nOptical flow based techniques generalize but are suitable only for short\ntemporal ranges. Many methods opt to project the video frames to a low\ndimensional latent space, achieving long-range predictions. However, these\nlatent representations are often non-interpretable, and therefore difficult to\nmanipulate. This work poses video prediction and interpolation as unsupervised\nlatent structure inference followed by a temporal prediction in this latent\nspace. The latent representations capture foreground semantics without explicit\nsupervision such as keypoints or poses. Further, as each landmark can be mapped\nto a coordinate indicating where a semantic part is positioned, we can reliably\ninterpolate within the coordinate domain to achieve predictable motion\ninterpolation. Given an image decoder capable of mapping these landmarks back\nto the image domain, we are able to achieve high-quality long-range video\ninterpolation and extrapolation by operating on the landmark representation\nspace.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 07:40:27 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Shih", "Kevin J.", ""], ["Dundar", "Aysegul", ""], ["Garg", "Animesh", ""], ["Pottorf", "Robert", ""], ["Tao", "Andrew", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1909.02755", "submitter": "Pankaj Mishra", "authors": "Claudio Piciarelli, Pankaj Mishra and Gian Luca Foresti", "title": "Image anomaly detection with capsule networks and imbalanced datasets", "comments": "Published in conference ICIAP 2019", "journal-ref": "[978-3-030-30641-0, ICIAP 2019, Part I, LNCS 11751, paper approval\n  (489497_1_En, Chapter 23)]", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image anomaly detection consists in finding images with anomalous, unusual\npatterns with respect to a set of normal data. Anomaly detection can be applied\nto several fields and has numerous practical applications, e.g. in industrial\ninspection, medical imaging, security enforcement, etc.. However, anomaly\ndetection techniques often still rely on traditional approaches such as\none-class Support Vector Machines, while the topic has not been fully developed\nyet in the context of modern deep learning approaches. In this paper, we\npropose an image anomaly detection system based on capsule networks under the\nassumption that anomalous data are available for training but their amount is\nscarce.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 08:07:56 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Piciarelli", "Claudio", ""], ["Mishra", "Pankaj", ""], ["Foresti", "Gian Luca", ""]]}, {"id": "1909.02759", "submitter": "Sebastian Werner", "authors": "Sebastian Werner, Henrik Sch\\\"afer, Matthias Hullin", "title": "A new operation mode for depth-focused high-sensitivity ToF range\n  finding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce pulsed correlation time-of-flight (PC-ToF) sensing, a new\noperation mode for correlation time-of-flight range sensors that combines a\nsub-nanosecond laser pulse source with a rectangular demodulation at the sensor\nside. In contrast to previous work, our proposed measurement scheme attempts\nnot to optimize depth accuracy over the full measurement: With PC-ToF we trade\nthe global sensitivity of a standard C-ToF setup for measurements with strongly\nlocalized high sensitivity -- we greatly enhance the depth resolution for the\nacquisition of scene features around a desired depth of interest. Using\nreal-world experiments, we show that our technique is capable of achieving\ndepth resolutions down to 2mm using a modulation frequency as low as 10MHz and\nan optical power as low as 1mW. This makes PC-ToF especially viable for\nlow-power applications.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 08:16:59 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Werner", "Sebastian", ""], ["Sch\u00e4fer", "Henrik", ""], ["Hullin", "Matthias", ""]]}, {"id": "1909.02765", "submitter": "Zhuoran Ji", "authors": "Zhuoran Ji", "title": "ILP-M Conv: Optimize Convolution Algorithm for Single-Image Convolution\n  Neural Network Inference on Mobile GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution neural networks are widely used for mobile applications. However,\nGPU convolution algorithms are designed for mini-batch neural network training,\nthe single-image convolution neural network inference algorithm on mobile GPUs\nis not well-studied. After discussing the usage difference and examining the\nexisting convolution algorithms, we proposed the HNTMP convolution algorithm.\nThe HNTMP convolution algorithm achieves $14.6 \\times$ speedup than the most\npopular \\textit{im2col} convolution algorithm, and $2.30 \\times$ speedup than\nthe fastest existing convolution algorithm (direct convolution) as far as we\nknow.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 08:36:05 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 06:04:24 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Ji", "Zhuoran", ""]]}, {"id": "1909.02799", "submitter": "Mikhail Belyaev", "authors": "Boris Shirokikh, Alexandra Dalechina, Alexey Shevtsov, Egor Krivov,\n  Valery Kostjuchenko, Amayak Durgaryan, Mikhail Galkin, Ivan Osinov, Andrey\n  Golanov, Mikhail Belyaev", "title": "Deep Learning for Brain Tumor Segmentation in Radiosurgery: Prospective\n  Clinical Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereotactic radiosurgery is a minimally-invasive treatment option for a\nlarge number of patients with intracranial tumors. As part of the therapy\ntreatment, accurate delineation of brain tumors is of great importance.\nHowever, slice-by-slice manual segmentation on T1c MRI could be time-consuming\n(especially for multiple metastases) and subjective. In our work, we compared\nseveral deep convolutional networks architectures and training procedures and\nevaluated the best model in a radiation therapy department for three types of\nbrain tumors: meningiomas, schwannomas and multiple brain metastases. The\ndeveloped semiautomatic segmentation system accelerates the contouring process\nby 2.2 times on average and increases inter-rater agreement from 92.0% to\n96.5%.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 10:05:24 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 07:23:54 GMT"}, {"version": "v3", "created": "Wed, 18 Dec 2019 08:35:00 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Shirokikh", "Boris", ""], ["Dalechina", "Alexandra", ""], ["Shevtsov", "Alexey", ""], ["Krivov", "Egor", ""], ["Kostjuchenko", "Valery", ""], ["Durgaryan", "Amayak", ""], ["Galkin", "Mikhail", ""], ["Osinov", "Ivan", ""], ["Golanov", "Andrey", ""], ["Belyaev", "Mikhail", ""]]}, {"id": "1909.02820", "submitter": "Minyoung Kim", "authors": "Minyoung Kim, Yuting Wang, Pritish Sahu, Vladimir Pavlovic", "title": "Bayes-Factor-VAE: Hierarchical Bayesian Deep Auto-Encoder Models for\n  Factor Disentanglement", "comments": "International Conference on Computer Vision (ICCV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a family of novel hierarchical Bayesian deep auto-encoder models\ncapable of identifying disentangled factors of variability in data. While many\nrecent attempts at factor disentanglement have focused on sophisticated\nlearning objectives within the VAE framework, their choice of a standard normal\nas the latent factor prior is both suboptimal and detrimental to performance.\nOur key observation is that the disentangled latent variables responsible for\nmajor sources of variability, the relevant factors, can be more appropriately\nmodeled using long-tail distributions. The typical Gaussian priors are, on the\nother hand, better suited for modeling of nuisance factors. Motivated by this,\nwe extend the VAE to a hierarchical Bayesian model by introducing hyper-priors\non the variances of Gaussian latent priors, mimicking an infinite mixture,\nwhile maintaining tractable learning and inference of the traditional VAEs.\nThis analysis signifies the importance of partitioning and treating in a\ndifferent manner the latent dimensions corresponding to relevant factors and\nnuisances. Our proposed models, dubbed Bayes-Factor-VAEs, are shown to\noutperform existing methods both quantitatively and qualitatively in terms of\nlatent disentanglement across several challenging benchmark tasks.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 11:20:42 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Kim", "Minyoung", ""], ["Wang", "Yuting", ""], ["Sahu", "Pritish", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "1909.02829", "submitter": "Patrick Horain", "authors": "Priyadarshini Adyasha Pattanaik (INTERMEDIA), Zelong Wang (TSP),\n  Patrick Horain (INTERMEDIA)", "title": "Deep CNN frameworks comparison for malaria diagnosis", "comments": null, "journal-ref": "IMVIP 2019 Irish Machine Vision and Image Processing Conference,\n  Sep 2019, Dublin, Ireland", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare Deep Convolutional Neural Networks (DCNN) frameworks, namely\nAlexNet and VGGNet, for the classification of healthy and malaria-infected\ncells in large, grayscale, low quality and low resolution microscopic images,\nin the case only a small training set is available. Experimental results\ndeliver promising results on the path to quick, automatic and precise\nclassification in unstained images.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 11:46:04 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Pattanaik", "Priyadarshini Adyasha", "", "INTERMEDIA"], ["Wang", "Zelong", "", "TSP"], ["Horain", "Patrick", "", "INTERMEDIA"]]}, {"id": "1909.02835", "submitter": "Yeshwanth Napolean", "authors": "Yeshwanth Napolean, Priadi Teguh Wibowo, Jan van Gemert", "title": "Running Event Visualization using Videos from Multiple Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizing the trajectory of multiple runners with videos collected at\ndifferent points in a race could be useful for sports performance analysis. The\nvideos and the trajectories can also aid in athlete health monitoring. While\nthe runners unique ID and their appearance are distinct, the task is not\nstraightforward because the video data does not contain explicit information as\nto which runners appear in each of the videos. There is no direct supervision\nof the model in tracking athletes, only filtering steps to remove irrelevant\ndetections. Other factors of concern include occlusion of runners and harsh\nillumination. To this end, we identify two methods for runner identification at\ndifferent points of the event, for determining their trajectory. One is scene\ntext detection which recognizes the runners by detecting a unique 'bib number'\nattached to their clothes and the other is person re-identification which\ndetects the runners based on their appearance. We train our method without\nground truth but to evaluate the proposed methods, we create a ground truth\ndatabase which consists of video and frame interval information where the\nrunners appear. The videos in the dataset was recorded by nine cameras at\ndifferent locations during the a marathon event. This data is annotated with\nbib numbers of runners appearing in each video. The bib numbers of runners\nknown to occur in the frame are used to filter irrelevant text and numbers\ndetected. Except for this filtering step, no supervisory signal is used. The\nexperimental evidence shows that the scene text recognition method achieves an\nF1-score of 74. Combining the two methods, that is - using samples collected by\ntext spotter to train the re-identification model yields a higher F1-score of\n85.8. Re-training the person re-identification model with identified inliers\nyields a slight improvement in performance(F1 score of 87.8).\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 11:51:59 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Napolean", "Yeshwanth", ""], ["Wibowo", "Priadi Teguh", ""], ["van Gemert", "Jan", ""]]}, {"id": "1909.02856", "submitter": "Jue Wang", "authors": "Jue Wang and Anoop Cherian", "title": "Discriminative Video Representation Learning Using Support Vector\n  Classifiers", "comments": "arXiv admin note: substantial text overlap with arXiv:1803.10628", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most popular deep models for action recognition in videos generate\nindependent predictions for short clips, which are then pooled heuristically to\nassign an action label to the full video segment. As not all frames may\ncharacterize the underlying action---many are common across multiple\nactions---pooling schemes that impose equal importance on all frames might be\nunfavorable. In an attempt to tackle this problem, we propose discriminative\npooling, based on the notion that among the deep features generated on all\nshort clips, there is at least one that characterizes the action. To identify\nthese useful features, we resort to a negative bag consisting of features that\nare known to be irrelevant, for example, they are sampled either from datasets\nthat are unrelated to our actions of interest or are CNN features produced via\nrandom noise as input. With the features from the video as a positive bag and\nthe irrelevant features as the negative bag, we cast an objective to learn a\n(nonlinear) hyperplane that separates the unknown useful features from the rest\nin a multiple instance learning formulation within a support vector machine\nsetup. We use the parameters of this separating hyperplane as a descriptor for\nthe full video segment. Since these parameters are directly related to the\nsupport vectors in a max-margin framework, they can be treated as a weighted\naverage pooling of the features from the bags, with zero weights given to\nnon-support vectors. Our pooling scheme is end-to-end trainable within a deep\nlearning framework. We report results from experiments on eight computer vision\nbenchmark datasets spanning a variety of video-related tasks and demonstrate\nstate-of-the-art performance across these tasks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 16:12:27 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Wang", "Jue", ""], ["Cherian", "Anoop", ""]]}, {"id": "1909.02860", "submitter": "Xuejing Liu", "authors": "Xuejing Liu, Liang Li, Shuhui Wang, Zheng-Jun Zha, Li Su, Qingming\n  Huang", "title": "Knowledge-guided Pairwise Reconstruction Network for Weakly Supervised\n  Referring Expression Grounding", "comments": "Accepted by ACMMM 2019. arXiv admin note: text overlap with\n  arXiv:1908.10568", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised referring expression grounding (REG) aims at localizing the\nreferential entity in an image according to linguistic query, where the mapping\nbetween the image region (proposal) and the query is unknown in the training\nstage. In referring expressions, people usually describe a target entity in\nterms of its relationship with other contextual entities as well as visual\nattributes. However, previous weakly supervised REG methods rarely pay\nattention to the relationship between the entities. In this paper, we propose a\nknowledge-guided pairwise reconstruction network (KPRN), which models the\nrelationship between the target entity (subject) and contextual entity (object)\nas well as grounds these two entities. Specifically, we first design a\nknowledge extraction module to guide the proposal selection of subject and\nobject. The prior knowledge is obtained in a specific form of semantic\nsimilarities between each proposal and the subject/object. Second, guided by\nsuch knowledge, we design the subject and object attention module to construct\nthe subject-object proposal pairs. The subject attention excludes the unrelated\nproposals from the candidate proposals. The object attention selects the most\nsuitable proposal as the contextual proposal. Third, we introduce a pairwise\nattention and an adaptive weighting scheme to learn the correspondence between\nthese proposal pairs and the query. Finally, a pairwise reconstruction module\nis used to measure the grounding for weakly supervised learning. Extensive\nexperiments on four large-scale datasets show our method outperforms existing\nstate-of-the-art methods by a large margin.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 13:22:08 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Liu", "Xuejing", ""], ["Li", "Liang", ""], ["Wang", "Shuhui", ""], ["Zha", "Zheng-Jun", ""], ["Su", "Li", ""], ["Huang", "Qingming", ""]]}, {"id": "1909.02875", "submitter": "Nima Ziaei", "authors": "Nima Ziaei", "title": "Geolocation of an aircraft using image registration coupling modes for\n  autonomous navigation", "comments": "14 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to study an alternative technology to the GPS system on\nfixed wing aircraft using the aerial shots of landscapes from a ventral\nmonocular camera integrated into the aircraft and based on the technology of\nimage registration for aircraft geolocation purpose. Different types of use of\nthe image registration technology exist: the relative registration and the\nabsolute registration. The relative one is able to readjust position of the\naircraft from two successive aerial shots by knowing the aircraft s position of\nimage 1 and the overlap between the two images. The absolute registration\ncompare a real time aerial shot with pre-referenced images stored in a database\nand permit the geolocation of the aircraft in comparing aerial shot with images\nof the database. Each kind of image registration technology has its own flaw\npreventing it to be used alone for aircraft geolocation. This study proposes to\nevaluate, according to different physical parameters ( aircraft speed, flight\naltitude, density of image points of interest), the coupling of these different\ntypes of image registration. Finally, this study also aims to quantify some\nimage registration performances, particularly its execution time or its drift.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 12:53:00 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Ziaei", "Nima", ""]]}, {"id": "1909.02902", "submitter": "Lingbo Liu", "authors": "Lingbo Liu, Jiajie Zhen, Guanbin Li, Geng Zhan, Zhaocheng He, Bowen\n  Du, Liang Lin", "title": "Dynamic Spatial-Temporal Representation Learning for Traffic Flow\n  Prediction", "comments": "Accepted by IEEE Transactions on Intelligent Transportation Systems.\n  arXiv admin note: text overlap with arXiv:1809.00101", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a crucial component in intelligent transportation systems, traffic flow\nprediction has recently attracted widespread research interest in the field of\nartificial intelligence (AI) with the increasing availability of massive\ntraffic mobility data. Its key challenge lies in how to integrate diverse\nfactors (such as temporal rules and spatial dependencies) to infer the\nevolution trend of traffic flow. To address this problem, we propose a unified\nneural network called Attentive Traffic Flow Machine (ATFM), which can\neffectively learn the spatial-temporal feature representations of traffic flow\nwith an attention mechanism. In particular, our ATFM is composed of two\nprogressive Convolutional Long Short-Term Memory (ConvLSTM\n\\cite{xingjian2015convolutional}) units connected with a convolutional layer.\nSpecifically, the first ConvLSTM unit takes normal traffic flow features as\ninput and generates a hidden state at each time-step, which is further fed into\nthe connected convolutional layer for spatial attention map inference. The\nsecond ConvLSTM unit aims at learning the dynamic spatial-temporal\nrepresentations from the attentionally weighted traffic flow features. Further,\nwe develop two deep learning frameworks based on ATFM to predict citywide\nshort-term/long-term traffic flow by adaptively incorporating the sequential\nand periodic data as well as other external influences. Extensive experiments\non two standard benchmarks well demonstrate the superiority of the proposed\nmethod for traffic flow prediction. Moreover, to verify the generalization of\nour method, we also apply the customized framework to forecast the passenger\npickup/dropoff demands in traffic prediction and show its superior performance.\nOur code and data are available at\n{\\color{blue}\\url{https://github.com/liulingbo918/ATFM}}.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 01:41:38 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 17:10:48 GMT"}, {"version": "v3", "created": "Sun, 26 Apr 2020 13:08:40 GMT"}, {"version": "v4", "created": "Sat, 13 Jun 2020 01:21:01 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Liu", "Lingbo", ""], ["Zhen", "Jiajie", ""], ["Li", "Guanbin", ""], ["Zhan", "Geng", ""], ["He", "Zhaocheng", ""], ["Du", "Bowen", ""], ["Lin", "Liang", ""]]}, {"id": "1909.02918", "submitter": "Ilia Shumailov", "authors": "Yiren Zhao, Ilia Shumailov, Han Cui, Xitong Gao, Robert Mullins, Ross\n  Anderson", "title": "Blackbox Attacks on Reinforcement Learning Agents Using Approximated\n  Temporal Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recent research on reinforcement learning (RL) has suggested that trained\nagents are vulnerable to maliciously crafted adversarial samples. In this work,\nwe show how such samples can be generalised from White-box and Grey-box attacks\nto a strong Black-box case, where the attacker has no knowledge of the agents,\ntheir training parameters and their training methods. We use\nsequence-to-sequence models to predict a single action or a sequence of future\nactions that a trained agent will make. First, we show our approximation model,\nbased on time-series information from the agent, consistently predicts RL\nagents' future actions with high accuracy in a Black-box setup on a wide range\nof games and RL algorithms. Second, we find that although adversarial samples\nare transferable from the target model to our RL agents, they often outperform\nrandom Gaussian noise only marginally. This highlights a serious methodological\ndeficiency in previous work on such agents; random jamming should have been\ntaken as the baseline for evaluation. Third, we propose a novel use for\nadversarial samplesin Black-box attacks of RL agents: they can be used to\ntrigger a trained agent to misbehave after a specific time delay. This appears\nto be a genuinely new type of attack. It potentially enables an attacker to use\ndevices controlled by RL agents as time bombs.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 14:06:21 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 19:07:45 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Zhao", "Yiren", ""], ["Shumailov", "Ilia", ""], ["Cui", "Han", ""], ["Gao", "Xitong", ""], ["Mullins", "Robert", ""], ["Anderson", "Ross", ""]]}, {"id": "1909.02946", "submitter": "Martin Beroiz", "authors": "Martin Beroiz, Juan B. Cabral, Bruno Sanchez", "title": "Astroalign: A Python module for astronomical image registration", "comments": "4 pages, 2 figures, Python package", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm implemented in the astroalign Python module for image\nregistration in astronomy. Our module does not rely on WCS information and\ninstead matches 3-point asterisms (triangles) on the images to find the most\naccurate linear transformation between the two. It is especially useful in the\ncontext of aligning images prior to stacking or performing difference image\nanalysis. Astroalign can match images of different point-spread functions,\nseeing, and atmospheric conditions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 14:56:51 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 17:20:01 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Beroiz", "Martin", ""], ["Cabral", "Juan B.", ""], ["Sanchez", "Bruno", ""]]}, {"id": "1909.02950", "submitter": "Douwe Kiela", "authors": "Douwe Kiela, Suvrat Bhooshan, Hamed Firooz, Ethan Perez, Davide\n  Testuggine", "title": "Supervised Multimodal Bitransformers for Classifying Images and Text", "comments": "Rejected from EMNLP, twice", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised bidirectional transformer models such as BERT have led to\ndramatic improvements in a wide variety of textual classification tasks. The\nmodern digital world is increasingly multimodal, however, and textual\ninformation is often accompanied by other modalities such as images. We\nintroduce a supervised multimodal bitransformer model that fuses information\nfrom text and image encoders, and obtain state-of-the-art performance on\nvarious multimodal classification benchmark tasks, outperforming strong\nbaselines, including on hard test sets specifically designed to measure\nmultimodal performance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 14:59:18 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 03:08:28 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Kiela", "Douwe", ""], ["Bhooshan", "Suvrat", ""], ["Firooz", "Hamed", ""], ["Perez", "Ethan", ""], ["Testuggine", "Davide", ""]]}, {"id": "1909.02953", "submitter": "Jianan Chen", "authors": "Jianan Chen, Laurent Milot, Helen M. C. Cheung and Anne L. Martel", "title": "Unsupervised Clustering of Quantitative Imaging Phenotypes using\n  Autoencoder and Gaussian Mixture Model", "comments": "Accepted at MICCAI 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32251-9_63", "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative medical image computing (radiomics) has been widely applied to\nbuild prediction models from medical images. However, overfitting is a\nsignificant issue in conventional radiomics, where a large number of radiomic\nfeatures are directly used to train and test models that predict genotypes or\nclinical outcomes. In order to tackle this problem, we propose an unsupervised\nlearning pipeline composed of an autoencoder for representation learning of\nradiomic features and a Gaussian mixture model based on minimum message length\ncriterion for clustering. By incorporating probabilistic modeling, disease\nheterogeneity has been taken into account. The performance of the proposed\npipeline was evaluated on an institutional MRI cohort of 108 patients with\ncolorectal cancer liver metastases. Our approach is capable of automatically\nselecting the optimal number of clusters and assigns patients into clusters\n(imaging subtypes) with significantly different survival rates. Our method\noutperforms other unsupervised clustering methods that have been used for\nradiomics analysis and has comparable performance to a state-of-the-art imaging\nbiomarker.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 15:00:24 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Chen", "Jianan", ""], ["Milot", "Laurent", ""], ["Cheung", "Helen M. C.", ""], ["Martel", "Anne L.", ""]]}, {"id": "1909.02959", "submitter": "Jinghao Zhou", "authors": "Jinghao Zhou, Peng Wang, and Haoyang Sun", "title": "Discriminative and Robust Online Learning for Siamese Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of visual object tracking has traditionally been handled by\nvariant tracking paradigms, either learning a model of the object's appearance\nexclusively online or matching the object with the target in an offline-trained\nembedding space. Despite the recent success, each method agonizes over its\nintrinsic constraint. The online-only approaches suffer from a lack of\ngeneralization of the model they learn thus are inferior in target regression,\nwhile the offline-only approaches (e.g., convolutional siamese trackers) lack\nthe target-specific context information thus are not discriminative enough to\nhandle distractors, and robust enough to deformation. Therefore, we propose an\nonline module with an attention mechanism for offline siamese networks to\nextract target-specific features under L2 error. We further propose a filter\nupdate strategy adaptive to treacherous background noises for discriminative\nlearning, and a template update strategy to handle large target deformations\nfor robust learning. Effectiveness can be validated in the consistent\nimprovement over three siamese baselines: SiamFC, SiamRPN++, and SiamMask.\nBeyond that, our model based on SiamRPN++ obtains the best results over six\npopular tracking benchmarks and can operate beyond real-time.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 15:06:26 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 14:31:21 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Zhou", "Jinghao", ""], ["Wang", "Peng", ""], ["Sun", "Haoyang", ""]]}, {"id": "1909.02967", "submitter": "Zhiwen Shao", "authors": "Zhiwen Shao, Hengliang Zhu, Junshu Tang, Xuequan Lu, Lizhuang Ma", "title": "Explicit Facial Expression Transfer via Fine-Grained Representations", "comments": "This paper has been accepted by IEEE Transactions on Image Processing\n  (TIP)", "journal-ref": null, "doi": "10.1109/TIP.2021.3073857", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression transfer between two unpaired images is a challenging\nproblem, as fine-grained expression is typically tangled with other facial\nattributes. Most existing methods treat expression transfer as an application\nof expression manipulation, and use predicted global expression, landmarks or\naction units (AUs) as a guidance. However, the prediction may be inaccurate,\nwhich limits the performance of transferring fine-grained expression. Instead\nof using an intermediate estimated guidance, we propose to explicitly transfer\nfacial expression by directly mapping two unpaired input images to two\nsynthesized images with swapped expressions. Specifically, considering AUs\nsemantically describe fine-grained expression details, we propose a novel\nmulti-class adversarial training method to disentangle input images into two\ntypes of fine-grained representations: AU-related feature and AU-free feature.\nThen, we can synthesize new images with preserved identities and swapped\nexpressions by combining AU-free features with swapped AU-related features.\nMoreover, to obtain reliable expression transfer results of the unpaired input,\nwe introduce a swap consistency loss to make the synthesized images and\nself-reconstructed images indistinguishable. Extensive experiments show that\nour approach outperforms the state-of-the-art expression manipulation methods\nfor transferring fine-grained expressions while preserving other attributes\nincluding identity and pose.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 15:24:28 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 06:53:14 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Shao", "Zhiwen", ""], ["Zhu", "Hengliang", ""], ["Tang", "Junshu", ""], ["Lu", "Xuequan", ""], ["Ma", "Lizhuang", ""]]}, {"id": "1909.03012", "submitter": "Amit Dhurandhar", "authors": "Vijay Arya, Rachel K. E. Bellamy, Pin-Yu Chen, Amit Dhurandhar,\n  Michael Hind, Samuel C. Hoffman, Stephanie Houde, Q. Vera Liao, Ronny Luss,\n  Aleksandra Mojsilovi\\'c, Sami Mourad, Pablo Pedemonte, Ramya Raghavendra,\n  John Richards, Prasanna Sattigeri, Karthikeyan Shanmugam, Moninder Singh,\n  Kush R. Varshney, Dennis Wei and Yunfeng Zhang", "title": "One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI\n  Explainability Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As artificial intelligence and machine learning algorithms make further\ninroads into society, calls are increasing from multiple stakeholders for these\nalgorithms to explain their outputs. At the same time, these stakeholders,\nwhether they be affected citizens, government regulators, domain experts, or\nsystem developers, present different requirements for explanations. Toward\naddressing these needs, we introduce AI Explainability 360\n(http://aix360.mybluemix.net/), an open-source software toolkit featuring eight\ndiverse and state-of-the-art explainability methods and two evaluation metrics.\nEqually important, we provide a taxonomy to help entities requiring\nexplanations to navigate the space of explanation methods, not only those in\nthe toolkit but also in the broader literature on explainability. For data\nscientists and other users of the toolkit, we have implemented an extensible\nsoftware architecture that organizes methods according to their place in the AI\nmodeling pipeline. We also discuss enhancements to bring research innovations\ncloser to consumers of explanations, ranging from simplified, more accessible\nversions of algorithms, to tutorials and an interactive web demo to introduce\nAI explainability to different audiences and application domains. Together, our\ntoolkit and taxonomy can help identify gaps where more explainability methods\nare needed and provide a platform to incorporate them as they are developed.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 16:53:01 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 15:08:57 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Arya", "Vijay", ""], ["Bellamy", "Rachel K. E.", ""], ["Chen", "Pin-Yu", ""], ["Dhurandhar", "Amit", ""], ["Hind", "Michael", ""], ["Hoffman", "Samuel C.", ""], ["Houde", "Stephanie", ""], ["Liao", "Q. Vera", ""], ["Luss", "Ronny", ""], ["Mojsilovi\u0107", "Aleksandra", ""], ["Mourad", "Sami", ""], ["Pedemonte", "Pablo", ""], ["Raghavendra", "Ramya", ""], ["Richards", "John", ""], ["Sattigeri", "Prasanna", ""], ["Shanmugam", "Karthikeyan", ""], ["Singh", "Moninder", ""], ["Varshney", "Kush R.", ""], ["Wei", "Dennis", ""], ["Zhang", "Yunfeng", ""]]}, {"id": "1909.03051", "submitter": "Ziyuan Zhang", "authors": "Ziyuan Zhang, Luan Tran, Feng Liu, Xiaoming Liu", "title": "On Learning Disentangled Representations for Gait Recognition", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.04925", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait, the walking pattern of individuals, is one of the important biometrics\nmodalities. Most of the existing gait recognition methods take silhouettes or\narticulated body models as gait features. These methods suffer from degraded\nrecognition performance when handling confounding variables, such as clothing,\ncarrying and viewing angle. To remedy this issue, we propose a novel\nAutoEncoder framework, GaitNet, to explicitly disentangle appearance, canonical\nand pose features from RGB imagery. The LSTM integrates pose features over time\nas a dynamic gait feature while canonical features are averaged as a static\ngait feature. Both of them are utilized as classification features. In\naddition, we collect a Frontal-View Gait (FVG) dataset to focus on gait\nrecognition from frontal-view walking, which is a challenging problem since it\ncontains minimal gait cues compared to other views. FVG also includes other\nimportant variations, e.g., walking speed, carrying, and clothing. With\nextensive experiments on CASIA-B, USF, and FVG datasets, our method\ndemonstrates superior performance to the SOTA quantitatively, the ability of\nfeature disentanglement qualitatively, and promising computational efficiency.\nWe further compare our GaitNet with state-of-the-art face recognition to\ndemonstrate the advantages of gait biometrics identification under certain\nscenarios, e.g., long distance/lower resolutions, cross viewing angles.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 20:50:10 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Zhang", "Ziyuan", ""], ["Tran", "Luan", ""], ["Liu", "Feng", ""], ["Liu", "Xiaoming", ""]]}, {"id": "1909.03101", "submitter": "Xingtong Liu", "authors": "Xingtong Liu, Ayushi Sinha, Masaru Ishii, Gregory D. Hager, Russell H.\n  Taylor, Mathias Unberath", "title": "Self-supervised Dense 3D Reconstruction from Monocular Endoscopic Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a self-supervised learning-based pipeline for dense 3D\nreconstruction from full-length monocular endoscopic videos without a priori\nmodeling of anatomy or shading. Our method only relies on unlabeled monocular\nendoscopic videos and conventional multi-view stereo algorithms, and requires\nneither manual interaction nor patient CT in both training and application\nphases. In a cross-patient study using CT scans as groundtruth, we show that\nour method is able to produce photo-realistic dense 3D reconstructions with\nsubmillimeter mean residual errors from endoscopic videos from unseen patients\nand scopes.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 19:41:05 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "Xingtong", ""], ["Sinha", "Ayushi", ""], ["Ishii", "Masaru", ""], ["Hager", "Gregory D.", ""], ["Taylor", "Russell H.", ""], ["Unberath", "Mathias", ""]]}, {"id": "1909.03108", "submitter": "Le Hou", "authors": "Le Hou, Youlong Cheng, Noam Shazeer, Niki Parmar, Yeqing Li,\n  Panagiotis Korfiatis, Travis M. Drucker, Daniel J. Blezek, Xiaodan Song", "title": "High Resolution Medical Image Analysis with Spatial Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical images such as 3D computerized tomography (CT) scans and pathology\nimages, have hundreds of millions or billions of voxels/pixels. It is\ninfeasible to train CNN models directly on such high resolution images, because\nneural activations of a single image do not fit in the memory of a single\nGPU/TPU, and naive data and model parallelism approaches do not work. Existing\nimage analysis approaches alleviate this problem by cropping or down-sampling\ninput images, which leads to complicated implementation and sub-optimal\nperformance due to information loss. In this paper, we implement spatial\npartitioning, which internally distributes the input and output of\nconvolutional layers across GPUs/TPUs. Our implementation is based on the\nMesh-TensorFlow framework and the computation distribution is transparent to\nend users. With this technique, we train a 3D Unet on up to 512 by 512 by 512\nresolution data. To the best of our knowledge, this is the first work for\nhandling such high resolution images end-to-end.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 19:58:11 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 01:05:10 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 18:53:36 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Hou", "Le", ""], ["Cheng", "Youlong", ""], ["Shazeer", "Noam", ""], ["Parmar", "Niki", ""], ["Li", "Yeqing", ""], ["Korfiatis", "Panagiotis", ""], ["Drucker", "Travis M.", ""], ["Blezek", "Daniel J.", ""], ["Song", "Xiaodan", ""]]}, {"id": "1909.03120", "submitter": "Xinyao Sun", "authors": "Xinyao Sun, Aaron Zimmer, Subhayan Mukherjee, Navaneeth Kamballur\n  Kottayil, Parwant Ghuman, Irene Cheng", "title": "DeepInSAR: A Deep Learning Framework for SAR Interferometric Phase\n  Restoration and Coherence Estimation", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, Interferometric Synthetic Aperture Radar (InSAR) has\nbecome a successful remote sensing technique. However, during the acquisition\nstep, microwave reflections received at satellite are usually disturbed by\nstrong noise, leading to a noisy single-look complex (SLC) SAR image. The\nquality of their interferometric phase is even worse. InSAR phase filtering is\nan ill-posed problem and plays a key role in subsequent processing. However,\nmost of existing methods usually require expert supervision or heavy runtime,\nwhich limits the usability and scalability for practical usages such as\nwide-area monitoring and forecasting. In this work, we propose a deep\nconvolutional neural network (CNN) based model DeepInSAR to intelligently solve\nboth the phase filtering and coherence estimation problems. We demonstrate our\nDeepInSAR using both simulated and real data. A teacher-student framework is\nproposed to deal with the issue that there is no ground truth sample for\nreal-world InSAR data. Quantitative and qualitative comparisons show that\nDeepInSAR achieves comparable or even better results than its stacked-based\nteacher method on new test datasets but requiring fewer pairs of SLCs as well\nas outperforms three other established non-stack based methods with less\nrunning time and no human supervision.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 20:38:04 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 05:18:43 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Sun", "Xinyao", ""], ["Zimmer", "Aaron", ""], ["Mukherjee", "Subhayan", ""], ["Kottayil", "Navaneeth Kamballur", ""], ["Ghuman", "Parwant", ""], ["Cheng", "Irene", ""]]}, {"id": "1909.03140", "submitter": "Dan Xu", "authors": "Dan Xu, Weidi Xie, Andrew Zisserman", "title": "Geometry-Aware Video Object Detection for Static Cameras", "comments": "Accepted at BMVC 2019 as ORAL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a geometry-aware model for video object detection.\nSpecifically, we consider the setting that cameras can be well approximated as\nstatic, e.g. in video surveillance scenarios, and scene pseudo depth maps can\ntherefore be inferred easily from the object scale on the image plane. We make\nthe following contributions: First, we extend the recent anchor-free detector\n(CornerNet [17]) to video object detections. In order to exploit the\nspatial-temporal information while maintaining high efficiency, the proposed\nmodel accepts video clips as input, and only makes predictions for the starting\nand the ending frames, i.e. heatmaps of object bounding box corners and the\ncorresponding embeddings for grouping. Second, to tackle the challenge from\nscale variations in object detection, scene geometry information, e.g. derived\ndepth maps, is explicitly incorporated into deep networks for multi-scale\nfeature selection and for the network prediction. Third, we validate the\nproposed architectures on an autonomous driving dataset generated from the\nCarla simulator [5], and on a real dataset for human detection (DukeMTMC\ndataset [28]). When comparing with the existing competitive single-stage or\ntwo-stage detectors, the proposed geometry-aware spatio-temporal network\nachieves significantly better results.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 22:34:45 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Xu", "Dan", ""], ["Xie", "Weidi", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1909.03169", "submitter": "Fawaz Sammani", "authors": "Fawaz Sammani, Mahmoud Elsayed", "title": "Look and Modify: Modification Networks for Image Captioning", "comments": "Published in BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based neural encoder-decoder frameworks have been widely used for\nimage captioning. Many of these frameworks deploy their full focus on\ngenerating the caption from scratch by relying solely on the image features or\nthe object detection regional features. In this paper, we introduce a novel\nframework that learns to modify existing captions from a given framework by\nmodeling the residual information, where at each timestep the model learns what\nto keep, remove or add to the existing caption allowing the model to fully\nfocus on \"what to modify\" rather than on \"what to predict\". We evaluate our\nmethod on the COCO dataset, trained on top of several image captioning\nframeworks and show that our model successfully modifies captions yielding\nbetter ones with better evaluation scores.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 02:15:24 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 06:20:49 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Sammani", "Fawaz", ""], ["Elsayed", "Mahmoud", ""]]}, {"id": "1909.03205", "submitter": "Mark Sandler", "authors": "Mark Sandler and Jonathan Baccash and Andrey Zhmoginov and Andrew\n  Howard", "title": "Non-discriminative data or weak model? On the relative importance of\n  data and model resolution", "comments": "ICCV 2019 Workshop on Real-World Recognition from Low-Quality Images\n  and Videos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the question of how the resolution of the input image (\"input\nresolution\") affects the performance of a neural network when compared to the\nresolution of the hidden layers (\"internal resolution\"). Adjusting these\ncharacteristics is frequently used as a hyperparameter providing a trade-off\nbetween model performance and accuracy. An intuitive interpretation is that the\nreduced information content in the low-resolution input causes decay in the\naccuracy. In this paper, we show that up to a point, the input resolution alone\nplays little role in the network performance, and it is the internal resolution\nthat is the critical driver of model quality. We then build on these insights\nto develop novel neural network architectures that we call \\emph{Isometric\nNeural Networks}. These models maintain a fixed internal resolution throughout\ntheir entire depth. We demonstrate that they lead to high accuracy models with\nlow activation footprint and parameter count.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 07:19:29 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 23:13:32 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Sandler", "Mark", ""], ["Baccash", "Jonathan", ""], ["Zhmoginov", "Andrey", ""], ["Howard", "Andrew", ""]]}, {"id": "1909.03234", "submitter": "Chen Gongwei", "authors": "Gongwei Chen, Xinhang Song, Haitao Zeng, Shuqiang Jiang", "title": "Scene Recognition with Prototype-agnostic Scene Layout", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.2986599", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract--- Exploiting the spatial structure in scene images is a key\nresearch direction for scene recognition. Due to the large intra-class\nstructural diversity, building and modeling flexible structural layout to adapt\nvarious image characteristics is a challenge. Existing structural modeling\nmethods in scene recognition either focus on predefined grids or rely on\nlearned prototypes, which all have limited representative ability. In this\npaper, we propose Prototype-agnostic Scene Layout (PaSL) construction method to\nbuild the spatial structure for each image without conforming to any prototype.\nOur PaSL can flexibly capture the diverse spatial characteristic of scene\nimages and have considerable generalization capability. Given a PaSL, we build\nLayout Graph Network (LGN) where regions in PaSL are defined as nodes and two\nkinds of independent relations between regions are encoded as edges. The LGN\naims to incorporate two topological structures (formed in spatial and semantic\nsimilarity dimensions) into image representations through graph convolution.\nExtensive experiments show that our approach achieves state-of-the-art results\non widely recognized MIT67 and SUN397 datasets without multi-model or\nmulti-scale fusion. Moreover, we also conduct the experiments on one of the\nlargest scale datasets, Places365. The results demonstrate the proposed method\ncan be well generalized and obtains competitive performance.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 10:04:01 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Chen", "Gongwei", ""], ["Song", "Xinhang", ""], ["Zeng", "Haitao", ""], ["Jiang", "Shuqiang", ""]]}, {"id": "1909.03252", "submitter": "Mingkui Tan", "authors": "Runhao Zeng, Wenbing Huang, Mingkui Tan, Yu Rong, Peilin Zhao, Junzhou\n  Huang, Chuang Gan", "title": "Graph Convolutional Networks for Temporal Action Localization", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art action localization systems process each action\nproposal individually, without explicitly exploiting their relations during\nlearning. However, the relations between proposals actually play an important\nrole in action localization, since a meaningful action always consists of\nmultiple proposals in a video. In this paper, we propose to exploit the\nproposal-proposal relations using Graph Convolutional Networks (GCNs). First,\nwe construct an action proposal graph, where each proposal is represented as a\nnode and their relations between two proposals as an edge. Here, we use two\ntypes of relations, one for capturing the context information for each proposal\nand the other one for characterizing the correlations between distinct actions.\nThen we apply the GCNs over the graph to model the relations among different\nproposals and learn powerful representations for the action classification and\nlocalization. Experimental results show that our approach significantly\noutperforms the state-of-the-art on THUMOS14 (49.1% versus 42.8%). Moreover,\naugmentation experiments on ActivityNet also verify the efficacy of modeling\naction proposal relationships. Codes are available at\nhttps://github.com/Alvin-Zeng/PGCN.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 11:51:43 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Zeng", "Runhao", ""], ["Huang", "Wenbing", ""], ["Tan", "Mingkui", ""], ["Rong", "Yu", ""], ["Zhao", "Peilin", ""], ["Huang", "Junzhou", ""], ["Gan", "Chuang", ""]]}, {"id": "1909.03258", "submitter": "Jingwen Fu", "authors": "Jingwen Fu, Xiaoyan Zhu, and Yingbin Li", "title": "Recognition Of Surface Defects On Steel Sheet Using Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic defect recognition is one of the research hotspots in steel\nproduction, but most of the current methods mainly extract features manually\nand use machine learning classifiers to recognize defects, which cannot tackle\nthe situation, where there are few data available to train and confine to a\ncertain scene. Therefore, in this paper, a new approach is proposed which\nconsists of part of pretrained VGG16 as a feature extractor and a new CNN\nneural network as a classifier to recognize the defect of steel strip surface\nbased on the feature maps created by the feature extractor. Our method achieves\nan accuracy of 99.1% and 96.0% while the dataset contains 150 images each class\nand 10 images each class respectively, which is much better than previous\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 12:19:10 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 13:17:24 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Fu", "Jingwen", ""], ["Zhu", "Xiaoyan", ""], ["Li", "Yingbin", ""]]}, {"id": "1909.03309", "submitter": "Sudhakar Kumawat", "authors": "Gagan Kanojia, Sudhakar Kumawat and Shanmuganathan Raman", "title": "Exploring Temporal Differences in 3D Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional 3D convolutions are computationally expensive, memory intensive,\nand due to large number of parameters, they often tend to overfit. On the other\nhand, 2D CNNs are less computationally expensive and less memory intensive than\n3D CNNs and have shown remarkable results in applications like image\nclassification and object recognition. However, in previous works, it has been\nobserved that they are inferior to 3D CNNs when applied on a spatio-temporal\ninput. In this work, we propose a convolutional block which extracts the\nspatial information by performing a 2D convolution and extracts the temporal\ninformation by exploiting temporal differences, i.e., the change in the spatial\ninformation at different time instances, using simple operations of shift,\nsubtract and add without utilizing any trainable parameters. The proposed\nconvolutional block has same number of parameters as of a 2D convolution kernel\nof size nxn, i.e. n^2, and has n times lesser parameters than an nxnxn 3D\nconvolution kernel. We show that the 3D CNNs perform better when the 3D\nconvolution kernels are replaced by the proposed convolutional blocks. We\nevaluate the proposed convolutional block on UCF101 and ModelNet datasets.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 17:45:27 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Kanojia", "Gagan", ""], ["Kumawat", "Sudhakar", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1909.03315", "submitter": "Martin Andrews", "authors": "Martin Andrews, Sam Witteveen", "title": "Relationships from Entity Stream", "comments": "Accepted paper for the ViGIL workshop at NIPS 2017. (4 pages +\n  references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational reasoning is a central component of intelligent behavior, but has\nproven difficult for neural networks to learn. The Relation Network (RN) module\nwas recently proposed by DeepMind to solve such problems, and demonstrated\nstate-of-the-art results on a number of datasets. However, the RN module scales\nquadratically in the size of the input, since it calculates relationship\nfactors between every patch in the visual field, including those that do not\ncorrespond to entities. In this paper, we describe an architecture that enables\nrelationships to be determined from a stream of entities obtained by an\nattention mechanism over the input field. The model is trained end-to-end, and\ndemonstrates equivalent performance with greater interpretability while\nrequiring only a fraction of the model parameters of the original RN module.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 18:24:57 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Andrews", "Martin", ""], ["Witteveen", "Sam", ""]]}, {"id": "1909.03354", "submitter": "Soufiane Belharbi", "authors": "J\\'er\\^ome Rony, Soufiane Belharbi, Jose Dolz, Ismail Ben Ayed, Luke\n  McCaffrey, Eric Granger", "title": "Deep weakly-supervised learning methods for classification and\n  localization in histology images: a survey", "comments": "35 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using state-of-the-art deep learning models for cancer diagnosis presents\nseveral challenges related to the nature and availability of labeled histology\nimages. In particular, cancer grading and localization in these images normally\nrelies on both image- and pixel-level labels, the latter requiring a costly\nannotation process. In this survey, deep weakly-supervised learning (WSL)\nmodels are investigated to identify and locate diseases in histology images,\nwithout the need for pixel-level annotations. Given training data with global\nimage-level labels, these models allow to simultaneously classify histology\nimages and yield pixel-wise localization scores, thereby identifying the\ncorresponding regions of interest (ROI). Since relevant WSL models have mainly\nbeen investigated within the computer vision community, and validated on\nnatural scene images, we assess the extent to which they apply to histology\nimages which have challenging properties, e.g. very large size, similarity\nbetween foreground/background, highly unstructured regions, stain\nheterogeneity, and noisy/ambiguous labels. The most relevant models for deep\nWSL are compared experimentally in terms of accuracy (classification and\npixel-wise localization) on several public benchmark histology datasets for\nbreast and colon cancer -- BACH ICIAR 2018, BreaKHis, CAMELYON16, and GlaS.\nFurthermore, for large-scale evaluation of WSL models on histology images, we\npropose a protocol to construct WSL datasets from Whole Slide Imaging. Results\nindicate that several deep learning models can provide a high level of\nclassification accuracy, although accurate pixel-wise localization of cancer\nregions remains an issue for such images. Code is publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 00:01:37 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 14:12:21 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 04:16:09 GMT"}, {"version": "v4", "created": "Thu, 31 Dec 2020 05:41:51 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Rony", "J\u00e9r\u00f4me", ""], ["Belharbi", "Soufiane", ""], ["Dolz", "Jose", ""], ["Ayed", "Ismail Ben", ""], ["McCaffrey", "Luke", ""], ["Granger", "Eric", ""]]}, {"id": "1909.03360", "submitter": "Yunlong Yu", "authors": "Yunlong Yu, Zhong Ji, Zhongfei Zhang, Jungong Han", "title": "Episode-based Prototype Generating Network for Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple yet effective episode-based training framework for\nzero-shot learning (ZSL), where the learning system requires to recognize\nunseen classes given only the corresponding class semantics. During training,\nthe model is trained within a collection of episodes, each of which is designed\nto simulate a zero-shot classification task. Through training multiple\nepisodes, the model progressively accumulates ensemble experiences on\npredicting the mimetic unseen classes, which will generalize well on the real\nunseen classes. Based on this training framework, we propose a novel generative\nmodel that synthesizes visual prototypes conditioned on the class semantic\nprototypes. The proposed model aligns the visual-semantic interactions by\nformulating both the visual prototype generation and the class semantic\ninference into an adversarial framework paired with a parameter-economic\nMulti-modal Cross-Entropy Loss to capture the discriminative information.\nExtensive experiments on four datasets under both traditional ZSL and\ngeneralized ZSL tasks show that our model outperforms the state-of-the-art\napproaches by large margins.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 01:06:15 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 03:21:04 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Yu", "Yunlong", ""], ["Ji", "Zhong", ""], ["Zhang", "Zhongfei", ""], ["Han", "Jungong", ""]]}, {"id": "1909.03375", "submitter": "Kai Guo", "authors": "Kai Guo, Seongwook Song, Soonkeun Chang, Tae-ui Kim, Seungmin Han,\n  Irina Kim", "title": "Robust Full-FoV Depth Estimation in Tele-wide Camera System", "comments": "5 pages, 7 figures, 2 tables", "journal-ref": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP)", "doi": "10.1109/ICASSP40776.2020.9053724", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tele-wide camera system with different Field of View (FoV) lenses becomes\nvery popular in recent mobile devices. Usually it is difficult to obtain\nfull-FoV depth based on traditional stereo-matching methods. Pure Deep Neural\nNetwork (DNN) based depth estimation methods can obtain full-FoV depth, but\nhave low robustness for scenarios which are not covered by training dataset. In\nthis paper, to address the above problems we propose a hierarchical hourglass\nnetwork for robust full-FoV depth estimation in tele-wide camera system, which\ncombines the robustness of traditional stereo-matching methods with the\naccuracy of DNN. More specifically, the proposed network comprises three major\nmodules: single image depth prediction module infers initial depth from input\ncolor image, depth propagation module propagates traditional stereo-matching\ntele-FoV depth to surrounding regions, and depth combination module fuses the\ninitial depth with the propagated depth to generate final output. Each of these\nmodules employs an hourglass model, which is a kind of encoder-decoder\nstructure with skip connections. Experimental results compared with\nstate-of-the-art depth estimation methods demonstrate that our method not only\nproduces robust and better subjective depth quality on wild test images, but\nalso obtains better quantitative results on standard datasets.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 02:50:17 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 03:32:22 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Guo", "Kai", ""], ["Song", "Seongwook", ""], ["Chang", "Soonkeun", ""], ["Kim", "Tae-ui", ""], ["Han", "Seungmin", ""], ["Kim", "Irina", ""]]}, {"id": "1909.03380", "submitter": "Xin Zhong", "authors": "Xin Zhong, Frank Y. Shih, Xiwang Guo", "title": "Automatic Image Pixel Clustering based on Mussels Wandering Optimiz", "comments": null, "journal-ref": null, "doi": "10.1142/S0218001421540057", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation as a clustering problem is to identify pixel groups on an\nimage without any preliminary labels available. It remains a challenge in\nmachine vision because of the variations in size and shape of image segments.\nFurthermore, determining the segment number in an image is NP-hard without\nprior knowledge of the image content. This paper presents an automatic color\nimage pixel clustering scheme based on mussels wandering optimization. By\napplying an activation variable to determine the number of clusters along with\nthe cluster centers optimization, an image is segmented with minimal prior\nknowledge and human intervention. By revising the within- and between-class sum\nof squares ratio for random natural image contents, we provide a novel fitness\nfunction for image pixel clustering tasks. Comprehensive empirical studies of\nthe proposed scheme against other state-of-the-art competitors on synthetic\ndata and the ASD dataset have demonstrated the promising performance of the\nproposed scheme.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 03:18:11 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Zhong", "Xin", ""], ["Shih", "Frank Y.", ""], ["Guo", "Xiwang", ""]]}, {"id": "1909.03385", "submitter": "Hokchhay Tann", "authors": "Hokchhay Tann, Heng Zhao, Sherief Reda", "title": "A Resource-Efficient Embedded Iris Recognition System Using Fully\n  Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of Fully Convolutional Networks (FCN) in iris segmentation have\nshown promising advances. For mobile and embedded systems, a significant\nchallenge is that the proposed FCN architectures are extremely computationally\ndemanding. In this article, we propose a resource-efficient, end-to-end iris\nrecognition flow, which consists of FCN-based segmentation, contour fitting,\nfollowed by Daugman normalization and encoding. To attain accurate and\nefficient FCN models, we propose a three-step SW/HW co-design methodology\nconsisting of FCN architectural exploration, precision quantization, and\nhardware acceleration. In our exploration, we propose multiple FCN models, and\nin comparison to previous works, our best-performing model requires 50X less\nFLOPs per inference while achieving a new state-of-the-art segmentation\naccuracy. Next, we select the most efficient set of models and further reduce\ntheir computational complexity through weights and activations quantization\nusing 8-bit dynamic fixed-point (DFP) format. Each model is then incorporated\ninto an end-to-end flow for true recognition performance evaluation. A few of\nour end-to-end pipelines outperform the previous state-of-the-art on two\ndatasets evaluated. Finally, we propose a novel DFP accelerator and fully\ndemonstrate the SW/HW co-design realization of our flow on an embedded FPGA\nplatform. In comparison with the embedded CPU, our hardware acceleration\nachieves up to 8.3X speedup for the overall pipeline while using less than 15%\nof the available FPGA resources. We also provide comparisons between the FPGA\nsystem and an embedded GPU showing different benefits and drawbacks for the two\nplatforms.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 04:21:10 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Tann", "Hokchhay", ""], ["Zhao", "Heng", ""], ["Reda", "Sherief", ""]]}, {"id": "1909.03388", "submitter": "Yilun Xu", "authors": "Yilun Xu, Peng Cao, Yuqing Kong, Yizhou Wang", "title": "L_DMI: An Information-theoretic Noise-robust Loss Function", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately annotating large scale dataset is notoriously expensive both in\ntime and in money. Although acquiring low-quality-annotated dataset can be much\ncheaper, it often badly damages the performance of trained models when using\nsuch dataset without particular treatment. Various methods have been proposed\nfor learning with noisy labels. However, most methods only handle limited kinds\nof noise patterns, require auxiliary information or steps (e.g. , knowing or\nestimating the noise transition matrix), or lack theoretical justification. In\nthis paper, we propose a novel information-theoretic loss function,\n$\\mathcal{L}_{DMI}$, for training deep neural networks robust to label noise.\nThe core of $\\mathcal{L}_{DMI}$ is a generalized version of mutual information,\ntermed Determinant based Mutual Information (DMI), which is not only\ninformation-monotone but also relatively invariant. \\emph{To the best of our\nknowledge, $\\mathcal{L}_{DMI}$ is the first loss function that is provably\nrobust to instance-independent label noise, regardless of noise pattern, and it\ncan be applied to any existing classification neural networks straightforwardly\nwithout any auxiliary information}. In addition to theoretical justification,\nwe also empirically show that using $\\mathcal{L}_{DMI}$ outperforms all other\ncounterparts in the classification task on both image dataset and natural\nlanguage dataset include Fashion-MNIST, CIFAR-10, Dogs vs. Cats, MR with a\nvariety of synthesized noise patterns and noise amounts, as well as a\nreal-world dataset Clothing1M. Codes are available at\nhttps://github.com/Newbeeer/L_DMI .\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 05:09:45 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 18:16:37 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Xu", "Yilun", ""], ["Cao", "Peng", ""], ["Kong", "Yuqing", ""], ["Wang", "Yizhou", ""]]}, {"id": "1909.03396", "submitter": "Tomer Levinboim", "authors": "Tomer Levinboim, Ashish V. Thapliyal, Piyush Sharma, Radu Soricut", "title": "Quality Estimation for Image Captions Based on Large-scale Human\n  Evaluations", "comments": "10 pages, 6 figures, 3 tables. Accepted to NAACL2021.\n  https://www.aclweb.org/anthology/2021.naacl-main.253/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic image captioning has improved significantly over the last few\nyears, but the problem is far from being solved, with state of the art models\nstill often producing low quality captions when used in the wild. In this\npaper, we focus on the task of Quality Estimation (QE) for image captions,\nwhich attempts to model the caption quality from a human perspective and\nwithout access to ground-truth references, so that it can be applied at\nprediction time to detect low-quality captions produced on previously unseen\nimages. For this task, we develop a human evaluation process that collects\ncoarse-grained caption annotations from crowdsourced users, which is then used\nto collect a large scale dataset spanning more than 600k caption quality\nratings. We then carefully validate the quality of the collected ratings and\nestablish baseline models for this new QE task. Finally, we further collect\nfine-grained caption quality annotations from trained raters, and use them to\ndemonstrate that QE models trained over the coarse ratings can effectively\ndetect and filter out low-quality image captions, thereby improving the user\nexperience from captioning systems.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 06:55:53 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 19:03:27 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Levinboim", "Tomer", ""], ["Thapliyal", "Ashish V.", ""], ["Sharma", "Piyush", ""], ["Soricut", "Radu", ""]]}, {"id": "1909.03402", "submitter": "Zilong Zhong", "authors": "Zilong Zhong, Zhong Qiu Lin, Rene Bidart, Xiaodan Hu, Ibrahim Ben\n  Daya, Zhifeng Li, Wei-Shi Zheng, Jonathan Li, Alexander Wong", "title": "Squeeze-and-Attention Networks for Semantic Segmentation", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The recent integration of attention mechanisms into segmentation networks\nimproves their representational capabilities through a great emphasis on more\ninformative features. However, these attention mechanisms ignore an implicit\nsub-task of semantic segmentation and are constrained by the grid structure of\nconvolution kernels. In this paper, we propose a novel squeeze-and-attention\nnetwork (SANet) architecture that leverages an effective squeeze-and-attention\n(SA) module to account for two distinctive characteristics of segmentation: i)\npixel-group attention, and ii) pixel-wise prediction. Specifically, the\nproposed SA modules impose pixel-group attention on conventional convolution by\nintroducing an 'attention' convolutional channel, thus taking into account\nspatial-channel inter-dependencies in an efficient manner. The final\nsegmentation results are produced by merging outputs from four hierarchical\nstages of a SANet to integrate multi-scale contexts for obtaining an enhanced\npixel-wise prediction. Empirical experiments on two challenging public datasets\nvalidate the effectiveness of the proposed SANets, which achieves 83.2% mIoU\n(without COCO pre-training) on PASCAL VOC and a state-of-the-art mIoU of 54.4%\non PASCAL Context.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 08:21:57 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 03:38:20 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 07:14:17 GMT"}, {"version": "v4", "created": "Wed, 1 Apr 2020 05:50:33 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Zhong", "Zilong", ""], ["Lin", "Zhong Qiu", ""], ["Bidart", "Rene", ""], ["Hu", "Xiaodan", ""], ["Daya", "Ibrahim Ben", ""], ["Li", "Zhifeng", ""], ["Zheng", "Wei-Shi", ""], ["Li", "Jonathan", ""], ["Wong", "Alexander", ""]]}, {"id": "1909.03403", "submitter": "Ziwei Liu", "authors": "Ziwei Liu, Zhongqi Miao, Xingang Pan, Xiaohang Zhan, Dahua Lin, Stella\n  X. Yu, Boqing Gong", "title": "Open Compound Domain Adaptation", "comments": "To appear in CVPR 2020 as an oral presentation. Code, datasets and\n  models are available at:\n  https://liuziwei7.github.io/projects/CompoundDomain.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical domain adaptation approach is to adapt models trained on the\nannotated data in a source domain (e.g., sunny weather) for achieving high\nperformance on the test data in a target domain (e.g., rainy weather). Whether\nthe target contains a single homogeneous domain or multiple heterogeneous\ndomains, existing works always assume that there exist clear distinctions\nbetween the domains, which is often not true in practice (e.g., changes in\nweather). We study an open compound domain adaptation (OCDA) problem, in which\nthe target is a compound of multiple homogeneous domains without domain labels,\nreflecting realistic data collection from mixed and novel situations. We\npropose a new approach based on two technical insights into OCDA: 1) a\ncurriculum domain adaptation strategy to bootstrap generalization across\ndomains in a data-driven self-organizing fashion and 2) a memory module to\nincrease the model's agility towards novel domains. Our experiments on digit\nclassification, facial expression recognition, semantic segmentation, and\nreinforcement learning demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 08:41:05 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 06:00:41 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Liu", "Ziwei", ""], ["Miao", "Zhongqi", ""], ["Pan", "Xingang", ""], ["Zhan", "Xiaohang", ""], ["Lin", "Dahua", ""], ["Yu", "Stella X.", ""], ["Gong", "Boqing", ""]]}, {"id": "1909.03410", "submitter": "Avik Pal", "authors": "Avik Pal and Aniket Das", "title": "TorchGAN: A Flexible Framework for GAN Training and Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TorchGAN is a PyTorch based framework for writing succinct and comprehensible\ncode for training and evaluation of Generative Adversarial Networks. The\nframework's modular design allows effortless customization of the model\narchitecture, loss functions, training paradigms, and evaluation metrics. The\nkey features of TorchGAN are its extensibility, built-in support for a large\nnumber of popular models, losses and evaluation metrics, and zero overhead\ncompared to vanilla PyTorch. By using the framework to implement several\npopular GAN models, we demonstrate its extensibility and ease of use. We also\nbenchmark the training time of our framework for said models against the\ncorresponding baseline PyTorch implementations and observe that TorchGAN's\nfeatures bear almost zero overhead.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 09:32:01 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Pal", "Avik", ""], ["Das", "Aniket", ""]]}, {"id": "1909.03413", "submitter": "Xugang Wu", "authors": "Xugang Wu, Xiaoping Wang, Xu Zhou, Songlei Jian", "title": "STA: Adversarial Attacks on Siamese Trackers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the majority of visual trackers adopt Convolutional Neural Network\n(CNN) as their backbone to achieve high tracking accuracy. However, less\nattention has been paid to the potential adversarial threats brought by CNN,\nincluding Siamese network.\n  In this paper, we first analyze the existing vulnerabilities in Siamese\ntrackers and propose the requirements for a successful adversarial attack. On\nthis basis, we formulate the adversarial generation problem and propose an\nend-to-end pipeline to generate a perturbed texture map for the 3D object that\ncauses the trackers to fail. Finally, we conduct thorough experiments to verify\nthe effectiveness of our algorithm. Experiment results show that adversarial\nexamples generated by our algorithm can successfully lower the tracking\naccuracy of victim trackers and even make them drift off. To the best of our\nknowledge, this is the first work to generate 3D adversarial examples on visual\ntrackers.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 09:43:49 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Wu", "Xugang", ""], ["Wang", "Xiaoping", ""], ["Zhou", "Xu", ""], ["Jian", "Songlei", ""]]}, {"id": "1909.03423", "submitter": "Mingyang Li", "authors": "Mingming Zhang, Xingxing Zuo, Yiming Chen, Yong Liu, Mingyang Li", "title": "Pose Estimation for Ground Robots: On Manifold Representation,\n  Integration, Re-Parameterization, and Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on motion estimation dedicated for non-holonomic\nground robots, by probabilistically fusing measurements from the wheel odometer\nand exteroceptive sensors. For ground robots, the wheel odometer is widely used\nin pose estimation tasks, especially in applications under planar-scene based\nenvironments. However, since the wheel odometer only provides 2D motion\nestimates, it is extremely challenging to use that for performing accurate full\n6D pose (3D position and 3D orientation) estimation. Traditional methods on 6D\npose estimation either approximate sensor or motion models, at the cost of\naccuracy reduction, or rely on other sensors, e.g., inertial measurement unit\n(IMU), to provide complementary measurements. By contrast, in this paper, we\npropose a novel method to utilize the wheel odometer for 6D pose estimation, by\nmodeling and utilizing motion manifold for ground robots. Our approach is\nprobabilistically formulated and only requires the wheel odometer and an\nexteroceptive sensor (e.g., a camera). Specifically, our method i) formulates\nthe motion manifold of ground robots by parametric representation, ii) performs\nmanifold based 6D integration with the wheel odometer measurements only, and\niii) re-parameterizes manifold equations periodically for error reduction. To\ndemonstrate the effectiveness and applicability of the proposed algorithmic\nmodules, we integrate that into a sliding-window pose estimator by using\nmeasurements from the wheel odometer and a monocular camera. By conducting\nextensive simulated and real-world experiments, we show that the proposed\nalgorithm outperforms competing state-of-the-art algorithms by a significant\nmargin in pose estimation accuracy, especially when deployed in complex\nlarge-scale real-world environments.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 10:28:41 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 02:49:30 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 04:53:34 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Zhang", "Mingming", ""], ["Zuo", "Xingxing", ""], ["Chen", "Yiming", ""], ["Liu", "Yong", ""], ["Li", "Mingyang", ""]]}, {"id": "1909.03442", "submitter": "Ambedkar Dukkipati", "authors": "Sourabh Balgi and Ambedkar Dukkipati", "title": "CUDA: Contradistinguisher for Unsupervised Domain Adaptation", "comments": "International Conference on Data Mining, ICDM 2019", "journal-ref": null, "doi": "10.1109/ICDM.2019.00012", "report-no": "8970790", "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple model referred as Contradistinguisher\n(CTDR) for unsupervised domain adaptation whose objective is to jointly learn\nto contradistinguish on unlabeled target domain in a fully unsupervised manner\nalong with prior knowledge acquired by supervised learning on an entirely\ndifferent domain. Most recent works in domain adaptation rely on an indirect\nway of first aligning the source and target domain distributions and then learn\na classifier on a labeled source domain to classify target domain. This\napproach of an indirect way of addressing the real task of unlabeled target\ndomain classification has three main drawbacks. (i) The sub-task of obtaining a\nperfect alignment of the domain in itself might be impossible due to large\ndomain shift (e.g., language domains). (ii) The use of multiple classifiers to\nalign the distributions unnecessarily increases the complexity of the neural\nnetworks leading to over-fitting in many cases. (iii) Due to distribution\nalignment, the domain-specific information is lost as the domains get morphed.\nIn this work, we propose a simple and direct approach that does not require\ndomain alignment. We jointly learn CTDR on both source and target distribution\nfor unsupervised domain adaptation task using contradistinguish loss for the\nunlabeled target domain in conjunction with a supervised loss for labeled\nsource domain. Our experiments show that avoiding domain alignment by directly\naddressing the task of unlabeled target domain classification using CTDR\nachieves state-of-the-art results on eight visual and four language benchmark\ndomain adaptation datasets.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 12:16:33 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Balgi", "Sourabh", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1909.03444", "submitter": "Shuaiyi Huang", "authors": "Shuaiyi Huang, Qiuyue Wang, Songyang Zhang, Shipeng Yan, Xuming He", "title": "Dynamic Context Correspondence Network for Semantic Alignment", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing semantic correspondence is a core problem in computer vision and\nremains challenging due to large intra-class variations and lack of annotated\ndata. In this paper, we aim to incorporate global semantic context in a\nflexible manner to overcome the limitations of prior work that relies on local\nsemantic representations. To this end, we first propose a context-aware\nsemantic representation that incorporates spatial layout for robust matching\nagainst local ambiguities. We then develop a novel dynamic fusion strategy\nbased on attention mechanism to weave the advantages of both local and context\nfeatures by integrating semantic cues from multiple scales. We instantiate our\nstrategy by designing an end-to-end learnable deep network, named as Dynamic\nContext Correspondence Network (DCCNet). To train the network, we adopt a\nmulti-auxiliary task loss to improve the efficiency of our weakly-supervised\nlearning procedure. Our approach achieves superior or competitive performance\nover previous methods on several challenging datasets, including PF-Pascal,\nPF-Willow, and TSS, demonstrating its effectiveness and generality.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 12:21:08 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Huang", "Shuaiyi", ""], ["Wang", "Qiuyue", ""], ["Zhang", "Songyang", ""], ["Yan", "Shipeng", ""], ["He", "Xuming", ""]]}, {"id": "1909.03449", "submitter": "Borui Wang", "authors": "Borui Wang, Ehsan Adeli, Hsu-kuang Chiu, De-An Huang, Juan Carlos\n  Niebles", "title": "Imitation Learning for Human Pose Prediction", "comments": "10 pages, 7 figures, accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling and prediction of human motion dynamics has long been a challenging\nproblem in computer vision, and most existing methods rely on the end-to-end\nsupervised training of various architectures of recurrent neural networks.\nInspired by the recent success of deep reinforcement learning methods, in this\npaper we propose a new reinforcement learning formulation for the problem of\nhuman pose prediction, and develop an imitation learning algorithm for\npredicting future poses under this formulation through a combination of\nbehavioral cloning and generative adversarial imitation learning. Our\nexperiments show that our proposed method outperforms all existing\nstate-of-the-art baseline models by large margins on the task of human pose\nprediction in both short-term predictions and long-term predictions, while also\nenjoying huge advantage in training speed.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 12:39:31 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Wang", "Borui", ""], ["Adeli", "Ehsan", ""], ["Chiu", "Hsu-kuang", ""], ["Huang", "De-An", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "1909.03459", "submitter": "Xiaoyu Li", "authors": "Xiaoyu Li, Bo Zhang, Pedro V. Sander, Jing Liao", "title": "Blind Geometric Distortion Correction on Images Through Deep Learning", "comments": "10 pages, 11 figures, published in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the first general framework to automatically correct different\ntypes of geometric distortion in a single input image. Our proposed method\nemploys convolutional neural networks (CNNs) trained by using a large synthetic\ndistortion dataset to predict the displacement field between distorted images\nand corrected images. A model fitting method uses the CNN output to estimate\nthe distortion parameters, achieving a more accurate prediction. The final\ncorrected image is generated based on the predicted flow using an efficient,\nhigh-quality resampling method. Experimental results demonstrate that our\nalgorithm outperforms traditional correction methods, and allows for\ninteresting applications such as distortion transfer, distortion exaggeration,\nand co-occurring distortion correction.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 13:13:12 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Li", "Xiaoyu", ""], ["Zhang", "Bo", ""], ["Sander", "Pedro V.", ""], ["Liao", "Jing", ""]]}, {"id": "1909.03462", "submitter": "Muhammad Usman Khalid", "authors": "Muhammad Usman Khalid, Janik M. Hager, Werner Kraus, Marco F. Huber,\n  Marc Toussaint", "title": "Deep Workpiece Region Segmentation for Bin Picking", "comments": "IEEE CASE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For most industrial bin picking solutions, the pose of a workpiece is\nlocalized by matching a CAD model to point cloud obtained from 3D sensor.\nDistinguishing flat workpieces from bottom of the bin in point cloud imposes\nchallenges in the localization of workpieces that lead to wrong or phantom\ndetections. In this paper, we propose a framework that solves this problem by\nautomatically segmenting workpiece regions from non-workpiece regions in a\npoint cloud data. It is done in real time by applying a fully convolutional\nneural network trained on both simulated and real data. The real data has been\nlabelled by our novel technique which automatically generates ground truth\nlabels for real point clouds. Along with real time workpiece segmentation, our\nframework also helps in improving the number of detected workpieces and\nestimating the correct object poses. Moreover, it decreases the computation\ntime by approximately 1s due to a reduction of the search space for the object\npose estimation.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 13:29:09 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Khalid", "Muhammad Usman", ""], ["Hager", "Janik M.", ""], ["Kraus", "Werner", ""], ["Huber", "Marco F.", ""], ["Toussaint", "Marc", ""]]}, {"id": "1909.03466", "submitter": "Muhammad Usman Khalid", "authors": "Muhammad Usman Khalid and Jie Yu", "title": "Multi-Modal Three-Stream Network for Action Recognition", "comments": "Presented in IEEE ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition in video is an active yet challenging research topic\ndue to high variation and complexity of data. In this paper, a novel video\nbased action recognition framework utilizing complementary cues is proposed to\nhandle this complex problem. Inspired by the successful two stream networks for\naction classification, additional pose features are studied and fused to\nenhance understanding of human action in a more abstract and semantic way.\nTowards practices, not only ground truth poses but also noisy estimated poses\nare incorporated in the framework with our proposed pre-processing module. The\nwhole framework and each cue are evaluated on varied benchmarking datasets as\nJHMDB, sub-JHMDB and Penn Action. Our results outperform state-of-the-art\nperformance on these datasets and show the strength of complementary cues.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 13:40:16 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Khalid", "Muhammad Usman", ""], ["Yu", "Jie", ""]]}, {"id": "1909.03471", "submitter": "\u00c8\u0098tefan S\u00c4\u0083ftescu", "authors": "\\c{S}tefan S\\u{a}ftescu and Paul Newman", "title": "Learning Geometrically Consistent Mesh Corrections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building good 3D maps is a challenging and expensive task, which requires\nhigh-quality sensors and careful, time-consuming scanning. We seek to reduce\nthe cost of building good reconstructions by correcting views of existing\nlow-quality ones in a post-hoc fashion using learnt priors over surfaces and\nappearance. We train a CNN model to predict the difference in inverse-depth\nfrom varying viewpoints of two meshes -- one of low quality that we wish to\ncorrect, and one of high-quality that we use as a reference.\n  In contrast to previous work, we pay attention to the problem of excessive\nsmoothing in corrected meshes. We address this with a suitable network\narchitecture, and introduce a loss-weighting mechanism that emphasises edges in\nthe prediction. Furthermore, smooth predictions result in geometrical\ninconsistencies. To deal with this issue, we present a loss function which\npenalises re-projection differences that are not due to occlusions. Our model\nreduces gross errors by 45.3%--77.5%, up to five times more than previous work.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 13:58:05 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["S\u0103ftescu", "\u015etefan", ""], ["Newman", "Paul", ""]]}, {"id": "1909.03472", "submitter": "Priyam Shah Mr", "authors": "Vivek Mange, Priyam Shah, Vishal Kothari", "title": "Autonomous Underwater Vehicle: Electronics and Software Implementation\n  of the Proton AUV", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with the software and the electronics unit for an autonomous\nunderwater vehicle. The implementation in the electronics unit is the\nconnection and communication between SBC, pixhawk controller and other sensory\nhardware and actuators. The major implementation of the software unit is the\nalgorithm for object detection based on Convolutional Neural Network (CNN) and\nits models. The Hyperparameters were tuned according to Odroid Xu4 for various\nmodels. The maneuvering algorithm uses the MAVLink protocol of the ArduSub\nproject for movement and its simulation.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 13:58:06 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Mange", "Vivek", ""], ["Shah", "Priyam", ""], ["Kothari", "Vishal", ""]]}, {"id": "1909.03482", "submitter": "Narges Mirehi", "authors": "Narges Mirehi and Maryam Tahmasbi and Alireza Tavakoli Targhi", "title": "New Graph-based Features For Shape Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape recognition is the main challenging problem in computer vision.\nDifferent approaches and tools are used to solve this problem. Most existing\napproaches to object recognition are based on pixels. Pixel-based methods are\ndependent on the geometry and nature of the pixels, so the destruction of\npixels reduces their performance. In this paper, we study the ability of graphs\nas shape recognition. We construct a graph that captures the topological and\ngeometrical properties of the object. Then, using the coordinate and relation\nof its vertices, we extract features that are robust to noise, rotation, scale\nvariation, and articulation. To evaluate our method, we provide different\ncomparisons with state-of-the-art results on various known benchmarks,\nincluding Kimia's, Tari56, Tetrapod, and Articulated dataset. We provide an\nanalysis of our method against different variations. The results confirm our\nperformance, especially against noise.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 15:16:26 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Mirehi", "Narges", ""], ["Tahmasbi", "Maryam", ""], ["Targhi", "Alireza Tavakoli", ""]]}, {"id": "1909.03483", "submitter": "Jianbo Jiao", "authors": "Jianbo Jiao, Ana I.L. Namburete, Aris T. Papageorghiou, J. Alison\n  Noble", "title": "Anatomy-Aware Self-supervised Fetal MRI Synthesis from Unpaired\n  Ultrasound Images", "comments": "MICCAI-MLMI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fetal brain magnetic resonance imaging (MRI) offers exquisite images of the\ndeveloping brain but is not suitable for anomaly screening. For this ultrasound\n(US) is employed. While expert sonographers are adept at reading US images, MR\nimages are much easier for non-experts to interpret. Hence in this paper we\nseek to produce images with MRI-like appearance directly from clinical US\nimages. Our own clinical motivation is to seek a way to communicate US findings\nto patients or clinical professionals unfamiliar with US, but in medical image\nanalysis such a capability is potentially useful, for instance, for US-MRI\nregistration or fusion. Our model is self-supervised and end-to-end trainable.\nSpecifically, based on an assumption that the US and MRI data share a similar\nanatomical latent space, we first utilise an extractor to determine shared\nlatent features, which are then used for data synthesis. Since paired data was\nunavailable for our study (and rare in practice), we propose to enforce the\ndistributions to be similar instead of employing pixel-wise constraints, by\nadversarial learning in both the image domain and latent space. Furthermore, we\npropose an adversarial structural constraint to regularise the anatomical\nstructures between the two modalities during the synthesis. A cross-modal\nattention scheme is proposed to leverage non-local spatial correlations. The\nfeasibility of the approach to produce realistic looking MR images is\ndemonstrated quantitatively and with a qualitative evaluation compared to real\nfetal MR images.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 15:28:35 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Jiao", "Jianbo", ""], ["Namburete", "Ana I. L.", ""], ["Papageorghiou", "Aris T.", ""], ["Noble", "J. Alison", ""]]}, {"id": "1909.03493", "submitter": "Donghyun Kim", "authors": "Donghyun Kim, Kuniaki Saito, Kate Saenko, Stan Sclaroff, Bryan A.\n  Plummer", "title": "MULE: Multimodal Universal Language Embedding", "comments": "Accepted as an oral at AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing vision-language methods typically support two languages at a time at\nmost. In this paper, we present a modular approach which can easily be\nincorporated into existing vision-language methods in order to support many\nlanguages. We accomplish this by learning a single shared Multimodal Universal\nLanguage Embedding (MULE) which has been visually-semantically aligned across\nall languages. Then we learn to relate MULE to visual data as if it were a\nsingle language. Our method is not architecture specific, unlike prior work\nwhich typically learned separate branches for each language, enabling our\napproach to easily be adapted to many vision-language methods and tasks. Since\nMULE learns a single language branch in the multimodal model, we can also scale\nto support many languages, and languages with fewer annotations can take\nadvantage of the good representation learned from other (more abundant)\nlanguage data. We demonstrate the effectiveness of MULE on the bidirectional\nimage-sentence retrieval task, supporting up to four languages in a single\nmodel. In addition, we show that Machine Translation can be used for data\naugmentation in multilingual learning, which, combined with MULE, improves mean\nrecall by up to 21.9% on a single-language compared to prior work, with the\nmost significant gains seen on languages with relatively few annotations. Our\ncode is publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 16:08:04 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2019 21:57:10 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Kim", "Donghyun", ""], ["Saito", "Kuniaki", ""], ["Saenko", "Kate", ""], ["Sclaroff", "Stan", ""], ["Plummer", "Bryan A.", ""]]}, {"id": "1909.03534", "submitter": "Narges Mirehi", "authors": "Narges Mirehi and Maryam Tahmasbi", "title": "A New GNG Graph-Based Hand Gesture Recognition Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand Gesture Recognition (HGR) is of major importance for Human-Computer\nInteraction (HCI) applications. In this paper, we present a new hand gesture\nrecognition approach called GNG-IEMD. In this approach, first, we use a Growing\nNeural Gas (GNG) graph to model the image. Then we extract features from this\ngraph. These features are not geometric or pixel-based, so do not depend on\nscale, rotation, and articulation. The dissimilarity between hand gestures is\nmeasured with a novel Improved Earth Mover\\textquotesingle s Distance (IEMD)\nmetric. We evaluate the performance of the proposed approach on challenging\npublic datasets including NTU Hand Digits, HKU, HKU multi-angle, and UESTC-ASL\nand compare the results with state-of-the-art approaches. The experimental\nresults demonstrate the performance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 19:27:30 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Mirehi", "Narges", ""], ["Tahmasbi", "Maryam", ""]]}, {"id": "1909.03552", "submitter": "Xin Liu", "authors": "Xin Liu, Seyran Khademi, Jan C. van Gemert", "title": "Cross Domain Image Matching in Presence of Outliers", "comments": "ICCV Workshop on Transferring and Adaptive Source Knowledge in\n  Computer Vision (TASK-CV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross domain image matching between image collections from different source\nand target domains is challenging in times of deep learning due to i) limited\nvariation of image conditions in a training set, ii) lack of paired-image\nlabels during training, iii) the existing of outliers that makes image matching\ndomains not fully overlap. To this end, we propose an end-to-end architecture\nthat can match cross domain images without labels in the target domain and\nhandle non-overlapping domains by outlier detection. We leverage domain\nadaptation and triplet constraints for training a network capable of learning\ndomain invariant and identity distinguishable representations, and iteratively\ndetecting the outliers with an entropy loss and our proposed weighted MK-MMD.\nExtensive experimental evidence on Office [17] dataset and our proposed\ndatasets Shape, Pitts-CycleGAN shows that the proposed approach yields\nstate-of-the-art cross domain image matching and outlier detection performance\non different benchmarks. The code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 22:00:30 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "Xin", ""], ["Khademi", "Seyran", ""], ["van Gemert", "Jan C.", ""]]}, {"id": "1909.03557", "submitter": "Bing Wang", "authors": "Bing Wang, Changhao Chen, Chris Xiaoxuan Lu, Peijun Zhao, Niki\n  Trigoni, Andrew Markham", "title": "AtLoc: Attention Guided Camera Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved impressive results in camera localization, but\ncurrent single-image techniques typically suffer from a lack of robustness,\nleading to large outliers. To some extent, this has been tackled by sequential\n(multi-images) or geometry constraint approaches, which can learn to reject\ndynamic objects and illumination conditions to achieve better performance. In\nthis work, we show that attention can be used to force the network to focus on\nmore geometrically robust objects and features, achieving state-of-the-art\nperformance in common benchmark, even if using only a single image as input.\nExtensive experimental evidence is provided through public indoor and outdoor\ndatasets. Through visualization of the saliency maps, we demonstrate how the\nnetwork learns to reject dynamic objects, yielding superior global camera pose\nregression performance. The source code is avaliable at\nhttps://github.com/BingCS/AtLoc.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 22:33:02 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 16:56:07 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Wang", "Bing", ""], ["Chen", "Changhao", ""], ["Lu", "Chris Xiaoxuan", ""], ["Zhao", "Peijun", ""], ["Trigoni", "Niki", ""], ["Markham", "Andrew", ""]]}, {"id": "1909.03573", "submitter": "Xuechen Zhang", "authors": "Wenming Yang, Xuechen Zhang, Yapeng Tian, Wei Wang, Jing-Hao Xue, and\n  Qingmin Liao", "title": "LCSCNet: Linear Compressing Based Skip-Connecting Network for Image\n  Super-Resolution", "comments": "Accepted by IEEE Transactions on Image Processing (IEEE-TIP)", "journal-ref": null, "doi": "10.1109/TIP.2019.2940679", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a concise but efficient network architecture called\nlinear compressing based skip-connecting network (LCSCNet) for image\nsuper-resolution. Compared with two representative network architectures with\nskip connections, ResNet and DenseNet, a linear compressing layer is designed\nin LCSCNet for skip connection, which connects former feature maps and\ndistinguishes them from newly-explored feature maps. In this way, the proposed\nLCSCNet enjoys the merits of the distinguish feature treatment of DenseNet and\nthe parameter-economic form of ResNet. Moreover, to better exploit hierarchical\ninformation from both low and high levels of various receptive fields in deep\nmodels, inspired by gate units in LSTM, we also propose an adaptive\nelement-wise fusion strategy with multi-supervised training. Experimental\nresults in comparison with state-of-the-art algorithms validate the\neffectiveness of LCSCNet.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 00:36:45 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Yang", "Wenming", ""], ["Zhang", "Xuechen", ""], ["Tian", "Yapeng", ""], ["Wang", "Wei", ""], ["Xue", "Jing-Hao", ""], ["Liao", "Qingmin", ""]]}, {"id": "1909.03580", "submitter": "Yucai Bai", "authors": "Yucai Bai, Qiang Dai, Long Chen, Lingxi Li, Zhengming Ding and Qin Zou", "title": "Extreme Low Resolution Activity Recognition with Spatial-Temporal\n  Attention Transfer", "comments": "8 pages, 4 fugures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity recognition on extreme low-resolution videos, e.g., a resolution of\n12 * 6 pixels, plays a vital role in far-view surveillance and\nprivacy-preserving multimedia analysis. Low-resolution videos only contain\nlimited information. Given the fact that one same activity may be represented\nby videos in both high resolution(HR) and low resolution (LR), it is worth\nstudying to utilize the relevant HR data to improve the LR activity\nrecognition. In this work, we propose a novel Spatial-Temporal Attention\nTransfer (STAT) for LR activity recognition. STAT can acquire information from\nHR data by reducing the attention differences with a transfer-learning\nstrategy. Experimental results on two well-known datasets, i.e., UCF101 and\nHMDB51, demonstrate that, the proposed method can effectively improve the\naccuracy of LR activity recognition, and achieves an accuracy of 58.12% on 12 *\n16 videos in HMDB51, a state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 01:02:11 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 15:19:47 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 15:31:44 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Bai", "Yucai", ""], ["Dai", "Qiang", ""], ["Chen", "Long", ""], ["Li", "Lingxi", ""], ["Ding", "Zhengming", ""], ["Zou", "Qin", ""]]}, {"id": "1909.03583", "submitter": "Kazuto Ichimaru", "authors": "Kazuto Ichimaru, Yuichi Taguchi and Hiroshi Kawasaki", "title": "Unified Underwater Structure-from-Motion", "comments": "Accepted in International Conference on 3D Vision (3DV 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that accurate underwater 3D shape reconstruction is possible\nusing a single camera, observing a target through a refractive interface. We\nprovide unified reconstruction techniques for a variety of scenarios such as\nsingle static camera and moving refractive interface, single moving camera and\nstatic refractive interface, and single moving camera and moving refractive\ninterface. In our basic setup, we assume that the refractive interface is\nplanar, and simultaneously estimate the unknown transformations of the planar\ninterface and the camera, and the unknown target shape using bundle adjustment.\nWe also extend it to relax the planarity assumption, which enables us to use\nwaves of the refractive interface for the reconstruction task. Experiments with\nreal data show the superiority of our method to existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 01:38:37 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Ichimaru", "Kazuto", ""], ["Taguchi", "Yuichi", ""], ["Kawasaki", "Hiroshi", ""]]}, {"id": "1909.03611", "submitter": "Jinlin Liu", "authors": "Jinlin Liu, Yuan Yao, Jianqiang Ren", "title": "An Acceleration Framework for High Resolution Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesis of high resolution images using Generative Adversarial Networks\n(GANs) is challenging, which usually requires numbers of high-end graphic cards\nwith large memory and long time of training. In this paper, we propose a\ntwo-stage framework to accelerate the training process of synthesizing high\nresolution images. High resolution images are first transformed to small codes\nvia the trained encoder and decoder networks. The code in latent space is times\nsmaller than the original high resolution images. Then, we train a code\ngeneration network to learn the distribution of the latent codes. In this way,\nthe generator only learns to generate small latent codes instead of large\nimages. Finally, we decode the generated latent codes to image space via the\ndecoder networks so as to output the synthesized high resolution images.\nExperimental results show that the proposed method accelerates the training\nprocess significantly and increases the quality of the generated samples. The\nproposed acceleration framework makes it possible to generate high resolution\nimages using less training time with limited hardware resource. After using the\nproposed acceleration method, it takes only 3 days to train a 1024 *1024 image\ngenerator on Celeba-HQ dataset using just one NVIDIA P100 graphic card.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 03:19:25 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "Jinlin", ""], ["Yao", "Yuan", ""], ["Ren", "Jianqiang", ""]]}, {"id": "1909.03613", "submitter": "Aakanksha Rana", "authors": "S. M. Iman Zolanvari, Susana Ruano, Aakanksha Rana, Alan Cummins,\n  Rogerio Eduardo da Silva, Morteza Rahbar, Aljosa Smolic", "title": "DublinCity: Annotated LiDAR Point Cloud and its Applications", "comments": "Accepted to the 30th British Machine Vision Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene understanding of full-scale 3D models of an urban area remains a\nchallenging task. While advanced computer vision techniques offer\ncost-effective approaches to analyse 3D urban elements, a precise and densely\nlabelled dataset is quintessential. The paper presents the first-ever labelled\ndataset for a highly dense Aerial Laser Scanning (ALS) point cloud at\ncity-scale. This work introduces a novel benchmark dataset that includes a\nmanually annotated point cloud for over 260 million laser scanning points into\n100'000 (approx.) assets from Dublin LiDAR point cloud [12] in 2015. Objects\nare labelled into 13 classes using hierarchical levels of detail from large\n(i.e., building, vegetation and ground) to refined (i.e., window, door and\ntree) elements. To validate the performance of our dataset, two different\napplications are showcased. Firstly, the labelled point cloud is employed for\ntraining Convolutional Neural Networks (CNNs) to classify urban elements. The\ndataset is tested on the well-known state-of-the-art CNNs (i.e., PointNet,\nPointNet++ and So-Net). Secondly, the complete ALS dataset is applied as\ndetailed ground truth for city-scale image-based 3D reconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 13:47:31 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Zolanvari", "S. M. Iman", ""], ["Ruano", "Susana", ""], ["Rana", "Aakanksha", ""], ["Cummins", "Alan", ""], ["da Silva", "Rogerio Eduardo", ""], ["Rahbar", "Morteza", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1909.03619", "submitter": "Ziyi Kou", "authors": "Ziyi Kou, Wentian Zhao, Guofeng Cui, Shaojie Wang", "title": "Weakly Supervised Localization Using Background Images", "comments": "Course project of CSC577, University of Rochester", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly Supervised Object Localization (WSOL) methodsusually rely on fully\nconvolutional networks in order to ob-tain class activation maps(CAMs) of\ntargeted labels. How-ever, these networks always highlight the most\ndiscriminativeparts to perform the task, the located areas are much smallerthan\nentire targeted objects. In this work, we propose a novelend-to-end model to\nenlarge CAMs generated from classifi-cation models, which can localize targeted\nobjects more pre-cisely. In detail, we add an additional module in\ntraditionalclassification networks to extract foreground object propos-als from\nimages without classifying them into specific cate-gories. Then we set these\nnormalized regions as unrestrictedpixel-level mask supervision for the\nfollowing classificationtask. We collect a set of images defined as Background\nImageSet from the Internet. The number of them is much smallerthan the targeted\ndataset but surprisingly well supports themethod to extract foreground regions\nfrom different pictures.The region extracted is independent from classification\ntask,where the extracted region in each image covers almost en-tire object\nrather than just a significant part. Therefore, theseregions can serve as masks\nto supervise the response mapgenerated from classification models to become\nlarger andmore precise. The method achieves state-of-the-art results\nonCUB-200-2011 in terms of Top-1 and Top-5 localization er-ror while has a\ncompetitive result on ILSVRC2016 comparedwith other approaches.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 03:34:34 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 01:46:30 GMT"}, {"version": "v3", "created": "Wed, 11 Sep 2019 00:33:11 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Kou", "Ziyi", ""], ["Zhao", "Wentian", ""], ["Cui", "Guofeng", ""], ["Wang", "Shaojie", ""]]}, {"id": "1909.03622", "submitter": "James O' Neill", "authors": "James O' Neill and Danushka Bollegala", "title": "Transfer Reward Learning for Policy Gradient-Based Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-specific scores are often used to optimize for and evaluate the\nperformance of conditional text generation systems. However, such scores are\nnon-differentiable and cannot be used in the standard supervised learning\nparadigm. Hence, policy gradient methods are used since the gradient can be\ncomputed without requiring a differentiable objective.\n  However, we argue that current n-gram overlap based measures that are used as\nrewards can be improved by using model-based rewards transferred from tasks\nthat directly compare the similarity of sentence pairs. These reward models\neither output a score of sentence-level syntactic and semantic similarity\nbetween entire predicted and target sentences as the expected return, or for\nintermediate phrases as segmented accumulative rewards.\n  We demonstrate that using a \\textit{Transferable Reward Learner} leads to\nimproved results on semantical evaluation measures in policy-gradient models\nfor image captioning tasks. Our InferSent actor-critic model improves over a\nBLEU trained actor-critic model on MSCOCO when evaluated on a Word Mover's\nDistance similarity measure by 6.97 points, also improving on a Sliding Window\nCosine Similarity measure by 10.48 points. Similar performance improvements are\nalso obtained on the smaller Flickr-30k dataset, demonstrating the general\napplicability of the proposed transfer learning method.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 03:36:42 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Neill", "James O'", ""], ["Bollegala", "Danushka", ""]]}, {"id": "1909.03625", "submitter": "Yudong Liu", "authors": "Yudong Liu, Yongtao Wang, Siwei Wang, TingTing Liang, Qijie Zhao, Zhi\n  Tang, Haibin Ling", "title": "CBNet: A Novel Composite Backbone Network Architecture for Object\n  Detection", "comments": "7 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In existing CNN based detectors, the backbone network is a very important\ncomponent for basic feature extraction, and the performance of the detectors\nhighly depends on it. In this paper, we aim to achieve better detection\nperformance by building a more powerful backbone from existing backbones like\nResNet and ResNeXt. Specifically, we propose a novel strategy for assembling\nmultiple identical backbones by composite connections between the adjacent\nbackbones, to form a more powerful backbone named Composite Backbone Network\n(CBNet). In this way, CBNet iteratively feeds the output features of the\nprevious backbone, namely high-level features, as part of input features to the\nsucceeding backbone, in a stage-by-stage fashion, and finally the feature maps\nof the last backbone (named Lead Backbone) are used for object detection. We\nshow that CBNet can be very easily integrated into most state-of-the-art\ndetectors and significantly improve their performances. For example, it boosts\nthe mAP of FPN, Mask R-CNN and Cascade R-CNN on the COCO dataset by about 1.5\nto 3.0 percent. Meanwhile, experimental results show that the instance\nsegmentation results can also be improved. Specially, by simply integrating the\nproposed CBNet into the baseline detector Cascade Mask R-CNN, we achieve a new\nstate-of-the-art result on COCO dataset (mAP of 53.3) with single model, which\ndemonstrates great effectiveness of the proposed CBNet architecture. Code will\nbe made available on https://github.com/PKUbahuangliuhe/CBNet.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 04:01:01 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "Yudong", ""], ["Wang", "Yongtao", ""], ["Wang", "Siwei", ""], ["Liang", "TingTing", ""], ["Zhao", "Qijie", ""], ["Tang", "Zhi", ""], ["Ling", "Haibin", ""]]}, {"id": "1909.03637", "submitter": "Lori Dalton", "authors": "Ali Foroughi pour and Lori A. Dalton", "title": "Theory of Optimal Bayesian Feature Filtering", "comments": "51 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal Bayesian feature filtering (OBF) is a supervised screening method\ndesigned for biomarker discovery. In this article, we prove two major\ntheoretical properties of OBF. First, optimal Bayesian feature selection under\na general family of Bayesian models reduces to filtering if and only if the\nunderlying Bayesian model assumes all features are mutually independent.\nTherefore, OBF is optimal if and only if one assumes all features are mutually\nindependent, and OBF is the only filter method that is optimal under at least\none model in the general Bayesian framework. Second, OBF under independent\nGaussian models is consistent under very mild conditions, including cases where\nthe data is non-Gaussian with correlated features. This result provides\nconditions where OBF is guaranteed to identify the correct feature set given\nenough data, and it justifies the use of OBF in non-design settings where its\nassumptions are invalid.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 05:41:10 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["pour", "Ali Foroughi", ""], ["Dalton", "Lori A.", ""]]}, {"id": "1909.03647", "submitter": "Wenhan Yang", "authors": "Jiaying Liu, Dong Liu, Wenhan Yang, Sifeng Xia, Xiaoshuai Zhang,\n  Yuanying Dai", "title": "A Comprehensive Benchmark for Single Image Compression Artifacts\n  Reduction", "comments": "https://flyywh.github.io/LIU4K_Website/", "journal-ref": null, "doi": "10.1109/TIP.2020.3007828", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive study and evaluation of existing single image\ncompression artifacts removal algorithms, using a new 4K resolution benchmark\nincluding diversified foreground objects and background scenes with rich\nstructures, called Large-scale Ideal Ultra high definition 4K (LIU4K)\nbenchmark. Compression artifacts removal, as a common post-processing\ntechnique, aims at alleviating undesirable artifacts such as blockiness,\nringing, and banding caused by quantization and approximation in the\ncompression process. In this work, a systematic listing of the reviewed methods\nis presented based on their basic models (handcrafted models and deep\nnetworks). The main contributions and novelties of these methods are\nhighlighted, and the main development directions, including architectures,\nmulti-domain sources, signal structures, and new targeted units, are\nsummarized. Furthermore, based on a unified deep learning configuration (i.e.\nsame training data, loss function, optimization algorithm, etc.), we evaluate\nrecent deep learning-based methods based on diversified evaluation measures.\nThe experimental results show the state-of-the-art performance comparison of\nexisting methods based on both full-reference, non-reference and task-driven\nmetrics. Our survey would give a comprehensive reference source for future\nresearch on single image compression artifacts removal and inspire new\ndirections of the related fields.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 06:26:24 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Liu", "Jiaying", ""], ["Liu", "Dong", ""], ["Yang", "Wenhan", ""], ["Xia", "Sifeng", ""], ["Zhang", "Xiaoshuai", ""], ["Dai", "Yuanying", ""]]}, {"id": "1909.03656", "submitter": "Peizhuo Li", "authors": "Peizhuo Li, Yunda Sun, Xue Wan", "title": "Saliency based Semi-supervised Learning for Orbiting Satellite Tracking", "comments": "The first two authors contributed equally to the paper. Corresponding\n  Author: Xue Wan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The trajectory and boundary of an orbiting satellite are fundamental\ninformation for on-orbit repairing and manipulation by space robots. This task,\nhowever, is challenging owing to the freely and rapidly motion of on-orbiting\nsatellites, the quickly varying background and the sudden change in\nillumination conditions. Traditional tracking usually relies on a single\nbounding box of the target object, however, more detailed information should be\nprovided by visual tracking such as binary mask. In this paper, we proposed a\nSSLT (Saliency-based Semi-supervised Learning for Tracking) algorithm that\nprovides both the bounding box and segmentation binary mask of target\nsatellites at 12 frame per second without requirement of annotated data. Our\nmethod, SSLT, improves the segmentation performance by generating a saliency\nmap based semi-supervised on-line learning approach within the initial bounding\nbox estimated by tracking. Once a customized segmentation model has been\ntrained, the bounding box and satellite trajectory will be refined using the\nbinary segmentation result. Experiment using real on-orbit rendezvous and\ndocking video from NASA (Nation Aeronautics and Space Administration),\nsimulated satellite animation sequence from ESA (European Space Agency) and\nimage sequences of 3D printed satellite model took in our laboratory\ndemonstrate the robustness, versatility and fast speed of our method compared\nto state-of-the-art tracking and segmentation methods. Our dataset will be\nreleased for academic use in future.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 06:50:33 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Li", "Peizhuo", ""], ["Sun", "Yunda", ""], ["Wan", "Xue", ""]]}, {"id": "1909.03669", "submitter": "Yongcheng Liu", "authors": "Yongcheng Liu, Bin Fan, Gaofeng Meng, Jiwen Lu, Shiming Xiang,\n  Chunhong Pan", "title": "DensePoint: Learning Densely Contextual Representation for Efficient\n  Point Cloud Processing", "comments": "Accepted to ICCV 2019. 15 pages, 8 figures, 16 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud processing is very challenging, as the diverse shapes formed by\nirregular points are often indistinguishable. A thorough grasp of the elusive\nshape requires sufficiently contextual semantic information, yet few works\ndevote to this. Here we propose DensePoint, a general architecture to learn\ndensely contextual representation for point cloud processing. Technically, it\nextends regular grid CNN to irregular point configuration by generalizing a\nconvolution operator, which holds the permutation invariance of points, and\nachieves efficient inductive learning of local patterns. Architecturally, it\nfinds inspiration from dense connection mode, to repeatedly aggregate\nmulti-level and multi-scale semantics in a deep hierarchy. As a result, densely\ncontextual information along with rich semantics, can be acquired by DensePoint\nin an organic manner, making it highly effective. Extensive experiments on\nchallenging benchmarks across four tasks, as well as thorough model analysis,\nverify DensePoint achieves the state of the arts.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 07:18:30 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "Yongcheng", ""], ["Fan", "Bin", ""], ["Meng", "Gaofeng", ""], ["Lu", "Jiwen", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1909.03677", "submitter": "Anne S. Wannenwetsch", "authors": "Anne S. Wannenwetsch, Martin Kiefel, Peter V. Gehler, Stefan Roth", "title": "Learning Task-Specific Generalized Convolutions in the Permutohedral\n  Lattice", "comments": "To appear at GCPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense prediction tasks typically employ encoder-decoder architectures, but\nthe prevalent convolutions in the decoder are not image-adaptive and can lead\nto boundary artifacts. Different generalized convolution operations have been\nintroduced to counteract this. We go beyond these by leveraging guidance data\nto redefine their inherent notion of proximity. Our proposed network layer\nbuilds on the permutohedral lattice, which performs sparse convolutions in a\nhigh-dimensional space allowing for powerful non-local operations despite small\nfilters. Multiple features with different characteristics span this\npermutohedral space. In contrast to prior work, we learn these features in a\ntask-specific manner by generalizing the basic permutohedral operations to\nlearnt feature representations. As the resulting objective is complex, a\ncarefully designed framework and learning procedure are introduced, yielding\nrich feature embeddings in practice. We demonstrate the general applicability\nof our approach in different joint upsampling tasks. When adding our network\nlayer to state-of-the-art networks for optical flow and semantic segmentation,\nboundary artifacts are removed and the accuracy is improved.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 07:36:02 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Wannenwetsch", "Anne S.", ""], ["Kiefel", "Martin", ""], ["Gehler", "Peter V.", ""], ["Roth", "Stefan", ""]]}, {"id": "1909.03683", "submitter": "Christopher Clark", "authors": "Christopher Clark, Mark Yatskar, Luke Zettlemoyer", "title": "Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known\n  Dataset Biases", "comments": "In EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art models often make use of superficial patterns in the data\nthat do not generalize well to out-of-domain or adversarial settings. For\nexample, textual entailment models often learn that particular key words imply\nentailment, irrespective of context, and visual question answering models learn\nto predict prototypical answers, without considering evidence in the image. In\nthis paper, we show that if we have prior knowledge of such biases, we can\ntrain a model to be more robust to domain shift. Our method has two stages: we\n(1) train a naive model that makes predictions exclusively based on dataset\nbiases, and (2) train a robust model as part of an ensemble with the naive one\nin order to encourage it to focus on other patterns in the data that are more\nlikely to generalize. Experiments on five datasets with out-of-domain test sets\nshow significantly improved robustness in all settings, including a 12 point\ngain on a changing priors visual question answering dataset and a 9 point gain\non an adversarial question answering test set.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 07:44:24 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Clark", "Christopher", ""], ["Yatskar", "Mark", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "1909.03712", "submitter": "Zhao Kang", "authors": "Xiaofan Bo and Zhao Kang and Zhitong Zhao and Yuanzhang Su and Wenyu\n  Chen", "title": "Latent Multi-view Semi-Supervised Classification", "comments": "ACML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To explore underlying complementary information from multiple views, in this\npaper, we propose a novel Latent Multi-view Semi-Supervised Classification\n(LMSSC) method. Unlike most existing multi-view semi-supervised classification\nmethods that learn the graph using original features, our method seeks an\nunderlying latent representation and performs graph learning and label\npropagation based on the learned latent representation. With the\ncomplementarity of multiple views, the latent representation could depict the\ndata more comprehensively than every single view individually, accordingly\nmaking the graph more accurate and robust as well. Finally, LMSSC integrates\nlatent representation learning, graph construction, and label propagation into\na unified framework, which makes each subtask optimized. Experimental results\non real-world benchmark datasets validate the effectiveness of our proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 09:18:39 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Bo", "Xiaofan", ""], ["Kang", "Zhao", ""], ["Zhao", "Zhitong", ""], ["Su", "Yuanzhang", ""], ["Chen", "Wenyu", ""]]}, {"id": "1909.03714", "submitter": "Yude Wang", "authors": "Yude Wang, Jie Zhang, Meina Kan, Shiguang Shan, Xilin Chen", "title": "Self-supervised Scale Equivariant Network for Weakly Supervised Semantic\n  Segmentation", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised semantic segmentation has attracted much research interest\nin recent years considering its advantage of low labeling cost. Most of the\nadvanced algorithms follow the design principle that expands and constrains the\nseed regions from class activation maps (CAM). As well-known, conventional CAM\ntends to be incomplete or over-activated due to weak supervision. Fortunately,\nwe find that semantic segmentation has a characteristic of spatial\ntransformation equivariance, which can form a few self-supervisions to help\nweakly supervised learning. This work mainly explores the advantages of scale\nequivariant constrains for CAM generation, formulated as a self-supervised\nscale equivariant network (SSENet). Specifically, a novel scale equivariant\nregularization is elaborately designed to ensure consistency of CAMs from the\nsame input image with different resolutions. This novel scale equivariant\nregularization can guide the whole network to learn more accurate class\nactivation. This regularized CAM can be embedded in most recent advanced weakly\nsupervised semantic segmentation framework. Extensive experiments on PASCAL VOC\n2012 datasets demonstrate that our method achieves the state-of-the-art\nperformance both quantitatively and qualitatively for weakly supervised\nsemantic segmentation. Code has been made available.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 09:23:11 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Wang", "Yude", ""], ["Zhang", "Jie", ""], ["Kan", "Meina", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1909.03748", "submitter": "Rao Muhammad Umer", "authors": "Rao Muhammad Umer, Gian Luca Foresti, Christian Micheloni", "title": "Deep Super-Resolution Network for Single Image Super-Resolution with\n  Realistic Degradations", "comments": "7 pages", "journal-ref": "13th International Conference on Distributed Smart Cameras (ICDSC\n  2019)", "doi": "10.1145/3349801.3349823", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single Image Super-Resolution (SISR) aims to generate a high-resolution (HR)\nimage of a given low-resolution (LR) image. The most of existing convolutional\nneural network (CNN) based SISR methods usually take an assumption that a LR\nimage is only bicubicly down-sampled version of an HR image. However, the true\ndegradation (i.e. the LR image is a bicubicly downsampled, blurred and noisy\nversion of an HR image) of a LR image goes beyond the widely used bicubic\nassumption, which makes the SISR problem highly ill-posed nature of inverse\nproblems. To address this issue, we propose a deep SISR network that works for\nblur kernels of different sizes, and different noise levels in an unified\nresidual CNN-based denoiser network, which significantly improves a practical\nCNN-based super-resolver for real applications. Extensive experimental results\non synthetic LR datasets and real images demonstrate that our proposed method\nnot only can produce better results on more realistic degradation but also\ncomputational efficient to practical SISR applications.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 10:40:06 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Umer", "Rao Muhammad", ""], ["Foresti", "Gian Luca", ""], ["Micheloni", "Christian", ""]]}, {"id": "1909.03749", "submitter": "Fabio Ferreira", "authors": "Fabio Ferreira, Lin Shao, Tamim Asfour, Jeannette Bohg", "title": "Learning Visual Dynamics Models of Rigid Objects using Relational\n  Inductive Biases", "comments": "short paper (4 pages, two figures), accepted to NeurIPS 2019 Graph\n  Representation Learning workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Endowing robots with human-like physical reasoning abilities remains\nchallenging. We argue that existing methods often disregard spatio-temporal\nrelations and by using Graph Neural Networks (GNNs) that incorporate a\nrelational inductive bias, we can shift the learning process towards exploiting\nrelations. In this work, we learn action-conditional forward dynamics models of\na simulated manipulation task from visual observations involving cluttered and\nirregularly shaped objects. We investigate two GNN approaches and empirically\nassess their capability to generalize to scenarios with novel and an increasing\nnumber of objects. The first, Graph Networks (GN) based approach, considers\nexplicitly defined edge attributes and not only does it consistently\nunderperform an auto-encoder baseline that we modified to predict future\nstates, our results indicate how different edge attributes can significantly\ninfluence the predictions. Consequently, we develop the Auto-Predictor that\ndoes not rely on explicitly defined edge attributes. It outperforms the\nbaseline and the GN-based models. Overall, our results show the sensitivity of\nGNN-based approaches to the task representation, the efficacy of relational\ninductive biases and advocate choosing lightweight approaches that implicitly\nreason about relations over ones that leave these decisions to human designers.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 10:43:56 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 21:00:07 GMT"}, {"version": "v3", "created": "Wed, 23 Oct 2019 17:32:04 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Ferreira", "Fabio", ""], ["Shao", "Lin", ""], ["Asfour", "Tamim", ""], ["Bohg", "Jeannette", ""]]}, {"id": "1909.03751", "submitter": "Zhang Youmin", "authors": "Youmin Zhang, Yimin Chen, Xiao Bai, Suihanjin Yu, Kun Yu, Zhiwei Li,\n  Kuiyuan Yang", "title": "Adaptive Unimodal Cost Volume Filtering for Deep Stereo Matching", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  State-of-the-art deep learning based stereo matching approaches treat\ndisparity estimation as a regression problem, where loss function is directly\ndefined on true disparities and their estimated ones. However, disparity is\njust a byproduct of a matching process modeled by cost volume, while indirectly\nlearning cost volume driven by disparity regression is prone to overfitting\nsince the cost volume is under constrained. In this paper, we propose to\ndirectly add constraints to the cost volume by filtering cost volume with\nunimodal distribution peaked at true disparities. In addition, variances of the\nunimodal distributions for each pixel are estimated to explicitly model\nmatching uncertainty under different contexts. The proposed architecture\nachieves state-of-the-art performance on Scene Flow and two KITTI stereo\nbenchmarks. In particular, our method ranked the $1^{st}$ place of KITTI 2012\nevaluation and the $4^{th}$ place of KITTI 2015 evaluation (recorded on\n2019.8.20). The codes of AcfNet are available at:\nhttps://github.com/DeepMotionAIResearch/DenseMatchingBenchmark.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 10:45:20 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 13:44:15 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Zhang", "Youmin", ""], ["Chen", "Yimin", ""], ["Bai", "Xiao", ""], ["Yu", "Suihanjin", ""], ["Yu", "Kun", ""], ["Li", "Zhiwei", ""], ["Yang", "Kuiyuan", ""]]}, {"id": "1909.03752", "submitter": "Dan Barnes", "authors": "Dan Barnes, Rob Weston and Ingmar Posner", "title": "Masking by Moving: Learning Distraction-Free Radar Odometry from Pose\n  Information", "comments": "Conference on Robot Learning (CoRL), 2019. Video summary:\n  https://youtu.be/eG4Q-j3_6dk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an end-to-end radar odometry system which delivers\nrobust, real-time pose estimates based on a learned embedding space free of\nsensing artefacts and distractor objects. The system deploys a fully\ndifferentiable, correlation-based radar matching approach. This provides the\nsame level of interpretability as established scan-matching methods and allows\nfor a principled derivation of uncertainty estimates. The system is trained in\na (self-)supervised way using only previously obtained pose information as a\ntraining signal. Using 280km of urban driving data, we demonstrate that our\napproach outperforms the previous state-of-the-art in radar odometry by\nreducing errors by up 68% whilst running an order of magnitude faster.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 10:46:15 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 11:19:34 GMT"}, {"version": "v3", "created": "Sun, 10 Nov 2019 12:41:10 GMT"}, {"version": "v4", "created": "Fri, 17 Jan 2020 16:14:26 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Barnes", "Dan", ""], ["Weston", "Rob", ""], ["Posner", "Ingmar", ""]]}, {"id": "1909.03765", "submitter": "Shuyu Lin", "authors": "Shuyu Lin, Stephen Roberts, Niki Trigoni, Ronald Clark", "title": "Balancing Reconstruction Quality and Regularisation in ELBO for VAEs", "comments": "8 pages for main contents and 15 pages for supplemental materials\n  that include data pre-processing, model architectures and more results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A trade-off exists between reconstruction quality and the prior\nregularisation in the Evidence Lower Bound (ELBO) loss that Variational\nAutoencoder (VAE) models use for learning. There are few satisfactory\napproaches to deal with a balance between the prior and reconstruction\nobjective, with most methods dealing with this problem through heuristics. In\nthis paper, we show that the noise variance (often set as a fixed value) in the\nGaussian likelihood p(x|z) for real-valued data can naturally act to provide\nsuch a balance. By learning this noise variance so as to maximise the ELBO\nloss, we automatically obtain an optimal trade-off between the reconstruction\nerror and the prior constraint on the posteriors. This variance can be\ninterpreted intuitively as the necessary noise level for the current model to\nbe the best explanation of the observed dataset. Further, by allowing the\nvariance inference to be more flexible it can conveniently be used as an\nuncertainty estimator for reconstructed or generated samples. We demonstrate\nthat optimising the noise variance is a crucial component of VAE learning, and\nshowcase the performance on MNIST, Fashion MNIST and CelebA datasets. We find\nour approach can significantly improve the quality of generated samples whilst\nmaintaining a smooth latent-space manifold to represent the data. The method\nalso offers an indication of uncertainty in the final generative model.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 11:18:52 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Lin", "Shuyu", ""], ["Roberts", "Stephen", ""], ["Trigoni", "Niki", ""], ["Clark", "Ronald", ""]]}, {"id": "1909.03812", "submitter": "Anastasia Ingacheva", "authors": "Alexander Sheshkus, Anastasia Ingacheva, Vladimir Arlazarov, Dmitry\n  Nikolaev", "title": "HoughNet: neural network architecture for vanishing points detection", "comments": "6 pages, 6 figures, 2 tables, 28 references, conference", "journal-ref": "15th International Conference on Document Analysis and Recognition\n  (ICDAR 2019)", "doi": "10.1109/ICDAR.2019.00140", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel neural network architecture based on Fast\nHough Transform layer. The layer of this type allows our neural network to\naccumulate features from linear areas across the entire image instead of local\nareas. We demonstrate its potential by solving the problem of vanishing points\ndetection in the images of documents. Such problem occurs when dealing with\ncamera shots of the documents in uncontrolled conditions. In this case, the\ndocument image can suffer several specific distortions including projective\ntransform. To train our model, we use MIDV-500 dataset and provide testing\nresults. The strong generalization ability of the suggested method is proven\nwith its applying to a completely different ICDAR 2011 dewarping contest. In\npreviously published papers considering these dataset authors measured the\nquality of vanishing point detection by counting correctly recognized words\nwith open OCR engine Tesseract. To compare with them, we reproduce this\nexperiment and show that our method outperforms the state-of-the-art result.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 12:45:19 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 07:41:38 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Sheshkus", "Alexander", ""], ["Ingacheva", "Anastasia", ""], ["Arlazarov", "Vladimir", ""], ["Nikolaev", "Dmitry", ""]]}, {"id": "1909.03817", "submitter": "Xinyue Zheng", "authors": "Xinyue Zheng, Peng Wang, Qigang Wang, Zhongchao shi, Feiyu Xu", "title": "Efficient Automatic Meta Optimization Search for Few-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous works on meta-learning either relied on elaborately hand-designed\nnetwork structures or adopted specialized learning rules to a particular\ndomain. We propose a universal framework to optimize the meta-learning process\nautomatically by adopting neural architecture search technique (NAS). NAS\nautomatically generates and evaluates meta-learner's architecture for few-shot\nlearning problems, while the meta-learner uses meta-learning algorithm to\noptimize its parameters based on the distribution of learning tasks. Parameter\nsharing and experience replay are adopted to accelerate the architectures\nsearching process, so it takes only 1-2 GPU days to find good architectures.\nExtensive experiments on Mini-ImageNet and Omniglot show that our algorithm\nexcels in few-shot learning tasks. The best architecture found on Mini-ImageNet\nachieves competitive results when transferred to Omniglot, which shows the high\ntransferability of architectures among different computer vision problems.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 02:48:52 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Zheng", "Xinyue", ""], ["Wang", "Peng", ""], ["Wang", "Qigang", ""], ["shi", "Zhongchao", ""], ["Xu", "Feiyu", ""]]}, {"id": "1909.03824", "submitter": "Yongqiang Tian", "authors": "Yongqiang Tian, Shiqing Ma, Ming Wen, Yepang Liu, Shing-Chi Cheung,\n  Xiangyu Zhang", "title": "Testing Deep Learning Models for Image Analysis Using Object-Relevant\n  Metamorphic Relations", "comments": "Please note that a later version of this paper is accepted by\n  Empirical Software Engineering in 2021. The title of the accepted paper is:\n  \"To What Extent Do DNN-based Image Classification Models Make Unreliable\n  Inferences?\". Please contact the first author if you are interested in the\n  accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are widely used for image analysis. While they offer\nhigh performance in terms of accuracy, people are concerned about if these\nmodels inappropriately make inferences using irrelevant features that are not\nencoded from the target object in a given image. To address the concern, we\npropose a metamorphic testing approach that assesses if a given inference is\nmade based on irrelevant features. Specifically, we propose two novel\nmetamorphic relations to detect such inappropriate inferences. We applied our\napproach to 10 image classification models and 10 object detection models, with\nthree large datasets, i.e., ImageNet, COCO, and Pascal VOC. Over 5.3% of the\ntop-5 correct predictions made by the image classification models are subject\nto inappropriate inferences using irrelevant features. The corresponding rate\nfor the object detection models is over 8.5%. Based on the findings, we further\ndesigned a new image generation strategy that can effectively attack existing\nmodels. Comparing with a baseline approach, our strategy can double the success\nrate of attacks.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 13:31:15 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 07:10:53 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Tian", "Yongqiang", ""], ["Ma", "Shiqing", ""], ["Wen", "Ming", ""], ["Liu", "Yepang", ""], ["Cheung", "Shing-Chi", ""], ["Zhang", "Xiangyu", ""]]}, {"id": "1909.03834", "submitter": "Dongsheng Ruan", "authors": "Dongsheng Ruan and Jun Wen and Nenggan Zheng and Min Zheng", "title": "Linear Context Transform Block", "comments": "AAAI-2020 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Squeeze-and-Excitation (SE) block presents a channel attention mechanism for\nmodeling global context via explicitly capturing dependencies across channels.\nHowever, we are still far from understanding how the SE block works. In this\nwork, we first revisit the SE block, and then present a detailed empirical\nstudy of the relationship between global context and attention distribution,\nbased on which we propose a simple yet effective module, called Linear Context\nTransform (LCT) block. We divide all channels into different groups and\nnormalize the globally aggregated context features within each channel group,\nreducing the disturbance from irrelevant channels. Through linear transform of\nthe normalized context features, we model global context for each channel\nindependently. The LCT block is extremely lightweight and easy to be plugged\ninto different backbone models while with negligible parameters and\ncomputational burden increase. Extensive experiments show that the LCT block\noutperforms the SE block in image classification task on the ImageNet and\nobject detection/segmentation on the COCO dataset with different backbone\nmodels. Moreover, LCT yields consistent performance gains over existing\nstate-of-the-art detection architectures, e.g., 1.5$\\sim$1.7% AP$^{bbox}$ and\n1.0$\\sim$1.2% AP$^{mask}$ improvements on the COCO benchmark, irrespective of\ndifferent baseline models of varied capacities. We hope our simple yet\neffective approach will shed some light on future research of attention-based\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 12:31:28 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 10:57:33 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Ruan", "Dongsheng", ""], ["Wen", "Jun", ""], ["Zheng", "Nenggan", ""], ["Zheng", "Min", ""]]}, {"id": "1909.03839", "submitter": "Haoyue Bai", "authors": "Haoyue Bai, Song Wen, S.-H. Gary Chan", "title": "Crowd Counting on Images with Scale Variation and Isolated Clusters", "comments": "Accepted at International Conference on Computer Vision (ICCV) 2019\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting is to estimate the number of objects (e.g., people or\nvehicles) in an image of unconstrained congested scenes. Designing a general\ncrowd counting algorithm applicable to a wide range of crowd images is\nchallenging, mainly due to the possibly large variation in object scales and\nthe presence of many isolated small clusters. Previous approaches based on\nconvolution operations with multi-branch architecture are effective for only\nsome narrow bands of scales and have not captured the long-range contextual\nrelationship due to isolated clustering. To address that, we propose SACANet, a\nnovel scale-adaptive long-range context-aware network for crowd counting.\nSACANet consists of three major modules: the pyramid contextual module which\nextracts long-range contextual information and enlarges the receptive field, a\nscale-adaptive self-attention multi-branch module to attain high scale\nsensitivity and detection accuracy of isolated clusters, and a hierarchical\nfusion module to fuse multi-level self-attention features. With group\nnormalization, SACANet achieves better optimality in the training process. We\nhave conducted extensive experiments using the VisDrone2019 People dataset, the\nVisDrone2019 Vehicle dataset, and some other challenging benchmarks. As\ncompared with the state-of-the-art methods, SACANet is shown to be effective,\nespecially for extremely crowded conditions with diverse scales and scattered\nclusters, and achieves much lower MAE as compared with baselines.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 13:17:26 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Bai", "Haoyue", ""], ["Wen", "Song", ""], ["Chan", "S. -H. Gary", ""]]}, {"id": "1909.03850", "submitter": "Wenwei Zhang", "authors": "Wenwei Zhang, Hui Zhou, Shuyang Sun, Zhe Wang, Jianping Shi, Chen\n  Change Loy", "title": "Robust Multi-Modality Multi-Object Tracking", "comments": "To appear in ICCV 2019. Code and models are available at\n  https://github.com/ZwwWayne/mmMOT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-sensor perception is crucial to ensure the reliability and accuracy in\nautonomous driving system, while multi-object tracking (MOT) improves that by\ntracing sequential movement of dynamic objects. Most current approaches for\nmulti-sensor multi-object tracking are either lack of reliability by tightly\nrelying on a single input source (e.g., center camera), or not accurate enough\nby fusing the results from multiple sensors in post processing without fully\nexploiting the inherent information. In this study, we design a generic\nsensor-agnostic multi-modality MOT framework (mmMOT), where each modality\n(i.e., sensors) is capable of performing its role independently to preserve\nreliability, and further improving its accuracy through a novel multi-modality\nfusion module. Our mmMOT can be trained in an end-to-end manner, enables joint\noptimization for the base feature extractor of each modality and an adjacency\nestimator for cross modality. Our mmMOT also makes the first attempt to encode\ndeep representation of point cloud in data association process in MOT. We\nconduct extensive experiments to evaluate the effectiveness of the proposed\nframework on the challenging KITTI benchmark and report state-of-the-art\nperformance. Code and models are available at\nhttps://github.com/ZwwWayne/mmMOT.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 13:37:09 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Zhang", "Wenwei", ""], ["Zhou", "Hui", ""], ["Sun", "Shuyang", ""], ["Wang", "Zhe", ""], ["Shi", "Jianping", ""], ["Loy", "Chen Change", ""]]}, {"id": "1909.03856", "submitter": "Andre Anjos", "authors": "Tim Laibacher and Andr\\'e Anjos", "title": "On the Evaluation and Real-World Usage Scenarios of Deep Vessel\n  Segmentation for Retinography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify and address three research gaps in the field of vessel\nsegmentation for funduscopy. The first focuses on the task of inference on\nhigh-resolution fundus images for which only a limited set of ground-truth data\nis publicly available. Notably, we highlight that simple rescaling and padding\nor cropping of lower resolution datasets is surprisingly effective.\nAdditionally we explore the effectiveness of semi-supervised learning for\nbetter domain adaptation. Our results show competitive performance on a set of\ncommon public retinal vessel datasets using a small and light-weight neural\nnetwork. For HRF, the only very high-resolution dataset currently available, we\nreach new state-of-the-art performance by solely relying on training images\nfrom lower-resolution datasets. The second topic concerns evaluation metrics.\nWe investigate the variability of the F1-score on the existing datasets and\nreport results for recent SOTA architectures. Our evaluation show that most\nSOTA results are actually comparable to each other in performance. Last, we\naddress the issue of reproducibility by open-sourcing our complete pipeline.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 13:43:34 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 09:18:46 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2020 06:42:40 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Laibacher", "Tim", ""], ["Anjos", "Andr\u00e9", ""]]}, {"id": "1909.03877", "submitter": "Ting Yao", "authors": "Fuchen Long and Ting Yao and Zhaofan Qiu and Xinmei Tian and Jiebo Luo\n  and Tao Mei", "title": "Gaussian Temporal Awareness Networks for Action Localization", "comments": "CVPR 2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporally localizing actions in a video is a fundamental challenge in video\nunderstanding. Most existing approaches have often drawn inspiration from image\nobject detection and extended the advances, e.g., SSD and Faster R-CNN, to\nproduce temporal locations of an action in a 1D sequence. Nevertheless, the\nresults can suffer from robustness problem due to the design of predetermined\ntemporal scales, which overlooks the temporal structure of an action and limits\nthe utility on detecting actions with complex variations. In this paper, we\npropose to address the problem by introducing Gaussian kernels to dynamically\noptimize temporal scale of each action proposal. Specifically, we present\nGaussian Temporal Awareness Networks (GTAN) --- a new architecture that novelly\nintegrates the exploitation of temporal structure into an one-stage action\nlocalization framework. Technically, GTAN models the temporal structure through\nlearning a set of Gaussian kernels, each for a cell in the feature maps. Each\nGaussian kernel corresponds to a particular interval of an action proposal and\na mixture of Gaussian kernels could further characterize action proposals with\nvarious length. Moreover, the values in each Gaussian curve reflect the\ncontextual contributions to the localization of an action proposal. Extensive\nexperiments are conducted on both THUMOS14 and ActivityNet v1.3 datasets, and\nsuperior results are reported when comparing to state-of-the-art approaches.\nMore remarkably, GTAN achieves 1.9% and 1.1% improvements in mAP on testing set\nof the two datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 14:13:48 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Long", "Fuchen", ""], ["Yao", "Ting", ""], ["Qiu", "Zhaofan", ""], ["Tian", "Xinmei", ""], ["Luo", "Jiebo", ""], ["Mei", "Tao", ""]]}, {"id": "1909.03879", "submitter": "Mingqing Xiao", "authors": "Mingqing Xiao, Adam Kortylewski, Ruihai Wu, Siyuan Qiao, Wei Shen,\n  Alan Yuille", "title": "TDAPNet: Prototype Network with Recurrent Top-Down Attention for Robust\n  Object Classification under Partial Occlusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite deep convolutional neural networks' great success in object\nclassification, it suffers from severe generalization performance drop under\nocclusion due to the inconsistency between training and testing data. Because\nof the large variance of occluders, our goal is a model trained on\nocclusion-free data while generalizable to occlusion conditions. In this work,\nwe integrate prototypes, partial matching and top-down attention regulation\ninto deep neural networks to realize robust object classification under\nocclusion. We first introduce prototype learning as its regularization\nencourages compact data clusters, which enables better generalization ability\nunder inconsistent conditions. Then, attention map at intermediate layer based\non feature dictionary and activation scale is estimated for partial matching,\nwhich sifts irrelevant information out when comparing features with prototypes.\nFurther, inspired by neuroscience research that reveals the important role of\nfeedback connection for object recognition under occlusion, a top-down feedback\nattention regulation is introduced into convolution layers, purposefully\nreducing the contamination by occlusion during feature extraction stage. Our\nexperiment results on partially occluded MNIST and vehicles from the PASCAL3D+\ndataset demonstrate that the proposed network significantly improves the\nrobustness of current deep neural networks under occlusion. Our code will be\nreleased.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 14:17:59 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 06:57:01 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Xiao", "Mingqing", ""], ["Kortylewski", "Adam", ""], ["Wu", "Ruihai", ""], ["Qiao", "Siyuan", ""], ["Shen", "Wei", ""], ["Yuille", "Alan", ""]]}, {"id": "1909.03889", "submitter": "Guangcan Liu", "authors": "Guangcan Liu, Wayne Zhang", "title": "Recovery of Future Data via Convolution Nuclear Norm Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of time series forecasting (TSF) from the\nperspective of compressed sensing. First of all, we convert TSF into a more\ninclusive problem called tensor completion with arbitrary sampling (TCAS),\nwhich is to restore a tensor from a subset of its entries sampled in an\narbitrary manner. While it is known that, in the framework of Tucker\nlow-rankness, it is theoretically impossible to identify the target tensor\nbased on some arbitrarily selected entries, in this work we shall show that\nTCAS is indeed tackleable in the light of a new concept called convolutional\nlow-rankness, which is a generalization of the well-known Fourier sparsity.\nThen we introduce a convex program termed Convolution Nuclear Norm Minimization\n(CNNM), and we prove that CNNM succeeds in solving TCAS as long as a sampling\ncondition--which depends on the convolution rank of the target tensor--is\nobeyed. Experiments on univariate time series, images and videos show\nencouraging results.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 07:52:22 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 08:00:23 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 08:04:08 GMT"}, {"version": "v4", "created": "Fri, 28 May 2021 08:12:17 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Liu", "Guangcan", ""], ["Zhang", "Wayne", ""]]}, {"id": "1909.03909", "submitter": "Ting Yao", "authors": "Yehao Li and Ting Yao and Yingwei Pan and Hongyang Chao and Tao Mei", "title": "Deep Metric Learning with Density Adaptivity", "comments": "Accepted by IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of distance metric learning is mostly considered from the\nperspective of learning an embedding space, where the distances between pairs\nof examples are in correspondence with a similarity metric. With the rise and\nsuccess of Convolutional Neural Networks (CNN), deep metric learning (DML)\ninvolves training a network to learn a nonlinear transformation to the\nembedding space. Existing DML approaches often express the supervision through\nmaximizing inter-class distance and minimizing intra-class variation. However,\nthe results can suffer from overfitting problem, especially when the training\nexamples of each class are embedded together tightly and the density of each\nclass is very high. In this paper, we integrate density, i.e., the measure of\ndata concentration in the representation, into the optimization of DML\nframeworks to adaptively balance inter-class similarity and intra-class\nvariation by training the architecture in an end-to-end manner. Technically,\nthe knowledge of density is employed as a regularizer, which is pluggable to\nany DML architecture with different objective functions such as contrastive\nloss, N-pair loss and triplet loss. Extensive experiments on three public\ndatasets consistently demonstrate clear improvements by amending three types of\nembedding with the density adaptivity. More remarkably, our proposal increases\nRecall@1 from 67.95% to 77.62%, from 52.01% to 55.64% and from 68.20% to 70.56%\non Cars196, CUB-200-2011 and Stanford Online Products dataset, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 15:04:26 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Li", "Yehao", ""], ["Yao", "Ting", ""], ["Pan", "Yingwei", ""], ["Chao", "Hongyang", ""], ["Mei", "Tao", ""]]}, {"id": "1909.03917", "submitter": "Sol Pedre Dr", "authors": "Juan Jose Tarrio, Claus Smitt, Sol Pedre", "title": "SE-SLAM: Semi-Dense Structured Edge-Based Monocular SLAM", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based Simultaneous Localization And Mapping (VSLAM) is a mature\nproblem in Robotics. Most VSLAM systems are feature based methods, which are\nrobust and present high accuracy, but yield sparse maps with limited\napplication for further navigation tasks. Most recently, direct methods which\noperate directly on image intensity have been introduced, capable of\nreconstructing richer maps at the cost of higher processing power. In this\nwork, an edge-based monocular SLAM system (SE-SLAM) is proposed as a middle\npoint: edges present good localization as point features, while enabling a\nstructural semidense map reconstruction. However, edges are not easy to\nassociate, track and optimize over time, as they lack descriptors and\nbiunivocal correspondence, unlike point features. To tackle these issues, this\npaper presents a method to match edges between frames in a consistent manner; a\nfeasible strategy to solve the optimization problem, since its size rapidly\nincreases when working with edges; and the use of non-linear optimization\ntechniques. The resulting system achieves comparable precision to state of the\nart feature-based and dense/semi-dense systems, while inherently building a\nstructural semi-dense reconstruction of the environment, providing relevant\nstructure data for further navigation algorithms. To achieve such accuracy,\nstate of the art non-linear optimization is needed, over a continuous feed of\n10000 edgepoints per frame, to optimize the full semi-dense output. Despite its\nheavy processing requirements, the system achieves near to real-time operation,\nthanks to a custom built solver and parallelization of its key stages. In order\nto encourage further development of edge-based SLAM systems, SE-SLAM source\ncode will be released as open source.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 15:18:14 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Tarrio", "Juan Jose", ""], ["Smitt", "Claus", ""], ["Pedre", "Sol", ""]]}, {"id": "1909.03918", "submitter": "Ting Yao", "authors": "Ting Yao and Yingwei Pan and Yehao Li and Tao Mei", "title": "Hierarchy Parsing for Image Captioning", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is always well believed that parsing an image into constituent visual\npatterns would be helpful for understanding and representing an image.\nNevertheless, there has not been evidence in support of the idea on describing\nan image with a natural-language utterance. In this paper, we introduce a new\ndesign to model a hierarchy from instance level (segmentation), region level\n(detection) to the whole image to delve into a thorough image understanding for\ncaptioning. Specifically, we present a HIerarchy Parsing (HIP) architecture\nthat novelly integrates hierarchical structure into image encoder. Technically,\nan image decomposes into a set of regions and some of the regions are resolved\ninto finer ones. Each region then regresses to an instance, i.e., foreground of\nthe region. Such process naturally builds a hierarchal tree. A tree-structured\nLong Short-Term Memory (Tree-LSTM) network is then employed to interpret the\nhierarchal structure and enhance all the instance-level, region-level and\nimage-level features. Our HIP is appealing in view that it is pluggable to any\nneural captioning models. Extensive experiments on COCO image captioning\ndataset demonstrate the superiority of HIP. More remarkably, HIP plus a\ntop-down attention-based LSTM decoder increases CIDEr-D performance from 120.1%\nto 127.2% on COCO Karpathy test split. When further endowing instance-level and\nregion-level features from HIP with semantic relation learnt through Graph\nConvolutional Networks (GCN), CIDEr-D is boosted up to 130.6%.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 15:18:21 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 13:39:52 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Yao", "Ting", ""], ["Pan", "Yingwei", ""], ["Li", "Yehao", ""], ["Mei", "Tao", ""]]}, {"id": "1909.03935", "submitter": "Dingfan Chen", "authors": "Dingfan Chen, Ning Yu, Yang Zhang, Mario Fritz", "title": "GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative\n  Models", "comments": "CCS 2020, 20 pages", "journal-ref": "Proceedings of the 2020 ACM SIGSAC Conference on Computer and\n  Communications Security (CCS)", "doi": "10.1145/3372297.3417238", "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved overwhelming success, spanning from discriminative\nmodels to generative models. In particular, deep generative models have\nfacilitated a new level of performance in a myriad of areas, ranging from media\nmanipulation to sanitized dataset generation. Despite the great success, the\npotential risks of privacy breach caused by generative models have not been\nanalyzed systematically. In this paper, we focus on membership inference attack\nagainst deep generative models that reveals information about the training data\nused for victim models. Specifically, we present the first taxonomy of\nmembership inference attacks, encompassing not only existing attacks but also\nour novel ones. In addition, we propose the first generic attack model that can\nbe instantiated in a large range of settings and is applicable to various kinds\nof deep generative models. Moreover, we provide a theoretically grounded attack\ncalibration technique, which consistently boosts the attack performance in all\ncases, across different attack settings, data modalities, and training\nconfigurations. We complement the systematic analysis of attack performance by\na comprehensive experimental study, that investigates the effectiveness of\nvarious attacks w.r.t. model type and training configurations, over three\ndiverse application scenarios (i.e., images, medical data, and location data).\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 15:34:07 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 19:31:24 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 18:11:05 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Chen", "Dingfan", ""], ["Yu", "Ning", ""], ["Zhang", "Yang", ""], ["Fritz", "Mario", ""]]}, {"id": "1909.03943", "submitter": "Matteo Poggi", "authors": "Alessio Tonioni, Matteo Poggi, Stefano Mattoccia, Luigi Di Stefano", "title": "Unsupervised Domain Adaptation for Depth Prediction from Images", "comments": "14 pages, 7 pages. Accepted to TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art approaches to infer dense depth measurements from images\nrely on CNNs trained end-to-end on a vast amount of data. However, these\napproaches suffer a drastic drop in accuracy when dealing with environments\nmuch different in appearance and/or context from those observed at training\ntime. This domain shift issue is usually addressed by fine-tuning on smaller\nsets of images from the target domain annotated with depth labels.\nUnfortunately, relying on such supervised labeling is seldom feasible in most\npractical settings. Therefore, we propose an unsupervised domain adaptation\ntechnique which does not require groundtruth labels. Our method relies only on\nimage pairs and leverages on classical stereo algorithms to produce disparity\nmeasurements alongside with confidence estimators to assess upon their\nreliability. We propose to fine-tune both depth-from-stereo as well as\ndepth-from-mono architectures by a novel confidence-guided loss function that\nhandles the measured disparities as noisy labels weighted according to the\nestimated confidence. Extensive experimental results based on standard datasets\nand evaluation protocols prove that our technique can address effectively the\ndomain shift issue with both stereo and monocular depth prediction\narchitectures and outperforms other state-of-the-art unsupervised loss\nfunctions that may be alternatively deployed to pursue domain adaptation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 15:43:26 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Tonioni", "Alessio", ""], ["Poggi", "Matteo", ""], ["Mattoccia", "Stefano", ""], ["Di Stefano", "Luigi", ""]]}, {"id": "1909.03957", "submitter": "Ikechukwu Onyenwe", "authors": "E Onyedinma, I Onyenwe, H Inyiama", "title": "Performance Evaluation of Histogram Equalization and Fuzzy image\n  Enhancement Techniques on Low Contrast Images", "comments": null, "journal-ref": "International Journal of Computer Science and Software Engineering\n  (IJCSSE), Volume 8, Issue 7, Page: 144-150, July2019. ISSN (Online):\n  2409-4285w w w.IJCSSE.org", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image enhancement aims at improving the information content of original image\nfor a specific purpose. This purpose could be for visual interpretation or for\neffective extraction of required details. Nevertheless, some acquired images\nare often associated with pixels of low dynamic range and as such result in low\ncontrast images. Enhancing the contrast therefore tends to increase the dynamic\nrange of the gray levels in the acquired image so as to span the full intensity\nrange. Techniques such as Histogram Equalization (HE) and fuzzy technique can\nbe adopted for contrast enhancement. HE adjusts the contrast of an input image\nby modifying the intensity distribution of its histogram. It is characterized\nby providing a global approach to image enhancement, computationally fast and\neasy to implement approach but can introduce unnatural artifacts and other\nundesirable elements to the resulting image. Fuzzy technique on its part\nenhances image by mapping the image gray level intensities into a fuzzy plane\nusing membership functions; modifying the membership functions as desired and\nmapping back into the gray level plane. Thus, details at desired areas can be\nenhanced at the expense of increase in computational cost. This paper explores\nthe effect of the use of HE and fuzzy technique to enhance low contrast images.\nTheir performances are evaluated using the Mean squared error (MSE), Peak to\nsignal noise ratio (PSNR), entropy and Absolute mean brightness error (AMBE).\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 08:47:17 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Onyedinma", "E", ""], ["Onyenwe", "I", ""], ["Inyiama", "H", ""]]}, {"id": "1909.04012", "submitter": "Jie Fu", "authors": "Jie Fu, Xinran Zhong, Ning Li, Ritchell Van Dams, John Lewis,\n  Kyunghyun Sung, Ann C. Raldow, Jing Jin, X. Sharon Qi", "title": "Deep Learning-based Radiomic Features for Improving Neoadjuvant\n  Chemoradiation Response Prediction in Locally Advanced Rectal Cancer", "comments": "Review in progress", "journal-ref": "2020 Phys. Med. Biol", "doi": "10.1088/1361-6560/ab7970", "report-no": null, "categories": "physics.med-ph cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiomic features achieve promising results in cancer diagnosis, treatment\nresponse prediction, and survival prediction. Our goal is to compare the\nhandcrafted (explicitly designed) and deep learning (DL)-based radiomic\nfeatures extracted from pre-treatment diffusion-weighted magnetic resonance\nimages (DWIs) for predicting neoadjuvant chemoradiation treatment (nCRT)\nresponse in patients with locally advanced rectal cancer (LARC). 43 patients\nreceiving nCRT were included. All patients underwent DWIs before nCRT and total\nmesorectal excision surgery 6-12 weeks after completion of nCRT. Gross tumor\nvolume (GTV) contours were drawn by an experienced radiation oncologist on\nDWIs. The patient-cohort was split into the responder group (n=22) and the\nnon-responder group (n=21) based on the post-nCRT response assessed by\npostoperative pathology, MRI or colonoscopy. Handcrafted and DL-based features\nwere extracted from the apparent diffusion coefficient (ADC) map of the DWI\nusing conventional computer-aided diagnosis methods and a pre-trained\nconvolution neural network, respectively. Least absolute shrinkage and\nselection operator (LASSO)-logistic regression models were constructed using\nextracted features for predicting treatment response. The model performance was\nevaluated with repeated 20 times stratified 4-fold cross-validation using\nreceiver operating characteristic (ROC) curves and compared using the corrected\nresampled t-test. The model built with handcrafted features achieved the mean\narea under the ROC curve (AUC) of 0.64, while the one built with DL-based\nfeatures yielded the mean AUC of 0.73. The corrected resampled t-test on AUC\nshowed P-value < 0.05. DL-based features extracted from pre-treatment DWIs\nachieved significantly better classification performance compared with\nhandcrafted features for predicting nCRT response in patients with LARC.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 17:48:27 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Fu", "Jie", ""], ["Zhong", "Xinran", ""], ["Li", "Ning", ""], ["Van Dams", "Ritchell", ""], ["Lewis", "John", ""], ["Sung", "Kyunghyun", ""], ["Raldow", "Ann C.", ""], ["Jin", "Jing", ""], ["Qi", "X. Sharon", ""]]}, {"id": "1909.04021", "submitter": "Yosuke Shinya", "authors": "Yosuke Shinya, Edgar Simo-Serra, Taiji Suzuki", "title": "Understanding the Effects of Pre-Training for Object Detectors via\n  Eigenspectrum", "comments": "ICCV 2019 Workshop on Neural Architects (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ImageNet pre-training has been regarded as essential for training accurate\nobject detectors for a long time. Recently, it has been shown that object\ndetectors trained from randomly initialized weights can be on par with those\nfine-tuned from ImageNet pre-trained models. However, the effects of\npre-training and the differences caused by pre-training are still not fully\nunderstood. In this paper, we analyze the eigenspectrum dynamics of the\ncovariance matrix of each feature map in object detectors. Based on our\nanalysis on ResNet-50, Faster R-CNN with FPN, and Mask R-CNN, we show that\nobject detectors trained from ImageNet pre-trained models and those trained\nfrom scratch behave differently from each other even if both object detectors\nhave similar accuracy. Furthermore, we propose a method for automatically\ndetermining the widths (the numbers of channels) of object detectors based on\nthe eigenspectrum. We train Faster R-CNN with FPN from randomly initialized\nweights, and show that our method can reduce ~27% of the parameters of\nResNet-50 without increasing Multiply-Accumulate operations and losing\naccuracy. Our results indicate that we should develop more appropriate methods\nfor transferring knowledge from image classification to object detection (or\nother tasks).\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 17:59:11 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Shinya", "Yosuke", ""], ["Simo-Serra", "Edgar", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1909.04032", "submitter": "Christian Reul", "authors": "Christian Reul, Dennis Christ, Alexander Hartelt, Nico Balbach,\n  Maximilian Wehner, Uwe Springmann, Christoph Wick, Christine Grundig, Andreas\n  B\\\"uttner, Frank Puppe", "title": "OCR4all -- An Open-Source Tool Providing a (Semi-)Automatic OCR Workflow\n  for Historical Printings", "comments": "submitted to MDPI - Applied Sciences", "journal-ref": "https://www.mdpi.com/2076-3417/9/22/4853/htm", "doi": "10.3390/app9224853", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Character Recognition (OCR) on historical printings is a challenging\ntask mainly due to the complexity of the layout and the highly variant\ntypography. Nevertheless, in the last few years great progress has been made in\nthe area of historical OCR, resulting in several powerful open-source tools for\npreprocessing, layout recognition and segmentation, character recognition and\npost-processing. The drawback of these tools often is their limited\napplicability by non-technical users like humanist scholars and in particular\nthe combined use of several tools in a workflow. In this paper we present an\nopen-source OCR software called OCR4all, which combines state-of-the-art OCR\ncomponents and continuous model training into a comprehensive workflow. A\ncomfortable GUI allows error corrections not only in the final output, but\nalready in early stages to minimize error propagations. Further on, extensive\nconfiguration capabilities are provided to set the degree of automation of the\nworkflow and to make adaptations to the carefully selected default parameters\nfor specific printings, if necessary. Experiments showed that users with\nminimal or no experience were able to capture the text of even the earliest\nprinted books with manageable effort and great quality, achieving excellent\ncharacter error rates (CERs) below 0.5%. The fully automated application on\n19th century novels showed that OCR4all can considerably outperform the\ncommercial state-of-the-art tool ABBYY Finereader on moderate layouts if\nsuitably pretrained mixed OCR models are available. The architecture of OCR4all\nallows the easy integration (or substitution) of newly developed tools for its\nmain components by standardized interfaces like PageXML, thus aiming at\ncontinual higher automation for historical printings.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 11:15:40 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Reul", "Christian", ""], ["Christ", "Dennis", ""], ["Hartelt", "Alexander", ""], ["Balbach", "Nico", ""], ["Wehner", "Maximilian", ""], ["Springmann", "Uwe", ""], ["Wick", "Christoph", ""], ["Grundig", "Christine", ""], ["B\u00fcttner", "Andreas", ""], ["Puppe", "Frank", ""]]}, {"id": "1909.04087", "submitter": "Bach Kim", "authors": "Bach Ngoc Kim, Jose Dolz, Pierre-Marc Jodoin and Christian Desrosiers", "title": "Privacy-Net: An Adversarial Approach for Identity-Obfuscated\n  Segmentation of Medical Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a client/server privacy-preserving network in the context\nof multicentric medical image analysis. Our approach is based on adversarial\nlearning which encodes images to obfuscate the patient identity while\npreserving enough information for a target task. Our novel architecture is\ncomposed of three components: 1) an encoder network which removes\nidentity-specific features from input medical images, 2) a discriminator\nnetwork that attempts to identify the subject from the encoded images, 3) a\nmedical image analysis network which analyzes the content of the encoded images\n(segmentation in our case). By simultaneously fooling the discriminator and\noptimizing the medical analysis network, the encoder learns to remove\nprivacy-specific features while keeping those essentials for the target task.\nOur approach is illustrated on the problem of segmenting brain MRI from the\nlarge-scale Parkinson Progression Marker Initiative (PPMI) dataset. Using\nlongitudinal data from PPMI, we show that the discriminator learns to heavily\ndistort input images while allowing for highly accurate segmentation results.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 18:17:10 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 13:08:26 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 19:00:09 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Kim", "Bach Ngoc", ""], ["Dolz", "Jose", ""], ["Jodoin", "Pierre-Marc", ""], ["Desrosiers", "Christian", ""]]}, {"id": "1909.04101", "submitter": "Maxwell Forbes", "authors": "Maxwell Forbes, Christine Kaeser-Chen, Piyush Sharma, Serge Belongie", "title": "Neural Naturalist: Generating Fine-Grained Image Comparisons", "comments": "Published at EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the new Birds-to-Words dataset of 41k sentences describing\nfine-grained differences between photographs of birds. The language collected\nis highly detailed, while remaining understandable to the everyday observer\n(e.g., \"heart-shaped face,\" \"squat body\"). Paragraph-length descriptions\nnaturally adapt to varying levels of taxonomic and visual distance---drawn from\na novel stratified sampling approach---with the appropriate level of detail. We\npropose a new model called Neural Naturalist that uses a joint image encoding\nand comparative module to generate comparative language, and evaluate the\nresults with humans who must use the descriptions to distinguish real images.\n  Our results indicate promising potential for neural models to explain\ndifferences in visual embedding space using natural language, as well as a\nconcrete path for machine learning to aid citizen scientists in their effort to\npreserve biodiversity.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 18:54:40 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 23:03:30 GMT"}, {"version": "v3", "created": "Thu, 14 Nov 2019 01:19:36 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Forbes", "Maxwell", ""], ["Kaeser-Chen", "Christine", ""], ["Sharma", "Piyush", ""], ["Belongie", "Serge", ""]]}, {"id": "1909.04104", "submitter": "Zengming Shen", "authors": "Zengming Shen, Yifan Chen, S.Kevin Zhou, Bogdan Georgescu, Xuqi Liu,\n  Thomas S. Huang", "title": "Towards Learning a Self-inverse Network for Bidirectional Image-to-image\n  Translation", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The one-to-one mapping is necessary for many bidirectional image-to-image\ntranslation applications, such as MRI image synthesis as MRI images are unique\nto the patient. State-of-the-art approaches for image synthesis from domain X\nto domain Y learn a convolutional neural network that meticulously maps between\nthe domains. A different network is typically implemented to map along the\nopposite direction, from Y to X. In this paper, we explore the possibility of\nonly wielding one network for bi-directional image synthesis. In other words,\nsuch an autonomous learning network implements a self-inverse function. A\nself-inverse network shares several distinct advantages: only one network\ninstead of two, better generalization and more restricted parameter space. Most\nimportantly, a self-inverse function guarantees a one-to-one mapping, a\nproperty that cannot be guaranteed by earlier approaches that are not\nself-inverse. The experiments on three datasets show that, compared with the\nbaseline approaches that use two separate models for the image synthesis along\ntwo directions, our self-inverse network achieves better synthesis results in\nterms of standard metrics. Finally, our sensitivity analysis confirms the\nfeasibility of learning a self-inverse function for the bidirectional image\ntranslation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 18:56:30 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 20:46:02 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Shen", "Zengming", ""], ["Chen", "Yifan", ""], ["Zhou", "S. Kevin", ""], ["Georgescu", "Bogdan", ""], ["Liu", "Xuqi", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1909.04108", "submitter": "Kaiyang Cheng", "authors": "Kaiyang Cheng, Claudia Iriondo, Francesco Caliv\\'a, Justin Krogue,\n  Sharmila Majumdar, Valentina Pedoia", "title": "Adversarial Policy Gradient for Deep Learning Image Augmentation", "comments": "9 pages, 2 figures, MICCAI 2019, First two authors contributed\n  equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of semantic segmentation for masking and cropping input images has\nproven to be a significant aid in medical imaging classification tasks by\ndecreasing the noise and variance of the training dataset. However,\nimplementing this approach with classical methods is challenging: the cost of\nobtaining a dense segmentation is high, and the precise input area that is most\ncrucial to the classification task is difficult to determine a-priori. We\npropose a novel joint-training deep reinforcement learning framework for image\naugmentation. A segmentation network, weakly supervised with policy gradient\noptimization, acts as an agent, and outputs masks as actions given samples as\nstates, with the goal of maximizing reward signals from the classification\nnetwork. In this way, the segmentation network learns to mask unimportant\nimaging features. Our method, Adversarial Policy Gradient Augmentation (APGA),\nshows promising results on Stanford's MURA dataset and on a hip fracture\nclassification task with an increase in global accuracy of up to 7.33% and\nimproved performance over baseline methods in 9/10 tasks evaluated. We discuss\nthe broad applicability of our joint training strategy to a variety of medical\nimaging tasks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 19:04:21 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Cheng", "Kaiyang", ""], ["Iriondo", "Claudia", ""], ["Caliv\u00e1", "Francesco", ""], ["Krogue", "Justin", ""], ["Majumdar", "Sharmila", ""], ["Pedoia", "Valentina", ""]]}, {"id": "1909.04110", "submitter": "Zengming Shen", "authors": "Zengming Shen, S.Kevin Zhou, Yifan Chen, Bogdan Georgescu, Xuqi Liu,\n  Thomas S. Huang", "title": "One-to-one Mapping for Unpaired Image-to-image Translation", "comments": "Accepted by WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently image-to-image translation has attracted significant interests in\nthe literature, starting from the successful use of the generative adversarial\nnetwork (GAN), to the introduction of cyclic constraint, to extensions to\nmultiple domains. However, in existing approaches, there is no guarantee that\nthe mapping between two image domains is unique or one-to-one. Here we propose\na self-inverse network learning approach for unpaired image-to-image\ntranslation. Building on top of CycleGAN, we learn a self-inverse function by\nsimply augmenting the training samples by swapping inputs and outputs during\ntraining and with separated cycle consistency loss for each mapping direction.\nThe outcome of such learning is a proven one-to-one mapping function. Our\nextensive experiments on a variety of datasets, including cross-modal medical\nimage synthesis, object transfiguration, and semantic labeling, consistently\ndemonstrate clear improvement over the CycleGAN method both qualitatively and\nquantitatively. Especially our proposed method reaches the state-of-the-art\nresult on the cityscapes benchmark dataset for the label to photo unpaired\ndirectional image translation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 19:10:05 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 15:41:37 GMT"}, {"version": "v3", "created": "Fri, 13 Sep 2019 14:26:52 GMT"}, {"version": "v4", "created": "Mon, 16 Sep 2019 20:52:09 GMT"}, {"version": "v5", "created": "Sat, 12 Oct 2019 07:35:28 GMT"}, {"version": "v6", "created": "Wed, 15 Jan 2020 03:13:18 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Shen", "Zengming", ""], ["Zhou", "S. Kevin", ""], ["Chen", "Yifan", ""], ["Georgescu", "Bogdan", ""], ["Liu", "Xuqi", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1909.04126", "submitter": "Ang Li", "authors": "Ang Li, Jiayi Guo, Huanrui Yang, Flora D. Salim, Yiran Chen", "title": "DeepObfuscator: Obfuscating Intermediate Representations with\n  Privacy-Preserving Adversarial Learning on Smartphones", "comments": "This paper is to be published in IoTDI'21", "journal-ref": null, "doi": "10.1145/3450268.3453519", "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been widely applied in many computer vision applications,\nwith remarkable success. However, running deep learning models on mobile\ndevices is generally challenging due to the limitation of computing resources.\nA popular alternative is to use cloud services to run deep learning models to\nprocess raw data. This, however, imposes privacy risks. Some prior arts\nproposed sending the features extracted from raw data to the cloud.\nUnfortunately, these extracted features can still be exploited by attackers to\nrecover raw images and to infer embedded private attributes. In this paper, we\npropose an adversarial training framework, DeepObfuscator, which prevents the\nusage of the features for reconstruction of the raw images and inference of\nprivate attributes. This is done while retaining useful information for the\nintended cloud service. DeepObfuscator includes a learnable obfuscator that is\ndesigned to hide privacy-related sensitive information from the features by\nperforming our proposed adversarial training algorithm. The proposed algorithm\nis designed by simulating the game between an attacker who makes efforts to\nreconstruct raw image and infer private attributes from the extracted features\nand a defender who aims to protect user privacy. By deploying the trained\nobfuscator on the smartphone, features can be locally extracted and then sent\nto the cloud. Our experiments on CelebA and LFW datasets show that the quality\nof the reconstructed images from the obfuscated features of the raw image is\ndramatically decreased from 0.9458 to 0.3175 in terms of multi-scale structural\nsimilarity. The person in the reconstructed image, hence, becomes hardly to be\nre-identified. The classification accuracy of the inferred private attributes\nthat can be achieved by the attacker is significantly reduced to a\nrandom-guessing level.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 19:57:01 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 02:46:26 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Li", "Ang", ""], ["Guo", "Jiayi", ""], ["Yang", "Huanrui", ""], ["Salim", "Flora D.", ""], ["Chen", "Yiran", ""]]}, {"id": "1909.04138", "submitter": "Jiang Lu", "authors": "Jiang Lu, Lei Li, Changshui Zhang", "title": "Self-reinforcing Unsupervised Matching", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remarkable gains in deep learning usually rely on tremendous supervised data.\nEnsuring the modality diversity for one object in training set is critical for\nthe generalization of cutting-edge deep models, but it burdens human with heavy\nmanual labor on data collection and annotation. In addition, some rare or\nunexpected modalities are new for the current model, causing reduced\nperformance under such emerging modalities. Inspired by the achievements in\nspeech recognition, psychology and behavioristics, we present a practical\nsolution, self-reinforcing unsupervised matching (SUM), to annotate the images\nwith 2D structure-preserving property in an emerging modality by cross-modality\nmatching. This approach requires no any supervision in emerging modality and\nonly one template in seen modality, providing a possible route towards\ncontinual learning.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 10:43:43 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Lu", "Jiang", ""], ["Li", "Lei", ""], ["Zhang", "Changshui", ""]]}, {"id": "1909.04141", "submitter": "Lin Xu", "authors": "Lin Xu, Cheng Xu, Yi Tong, Yu Chun Su", "title": "Detection and Classification of Breast Cancer Metastates Based on U-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents U-net based breast cancer metastases detection and\nclassification in lymph nodes, as well as patient-level classification based on\nmetastases detection. The whole pipeline can be divided into five steps:\npreprocessing and data argumentation, patch-based segmentation, post\nprocessing, slide-level classification, and patient-level classification. In\norder to reduce overfitting and speedup convergence, we applied batch\nnormalization and dropout into U-Net. The final Kappa score reaches 0.902 on\ntraining data.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 20:34:32 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Xu", "Lin", ""], ["Xu", "Cheng", ""], ["Tong", "Yi", ""], ["Su", "Yu Chun", ""]]}, {"id": "1909.04142", "submitter": "Lin Xu", "authors": "Justin Quan, Lin Xu, Rene Xu, Tyrael Tong, and Jean Su", "title": "DaTscan SPECT Image Classification for Parkinson's Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parkinson's Disease (PD) is a neurodegenerative disease that currently does\nnot have a cure. In order to facilitate disease management and reduce the speed\nof symptom progression, early diagnosis is essential. The current clinical,\ndiagnostic approach is to have radiologists perform human visual analysis of\nthe degeneration of dopaminergic neurons in the substantia nigra region of the\nbrain. Clinically, dopamine levels are monitored through observing dopamine\ntransporter (DaT) activity. One method of DaT activity analysis is performed\nwith the injection of an Iodine-123 fluoropropyl (123I-FP-CIT) tracer combined\nwith single photon emission computerized tomography (SPECT) imaging. The tracer\nillustrates the region of interest in the resulting DaTscan SPECT images. Human\nvisual analysis is slow and vulnerable to subjectivity between radiologists, so\nthe goal was to develop an introductory implementation of a deep convolutional\nneural network that can objectively and accurately classify DaTscan SPECT\nimages as Parkinson's Disease or normal. This study illustrates the approach of\nusing a deep convolutional neural network and evaluates its performance on\nDaTscan SPECT image classification. The data used in this study was obtained\nthrough a database provided by the Parkinson's Progression Markers Initiative\n(PPMI). The deep neural network in this study utilizes the InceptionV3\narchitecture, 1st runner up in the 2015 ImageNet Large Scale Visual Recognition\nCompetition (ILSVRC), as a base model. A custom, binary classifier block was\nadded on top of this base. In order to account for the small dataset size, a\nten fold cross validation was implemented to evaluate the model's performance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 20:35:23 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Quan", "Justin", ""], ["Xu", "Lin", ""], ["Xu", "Rene", ""], ["Tong", "Tyrael", ""], ["Su", "Jean", ""]]}, {"id": "1909.04147", "submitter": "Steven Le Moan", "authors": "Steven Le Moan, Marius Pedersen", "title": "A Three-Feature Model to Predict Colour Change Blindness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change blindness is a striking shortcoming of our visual system which is\nexploited in the popular \"Spot the difference\" game. It makes us unable to\nnotice large visual changes happening right before our eyes and illustrates the\nfact that we see much less than we think we do. We introduce a fully automated\nmodel to predict colour change blindness in cartoon images based on two\nlow-level image features and observer experience. Using linear regression with\nonly three parameters, the predictions of the proposed model correlate\nsignificantly with measured detection times. We also demonstrate the efficacy\nof the model to classify stimuli in terms of difficulty.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 23:20:18 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Moan", "Steven Le", ""], ["Pedersen", "Marius", ""]]}, {"id": "1909.04148", "submitter": "Yanhao Zhu", "authors": "Yanhao Zhu and Zhineng Chen and Shuai Zhao and Hongtao Xie and Wenming\n  Guo and Yongdong Zhang", "title": "ACE-Net: Biomedical Image Segmentation with Augmented Contracting and\n  Expansive Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays U-net-like FCNs predominate various biomedical image segmentation\napplications and attain promising performance, largely due to their elegant\narchitectures, e.g., symmetric contracting and expansive paths as well as\nlateral skip-connections. It remains a research direction to devise novel\narchitectures to further benefit the segmentation. In this paper, we develop an\nACE-net that aims to enhance the feature representation and utilization by\naugmenting the contracting and expansive paths. In particular, we augment the\npaths by the recently proposed advanced techniques including ASPP, dense\nconnection and deep supervision mechanisms, and novel connections such as\ndirectly connecting the raw image to the expansive side. With these\naugmentations, ACE-net can utilize features from multiple sources, scales and\nreception fields to segment while still maintains a relative simple\narchitecture. Experiments on two typical biomedical segmentation tasks validate\nits effectiveness, where highly competitive results are obtained in both tasks\nwhile ACE-net still runs fast at inference.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 07:03:48 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Zhu", "Yanhao", ""], ["Chen", "Zhineng", ""], ["Zhao", "Shuai", ""], ["Xie", "Hongtao", ""], ["Guo", "Wenming", ""], ["Zhang", "Yongdong", ""]]}, {"id": "1909.04150", "submitter": "Muhammad Siraj", "authors": "Muhammad Siraj", "title": "Machine and Deep Learning for Crowd Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high population cities, the gatherings of large crowds in public places\nand public areas accelerate or jeopardize people safety and transportation,\nwhich is a key challenge to the researchers. Although much research has been\ncarried out on crowd analytics, many of existing methods are problem-specific,\ni.e., methods learned from a specific scene cannot be properly adopted to other\nvideos. Therefore, this presents weakness and the discovery of these\nresearches, since additional training samples have to be found from diverse\nvideos. This paper will investigate diverse scene crowd analytics with\ntraditional and deep learning models. We will also consider pros and cons of\nthese approaches. However, once general deep methods are investigated from\nlarge datasets, they can be consider to investigate different crowd videos and\nimages. Therefore, it would be able to cope with the problem including to not\nlimited to crowd density estimation, crowd people counting, and crowd event\nrecognition. Deep learning models and approaches are required to have large\ndatasets for training and testing. Many datasets are collected taking into\naccount many different and various problems related to building crowd datasets,\nincluding manual annotations and increasing diversity of videos and images. In\nthis paper, we will also propose many models of deep neural networks and\ntraining approaches to learn the feature modeling for crowd analytics.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 23:43:39 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Siraj", "Muhammad", ""]]}, {"id": "1909.04161", "submitter": "Yu Zeng", "authors": "Yu Zeng, Yunzhi Zhuge, Huchuan Lu, Lihe Zhang", "title": "Joint Learning of Saliency Detection and Weakly Supervised Semantic\n  Segmentation", "comments": "Accepted by ICCV19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing weakly supervised semantic segmentation (WSSS) methods usually\nutilize the results of pre-trained saliency detection (SD) models without\nexplicitly modeling the connections between the two tasks, which is not the\nmost efficient configuration. Here we propose a unified multi-task learning\nframework to jointly solve WSSS and SD using a single network, \\ie saliency,\nand segmentation network (SSNet). SSNet consists of a segmentation network (SN)\nand a saliency aggregation module (SAM). For an input image, SN generates the\nsegmentation result and, SAM predicts the saliency of each category and\naggregating the segmentation masks of all categories into a saliency map. The\nproposed network is trained end-to-end with image-level category labels and\nclass-agnostic pixel-level saliency labels. Experiments on PASCAL VOC 2012\nsegmentation dataset and four saliency benchmark datasets show the performance\nof our method compares favorably against state-of-the-art weakly supervised\nsegmentation methods and fully supervised saliency detection methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 21:17:10 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Zeng", "Yu", ""], ["Zhuge", "Yunzhi", ""], ["Lu", "Huchuan", ""], ["Zhang", "Lihe", ""]]}, {"id": "1909.04163", "submitter": "JIan Deng", "authors": "Jian Deng and Krzysztof Czarnecki", "title": "MLOD: A multi-view 3D object detection based on robust feature fusion\n  method", "comments": "6 pages, 6 figures, 2019 22st International Conference on Intelligent\n  Transportation Systems (ITSC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Multi-view Labelling Object Detector (MLOD). The detector\ntakes an RGB image and a LIDAR point cloud as input and follows the two-stage\nobject detection framework. A Region Proposal Network (RPN) generates 3D\nproposals in a Bird's Eye View (BEV) projection of the point cloud. The second\nstage projects the 3D proposal bounding boxes to the image and BEV feature maps\nand sends the corresponding map crops to a detection header for classification\nand bounding-box regression. Unlike other multi-view based methods, the cropped\nimage features are not directly fed to the detection header, but masked by the\ndepth information to filter out parts outside 3D bounding boxes. The fusion of\nimage and BEV features is challenging, as they are derived from different\nperspectives. We introduce a novel detection header, which provides detection\nresults not just from fusion layer, but also from each sensor channel. Hence\nthe object detector can be trained on data labelled in different views to avoid\nthe degeneration of feature extractors. MLOD achieves state-of-the-art\nperformance on the KITTI 3D object detection benchmark. Most importantly, the\nevaluation shows that the new header architecture is effective in preventing\nimage feature extractor degeneration.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 21:18:41 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Deng", "Jian", ""], ["Czarnecki", "Krzysztof", ""]]}, {"id": "1909.04182", "submitter": "Yi Fang", "authors": "Jing Zhu, Yi Fang, Husam Abu-Haimed, Kuo-Chin Lien, Dongdong Fu and\n  Junli Gu", "title": "Learning Object-specific Distance from a Monocular Image", "comments": "10 pages, 6 figures, accepted by International Conference on Computer\n  Vision (ICCV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environment perception, including object detection and distance estimation,\nis one of the most crucial tasks for autonomous driving. Many attentions have\nbeen paid on the object detection task, but distance estimation only arouse few\ninterests in the computer vision community. Observing that the traditional\ninverse perspective mapping algorithm performs poorly for objects far away from\nthe camera or on the curved road, in this paper, we address the challenging\ndistance estimation problem by developing the first end-to-end learning-based\nmodel to directly predict distances for given objects in the images. Besides\nthe introduction of a learning-based base model, we further design an enhanced\nmodel with a keypoint regressor, where a projection loss is defined to enforce\na better distance estimation, especially for objects close to the camera. To\nfacilitate the research on this task, we construct the extented KITTI and\nnuScenes (mini) object detection datasets with a distance for each object. Our\nexperiments demonstrate that our proposed methods outperform alternative\napproaches (e.g., the traditional IPM, SVR) on object-specific distance\nestimation, particularly for the challenging cases that objects are on a curved\nroad. Moreover, the performance margin implies the effectiveness of our\nenhanced method.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 22:13:51 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Zhu", "Jing", ""], ["Fang", "Yi", ""], ["Abu-Haimed", "Husam", ""], ["Lien", "Kuo-Chin", ""], ["Fu", "Dongdong", ""], ["Gu", "Junli", ""]]}, {"id": "1909.04207", "submitter": "Rajeev Yasarla", "authors": "Rajeev Yasarla, Vishal M. Patel", "title": "Confidence Measure Guided Single Image De-raining", "comments": "TIP2019 submission. arXiv admin note: substantial text overlap with\n  arXiv:1906.11129", "journal-ref": null, "doi": "10.1109/TIP.2020.2973802", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image de-raining is an extremely challenging problem since the rainy\nimages contain rain streaks which often vary in size, direction and density.\nThis varying characteristic of rain streaks affect different parts of the image\ndifferently. Previous approaches have attempted to address this problem by\nleveraging some prior information to remove rain streaks from a single image.\nOne of the major limitations of these approaches is that they do not consider\nthe location information of rain drops in the image. The proposed Image\nQuality-based single image Deraining using Confidence measure (QuDeC), network\naddresses this issue by learning the quality or distortion level of each patch\nin the rainy image, and further processes this information to learn the rain\ncontent at different scales. In addition, we introduce a technique which guides\nthe network to learn the network weights based on the confidence measure about\nthe estimate of both quality at each location and residual rain streak\ninformation (residual map). Extensive experiments on synthetic and real\ndatasets demonstrate that the proposed method achieves significant improvements\nover the recent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 00:42:51 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Yasarla", "Rajeev", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1909.04217", "submitter": "Xinyi Ding", "authors": "Xinyi Ding, Zohreh Raziei, Eric C. Larson, Eli V. Olinick, Paul\n  Krueger, Michael Hahsler", "title": "Swapped Face Detection using Deep Learning and Subjective Assessment", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tremendous success of deep learning for imaging applications has resulted\nin numerous beneficial advances. Unfortunately, this success has also been a\ncatalyst for malicious uses such as photo-realistic face swapping of parties\nwithout consent. Transferring one person's face from a source image to a target\nimage of another person, while keeping the image photo-realistic overall has\nbecome increasingly easy and automatic, even for individuals without much\nknowledge of image processing. In this study, we use deep transfer learning for\nface swapping detection, showing true positive rates >96% with very few false\nalarms. Distinguished from existing methods that only provide detection\naccuracy, we also provide uncertainty for each prediction, which is critical\nfor trust in the deployment of such detection systems. Moreover, we provide a\ncomparison to human subjects. To capture human recognition performance, we\nbuild a website to collect pairwise comparisons of images from human subjects.\nBased on these comparisons, images are ranked from most real to most fake. We\ncompare this ranking to the outputs from our automatic model, showing good, but\nimperfect, correspondence with linear correlations >0.75. Overall, the results\nshow the effectiveness of our method. As part of this study, we create a novel,\npublicly available dataset that is, to the best of our knowledge, the largest\npublic swapped face dataset created using still images. Our goal of this study\nis to inspire more research in the field of image forensics through the\ncreation of a public dataset and initial analysis.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 01:06:43 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Ding", "Xinyi", ""], ["Raziei", "Zohreh", ""], ["Larson", "Eric C.", ""], ["Olinick", "Eli V.", ""], ["Krueger", "Paul", ""], ["Hahsler", "Michael", ""]]}, {"id": "1909.04247", "submitter": "Zihao Li", "authors": "Zihao Li, Shu Zhang, Junge Zhang, Kaiqi Huang, Yizhou Wang, Yizhou Yu", "title": "MVP-Net: Multi-view FPN with Position-aware Attention for Deep Universal\n  Lesion Detection", "comments": "Accepted by MICCAI 2019 (Medical Image Computing and Computer\n  Assisted Intervention)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal lesion detection (ULD) on computed tomography (CT) images is an\nimportant but underdeveloped problem. Recently, deep learning-based approaches\nhave been proposed for ULD, aiming to learn representative features from\nannotated CT data. However, the hunger for data of deep learning models and the\nscarcity of medical annotation hinders these approaches to advance further. In\nthis paper, we propose to incorporate domain knowledge in clinical practice\ninto the model design of universal lesion detectors. Specifically, as\nradiologists tend to inspect multiple windows for an accurate diagnosis, we\nexplicitly model this process and propose a multi-view feature pyramid network\n(FPN), where multi-view features are extracted from images rendered with varied\nwindow widths and window levels; to effectively combine this multi-view\ninformation, we further propose a position-aware attention module. With the\nproposed model design, the data-hunger problem is relieved as the learning task\nis made easier with the correctly induced clinical practice prior. We show\npromising results with the proposed model, achieving an absolute gain of\n$\\mathbf{5.65\\%}$ (in the sensitivity of FPs@4.0) over the previous\nstate-of-the-art on the NIH DeepLesion dataset.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 02:48:20 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 03:50:44 GMT"}, {"version": "v3", "created": "Mon, 16 Dec 2019 12:14:12 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Li", "Zihao", ""], ["Zhang", "Shu", ""], ["Zhang", "Junge", ""], ["Huang", "Kaiqi", ""], ["Wang", "Yizhou", ""], ["Yu", "Yizhou", ""]]}, {"id": "1909.04250", "submitter": "Kaixuan Wang", "authors": "Kaixuan Wang and Fei Gao and Shaojie Shen", "title": "Real-time Scalable Dense Surfel Mapping", "comments": "This is a ICRA 2019 paper. Source code available at\n  https://github.com/HKUST-Aerial-Robotics/DenseSurfelMapping", "journal-ref": "ICRA 2019", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel dense surfel mapping system that scales\nwell in different environments with only CPU computation. Using a sparse SLAM\nsystem to estimate camera poses, the proposed mapping system can fuse intensity\nimages and depth images into a globally consistent model. The system is\ncarefully designed so that it can build from room-scale environments to\nurban-scale environments using depth images from RGB-D cameras, stereo cameras\nor even a monocular camera. First, superpixels extracted from both intensity\nand depth images are used to model surfels in the system. superpixel-based\nsurfels make our method both run-time efficient and memory efficient. Second,\nsurfels are further organized according to the pose graph of the SLAM system to\nachieve $O(1)$ fusion time regardless of the scale of reconstructed models.\nThird, a fast map deformation using the optimized pose graph enables the map to\nachieve global consistency in real-time. The proposed surfel mapping system is\ncompared with other state-of-the-art methods on synthetic datasets. The\nperformances of urban-scale and room-scale reconstruction are demonstrated\nusing the KITTI dataset and autonomous aggressive flights, respectively. The\ncode is available for the benefit of the community.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 02:56:40 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Wang", "Kaixuan", ""], ["Gao", "Fei", ""], ["Shen", "Shaojie", ""]]}, {"id": "1909.04269", "submitter": "Zheming Zhou", "authors": "Zheming Zhou, Tianyang Pan, Shiyu Wu, Haonan Chang, Odest Chadwicke\n  Jenkins", "title": "GlassLoc: Plenoptic Grasp Pose Detection in Transparent Clutter", "comments": "Accepted to the 2019 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems. Contact: Zheming Zhou, zhezhou@umich.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transparent objects are prevalent across many environments of interest for\ndexterous robotic manipulation. Such transparent material leads to considerable\nuncertainty for robot perception and manipulation, and remains an open\nchallenge for robotics. This problem is exacerbated when multiple transparent\nobjects cluster into piles of clutter. In household environments, for example,\nit is common to encounter piles of glassware in kitchens, dining rooms, and\nreception areas, which are essentially invisible to modern robots. We present\nthe GlassLoc algorithm for grasp pose detection of transparent objects in\ntransparent clutter using plenoptic sensing. GlassLoc classifies graspable\nlocations in space informed by a Depth Likelihood Volume (DLV) descriptor. We\nextend the DLV to infer the occupancy of transparent objects over a given space\nfrom multiple plenoptic viewpoints. We demonstrate and evaluate the GlassLoc\nalgorithm on a Michigan Progress Fetch mounted with a first-generation Lytro.\nThe effectiveness of our algorithm is evaluated through experiments for grasp\ndetection and execution with a variety of transparent glassware in minor\nclutter.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 03:53:15 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 18:25:19 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Zhou", "Zheming", ""], ["Pan", "Tianyang", ""], ["Wu", "Shiyu", ""], ["Chang", "Haonan", ""], ["Jenkins", "Odest Chadwicke", ""]]}, {"id": "1909.04286", "submitter": "Soma Minami", "authors": "Soma Minami, Tsubasa Hirakawa, Takayoshi Yamashita, Hironobu Fujiyoshi", "title": "Knowledge Transfer Graph for Deep Collaborative Learning", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge transfer among multiple networks using their outputs or\nintermediate activations have evolved through extensive manual design from a\nsimple teacher-student approach (knowledge distillation) to a bidirectional\ncohort one (deep mutual learning). The key factors of such knowledge transfer\ninvolve the network size, the number of networks, the transfer direction, and\nthe design of the loss function. However, because these factors are enormous\nwhen combined and become intricately entangled, the methods of conventional\nknowledge transfer have explored only limited combinations. In this paper, we\npropose a new graph-based approach for more flexible and diverse combinations\nof knowledge transfer. To achieve the knowledge transfer, we propose a novel\ngraph representation called knowledge transfer graph that provides a unified\nview of the knowledge transfer and has the potential to represent diverse\nknowledge transfer patterns. We also propose four gate functions that are\nintroduced into loss functions. The four gates, which control the gradient, can\ndeliver diverse combinations of knowledge transfer. Searching the graph\nstructure enables us to discover more effective knowledge transfer methods than\na manually designed one. Experimental results on the CIFAR-10, -100, and\nTiny-ImageNet datasets show that the proposed method achieved significant\nperformance improvements and was able to find remarkable graph structures.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 04:56:29 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 03:46:16 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Minami", "Soma", ""], ["Hirakawa", "Tsubasa", ""], ["Yamashita", "Takayoshi", ""], ["Fujiyoshi", "Hironobu", ""]]}, {"id": "1909.04306", "submitter": "Yi Wu", "authors": "Yi Wu, Yuxin Wu, Aviv Tamar, Stuart Russell, Georgia Gkioxari,\n  Yuandong Tian", "title": "Bayesian Relational Memory for Semantic Visual Navigation", "comments": "Accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new memory architecture, Bayesian Relational Memory (BRM), to\nimprove the generalization ability for semantic visual navigation agents in\nunseen environments, where an agent is given a semantic target to navigate\ntowards. BRM takes the form of a probabilistic relation graph over semantic\nentities (e.g., room types), which allows (1) capturing the layout prior from\ntraining environments, i.e., prior knowledge, (2) estimating posterior layout\nat test time, i.e., memory update, and (3) efficient planning for navigation,\naltogether. We develop a BRM agent consisting of a BRM module for producing\nsub-goals and a goal-conditioned locomotion module for control. When testing in\nunseen environments, the BRM agent outperforms baselines that do not explicitly\nutilize the probabilistic relational memory structure\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 06:02:15 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Wu", "Yi", ""], ["Wu", "Yuxin", ""], ["Tamar", "Aviv", ""], ["Russell", "Stuart", ""], ["Gkioxari", "Georgia", ""], ["Tian", "Yuandong", ""]]}, {"id": "1909.04312", "submitter": "Wei Zhang", "authors": "Shuo Yang, Wei Zhang, Weizhi Lu, Hesheng Wang, and Yibin Li", "title": "Learning Actions from Human Demonstration Video for Robotic Manipulation", "comments": "Accepted by IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning actions from human demonstration is an emerging trend for designing\nintelligent robotic systems, which can be referred as video to command. The\nperformance of such approach highly relies on the quality of video captioning.\nHowever, the general video captioning methods focus more on the understanding\nof the full frame, lacking of consideration on the specific object of interests\nin robotic manipulations. We propose a novel deep model to learn actions from\nhuman demonstration video for robotic manipulation. It consists of two deep\nnetworks, grasp detection network (GNet) and video captioning network (CNet).\nGNet performs two functions: providing grasp solutions and extracting the local\nfeatures for the object of interests in robotic manipulation. CNet outputs the\ncaptioning results by fusing the features of both full frames and local\nobjects. Experimental results on UR5 robotic arm show that our method could\nproduce more accurate command from video demonstration than state-of-the-art\nwork, thereby leading to more robust grasping performance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 06:20:46 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Yang", "Shuo", ""], ["Zhang", "Wei", ""], ["Lu", "Weizhi", ""], ["Wang", "Hesheng", ""], ["Li", "Yibin", ""]]}, {"id": "1909.04324", "submitter": "Xianglei Xing", "authors": "Xianglei Xing, Tianfu Wu, Song-Chun Zhu, Ying Nian Wu", "title": "Inducing Hierarchical Compositional Model by Sparsifying Generator\n  Network", "comments": "This is the CVPR version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to learn hierarchical compositional AND-OR model for\ninterpretable image synthesis by sparsifying the generator network. The\nproposed method adopts the scene-objects-parts-subparts-primitives hierarchy in\nimage representation. A scene has different types (i.e., OR) each of which\nconsists of a number of objects (i.e., AND). This can be recursively formulated\nacross the scene-objects-parts-subparts hierarchy and is terminated at the\nprimitive level (e.g., wavelets-like basis). To realize this AND-OR hierarchy\nin image synthesis, we learn a generator network that consists of the following\ntwo components: (i) Each layer of the hierarchy is represented by an\nover-complete set of convolutional basis functions. Off-the-shelf convolutional\nneural architectures are exploited to implement the hierarchy. (ii)\nSparsity-inducing constraints are introduced in end-to-end training, which\ninduces a sparsely activated and sparsely connected AND-OR model from the\ninitially densely connected generator network. A straightforward\nsparsity-inducing constraint is utilized, that is to only allow the top-$k$\nbasis functions to be activated at each layer (where $k$ is a hyper-parameter).\nThe learned basis functions are also capable of image reconstruction to explain\nthe input images. In experiments, the proposed method is tested on four\nbenchmark datasets. The results show that meaningful and interpretable\nhierarchical representations are learned with better qualities of image\nsynthesis and reconstruction obtained than baselines.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 07:06:33 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 05:02:00 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Xing", "Xianglei", ""], ["Wu", "Tianfu", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1909.04326", "submitter": "Cihang Xie", "authors": "Lifeng Huang, Chengying Gao, Yuyin Zhou, Cihang Xie, Alan Yuille,\n  Changqing Zou, Ning Liu", "title": "Universal Physical Camouflage Attacks on Object Detectors", "comments": "CVPR 2020; codes, models, and demos are available at\n  https://mesunhlf.github.io/index_physical.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study physical adversarial attacks on object detectors in\nthe wild. Previous works mostly craft instance-dependent perturbations only for\nrigid or planar objects. To this end, we propose to learn an adversarial\npattern to effectively attack all instances belonging to the same object\ncategory, referred to as Universal Physical Camouflage Attack (UPC).\nConcretely, UPC crafts camouflage by jointly fooling the region proposal\nnetwork, as well as misleading the classifier and the regressor to output\nerrors. In order to make UPC effective for non-rigid or non-planar objects, we\nintroduce a set of transformations for mimicking deformable properties. We\nadditionally impose optimization constraint to make generated patterns look\nnatural to human observers. To fairly evaluate the effectiveness of different\nphysical-world attacks, we present the first standardized virtual database,\nAttackScenes, which simulates the real 3D world in a controllable and\nreproducible environment. Extensive experiments suggest the superiority of our\nproposed UPC compared with existing physical adversarial attackers not only in\nvirtual environments (AttackScenes), but also in real-world physical\nenvironments. Code and dataset are available at\nhttps://mesunhlf.github.io/index_physical.html.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 07:16:32 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 22:27:51 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Huang", "Lifeng", ""], ["Gao", "Chengying", ""], ["Zhou", "Yuyin", ""], ["Xie", "Cihang", ""], ["Yuille", "Alan", ""], ["Zou", "Changqing", ""], ["Liu", "Ning", ""]]}, {"id": "1909.04332", "submitter": "Ziyang Wu", "authors": "Ziyang Wu, Yuwei Li, Lihua Guo and Kui Jia", "title": "PARN: Position-Aware Relation Networks for Few-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning presents a challenge that a classifier must quickly adapt\nto new classes that do not appear in the training set, given only a few labeled\nexamples of each new class. This paper proposes a position-aware relation\nnetwork (PARN) to learn a more flexible and robust metric ability for few-shot\nlearning. Relation networks (RNs), a kind of architectures for relational\nreasoning, can acquire a deep metric ability for images by just being designed\nas a simple convolutional neural network (CNN) [23]. However, due to the\ninherent local connectivity of CNN, the CNN-based relation network (RN) can be\nsensitive to the spatial position relationship of semantic objects in two\ncompared images. To address this problem, we introduce a deformable feature\nextractor (DFE) to extract more efficient features, and design a dual\ncorrelation attention mechanism (DCA) to deal with its inherent local\nconnectivity. Successfully, our proposed approach extents the potential of RN\nto be position-aware of semantic objects by introducing only a small number of\nparameters. We evaluate our approach on two major benchmark datasets, i.e.,\nOmniglot and Mini-Imagenet, and on both of the datasets our approach achieves\nstate-of-the-art performance with the setting of using a shallow feature\nextraction network. It's worth noting that our 5-way 1-shot result on Omniglot\neven outperforms the previous 5-way 5-shot results.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 07:39:32 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Wu", "Ziyang", ""], ["Li", "Yuwei", ""], ["Guo", "Lihua", ""], ["Jia", "Kui", ""]]}, {"id": "1909.04344", "submitter": "Vinay Verma Kumar", "authors": "Vinay Kumar Verma, Dhanajit Brahma and Piyush Rai", "title": "A Meta-Learning Framework for Generalized Zero-Shot Learning", "comments": "Under Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to classify unseen class samples at test time is popularly referred\nto as zero-shot learning (ZSL). If test samples can be from training (seen) as\nwell as unseen classes, it is a more challenging problem due to the existence\nof strong bias towards seen classes. This problem is generally known as\n\\emph{generalized} zero-shot learning (GZSL). Thanks to the recent advances in\ngenerative models such as VAEs and GANs, sample synthesis based approaches have\ngained considerable attention for solving this problem. These approaches are\nable to handle the problem of class bias by synthesizing unseen class samples.\nHowever, these ZSL/GZSL models suffer due to the following key limitations:\n$(i)$ Their training stage learns a class-conditioned generator using only\n\\emph{seen} class data and the training stage does not \\emph{explicitly} learn\nto generate the unseen class samples; $(ii)$ They do not learn a generic\noptimal parameter which can easily generalize for both seen and unseen class\ngeneration; and $(iii)$ If we only have access to a very few samples per seen\nclass, these models tend to perform poorly. In this paper, we propose a\nmeta-learning based generative model that naturally handles these limitations.\nThe proposed model is based on integrating model-agnostic meta learning with a\nWasserstein GAN (WGAN) to handle $(i)$ and $(iii)$, and uses a novel task\ndistribution to handle $(ii)$. Our proposed model yields significant\nimprovements on standard ZSL as well as more challenging GZSL setting. In ZSL\nsetting, our model yields 4.5\\%, 6.0\\%, 9.8\\%, and 27.9\\% relative improvements\nover the current state-of-the-art on CUB, AWA1, AWA2, and aPY datasets,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 08:11:46 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Verma", "Vinay Kumar", ""], ["Brahma", "Dhanajit", ""], ["Rai", "Piyush", ""]]}, {"id": "1909.04349", "submitter": "Christian Zimmermann", "authors": "Christian Zimmermann, Duygu Ceylan, Jimei Yang, Bryan Russell, Max\n  Argus and Thomas Brox", "title": "FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape from\n  Single RGB Images", "comments": "Accepted to ICCV 2019, Project page:\n  https://lmb.informatik.uni-freiburg.de/projects/freihand/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating 3D hand pose from single RGB images is a highly ambiguous problem\nthat relies on an unbiased training dataset. In this paper, we analyze\ncross-dataset generalization when training on existing datasets. We find that\napproaches perform well on the datasets they are trained on, but do not\ngeneralize to other datasets or in-the-wild scenarios. As a consequence, we\nintroduce the first large-scale, multi-view hand dataset that is accompanied by\nboth 3D hand pose and shape annotations. For annotating this real-world\ndataset, we propose an iterative, semi-automated `human-in-the-loop' approach,\nwhich includes hand fitting optimization to infer both the 3D pose and shape\nfor each sample. We show that methods trained on our dataset consistently\nperform well when tested on other datasets. Moreover, the dataset allows us to\ntrain a network that predicts the full articulated hand shape from a single RGB\nimage. The evaluation set can serve as a benchmark for articulated hand shape\nestimation.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 08:29:58 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 08:37:23 GMT"}, {"version": "v3", "created": "Fri, 13 Sep 2019 09:04:40 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Zimmermann", "Christian", ""], ["Ceylan", "Duygu", ""], ["Yang", "Jimei", ""], ["Russell", "Bryan", ""], ["Argus", "Max", ""], ["Brox", "Thomas", ""]]}, {"id": "1909.04365", "submitter": "Chaoyou Fu", "authors": "Boyan Duan, Chaoyou Fu, Yi Li, Xingguang Song, Ran He", "title": "Cross-Spectral Face Hallucination via Disentangling Independent Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cross-sensor gap is one of the challenges that have aroused much research\ninterests in Heterogeneous Face Recognition (HFR). Although recent methods have\nattempted to fill the gap with deep generative networks, most of them suffer\nfrom the inevitable misalignment between different face modalities. Instead of\nimaging sensors, the misalignment primarily results from facial geometric\nvariations that are independent of the spectrum. Rather than building a\nmonolithic but complex structure, this paper proposes a Pose Aligned\nCross-spectral Hallucination (PACH) approach to disentangle the independent\nfactors and deal with them in individual stages. In the first stage, an\nUnsupervised Face Alignment (UFA) module is designed to align the facial shapes\nof the near-infrared (NIR) images with those of the visible (VIS) images in a\ngenerative way, where UV maps are effectively utilized as the shape guidance.\nThus the task of the second stage becomes spectrum translation with aligned\npaired data. We develop a Texture Prior Synthesis (TPS) module to achieve\ncomplexion control and consequently generate more realistic VIS images than\nexisting methods. Experiments on three challenging NIR-VIS datasets verify the\neffectiveness of our approach in producing visually appealing images and\nachieving state-of-the-art performance in HFR.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 09:26:42 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 08:52:08 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Duan", "Boyan", ""], ["Fu", "Chaoyou", ""], ["Li", "Yi", ""], ["Song", "Xingguang", ""], ["He", "Ran", ""]]}, {"id": "1909.04366", "submitter": "Yingyue Xu", "authors": "Yingyue Xu, Dan Xu, Xiaopeng Hong, Wanli Ouyang, Rongrong Ji, Min Xu,\n  Guoying Zhao", "title": "Structured Modeling of Joint Deep Feature and Prediction Refinement for\n  Salient Object Detection", "comments": "Accepted to ICCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent saliency models extensively explore to incorporate multi-scale\ncontextual information from Convolutional Neural Networks (CNNs). Besides\ndirect fusion strategies, many approaches introduce message-passing to enhance\nCNN features or predictions. However, the messages are mainly transmitted in\ntwo ways, by feature-to-feature passing, and by prediction-to-prediction\npassing. In this paper, we add message-passing between features and predictions\nand propose a deep unified CRF saliency model . We design a novel cascade CRFs\narchitecture with CNN to jointly refine deep features and predictions at each\nscale and progressively compute a final refined saliency map. We formulate the\nCRF graphical model that involves message-passing of feature-feature,\nfeature-prediction, and prediction-prediction, from the coarse scale to the\nfiner scale, to update the features and the corresponding predictions. Also, we\nformulate the mean-field updates for joint end-to-end model training with CNN\nthrough back propagation. The proposed deep unified CRF saliency model is\nevaluated over six datasets and shows highly competitive performance among the\nstate of the arts.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 09:29:50 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Xu", "Yingyue", ""], ["Xu", "Dan", ""], ["Hong", "Xiaopeng", ""], ["Ouyang", "Wanli", ""], ["Ji", "Rongrong", ""], ["Xu", "Min", ""], ["Zhao", "Guoying", ""]]}, {"id": "1909.04373", "submitter": "Zhendong Zhang", "authors": "Zhendong Zhang and Cheolkon Jung", "title": "GBDT-MO: Gradient Boosted Decision Trees for Multiple Outputs", "comments": "10 pages, 5 figtures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gradient boosted decision trees (GBDTs) are widely used in machine learning,\nand the output of current GBDT implementations is a single variable. When there\nare multiple outputs, GBDT constructs multiple trees corresponding to the\noutput variables. The correlations between variables are ignored by such a\nstrategy causing redundancy of the learned tree structures. In this paper, we\npropose a general method to learn GBDT for multiple outputs, called GBDT-MO.\nEach leaf of GBDT-MO constructs predictions of all variables or a subset of\nautomatically selected variables. This is achieved by considering the summation\nof objective gains over all output variables. Moreover, we extend histogram\napproximation into multiple output case to speed up the training process.\nVarious experiments on synthetic and real-world datasets verify that GBDT-MO\nachieves outstanding performance in terms of both accuracy and training speed.\nOur codes are available on-line.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 09:48:04 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2019 08:29:43 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Zhang", "Zhendong", ""], ["Jung", "Cheolkon", ""]]}, {"id": "1909.04376", "submitter": "Shifeng Zhang", "authors": "Shifeng Zhang, Cheng Chi, Zhen Lei, Stan Z. Li", "title": "RefineFace: Refinement Neural Network for High Performance Face\n  Detection", "comments": "Journal extension of our previous conference paper: arXiv:1809.02693.\n  arXiv admin note: text overlap with arXiv:1901.02350 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection has achieved significant progress in recent years. However,\nhigh performance face detection still remains a very challenging problem,\nespecially when there exists many tiny faces. In this paper, we present a\nsingle-shot refinement face detector namely RefineFace to achieve high\nperformance. Specifically, it consists of five modules: Selective Two-step\nRegression (STR), Selective Two-step Classification (STC), Scale-aware Margin\nLoss (SML), Feature Supervision Module (FSM) and Receptive Field Enhancement\n(RFE). To enhance the regression ability for high location accuracy, STR\ncoarsely adjusts locations and sizes of anchors from high level detection\nlayers to provide better initialization for subsequent regressor. To improve\nthe classification ability for high recall efficiency, STC first filters out\nmost simple negatives from low level detection layers to reduce search space\nfor subsequent classifier, then SML is applied to better distinguish faces from\nbackground at various scales and FSM is introduced to let the backbone learn\nmore discriminative features for classification. Besides, RFE is presented to\nprovide more diverse receptive field to better capture faces in some extreme\nposes. Extensive experiments conducted on WIDER FACE, AFW, PASCAL Face, FDDB,\nMAFA demonstrate that our method achieves state-of-the-art results and runs at\n$37.3$ FPS with ResNet-18 for VGA-resolution images.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 09:58:50 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Zhang", "Shifeng", ""], ["Chi", "Cheng", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""]]}, {"id": "1909.04385", "submitter": "Aditya Ganeshan Master", "authors": "Aditya Ganeshan, B.S. Vivek, R. Venkatesh Babu", "title": "FDA: Feature Disruptive Attack", "comments": "Accepted in ICCV;19. Code Available at\n  https://github.com/BardOfCodes/fda", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though Deep Neural Networks (DNN) show excellent performance across various\ncomputer vision tasks, several works show their vulnerability to adversarial\nsamples, i.e., image samples with imperceptible noise engineered to manipulate\nthe network's prediction. Adversarial sample generation methods range from\nsimple to complex optimization techniques. Majority of these methods generate\nadversaries through optimization objectives that are tied to the pre-softmax or\nsoftmax output of the network. In this work we, (i) show the drawbacks of such\nattacks, (ii) propose two new evaluation metrics: Old Label New Rank (OLNR) and\nNew Label Old Rank (NLOR) in order to quantify the extent of damage made by an\nattack, and (iii) propose a new adversarial attack FDA: Feature Disruptive\nAttack, to address the drawbacks of existing attacks. FDA works by generating\nimage perturbation that disrupt features at each layer of the network and\ncauses deep-features to be highly corrupt. This allows FDA adversaries to\nseverely reduce the performance of deep networks. We experimentally validate\nthat FDA generates stronger adversaries than other state-of-the-art methods for\nimage classification, even in the presence of various defense measures. More\nimportantly, we show that FDA disrupts feature-representation based tasks even\nwithout access to the task-specific network or methodology. Code available at:\nhttps://github.com/BardOfCodes/fda\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 10:09:38 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Ganeshan", "Aditya", ""], ["Vivek", "B. S.", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1909.04391", "submitter": "Soo Ye Kim", "authors": "Soo Ye Kim, Jihyong Oh, Munchurl Kim", "title": "JSI-GAN: GAN-Based Joint Super-Resolution and Inverse Tone-Mapping with\n  Pixel-Wise Task-Specific Filters for UHD HDR Video", "comments": "The first two authors contributed equally to this work. Accepted at\n  AAAI 2020. (Camera-ready version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint learning of super-resolution (SR) and inverse tone-mapping (ITM) has\nbeen explored recently, to convert legacy low resolution (LR) standard dynamic\nrange (SDR) videos to high resolution (HR) high dynamic range (HDR) videos for\nthe growing need of UHD HDR TV/broadcasting applications. However, previous\nCNN-based methods directly reconstruct the HR HDR frames from LR SDR frames,\nand are only trained with a simple L2 loss. In this paper, we take a\ndivide-and-conquer approach in designing a novel GAN-based joint SR-ITM\nnetwork, called JSI-GAN, which is composed of three task-specific subnets: an\nimage reconstruction subnet, a detail restoration (DR) subnet and a local\ncontrast enhancement (LCE) subnet. We delicately design these subnets so that\nthey are appropriately trained for the intended purpose, learning a pair of\npixel-wise 1D separable filters via the DR subnet for detail restoration and a\npixel-wise 2D local filter by the LCE subnet for contrast enhancement.\nMoreover, to train the JSI-GAN effectively, we propose a novel detail GAN loss\nalongside the conventional GAN loss, which helps enhancing both local details\nand contrasts to reconstruct high quality HR HDR results. When all subnets are\njointly trained well, the predicted HR HDR results of higher quality are\nobtained with at least 0.41 dB gain in PSNR over those generated by the\nprevious methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 10:30:35 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 06:20:32 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Kim", "Soo Ye", ""], ["Oh", "Jihyong", ""], ["Kim", "Munchurl", ""]]}, {"id": "1909.04402", "submitter": "Mitja Nikolaus", "authors": "Mitja Nikolaus, Mostafa Abdou, Matthew Lamm, Rahul Aralikatte and\n  Desmond Elliott", "title": "Compositional Generalization in Image Captioning", "comments": "To appear at CoNLL 2019, EMNLP", "journal-ref": "Proceedings of the 23rd Conference on Computational Natural\n  Language Learning (CoNLL), pp. 87--98, ACL, 2019", "doi": "10.18653/v1/K19-1009", "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning models are usually evaluated on their ability to describe a\nheld-out set of images, not on their ability to generalize to unseen concepts.\nWe study the problem of compositional generalization, which measures how well a\nmodel composes unseen combinations of concepts when describing images.\nState-of-the-art image captioning models show poor generalization performance\non this task. We propose a multi-task model to address the poor performance,\nthat combines caption generation and image--sentence ranking, and uses a\ndecoding mechanism that re-ranks the captions according their similarity to the\nimage. This model is substantially better at generalizing to unseen\ncombinations of concepts compared to state-of-the-art captioning models.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 10:55:56 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 15:53:45 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Nikolaus", "Mitja", ""], ["Abdou", "Mostafa", ""], ["Lamm", "Matthew", ""], ["Aralikatte", "Rahul", ""], ["Elliott", "Desmond", ""]]}, {"id": "1909.04410", "submitter": "Chunxue Wu", "authors": "Chunxue Wu, Bobo Ju, Naixue Xiong, Guisong Yang, Yan Wu, Hongming\n  Yang, Jiaying Huang, Zhiyong Xu", "title": "U-net super-neural segmentation and similarity calculation to realize\n  vegetation change assessment in satellite imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vegetation is the natural linkage connecting soil, atmosphere and water. It\ncan represent the change of land cover to a certain extent and serve as an\nindicator for global change research. Methods for measuring coverage can be\ndivided into two types: surface measurement and remote sensing. Because\nvegetation cover has significant spatial and temporal differentiation\ncharacteristics, remote sensing has become an important technical means to\nestimate vegetation coverage. This paper firstly uses U-net to perform remote\nsensing image semantic segmentation training, then uses the result of semantic\nsegmentation, and then uses the integral progressive method to calculate the\nforestland change rate, and finally realizes automated valuation of woodland\nchange rate.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 11:13:55 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Wu", "Chunxue", ""], ["Ju", "Bobo", ""], ["Xiong", "Naixue", ""], ["Yang", "Guisong", ""], ["Wu", "Yan", ""], ["Yang", "Hongming", ""], ["Huang", "Jiaying", ""], ["Xu", "Zhiyong", ""]]}, {"id": "1909.04412", "submitter": "Wei Luo", "authors": "Wei Luo, Xitong Yang, Xianjie Mo, Yuheng Lu, Larry S. Davis, Jun Li,\n  Jian Yang, Ser-Nam Lim", "title": "Cross-X Learning for Fine-Grained Visual Categorization", "comments": "accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing objects from subcategories with very subtle differences remains a\nchallenging task due to the large intra-class and small inter-class variation.\nRecent work tackles this problem in a weakly-supervised manner: object parts\nare first detected and the corresponding part-specific features are extracted\nfor fine-grained classification. However, these methods typically treat the\npart-specific features of each image in isolation while neglecting their\nrelationships between different images. In this paper, we propose Cross-X\nlearning, a simple yet effective approach that exploits the relationships\nbetween different images and between different network layers for robust\nmulti-scale feature learning. Our approach involves two novel components: (i) a\ncross-category cross-semantic regularizer that guides the extracted features to\nrepresent semantic parts and, (ii) a cross-layer regularizer that improves the\nrobustness of multi-scale features by matching the prediction distribution\nacross multiple layers. Our approach can be easily trained end-to-end and is\nscalable to large datasets like NABirds. We empirically analyze the\ncontributions of different components of our approach and demonstrate its\nrobustness, effectiveness and state-of-the-art performance on five benchmark\ndatasets. Code is available at \\url{https://github.com/cswluo/CrossX}.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 11:20:07 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Luo", "Wei", ""], ["Yang", "Xitong", ""], ["Mo", "Xianjie", ""], ["Lu", "Yuheng", ""], ["Davis", "Larry S.", ""], ["Li", "Jun", ""], ["Yang", "Jian", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "1909.04422", "submitter": "Christian Ertler", "authors": "Christian Ertler, Jerneja Mislej, Tobias Ollmann, Lorenzo Porzi,\n  Gerhard Neuhold, Yubin Kuang", "title": "The Mapillary Traffic Sign Dataset for Detection and Classification on a\n  Global Scale", "comments": "17 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traffic signs are essential map features globally in the era of autonomous\ndriving and smart cities. To develop accurate and robust algorithms for traffic\nsign detection and classification, a large-scale and diverse benchmark dataset\nis required. In this paper, we introduce a traffic sign benchmark dataset of\n100K street-level images around the world that encapsulates diverse scenes,\nwide coverage of geographical locations, and varying weather and lighting\nconditions and covers more than 300 manually annotated traffic sign classes.\nThe dataset includes 52K images that are fully annotated and 48K images that\nare partially annotated. This is the largest and the most diverse traffic sign\ndataset consisting of images from all over world with fine-grained annotations\nof traffic sign classes. We have run extensive experiments to establish strong\nbaselines for both the detection and the classification tasks. In addition, we\nhave verified that the diversity of this dataset enables effective transfer\nlearning for existing large-scale benchmark datasets on traffic sign detection\nand classification. The dataset is freely available for academic research:\nhttps://www.mapillary.com/dataset/trafficsign.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 11:41:01 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 11:10:54 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Ertler", "Christian", ""], ["Mislej", "Jerneja", ""], ["Ollmann", "Tobias", ""], ["Porzi", "Lorenzo", ""], ["Neuhold", "Gerhard", ""], ["Kuang", "Yubin", ""]]}, {"id": "1909.04427", "submitter": "Mykhailo Vladymyrov", "authors": "Mykhailo Vladymyrov, Akitaka Ariga", "title": "Novel tracking approach based on fully-unsupervised disentanglement of\n  the geometrical factors of variation", "comments": "Accepted for publication in JINST", "journal-ref": null, "doi": "10.1088/1748-0221/15/03/P03009", "report-no": null, "categories": "cs.CV hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient tracking algorithms are a crucial part of particle tracking\ndetectors. While a lot of work has been done in designing a plethora of\nalgorithms, these usually require tedious tuning for each use case. (Weakly)\nsupervised Machine Learning-based approaches can leverage the actual raw data\nfor maximal performance. Yet in realistic scenarios, sufficient high-quality\nlabeled data is not available. While training might be performed on simulated\ndata, the reproduction of realistic signal and noise in the detector requires\nsubstantial effort, compromising this approach.\n  Here we propose a novel, fully unsupervised, approach to track\nreconstruction. The introduced model for learning to disentangle the factors of\nvariation in a geometrically meaningful way employs geometrical space\ninvariances. We train it through constraints on the equivariance between the\nimage space and the latent representation in a Deep Convolutional Autoencoder.\nUsing experimental results on synthetic data we show that a combination of\ndifferent space transformations is required for meaningful disentanglement of\nfactors of variation. We also demonstrate the performance of our model on real\ndata from tracking detectors.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 12:05:59 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 15:29:58 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Vladymyrov", "Mykhailo", ""], ["Ariga", "Akitaka", ""]]}, {"id": "1909.04469", "submitter": "Anoop Katti", "authors": "Christian Reisswig, Anoop R Katti, Marco Spinaci, Johannes H\\\"ohne", "title": "Chargrid-OCR: End-to-end Trainable Optical Character Recognition for\n  Printed Documents using Instance Segmentation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end trainable approach for Optical Character Recognition\n(OCR) on printed documents. Specifically, we propose a model that predicts a) a\ntwo-dimensional character grid (\\emph{chargrid}) representation of a document\nimage as a semantic segmentation task and b) character boxes for delineating\ncharacter instances as an object detection task. For training the model, we\nbuild two large-scale datasets without resorting to any manual annotation -\nsynthetic documents with clean labels and real documents with noisy labels. We\ndemonstrate experimentally that our method, trained on the combination of these\ndatasets, (i) outperforms previous state-of-the-art approaches in accuracy (ii)\nis easily parallelizable on GPU and is, therefore, significantly faster and\n(iii) is easy to train and adapt to a new domain.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 13:30:55 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 09:07:54 GMT"}, {"version": "v3", "created": "Thu, 5 Dec 2019 16:21:15 GMT"}, {"version": "v4", "created": "Thu, 27 Feb 2020 12:44:54 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Reisswig", "Christian", ""], ["Katti", "Anoop R", ""], ["Spinaci", "Marco", ""], ["H\u00f6hne", "Johannes", ""]]}, {"id": "1909.04485", "submitter": "Jose M. Alvarez", "authors": "Shuang Gao and Xin Liu and Lung-Sheng Chien and William Zhang and Jose\n  M. Alvarez", "title": "VACL: Variance-Aware Cross-Layer Regularization for Pruning Deep\n  Residual Networks", "comments": "ICCV Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving weight sparsity is a common strategy for producing light-weight\ndeep neural networks. However, pruning models with residual learning is more\nchallenging. In this paper, we introduce Variance-Aware Cross-Layer (VACL), a\nnovel approach to address this problem. VACL consists of two parts, a\nCross-Layer grouping and a Variance Aware regularization. In Cross-Layer\ngrouping the $i^{th}$ filters of layers connected by skip-connections are\ngrouped into one regularization group. Then, the Variance-Aware regularization\nterm takes into account both the first and second-order statistics of the\nconnected layers to constrain the variance within a group. Our approach can\neffectively improve the structural sparsity of residual models. For CIFAR10,\nthe proposed method reduces a ResNet model by up to 79.5% with no accuracy drop\nand reduces a ResNeXt model by up to 82% with less than 1% accuracy drop. For\nImageNet, it yields a pruned ratio of up to 63.3% with less than 1% top-5\naccuracy drop. Our experimental results show that the proposed approach\nsignificantly outperforms other state-of-the-art methods in terms of overall\nmodel size and accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 13:59:04 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Gao", "Shuang", ""], ["Liu", "Xin", ""], ["Chien", "Lung-Sheng", ""], ["Zhang", "William", ""], ["Alvarez", "Jose M.", ""]]}, {"id": "1909.04518", "submitter": "Thanh Nguyen", "authors": "Thanh Nguyen, Vy Bui, Anh Thai, Van Lam, Christopher B. Raub,\n  Lin-Ching Chang, and George Nehmetallah", "title": "Virtual organelle self-coding for fluorescence imaging via adversarial\n  learning", "comments": "20 pages, 9 figures", "journal-ref": null, "doi": "10.1117/1.JBO.25.9.096009", "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluorescence microscopy plays a vital role in understanding the subcellular\nstructures of living cells. However, it requires considerable effort in sample\npreparation related to chemical fixation, staining, cost, and time. To reduce\nthose factors, we present a virtual fluorescence staining method based on deep\nneural networks (VirFluoNet) to transform fluorescence images of molecular\nlabels into other molecular fluorescence labels in the same field-of-view. To\nachieve this goal, we develop and train a conditional generative adversarial\nnetwork (cGAN) to perform digital fluorescence imaging demonstrated on human\nosteosarcoma U2OS cell fluorescence images captured under Cell Painting\nstaining protocol. A detailed comparative analysis is also conducted on the\nperformance of the cGAN network between predicting fluorescence channels based\non phase contrast or based on another fluorescence channel using human breast\ncancer MDA-MB-231 cell line as a test case. In addition, we implement a deep\nlearning model to perform autofocusing on another human U2OS fluorescence\ndataset as a preprocessing step to defocus an out-focus channel in U2OS\ndataset. A quantitative index of image prediction error is introduced based on\nsignal pixel-wise spatial and intensity differences with ground truth to\nevaluate the performance of prediction to high-complex and throughput\nfluorescence. This index provides a rational way to perform image segmentation\non error signals and to understand the likelihood of mis-interpreting biology\nfrom the predicted image. In total, these findings contribute to the utility of\ndeep learning image regression for fluorescence microscopy datasets of\nbiological cells, balanced against savings of cost, time, and experimental\neffort. Furthermore, the approach introduced here holds promise for modeling\nthe internal relationships between organelles and biomolecules within living\ncells.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 14:26:38 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Nguyen", "Thanh", ""], ["Bui", "Vy", ""], ["Thai", "Anh", ""], ["Lam", "Van", ""], ["Raub", "Christopher B.", ""], ["Chang", "Lin-Ching", ""], ["Nehmetallah", "George", ""]]}, {"id": "1909.04525", "submitter": "Andre Pacheco", "authors": "Andre G. C. Pacheco and Abder-Rahman Ali and Thomas Trappenberg", "title": "Skin cancer detection based on deep learning and entropy to detect\n  outlier samples", "comments": "3rd and 4th places in tasks 1 and 2 respectively, at ISIC challenge\n  2019 @ MICCAI workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe our methods that achieved the 3rd and 4th places in tasks 1 and\n2, respectively, at ISIC challenge 2019. The goal of this challenge is to\nprovide the diagnostic for skin cancer using images and meta-data. There are\nnine classes in the dataset, nonetheless, one of them is an outlier and is not\npresent on it. To tackle the challenge, we apply an ensemble of classifiers,\nwhich has 13 convolutional neural networks (CNN), we develop two approaches to\nhandle the outlier class and we propose a straightforward method to use the\nmeta-data along with the images. Throughout this report, we detail each\nmethodology and parameters to make it easy to replicate our work. The results\nobtained are in accordance with the previous challenges and the approaches to\ndetect the outlier class and to address the meta-data seem to be work properly.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 14:36:16 GMT"}, {"version": "v2", "created": "Sun, 5 Jan 2020 22:45:19 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Pacheco", "Andre G. C.", ""], ["Ali", "Abder-Rahman", ""], ["Trappenberg", "Thomas", ""]]}, {"id": "1909.04538", "submitter": "H{\\aa}kon Hukkel{\\aa}s", "authors": "H{\\aa}kon Hukkel{\\aa}s, Rudolf Mester and Frank Lindseth", "title": "DeepPrivacy: A Generative Adversarial Network for Face Anonymization", "comments": "Accepted to ISVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a novel architecture which is able to automatically anonymize\nfaces in images while retaining the original data distribution. We ensure total\nanonymization of all faces in an image by generating images exclusively on\nprivacy-safe information. Our model is based on a conditional generative\nadversarial network, generating images considering the original pose and image\nbackground. The conditional information enables us to generate highly realistic\nfaces with a seamless transition between the generated face and the existing\nbackground. Furthermore, we introduce a diverse dataset of human faces,\nincluding unconventional poses, occluded faces, and a vast variability in\nbackgrounds. Finally, we present experimental results reflecting the capability\nof our model to anonymize images while preserving the data distribution, making\nthe data suitable for further training of deep learning models. As far as we\nknow, no other solution has been proposed that guarantees the anonymization of\nfaces while generating realistic images.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 14:52:24 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Hukkel\u00e5s", "H\u00e5kon", ""], ["Mester", "Rudolf", ""], ["Lindseth", "Frank", ""]]}, {"id": "1909.04542", "submitter": "Jue Jiang Dr.", "authors": "Jue Jiang, Jason Hu, Neelam Tyagi, Andreas Rimner, Sean L. Berry,\n  Joseph O. Deasy, Harini Veeraraghavan", "title": "Integrating cross-modality hallucinated MRI with CT to aid mediastinal\n  lung tumor segmentation", "comments": "This paper has been accepted by MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung tumors, especially those located close to or surrounded by soft tissues\nlike the mediastinum, are difficult to segment due to the low soft tissue\ncontrast on computed tomography images. Magnetic resonance images contain\nsuperior soft-tissue contrast information that can be leveraged if both\nmodalities were available for training. Therefore, we developed a\ncross-modality educed learning approach where MR information that is educed\nfrom CT is used to hallucinate MRI and improve CT segmentation. Our approach,\ncalled cross-modality educed deep learning segmentation (CMEDL) combines CT and\npseudo MR produced from CT by aligning their features to obtain segmentation on\nCT. Features computed in the last two layers of parallelly trained CT and MR\nsegmentation networks are aligned. We implemented this approach on U-net and\ndense fully convolutional networks (dense-FCN). Our networks were trained on\nunrelated cohorts from open-source the Cancer Imaging Archive CT images\n(N=377), an internal archive T2-weighted MR (N=81), and evaluated using\nseparate validation (N=304) and testing (N=333) CT-delineated tumors. Our\napproach using both networks were significantly more accurate (U-net $P\n<0.001$; denseFCN $P <0.001$) than CT-only networks and achieved an accuracy\n(Dice similarity coefficient) of 0.71$\\pm$0.15 (U-net), 0.74$\\pm$0.12\n(denseFCN) on validation and 0.72$\\pm$0.14 (U-net), 0.73$\\pm$0.12 (denseFCN) on\nthe testing sets. Our novel approach demonstrated that educing cross-modality\ninformation through learned priors enhances CT segmentation performance\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 14:56:32 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Jiang", "Jue", ""], ["Hu", "Jason", ""], ["Tyagi", "Neelam", ""], ["Rimner", "Andreas", ""], ["Berry", "Sean L.", ""], ["Deasy", "Joseph O.", ""], ["Veeraraghavan", "Harini", ""]]}, {"id": "1909.04564", "submitter": "Chenyang Lu", "authors": "Chenyang Lu and Gijs Dubbelman", "title": "Semantic Foreground Inpainting from Weak Supervision", "comments": "RA-L and ICRA'20", "journal-ref": null, "doi": "10.1109/LRA.2020.2967712", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic scene understanding is an essential task for self-driving vehicles\nand mobile robots. In our work, we aim to estimate a semantic segmentation map,\nin which the foreground objects are removed and semantically inpainted with\nbackground classes, from a single RGB image. This semantic foreground\ninpainting task is performed by a single-stage convolutional neural network\n(CNN) that contains our novel max-pooling as inpainting (MPI) module, which is\ntrained with weak supervision, i.e., it does not require manual background\nannotations for the foreground regions to be inpainted. Our approach is\ninherently more efficient than the previous two-stage state-of-the-art method,\nand outperforms it by a margin of 3% IoU for the inpainted foreground regions\non Cityscapes. The performance margin increases to 6% IoU, when tested on the\nunseen KITTI dataset. The code and the manually annotated datasets for testing\nare shared with the research community at\nhttps://github.com/Chenyang-Lu/semantic-foreground-inpainting.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 15:17:24 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 14:04:22 GMT"}, {"version": "v3", "created": "Sun, 16 Feb 2020 14:34:26 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Lu", "Chenyang", ""], ["Dubbelman", "Gijs", ""]]}, {"id": "1909.04588", "submitter": "Qinghui Liu", "authors": "Qinghui Liu, Michael Kampffmeyer, Robert Jenssen, Arnt-B{\\o}rre\n  Salberg", "title": "Road Mapping In LiDAR Images Using A Joint-Task Dense Dilated\n  Convolutions Merging Network", "comments": "IGARSS 2019. arXiv admin note: text overlap with arXiv:1908.11799", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is important, but challenging, for the forest industry to accurately map\nroads which are used for timber transport by trucks. In this work, we propose a\nDense Dilated Convolutions Merging Network (DDCM-Net) to detect these roads in\nlidar images. The DDCM-Net can effectively recognize multi-scale and complex\nshaped roads with similar texture and colors, and also is shown to have\nsuperior performance over existing methods. To further improve its ability to\naccurately infer categories of roads, we propose the use of a joint-task\nlearning strategy that utilizes two auxiliary output branches, i.e, multi-class\nclassification and binary segmentation, joined with the main output of\nfull-class segmentation. This pushes the network towards learning more robust\nrepresentations that are expected to boost the ultimate performance of the main\ntask. In addition, we introduce an iterative-random-weighting method to\nautomatically weigh the joint losses for auxiliary tasks. This can avoid the\ndifficult and expensive process of tuning the weights of each task's loss by\nhand. The experiments demonstrate that our proposed joint-task DDCM-Net can\nachieve better performance with fewer parameters and higher computational\nefficiency than previous state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 16:35:55 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Liu", "Qinghui", ""], ["Kampffmeyer", "Michael", ""], ["Jenssen", "Robert", ""], ["Salberg", "Arnt-B\u00f8rre", ""]]}, {"id": "1909.04594", "submitter": "Yi Fang", "authors": "Jing Zhu, Yunxiao Shi, Mengwei Ren, Yi Fang, Kuo-Chin Lien, Junli Gu", "title": "Structure-Attentioned Memory Network for Monocular Depth Estimation", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth estimation is a challenging task that aims to predict a\ncorresponding depth map from a given single RGB image. Recent deep learning\nmodels have been proposed to predict the depth from the image by learning the\nalignment of deep features between the RGB image and the depth domains. In this\npaper, we present a novel approach, named Structure-Attentioned Memory Network,\nto more effectively transfer domain features for monocular depth estimation by\ntaking into account the common structure regularities (e.g., repetitive\nstructure patterns, planar surfaces, symmetries) in domain adaptation. To this\nend, we introduce a new Structure-Oriented Memory (SOM) module to learn and\nmemorize the structure-specific information between RGB image domain and the\ndepth domain. More specifically, in the SOM module, we develop a Memorable Bank\nof Filters (MBF) unit to learn a set of filters that memorize the\nstructure-aware image-depth residual pattern, and also an Attention Guided\nController (AGC) unit to control the filter selection in the MBF given image\nfeatures queries. Given the query image feature, the trained SOM module is able\nto adaptively select the best customized filters for cross-domain feature\ntransferring with an optimal structural disparity between image and depth. In\nsummary, we focus on addressing this structure-specific domain adaption\nchallenge by proposing a novel end-to-end multi-scale memorable network for\nmonocular depth estimation. The experiments show that our proposed model\ndemonstrates the superior performance compared to the existing supervised\nmonocular depth estimation approaches on the challenging KITTI and NYU Depth V2\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 16:05:38 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Zhu", "Jing", ""], ["Shi", "Yunxiao", ""], ["Ren", "Mengwei", ""], ["Fang", "Yi", ""], ["Lien", "Kuo-Chin", ""], ["Gu", "Junli", ""]]}, {"id": "1909.04596", "submitter": "Mehul S. Raval", "authors": "Rupal Agravat, Mehul S Raval", "title": "Prediction of Overall Survival of Brain Tumor Patients", "comments": "5 pages, IEEE TENCON 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated brain tumor segmentation plays an important role in the diagnosis\nand prognosis of the patient. In addition, features from the tumorous brain\nhelp in predicting patients overall survival. The main focus of this paper is\nto segment tumor from BRATS 2018 benchmark dataset and use age, shape and\nvolumetric features to predict overall survival of patients. The random forest\nclassifier achieves overall survival accuracy of 59% on the test dataset and\n67% on the dataset with resection status as gross total resection. The proposed\napproach uses fewer features but achieves better accuracy than state of the art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 16:09:12 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Agravat", "Rupal", ""], ["Raval", "Mehul S", ""]]}, {"id": "1909.04614", "submitter": "Weiwei Song", "authors": "Weiwei Song, Shutao Li, and Jon Atli Benediktsson", "title": "Deep Hashing Learning for Visual and Semantic Retrieval of Remote\n  Sensing Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by the urgent demand for managing remote sensing big data, large-scale\nremote sensing image retrieval (RSIR) attracts increasing attention in the\nremote sensing field. In general, existing retrieval methods can be regarded as\nvisual-based retrieval approaches which search and return a set of similar\nimages from a database to a given query image. Although retrieval methods have\nachieved great success, there is still a question that needs to be responded\nto: Can we obtain the accurate semantic labels of the returned similar images\nto further help analyzing and processing imagery? Inspired by the above\nquestion, in this paper, we redefine the image retrieval problem as visual and\nsemantic retrieval of images. Specifically, we propose a novel deep hashing\nconvolutional neural network (DHCNN) to simultaneously retrieve the similar\nimages and classify their semantic labels in a unified framework. In more\ndetail, a convolutional neural network (CNN) is used to extract\nhigh-dimensional deep features. Then, a hash layer is perfectly inserted into\nthe network to transfer the deep features into compact hash codes. In addition,\na fully connected layer with a softmax function is performed on hash layer to\ngenerate class distribution. Finally, a loss function is elaborately designed\nto simultaneously consider the label loss of each image and similarity loss of\npairs of images. Experimental results on two remote sensing datasets\ndemonstrate that the proposed method achieves the state-of-art retrieval and\nclassification performance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 16:45:02 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Song", "Weiwei", ""], ["Li", "Shutao", ""], ["Benediktsson", "Jon Atli", ""]]}, {"id": "1909.04656", "submitter": "Tengda Han", "authors": "Tengda Han, Weidi Xie, Andrew Zisserman", "title": "Video Representation Learning by Dense Predictive Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is self-supervised learning of spatio-temporal\nembeddings from video, suitable for human action recognition. We make three\ncontributions: First, we introduce the Dense Predictive Coding (DPC) framework\nfor self-supervised representation learning on videos. This learns a dense\nencoding of spatio-temporal blocks by recurrently predicting future\nrepresentations; Second, we propose a curriculum training scheme to predict\nfurther into the future with progressively less temporal context. This\nencourages the model to only encode slowly varying spatial-temporal signals,\ntherefore leading to semantic representations; Third, we evaluate the approach\nby first training the DPC model on the Kinetics-400 dataset with\nself-supervised learning, and then finetuning the representation on a\ndownstream task, i.e. action recognition. With single stream (RGB only), DPC\npretrained representations achieve state-of-the-art self-supervised performance\non both UCF101(75.7% top1 acc) and HMDB51(35.7% top1 acc), outperforming all\nprevious learning methods by a significant margin, and approaching the\nperformance of a baseline pre-trained on ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 17:58:32 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 12:57:22 GMT"}, {"version": "v3", "created": "Fri, 27 Sep 2019 00:35:02 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Han", "Tengda", ""], ["Xie", "Weidi", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1909.04686", "submitter": "Haibin Huang", "authors": "Shaofan Cai, Xiaoshuai Zhang, Haoqiang Fan, Haibin Huang, Jiangyu Liu,\n  Jiaming Liu, Jiaying Liu, Jue Wang, Jian Sun", "title": "Disentangled Image Matting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most previous image matting methods require a roughly-specificed trimap as\ninput, and estimate fractional alpha values for all pixels that are in the\nunknown region of the trimap. In this paper, we argue that directly estimating\nthe alpha matte from a coarse trimap is a major limitation of previous methods,\nas this practice tries to address two difficult and inherently different\nproblems at the same time: identifying true blending pixels inside the trimap\nregion, and estimate accurate alpha values for them. We propose AdaMatting, a\nnew end-to-end matting framework that disentangles this problem into two\nsub-tasks: trimap adaptation and alpha estimation. Trimap adaptation is a\npixel-wise classification problem that infers the global structure of the input\nimage by identifying definite foreground, background, and semi-transparent\nimage regions. Alpha estimation is a regression problem that calculates the\nopacity value of each blended pixel. Our method separately handles these two\nsub-tasks within a single deep convolutional neural network (CNN). Extensive\nexperiments show that AdaMatting has additional structure awareness and trimap\nfault-tolerance. Our method achieves the state-of-the-art performance on Adobe\nComposition-1k dataset both qualitatively and quantitatively. It is also the\ncurrent best-performing method on the alphamatting.com online evaluation for\nall commonly-used metrics.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 18:00:58 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Cai", "Shaofan", ""], ["Zhang", "Xiaoshuai", ""], ["Fan", "Haoqiang", ""], ["Huang", "Haibin", ""], ["Liu", "Jiangyu", ""], ["Liu", "Jiaming", ""], ["Liu", "Jiaying", ""], ["Wang", "Jue", ""], ["Sun", "Jian", ""]]}, {"id": "1909.04689", "submitter": "Binod Bhattarai", "authors": "Binod Bhattarai, Seungryul Baek, Rumeysa Bodur, Tae-Kyun Kim", "title": "Sampling Strategies for GAN Synthetic Data", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9054677", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have been used widely to generate\nlarge volumes of synthetic data. This data is being utilized for augmenting\nwith real examples in order to train deep Convolutional Neural Networks (CNNs).\nStudies have shown that the generated examples lack sufficient realism to train\ndeep CNNs and are poor in diversity. Unlike previous studies of randomly\naugmenting the synthetic data with real data, we present our simple, effective\nand easy to implement synthetic data sampling methods to train deep CNNs more\nefficiently and accurately. To this end, we propose to maximally utilize the\nparameters learned during training of the GAN itself. These include\ndiscriminator's realism confidence score and the confidence on the target label\nof the synthetic data. In addition to this, we explore reinforcement learning\n(RL) to automatically search a subset of meaningful synthetic examples from a\nlarge pool of GAN synthetic data. We evaluate our method on two challenging\nface attribute classification data sets viz. AffectNet and CelebA. Our\nextensive experiments clearly demonstrate the need of sampling synthetic data\nbefore augmentation, which also improves the performance of one of the\nstate-of-the-art deep CNNs in vitro.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 18:07:32 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Bhattarai", "Binod", ""], ["Baek", "Seungryul", ""], ["Bodur", "Rumeysa", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1909.04696", "submitter": "Arijit Ray", "authors": "Arijit Ray, Karan Sikka, Ajay Divakaran, Stefan Lee, Giedrius Burachas", "title": "Sunny and Dark Outside?! Improving Answer Consistency in VQA through\n  Entailed Question Generation", "comments": "2019 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While models for Visual Question Answering (VQA) have steadily improved over\nthe years, interacting with one quickly reveals that these models lack\nconsistency. For instance, if a model answers \"red\" to \"What color is the\nballoon?\", it might answer \"no\" if asked, \"Is the balloon red?\". These\nresponses violate simple notions of entailment and raise questions about how\neffectively VQA models ground language. In this work, we introduce a dataset,\nConVQA, and metrics that enable quantitative evaluation of consistency in VQA.\nFor a given observable fact in an image (e.g. the balloon's color), we generate\na set of logically consistent question-answer (QA) pairs (e.g. Is the balloon\nred?) and also collect a human-annotated set of common-sense based consistent\nQA pairs (e.g. Is the balloon the same color as tomato sauce?). Further, we\npropose a consistency-improving data augmentation module, a Consistency Teacher\nModule (CTM). CTM automatically generates entailed (or similar-intent)\nquestions for a source QA pair and fine-tunes the VQA model if the VQA's answer\nto the entailed question is consistent with the source QA pair. We demonstrate\nthat our CTM-based training improves the consistency of VQA models on the\nConVQA datasets and is a strong baseline for further research.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 18:18:45 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Ray", "Arijit", ""], ["Sikka", "Karan", ""], ["Divakaran", "Ajay", ""], ["Lee", "Stefan", ""], ["Burachas", "Giedrius", ""]]}, {"id": "1909.04743", "submitter": "Tete Xiao", "authors": "Tete Xiao, Quanfu Fan, Dan Gutfreund, Mathew Monfort, Aude Oliva,\n  Bolei Zhou", "title": "Reasoning About Human-Object Interactions Through Dual Attention\n  Networks", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects are entities we act upon, where the functionality of an object is\ndetermined by how we interact with it. In this work we propose a Dual Attention\nNetwork model which reasons about human-object interactions. The\ndual-attentional framework weights the important features for objects and\nactions respectively. As a result, the recognition of objects and actions\nmutually benefit each other. The proposed model shows competitive\nclassification performance on the human-object interaction dataset\nSomething-Something. Besides, it can perform weak spatiotemporal localization\nand affordance segmentation, despite being trained only with video-level\nlabels. The model not only finds when an action is happening and which object\nis being manipulated, but also identifies which part of the object is being\ninteracted with. Project page: \\url{https://dual-attention-network.github.io/}.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 20:45:08 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Xiao", "Tete", ""], ["Fan", "Quanfu", ""], ["Gutfreund", "Dan", ""], ["Monfort", "Mathew", ""], ["Oliva", "Aude", ""], ["Zhou", "Bolei", ""]]}, {"id": "1909.04779", "submitter": "Eitan Rothberg", "authors": "Eitan Rothberg, Tingting Chen, Luo Jie, Hao Ji", "title": "Localized Adversarial Training for Increased Accuracy and Robustness in\n  Image Classification", "comments": "4 pages (excluding references). Presented at AdvML: 1st Workshop on\n  Adversarial Learning Methods for Machine Learning and Data Mining at KDD '19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's state-of-the-art image classifiers fail to correctly classify\ncarefully manipulated adversarial images. In this work, we develop a new,\nlocalized adversarial attack that generates adversarial examples by\nimperceptibly altering the backgrounds of normal images. We first use this\nattack to highlight the unnecessary sensitivity of neural networks to changes\nin the background of an image, then use it as part of a new training technique:\nlocalized adversarial training. By including locally adversarial images in the\ntraining set, we are able to create a classifier that suffers less loss than a\nnon-adversarially trained counterpart model on both natural and adversarial\ninputs. The evaluation of our localized adversarial training algorithm on MNIST\nand CIFAR-10 datasets shows decreased accuracy loss on natural images, and\nincreased robustness against adversarial inputs.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 22:26:48 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Rothberg", "Eitan", ""], ["Chen", "Tingting", ""], ["Jie", "Luo", ""], ["Ji", "Hao", ""]]}, {"id": "1909.04790", "submitter": "Shabnam Daghaghi", "authors": "Shabnam Daghaghi, Tharun Medini, Anshumali Shrivastava", "title": "SDM-Net: A Simple and Effective Model for Generalized Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot Learning (ZSL) is a classification task where we do not have even a\nsingle training labeled example from a set of unseen classes. Instead, we only\nhave prior information (or description) about seen and unseen classes, often in\nthe form of physically realizable or descriptive attributes. Lack of any single\ntraining example from a set of classes prohibits use of standard classification\ntechniques and losses, including the popular crossentropy loss. Currently,\nstate-of-the-art approaches encode the prior class information into dense\nvectors and optimize some distance between the learned projections of the input\nvector and the corresponding class vector (collectively known as embedding\nmodels). In this paper, we propose a novel architecture of casting zero-shot\nlearning as a standard neural-network with crossentropy loss. During training\nour approach performs soft-labeling by combining the observed training data for\nthe seen classes with the similarity information from the attributes for which\nwe have no training data or unseen classes. To the best of our knowledge, such\nsimilarity based soft-labeling is not explored in the field of deep learning.\nWe evaluate the proposed model on the four benchmark datasets for zero-shot\nlearning, AwA, aPY, SUN and CUB datasets, and show that our model achieves\nsignificant improvement over the state-of-the-art methods in Generalized-ZSL\nand ZSL settings on all of these datasets consistently.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 23:27:24 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 10:27:37 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Daghaghi", "Shabnam", ""], ["Medini", "Tharun", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1909.04797", "submitter": "Raunak Dey", "authors": "Raunak Dey, Yi Hong", "title": "Hybrid Cascaded Neural Network for Liver Lesion Segmentation", "comments": null, "journal-ref": null, "doi": "10.1109/ISBI45749.2020.9098656", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic liver lesion segmentation is a challenging task while having a\nsignificant impact on assisting medical professionals in the designing of\neffective treatment and planning proper care. In this paper we propose a\ncascaded system that combines both 2D and 3D convolutional neural networks to\neffectively segment hepatic lesions. Our 2D network operates on a slice by\nslice basis to segment the liver and larger tumors, while we use a 3D network\nto detect small lesions that are often missed in a 2D segmentation design. We\nemploy this algorithm on the LiTS challenge obtaining a Dice score per case of\n68.1%, which performs the best among all non pre-trained models and the second\nbest among published methods. We also perform two-fold cross-validation to\nreveal the over- and under-segmentation issues in the LiTS annotations.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 00:11:14 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 16:38:34 GMT"}, {"version": "v3", "created": "Tue, 8 Oct 2019 05:25:43 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Dey", "Raunak", ""], ["Hong", "Yi", ""]]}, {"id": "1909.04800", "submitter": "Badri Narayana Patro", "authors": "Badri N. Patro, Anupriy, Vinay P. Namboodiri", "title": "Probabilistic framework for solving Visual Dialog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a probabilistic framework for solving the task of\n`Visual Dialog'. Solving this task requires reasoning and understanding of\nvisual modality, language modality, and common sense knowledge to answer.\nVarious architectures have been proposed to solve this task by variants of\nmulti-modal deep learning techniques that combine visual and language\nrepresentations. However, we believe that it is crucial to understand and\nanalyze the sources of uncertainty for solving this task. Our approach allows\nfor estimating uncertainty and also aids a diverse generation of answers. The\nproposed approach is obtained through a probabilistic representation module\nthat provides us with representations for image, question and conversation\nhistory, a module that ensures that diverse latent representations for\ncandidate answers are obtained given the probabilistic representations and an\nuncertainty representation module that chooses the appropriate answer that\nminimizes uncertainty. We thoroughly evaluate the model with a detailed\nablation analysis, comparison with state of the art and visualization of the\nuncertainty that aids in the understanding of the method. Using the proposed\nprobabilistic framework, we thus obtain an improved visual dialog system that\nis also more explainable.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 00:25:12 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 07:30:39 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Patro", "Badri N.", ""], ["Anupriy", "", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1909.04802", "submitter": "Yoojin Choi", "authors": "Yoojin Choi, Mostafa El-Khamy, Jungwon Lee", "title": "Variable Rate Deep Image Compression With a Conditional Autoencoder", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel variable-rate learned image compression\nframework with a conditional autoencoder. Previous learning-based image\ncompression methods mostly require training separate networks for different\ncompression rates so they can yield compressed images of varying quality. In\ncontrast, we train and deploy only one variable-rate image compression network\nimplemented with a conditional autoencoder. We provide two rate control\nparameters, i.e., the Lagrange multiplier and the quantization bin size, which\nare given as conditioning variables to the network. Coarse rate adaptation to a\ntarget is performed by changing the Lagrange multiplier, while the rate can be\nfurther fine-tuned by adjusting the bin size used in quantizing the encoded\nrepresentation. Our experimental results show that the proposed scheme provides\na better rate-distortion trade-off than the traditional variable-rate image\ncompression codecs such as JPEG2000 and BPG. Our model also shows comparable\nand sometimes better performance than the state-of-the-art learned image\ncompression models that deploy multiple networks trained for varying rates.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 00:33:52 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Choi", "Yoojin", ""], ["El-Khamy", "Mostafa", ""], ["Lee", "Jungwon", ""]]}, {"id": "1909.04810", "submitter": "Sulabh Kumra", "authors": "Sulabh Kumra, Shirin Joshi, Ferat Sahin", "title": "Antipodal Robotic Grasping using Generative Residual Convolutional\n  Neural Network", "comments": "8 pages, 5 figures, IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a modular robotic system to tackle the problem of\ngenerating and performing antipodal robotic grasps for unknown objects from\nn-channel image of the scene. We propose a novel Generative Residual\nConvolutional Neural Network (GR-ConvNet) model that can generate robust\nantipodal grasps from n-channel input at real-time speeds (~20ms). We evaluate\nthe proposed model architecture on standard datasets and a diverse set of\nhousehold objects. We achieved state-of-the-art accuracy of 97.7% and 94.6% on\nCornell and Jacquard grasping datasets respectively. We also demonstrate a\ngrasp success rate of 95.4% and 93% on household and adversarial objects\nrespectively using a 7 DoF robotic arm.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 01:44:07 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 23:09:58 GMT"}, {"version": "v3", "created": "Sat, 1 Aug 2020 19:00:42 GMT"}, {"version": "v4", "created": "Thu, 10 Jun 2021 20:11:46 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Kumra", "Sulabh", ""], ["Joshi", "Shirin", ""], ["Sahin", "Ferat", ""]]}, {"id": "1909.04813", "submitter": "Yukun Zhou", "authors": "Yukun Zhou, Zailiang Chen, Hailan Shen, Qing Liu, Rongchang Zhao, and\n  Yixiong Liang", "title": "Dual-attention Focused Module for Weakly Supervised Object Localization", "comments": "8 pages, 6 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research on recognizing the most discriminative regions provides\nreferential information for weakly supervised object localization with only\nimage-level annotations. However, the most discriminative regions usually\nconceal the other parts of the object, thereby impeding entire object\nrecognition and localization. To tackle this problem, the Dual-attention\nFocused Module (DFM) is proposed to enhance object localization performance.\nSpecifically, we present a dual attention module for information fusion,\nconsisting of a position branch and a channel one. In each branch, the input\nfeature map is deduced into an enhancement map and a mask map, thereby\nhighlighting the most discriminative parts or hiding them. For the position\nmask map, we introduce a focused matrix to enhance it, which utilizes the\nprinciple that the pixels of an object are continuous. Between these two\nbranches, the enhancement map is integrated with the mask map, aiming at\npartially compensating the lost information and diversifies the features. With\nthe dual-attention module and focused matrix, the entire object region could be\nprecisely recognized with implicit information. We demonstrate outperforming\nresults of DFM in experiments. In particular, DFM achieves state-of-the-art\nperformance in localization accuracy in ILSVRC 2016 and CUB-200-2011.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 01:49:25 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Zhou", "Yukun", ""], ["Chen", "Zailiang", ""], ["Shen", "Hailan", ""], ["Liu", "Qing", ""], ["Zhao", "Rongchang", ""], ["Liang", "Yixiong", ""]]}, {"id": "1909.04837", "submitter": "Xiaojun Jia", "authors": "Xiaojun Jia, Xingxing Wei, Xiaochun Cao", "title": "Identifying and Resisting Adversarial Videos Using Temporal Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Video classification is a challenging task in computer vision. Although Deep\nNeural Networks (DNNs) have achieved excellent performance in video\nclassification, recent research shows adding imperceptible perturbations to\nclean videos can make the well-trained models output wrong labels with high\nconfidence. In this paper, we propose an effective defense framework to\ncharacterize and defend adversarial videos. The proposed method contains two\nphases: (1) adversarial video detection using temporal consistency between\nadjacent frames, and (2) adversarial perturbation reduction via denoisers in\nthe spatial and temporal domains respectively. Specifically, because of the\nlinear nature of DNNs, the imperceptible perturbations will enlarge with the\nincreasing of DNNs depth, which leads to the inconsistency of DNNs output\nbetween adjacent frames. However, the benign video frames often have the same\noutputs with their neighbor frames owing to the slight changes. Based on this\nobservation, we can distinguish between adversarial videos and benign videos.\nAfter that, we utilize different defense strategies against different attacks.\nWe propose the temporal defense, which reconstructs the polluted frames with\ntheir temporally neighbor clean frames, to deal with the adversarial videos\nwith sparse polluted frames. For the videos with dense polluted frames, we use\nan efficient adversarial denoiser to process each frame in the spatial domain,\nand thus purify the perturbations (we call it as spatial defense). A series of\nexperiments conducted on the UCF-101 dataset demonstrate that the proposed\nmethod significantly improves the robustness of video classifiers against\nadversarial attacks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 03:11:52 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Jia", "Xiaojun", ""], ["Wei", "Xingxing", ""], ["Cao", "Xiaochun", ""]]}, {"id": "1909.04839", "submitter": "Hang Yu", "authors": "Hang Yu, Aishan Liu, Xianglong Liu, Gengchao Li, Ping Luo, Ran Cheng,\n  Jichen Yang, Chongzhi Zhang", "title": "PDA: Progressive Data Augmentation for General Robustness of Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial images are designed to mislead deep neural networks (DNNs),\nattracting great attention in recent years. Although several defense strategies\nachieved encouraging robustness against adversarial samples, most of them fail\nto improve the robustness on common corruptions such as noise, blur, and\nweather/digital effects (e.g. frost, pixelate). To address this problem, we\npropose a simple yet effective method, named Progressive Data Augmentation\n(PDA), which enables general robustness of DNNs by progressively injecting\ndiverse adversarial noises during training. In other words, DNNs trained with\nPDA are able to obtain more robustness against both adversarial attacks as well\nas common corruptions than the recent state-of-the-art methods. We also find\nthat PDA is more efficient than prior arts and able to prevent accuracy drop on\nclean samples without being attacked. Furthermore, we theoretically show that\nPDA can control the perturbation bound and guarantee better generalization\nability than existing work. Extensive experiments on many benchmarks such as\nCIFAR-10, SVHN, and ImageNet demonstrate that PDA significantly outperforms its\ncounterparts in various experimental setups.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 03:27:54 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 05:01:22 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2020 11:58:05 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Yu", "Hang", ""], ["Liu", "Aishan", ""], ["Liu", "Xianglong", ""], ["Li", "Gengchao", ""], ["Luo", "Ping", ""], ["Cheng", "Ran", ""], ["Yang", "Jichen", ""], ["Zhang", "Chongzhi", ""]]}, {"id": "1909.04860", "submitter": "Chanho Ahn", "authors": "Chanho Ahn, Eunwoo Kim, Songhwai Oh", "title": "Deep Elastic Networks with Model Selection for Multi-Task Learning", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the problem of instance-wise dynamic network model\nselection for multi-task learning. To this end, we propose an efficient\napproach to exploit a compact but accurate model in a backbone architecture for\neach instance of all tasks. The proposed method consists of an estimator and a\nselector. The estimator is based on a backbone architecture and structured\nhierarchically. It can produce multiple different network models of different\nconfigurations in a hierarchical structure. The selector chooses a model\ndynamically from a pool of candidate models given an input instance. The\nselector is a relatively small-size network consisting of a few layers, which\nestimates a probability distribution over the candidate models when an input\ninstance of a task is given. Both estimator and selector are jointly trained in\na unified learning framework in conjunction with a sampling-based learning\nstrategy, without additional computation steps. We demonstrate the proposed\napproach for several image classification tasks compared to existing approaches\nperforming model selection or learning multiple tasks. Experimental results\nshow that our approach gives not only outstanding performance compared to other\ncompetitors but also the versatility to perform instance-wise model selection\nfor multiple tasks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 05:46:58 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Ahn", "Chanho", ""], ["Kim", "Eunwoo", ""], ["Oh", "Songhwai", ""]]}, {"id": "1909.04866", "submitter": "Dylan Campbell", "authors": "Stephen Gould, Richard Hartley and Dylan Campbell", "title": "Deep Declarative Networks: A New Hope", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a new class of end-to-end learnable models wherein data processing\nnodes (or network layers) are defined in terms of desired behavior rather than\nan explicit forward function. Specifically, the forward function is implicitly\ndefined as the solution to a mathematical optimization problem. Consistent with\nnomenclature in the programming languages community, we name these models deep\ndeclarative networks. Importantly, we show that the class of deep declarative\nnetworks subsumes current deep learning models. Moreover, invoking the implicit\nfunction theorem, we show how gradients can be back-propagated through many\ndeclaratively defined data processing nodes thereby enabling end-to-end\nlearning. We show how these declarative processing nodes can be implemented in\nthe popular PyTorch deep learning software library allowing declarative and\nimperative nodes to co-exist within the same network. We also provide numerous\ninsights and illustrative examples of declarative nodes and demonstrate their\napplication for image and point cloud classification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 06:19:25 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 03:56:39 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Gould", "Stephen", ""], ["Hartley", "Richard", ""], ["Campbell", "Dylan", ""]]}, {"id": "1909.04868", "submitter": "Chen Joya", "authors": "Joya Chen, Dong Liu, Tong Xu, Shiwei Wu, Yifei Chen, Enhong Chen", "title": "Is Heuristic Sampling Necessary in Training Deep Object Detectors?", "comments": "Tech Report (12 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To train accurate deep object detectors under the extreme\nforeground-background imbalance, heuristic sampling methods are always\nnecessary, which either re-sample a subset of all training samples (hard\nsampling methods, \\eg biased sampling, OHEM), or use all training samples but\nre-weight them discriminatively (soft sampling methods, \\eg Focal Loss, GHM).\nIn this paper, we challenge the necessity of such hard/soft sampling methods\nfor training accurate deep object detectors. While previous studies have shown\nthat training detectors without heuristic sampling methods would significantly\ndegrade accuracy, we reveal that this degradation comes from an unreasonable\nclassification gradient magnitude caused by the imbalance, rather than a lack\nof re-sampling/re-weighting. Motivated by our discovery, we propose a simple\nyet effective \\emph{Sampling-Free} mechanism to achieve a reasonable\nclassification gradient magnitude by initialization and loss scaling. Unlike\nheuristic sampling methods with multiple hyperparameters, our Sampling-Free\nmechanism is fully data diagnostic, without laborious hyperparameters\nsearching. We verify the effectiveness of our method in training anchor-based\nand anchor-free object detectors, where our method always achieves higher\ndetection accuracy than heuristic sampling methods on COCO and PASCAL VOC\ndatasets. Our Sampling-Free mechanism provides a new perspective to address the\nforeground-background imbalance. Our code will be released at\n\\url{https://github.com/ChenJoya/sampling-free}.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 06:29:58 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 05:52:11 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 02:11:33 GMT"}, {"version": "v4", "created": "Mon, 4 Nov 2019 17:53:00 GMT"}, {"version": "v5", "created": "Sat, 16 Nov 2019 07:43:58 GMT"}, {"version": "v6", "created": "Sat, 9 May 2020 15:41:23 GMT"}, {"version": "v7", "created": "Mon, 10 May 2021 05:25:56 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Chen", "Joya", ""], ["Liu", "Dong", ""], ["Xu", "Tong", ""], ["Wu", "Shiwei", ""], ["Chen", "Yifei", ""], ["Chen", "Enhong", ""]]}, {"id": "1909.04913", "submitter": "Jia Li", "authors": "Jia Li, Jinming Su, Changqun Xia, Yonghong Tian", "title": "Distortion-adaptive Salient Object Detection in 360$^\\circ$\n  Omnidirectional Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based salient object detection (SOD) has been extensively explored in\nthe past decades. However, SOD on 360$^\\circ$ omnidirectional images is less\nstudied owing to the lack of datasets with pixel-level annotations. Toward this\nend, this paper proposes a 360$^\\circ$ image-based SOD dataset that contains\n500 high-resolution equirectangular images. We collect the representative\nequirectangular images from five mainstream 360$^\\circ$ video datasets and\nmanually annotate all objects and regions over these images with precise masks\nwith a free-viewpoint way. To the best of our knowledge, it is the first public\navailable dataset for salient object detection on 360$^\\circ$ scenes. By\nobserving this dataset, we find that distortion from projection, large-scale\ncomplex scene and small salient objects are the most prominent characteristics.\nInspired by these foundings, this paper proposes a baseline model for SOD on\nequirectangular images. In the proposed approach, we construct a\ndistortion-adaptive module to deal with the distortion caused by the\nequirectangular projection. In addition, a multi-scale contextual integration\nblock is introduced to perceive and distinguish the rich scenes and objects in\nomnidirectional scenes. The whole network is organized in a progressively\nmanner with deep supervision. Experimental results show the proposed baseline\napproach outperforms the top-performanced state-of-the-art methods on\n360$^\\circ$ SOD dataset. Moreover, benchmarking results of the proposed\nbaseline approach and other methods on 360$^\\circ$ SOD dataset show the\nproposed dataset is very challenging, which also validate the usefulness of the\nproposed dataset and approach to boost the development of SOD on 360$^\\circ$\nomnidirectional scenes.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 08:33:11 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Li", "Jia", ""], ["Su", "Jinming", ""], ["Xia", "Changqun", ""], ["Tian", "Yonghong", ""]]}, {"id": "1909.04930", "submitter": "Mustafa Teke", "authors": "Mustafa Teke and Yasemin Yard{\\i}mc{\\i}", "title": "Multi-Year Vector Dynamic Time Warping Based Crop Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent automated crop mapping via supervised learning-based methods have\ndemonstrated unprecedented improvement over classical techniques. However, most\ncrop mapping studies are limited to same-year crop mapping in which the present\nyear's labeled data is used to predict the same year's crop map. Classification\naccuracies of these methods degrade considerably in cross-year mapping.\nCross-year crop mapping is more useful as it allows the prediction of the\nfollowing years' crop maps using previously labeled data. We propose Vector\nDynamic Time Warping (VDTW), a novel multi-year classification approach based\non warping of angular distances between phenological vectors. The results prove\nthat the proposed VDTW method is robust to temporal and spectral variations\ncompensating for different farming practices, climate and atmospheric effects,\nand measurement errors between years. We also describe a method for determining\nthe most discriminative time window that allows high classification accuracies\nwith limited data. We carried out tests of our approach with Landsat 8\ntime-series imagery from years 2013 to 2016 for classification of corn and\ncotton in the Harran Plain, and corn, cotton, and soybean in the Bismil Plain\nof Southeastern Turkey. In addition, we tested VDTW corn and soybean in Kansas,\nthe US for 2017 and 2018 with the Harmonized Landsat Sentinel data. The VDTW\nmethod achieved 99.85% and 99.74% overall accuracies for the same and cross\nyears, respectively with fewer training samples compared to other\nstate-of-the-art approaches, i.e. spectral angle mapper (SAM), dynamic time\nwarping (DTW), time-weighted DTW (TWDTW), random forest (RF), support vector\nmachine (SVM) and deep long short-term memory (LSTM) methods. The proposed\nmethod could be expanded for other crop types and/or geographical areas.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 09:05:05 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 08:28:22 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Teke", "Mustafa", ""], ["Yard\u0131mc\u0131", "Yasemin", ""]]}, {"id": "1909.04942", "submitter": "Peiliang Li", "authors": "Peiliang Li, Siqi Liu and Shaojie Shen", "title": "Multi-Sensor 3D Object Box Refinement for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a 3D object detection system with multi-sensor refinement in the\ncontext of autonomous driving. In our framework, the monocular camera serves as\nthe fundamental sensor for 2D object proposal and initial 3D bounding box\nprediction. While the stereo cameras and LiDAR are treated as adaptive plug-in\nsensors to refine the 3D box localization performance. For each observed\nelement in the raw measurement domain (e.g., pixels for stereo, 3D points for\nLiDAR), we model the local geometry as an instance vector representation, which\nindicates the 3D coordinate of each element respecting to the object frame.\nUsing this unified geometric representation, the 3D object location can be\nunified refined by the stereo photometric alignment or point cloud alignment.\nWe demonstrate superior 3D detection and localization performance compared to\nstate-of-the-art monocular, stereo methods and competitive performance compared\nwith the baseline LiDAR method on the KITTI object benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 09:38:56 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 05:36:55 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Li", "Peiliang", ""], ["Liu", "Siqi", ""], ["Shen", "Shaojie", ""]]}, {"id": "1909.04948", "submitter": "Timo Denk", "authors": "Timo I. Denk, Christian Reisswig", "title": "BERTgrid: Contextualized Embedding for 2D Document Representation and\n  Understanding", "comments": "4 pages, accepted at the \"Document Intelligence\" workshop of 33rd\n  Conference on Neural Information Processing Systems (NeurIPS 2019),\n  Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For understanding generic documents, information like font sizes, column\nlayout, and generally the positioning of words may carry semantic information\nthat is crucial for solving a downstream document intelligence task. Our novel\nBERTgrid, which is based on Chargrid by Katti et al. (2018), represents a\ndocument as a grid of contextualized word piece embedding vectors, thereby\nmaking its spatial structure and semantics accessible to the processing neural\nnetwork. The contextualized embedding vectors are retrieved from a BERT\nlanguage model. We use BERTgrid in combination with a fully convolutional\nnetwork on a semantic instance segmentation task for extracting fields from\ninvoices. We demonstrate its performance on tabulated line item and document\nheader field extraction.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 09:51:02 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 09:55:39 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Denk", "Timo I.", ""], ["Reisswig", "Christian", ""]]}, {"id": "1909.04951", "submitter": "Muhammad Haris Khan", "authors": "Muhammad Haris Khan, John McDonagh, Salman Khan, Muhammad Shahabuddin,\n  Aditya Arora, Fahad Shahbaz Khan, Ling Shao, Georgios Tzimiropoulos", "title": "AnimalWeb: A Large-Scale Hierarchical Dataset of Annotated Animal Faces", "comments": "15 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Being heavily reliant on animals, it is our ethical obligation to improve\ntheir well-being by understanding their needs. Several studies show that animal\nneeds are often expressed through their faces. Though remarkable progress has\nbeen made towards the automatic understanding of human faces, this has\nregrettably not been the case with animal faces. There exists significant room\nand appropriate need to develop automatic systems capable of interpreting\nanimal faces. Among many transformative impacts, such a technology will foster\nbetter and cheaper animal healthcare, and further advance animal psychology\nunderstanding.\n  We believe the underlying research progress is mainly obstructed by the lack\nof an adequately annotated dataset of animal faces, covering a wide spectrum of\nanimal species. To this end, we introduce a large-scale, hierarchical annotated\ndataset of animal faces, featuring 21.9K faces from 334 diverse species and 21\nanimal orders across biological taxonomy. These faces are captured\n`in-the-wild' conditions and are consistently annotated with 9 landmarks on key\nfacial features. The proposed dataset is structured and scalable by design; its\ndevelopment underwent four systematic stages involving rigorous, manual\nannotation effort of over 6K man-hours. We benchmark it for face alignment\nusing the existing art under novel problem settings. Results showcase its\nchallenging nature, unique attributes and present definite prospects for novel,\nadaptive, and generalized face-oriented CV algorithms. We further benchmark the\ndataset for face detection and fine-grained recognition tasks, to demonstrate\nmulti-task applications and room for improvement. Experiments indicate that\nthis dataset will push the algorithmic advancements across many related CV\ntasks and encourage the development of novel systems for animal facial\nbehaviour monitoring. We will make the dataset publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 09:55:56 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Khan", "Muhammad Haris", ""], ["McDonagh", "John", ""], ["Khan", "Salman", ""], ["Shahabuddin", "Muhammad", ""], ["Arora", "Aditya", ""], ["Khan", "Fahad Shahbaz", ""], ["Shao", "Ling", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "1909.04972", "submitter": "Zhaoyang Zeng", "authors": "Zhaoyang Zeng, Bei Liu, Jianlong Fu, Hongyang Chao and Lei Zhang", "title": "WSOD^2: Learning Bottom-up and Top-down Objectness Distillation for\n  Weakly-supervised Object Detection", "comments": "Accepted as a ICCV 2019 poster paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study on weakly-supervised object detection (WSOD) which plays a vital\nrole in relieving human involvement from object-level annotations. Predominant\nworks integrate region proposal mechanisms with convolutional neural networks\n(CNN). Although CNN is proficient in extracting discriminative local features,\ngrand challenges still exist to measure the likelihood of a bounding box\ncontaining a complete object (i.e., \"objectness\"). In this paper, we propose a\nnovel WSOD framework with Objectness Distillation (i.e., WSOD^2) by designing a\ntailored training mechanism for weakly-supervised object detection. Multiple\nregression targets are specifically determined by jointly considering bottom-up\n(BU) and top-down (TD) objectness from low-level measurement and CNN\nconfidences with an adaptive linear combination. As bounding box regression can\nfacilitate a region proposal learning to approach its regression target with\nhigh objectness during training, deep objectness representation learned from\nbottom-up evidences can be gradually distilled into CNN by optimization. We\nexplore different adaptive training curves for BU/TD objectness, and show that\nthe proposed WSOD^2 can achieve state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 11:08:55 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Zeng", "Zhaoyang", ""], ["Liu", "Bei", ""], ["Fu", "Jianlong", ""], ["Chao", "Hongyang", ""], ["Zhang", "Lei", ""]]}, {"id": "1909.04973", "submitter": "Bartosz Borucki", "authors": "Piotr Woznicki, Przemyslaw Przybyszewski, Norbert Kapinski, Jakub\n  Zielinski, Beata Ciszkowska-Lyson, Bartosz A. Borucki, Tomasz Trzcinski,\n  Krzysztof S. Nowinski", "title": "Monitoring Achilles tendon healing progress in ultrasound imaging with\n  convolutional neural networks", "comments": "Paper accepted to MICCAI'19 SUSI workshop", "journal-ref": null, "doi": "10.1007/978-3-030-32875-7_8", "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achilles tendon rupture is a debilitating injury, which is typically treated\nwith surgical repair and long-term rehabilitation. The recovery, however, is\nprotracted and often incomplete. Diagnosis, as well as healing progress\nassessment, are largely based on ultrasound and magnetic resonance imaging. In\nthis paper, we propose an automatic method based on deep learning for analysis\nof Achilles tendon condition and estimation of its healing progress on\nultrasound images. We develop custom convolutional neural networks for\nclassification and regression on healing score and feature extraction. Our\nmodels are trained and validated on an acquired dataset of over 250.000\nsagittal and over 450.000 axial ultrasound slices. The obtained estimates show\na high correlation with the assessment of expert radiologists, with respect to\nall key parameters describing healing progress. We also observe that parameters\nassociated with i.a. intratendinous healing processes are better modeled with\nsagittal slices. We prove that ultrasound imaging is quantitatively useful for\nclinical assessment of Achilles tendon healing process and should be viewed as\ncomplementary to magnetic resonance imaging.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 11:09:22 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Woznicki", "Piotr", ""], ["Przybyszewski", "Przemyslaw", ""], ["Kapinski", "Norbert", ""], ["Zielinski", "Jakub", ""], ["Ciszkowska-Lyson", "Beata", ""], ["Borucki", "Bartosz A.", ""], ["Trzcinski", "Tomasz", ""], ["Nowinski", "Krzysztof S.", ""]]}, {"id": "1909.04974", "submitter": "Richard Jiang", "authors": "Khan Faraz, Ahmed Bouridane, Richard Jiang, Tiancheng Xia, Paul\n  Chazot, Abdel Ennaceur", "title": "Computer-Aided Automated Detection of Gene-Controlled Social Actions of\n  Drosophila", "comments": "published on International Conference on Smart Cities at Cambridge\n  2018", "journal-ref": "International Conference on Smart Cities at Cambridge 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene expression of social actions in Drosophilae has been attracting wide\ninterest from biologists, medical scientists and psychologists. Gene-edited\nDrosophilae have been used as a test platform for experimental investigation.\nFor example, Parkinson's genes can be embedded into a group of newly bred\nDrosophilae for research purpose. However, human observation of numerous tiny\nDrosophilae for a long term is an arduous work, and the dependence on human's\nacute perception is highly unreliable. As a result, an automated system of\nsocial action detection using machine learning has been highly demanded. In\nthis study, we propose to automate the detection and classification of two\ninnate aggressive actions demonstrated by Drosophilae. Robust keypoint\ndetection is achieved using selective spatio-temporal interest points (sSTIP)\nwhich are then described using the 3D Scale Invariant Feature Transform\n(3D-SIFT) descriptors. Dimensionality reduction is performed using Spectral\nRegression Kernel Discriminant Analysis (SR-KDA) and classification is done\nusing the nearest centre rule. The classification accuracy shown demonstrates\nthe feasibility of the proposed system.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 11:15:10 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Faraz", "Khan", ""], ["Bouridane", "Ahmed", ""], ["Jiang", "Richard", ""], ["Xia", "Tiancheng", ""], ["Chazot", "Paul", ""], ["Ennaceur", "Abdel", ""]]}, {"id": "1909.04977", "submitter": "Zhaohui Yang", "authors": "Zhaohui Yang, Yunhe Wang, Xinghao Chen, Boxin Shi, Chao Xu, Chunjing\n  Xu, Qi Tian, Chang Xu", "title": "CARS: Continuous Evolution for Efficient Neural Architecture Search", "comments": "To be published in CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching techniques in most of existing neural architecture search (NAS)\nalgorithms are mainly dominated by differentiable methods for the efficiency\nreason. In contrast, we develop an efficient continuous evolutionary approach\nfor searching neural networks. Architectures in the population that share\nparameters within one SuperNet in the latest generation will be tuned over the\ntraining dataset with a few epochs. The searching in the next evolution\ngeneration will directly inherit both the SuperNet and the population, which\naccelerates the optimal network generation. The non-dominated sorting strategy\nis further applied to preserve only results on the Pareto front for accurately\nupdating the SuperNet. Several neural networks with different model sizes and\nperformances will be produced after the continuous search with only 0.4 GPU\ndays. As a result, our framework provides a series of networks with the number\nof parameters ranging from 3.7M to 5.1M under mobile settings. These networks\nsurpass those produced by the state-of-the-art methods on the benchmark\nImageNet dataset.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 11:17:22 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 03:18:12 GMT"}, {"version": "v3", "created": "Sun, 17 Nov 2019 04:25:21 GMT"}, {"version": "v4", "created": "Sun, 17 Nov 2019 04:51:51 GMT"}, {"version": "v5", "created": "Wed, 4 Mar 2020 09:13:01 GMT"}, {"version": "v6", "created": "Mon, 9 Mar 2020 09:11:53 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Yang", "Zhaohui", ""], ["Wang", "Yunhe", ""], ["Chen", "Xinghao", ""], ["Shi", "Boxin", ""], ["Xu", "Chao", ""], ["Xu", "Chunjing", ""], ["Tian", "Qi", ""], ["Xu", "Chang", ""]]}, {"id": "1909.04988", "submitter": "Zhaoxiang Liu", "authors": "Zipeng Wang, Zhaoxiang Liu, Jianfeng Huang, Shiguo Lian, Yimin Lin", "title": "How Old Are You? Face Age Translation with Identity Preservation Using\n  GANs", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework to generate images of different age while\npreserving identity information, which is known as face aging. Different from\nmost recent popular face aging networks utilizing Generative Adversarial\nNetworks(GANs) application, our approach do not simply transfer a young face to\nan old one. Instead, we employ the edge map as intermediate representations,\nfirstly edge maps of young faces are extracted, a CycleGAN-based network is\nadopted to transfer them into edge maps of old faces, then another\npix2pixHD-based network is adopted to transfer the synthesized edge maps,\nconcatenated with identity information, into old faces. In this way, our method\ncan generate more realistic transfered images, simultaneously ensuring that\nface identity information be preserved well, and the apparent age of the\ngenerated image be accurately appropriate. Experimental results demonstrate\nthat our method is feasible for face age translation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 11:55:23 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Wang", "Zipeng", ""], ["Liu", "Zhaoxiang", ""], ["Huang", "Jianfeng", ""], ["Lian", "Shiguo", ""], ["Lin", "Yimin", ""]]}, {"id": "1909.04999", "submitter": "Yongseok Choi", "authors": "Yongseok Choi, Junyoung Park, Subin Yi, Dong-Yeon Cho", "title": "Domain-Agnostic Few-Shot Classification by Learning Disparate Modulators", "comments": "Presented at NeurIPS 2019 Workshop on Meta-Learning (MetaLearn 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although few-shot learning research has advanced rapidly with the help of\nmeta-learning, its practical usefulness is still limited because most of them\nassumed that all meta-training and meta-testing examples came from a single\ndomain. We propose a simple but effective way for few-shot classification in\nwhich a task distribution spans multiple domains including ones never seen\nduring meta-training. The key idea is to build a pool of models to cover this\nwide task distribution and learn to select the best one for a particular task\nthrough cross-domain meta-learning. All models in the pool share a base network\nwhile each model has a separate modulator to refine the base network in its own\nway. This framework allows the pool to have representational diversity without\nlosing beneficial domain-invariant features. We verify the effectiveness of the\nproposed algorithm through experiments on various datasets across diverse\ndomains.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 12:18:15 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 12:09:35 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Choi", "Yongseok", ""], ["Park", "Junyoung", ""], ["Yi", "Subin", ""], ["Cho", "Dong-Yeon", ""]]}, {"id": "1909.05003", "submitter": "Ali Shafti", "authors": "Alexander Makrigiorgos, Ali Shafti, Alex Harston, Julien Gerard, A.\n  Aldo Faisal", "title": "Human Visual Attention Prediction Boosts Learning & Performance of\n  Autonomous Driving Agents", "comments": "7 pages, 6 figures, 2 tables. Submitted to IEEE RA-L with ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving is a multi-task problem requiring a deep understanding of\nthe visual environment. End-to-end autonomous systems have attracted increasing\ninterest as a method of learning to drive without exhaustively programming\nbehaviours for different driving scenarios. When humans drive, they rely on a\nfinely tuned sensory system which enables them to quickly acquire the\ninformation they need while filtering unnecessary details. This ability to\nidentify task-specific high-interest regions within an image could be\nbeneficial to autonomous driving agents and machine learning systems in\ngeneral. To create a system capable of imitating human gaze patterns and visual\nattention, we collect eye movement data from human drivers in a virtual reality\nenvironment. We use this data to train deep neural networks predicting where\nhumans are most likely to look when driving. We then use the outputs of this\ntrained network to selectively mask driving images using a variety of masking\ntechniques. Finally, autonomous driving agents are trained using these masked\nimages as input. Upon comparison, we found that a dual-branch architecture\nwhich processes both raw and attention-masked images substantially outperforms\nall other models, reducing error in control signal predictions by 25.5\\%\ncompared to a standard end-to-end model trained only on raw images.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 12:25:22 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Makrigiorgos", "Alexander", ""], ["Shafti", "Ali", ""], ["Harston", "Alex", ""], ["Gerard", "Julien", ""], ["Faisal", "A. Aldo", ""]]}, {"id": "1909.05010", "submitter": "Jingwen Wang", "authors": "Jingwen Wang, Lin Ma, Wenhao Jiang", "title": "Temporally Grounding Language Queries in Videos by Contextual\n  Boundary-aware Prediction", "comments": "Accepted to AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of temporally grounding language queries in videos is to temporally\nlocalize the best matched video segment corresponding to a given language\n(sentence). It requires certain models to simultaneously perform visual and\nlinguistic understandings. Previous work predominantly ignores the precision of\nsegment localization. Sliding window based methods use predefined search window\nsizes, which suffer from redundant computation, while existing anchor-based\napproaches fail to yield precise localization. We address this issue by\nproposing an end-to-end boundary-aware model, which uses a lightweight branch\nto predict semantic boundaries corresponding to the given linguistic\ninformation. To better detect semantic boundaries, we propose to aggregate\ncontextual information by explicitly modeling the relationship between the\ncurrent element and its neighbors. The most confident segments are subsequently\nselected based on both anchor and boundary predictions at the testing stage.\nThe proposed model, dubbed Contextual Boundary-aware Prediction (CBP),\noutperforms its competitors with a clear margin on three public datasets. All\ncodes are available on https://github.com/JaywongWang/CBP .\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 12:38:38 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 03:18:44 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Wang", "Jingwen", ""], ["Ma", "Lin", ""], ["Jiang", "Wenhao", ""]]}, {"id": "1909.05024", "submitter": "Lu Liu", "authors": "Lu Liu, Tianyi Zhou, Guodong Long, Jing Jiang, Chengqi Zhang", "title": "Learning to Propagate for Graph Meta-Learning", "comments": "Accepted to NeurIPS 2019, code at\n  https://github.com/liulu112601/Gated-Propagation-Net, slides at\n  https://liulu112601.github.io/resources/GPN-NeurIPS-Slides-revised.pdf,\n  Poster at\n  https://liulu112601.github.io/resources/Graph-Meta-Learning-Poster-revised.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning extracts common knowledge from learning different tasks and\nuses it for unseen tasks. It can significantly improve tasks that suffer from\ninsufficient training data, e.g., few shot learning. In most meta-learning\nmethods, tasks are implicitly related by sharing parameters or optimizer. In\nthis paper, we show that a meta-learner that explicitly relates tasks on a\ngraph describing the relations of their output dimensions (e.g., classes) can\nsignificantly improve few shot learning. The graph's structure is usually free\nor cheap to obtain but has rarely been explored in previous works. We develop a\nnovel meta-learner of this type for prototype-based classification, in which a\nprototype is generated for each class, such that the nearest neighbor search\namong the prototypes produces an accurate classification. The meta-learner,\ncalled \"Gated Propagation Network (GPN)\", learns to propagate messages between\nprototypes of different classes on the graph, so that learning the prototype of\neach class benefits from the data of other related classes. In GPN, an\nattention mechanism aggregates messages from neighboring classes of each class,\nwith a gate choosing between the aggregated message and the message from the\nclass itself. We train GPN on a sequence of tasks from many-shot to few shot\ngenerated by subgraph sampling. During training, it is able to reuse and update\npreviously achieved prototypes from the memory in a life-long learning cycle.\nIn experiments, under different training-test discrepancy and test task\ngeneration settings, GPN outperforms recent meta-learning methods on two\nbenchmark datasets. The code of GPN and dataset generation is available at\nhttps://github.com/liulu112601/Gated-Propagation-Net.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 13:00:24 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 08:46:48 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Liu", "Lu", ""], ["Zhou", "Tianyi", ""], ["Long", "Guodong", ""], ["Jiang", "Jing", ""], ["Zhang", "Chengqi", ""]]}, {"id": "1909.05040", "submitter": "Francesco Croce", "authors": "Francesco Croce, Matthias Hein", "title": "Sparse and Imperceivable Adversarial Attacks", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have been proven to be vulnerable to a variety of adversarial\nattacks. From a safety perspective, highly sparse adversarial attacks are\nparticularly dangerous. On the other hand the pixelwise perturbations of sparse\nattacks are typically large and thus can be potentially detected. We propose a\nnew black-box technique to craft adversarial examples aiming at minimizing\n$l_0$-distance to the original image. Extensive experiments show that our\nattack is better or competitive to the state of the art. Moreover, we can\nintegrate additional bounds on the componentwise perturbation. Allowing pixels\nto change only in region of high variation and avoiding changes along\naxis-aligned edges makes our adversarial examples almost non-perceivable.\nMoreover, we adapt the Projected Gradient Descent attack to the $l_0$-norm\nintegrating componentwise constraints. This allows us to do adversarial\ntraining to enhance the robustness of classifiers against sparse and\nimperceivable adversarial manipulations.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 13:28:44 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Croce", "Francesco", ""], ["Hein", "Matthias", ""]]}, {"id": "1909.05054", "submitter": "Jue Jiang Dr.", "authors": "Jue Jiang, Elguindi Sharif, Hyemin Um, Sean Berry, Harini\n  Veeraraghavan", "title": "Local block-wise self attention for normal organ segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a new and computationally simple local block-wise self attention\nbased normal structures segmentation approach applied to head and neck computed\ntomography (CT) images. Our method uses the insight that normal organs exhibit\nregularity in their spatial location and inter-relation within images, which\ncan be leveraged to simplify the computations required to aggregate feature\ninformation. We accomplish this by using local self attention blocks that pass\ninformation between each other to derive the attention map. We show that adding\nadditional attention layers increases the contextual field and captures focused\nattention from relevant structures. We developed our approach using U-net and\ncompared it against multiple state-of-the-art self attention methods. All\nmodels were trained on 48 internal headneck CT scans and tested on 48 CT scans\nfrom the external public domain database of computational anatomy dataset. Our\nmethod achieved the highest Dice similarity coefficient segmentation accuracy\nof 0.85$\\pm$0.04, 0.86$\\pm$0.04 for left and right parotid glands,\n0.79$\\pm$0.07 and 0.77$\\pm$0.05 for left and right submandibular glands,\n0.93$\\pm$0.01 for mandible and 0.88$\\pm$0.02 for the brain stem with the lowest\nincrease of 66.7\\% computing time per image and 0.15\\% increase in model\nparameters compared with standard U-net. The best state-of-the-art method\ncalled point-wise spatial attention, achieved \\textcolor{black}{comparable\naccuracy but with 516.7\\% increase in computing time and 8.14\\% increase in\nparameters compared with standard U-net.} Finally, we performed ablation tests\nand studied the impact of attention block size, overlap of the attention\nblocks, additional attention layers, and attention block placement on\nsegmentation performance.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 13:50:53 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Jiang", "Jue", ""], ["Sharif", "Elguindi", ""], ["Um", "Hyemin", ""], ["Berry", "Sean", ""], ["Veeraraghavan", "Harini", ""]]}, {"id": "1909.05073", "submitter": "Xiaolong Ma", "authors": "Xiaolong Ma, Fu-Ming Guo, Wei Niu, Xue Lin, Jian Tang, Kaisheng Ma,\n  Bin Ren, Yanzhi Wang", "title": "PCONV: The Missing but Desirable Sparsity in DNN Weight Pruning for\n  Real-time Execution on Mobile Devices", "comments": "To appear in Proceedings of the 34th AAAI Conference on Artificial\n  Intelligence (AAAI-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model compression techniques on Deep Neural Network (DNN) have been widely\nacknowledged as an effective way to achieve acceleration on a variety of\nplatforms, and DNN weight pruning is a straightforward and effective method.\nThere are currently two mainstreams of pruning methods representing two\nextremes of pruning regularity: non-structured, fine-grained pruning can\nachieve high sparsity and accuracy, but is not hardware friendly; structured,\ncoarse-grained pruning exploits hardware-efficient structures in pruning, but\nsuffers from accuracy drop when the pruning rate is high. In this paper, we\nintroduce PCONV, comprising a new sparsity dimension, -- fine-grained pruning\npatterns inside the coarse-grained structures. PCONV comprises two types of\nsparsities, Sparse Convolution Patterns (SCP) which is generated from\nintra-convolution kernel pruning and connectivity sparsity generated from\ninter-convolution kernel pruning. Essentially, SCP enhances accuracy due to its\nspecial vision properties, and connectivity sparsity increases pruning rate\nwhile maintaining balanced workload on filter computation. To deploy PCONV, we\ndevelop a novel compiler-assisted DNN inference framework and execute PCONV\nmodels in real-time without accuracy compromise, which cannot be achieved in\nprior work. Our experimental results show that, PCONV outperforms three\nstate-of-art end-to-end DNN frameworks, TensorFlow-Lite, TVM, and Alibaba\nMobile Neural Network with speedup up to 39.2x, 11.4x, and 6.3x, respectively,\nwith no accuracy loss. Mobile devices can achieve real-time inference on\nlarge-scale DNNs.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 03:58:29 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 01:33:36 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 00:18:07 GMT"}, {"version": "v4", "created": "Wed, 4 Mar 2020 19:39:06 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Ma", "Xiaolong", ""], ["Guo", "Fu-Ming", ""], ["Niu", "Wei", ""], ["Lin", "Xue", ""], ["Tang", "Jian", ""], ["Ma", "Kaisheng", ""], ["Ren", "Bin", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1909.05084", "submitter": "Amit Gurung", "authors": "Amit Gurung and Sangyal Lama Tamang", "title": "Image Segmentation using Multi-Threshold technique by Histogram Sampling", "comments": "28 pages, 6 figures, 3 tables, unpublished work, in a process to\n  submit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of digital images is one of the essential steps in image\nprocessing or a computer vision system. It helps in separating the pixels into\ndifferent regions according to their intensity level. A large number of\nsegmentation techniques have been proposed, and a few of them use complex\ncomputational operations. Among all, the most straightforward procedure that\ncan be easily implemented is thresholding. In this paper, we present a unique\nheuristic approach for image segmentation that automatically determines\nmultilevel thresholds by sampling the histogram of a digital image. Our\napproach emphasis on selecting a valley as optimal threshold values. We\ndemonstrated that our approach outperforms the popular Otsu's method in terms\nof CPU computational time. We demonstrated that our approach outperforms the\npopular Otsu's method in terms of CPU computational time. We observed a maximum\nspeed-up of 35.58x and a minimum speed-up of 10.21x on popular image processing\nbenchmarks. To demonstrate the correctness of our approach in determining\nthreshold values, we compute PSNR, SSIM, and FSIM values to compare with the\nvalues obtained by Otsu's method. This evaluation shows that our approach is\ncomparable and better in many cases as compared to well known Otsu's method.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 14:37:47 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Gurung", "Amit", ""], ["Tamang", "Sangyal Lama", ""]]}, {"id": "1909.05085", "submitter": "Dennis Bontempi", "authors": "Dennis Bontempi, Sergio Benini, Alberto Signoroni, Michele Svanera,\n  Lars Muckli", "title": "CEREBRUM: a fast and fully-volumetric Convolutional Encoder-decodeR for\n  weakly-supervised sEgmentation of BRain strUctures from out-of-the-scanner\n  MRI", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many functional and structural neuroimaging studies call for accurate\nmorphometric segmentation of different brain structures starting from image\nintensity values of MRI scans. Current automatic (multi-) atlas-based\nsegmentation strategies often lack accuracy on difficult-to-segment brain\nstructures and, since these methods rely on atlas-to-scan alignment, they may\ntake long processing times. Recently, methods deploying solutions based on\nConvolutional Neural Networks (CNNs) are making the direct analysis of\nout-of-the-scanner data feasible. However, current CNN-based solutions\npartition the test volume into 2D or 3D patches, which are processed\nindependently. This entails a loss of global contextual information thereby\nnegatively impacting the segmentation accuracy. In this work, we design and\ntest an optimised end-to-end CNN architecture that makes the exploitation of\nglobal spatial information computationally tractable, allowing to process a\nwhole MRI volume at once. We adopt a weakly supervised learning strategy by\nexploiting a large dataset composed by 947 out-of-the-scanner (3 Tesla\nT1-weighted 1mm isotropic MP-RAGE 3D sequences) MR Images. The resulting model\nis able to produce accurate multi-structure segmentation results in only few\nseconds. Different quantitative measures demonstrate an improved accuracy of\nour solution when compared to state-of-the-art techniques. Moreover, through a\nrandomised survey involving expert neuroscientists, we show that subjective\njudgements clearly prefer our solution with respect to the widely adopted\natlas-based FreeSurfer software.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 14:40:30 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 13:01:41 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Bontempi", "Dennis", ""], ["Benini", "Sergio", ""], ["Signoroni", "Alberto", ""], ["Svanera", "Michele", ""], ["Muckli", "Lars", ""]]}, {"id": "1909.05090", "submitter": "Kun Zhang", "authors": "Kun Zhang, Peng He, Ping Yao, Ge Chen, Rui Wu, Min Du, Huimin Li, Li\n  Fu, Tianyao Zheng", "title": "Learning Enhanced Resolution-wise features for Human Pose Estimation", "comments": "Published on ICIP 2020", "journal-ref": null, "doi": "10.1109/ICIP40778.2020.9191174", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, multi-resolution networks (such as Hourglass, CPN, HRNet, etc.)\nhave achieved significant performance on pose estimation by combining feature\nmaps of various resolutions. In this paper, we propose a Resolution-wise\nAttention Module (RAM) and Gradual Pyramid Refinement (GPR), to learn enhanced\nresolution-wise feature maps for precise pose estimation. Specifically, RAM\nlearns a group of weights to represent the different importance of feature maps\nacross resolutions, and the GPR gradually merges every two feature maps from\nlow to high resolutions to regress final human keypoint heatmaps. With the\nenhanced resolution-wise features learnt by CNN, we obtain more accurate human\nkeypoint locations. The efficacies of our proposed methods are demonstrated on\nMS-COCO dataset, achieving state-of-the-art performance with average precision\nof 77.7 on COCO val2017 set and 77.0 on test-dev2017 set without using extra\nhuman keypoint training dataset.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 14:46:28 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 06:43:59 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2019 09:53:05 GMT"}, {"version": "v4", "created": "Sun, 13 Dec 2020 15:22:41 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zhang", "Kun", ""], ["He", "Peng", ""], ["Yao", "Ping", ""], ["Chen", "Ge", ""], ["Wu", "Rui", ""], ["Du", "Min", ""], ["Li", "Huimin", ""], ["Fu", "Li", ""], ["Zheng", "Tianyao", ""]]}, {"id": "1909.05145", "submitter": "Vinay Kumar Venkataramana", "authors": "Vinay Kumar V and P Nagabhushan", "title": "Appearance invariant Entry-Exit matching using visual soft biometric\n  traits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of appearance invariant subject recognition for Entry-Exit\nsurveillance applications is addressed. A novel Semantic Entry-Exit matching\nmodel that makes use of ancillary information about subjects such as height,\nbuild, complexion and clothing color to endorse exit of every subject who had\nentered private area is proposed in this paper. The proposed method is robust\nto variations in clothing. Each describing attribute is given equal weight\nwhile computing the matching score and hence the proposed model achieves high\nrank-k accuracy on benchmark datasets. The soft biometric traits used as a\ncombination though cannot achieve high rank-1 accuracy, it helps to narrow down\nthe search to match using reliable biometric traits such as gait and face whose\nlearning and matching time is costlier when compared to the visual soft\nbiometrics.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 07:04:58 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Kumar", "Vinay", "V"], ["Nagabhushan", "P", ""]]}, {"id": "1909.05146", "submitter": "Amarnath R", "authors": "Amarnath R, P. Nagabhushan and Mohammed Javed", "title": "Word and character segmentation directly in run-length compressed\n  handwritten document images", "comments": "17 pages,19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From the literature, it is demonstrated that performing text-line\nsegmentation directly in the run-length compressed handwritten document images\nsignificantly reduces the computational time and memory space. In this paper,\nwe investigate the issues of word and character segmentation directly on the\nrun-length compressed document images. Primarily, the spreads of the characters\nare intelligently extracted from the foreground runs of the compressed data and\nsubsequently connected components are established. The spacing between the\nconnected components would be larger between the adjacent words when compared\nto that of intra-words. With this knowledge, a threshold is empirically chosen\nfor inter-word separation. Every connected component within a word is further\nanalysed for character segmentation. Here, min-cut graph concept is used for\nseparating the touching characters. Over-segmentation and under-segmentation\nissues are addressed by insertion and deletion operations respectively. The\napproach has been developed particularly for compressed handwritten English\ndocument images. However, the model has been tested on non-English document\nimages.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 09:48:52 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["R", "Amarnath", ""], ["Nagabhushan", "P.", ""], ["Javed", "Mohammed", ""]]}, {"id": "1909.05152", "submitter": "Alireza Rahimpour", "authors": "Alireza Rahimpour, Sujitha Martin, Ashish Tawari, Hairong Qi", "title": "Context Aware Road-user Importance Estimation (iCARE)", "comments": "Published in: IEEE Intelligent Vehicles (IV), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road-users are a critical part of decision-making for both self-driving cars\nand driver assistance systems. Some road-users, however, are more important for\ndecision-making than others because of their respective intentions, ego\nvehicle's intention and their effects on each other. In this paper, we propose\na novel architecture for road-user importance estimation which takes advantage\nof the local and global context of the scene. For local context, the model\nexploits the appearance of the road users (which captures orientation,\nintention, etc.) and their location relative to ego-vehicle. The global context\nin our model is defined based on the feature map of the convolutional layer of\nthe module which predicts the future path of the ego-vehicle and contains rich\nglobal information of the scene (e.g., infrastructure, road lanes, etc.), as\nwell as the ego vehicle's intention information. Moreover, this paper\nintroduces a new data set of real-world driving, concentrated around\ninter-sections and includes annotations of important road users. Systematic\nevaluations of our proposed method against several baselines show promising\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 05:54:44 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Rahimpour", "Alireza", ""], ["Martin", "Sujitha", ""], ["Tawari", "Ashish", ""], ["Qi", "Hairong", ""]]}, {"id": "1909.05163", "submitter": "Ziqi Wang", "authors": "Ziqi Wang, Jiahui Li, Seyran Khademi, Jan van Gemert", "title": "Attention-Aware Age-Agnostic Visual Place Recognition", "comments": "Presented at ICCV WORKSHOP ON E-HERITAGE 2019, Seoul, South Korea", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cross-domain visual place recognition (VPR) task is proposed in this work,\ni.e., matching images of the same architectures depicted in different domains.\nVPR is commonly treated as an image retrieval task, where a query image from an\nunknown location is matched with relevant instances from geo-tagged gallery\ndatabase. Different from conventional VPR settings where the query images and\ngallery images come from the same domain, we propose a more common but\nchallenging setup where the query images are collected under a new unseen\ncondition. The two domains involved in this work are contemporary street view\nimages of Amsterdam from the Mapillary dataset (source domain) and historical\nimages of the same city from Beeldbank dataset (target domain). We tailored an\nage-invariant feature learning CNN that can focus on domain invariant objects\nand learn to match images based on a weakly supervised ranking loss. We propose\nan attention aggregation module that is robust to domain discrepancy between\nthe train and the test data. Further, a multi-kernel maximum mean discrepancy\n(MK-MMD) domain adaptation loss is adopted to improve the cross-domain ranking\nperformance. Both attention and adaptation modules are unsupervised while the\nranking loss uses weak supervision. Visual inspection shows that the attention\nmodule focuses on built forms while the dramatically changing environment are\nless weighed. Our proposed CNN achieves state of the art results (99% accuracy)\non the single-domain VPR task and 20% accuracy at its best on the cross-domain\nVPR task, revealing the difficulty of age-invariant VPR.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 16:04:42 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Wang", "Ziqi", ""], ["Li", "Jiahui", ""], ["Khademi", "Seyran", ""], ["van Gemert", "Jan", ""]]}, {"id": "1909.05165", "submitter": "Okan K\\\"op\\\"ukl\\\"u", "authors": "Okan K\\\"op\\\"ukl\\\"u, Fabian Herzog, Gerhard Rigoll", "title": "Comparative Analysis of CNN-based Spatiotemporal Reasoning in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding actions and gestures in video streams requires temporal\nreasoning of the spatial content from different time instants, i.e.,\nspatiotemporal (ST) modeling. In this survey paper, we have made a comparative\nanalysis of different ST modeling techniques for action and gecture recognition\ntasks. Since Convolutional Neural Networks (CNNs) are proved to be an effective\ntool as a feature extractor for static images, we apply ST modeling techniques\non the features of static images from different time instants extracted by\nCNNs. All techniques are trained end-to-end together with a CNN feature\nextraction part and evaluated on two publicly available benchmarks: The Jester\nand the Something-Something datasets. The Jester dataset contains various\ndynamic and static hand gestures, whereas the Something-Something dataset\ncontains actions of human-object interactions. The common characteristic of\nthese two benchmarks is that the designed architectures need to capture the\nfull temporal content of videos in order to correctly classify\nactions/gestures. Contrary to expectations, experimental results show that\nRecurrent Neural Network (RNN) based ST modeling techniques yield inferior\nresults compared to other techniques such as fully convolutional architectures.\nCodes and pretrained models of this work are publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 16:10:00 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 13:53:46 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["K\u00f6p\u00fckl\u00fc", "Okan", ""], ["Herzog", "Fabian", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "1909.05184", "submitter": "Chen Xihao", "authors": "Xihao Chen and Jingya Yu and Li Chen and Shaoqun Zeng and Xiuli Liu\n  and Shenghua Cheng", "title": "Multi-stage domain adversarial style reconstruction for cytopathological\n  image stain normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The different stain styles of cytopathological images have a negative effect\non the generalization ability of automated image analysis algorithms. This\narticle proposes a new framework that normalizes the stain style for\ncytopathological images through a stain removal module and a multi-stage domain\nadversarial style reconstruction module. We convert colorful images into\ngrayscale images with a color-encoding mask. Using the mask, reconstructed\nimages retain their basic color without red and blue mixing, which is important\nfor cytopathological image interpretation. The style reconstruction module\nconsists of per-pixel regression with intradomain adversarial learning,\ninter-domain adversarial learning, and optional task-based refining. Per-pixel\nregression with intradomain adversarial learning establishes the generative\nnetwork from the decolorized input to the reconstructed output. The interdomain\nadversarial learning further reduces the difference in stain style. The\ngeneration network can be optimized by combining it with the task network.\nExperimental results show that the proposed techniques help to optimize the\ngeneration network. The average accuracy increases from 75.41% to 84.79% after\nthe intra-domain adversarial learning, and to 87.00% after interdomain\nadversarial learning. Under the guidance of the task network, the average\naccuracy rate reaches 89.58%. The proposed method achieves unsupervised stain\nnormalization of cytopathological images, while preserving the cell structure,\ntexture structure, and cell color properties of the image. This method\novercomes the problem of generalizing the task models between different stain\nstyles of cytopathological images.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 16:34:25 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Chen", "Xihao", ""], ["Yu", "Jingya", ""], ["Chen", "Li", ""], ["Zeng", "Shaoqun", ""], ["Liu", "Xiuli", ""], ["Cheng", "Shenghua", ""]]}, {"id": "1909.05215", "submitter": "Ellen Zhong", "authors": "Ellen D. Zhong, Tristan Bepler, Joseph H. Davis, Bonnie Berger", "title": "Reconstructing continuous distributions of 3D protein structure from\n  cryo-EM images", "comments": null, "journal-ref": "International Conference on Learning Representations (ICLR), 2020", "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryo-electron microscopy (cryo-EM) is a powerful technique for determining\nthe structure of proteins and other macromolecular complexes at near-atomic\nresolution. In single particle cryo-EM, the central problem is to reconstruct\nthe three-dimensional structure of a macromolecule from $10^{4-7}$ noisy and\nrandomly oriented two-dimensional projections. However, the imaged protein\ncomplexes may exhibit structural variability, which complicates reconstruction\nand is typically addressed using discrete clustering approaches that fail to\ncapture the full range of protein dynamics. Here, we introduce a novel method\nfor cryo-EM reconstruction that extends naturally to modeling continuous\ngenerative factors of structural heterogeneity. This method encodes structures\nin Fourier space using coordinate-based deep neural networks, and trains these\nnetworks from unlabeled 2D cryo-EM images by combining exact inference over\nimage orientation with variational inference for structural heterogeneity. We\ndemonstrate that the proposed method, termed cryoDRGN, can perform ab initio\nreconstruction of 3D protein complexes from simulated and real 2D cryo-EM image\ndata. To our knowledge, cryoDRGN is the first neural network-based approach for\ncryo-EM reconstruction and the first end-to-end method for directly\nreconstructing continuous ensembles of protein structures from cryo-EM images.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 17:13:06 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 23:45:23 GMT"}, {"version": "v3", "created": "Sat, 15 Feb 2020 04:31:46 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Zhong", "Ellen D.", ""], ["Bepler", "Tristan", ""], ["Davis", "Joseph H.", ""], ["Berger", "Bonnie", ""]]}, {"id": "1909.05235", "submitter": "Qi Qian", "authors": "Qi Qian, Lei Shang, Baigui Sun, Juhua Hu, Hao Li, Rong Jin", "title": "SoftTriple Loss: Deep Metric Learning Without Triplet Sampling", "comments": "accepted by ICCV'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance metric learning (DML) is to learn the embeddings where examples from\nthe same class are closer than examples from different classes. It can be cast\nas an optimization problem with triplet constraints. Due to the vast number of\ntriplet constraints, a sampling strategy is essential for DML. With the\ntremendous success of deep learning in classifications, it has been applied for\nDML. When learning embeddings with deep neural networks (DNNs), only a\nmini-batch of data is available at each iteration. The set of triplet\nconstraints has to be sampled within the mini-batch. Since a mini-batch cannot\ncapture the neighbors in the original set well, it makes the learned embeddings\nsub-optimal. On the contrary, optimizing SoftMax loss, which is a\nclassification loss, with DNN shows a superior performance in certain DML\ntasks. It inspires us to investigate the formulation of SoftMax. Our analysis\nshows that SoftMax loss is equivalent to a smoothed triplet loss where each\nclass has a single center. In real-world data, one class can contain several\nlocal clusters rather than a single one, e.g., birds of different poses.\nTherefore, we propose the SoftTriple loss to extend the SoftMax loss with\nmultiple centers for each class. Compared with conventional deep metric\nlearning algorithms, optimizing SoftTriple loss can learn the embeddings\nwithout the sampling phase by mildly increasing the size of the last fully\nconnected layer. Experiments on the benchmark fine-grained data sets\ndemonstrate the effectiveness of the proposed loss function. Code is available\nat https://github.com/idstcv/SoftTriple\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 17:47:25 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 02:17:42 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Qian", "Qi", ""], ["Shang", "Lei", ""], ["Sun", "Baigui", ""], ["Hu", "Juhua", ""], ["Li", "Hao", ""], ["Jin", "Rong", ""]]}, {"id": "1909.05249", "submitter": "Sean Moran", "authors": "Hao Guan, Liu Liu, Sean Moran, Fenglong Song, Gregory Slabaugh", "title": "NODE: Extreme Low Light Raw Image Denoising using a Noise Decomposition\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Denoising extreme low light images is a challenging task due to the high\nnoise level. When the illumination is low, digital cameras increase the ISO\n(electronic gain) to amplify the brightness of captured data. However, this in\nturn amplifies the noise, arising from read, shot, and defective pixel sources.\nIn the raw domain, read and shot noise are effectively modelled using Gaussian\nand Poisson distributions respectively, whereas defective pixels can be modeled\nwith impulsive noise. In extreme low light imaging, noise removal becomes a\ncritical challenge to produce a high quality, detailed image with low noise. In\nthis paper, we propose a multi-task deep neural network called Noise\nDecomposition (NODE) that explicitly and separately estimates defective pixel\nnoise, in conjunction with Gaussian and Poisson noise, to denoise an extreme\nlow light image. Our network is purposely designed to work with raw data, for\nwhich the noise is more easily modeled before going through non-linear\ntransformations in the image signal processing (ISP) pipeline. Quantitative and\nqualitative evaluation show the proposed method to be more effective at\ndenoising real raw images than state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 17:27:34 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Guan", "Hao", ""], ["Liu", "Liu", ""], ["Moran", "Sean", ""], ["Song", "Fenglong", ""], ["Slabaugh", "Gregory", ""]]}, {"id": "1909.05305", "submitter": "Kamyar Nazeri", "authors": "Kamyar Nazeri, Harrish Thasarathan, Mehran Ebrahimi", "title": "Edge-Informed Single Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The recent increase in the extensive use of digital imaging technologies has\nbrought with it a simultaneous demand for higher-resolution images. We develop\na novel edge-informed approach to single image super-resolution (SISR). The\nSISR problem is reformulated as an image inpainting task. We use a two-stage\ninpainting model as a baseline for super-resolution and show its effectiveness\nfor different scale factors (x2, x4, x8) compared to basic interpolation\nschemes. This model is trained using a joint optimization of image contents\n(texture and color) and structures (edges). Quantitative and qualitative\ncomparisons are included and the proposed model is compared with current\nstate-of-the-art techniques. We show that our method of decoupling structure\nand texture reconstruction improves the quality of the final reconstructed\nhigh-resolution image. Code and models available at:\nhttps://github.com/knazeri/edge-informed-sisr\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 18:54:41 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Nazeri", "Kamyar", ""], ["Thasarathan", "Harrish", ""], ["Ebrahimi", "Mehran", ""]]}, {"id": "1909.05321", "submitter": "Riqiang Gao", "authors": "Riqiang Gao, Yuankai Huo, Shunxing Bao, Yucheng Tang, Sanja L. Antic,\n  Emily S. Epstein, Aneri B. Balar, Steve Deppen, Alexis B. Paulson, Kim L.\n  Sandler, Pierre P. Massion, Bennett A. Landman", "title": "Distanced LSTM: Time-Distanced Gates in Long Short-Term Memory Models\n  for Lung Cancer Detection", "comments": "This paper is accepted by MLMI (oral), MICCAI workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of lung nodule detection and cancer prediction has been rapidly\ndeveloping with the support of large public data archives. Previous studies\nhave largely focused on cross-sectional (single) CT data. Herein, we consider\nlongitudinal data. The Long Short-Term Memory (LSTM) model addresses learning\nwith regularly spaced time points (i.e., equal temporal intervals). However,\nclinical imaging follows patient needs with often heterogeneous, irregular\nacquisitions. To model both regular and irregular longitudinal samples, we\ngeneralize the LSTM model with the Distanced LSTM (DLSTM) for temporally varied\nacquisitions. The DLSTM includes a Temporal Emphasis Model (TEM) that enables\nlearning across regularly and irregularly sampled intervals. Briefly, (1) the\ntime intervals between longitudinal scans are modeled explicitly, (2)\ntemporally adjustable forget and input gates are introduced for irregular\ntemporal sampling; and (3) the latest longitudinal scan has an additional\nemphasis term. We evaluate the DLSTM framework in three datasets including\nsimulated data, 1794 National Lung Screening Trial (NLST) scans, and 1420\nclinically acquired data with heterogeneous and irregular temporal accession.\nThe experiments on the first two datasets demonstrate that our method achieves\ncompetitive performance on both simulated and regularly sampled datasets (e.g.\nimprove LSTM from 0.6785 to 0.7085 on F1 score in NLST). In external validation\nof clinically and irregularly acquired data, the benchmarks achieved 0.8350\n(CNN feature) and 0.8380 (LSTM) on the area under the ROC curve (AUC) score,\nwhile the proposed DLSTM achieves 0.8905.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 19:25:40 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Gao", "Riqiang", ""], ["Huo", "Yuankai", ""], ["Bao", "Shunxing", ""], ["Tang", "Yucheng", ""], ["Antic", "Sanja L.", ""], ["Epstein", "Emily S.", ""], ["Balar", "Aneri B.", ""], ["Deppen", "Steve", ""], ["Paulson", "Alexis B.", ""], ["Sandler", "Kim L.", ""], ["Massion", "Pierre P.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1909.05379", "submitter": "Oron Ashual", "authors": "Oron Ashual, Lior Wolf", "title": "Specifying Object Attributes and Relations in Interactive Scene\n  Generation", "comments": "Best Paper Honorable Mention in ICCV 2019", "journal-ref": "The IEEE International Conference on Computer Vision (ICCV), 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for the generation of images from an input scene graph.\nThe method separates between a layout embedding and an appearance embedding.\nThe dual embedding leads to generated images that better match the scene graph,\nhave higher visual quality, and support more complex scene graphs. In addition,\nthe embedding scheme supports multiple and diverse output images per scene\ngraph, which can be further controlled by the user. We demonstrate two modes of\nper-object control: (i) importing elements from other images, and (ii)\nnavigation in the object space, by selecting an appearance archetype. Our code\nis publicly available at https://www.github.com/ashual/scene_generation\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 21:16:38 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 15:33:05 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Ashual", "Oron", ""], ["Wolf", "Lior", ""]]}, {"id": "1909.05382", "submitter": "Yun Liu", "authors": "Yuan Liu, Ayush Jain, Clara Eng, David H. Way, Kang Lee, Peggy Bui,\n  Kimberly Kanada, Guilherme de Oliveira Marinho, Jessica Gallegos, Sara\n  Gabriele, Vishakha Gupta, Nalini Singh, Vivek Natarajan, Rainer\n  Hofmann-Wellenhof, Greg S. Corrado, Lily H. Peng, Dale R. Webster, Dennis Ai,\n  Susan Huang, Yun Liu, R. Carter Dunn, David Coz", "title": "A deep learning system for differential diagnosis of skin diseases", "comments": null, "journal-ref": "Nature Medicine (2020)", "doi": "10.1038/s41591-020-0842-3", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin conditions affect an estimated 1.9 billion people worldwide. A shortage\nof dermatologists causes long wait times and leads patients to seek\ndermatologic care from general practitioners. However, the diagnostic accuracy\nof general practitioners has been reported to be only 0.24-0.70 (compared to\n0.77-0.96 for dermatologists), resulting in referral errors, delays in care,\nand errors in diagnosis and treatment. In this paper, we developed a deep\nlearning system (DLS) to provide a differential diagnosis of skin conditions\nfor clinical cases (skin photographs and associated medical histories). The DLS\ndistinguishes between 26 skin conditions that represent roughly 80% of the\nvolume of skin conditions seen in primary care. The DLS was developed and\nvalidated using de-identified cases from a teledermatology practice serving 17\nclinical sites via a temporal split: the first 14,021 cases for development and\nthe last 3,756 cases for validation. On the validation set, where a panel of\nthree board-certified dermatologists defined the reference standard for every\ncase, the DLS achieved 0.71 and 0.93 top-1 and top-3 accuracies respectively.\nFor a random subset of the validation set (n=963 cases), 18 clinicians reviewed\nthe cases for comparison. On this subset, the DLS achieved a 0.67 top-1\naccuracy, non-inferior to board-certified dermatologists (0.63, p<0.001), and\nhigher than primary care physicians (PCPs, 0.45) and nurse practitioners (NPs,\n0.41). The top-3 accuracy showed a similar trend: 0.90 DLS, 0.75\ndermatologists, 0.60 PCPs, and 0.55 NPs. These results highlight the potential\nof the DLS to augment general practitioners to accurately diagnose skin\nconditions by suggesting differential diagnoses that may not have been\nconsidered. Future work will be needed to prospectively assess the clinical\nimpact of using this tool in actual clinical workflows.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 21:26:42 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Liu", "Yuan", ""], ["Jain", "Ayush", ""], ["Eng", "Clara", ""], ["Way", "David H.", ""], ["Lee", "Kang", ""], ["Bui", "Peggy", ""], ["Kanada", "Kimberly", ""], ["Marinho", "Guilherme de Oliveira", ""], ["Gallegos", "Jessica", ""], ["Gabriele", "Sara", ""], ["Gupta", "Vishakha", ""], ["Singh", "Nalini", ""], ["Natarajan", "Vivek", ""], ["Hofmann-Wellenhof", "Rainer", ""], ["Corrado", "Greg S.", ""], ["Peng", "Lily H.", ""], ["Webster", "Dale R.", ""], ["Ai", "Dennis", ""], ["Huang", "Susan", ""], ["Liu", "Yun", ""], ["Dunn", "R. Carter", ""], ["Coz", "David", ""]]}, {"id": "1909.05393", "submitter": "Richard Jiang", "authors": "Tiancheng Xia, Richard Jiang, YongQing Fu and Nanlin Jin", "title": "Automated Blood Cell Detection and Counting via Deep Learning for\n  Microfluidic Point-of-Care Medical Devices", "comments": null, "journal-ref": "Proceeding of 2019 3rd International Conference on Artificial\n  Intelligence Applications and Technologies (AIAAT 2019)", "doi": "10.1088/1757-899X/646/1/012048", "report-no": null, "categories": "cs.CV cs.AI cs.ET cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated in-vitro cell detection and counting have been a key theme for\nartificial and intelligent biological analysis such as biopsy, drug analysis\nand decease diagnosis. Along with the rapid development of microfluidics and\nlab-on-chip technologies, in-vitro live cell analysis has been one of the\ncritical tasks for both research and industry communities. However, it is a\ngreat challenge to obtain and then predict the precise information of live\ncells from numerous microscopic videos and images. In this paper, we\ninvestigated in-vitro detection of white blood cells using deep neural\nnetworks, and discussed how state-of-the-art machine learning techniques could\nfulfil the needs of medical diagnosis. The approach we used in this study was\nbased on Faster Region-based Convolutional Neural Networks (Faster RCNNs), and\na transfer learning process was applied to apply this technique to the\nmicroscopic detection of blood cells. Our experimental results demonstrated\nthat fast and efficient analysis of blood cells via automated microscopic\nimaging can achieve much better accuracy and faster speed than the\nconventionally applied methods, implying a promising future of this technology\nto be applied to the microfluidic point-of-care medical devices.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 22:14:03 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Xia", "Tiancheng", ""], ["Jiang", "Richard", ""], ["Fu", "YongQing", ""], ["Jin", "Nanlin", ""]]}, {"id": "1909.05405", "submitter": "Yang Li", "authors": "Yang Li, Florian Richter, Jingpei Lu, Emily K. Funk, Ryan K. Orosco,\n  Jianke Zhu and Michael C. Yip", "title": "SuPer: A Surgical Perception Framework for Endoscopic Tissue\n  Manipulation with Surgical Robotics", "comments": "The first two authors made equal contribution on this paper", "journal-ref": "IEEE Robotics and Automation Letters (RA-L), vol. 5, no. 2, pp.\n  2294-2301, April 2020", "doi": "10.1109/LRA.2020.2970659", "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional control and task automation have been successfully demonstrated\nin a variety of structured, controlled environments through the use of highly\nspecialized modeled robotic systems in conjunction with multiple sensors.\nHowever, the application of autonomy in endoscopic surgery is very challenging,\nparticularly in soft tissue work, due to the lack of high-quality images and\nthe unpredictable, constantly deforming environment. In this work, we propose a\nnovel surgical perception framework, SuPer, for surgical robotic control. This\nframework continuously collects 3D geometric information that allows for\nmapping a deformable surgical field while tracking rigid instruments within the\nfield. To achieve this, a model-based tracker is employed to localize the\nsurgical tool with a kinematic prior in conjunction with a model-free tracker\nto reconstruct the deformable environment and provide an estimated point cloud\nas a mapping of the environment. The proposed framework was implemented on the\nda Vinci Surgical System in real-time with an end-effector controller where the\ntarget configurations are set and regulated through the framework. Our proposed\nframework successfully completed soft tissue manipulation tasks with high\naccuracy. The demonstration of this novel framework is promising for the future\nof surgical autonomy. In addition, we provide our dataset for further surgical\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 23:57:32 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 06:27:34 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Li", "Yang", ""], ["Richter", "Florian", ""], ["Lu", "Jingpei", ""], ["Funk", "Emily K.", ""], ["Orosco", "Ryan K.", ""], ["Zhu", "Jianke", ""], ["Yip", "Michael C.", ""]]}, {"id": "1909.05417", "submitter": "Myungsu Chae", "authors": "Hyoung-Kyu Song, Ebrahim AlAlkeem, Jaewoong Yun, Tae-Ho Kim, Tae-Ho\n  Kim, Hyerin Yoo, Dasom Heo, Chan Yeob Yeun, and Myungsu Chae", "title": "Deep User Identification Model with Multiple Biometrics", "comments": "Accepted, CIKM 2019 Workshop on DTMBio", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification using biometrics is an important yet challenging task.\nAbundant research has been conducted on identifying personal identity or gender\nusing given signals. Various types of biometrics such as electrocardiogram\n(ECG), electroencephalogram (EEG), face, fingerprint, and voice have been used\nfor these tasks. Most research has only focused on single modality or a single\ntask, while the combination of input modality or tasks is yet to be\ninvestigated. In this paper, we propose deep identification and gender\nclassification using multimodal biometrics. Our model uses ECG, fingerprint,\nand facial data. It then performs two tasks: gender identification and\nclassification. By engaging multi-modality, a single model can handle various\ninput domains without training each modality independently, and the correlation\nbetween domains can increase its generalization performance on the tasks.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 09:13:11 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Song", "Hyoung-Kyu", ""], ["AlAlkeem", "Ebrahim", ""], ["Yun", "Jaewoong", ""], ["Kim", "Tae-Ho", ""], ["Kim", "Tae-Ho", ""], ["Yoo", "Hyerin", ""], ["Heo", "Dasom", ""], ["Yeun", "Chan Yeob", ""], ["Chae", "Myungsu", ""]]}, {"id": "1909.05452", "submitter": "Kaixuan Wang", "authors": "Kaixuan Wang and Shaojie Shen", "title": "Flow-Motion and Depth Network for Monocular Stereo and Beyond", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learning-based method that solves monocular stereo and can be\nextended to fuse depth information from multiple target frames. Given two\nunconstrained images from a monocular camera with known intrinsic calibration,\nour network estimates relative camera poses and the depth map of the source\nimage. The core contribution of the proposed method is threefold. First, a\nnetwork is tailored for static scenes that jointly estimates the optical flow\nand camera motion. By the joint estimation, the optical flow search space is\ngradually reduced resulting in an efficient and accurate flow estimation.\nSecond, a novel triangulation layer is proposed to encode the estimated optical\nflow and camera motion while avoiding common numerical issues caused by\nepipolar. Third, beyond two-view depth estimation, we further extend the above\nnetworks to fuse depth information from multiple target images and estimate the\ndepth map of the source image. To further benefit the research community, we\nintroduce tools to generate photorealistic structure-from-motion datasets such\nthat deep networks can be well trained and evaluated. The proposed method is\ncompared with previous methods and achieves state-of-the-art results within\nless time. Images from real-world applications and Google Earth are used to\ndemonstrate the generalization ability of the method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 04:49:38 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Wang", "Kaixuan", ""], ["Shen", "Shaojie", ""]]}, {"id": "1909.05479", "submitter": "Vishnu Suresh Lokhande", "authors": "Vishnu Suresh Lokhande, Songwong Tasneeyapant, Abhay Venkatesh, Sathya\n  N. Ravi and Vikas Singh", "title": "Generating Accurate Pseudo-labels in Semi-Supervised Learning and\n  Avoiding Overconfident Predictions via Hermite Polynomial Activations", "comments": "Accepted at 2020 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rectified Linear Units (ReLUs) are among the most widely used activation\nfunction in a broad variety of tasks in vision. Recent theoretical results\nsuggest that despite their excellent practical performance, in various cases, a\nsubstitution with basis expansions (e.g., polynomials) can yield significant\nbenefits from both the optimization and generalization perspective.\nUnfortunately, the existing results remain limited to networks with a couple of\nlayers, and the practical viability of these results is not yet known.\nMotivated by some of these results, we explore the use of Hermite polynomial\nexpansions as a substitute for ReLUs in deep networks. While our experiments\nwith supervised learning do not provide a clear verdict, we find that this\nstrategy offers considerable benefits in semi-supervised learning (SSL) /\ntransductive learning settings. We carefully develop this idea and show how the\nuse of Hermite polynomials based activations can yield improvements in\npseudo-label accuracies and sizable financial savings (due to concurrent\nruntime benefits). Further, we show via theoretical analysis, that the networks\n(with Hermite activations) offer robustness to noise and other attractive\nmathematical properties.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 06:42:08 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 06:01:54 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Lokhande", "Vishnu Suresh", ""], ["Tasneeyapant", "Songwong", ""], ["Venkatesh", "Abhay", ""], ["Ravi", "Sathya N.", ""], ["Singh", "Vikas", ""]]}, {"id": "1909.05483", "submitter": "Simon Niklaus", "authors": "Simon Niklaus, Long Mai, Jimei Yang, Feng Liu", "title": "3D Ken Burns Effect from a Single Image", "comments": "TOG 2019, http://sniklaus.com/kenburns", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ken Burns effect allows animating still images with a virtual camera scan\nand zoom. Adding parallax, which results in the 3D Ken Burns effect, enables\nsignificantly more compelling results. Creating such effects manually is\ntime-consuming and demands sophisticated editing skills. Existing automatic\nmethods, however, require multiple input images from varying viewpoints. In\nthis paper, we introduce a framework that synthesizes the 3D Ken Burns effect\nfrom a single image, supporting both a fully automatic mode and an interactive\nmode with the user controlling the camera. Our framework first leverages a\ndepth prediction pipeline, which estimates scene depth that is suitable for\nview synthesis tasks. To address the limitations of existing depth estimation\nmethods such as geometric distortions, semantic distortions, and inaccurate\ndepth boundaries, we develop a semantic-aware neural network for depth\nprediction, couple its estimate with a segmentation-based depth adjustment\nprocess, and employ a refinement neural network that facilitates accurate depth\npredictions at object boundaries. According to this depth estimate, our\nframework then maps the input image to a point cloud and synthesizes the\nresulting video frames by rendering the point cloud from the corresponding\ncamera positions. To address disocclusions while maintaining geometrically and\ntemporally coherent synthesis results, we utilize context-aware color- and\ndepth-inpainting to fill in the missing information in the extreme views of the\ncamera path, thus extending the scene geometry of the point cloud. Experiments\nwith a wide variety of image content show that our method enables realistic\nsynthesis results. Our study demonstrates that our system allows users to\nachieve better results while requiring little effort compared to existing\nsolutions for the 3D Ken Burns effect creation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 06:55:07 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Niklaus", "Simon", ""], ["Mai", "Long", ""], ["Yang", "Jimei", ""], ["Liu", "Feng", ""]]}, {"id": "1909.05488", "submitter": "Yashu Liu", "authors": "Yashu Liu, Wei Wang, Kuanquan Wang, Chengqin Ye, Gongning Luo", "title": "An Automatic Cardiac Segmentation Framework based on Multi-sequence MR\n  Image", "comments": "accepted by STACOM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LGE CMR is an efficient technology for detecting infarcted myocardium. An\nefficient and objective ventricle segmentation method in LGE can benefit the\nlocation of the infarcted myocardium. In this paper, we proposed an automatic\nframework for LGE image segmentation. There are just 5 labeled LGE volumes with\nabout 15 slices of each volume. We adopted histogram match, an invariant of\nrotation registration method, on the other labeled modalities to achieve\neffective augmentation of the training data. A CNN segmentation model was\ntrained based on the augmented training data by leave-one-out strategy. The\npredicted result of the model followed a connected component analysis for each\nclass to remain the largest connected component as the final segmentation\nresult. Our model was evaluated by the 2019 Multi-sequence Cardiac MR\nSegmentation Challenge. The mean testing result of 40 testing volumes on Dice\nscore, Jaccard score, Surface distance, and Hausdorff distance is 0.8087,\n0.6976, 2.8727mm, and 15.6387mm, respectively. The experiment result shows a\nsatisfying performance of the proposed framework. Code is available at\nhttps://github.com/Suiiyu/MS-CMR2019.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 07:19:11 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Liu", "Yashu", ""], ["Wang", "Wei", ""], ["Wang", "Kuanquan", ""], ["Ye", "Chengqin", ""], ["Luo", "Gongning", ""]]}, {"id": "1909.05506", "submitter": "Zihao Wang", "authors": "Zihao Wang, Xihui Liu, Hongsheng Li, Lu Sheng, Junjie Yan, Xiaogang\n  Wang, Jing Shao", "title": "CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-image cross-modal retrieval is a challenging task in the field of\nlanguage and vision. Most previous approaches independently embed images and\nsentences into a joint embedding space and compare their similarities. However,\nprevious approaches rarely explore the interactions between images and\nsentences before calculating similarities in the joint space. Intuitively, when\nmatching between images and sentences, human beings would alternatively attend\nto regions in images and words in sentences, and select the most salient\ninformation considering the interaction between both modalities. In this paper,\nwe propose Cross-modal Adaptive Message Passing (CAMP), which adaptively\ncontrols the information flow for message passing across modalities. Our\napproach not only takes comprehensive and fine-grained cross-modal interactions\ninto account, but also properly handles negative pairs and irrelevant\ninformation with an adaptive gating scheme. Moreover, instead of conventional\njoint embedding approaches for text-image matching, we infer the matching score\nbased on the fused features, and propose a hardest negative binary\ncross-entropy loss for training. Results on COCO and Flickr30k significantly\nsurpass state-of-the-art methods, demonstrating the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 08:46:11 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Wang", "Zihao", ""], ["Liu", "Xihui", ""], ["Li", "Hongsheng", ""], ["Sheng", "Lu", ""], ["Yan", "Junjie", ""], ["Wang", "Xiaogang", ""], ["Shao", "Jing", ""]]}, {"id": "1909.05507", "submitter": "Mateusz Ostaszewski", "authors": "Wojciech Masarczyk, Przemys{\\l}aw G{\\l}omb, Bartosz Grabowski, Mateusz\n  Ostaszewski", "title": "Effective training of deep convolutional neural networks for\n  hyperspectral image classification through artificial labeling", "comments": null, "journal-ref": "Remote Sens. 2020, 12, 2653", "doi": "10.3390/rs12162653", "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral imaging is a rich source of data, allowing for multitude of\neffective applications. However, such imaging remains challenging because of\nlarge data dimension and, typically, small pool of available training examples.\nWhile deep learning approaches have been shown to be successful in providing\neffective classification solutions, especially for high dimensional problems,\nunfortunately they work best with a lot of labelled examples available. To\nalleviate the second requirement for a particular dataset the transfer learning\napproach can be used: first the network is pre-trained on some dataset with\nlarge amount of training labels available, then the actual dataset is used to\nfine-tune the network. This strategy is not straightforward to apply with\nhyperspectral images, as it is often the case that only one particular image of\nsome type or characteristic is available. In this paper, we propose and\ninvestigate a simple and effective strategy of transfer learning that uses\nunsupervised pre-training step without label information. This approach can be\napplied to many of the hyperspectral classification problems. Performed\nexperiments show that it is very effective at improving the classification\naccuracy without being restricted to a particular image type or neural network\narchitecture. The experiments were carried out on several deep neural network\narchitectures and various sizes of labeled training sets. The greatest\nimprovement in overall accuracy on the Indian Pines and Pavia University\ndatasets is over 21 and 13 percentage points, respectively. An additional\nadvantage of the proposed approach is the unsupervised nature of the\npre-training step, which can be done immediately after image acquisition,\nwithout the need of the potentially costly expert's time.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 08:47:21 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 08:48:33 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Masarczyk", "Wojciech", ""], ["G\u0142omb", "Przemys\u0142aw", ""], ["Grabowski", "Bartosz", ""], ["Ostaszewski", "Mateusz", ""]]}, {"id": "1909.05568", "submitter": "Julio Cezar Silveira Jacques Junior", "authors": "Ricardo Dar\\'io P\\'erez Principi, Cristina Palmero, Julio C. S.\n  Jacques Junior, and Sergio Escalera", "title": "On the Effect of Observed Subject Biases in Apparent Personality\n  Analysis from Audio-visual Signals", "comments": "Accepted in IEEE Transactions on Affective Computing (TAC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personality perception is implicitly biased due to many subjective factors,\nsuch as cultural, social, contextual, gender and appearance. Approaches\ndeveloped for automatic personality perception are not expected to predict the\nreal personality of the target, but the personality external observers\nattributed to it. Hence, they have to deal with human bias, inherently\ntransferred to the training data. However, bias analysis in personality\ncomputing is an almost unexplored area. In this work, we study different\npossible sources of bias affecting personality perception, including emotions\nfrom facial expressions, attractiveness, age, gender, and ethnicity, as well as\ntheir influence on prediction ability for apparent personality estimation. To\nthis end, we propose a multi-modal deep neural network that combines raw audio\nand visual information alongside predictions of attribute-specific models to\nregress apparent personality. We also analyse spatio-temporal aggregation\nschemes and the effect of different time intervals on first impressions. We\nbase our study on the ChaLearn First Impressions dataset, consisting of\none-person conversational videos. Our model shows state-of-the-art results\nregressing apparent personality based on the Big-Five model. Furthermore, given\nthe interpretability nature of our network design, we provide an incremental\nanalysis on the impact of each possible source of bias on final network\npredictions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 11:18:54 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 17:00:59 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Principi", "Ricardo Dar\u00edo P\u00e9rez", ""], ["Palmero", "Cristina", ""], ["Junior", "Julio C. S. Jacques", ""], ["Escalera", "Sergio", ""]]}, {"id": "1909.05587", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Mirko Nentwig, Yohannes Kassahun, Francis Li,\n  Stanislav Bochkarev, Akif Kamal, David Dolson, Secil Altintas, Arif Virani,\n  and Alexander Wong", "title": "Human-Machine Collaborative Design for Accelerated Design of Compact\n  Deep Neural Networks for Autonomous Driving", "comments": "7 pages; BMVC Workshop on Visual AI and Entrepreneurship (VAIE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An effective deep learning development process is critical for widespread\nindustrial adoption, particularly in the automotive sector. A typical\nindustrial deep learning development cycle involves customizing and\nre-designing an off-the-shelf network architecture to meet the operational\nrequirements of the target application, leading to considerable trial and error\nwork by a machine learning practitioner. This approach greatly impedes\ndevelopment with a long turnaround time and the unsatisfactory quality of the\ncreated models. As a result, a development platform that can aid engineers in\ngreatly accelerating the design and production of compact, optimized deep\nneural networks is highly desirable. In this joint industrial case study, we\nstudy the efficacy of the GenSynth AI-assisted AI design platform for\naccelerating the design of custom, optimized deep neural networks for\nautonomous driving through human-machine collaborative design. We perform a\nquantitative examination by evaluating 10 different compact deep neural\nnetworks produced by GenSynth for the purpose of object detection via a\nNASNet-based user network prototype design, targeted at a low-cost GPU-based\naccelerated embedded system. Furthermore, we quantitatively assess the talent\nhours and GPU processing hours used by the GenSynth process and three other\napproaches based on the typical industrial development process. In addition, we\nquantify the annual cloud cost savings for comprehensive testing using networks\nproduced by GenSynth. Finally, we assess the usability and merits of the\nGenSynth process through user feedback. The findings of this case study showed\nthat GenSynth is easy to use and can be effective at accelerating the design\nand production of compact, customized deep neural network.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 12:00:50 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Nentwig", "Mirko", ""], ["Kassahun", "Yohannes", ""], ["Li", "Francis", ""], ["Bochkarev", "Stanislav", ""], ["Kamal", "Akif", ""], ["Dolson", "David", ""], ["Altintas", "Secil", ""], ["Virani", "Arif", ""], ["Wong", "Alexander", ""]]}, {"id": "1909.05620", "submitter": "Wan-Yi Lin", "authors": "Govind Rathore and Wan-Yi Lin and Ji Eun Kim", "title": "DeepBbox: Accelerating Precise Ground Truth Generation for Autonomous\n  Driving Datasets", "comments": "accepted by ITSC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving requires various computer vision algorithms, such as\nobject detection and tracking.Precisely-labeled datasets (i.e., objects are\nfully contained in bounding boxes with only a few extra pixels) are preferred\nfor training such algorithms, so that the algorithms can detect exact locations\nof the objects. However, it is very time-consuming and hence expensive to\ngenerate precise labels for image sequences at scale. In this paper, we propose\nDeepBbox, an algorithm that corrects loose object labels into right bounding\nboxes to reduce human annotation efforts. We use Cityscapes dataset to show\nannotation efficiency and accuracy improvement using DeepBbox. Experimental\nresults show that, with DeepBbox,we can increase the number of object edges\nthat are labeled automatically (within 1\\% error) by 50% to reduce manual\nannotation time.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 20:04:35 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Rathore", "Govind", ""], ["Lin", "Wan-Yi", ""], ["Kim", "Ji Eun", ""]]}, {"id": "1909.05621", "submitter": "Chaoquan Zhang", "authors": "Chaoquan Zhang, Hongchao Fan, Wanzhi Li, Bo Mao, Xuan Ding", "title": "Automated Detecting and Placing Road Objects from Street-level Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigation services utilized by autonomous vehicles or ordinary users require\nthe availability of detailed information about road-related objects and their\ngeolocations, especially at road intersections. However, these road\nintersections are mainly represented as point elements without detailed\ninformation, or are even not available in current versions of crowdsourced\nmapping databases including OpenStreetMap(OSM). This study develops an approach\nto automatically detect road objects and place them to right location from\nstreet-level images. Our processing pipeline relies on two convolutional neural\nnetworks: the first segments the images, while the second detects and\nclassifies the specific objects. Moreover, to locate the detected objects, we\nestablish an attributed topological binary tree(ATBT) based on urban grammar\nfor each image to depict the coherent relations of topologies, attributes and\nsemantics of the road objects. Then the ATBT is further matched with map\nfeatures on OSM to determine the right placed location. The proposed method has\nbeen applied to a case study in Berlin, Germany. We validate the effectiveness\nof our method on two object classes: traffic signs and traffic lights.\nExperimental results demonstrate that the proposed approach provides\nnear-precise localization results in terms of completeness and positional\naccuracy. Among many potential applications, the output may be combined with\nother sources of data to guide autonomous vehicles\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 14:45:21 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 02:20:40 GMT"}, {"version": "v3", "created": "Tue, 17 Sep 2019 11:06:37 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Zhang", "Chaoquan", ""], ["Fan", "Hongchao", ""], ["Li", "Wanzhi", ""], ["Mao", "Bo", ""], ["Ding", "Xuan", ""]]}, {"id": "1909.05622", "submitter": "Matin Hosseini", "authors": "Matin Hosseini, Anthony S. Maida, Majid Hosseini, Gottumukkala Raju", "title": "Inception-inspired LSTM for Next-frame Video Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of video frame prediction has received much interest due to its\nrelevance to many computer vision applications such as autonomous vehicles or\nrobotics. Supervised methods for video frame prediction rely on labeled data,\nwhich may not always be available. In this paper, we provide a novel\nunsupervised deep-learning method called Inception-based LSTM for video frame\nprediction. The general idea of inception networks is to implement wider\nnetworks instead of deeper networks. This network design was shown to improve\nthe performance of image classification. The proposed method is evaluated on\nboth Inception-v1 and Inception-v2 structures. The proposed Inception LSTM\nmethods are compared with convolutional LSTM when applied using PredNet\npredictive coding framework for both the KITTI and KTH data sets. We observed\nthat the Inception based LSTM outperforms the convolutional LSTM. Also,\nInception LSTM has better prediction performance compared to Inception v2 LSTM.\nHowever, Inception v2 LSTM has a lower computational cost compared to Inception\nLSTM.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 03:49:07 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 17:06:36 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Hosseini", "Matin", ""], ["Maida", "Anthony S.", ""], ["Hosseini", "Majid", ""], ["Raju", "Gottumukkala", ""]]}, {"id": "1909.05624", "submitter": "Vaidheeswaran Archana", "authors": "Murugesan Vadivel, SelvaKumar Murugan, Suriyadeepan Ramamoorthy,\n  Vaidheeswaran Archana, Malaikannan Sankarasubbu", "title": "Detecting Parking Spaces in a Parcel using Satellite Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote Sensing Images from satellites have been used in various domains for\ndetecting and understanding structures on the ground surface. In this work,\nsatellite images were used for localizing parking spaces and vehicles in\nparking lots for a given parcel using an RCNN based Neural Network\nArchitectures. Parcel shapefiles and raster images from USGS image archive were\nused for developing images for both training and testing. Feature Pyramid based\nMask RCNN yields average class accuracy of 97.56% for both parking spaces and\nvehicles\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 07:00:13 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 13:37:07 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Vadivel", "Murugesan", ""], ["Murugan", "SelvaKumar", ""], ["Ramamoorthy", "Suriyadeepan", ""], ["Archana", "Vaidheeswaran", ""], ["Sankarasubbu", "Malaikannan", ""]]}, {"id": "1909.05626", "submitter": "Hongsun Choi", "authors": "Hongsun Choi, Mincheul Kang, Youngsun Kwon and Sung-eui Yoon", "title": "An Objectness Score for Accurate and Fast Detection during Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method utilizing an objectness score for maintaining the\nlocations and classes of objects detected from Mask R-CNN during mobile robot\nnavigation. The objectness score is defined to measure how well the detector\nidentifies the locations and classes of objects during navigation.\nSpecifically, it is designed to increase when there is sufficient distance\nbetween a detected object and the camera. During the navigation process, we\ntransform the locations of objects in 3D world coordinates into 2D image\ncoordinates through an affine projection and decide whether to retain the\nclasses of detected objects using the objectness score. We conducted\nexperiments to determine how well the locations and classes of detected objects\nare maintained at various angles and positions. Experimental results showed\nthat our approach is efficient and robust, regardless of changing angles and\ndistances.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 02:22:08 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Choi", "Hongsun", ""], ["Kang", "Mincheul", ""], ["Kwon", "Youngsun", ""], ["Yoon", "Sung-eui", ""]]}, {"id": "1909.05630", "submitter": "Walid Abdullah Al", "authors": "Walid Abdullah Al, Il Dong Yun", "title": "Reinforcing Medical Image Classifier to Improve Generalization on Small\n  Datasets", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advents of deep learning, improved image classification with complex\ndiscriminative models has been made possible. However, such deep models with\nincreased complexity require a huge set of labeled samples to generalize the\ntraining. Such classification models can easily overfit when applied for\nmedical images because of limited training data, which is a common problem in\nthe field of medical image analysis. This paper proposes and investigates a\nreinforced classifier for improving the generalization under a few available\ntraining data. Partially following the idea of reinforcement learning, the\nproposed classifier uses a generalization-feedback from a subset of the\ntraining data to update its parameter instead of only using the conventional\ncross-entropy loss about the training data. We evaluate the improvement of the\nproposed classifier by applying it on three different classification problems\nagainst the standard deep classifiers equipped with existing\noverfitting-prevention techniques. Besides an overall improvement in\nclassification performance, the proposed classifier showed remarkable\ncharacteristics of generalized learning, which can have great potential in\nmedical classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 09:12:36 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 04:28:33 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Al", "Walid Abdullah", ""], ["Yun", "Il Dong", ""]]}, {"id": "1909.05631", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Simon Alford, Vijay Gadepally, Michael Jones, Lauren\n  Milechin, Ryan Robinett, Sid Samsi", "title": "Sparse Deep Neural Network Graph Challenge", "comments": "7 pages, 5 figures, 3 tables, 60 references, accepted to IEEE HPEC\n  2019. arXiv admin note: substantial text overlap with arXiv:1807.03165,\n  arXiv:1708.02937, arXiv:1708.06866", "journal-ref": null, "doi": "10.1109/HPEC.2019.8916336", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MIT/IEEE/Amazon GraphChallenge.org encourages community approaches to\ndeveloping new solutions for analyzing graphs and sparse data. Sparse AI\nanalytics present unique scalability difficulties. The proposed Sparse Deep\nNeural Network (DNN) Challenge draws upon prior challenges from machine\nlearning, high performance computing, and visual analytics to create a\nchallenge that is reflective of emerging sparse AI systems. The Sparse DNN\nChallenge is based on a mathematically well-defined DNN inference computation\nand can be implemented in any programming environment. Sparse DNN inference is\namenable to both vertex-centric implementations and array-based implementations\n(e.g., using the GraphBLAS.org standard). The computations are simple enough\nthat performance predictions can be made based on simple computing hardware\nmodels. The input data sets are derived from the MNIST handwritten letters. The\nsurrounding I/O and verification provide the context for each sparse DNN\ninference that allows rigorous definition of both the input and the output.\nFurthermore, since the proposed sparse DNN challenge is scalable in both\nproblem size and hardware, it can be used to measure and quantitatively compare\na wide range of present day and future systems. Reference implementations have\nbeen implemented and their serial and parallel performance have been measured.\nSpecifications, data, and software are publicly available at GraphChallenge.org\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 02:29:52 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Kepner", "Jeremy", ""], ["Alford", "Simon", ""], ["Gadepally", "Vijay", ""], ["Jones", "Michael", ""], ["Milechin", "Lauren", ""], ["Robinett", "Ryan", ""], ["Samsi", "Sid", ""]]}, {"id": "1909.05632", "submitter": "Arno Khachatourian", "authors": "Arno Khachatourian", "title": "Reusing Convolutional Activations from Frame to Frame to Speed up\n  Training and Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When processing similar frames in succession, we can take advantage of the\nlocality of the convolution operation to reevaluate only portions of the image\nthat changed from the previous frame. By saving the output of a layer of\nconvolutions and calculating the change from frame to frame, we can reuse\nprevious activations and save computational resources that would otherwise be\nwasted recalculating convolutions whose outputs we have already observed. This\ntechnique can be applied to many domains, such as processing videos from\nstationary video cameras, studying the effects of occluding or distorting\nsections of images, applying convolution to multiple frames of audio or time\nseries data, or playing Atari games. Furthermore, this technique can be applied\nto speed up both training and inference.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 00:21:03 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 05:59:09 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Khachatourian", "Arno", ""]]}, {"id": "1909.05637", "submitter": "Tao-Yang Fu", "authors": "Tao-yang Fu, Wang-Chien Lee", "title": "DeepIST: Deep Image-based Spatio-Temporal Network for Travel Time\n  Estimation", "comments": "10 pages, accepted by The 28th ACM International Conference on\n  Information and Knowledge Management (CIKM) 2019", "journal-ref": "The 28th ACM International Conference on Information and Knowledge\n  Management (CIKM) 2019", "doi": "10.1145/3357384.3357870", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the travel time for a given path is a fundamental problem in many\nurban transportation systems. However, prior works fail to well capture moving\nbehaviors embedded in paths and thus do not estimate the travel time\naccurately. To fill in this gap, in this work, we propose a novel neural\nnetwork framework, namely {\\em Deep Image-based Spatio-Temporal network\n(DeepIST)}, for travel time estimation of a given path. The novelty of DeepIST\nlies in the following aspects: 1) we propose to plot a path as a sequence of\n\"generalized images\" which include sub-paths along with additional information,\nsuch as traffic conditions, road network and traffic signals, in order to\nharness the power of convolutional neural network model (CNN) on image\nprocessing; 2) we design a novel two-dimensional CNN, namely {\\em PathCNN}, to\nextract spatial patterns for lines in images by regularization and adopting\nmultiple pooling methods; and 3) we apply a one-dimensional CNN to capture\ntemporal patterns among the spatial patterns along the paths for the\nestimation. Empirical results show that DeepIST soundly outperforms the\nstate-of-the-art travel time estimation models by 24.37\\% to 25.64\\% of mean\nabsolute error (MAE) in multiple large-scale real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 04:38:42 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Fu", "Tao-yang", ""], ["Lee", "Wang-Chien", ""]]}, {"id": "1909.05638", "submitter": "Lahiru D. Chamain Hewa Gamage", "authors": "Lahiru D. Chamain, Zhi Ding", "title": "Faster and Accurate Classification for JPEG2000 Compressed Images in\n  Networked Applications", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  JPEG2000 (j2k) is a highly popular format for image and video\ncompression.With the rapidly growing applications of cloud based image\nclassification, most existing j2k-compatible schemes would stream compressed\ncolor images from the source before reconstruction at the processing center as\ninputs to deep CNNs. We propose to remove the computationally costly\nreconstruction step by training a deep CNN image classifier using the CDF 9/7\nDiscrete Wavelet Transformed (DWT) coefficients directly extracted from\nj2k-compressed images. We demonstrate additional computation savings by\nutilizing shallower CNN to achieve classification of good accuracy in the DWT\ndomain. Furthermore, we show that traditional augmentation transforms such as\nflipping/shifting are ineffective in the DWT domain and present different\naugmentation transformations to achieve more accurate classification without\nany additional cost. This way, faster and more accurate classification is\npossible for j2k encoded images without image reconstruction. Through\nexperiments on CIFAR-10 and Tiny ImageNet data sets, we show that the\nperformance of the proposed solution is consistent for image transmission over\nlimited channel bandwidth.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 19:03:35 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Chamain", "Lahiru D.", ""], ["Ding", "Zhi", ""]]}, {"id": "1909.05644", "submitter": "Richard Tomsett", "authors": "David Mott, Richard Tomsett", "title": "Illuminated Decision Trees with Lucid", "comments": "Presented at BMVC 2019: Workshop on Interpretable and Explainable\n  Machine Vision, Cardiff, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lucid methods described by Olah et al. (2018) provide a way to inspect\nthe inner workings of neural networks trained on image classification tasks\nusing feature visualization. Such methods have generally been applied to\nnetworks trained on visually rich, large-scale image datasets like ImageNet,\nwhich enables them to produce enticing feature visualizations. To investigate\nthese methods further, we applied them to classifiers trained to perform the\nmuch simpler (in terms of dataset size and visual richness), yet challenging\ntask of distinguishing between different kinds of white blood cell from\nmicroscope images. Such a task makes generating useful feature visualizations\ndifficult, as the discriminative features are inherently hard to identify and\ninterpret. We address this by presenting the \"Illuminated Decision Tree\"\napproach, in which we use a neural network trained on the task as a feature\nextractor, then learn a decision tree based on these features, and provide\nLucid visualizations for each node in the tree. We demonstrate our approach\nwith several examples, showing how this approach could be useful both in model\ndevelopment and debugging, and when explaining model outputs to non-experts.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 12:32:44 GMT"}], "update_date": "2019-09-15", "authors_parsed": [["Mott", "David", ""], ["Tomsett", "Richard", ""]]}, {"id": "1909.05647", "submitter": "Laurie Bose", "authors": "Laurie Bose, Jianing Chen, Stephen J. Carey, Piotr Dudek, Walterio\n  Mayol-Cuevas", "title": "A Camera That CNNs: Towards Embedded Neural Networks on Pixel Processor\n  Arrays", "comments": "Accepted into ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a convolutional neural network implementation for pixel processor\narray (PPA) sensors. PPA hardware consists of a fine-grained array of\ngeneral-purpose processing elements, each capable of light capture, data\nstorage, program execution, and communication with neighboring elements. This\nallows images to be stored and manipulated directly at the point of light\ncapture, rather than having to transfer images to external processing hardware.\nOur CNN approach divides this array up into 4x4 blocks of processing elements,\nessentially trading-off image resolution for increased local memory capacity\nper 4x4 \"pixel\". We implement parallel operations for image addition,\nsubtraction and bit-shifting images in this 4x4 block format. Using these\ncomponents we formulate how to perform ternary weight convolutions upon these\nimages, compactly store results of such convolutions, perform max-pooling, and\ntransfer the resulting sub-sampled data to an attached micro-controller. We\ntrain ternary weight filter CNNs for digit recognition and a simple tracking\ntask, and demonstrate inference of these networks upon the SCAMP5 PPA system.\nThis work represents a first step towards embedding neural network processing\ncapability directly onto the focal plane of a sensor.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 13:39:42 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 13:07:04 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Bose", "Laurie", ""], ["Chen", "Jianing", ""], ["Carey", "Stephen J.", ""], ["Dudek", "Piotr", ""], ["Mayol-Cuevas", "Walterio", ""]]}, {"id": "1909.05651", "submitter": "Ji Yuanfeng", "authors": "Yuanfeng Ji and Hao Chen and Dan Lin and Xiaohua Wu and Di Lin", "title": "PRSNet: Part Relation and Selection Network for Bone Age Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bone age is one of the most important indicators for assessing bone's\nmaturity, which can help to interpret human's growth development level and\npotential progress. In the clinical practice, bone age assessment (BAA) of\nX-ray images requires the joint consideration of the appearance and location\ninformation of hand bones. These kinds of information can be effectively\ncaptured by the relation of different anatomical parts of hand bone. Recently\ndeveloped methods differ mostly in how they model the part relation and choose\nuseful parts for BAA. However, these methods neglect the mining of relationship\namong different parts, which can help to improve the assessment accuracy. In\nthis paper, we propose a novel part relation module, which accurately discovers\nthe underlying concurrency of parts by using multi-scale context information of\ndeep learning feature representation. Furthermore, based on the part relation,\nwe explore a new part selection module, which comprehensively measures the\nimportance of parts and select the top ranking parts for assisting BAA. We\njointly train our part relation and selection modules in an end-to-end way,\nachieving state-of-the-art performance on the public RSNA 2017 Pediatric Bone\nAge benchmark dataset and outperforming other competitive methods by a\nsignificant margin.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 08:35:33 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Ji", "Yuanfeng", ""], ["Chen", "Hao", ""], ["Lin", "Dan", ""], ["Wu", "Xiaohua", ""], ["Lin", "Di", ""]]}, {"id": "1909.05653", "submitter": "Mohammad Farhadi Bajestani", "authors": "Mohammad Farhadi, Mehdi Ghasemi, Yezhou Yang", "title": "A Novel Design of Adaptive and Hierarchical Convolutional Neural\n  Networks using Partial Reconfiguration on FPGA", "comments": "2019 IEEE High Performance Extreme Computing Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays most research in visual recognition using Convolutional Neural\nNetworks (CNNs) follows the \"deeper model with deeper confidence\" belief to\ngain a higher recognition accuracy. At the same time, deeper model brings\nheavier computation. On the other hand, for a large chunk of recognition\nchallenges, a system can classify images correctly using simple models or\nso-called shallow networks. Moreover, the implementation of CNNs faces with the\nsize, weight, and energy constraints on the embedded devices. In this paper, we\nimplement the adaptive switching between shallow and deep networks to reach the\nhighest throughput on a resource-constrained MPSoC with CPU and FPGA. To this\nend, we develop and present a novel architecture for the CNNs where a gate\nmakes the decision whether using the deeper model is beneficial or not. Due to\nresource limitation on FPGA, the idea of partial reconfiguration has been used\nto accommodate deep CNNs on the FPGA resources. We report experimental results\non CIFAR-10, CIFAR-100, and SVHN datasets to validate our approach. Using\nconfidence metric as the decision making factor, only 69.8%, 71.8%, and 43.8%\nof the computation in the deepest network is done for CIFAR-10, CIFAR-100, and\nSVHN while it can maintain the desired accuracy with the throughput of around\n400 images per second for SVHN dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 16:00:19 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Farhadi", "Mohammad", ""], ["Ghasemi", "Mehdi", ""], ["Yang", "Yezhou", ""]]}, {"id": "1909.05654", "submitter": "Di Fu", "authors": "Di Fu, Cornelius Weber, Guochun Yang, Matthias Kerzel, Weizhi Nan,\n  Pablo Barros, Haiyan Wu, Xun Liu, Stefan Wermter", "title": "What can computational models learn from human selective attention? A\n  review from an audiovisual crossmodal perspective", "comments": "29pages, 5 figures, 1 table, journal article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Selective attention plays an essential role in information acquisition and\nutilization from the environment. In the past 50 years, research on selective\nattention has been a central topic in cognitive science. Compared with unimodal\nstudies, crossmodal studies are more complex but necessary to solve real-world\nchallenges in both human experiments and computational modeling. Although an\nincreasing number of findings on crossmodal selective attention have shed light\non humans' behavioral patterns and neural underpinnings, a much better\nunderstanding is still necessary to yield the same benefit for computational\nintelligent agents. This article reviews studies of selective attention in\nunimodal visual and auditory and crossmodal audiovisual setups from the\nmultidisciplinary perspectives of psychology and cognitive neuroscience, and\nevaluates different ways to simulate analogous mechanisms in computational\nmodels and robotics. We discuss the gaps between these fields in this\ninterdisciplinary review and provide insights about how to use psychological\nfindings and theories in artificial intelligence from different perspectives.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 18:12:05 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Fu", "Di", ""], ["Weber", "Cornelius", ""], ["Yang", "Guochun", ""], ["Kerzel", "Matthias", ""], ["Nan", "Weizhi", ""], ["Barros", "Pablo", ""], ["Wu", "Haiyan", ""], ["Liu", "Xun", ""], ["Wermter", "Stefan", ""]]}, {"id": "1909.05655", "submitter": "Henry Griffith", "authors": "Henry K. Griffith, Dmytro Katrychuk, Oleg V. Komogortsev", "title": "Assessment of Shift-Invariant CNN Gaze Mappings for PS-OG Eye Movement\n  Sensors", "comments": "Accepted to be published in the 2019 OpenEDS Workshop: Eye Tracking\n  for VR and AR at the International Conference on Computer Vision (ICCV),\n  October 27- November 3, 2019, Seoul, Korea", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photosensor oculography (PS-OG) eye movement sensors offer desirable\nperformance characteristics for integration within wireless head mounted\ndevices (HMDs), including low power consumption and high sampling rates. To\naddress the known performance degradation of these sensors due to HMD shifts,\nvarious machine learning techniques have been proposed for mapping sensor\noutputs to gaze location. This paper advances the understanding of a recently\nintroduced convolutional neural network designed to provide shift invariant\ngaze mapping within a specified range of sensor translations. Performance is\nassessed for shift training examples which better reflect the distribution of\nvalues that would be generated through manual repositioning of the HMD during a\ndedicated collection of training data. The network is shown to exhibit\ncomparable accuracy for this realistic shift distribution versus a previously\nconsidered rectangular grid, thereby enhancing the feasibility of in-field\nset-up. In addition, this work further demonstrates the practical viability of\nthe proposed initialization process by demonstrating robust mapping performance\nversus training data scale. The ability to maintain reasonable accuracy for\nshifts extending beyond those introduced during training is also demonstrated.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 17:57:45 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Griffith", "Henry K.", ""], ["Katrychuk", "Dmytro", ""], ["Komogortsev", "Oleg V.", ""]]}, {"id": "1909.05659", "submitter": "Nutan Chen Ph.D.", "authors": "Nutan Chen, G\\\"oran Westling, Benoni B. Edin, Patrick van der Smagt", "title": "Estimating Fingertip Forces, Torques, and Local Curvatures from\n  Fingernail Images", "comments": "Robotica", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of dexterous manipulation has provided important insights in humans\nsensorimotor control as well as inspiration for manipulation strategies in\nrobotic hands. Previous work focused on experimental environment with\nrestrictions. Here we describe a method using the deformation and color\ndistribution of the fingernail and its surrounding skin, to estimate the\nfingertip forces, torques and contact surface curvatures for various objects,\nincluding the shape and material of the contact surfaces and the weight of the\nobjects. The proposed method circumvents limitations associated with sensorized\nobjects, gloves or fixed contact surface type. In addition, compared with\nprevious single finger estimation in an experimental environment, we extend the\napproach to multiple finger force estimation, which can be used for\napplications such as human grasping analysis. Four algorithms are used, c.q.,\nGaussian process (GP), Convolutional Neural Networks (CNN), Neural Networks\nwith Fast Dropout (NN-FD) and Recurrent Neural Networks with Fast Dropout\n(RNN-FD), to model a mapping from images to the corresponding labels. The\nresults further show that the proposed method has high accuracy to predict\nforce, torque and contact surface.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 19:22:32 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Chen", "Nutan", ""], ["Westling", "G\u00f6ran", ""], ["Edin", "Benoni B.", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1909.05663", "submitter": "Riccardo La Grassa", "authors": "Ignazio Gallo, Shah Nawaz, Alessandro Calefati, Riccardo La Grassa,\n  Nicola Landro", "title": "Picture What you Read", "comments": "7 pages, Dicta2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visualization refers to our ability to create an image in our head based on\nthe text we read or the words we hear. It is one of the many skills that makes\nreading comprehension possible. Convolutional Neural Networks (CNN) are an\nexcellent tool for recognizing and classifying text documents. In addition, it\ncan generate images conditioned on natural language. In this work, we utilize\nCNNs capabilities to generate realistic images representative of the text\nillustrating the semantic concept. We conducted various experiments to\nhighlight the capacity of the proposed model to generate representative images\nof the text descriptions used as input to the proposed model.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 11:26:35 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Gallo", "Ignazio", ""], ["Nawaz", "Shah", ""], ["Calefati", "Alessandro", ""], ["La Grassa", "Riccardo", ""], ["Landro", "Nicola", ""]]}, {"id": "1909.05664", "submitter": "Aly Magassouba", "authors": "Aly Magassouba and Komei Sugiura and Hisashi Kawai", "title": "Multimodal Attention Branch Network for Perspective-Free Sentence\n  Generation", "comments": "10 pages, 4 figures. Accepted for CoRL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the automatic sentence generation of fetching\ninstructions for domestic service robots. Typical fetching commands such as\n\"bring me the yellow toy from the upper part of the white shelf\" includes\nreferring expressions, i.e., \"from the white upper part of the white shelf\". To\nsolve this task, we propose a multimodal attention branch network (Multi-ABN)\nwhich generates natural sentences in an end-to-end manner. Multi-ABN uses\nmultiple images of the same fixed scene to generate sentences that are not tied\nto a particular viewpoint. This approach combines a linguistic attention branch\nmechanism with several attention branch mechanisms. We evaluated our approach,\nwhich outperforms the state-of-the-art method on a standard metrics. Our method\nalso allows us to visualize the alignment between the linguistic input and the\nvisual features.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 01:10:24 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Magassouba", "Aly", ""], ["Sugiura", "Komei", ""], ["Kawai", "Hisashi", ""]]}, {"id": "1909.05666", "submitter": "Yumeng Zhang", "authors": "Yumeng Zhang, Li Chen, Yufeng Liu, Junhai Yong, Wen Zheng", "title": "Adaptive Wasserstein Hourglass for Weakly Supervised Hand Pose\n  Estimation from Monocular RGB", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insufficient labeled training datasets is one of the bottlenecks of 3D hand\npose estimation from monocular RGB images. Synthetic datasets have a large\nnumber of images with precise annotations, but the obvious difference with\nreal-world datasets impacts the generalization. Little work has been done to\nbridge the gap between two domains over their wide difference. In this paper,\nwe propose a domain adaptation method called Adaptive Wasserstein Hourglass (AW\nHourglass) for weakly-supervised 3D hand pose estimation, which aims to\ndistinguish the difference and explore the common characteristics (e.g. hand\nstructure) of synthetic and real-world datasets. Learning the common\ncharacteristics helps the network focus on pose-related information. The\nsimilarity of the characteristics makes it easier to enforce domain-invariant\nconstraints. During training, based on the relation between these common\ncharacteristics and 3D pose learned from fully-annotated synthetic datasets, it\nis beneficial for the network to restore the 3D pose of weakly labeled\nreal-world datasets with the aid of 2D annotations and depth images. While in\ntesting, the network predicts the 3D pose with the input of RGB.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 01:26:19 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Zhang", "Yumeng", ""], ["Chen", "Li", ""], ["Liu", "Yufeng", ""], ["Yong", "Junhai", ""], ["Zheng", "Wen", ""]]}, {"id": "1909.05667", "submitter": "Liam Hiley BSc", "authors": "Liam Hiley, Alun Preece, Yulia Hicks", "title": "Explainable Deep Learning for Video Recognition Tasks: A Framework &\n  Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of Deep Learning for real-world applications is ever-growing.\nWith the introduction of high performance hardware, applications are no longer\nlimited to image recognition. With the introduction of more complex problems\ncomes more and more complex solutions, and the increasing need for explainable\nAI. Deep Neural Networks for Video tasks are amongst the most complex models,\nwith at least twice the parameters of their Image counterparts. However,\nexplanations for these models are often ill-adapted to the video domain. The\ncurrent work in explainability for video models is still overshadowed by Image\ntechniques, while Video Deep Learning itself is quickly gaining on methods for\nstill images. This paper seeks to highlight the need for explainability methods\ndesigned with video deep learning models, and by association spatio-temporal\ninput in mind, by first illustrating the cutting edge for video deep learning,\nand then noting the scarcity of research into explanations for these methods.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 19:34:48 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Hiley", "Liam", ""], ["Preece", "Alun", ""], ["Hicks", "Yulia", ""]]}, {"id": "1909.05669", "submitter": "Richard Mammone", "authors": "Christine Podilchuk, Siddhartha Pachhai, Robert Warfsman and Richard\n  Mammone", "title": "On-demand teleradiology using smartphone photographs as proxies for\n  DICOM images", "comments": "4 pages, 9 figures , IEEE SPMB19 conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of photographs of the screen of displayed medical images is explored\nto circumvent the challenges involved in transferring images between sites. The\nphotographs can be conveniently taken with a smartphone and analyzed remotely\nby either human or AI experts. An autoencoder preprocessor is shown to improve\nthe performance for human experts. The AI performance provided by photographs\nis shown to be statistically equivalent to using the original DICOM images. The\nautoencoder preprocessor increases the PSNR by 15 dB or greater and provides an\nAUC that is statistically equivalent to using the original DICOM images. The\nphoto approach is an alternative to IHE-based teleradiology applications while\navoiding the problems inherit in navigating the proprietary and security\nbarriers that limit DICOM communication between PACS in practice.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 06:09:36 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 14:50:09 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Podilchuk", "Christine", ""], ["Pachhai", "Siddhartha", ""], ["Warfsman", "Robert", ""], ["Mammone", "Richard", ""]]}, {"id": "1909.05675", "submitter": "Mostafa Elhoushi", "authors": "Mostafa Elhoushi, Ye Henry Tian, Zihao Chen, Farhan Shafiq, Joey Yiwei\n  Li", "title": "Accelerating Training using Tensor Decomposition", "comments": null, "journal-ref": "AAAI 2020 Artificial Intelligence of Things Workshop", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor decomposition is one of the well-known approaches to reduce the\nlatency time and number of parameters of a pre-trained model. However, in this\npaper, we propose an approach to use tensor decomposition to reduce training\ntime of training a model from scratch. In our approach, we train the model from\nscratch (i.e., randomly initialized weights) with its original architecture for\na small number of epochs, then the model is decomposed, and then continue\ntraining the decomposed model till the end. There is an optional step in our\napproach to convert the decomposed architecture back to the original\narchitecture. We present results of using this approach on both CIFAR10 and\nImagenet datasets, and show that there can be upto 2x speed up in training time\nwith accuracy drop of upto 1.5% only, and in other cases no accuracy drop. This\ntraining acceleration approach is independent of hardware and is expected to\nhave similar speed ups on both CPU and GPU platforms.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 21:15:46 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Elhoushi", "Mostafa", ""], ["Tian", "Ye Henry", ""], ["Chen", "Zihao", ""], ["Shafiq", "Farhan", ""], ["Li", "Joey Yiwei", ""]]}, {"id": "1909.05677", "submitter": "Anthony Bourached", "authors": "Anthony Bourached, George Cann", "title": "Raiders of the Lost Art", "comments": "Submitted to NeurIPS workshop on Machine Learning for Creativity and\n  Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural style transfer, first proposed by Gatys et al. (2015), can be used to\ncreate novel artistic work through rendering a content image in the form of a\nstyle image. We present a novel method of reconstructing lost artwork, by\napplying neural style transfer to x-radiographs of artwork with secondary\ninterior artwork beneath a primary exterior, so as to reconstruct lost artwork.\nFinally we reflect on AI art exhibitions and discuss the social, cultural,\nethical, and philosophical impact of these technical innovations.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 12:14:04 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Bourached", "Anthony", ""], ["Cann", "George", ""]]}, {"id": "1909.05686", "submitter": "Preeti Gopal Ms.", "authors": "Preeti Gopal and Sharat Chandran and Imants Svalbe and Ajit Rajwade", "title": "Tomographic reconstruction to detect evolving structures", "comments": "33 pages, 18 figures. arXiv admin note: text overlap with\n  arXiv:1812.10998", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for tomographic reconstruction from sparse measurements arises when\nthe measurement process is potentially harmful, needs to be rapid, or is\nuneconomical. In such cases, information from previous longitudinal scans of\nthe same object helps to reconstruct the current object while requiring\nsignificantly fewer updating measurements. Our work is based on longitudinal\ndata acquisition scenarios where we wish to study new changes that evolve\nwithin an object over time, such as in repeated scanning for disease\nmonitoring, or in tomography-guided surgical procedures. While this is easily\nfeasible when measurements are acquired from a large number of projection\nviews, it is challenging when the number of views is limited. If the goal is to\ntrack the changes while simultaneously reducing sub-sampling artefacts, we\npropose (1) acquiring measurements from a small number of views and using a\nglobal unweighted prior-based reconstruction. If the goal is to observe details\nof new changes, we propose (2) acquiring measurements from a moderate number of\nviews and using a more involved reconstruction routine. We show that in the\nlatter case, a weighted technique is necessary in order to prevent the prior\nfrom adversely affecting the reconstruction of new structures that are absent\nin any of the earlier scans. The reconstruction of new regions is safeguarded\nfrom the bias of the prior by computing regional weights that moderate the\nlocal influence of the priors. We are thus able to effectively reconstruct both\nthe old and the new structures in the test. In addition to testing on simulated\ndata, we have validated the efficacy of our method on real tomographic data.\nThe results demonstrate the use of both unweighted and weighted priors in\ndifferent scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 13:48:42 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Gopal", "Preeti", ""], ["Chandran", "Sharat", ""], ["Svalbe", "Imants", ""], ["Rajwade", "Ajit", ""]]}, {"id": "1909.05687", "submitter": "Bartosz Borucki", "authors": "Norbert Kapinski, Jedrzej M. Nowosielski, Maciej E. Marchwiany, Jakub\n  Zielinski, Beata Ciszkowska-Lyson, Bartosz A. Borucki, Tomasz Trzcinski,\n  Krzysztof S. Nowinski", "title": "Late fusion of deep learning and hand-crafted features for Achilles\n  tendon healing monitoring", "comments": "Paper accepted to MICCAI'19 MSKI workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Healing process assessment of the Achilles tendon is usually a complex\nprocedure that relies on a combination of biomechanical and medical imaging\ntests. As a result, diagnostics remains a tedious and long-lasting task.\nRecently, a novel method for the automatic assessment of tendon healing based\non Magnetic Resonance Imaging and deep learning was introduced. The method\nassesses six parameters related to the treatment progress utilizing a modified\npre-trained network, PCA-reduced space, and linear regression. In this paper,\nwe propose to improve this approach by incorporating hand-crafted features. We\nfirst perform a feature selection in order to obtain optimal sets of mixed\nhand-crafted and deep learning predictors. With the use of approx. 20,000 MRI\nslices, we then train a meta-regression algorithm that performs the tendon\nhealing assessment. Finally, we evaluate the method against scores given by an\nexperienced radiologist. In comparison with the previous baseline method, our\napproach significantly improves correlation in all of the six parameters\nassessed. Furthermore, our method uses only one MRI protocol and saves up to\n60\\% of the time needed for data acquisition.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 11:21:25 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Kapinski", "Norbert", ""], ["Nowosielski", "Jedrzej M.", ""], ["Marchwiany", "Maciej E.", ""], ["Zielinski", "Jakub", ""], ["Ciszkowska-Lyson", "Beata", ""], ["Borucki", "Bartosz A.", ""], ["Trzcinski", "Tomasz", ""], ["Nowinski", "Krzysztof S.", ""]]}, {"id": "1909.05690", "submitter": "Kaili Wang", "authors": "Kaili Wang, Jose Oramas, Tinne Tuytelaars", "title": "In Defense of LSTMs for Addressing Multiple Instance Learning Problems", "comments": "accepted in ACCV 2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSTMs have a proven track record in analyzing sequential data. But what about\nunordered instance bags, as found under a Multiple Instance Learning (MIL)\nsetting? While not often used for this, we show LSTMs excell under this setting\ntoo. In addition, we show thatLSTMs are capable of indirectly capturing\ninstance-level information us-ing only bag-level annotations. Thus, they can be\nused to learn instance-level models in a weakly supervised manner. Our\nempirical evaluation on both simplified (MNIST) and realistic (Lookbook and\nHistopathology) datasets shows that LSTMs are competitive with or even surpass\nstate-of-the-art methods specially designed for handling specific MIL problems.\nMoreover, we show that their performance on instance-level prediction is close\nto that of fully-supervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 09:14:08 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 09:19:30 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2020 20:56:13 GMT"}, {"version": "v4", "created": "Fri, 18 Sep 2020 09:46:33 GMT"}, {"version": "v5", "created": "Thu, 14 Jan 2021 09:56:52 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wang", "Kaili", ""], ["Oramas", "Jose", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1909.05693", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Zizhou Jia, Hui Chen, Leida Li, Guiguang Ding, Kurt\n  Keutzer", "title": "PDANet: Polarity-consistent Deep Attention Network for Fine-grained\n  Visual Emotion Regression", "comments": "Accepted by ACM Multimedia 2019", "journal-ref": null, "doi": "10.1145/3343031.3351062", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods on visual emotion analysis mainly focus on coarse-grained\nemotion classification, i.e. assigning an image with a dominant discrete\nemotion category. However, these methods cannot well reflect the complexity and\nsubtlety of emotions. In this paper, we study the fine-grained regression\nproblem of visual emotions based on convolutional neural networks (CNNs).\nSpecifically, we develop a Polarity-consistent Deep Attention Network (PDANet),\na novel network architecture that integrates attention into a CNN with an\nemotion polarity constraint. First, we propose to incorporate both spatial and\nchannel-wise attentions into a CNN for visual emotion regression, which jointly\nconsiders the local spatial connectivity patterns along each channel and the\ninterdependency between different channels. Second, we design a novel\nregression loss, i.e. polarity-consistent regression (PCR) loss, based on the\nweakly supervised emotion polarity to guide the attention generation. By\noptimizing the PCR loss, PDANet can generate a polarity preserved attention map\nand thus improve the emotion regression performance. Extensive experiments are\nconducted on the IAPS, NAPS, and EMOTIC datasets, and the results demonstrate\nthat the proposed PDANet outperforms the state-of-the-art approaches by a large\nmargin for fine-grained visual emotion regression. Our source code is released\nat: https://github.com/ZizhouJia/PDANet.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 05:16:36 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Zhao", "Sicheng", ""], ["Jia", "Zizhou", ""], ["Chen", "Hui", ""], ["Li", "Leida", ""], ["Ding", "Guiguang", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1909.05704", "submitter": "Carlos Caetano", "authors": "Carlos Caetano, Fran\\c{c}ois Br\\'emond, William Robson Schwartz", "title": "Skeleton Image Representation for 3D Action Recognition based on Tree\n  Structure and Reference Joints", "comments": "Conference on Graphics, Patterns and Images (SIBGRAPI2019). arXiv\n  admin note: substantial text overlap with arXiv:1907.13025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last years, the computer vision research community has studied on how\nto model temporal dynamics in videos to employ 3D human action recognition. To\nthat end, two main baseline approaches have been researched: (i) Recurrent\nNeural Networks (RNNs) with Long-Short Term Memory (LSTM); and (ii) skeleton\nimage representations used as input to a Convolutional Neural Network (CNN).\nAlthough RNN approaches present excellent results, such methods lack the\nability to efficiently learn the spatial relations between the skeleton joints.\nOn the other hand, the representations used to feed CNN approaches present the\nadvantage of having the natural ability of learning structural information from\n2D arrays (i.e., they learn spatial relations from the skeleton joints). To\nfurther improve such representations, we introduce the Tree Structure Reference\nJoints Image (TSRJI), a novel skeleton image representation to be used as input\nto CNNs. The proposed representation has the advantage of combining the use of\nreference joints and a tree structure skeleton. While the former incorporates\ndifferent spatial relationships between the joints, the latter preserves\nimportant spatial relations by traversing a skeleton tree with a depth-first\norder algorithm. Experimental results demonstrate the effectiveness of the\nproposed representation for 3D action recognition on two datasets achieving\nstate-of-the-art results on the recent NTU RGB+D~120 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 16:35:06 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Caetano", "Carlos", ""], ["Br\u00e9mond", "Fran\u00e7ois", ""], ["Schwartz", "William Robson", ""]]}, {"id": "1909.05730", "submitter": "Timothy Patten", "authors": "Dominik Bauer, Timothy Patten, Markus Vincze", "title": "VeREFINE: Integrating Object Pose Verification with Physics-guided\n  Iterative Refinement", "comments": "Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and robust object pose estimation for robotics applications requires\nverification and refinement steps. In this work, we propose to integrate\nhypotheses verification with object pose refinement guided by physics\nsimulation. This allows the physical plausibility of individual object pose\nestimates and the stability of the estimated scene to be considered in a\nunified optimization. The proposed method is able to adapt to scenes of\nmultiple objects and efficiently focuses on refining the most promising object\nposes in multi-hypotheses scenarios. We call this integrated approach VeREFINE\nand evaluate it on three datasets with varying scene complexity. The generality\nof the approach is shown by using three state-of-the-art pose estimators and\nthree baseline refiners. Results show improvements over all baselines and on\nall datasets. Furthermore, our approach is applied in real-world grasping\nexperiments and outperforms competing methods in terms of grasp success rate.\nCode is publicly available at github.com/dornik/verefine.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 14:48:00 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 14:19:09 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 11:36:02 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Bauer", "Dominik", ""], ["Patten", "Timothy", ""], ["Vincze", "Markus", ""]]}, {"id": "1909.05736", "submitter": "Boyang Deng", "authors": "Boyang Deng, Kyle Genova, Soroosh Yazdani, Sofien Bouaziz, Geoffrey\n  Hinton, Andrea Tagliasacchi", "title": "CvxNet: Learnable Convex Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any solid object can be decomposed into a collection of convex polytopes (in\nshort, convexes). When a small number of convexes are used, such a\ndecomposition can be thought of as a piece-wise approximation of the geometry.\nThis decomposition is fundamental in computer graphics, where it provides one\nof the most common ways to approximate geometry, for example, in real-time\nphysics simulation. A convex object also has the property of being\nsimultaneously an explicit and implicit representation: one can interpret it\nexplicitly as a mesh derived by computing the vertices of a convex hull, or\nimplicitly as the collection of half-space constraints or support functions.\nTheir implicit representation makes them particularly well suited for neural\nnetwork training, as they abstract away from the topology of the geometry they\nneed to represent. However, at testing time, convexes can also generate\nexplicit representations -- polygonal meshes -- which can then be used in any\ndownstream application. We introduce a network architecture to represent a low\ndimensional family of convexes. This family is automatically derived via an\nauto-encoding process. We investigate the applications of this architecture\nincluding automatic convex decomposition, image to 3D reconstruction, and\npart-based shape retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 14:59:52 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 15:59:46 GMT"}, {"version": "v3", "created": "Wed, 29 Jan 2020 19:18:09 GMT"}, {"version": "v4", "created": "Sun, 12 Apr 2020 23:43:12 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Deng", "Boyang", ""], ["Genova", "Kyle", ""], ["Yazdani", "Soroosh", ""], ["Bouaziz", "Sofien", ""], ["Hinton", "Geoffrey", ""], ["Tagliasacchi", "Andrea", ""]]}, {"id": "1909.05742", "submitter": "Dror Simon", "authors": "Dror Simon and Michael Elad", "title": "Rethinking the CSC Model for Natural Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation with respect to an overcomplete dictionary is often\nused when regularizing inverse problems in signal and image processing. In\nrecent years, the Convolutional Sparse Coding (CSC) model, in which the\ndictionary consists of shift-invariant filters, has gained renewed interest.\nWhile this model has been successfully used in some image processing problems,\nit still falls behind traditional patch-based methods on simple tasks such as\ndenoising.\n  In this work we provide new insights regarding the CSC model and its\ncapability to represent natural images, and suggest a Bayesian connection\nbetween this model and its patch-based ancestor. Armed with these observations,\nwe suggest a novel feed-forward network that follows an MMSE approximation\nprocess to the CSC model, using strided convolutions. The performance of this\nsupervised architecture is shown to be on par with state of the art methods\nwhile using much fewer parameters.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 15:10:21 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Simon", "Dror", ""], ["Elad", "Michael", ""]]}, {"id": "1909.05770", "submitter": "Fu-Jen Chu", "authors": "Fu-Jen Chu, Ruinian Xu, Chao Tang and Patricio A. Vela", "title": "Recognizing Object Affordances to Support Scene Reasoning for\n  Manipulation Tasks", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affordance information about a scene provides important clues as to what\nactions may be executed in pursuit of meeting a specified goal state. Thus,\nintegrating affordance-based reasoning into symbolic action plannning pipelines\nwould enhance the flexibility of robot manipulation. Unfortunately, the top\nperforming affordance recognition methods use object category priors to boost\nthe accuracy of affordance detection and segmentation. Object priors limit\ngeneralization to unknown object categories. This paper describes an affordance\nrecognition pipeline based on a category-agnostic region proposal network for\nproposing instance regions of an image across categories. To guide affordance\nlearning in the absence of category priors, the training process includes the\nauxiliary task of explicitly inferencing existing affordances within a\nproposal. Secondly, a self-attention mechanism trained to interpret each\nproposal learns to capture rich contextual dependencies through the region.\nVisual benchmarking shows that the trained network, called AffContext, reduces\nthe performance gap between object-agnostic and object-informed affordance\nrecognition. AffContext is linked to the Planning Domain Definition Language\n(PDDL) with an augmented state keeper for action planning across temporally\nspaced goal-oriented tasks. Manipulation experiments show that AffContext can\nsuccessfully parse scene content to seed a symbolic planner problem\nspecification, whose execution completes the target task. Additionally,\ntask-oriented grasping for cutting and pounding actions demonstrate the\nexploitation of multiple affordances for a given object to complete specified\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 15:58:30 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 22:39:14 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Chu", "Fu-Jen", ""], ["Xu", "Ruinian", ""], ["Tang", "Chao", ""], ["Vela", "Patricio A.", ""]]}, {"id": "1909.05773", "submitter": "Tomer Weiss", "authors": "Tomer Weiss, Ortal Senouf, Sanketh Vedula, Oleg Michailovich, Michael\n  Zibulevsky, Alex Bronstein", "title": "PILOT: Physics-Informed Learned Optimized Trajectories for Accelerated\n  MRI", "comments": "Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Magnetic Resonance Imaging (MRI) has long been considered to be among \"the\ngold standards\" of diagnostic medical imaging. The long acquisition times,\nhowever, render MRI prone to motion artifacts, let alone their adverse\ncontribution to the relative high costs of MRI examination. Over the last few\ndecades, multiple studies have focused on the development of both physical and\npost-processing methods for accelerated acquisition of MRI scans. These two\napproaches, however, have so far been addressed separately. On the other hand,\nrecent works in optical computational imaging have demonstrated growing success\nof concurrent learning-based design of data acquisition and image\nreconstruction schemes. Such schemes have already demonstrated substantial\neffectiveness, leading to considerably shorter acquisition times and improved\nquality of image reconstruction. Inspired by this initial success, in this\nwork, we propose a novel approach to the learning of optimal schemes for\nconjoint acquisition and reconstruction of MRI scans, with the optimization\ncarried out simultaneously with respect to the time-efficiency of data\nacquisition and the quality of resulting reconstructions. To be of a practical\nvalue, the schemes are encoded in the form of general k-space trajectories,\nwhose associated magnetic gradients are constrained to obey a set of predefined\nhardware requirements (as defined in terms of, e.g., peak currents and maximum\nslew rates of magnetic gradients). With this proviso in mind, we propose a\nnovel algorithm for the end-to-end training of a combined\nacquisition-reconstruction pipeline using a deep neural network with\ndifferentiable forward- and back-propagation operators. We demonstrate its\neffectiveness on image reconstruction and image segmentation tasks, reporting\nsubstantial improvements in terms of acceleration factors as well as the\nquality of these tasks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 16:10:31 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 12:01:38 GMT"}, {"version": "v3", "created": "Mon, 20 Jan 2020 11:32:35 GMT"}, {"version": "v4", "created": "Sat, 22 Aug 2020 12:44:33 GMT"}, {"version": "v5", "created": "Tue, 13 Apr 2021 06:02:39 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Weiss", "Tomer", ""], ["Senouf", "Ortal", ""], ["Vedula", "Sanketh", ""], ["Michailovich", "Oleg", ""], ["Zibulevsky", "Michael", ""], ["Bronstein", "Alex", ""]]}, {"id": "1909.05776", "submitter": "Yu Chen", "authors": "Seyed Yahya Nikouei, Yu Chen, Alexander Aved, Erik Blasch, Timothy R.\n  Faughnan", "title": "I-SAFE: Instant Suspicious Activity identiFication at the Edge using\n  Fuzzy Decision Making", "comments": "Manuscript has been accepted and to be presented at the Fourth\n  ACM/IEEE Symposium on Edge Computing, Washington DC, November 7-9, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban imagery usually serves as forensic analysis and by design is available\nfor incident mitigation. As more imagery collected, it is harder to narrow down\nto certain frames among thousands of video clips to a specific incident. A\nreal-time, proactive surveillance system is desirable, which could instantly\ndetect dubious personnel, identify suspicious activities, or raise momentous\nalerts. The recent proliferation of the edge computing paradigm allows more\ndata-intensive tasks to be accomplished by smart edge devices with lightweight\nbut powerful algorithms. This paper presents a forensic surveillance strategy\nby introducing an Instant Suspicious Activity identiFication at the Edge\n(I-SAFE) using fuzzy decision making. A fuzzy control system is proposed to\nmimic the decision-making process of a security officer. Decisions are made\nbased on video features extracted by a lightweight Deep Machine Learning (DML)\nmodel. Based on the requirements from the first-line law enforcement officers,\nseveral features are selected and fuzzified to cope with the state of\nuncertainty that exists in the officers' decision-making process. Using\nfeatures in the edge hierarchy minimizes the communication delay such that\ninstant alerting is achieved. Additionally, leveraging the Microservices\narchitecture, the I-SAFE scheme possesses good scalability given the increasing\ncomplexities at the network edge. Implemented as an edge-based application and\ntested using exemplary and various labeled dataset surveillance videos, the\nI-SAFE scheme raises alerts by identifying the suspicious activity in an\naverage of 0.002 seconds. Compared to four other state-of-the-art methods over\ntwo other data sets, the experimental study verified the superiority of the\nI-SAFE decentralized method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 16:14:37 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Nikouei", "Seyed Yahya", ""], ["Chen", "Yu", ""], ["Aved", "Alexander", ""], ["Blasch", "Erik", ""], ["Faughnan", "Timothy R.", ""]]}, {"id": "1909.05829", "submitter": "Suraj Nair", "authors": "Suraj Nair, Chelsea Finn", "title": "Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks\n  via Visual Subgoal Generation", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video prediction models combined with planning algorithms have shown promise\nin enabling robots to learn to perform many vision-based tasks through only\nself-supervision, reaching novel goals in cluttered scenes with unseen objects.\nHowever, due to the compounding uncertainty in long horizon video prediction\nand poor scalability of sampling-based planning optimizers, one significant\nlimitation of these approaches is the ability to plan over long horizons to\nreach distant goals. To that end, we propose a framework for subgoal generation\nand planning, hierarchical visual foresight (HVF), which generates subgoal\nimages conditioned on a goal image, and uses them for planning. The subgoal\nimages are directly optimized to decompose the task into easy to plan segments,\nand as a result, we observe that the method naturally identifies semantically\nmeaningful states as subgoals. Across three out of four simulated vision-based\nmanipulation tasks, we find that our method achieves nearly a 200% performance\nimprovement over planning without subgoals and model-free RL approaches.\nFurther, our experiments illustrate that our approach extends to real,\ncluttered visual scenes. Project page:\nhttps://sites.google.com/stanford.edu/hvf\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 17:36:45 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Nair", "Suraj", ""], ["Finn", "Chelsea", ""]]}, {"id": "1909.05845", "submitter": "Shivam Duggal", "authors": "Shivam Duggal, Shenlong Wang, Wei-Chiu Ma, Rui Hu, Raquel Urtasun", "title": "DeepPruner: Learning Efficient Stereo Matching via Differentiable\n  PatchMatch", "comments": "Accepted at International Conference on Computer Vision (ICCV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to significantly speed up the runtime of current state-of-the-art\nstereo algorithms to enable real-time inference. Towards this goal, we\ndeveloped a differentiable PatchMatch module that allows us to discard most\ndisparities without requiring full cost volume evaluation. We then exploit this\nrepresentation to learn which range to prune for each pixel. By progressively\nreducing the search space and effectively propagating such information, we are\nable to efficiently compute the cost volume for high likelihood hypotheses and\nachieve savings in both memory and computation. Finally, an image guided\nrefinement module is exploited to further improve the performance. Since all\nour components are differentiable, the full network can be trained end-to-end.\nOur experiments show that our method achieves competitive results on KITTI and\nSceneFlow datasets while running in real-time at 62ms.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 17:50:04 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Duggal", "Shivam", ""], ["Wang", "Shenlong", ""], ["Ma", "Wei-Chiu", ""], ["Hu", "Rui", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1909.05897", "submitter": "Prajwal Chidananda", "authors": "Prajwal Chidananda, Ayan Sinha, Adithya Rao, Douglas Lee, Andrew\n  Rabinovich (Magic Leap, Inc)", "title": "Efficient 2.5D Hand Pose Estimation via Auxiliary Multi-Task Training\n  for Embedded Devices", "comments": "CVPR Workshop on Computer Vision for Augmented and Virtual Reality,\n  Long Beach, CA, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  2D Key-point estimation is an important precursor to 3D pose estimation\nproblems for human body and hands. In this work, we discuss the data,\narchitecture, and training procedure necessary to deploy extremely efficient\n2.5D hand pose estimation on embedded devices with highly constrained memory\nand compute envelope, such as AR/VR wearables. Our 2.5D hand pose estimation\nconsists of 2D key-point estimation of joint positions on an egocentric image,\ncaptured by a depth sensor, and lifted to 2.5D using the corresponding depth\nvalues. Our contributions are two fold: (a) We discuss data labeling and\naugmentation strategies, the modules in the network architecture that\ncollectively lead to $3\\%$ the flop count and $2\\%$ the number of parameters\nwhen compared to the state of the art MobileNetV2 architecture. (b) We propose\nan auxiliary multi-task training strategy needed to compensate for the small\ncapacity of the network while achieving comparable performance to MobileNetV2.\nOur 32-bit trained model has a memory footprint of less than 300 Kilobytes,\noperates at more than 50 Hz with less than 35 MFLOPs.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 18:33:05 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Chidananda", "Prajwal", "", "Magic Leap, Inc"], ["Sinha", "Ayan", "", "Magic Leap, Inc"], ["Rao", "Adithya", "", "Magic Leap, Inc"], ["Lee", "Douglas", "", "Magic Leap, Inc"], ["Rabinovich", "Andrew", "", "Magic Leap, Inc"]]}, {"id": "1909.05904", "submitter": "Nina Tuluptceva", "authors": "Nina Tuluptceva, Bart Bakker, Irina Fedulova, Anton Konushin", "title": "Perceptual Image Anomaly Detection", "comments": "The final authenticated publication is available online at\n  https://doi.org/10.1007/978-3-030-41404-7_12", "journal-ref": "In: Palaiahnakote S., Sanniti di Baja G., Wang L., Yan W. (eds)\n  Pattern Recognition. ACPR 2019. Lecture Notes in Computer Science, vol 12046.\n  Springer, Cham", "doi": "10.1007/978-3-030-41404-7_12", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a novel method for image anomaly detection, where algorithms that\nuse samples drawn from some distribution of \"normal\" data, aim to detect\nout-of-distribution (abnormal) samples. Our approach includes a combination of\nencoder and generator for mapping an image distribution to a predefined latent\ndistribution and vice versa. It leverages Generative Adversarial Networks to\nlearn these data distributions and uses perceptual loss for the detection of\nimage abnormality. To accomplish this goal, we introduce a new similarity\nmetric, which expresses the perceived similarity between images and is robust\nto changes in image contrast. Secondly, we introduce a novel approach for the\nselection of weights of a multi-objective loss function (image reconstruction\nand distribution mapping) in the absence of a validation dataset for\nhyperparameter tuning. After training, our model measures the abnormality of\nthe input image as the perceptual dissimilarity between it and the closest\ngenerated image of the modeled data distribution. The proposed approach is\nextensively evaluated on several publicly available image benchmarks and\nachieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 18:50:08 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 09:09:06 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Tuluptceva", "Nina", ""], ["Bakker", "Bart", ""], ["Fedulova", "Irina", ""], ["Konushin", "Anton", ""]]}, {"id": "1909.05917", "submitter": "Alessandro Bruno", "authors": "Alessandro Bruno, Anna Anzalone, Carlo Vigorito (for the JEM-EUSO\n  collaboration)", "title": "A method for Cloud Mapping in the Field of View of the Infra-Red Camera\n  during the EUSO-SPB1 flight", "comments": "7 pages, 8 figures, 36th International Cosmic Ray Conference\n  -ICRC2019", "journal-ref": "36th International Cosmic Ray Conference (ICRC2019), volume=36,\n  year=2019 }", "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.EP cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  EUSO-SPB1 was released on April 24th, 2017, from the NASA balloon launch site\nin Wanaka (New Zealand) and landed on the South Pacific Ocean on May 7th. The\ndata collected by the instruments onboard the balloon were analyzed to search\nUV pulse signatures of UHECR (Ultra High Energy Cosmic Rays) air showers.\nIndirect measurements of UHECRs can be affected by cloud presence during\nnighttime, therefore it is crucial to know the meteorological conditions during\nthe observation period of the detector. During the flight, the onboard\nEUSO-SPB1 UCIRC camera (University of Chicago Infra-Red Camera), acquired\nimages in the field of view of the UV telescope. The available nighttime and\ndaytime images include information on meteorological conditions of the\natmosphere observed in two infra-red bands. The presence of clouds has been\ninvestigated employing a method developed to provide a dense cloudiness map for\neach available infra-red image. The final masks are intended to give pixel\ncloudiness information at the IR-camera pixel resolution that is nearly 4-times\nhigher than the one of the UV-camera. In this work, cloudiness maps are\nobtained by using an expert system based on the analysis of different low-level\nimage features. Furthermore, an image enhancement step was needed to be applied\nas a preprocessing step to deal with uncalibrated data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 19:26:19 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Bruno", "Alessandro", "", "for the JEM-EUSO\n  collaboration"], ["Anzalone", "Anna", "", "for the JEM-EUSO\n  collaboration"], ["Vigorito", "Carlo", "", "for the JEM-EUSO\n  collaboration"]]}, {"id": "1909.05921", "submitter": "Pratik Vaishnavi", "authors": "Pratik Vaishnavi, Kevin Eykholt, Atul Prakash, Amir Rahmati", "title": "Towards Model-Agnostic Adversarial Defenses using Adversarially Trained\n  Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial machine learning is a well-studied field of research where an\nadversary causes predictable errors in a machine learning algorithm through\nprecise manipulation of the input. Numerous techniques have been proposed to\nharden machine learning algorithms and mitigate the effect of adversarial\nattacks. Of these techniques, adversarial training, which augments the training\ndata with adversarial samples, has proven to be an effective defense with\nrespect to a certain class of attacks. However, adversarial training is\ncomputationally expensive and its improvements are limited to a single model.\nIn this work, we take a first step toward creating a model-agnostic adversarial\ndefense. We propose Adversarially-Trained Autoencoder Augmentation (AAA), the\nfirst model-agnostic adversarial defense that is robust against certain\nadaptive adversaries. We show that AAA allows us to achieve a partially\nmodel-agnostic defense by training a single autoencoder to protect multiple\npre-trained classifiers; achieving adversarial performance on par or better\nthan adversarial training without modifying the classifiers. Furthermore, we\ndemonstrate that AAA can be used to create a fully model-agnostic defense for\nMNIST and Fashion MNIST datasets by improving the adversarial performance of a\nnever before seen pre-trained classifier by at least 45% with no additional\ntraining. Finally, using a natural image corruption dataset, we show that our\napproach improves robustness to naturally corrupted images,which has been\nidentified as strongly indicative of true adversarial robustness.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 19:51:14 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 22:50:06 GMT"}, {"version": "v3", "created": "Sun, 29 Mar 2020 23:38:30 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Vaishnavi", "Pratik", ""], ["Eykholt", "Kevin", ""], ["Prakash", "Atul", ""], ["Rahmati", "Amir", ""]]}, {"id": "1909.05926", "submitter": "Rodney LaLonde III", "authors": "Rodney LaLonde, Drew Torigian, Ulas Bagci", "title": "Encoding Visual Attributes in Capsules for Explainable Medical Diagnoses", "comments": "Accepted for publication at MICCAI 2020 (23rd International\n  Conference on Medical Image Computing and Computer Assisted Intervention).\n  Code is publicly available at https://github.com/lalonderodney/X-Caps", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network based systems have largely failed to be adopted\nin many high-risk application areas, including healthcare, military, security,\ntransportation, finance, and legal, due to their highly uninterpretable\n\"black-box\" nature. Towards solving this deficiency, we teach a novel\nmulti-task capsule network to improve the explainability of predictions by\nembodying the same high-level language used by human-experts. Our explainable\ncapsule network, X-Caps, encodes high-level visual object attributes within the\nvectors of its capsules, then forms predictions based solely on these\nhuman-interpretable features. To encode attributes, X-Caps utilizes a new\nrouting sigmoid function to independently route information from child capsules\nto parents. Further, to provide radiologists with an estimate of model\nconfidence, we train our network on a distribution of expert labels, modeling\ninter-observer agreement and punishing over/under confidence during training,\nsupervised by human-experts' agreement. X-Caps simultaneously learns attribute\nand malignancy scores from a multi-center dataset of over 1000 CT scans of lung\ncancer screening patients. We demonstrate a simple 2D capsule network can\noutperform a state-of-the-art deep dense dual-path 3D CNN at capturing\nvisually-interpretable high-level attributes and malignancy prediction, while\nproviding malignancy prediction scores approaching that of non-explainable 3D\nCNNs. To the best of our knowledge, this is the first study to investigate\ncapsule networks for making predictions based on radiologist-level\ninterpretable attributes and its applications to medical image diagnosis. Code\nis publicly available at https://github.com/lalonderodney/X-Caps .\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 20:04:16 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 07:58:34 GMT"}, {"version": "v3", "created": "Wed, 8 Apr 2020 17:58:42 GMT"}, {"version": "v4", "created": "Sun, 7 Jun 2020 03:02:06 GMT"}, {"version": "v5", "created": "Sat, 20 Jun 2020 23:52:39 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["LaLonde", "Rodney", ""], ["Torigian", "Drew", ""], ["Bagci", "Ulas", ""]]}, {"id": "1909.05948", "submitter": "Filippo Maria Bianchi", "authors": "Luigi T. Luppino, Filippo M. Bianchi, Gabriele Moser, Stian N.\n  Anfinsen", "title": "Unsupervised Image Regression for Heterogeneous Change Detection", "comments": "arXiv admin note: text overlap with arXiv:1807.11766", "journal-ref": null, "doi": "10.1109/TGRS.2019.2930348", "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection in heterogeneous multitemporal satellite images is an\nemerging and challenging topic in remote sensing. In particular, one of the\nmain challenges is to tackle the problem in an unsupervised manner. In this\npaper we propose an unsupervised framework for bitemporal heterogeneous change\ndetection based on the comparison of affinity matrices and image regression.\nFirst, our method quantifies the similarity of affinity matrices computed from\nco-located image patches in the two images. This is done to automatically\nidentify pixels that are likely to be unchanged. With the identified pixels as\npseudo-training data, we learn a transformation to map the first image to the\ndomain of the other image, and vice versa. Four regression methods are selected\nto carry out the transformation: Gaussian process regression, support vector\nregression, random forest regression, and a recently proposed kernel regression\nmethod called homogeneous pixel transformation. To evaluate the potentials and\nlimitations of our framework, and also the benefits and disadvantages of each\nregression method, we perform experiments on two real data sets. The results\nindicate that the comparison of the affinity matrices can already be considered\na change detection method by itself. However, image regression is shown to\nimprove the results obtained by the previous step alone and produces accurate\nchange detection maps despite of the heterogeneity of the multitemporal input\ndata. Notably, the random forest regression approach excels by achieving\nsimilar accuracy as the other methods, but with a significantly lower\ncomputational cost and with fast and robust tuning of hyperparameters.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 12:26:11 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Luppino", "Luigi T.", ""], ["Bianchi", "Filippo M.", ""], ["Moser", "Gabriele", ""], ["Anfinsen", "Stian N.", ""]]}, {"id": "1909.05962", "submitter": "Ken C. L. Wong", "authors": "Ken C. L. Wong, Mehdi Moradi", "title": "SegNAS3D: Network Architecture Search with Derivative-Free Global\n  Optimization for 3D Image Segmentation", "comments": "This paper was accepted by the International Conference on Medical\n  Image Computing and Computer-Assisted Intervention - MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has largely reduced the need for manual feature selection in\nimage segmentation. Nevertheless, network architecture optimization and\nhyperparameter tuning are mostly manual and time consuming. Although there are\nincreasing research efforts on network architecture search in computer vision,\nmost works concentrate on image classification but not segmentation, and there\nare very limited efforts on medical image segmentation especially in 3D. To\nremedy this, here we propose a framework, SegNAS3D, for network architecture\nsearch of 3D image segmentation. In this framework, a network architecture\ncomprises interconnected building blocks that consist of operations such as\nconvolution and skip connection. By representing the block structure as a\nlearnable directed acyclic graph, hyperparameters such as the number of feature\nchannels and the option of using deep supervision can be learned together\nthrough derivative-free global optimization. Experiments on 43 3D brain\nmagnetic resonance images with 19 structures achieved an average Dice\ncoefficient of 82%. Each architecture search required less than three days on\nthree GPUs and produced architectures that were much smaller than the\nstate-of-the-art manually created architectures.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 21:51:28 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Wong", "Ken C. L.", ""], ["Moradi", "Mehdi", ""]]}, {"id": "1909.05983", "submitter": "Chuan Wang", "authors": "Jirong Zhang, Chuan Wang, Shuaicheng Liu, Lanpeng Jia, Nianjin Ye, Jue\n  Wang, Ji Zhou, Jian Sun", "title": "Content-Aware Unsupervised Deep Homography Estimation", "comments": "Accepted by ECCV 2020 (Oral, Top 2%, 3 over 3 Strong Accepts). Jirong\n  Zhang and Chuan Wang are joint first authors, and Shuaicheng Liu is the\n  corresponding author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homography estimation is a basic image alignment method in many applications.\nIt is usually conducted by extracting and matching sparse feature points, which\nare error-prone in low-light and low-texture images. On the other hand,\nprevious deep homography approaches use either synthetic images for supervised\nlearning or aerial images for unsupervised learning, both ignoring the\nimportance of handling depth disparities and moving objects in real world\napplications. To overcome these problems, in this work we propose an\nunsupervised deep homography method with a new architecture design. In the\nspirit of the RANSAC procedure in traditional methods, we specifically learn an\noutlier mask to only select reliable regions for homography estimation. We\ncalculate loss with respect to our learned deep features instead of directly\ncomparing image content as did previously. To achieve the unsupervised\ntraining, we also formulate a novel triplet loss customized for our network. We\nverify our method by conducting comprehensive comparisons on a new dataset that\ncovers a wide range of scenes with varying degrees of difficulties for the\ntask. Experimental results reveal that our method outperforms the\nstate-of-the-art including deep solutions and feature-based solutions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 22:55:21 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 10:17:03 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhang", "Jirong", ""], ["Wang", "Chuan", ""], ["Liu", "Shuaicheng", ""], ["Jia", "Lanpeng", ""], ["Ye", "Nianjin", ""], ["Wang", "Jue", ""], ["Zhou", "Ji", ""], ["Sun", "Jian", ""]]}, {"id": "1909.05992", "submitter": "David Ho", "authors": "David Joon Ho, Shuo Han, Chichen Fu, Paul Salama, Kenneth W. Dunn,\n  Edward J. Delp", "title": "Center-Extraction-Based Three Dimensional Nuclei Instance Segmentation\n  of Fluorescence Microscopy Images", "comments": "Presented at the IEEE-EMBS International Conference on Biomedical and\n  Health Informatics (BHI 2019)", "journal-ref": null, "doi": "10.1109/BHI.2019.8834516", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluorescence microscopy is an essential tool for the analysis of 3D\nsubcellular structures in tissue. An important step in the characterization of\ntissue involves nuclei segmentation. In this paper, a two-stage method for\nsegmentation of nuclei using convolutional neural networks (CNNs) is described.\nIn particular, since creating labeled volumes manually for training purposes is\nnot practical due to the size and complexity of the 3D data sets, the paper\ndescribes a method for generating synthetic microscopy volumes based on a\nspatially constrained cycle-consistent adversarial network. The proposed method\nis tested on multiple real microscopy data sets and outperforms other commonly\nused segmentation techniques.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 00:48:00 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Ho", "David Joon", ""], ["Han", "Shuo", ""], ["Fu", "Chichen", ""], ["Salama", "Paul", ""], ["Dunn", "Kenneth W.", ""], ["Delp", "Edward J.", ""]]}, {"id": "1909.05994", "submitter": "Jianing Sun", "authors": "Jianing Sun, Katarzyna Radecka, Zeljko Zilic", "title": "FoodTracker: A Real-time Food Detection Mobile Application by Deep\n  Convolutional Neural Networks", "comments": "The 16th International Conference on Machine Vision Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a mobile application made to recognize food items of multi-object\nmeal from a single image in real-time, and then return the nutrition facts with\ncomponents and approximate amounts. Our work is organized in two parts. First,\nwe build a deep convolutional neural network merging with YOLO, a\nstate-of-the-art detection strategy, to achieve simultaneous multi-object\nrecognition and localization with nearly 80% mean average precision. Second, we\nadapt our model into a mobile application with extending function for nutrition\nanalysis. After inferring and decoding the model output in the app side, we\npresent detection results that include bounding box position and class label in\neither real-time or local mode. Our model is well-suited for mobile devices\nwith negligible inference time and small memory requirements with a deep\nlearning algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 01:12:53 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 00:51:47 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Sun", "Jianing", ""], ["Radecka", "Katarzyna", ""], ["Zilic", "Zeljko", ""]]}, {"id": "1909.05995", "submitter": "Kai Li", "authors": "Kai Li and Martin Renqiang Min and Yun Fu", "title": "Rethinking Zero-Shot Learning: A Conditional Visual Classification\n  Perspective", "comments": "Accepted to ICCV 2019. First update: add project link and correct\n  some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) aims to recognize instances of unseen classes solely\nbased on the semantic descriptions of the classes. Existing algorithms usually\nformulate it as a semantic-visual correspondence problem, by learning mappings\nfrom one feature space to the other. Despite being reasonable, previous\napproaches essentially discard the highly precious discriminative power of\nvisual features in an implicit way, and thus produce undesirable results. We\ninstead reformulate ZSL as a conditioned visual classification problem, i.e.,\nclassifying visual features based on the classifiers learned from the semantic\ndescriptions. With this reformulation, we develop algorithms targeting various\nZSL settings: For the conventional setting, we propose to train a deep neural\nnetwork that directly generates visual feature classifiers from the semantic\nattributes with an episode-based training scheme; For the generalized setting,\nwe concatenate the learned highly discriminative classifiers for seen classes\nand the generated classifiers for unseen classes to classify visual features of\nall classes; For the transductive setting, we exploit unlabeled data to\neffectively calibrate the classifier generator using a novel\nlearning-without-forgetting self-training mechanism and guide the process by a\nrobust generalized cross-entropy loss. Extensive experiments show that our\nproposed algorithms significantly outperform state-of-the-art methods by large\nmargins on most benchmark datasets in all the ZSL settings. Our code is\navailable at \\url{https://github.com/kailigo/cvcZSL}\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 01:26:55 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 16:22:41 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Li", "Kai", ""], ["Min", "Martin Renqiang", ""], ["Fu", "Yun", ""]]}, {"id": "1909.05999", "submitter": "Mengnan Du", "authors": "Mengnan Du, Shiva Pentyala, Yuening Li, Xia Hu", "title": "Towards Generalizable Deepfake Detection with Locality-aware AutoEncoder", "comments": "Accepted by CIKM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With advancements of deep learning techniques, it is now possible to generate\nsuper-realistic images and videos, i.e., deepfakes. These deepfakes could reach\nmass audience and result in adverse impacts on our society. Although lots of\nefforts have been devoted to detect deepfakes, their performance drops\nsignificantly on previously unseen but related manipulations and the detection\ngeneralization capability remains a problem. Motivated by the fine-grained\nnature and spatial locality characteristics of deepfakes, we propose\nLocality-Aware AutoEncoder (LAE) to bridge the generalization gap. In the\ntraining process, we use a pixel-wise mask to regularize local interpretation\nof LAE to enforce the model to learn intrinsic representation from the forgery\nregion, instead of capturing artifacts in the training set and learning\nsuperficial correlations to perform detection. We further propose an active\nlearning framework to select the challenging candidates for labeling, which\nrequires human masks for less than 3% of the training data, dramatically\nreducing the annotation efforts to regularize interpretations. Experimental\nresults on three deepfake detection tasks indicate that LAE could focus on the\nforgery regions to make decisions. The analysis further shows that LAE\noutperforms the state-of-the-arts by 6.52%, 12.03%, and 3.08% respectively on\nthree deepfake detection tasks in terms of generalization accuracy on\npreviously unseen manipulations.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 02:01:32 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 02:27:30 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Du", "Mengnan", ""], ["Pentyala", "Shiva", ""], ["Li", "Yuening", ""], ["Hu", "Xia", ""]]}, {"id": "1909.06008", "submitter": "Zhao Kang", "authors": "Zhao Kang and Zipeng Guo and Shudong Huang and Siying Wang and Wenyu\n  Chen and Yuanzhang Su and Zenglin Xu", "title": "Multiple Partitions Aligned Clustering", "comments": "IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view clustering is an important yet challenging task due to the\ndifficulty of integrating the information from multiple representations. Most\nexisting multi-view clustering methods explore the heterogeneous information in\nthe space where the data points lie. Such common practice may cause significant\ninformation loss because of unavoidable noise or inconsistency among views.\nSince different views admit the same cluster structure, the natural space\nshould be all partitions. Orthogonal to existing techniques, in this paper, we\npropose to leverage the multi-view information by fusing partitions.\nSpecifically, we align each partition to form a consensus cluster indicator\nmatrix through a distinct rotation matrix. Moreover, a weight is assigned for\neach view to account for the clustering capacity differences of views. Finally,\nthe basic partitions, weights, and consensus clustering are jointly learned in\na unified framework. We demonstrate the effectiveness of our approach on\nseveral real datasets, where significant improvement is found over other\nstate-of-the-art multi-view clustering methods.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 02:45:13 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Kang", "Zhao", ""], ["Guo", "Zipeng", ""], ["Huang", "Shudong", ""], ["Wang", "Siying", ""], ["Chen", "Wenyu", ""], ["Su", "Yuanzhang", ""], ["Xu", "Zenglin", ""]]}, {"id": "1909.06012", "submitter": "Chao Huang", "authors": "Chao Huang, Hu Han, Qingsong Yao, Shankuan Zhu, and S. Kevin Zhou", "title": "3D U$^2$-Net: A 3D Universal U-Net for Multi-Domain Medical Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional neural networks like U-Net have been the state-of-the-art\nmethods in medical image segmentation. Practically, a network is highly\nspecialized and trained separately for each segmentation task. Instead of a\ncollection of multiple models, it is highly desirable to learn a universal data\nrepresentation for different tasks, ideally a single model with the addition of\na minimal number of parameters steered to each task. Inspired by the recent\nsuccess of multi-domain learning in image classification, for the first time we\nexplore a promising universal architecture that handles multiple medical\nsegmentation tasks and is extendable for new tasks, regardless of different\norgans and imaging modalities. Our 3D Universal U-Net (3D U$^2$-Net) is built\nupon separable convolution, assuming that {\\it images from different domains\nhave domain-specific spatial correlations which can be probed with channel-wise\nconvolution while also share cross-channel correlations which can be modeled\nwith pointwise convolution}. We evaluate the 3D U$^2$-Net on five organ\nsegmentation datasets. Experimental results show that this universal network is\ncapable of competing with traditional models in terms of segmentation accuracy,\nwhile requiring only about $1\\%$ of the parameters. Additionally, we observe\nthat the architecture can be easily and effectively adapted to a new domain\nwithout sacrificing performance in the domains used to learn the shared\nparameterization of the universal network. We put the code of 3D U$^2$-Net into\npublic domain. \\url{https://github.com/huangmozhilv/u2net_torch/}\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 15:03:08 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Huang", "Chao", ""], ["Han", "Hu", ""], ["Yao", "Qingsong", ""], ["Zhu", "Shankuan", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "1909.06023", "submitter": "Chunhua Shen", "authors": "Xinyu Zhang, Rufeng Zhang, Jiewei Cao, Dong Gong, Mingyu You, Chunhua\n  Shen", "title": "Part-Guided Attention Learning for Vehicle Instance Retrieval", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Vehicle instance retrieval often requires one to recognize the fine-grained\nvisual differences between vehicles. Besides the holistic appearance of\nvehicles which is easily affected by the viewpoint variation and distortion,\nvehicle parts also provide crucial cues to differentiate near-identical\nvehicles. Motivated by these observations, we introduce a Part-Guided Attention\nNetwork (PGAN) to pinpoint the prominent part regions and effectively combine\nthe global and part information for discriminative feature learning. PGAN first\ndetects the locations of different part components and salient regions\nregardless of the vehicle identity, which serve as the bottom-up attention to\nnarrow down the possible searching regions. To estimate the importance of\ndetected parts, we propose a Part Attention Module (PAM) to adaptively locate\nthe most discriminative regions with high-attention weights and suppress the\ndistraction of irrelevant parts with relatively low weights. The PAM is guided\nby the instance retrieval loss and therefore provides top-down attention that\nenables attention to be calculated at the level of car parts and other salient\nregions. Finally, we aggregate the global appearance and part features to\nimprove the feature performance further. The PGAN combines part-guided\nbottom-up and top-down attention, global and part visual features in an\nend-to-end framework. Extensive experiments demonstrate that the proposed\nmethod achieves new state-of-the-art vehicle instance retrieval performance on\nfour large-scale benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 03:58:18 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 00:42:52 GMT"}, {"version": "v3", "created": "Mon, 17 Feb 2020 07:12:49 GMT"}, {"version": "v4", "created": "Sat, 26 Sep 2020 09:24:41 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Zhang", "Xinyu", ""], ["Zhang", "Rufeng", ""], ["Cao", "Jiewei", ""], ["Gong", "Dong", ""], ["You", "Mingyu", ""], ["Shen", "Chunhua", ""]]}, {"id": "1909.06035", "submitter": "Jiacheng Sun", "authors": "Hanwen Liang, Shifeng Zhang, Jiacheng Sun, Xingqiu He, Weiran Huang,\n  Kechen Zhuang, Zhenguo Li", "title": "DARTS+: Improved Differentiable Architecture Search with Early Stopping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a growing interest in automating the process of\nneural architecture design, and the Differentiable Architecture Search (DARTS)\nmethod makes the process available within a few GPU days. However, the\nperformance of DARTS is often observed to collapse when the number of search\nepochs becomes large. Meanwhile, lots of \"{\\em skip-connect}s\" are found in the\nselected architectures. In this paper, we claim that the cause of the collapse\nis that there exists overfitting in the optimization of DARTS. Therefore, we\npropose a simple and effective algorithm, named \"DARTS+\", to avoid the collapse\nand improve the original DARTS, by \"early stopping\" the search procedure when\nmeeting a certain criterion. We also conduct comprehensive experiments on\nbenchmark datasets and different search spaces and show the effectiveness of\nour DARTS+ algorithm, and DARTS+ achieves $2.32\\%$ test error on CIFAR10,\n$14.87\\%$ on CIFAR100, and $23.7\\%$ on ImageNet. We further remark that the\nidea of \"early stopping\" is implicitly included in some existing DARTS variants\nby manually setting a small number of search epochs, while we give an {\\em\nexplicit} criterion for \"early stopping\".\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 05:07:57 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 06:21:28 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Liang", "Hanwen", ""], ["Zhang", "Shifeng", ""], ["Sun", "Jiacheng", ""], ["He", "Xingqiu", ""], ["Huang", "Weiran", ""], ["Zhuang", "Kechen", ""], ["Li", "Zhenguo", ""]]}, {"id": "1909.06043", "submitter": "Bo Chen", "authors": "Bo Chen, Alvaro Parra, Jiewei Cao, Nan Li, Tat-Jun Chin", "title": "End-to-End Learnable Geometric Vision by Backpropagating PnP\n  Optimization", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks excel in learning patterns from large amounts of data. On the\nother hand, many geometric vision tasks are specified as optimization problems.\nTo seamlessly combine deep learning and geometric vision, it is vital to\nperform learning and geometric optimization end-to-end. Towards this aim, we\npresent BPnP, a novel network module that backpropagates gradients through a\nPerspective-n-Points (PnP) solver to guide parameter updates of a neural\nnetwork. Based on implicit differentiation, we show that the gradients of a\n\"self-contained\" PnP solver can be derived accurately and efficiently, as if\nthe optimizer block were a differentiable function. We validate BPnP by\nincorporating it in a deep model that can learn camera intrinsics, camera\nextrinsics (poses) and 3D structure from training datasets. Further, we develop\nan end-to-end trainable pipeline for object pose estimation, which achieves\ngreater accuracy by combining feature-based heatmap losses with 2D-3D\nreprojection errors. Since our approach can be extended to other optimization\nproblems, our work helps to pave the way to perform learnable geometric vision\nin a principled manner. Our PyTorch implementation of BPnP is available on\nhttp://github.com/BoChenYS/BPnP.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 05:45:25 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 07:01:43 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2020 08:05:16 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Chen", "Bo", ""], ["Parra", "Alvaro", ""], ["Cao", "Jiewei", ""], ["Li", "Nan", ""], ["Chin", "Tat-Jun", ""]]}, {"id": "1909.06045", "submitter": "Ajay Kumar", "authors": "Ritesh Vyas and Ajay Kumar", "title": "A Collaborative Approach using Ridge-Valley Minutiae for More Accurate\n  Contactless Fingerprint Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contactless fingerprint identification has emerged as an reliable and user\nfriendly alternative for the personal identification in a range of e-business\nand law-enforcement applications. It is however quite known from the literature\nthat the contactless fingerprint images deliver remarkably low matching\naccuracies as compared with those obtained from the contact-based fingerprint\nsensors. This paper develops a new approach to significantly improve\ncontactless fingerprint matching capabilities available today. We\nsystematically analyze the extent of complimentary ridge-valley information and\nintroduce new approaches to achieve significantly higher matching accuracy over\nstate-of-art fingerprint matchers commonly employed today. We also investigate\nleast explored options for the fingerprint color-space conversions, which can\nplay a key-role for more accurate contactless fingerprint matching. This paper\npresents experimental results from different publicly available contactless\nfingerprint databases using NBIS, MCC and COTS matchers. Our consistently\noutperforming results validate the effectiveness of the proposed approach for\nmore accurate contactless fingerprint identification.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 05:56:52 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 07:40:23 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Vyas", "Ritesh", ""], ["Kumar", "Ajay", ""]]}, {"id": "1909.06087", "submitter": "Zheng Zhu", "authors": "Zheng Zhu, Hongxuan Ma, Wei Zou", "title": "Human Following for Wheeled Robot with Monocular Pan-tilt Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human following on mobile robots has witnessed significant advances due to\nits potentials for real-world applications. Currently most human following\nsystems are equipped with depth sensors to obtain distance information between\nhuman and robot, which suffer from the perception requirements and noises. In\nthis paper, we design a wheeled mobile robot system with monocular pan-tilt\ncamera to follow human, which can stay the target in the field of view and keep\nfollowing simultaneously. The system consists of fast human detector, real-time\nand accurate visual tracker, and unified controller for mobile robot and\npan-tilt camera. In visual tracking algorithm, both Siamese networks and\noptical flow information are exploited to locate and regress human\nsimultaneously. In order in perform following with a monocular camera, the\nconstraint of human height is introduced to design the controller. In\nexperiments, human following are conducted and analysed in simulations and a\nreal robot platform, which demonstrate the effectiveness and robustness of the\noverall system.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 08:49:12 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Zhu", "Zheng", ""], ["Ma", "Hongxuan", ""], ["Zou", "Wei", ""]]}, {"id": "1909.06119", "submitter": "Guillaume Rochette", "authors": "Guillaume Rochette, Chris Russell, Richard Bowden", "title": "Weakly-Supervised 3D Pose Estimation from a Single Image using\n  Multi-View Consistency", "comments": "BMVC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel data-driven regularizer for weakly-supervised learning of\n3D human pose estimation that eliminates the drift problem that affects\nexisting approaches. We do this by moving the stereo reconstruction problem\ninto the loss of the network itself. This avoids the need to reconstruct 3D\ndata prior to training and unlike previous semi-supervised approaches, avoids\nthe need for a warm-up period of supervised training. The conceptual and\nimplementational simplicity of our approach is fundamental to its appeal. Not\nonly is it straightforward to augment many weakly-supervised approaches with\nour additional re-projection based loss, but it is obvious how it shapes\nreconstructions and prevents drift. As such we believe it will be a valuable\ntool for any researcher working in weakly-supervised 3D reconstruction.\nEvaluating on Panoptic, the largest multi-camera and markerless dataset\navailable, we obtain an accuracy that is essentially indistinguishable from a\nstrongly-supervised approach making full use of 3D groundtruth in training.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 09:59:12 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Rochette", "Guillaume", ""], ["Russell", "Chris", ""], ["Bowden", "Richard", ""]]}, {"id": "1909.06121", "submitter": "Li Zhang", "authors": "Li Zhang, Xiangtai Li, Anurag Arnab, Kuiyuan Yang, Yunhai Tong, Philip\n  H.S. Torr", "title": "Dual Graph Convolutional Network for Semantic Segmentation", "comments": "BMVC 2019. Code is available at\n  \\url{https://github.com/lxtGH/GALD-DGCNet}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting long-range contextual information is key for pixel-wise prediction\ntasks such as semantic segmentation. In contrast to previous work that uses\nmulti-scale feature fusion or dilated convolutions, we propose a novel\ngraph-convolutional network (GCN) to address this problem. Our Dual Graph\nConvolutional Network (DGCNet) models the global context of the input feature\nby modelling two orthogonal graphs in a single framework. The first component\nmodels spatial relationships between pixels in the image, whilst the second\nmodels interdependencies along the channel dimensions of the network's feature\nmap. This is done efficiently by projecting the feature into a new,\nlower-dimensional space where all pairwise interactions can be modelled, before\nreprojecting into the original space. Our simple method provides substantial\nbenefits over a strong baseline and achieves state-of-the-art results on both\nCityscapes (82.0% mean IoU) and Pascal Context (53.7% mean IoU) datasets. Code\nand models are made available to foster any further research\n(\\url{https://github.com/lxtGH/GALD-DGCNet}).\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 10:06:55 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 13:30:10 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 04:52:15 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Zhang", "Li", ""], ["Li", "Xiangtai", ""], ["Arnab", "Anurag", ""], ["Yang", "Kuiyuan", ""], ["Tong", "Yunhai", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1909.06122", "submitter": "Run Wang", "authors": "Run Wang, Felix Juefei-Xu, Lei Ma, Xiaofei Xie, Yihao Huang, Jian\n  Wang, Yang Liu", "title": "FakeSpotter: A Simple yet Robust Baseline for Spotting AI-Synthesized\n  Fake Faces", "comments": "Accepted to IJCAI 2020; SOLE copyright holder is IJCAI (international\n  Joint Conferences on Artificial Intelligence), all rights reserved.\n  https://www.ijcai.org/Proceedings/2020/333", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, generative adversarial networks (GANs) and its variants have\nachieved unprecedented success in image synthesis. They are widely adopted in\nsynthesizing facial images which brings potential security concerns to humans\nas the fakes spread and fuel the misinformation. However, robust detectors of\nthese AI-synthesized fake faces are still in their infancy and are not ready to\nfully tackle this emerging challenge. In this work, we propose a novel\napproach, named FakeSpotter, based on monitoring neuron behaviors to spot\nAI-synthesized fake faces. The studies on neuron coverage and interactions have\nsuccessfully shown that they can be served as testing criteria for deep\nlearning systems, especially under the settings of being exposed to adversarial\nattacks. Here, we conjecture that monitoring neuron behavior can also serve as\nan asset in detecting fake faces since layer-by-layer neuron activation\npatterns may capture more subtle features that are important for the fake\ndetector. Experimental results on detecting four types of fake faces\nsynthesized with the state-of-the-art GANs and evading four perturbation\nattacks show the effectiveness and robustness of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 10:08:44 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 06:02:29 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 06:44:53 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Wang", "Run", ""], ["Juefei-Xu", "Felix", ""], ["Ma", "Lei", ""], ["Xie", "Xiaofei", ""], ["Huang", "Yihao", ""], ["Wang", "Jian", ""], ["Liu", "Yang", ""]]}, {"id": "1909.06148", "submitter": "Minghan Li", "authors": "Minghan Li, Xiangyong Cao, Qian Zhao, Lei Zhang, Chenqiang Gao, Deyu\n  Meng", "title": "Video Rain/Snow Removal by Transformed Online Multiscale Convolutional\n  Sparse Coding", "comments": "14 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video rain/snow removal from surveillance videos is an important task in the\ncomputer vision community since rain/snow existed in videos can severely\ndegenerate the performance of many surveillance system. Various methods have\nbeen investigated extensively, but most only consider consistent rain/snow\nunder stable background scenes. Rain/snow captured from practical surveillance\ncamera, however, is always highly dynamic in time with the background scene\ntransformed occasionally. To this issue, this paper proposes a novel rain/snow\nremoval approach, which fully considers dynamic statistics of both rain/snow\nand background scenes taken from a video sequence. Specifically, the rain/snow\nis encoded as an online multi-scale convolutional sparse coding (OMS-CSC)\nmodel, which not only finely delivers the sparse scattering and multi-scale\nshapes of real rain/snow, but also well encodes their temporally dynamic\nconfigurations by real-time ameliorated parameters in the model. Furthermore, a\ntransformation operator imposed on the background scenes is further embedded\ninto the proposed model, which finely conveys the dynamic background\ntransformations, such as rotations, scalings and distortions, inevitably\nexisted in a real video sequence. The approach so constructed can naturally\nbetter adapt to the dynamic rain/snow as well as background changes, and also\nsuitable to deal with the streaming video attributed its online learning mode.\nThe proposed model is formulated in a concise maximum a posterior (MAP)\nframework and is readily solved by the ADMM algorithm. Compared with the\nstate-of-the-art online and offline video rain/snow removal methods, the\nproposed method achieves better performance on synthetic and real videos\ndatasets both visually and quantitatively. Specifically, our method can be\nimplemented in relatively high efficiency, showing its potential to real-time\nvideo rain/snow removal.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 11:22:29 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Li", "Minghan", ""], ["Cao", "Xiangyong", ""], ["Zhao", "Qian", ""], ["Zhang", "Lei", ""], ["Gao", "Chenqiang", ""], ["Meng", "Deyu", ""]]}, {"id": "1909.06161", "submitter": "Jonas Kubilius", "authors": "Jonas Kubilius, Martin Schrimpf, Kohitij Kar, Ha Hong, Najib J. Majaj,\n  Rishi Rajalingham, Elias B. Issa, Pouya Bashivan, Jonathan Prescott-Roy,\n  Kailyn Schmidt, Aran Nayebi, Daniel Bear, Daniel L. K. Yamins, and James J.\n  DiCarlo", "title": "Brain-Like Object Recognition with High-Performing Shallow Recurrent\n  ANNs", "comments": "NeurIPS 2019 (Oral). Code available at\n  https://github.com/dicarlolab/neurips2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional artificial neural networks (ANNs) are the leading class of\ncandidate models of the mechanisms of visual processing in the primate ventral\nstream. While initially inspired by brain anatomy, over the past years, these\nANNs have evolved from a simple eight-layer architecture in AlexNet to\nextremely deep and branching architectures, demonstrating increasingly better\nobject categorization performance, yet bringing into question how brain-like\nthey still are. In particular, typical deep models from the machine learning\ncommunity are often hard to map onto the brain's anatomy due to their vast\nnumber of layers and missing biologically-important connections, such as\nrecurrence. Here we demonstrate that better anatomical alignment to the brain\nand high performance on machine learning as well as neuroscience measures do\nnot have to be in contradiction. We developed CORnet-S, a shallow ANN with four\nanatomically mapped areas and recurrent connectivity, guided by Brain-Score, a\nnew large-scale composite of neural and behavioral benchmarks for quantifying\nthe functional fidelity of models of the primate ventral visual stream. Despite\nbeing significantly shallower than most models, CORnet-S is the top model on\nBrain-Score and outperforms similarly compact models on ImageNet. Moreover, our\nextensive analyses of CORnet-S circuitry variants reveal that recurrence is the\nmain predictive factor of both Brain-Score and ImageNet top-1 performance.\nFinally, we report that the temporal evolution of the CORnet-S \"IT\" neural\npopulation resembles the actual monkey IT population dynamics. Taken together,\nthese results establish CORnet-S, a compact, recurrent ANN, as the current best\nmodel of the primate ventral visual stream.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 12:09:34 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 07:30:42 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Kubilius", "Jonas", ""], ["Schrimpf", "Martin", ""], ["Kar", "Kohitij", ""], ["Hong", "Ha", ""], ["Majaj", "Najib J.", ""], ["Rajalingham", "Rishi", ""], ["Issa", "Elias B.", ""], ["Bashivan", "Pouya", ""], ["Prescott-Roy", "Jonathan", ""], ["Schmidt", "Kailyn", ""], ["Nayebi", "Aran", ""], ["Bear", "Daniel", ""], ["Yamins", "Daniel L. K.", ""], ["DiCarlo", "James J.", ""]]}, {"id": "1909.06175", "submitter": "Markus Roland Ernst", "authors": "Markus Roland Ernst, Jochen Triesch and Thomas Burwick", "title": "Recurrent Connectivity Aids Recognition of Partly Occluded Objects", "comments": "9 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:1907.08831", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feedforward convolutional neural networks are the prevalent model of core\nobject recognition. For challenging conditions, such as occlusion,\nneuroscientists believe that the recurrent connectivity in the visual cortex\naids object recognition. In this work we investigate if and how artificial\nneural networks can also benefit from recurrent connectivity. For this we\nsystematically compare architectures comprised of bottom-up (B), lateral (L)\nand top-down (T) connections. To evaluate performance, we introduce two novel\nstereoscopic occluded object datasets, which bridge the gap from classifying\ndigits to recognizing 3D objects. The task consists of recognizing one target\nobject occluded by multiple occluder objects. We find that recurrent models\nperform significantly better than their feedforward counterparts, which were\nmatched in parametric complexity. We show that for challenging stimuli, the\nrecurrent feedback is able to correctly revise the initial feedforward guess of\nthe network. Overall, our results suggest that both artificial and biological\nneural networks can exploit recurrence for improved object recognition.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 16:42:34 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Ernst", "Markus Roland", ""], ["Triesch", "Jochen", ""], ["Burwick", "Thomas", ""]]}, {"id": "1909.06216", "submitter": "Xiaotian Li", "authors": "Xiaotian Li, Shuzhe Wang, Yi Zhao, Jakob Verbeek, Juho Kannala", "title": "Hierarchical Scene Coordinate Classification and Regression for Visual\n  Localization", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual localization is critical to many applications in computer vision and\nrobotics. To address single-image RGB localization, state-of-the-art\nfeature-based methods match local descriptors between a query image and a\npre-built 3D model. Recently, deep neural networks have been exploited to\nregress the mapping between raw pixels and 3D coordinates in the scene, and\nthus the matching is implicitly performed by the forward pass through the\nnetwork. However, in a large and ambiguous environment, learning such a\nregression task directly can be difficult for a single network. In this work,\nwe present a new hierarchical scene coordinate network to predict pixel scene\ncoordinates in a coarse-to-fine manner from a single RGB image. The network\nconsists of a series of output layers, each of them conditioned on the previous\nones. The final output layer predicts the 3D coordinates and the others produce\nprogressively finer discrete location labels. The proposed method outperforms\nthe baseline regression-only network and allows us to train compact models\nwhich scale robustly to large environments. It sets a new state-of-the-art for\nsingle-image RGB localization performance on the 7-Scenes, 12-Scenes, Cambridge\nLandmarks datasets, and three combined scenes. Moreover, for large-scale\noutdoor localization on the Aachen Day-Night dataset, we present a hybrid\napproach which outperforms existing scene coordinate regression methods, and\nreduces significantly the performance gap w.r.t. explicit feature matching\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 13:21:52 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 18:22:21 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2020 22:28:48 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Li", "Xiaotian", ""], ["Wang", "Shuzhe", ""], ["Zhao", "Yi", ""], ["Verbeek", "Jakob", ""], ["Kannala", "Juho", ""]]}, {"id": "1909.06236", "submitter": "Sohrab Ferdowsi", "authors": "Sohrab Ferdowsi, Maurits Diephuis, Shideh Rezaeifar, Slava\n  Voloshynovskiy", "title": "$\\rho$-VAE: Autoregressive parametrization of the VAE encoder", "comments": "Submitted to NeurIPS workshop on Bayesian deep learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We make a minimal, but very effective alteration to the VAE model. This is\nabout a drop-in replacement for the (sample-dependent) approximate posterior to\nchange it from the standard white Gaussian with diagonal covariance to the\nfirst-order autoregressive Gaussian. We argue that this is a more reasonable\nchoice to adopt for natural signals like images, as it does not force the\nexisting correlation in the data to disappear in the posterior. Moreover, it\nallows more freedom for the approximate posterior to match the true posterior.\nThis allows for the repararametrization trick, as well as the KL-divergence\nterm to still have closed-form expressions, obviating the need for its\nsample-based estimation. Although providing more freedom to adapt to correlated\ndistributions, our parametrization has even less number of parameters than the\ndiagonal covariance, as it requires only two scalars, $\\rho$ and $s$, to\ncharacterize correlation and scaling, respectively. As validated by the\nexperiments, our proposition noticeably and consistently improves the quality\nof image generation in a plug-and-play manner, needing no further parameter\ntuning, and across all setups. The code to reproduce our experiments is\navailable at \\url{https://github.com/sssohrab/rho_VAE/}.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 14:01:33 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Ferdowsi", "Sohrab", ""], ["Diephuis", "Maurits", ""], ["Rezaeifar", "Shideh", ""], ["Voloshynovskiy", "Slava", ""]]}, {"id": "1909.06238", "submitter": "Masakiyo Kitazawa", "authors": "Takuya Matsumoto, Masakiyo Kitazawa, Yasuhiro Kohno", "title": "Classifying Topological Charge in SU(3) Yang-Mills Theory with Machine\n  Learning", "comments": "28 pages, 12 figures, version to appear in PTEP", "journal-ref": null, "doi": null, "report-no": "J-PARC-TH-0170", "categories": "hep-lat cs.CV cs.LG hep-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply a machine learning technique for identifying the topological charge\nof quantum gauge configurations in four-dimensional SU(3) Yang-Mills theory.\nThe topological charge density measured on the original and smoothed gauge\nconfigurations with and without dimensional reduction is used as inputs for the\nneural networks (NN) with and without convolutional layers. The gradient flow\nis used for the smoothing of the gauge field. We find that the topological\ncharge determined at a large flow time can be predicted with high accuracy from\nthe data at small flow times by the trained NN; for example, the accuracy\nexceeds $99\\%$ with the data at $t/a^2\\le0.3$. High robustness against the\nchange of simulation parameters is also confirmed with a fixed physical volume.\nWe find that the best performance is obtained when the spatial coordinates of\nthe topological charge density are fully integrated out in preprocessing, which\nimplies that our convolutional NN does not find characteristic structures in\nmulti-dimensional space relevant for the determination of the topological\ncharge.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 14:02:10 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 12:20:38 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Matsumoto", "Takuya", ""], ["Kitazawa", "Masakiyo", ""], ["Kohno", "Yasuhiro", ""]]}, {"id": "1909.06264", "submitter": "Marcos Bedo", "authors": "Gustavo Blanco, Agma J. M. Traina, Caetano Traina Jr., and Paulo M.\n  Azevedo-Marques, Ana E. S. Jorge, Daniel de Oliveira, and Marcos V. N. Bedo", "title": "A superpixel-driven deep learning approach for the analysis of\n  dermatological wounds", "comments": null, "journal-ref": null, "doi": "10.1016/j.cmpb.2019.105079", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background. The image-based identification of distinct tissues within\ndermatological wounds enhances patients' care since it requires no intrusive\nevaluations. This manuscript presents an approach, we named QTDU, that combines\ndeep learning models with superpixel-driven segmentation methods for assessing\nthe quality of tissues from dermatological ulcers.\n  Method. QTDU consists of a three-stage pipeline for the obtaining of ulcer\nsegmentation, tissues' labeling, and wounded area quantification. We set up our\napproach by using a real and annotated set of dermatological ulcers for\ntraining several deep learning models to the identification of ulcered\nsuperpixels.\n  Results. Empirical evaluations on 179,572 superpixels divided into four\nclasses showed QTDU accurately spot wounded tissues (AUC = 0.986, sensitivity =\n0.97, and specificity = 0.974) and outperformed machine-learning approaches in\nup to 8.2% regarding F1-Score through fine-tuning of a ResNet-based model.\nLast, but not least, experimental evaluations also showed QTDU correctly\nquantified wounded tissue areas within a 0.089 Mean Absolute Error ratio.\n  Conclusions. Results indicate QTDU effectiveness for both tissue segmentation\nand wounded area quantification tasks. When compared to existing\nmachine-learning approaches, the combination of superpixels and deep learning\nmodels outperformed the competitors within strong significant levels.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 14:41:19 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 21:49:54 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Blanco", "Gustavo", ""], ["Traina", "Agma J. M.", ""], ["Traina", "Caetano", "Jr."], ["Azevedo-Marques", "Paulo M.", ""], ["Jorge", "Ana E. S.", ""], ["de Oliveira", "Daniel", ""], ["Bedo", "Marcos V. N.", ""]]}, {"id": "1909.06271", "submitter": "Ziming Zhang", "authors": "Zudi Lin and Hanspeter Pfister and Ziming Zhang", "title": "White-Box Adversarial Defense via Self-Supervised Data Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of how to defend classifiers against\nadversarial attacks that fool the classifiers using subtly modified input data.\nIn contrast to previous works, here we focus on the white-box adversarial\ndefense where the attackers are granted full access to not only the classifiers\nbut also defenders to produce as strong attacks as possible. In such a context\nwe propose viewing a defender as a functional, a higher-order function that\ntakes functions as its argument to represent a function space, rather than\nfixed functions conventionally. From this perspective, a defender should be\nrealized and optimized individually for each adversarial input. To this end, we\npropose RIDE, an efficient and provably convergent self-supervised learning\nalgorithm for individual data estimation to protect the predictions from\nadversarial attacks. We demonstrate the significant improvement of adversarial\ndefense performance on image recognition, eg, 98%, 76%, 43% test accuracy on\nMNIST, CIFAR-10, and ImageNet datasets respectively under the state-of-the-art\nBPDA attacker.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 14:51:50 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Lin", "Zudi", ""], ["Pfister", "Hanspeter", ""], ["Zhang", "Ziming", ""]]}, {"id": "1909.06319", "submitter": "Yang Li", "authors": "Yang Li, Shoaib Akbar, Junier B. Oliva", "title": "Flow Models for Arbitrary Conditional Likelihoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the dependencies among features of a dataset is at the core of\nmost unsupervised learning tasks. However, a majority of generative modeling\napproaches are focused solely on the joint distribution $p(x)$ and utilize\nmodels where it is intractable to obtain the conditional distribution of some\narbitrary subset of features $x_u$ given the rest of the observed covariates\n$x_o$: $p(x_u \\mid x_o)$. Traditional conditional approaches provide a model\nfor a fixed set of covariates conditioned on another fixed set of observed\ncovariates. Instead, in this work we develop a model that is capable of\nyielding all conditional distributions $p(x_u \\mid x_o)$ (for arbitrary $x_u$)\nvia tractable conditional likelihoods. We propose a novel extension of (change\nof variables based) flow generative models, arbitrary conditioning flow models\n(AC-Flow), that can be conditioned on arbitrary subsets of observed covariates,\nwhich was previously infeasible. We apply AC-Flow to the imputation of\nfeatures, and also develop a unified platform for both multiple and single\nimputation by introducing an auxiliary objective that provides a principled\nsingle \"best guess\" for flow models. Extensive empirical evaluations show that\nour models achieve state-of-the-art performance in both single and multiple\nimputation across image inpainting and feature imputation in synthetic and\nreal-world datasets. Code is available at https://github.com/lupalab/ACFlow.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 16:35:17 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 13:30:33 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Li", "Yang", ""], ["Akbar", "Shoaib", ""], ["Oliva", "Junier B.", ""]]}, {"id": "1909.06326", "submitter": "Justin Krogue", "authors": "Justin D Krogue, Kaiyang V Cheng, Kevin M Hwang, Paul Toogood, Eric G\n  Meinberg, Erik J Geiger, Musa Zaid, Kevin C McGill, Rina Patel, Jae Ho Sohn,\n  Alexandra Wright, Bryan F Darger, Kevin A Padrez, Eugene Ozhinsky, Sharmila\n  Majumdar, Valentina Pedoia", "title": "Automatic Hip Fracture Identification and Functional Subclassification\n  with Deep Learning", "comments": "Presented at Orthopaedic Research Society, Austin, TX, Feb 2, 2019,\n  currently in submission for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG eess.IV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Hip fractures are a common cause of morbidity and mortality.\nAutomatic identification and classification of hip fractures using deep\nlearning may improve outcomes by reducing diagnostic errors and decreasing time\nto operation. Methods: Hip and pelvic radiographs from 1118 studies were\nreviewed and 3034 hips were labeled via bounding boxes and classified as\nnormal, displaced femoral neck fracture, nondisplaced femoral neck fracture,\nintertrochanteric fracture, previous ORIF, or previous arthroplasty. A deep\nlearning-based object detection model was trained to automate the placement of\nthe bounding boxes. A Densely Connected Convolutional Neural Network (DenseNet)\nwas trained on a subset of the bounding box images, and its performance\nevaluated on a held out test set and by comparison on a 100-image subset to two\ngroups of human observers: fellowship-trained radiologists and orthopaedists,\nand senior residents in emergency medicine, radiology, and orthopaedics.\nResults: The binary accuracy for fracture of our model was 93.8% (95% CI,\n91.3-95.8%), with sensitivity of 92.7% (95% CI, 88.7-95.6%), and specificity\n95.0% (95% CI, 91.5-97.3%). Multiclass classification accuracy was 90.4% (95%\nCI, 87.4-92.9%). When compared to human observers, our model achieved at least\nexpert-level classification under all conditions. Additionally, when the model\nwas used as an aid, human performance improved, with aided resident performance\napproximating unaided fellowship-trained expert performance. Conclusions: Our\ndeep learning model identified and classified hip fractures with at least\nexpert-level accuracy, and when used as an aid improved human performance, with\naided resident performance approximating that of unaided fellowship-trained\nattendings.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 15:03:43 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Krogue", "Justin D", ""], ["Cheng", "Kaiyang V", ""], ["Hwang", "Kevin M", ""], ["Toogood", "Paul", ""], ["Meinberg", "Eric G", ""], ["Geiger", "Erik J", ""], ["Zaid", "Musa", ""], ["McGill", "Kevin C", ""], ["Patel", "Rina", ""], ["Sohn", "Jae Ho", ""], ["Wright", "Alexandra", ""], ["Darger", "Bryan F", ""], ["Padrez", "Kevin A", ""], ["Ozhinsky", "Eugene", ""], ["Majumdar", "Sharmila", ""], ["Pedoia", "Valentina", ""]]}, {"id": "1909.06335", "submitter": "Tzu-Ming Harry Hsu", "authors": "Tzu-Ming Harry Hsu, Hang Qi, Matthew Brown", "title": "Measuring the Effects of Non-Identical Data Distribution for Federated\n  Visual Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning enables visual models to be trained in a\nprivacy-preserving way using real-world data from mobile devices. Given their\ndistributed nature, the statistics of the data across these devices is likely\nto differ significantly. In this work, we look at the effect such non-identical\ndata distributions has on visual classification via Federated Learning. We\npropose a way to synthesize datasets with a continuous range of identicalness\nand provide performance measures for the Federated Averaging algorithm. We show\nthat performance degrades as distributions differ more, and propose a\nmitigation strategy via server momentum. Experiments on CIFAR-10 demonstrate\nimproved classification performance over a range of non-identicalness, with\nclassification accuracy improved from 30.1% to 76.9% in the most skewed\nsettings.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 17:26:20 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Hsu", "Tzu-Ming Harry", ""], ["Qi", "Hang", ""], ["Brown", "Matthew", ""]]}, {"id": "1909.06337", "submitter": "Mohammadreza Soltaninejad PhD", "authors": "Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou, Guang Yang,\n  Nigel Allinson, Xujiong Ye", "title": "MRI Brain Tumor Segmentation using Random Forests and Fully\n  Convolutional Networks", "comments": "Published in the pre-conference proceeding of \"2017 International\n  MICCAI BraTS Challenge\"", "journal-ref": "In Proceeding of 2017 International MICCAI BraTS Challenge, pp.\n  279-283 (2017)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel learning based method for automated\nsegmentation of brain tumor in multimodal MRI images, which incorporates two\nsets of machine -learned and hand crafted features. Fully convolutional\nnetworks (FCN) forms the machine learned features and texton based features are\nconsidered as hand-crafted features. Random forest (RF) is used to classify the\nMRI image voxels into normal brain tissues and different parts of tumors, i.e.\nedema, necrosis and enhancing tumor. The method was evaluated on BRATS 2017\nchallenge dataset. The results show that the proposed method provides promising\nsegmentations. The mean Dice overlap measure for automatic brain tumor\nsegmentation against ground truth is 0.86, 0.78 and 0.66 for whole tumor, core\nand enhancing tumor, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 17:26:56 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Soltaninejad", "Mohammadreza", ""], ["Zhang", "Lei", ""], ["Lambrou", "Tryphon", ""], ["Yang", "Guang", ""], ["Allinson", "Nigel", ""], ["Ye", "Xujiong", ""]]}, {"id": "1909.06395", "submitter": "Elisabeth Hoppe", "authors": "Elisabeth Hoppe, Florian Thamm, Gregor K\\\"orzd\\\"orfer, Christopher\n  Syben, Franziska Schirrmacher, Mathias Nittka, Josef Pfeuffer, Heiko Meyer,\n  Andreas Maier", "title": "Magnetic Resonance Fingerprinting Reconstruction Using Recurrent Neural\n  Networks", "comments": "Accepted and presented at the German Medical Data Sciences (GMDS)\n  conference 2019 (Dortmund, Germany)", "journal-ref": "Studies in Health Technology and Informatics [01 Sep 2019,\n  267:126-133]", "doi": "10.3233/SHTI190816", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Fingerprinting (MRF) is an imaging technique acquiring\nunique time signals for different tissues. Although the acquisition is highly\naccelerated, the reconstruction time remains a problem, as the state-of-the-art\ntemplate matching compares every signal with a set of possible signals. To\novercome this limitation, deep learning based approaches, e.g. Convolutional\nNeural Networks (CNNs) have been proposed. In this work, we investigate the\napplicability of Recurrent Neural Networks (RNNs) for this reconstruction\nproblem, as the signals are correlated in time. Compared to previous methods\nbased on CNNs, RNN models yield significantly improved results using in-vivo\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 18:17:19 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Hoppe", "Elisabeth", ""], ["Thamm", "Florian", ""], ["K\u00f6rzd\u00f6rfer", "Gregor", ""], ["Syben", "Christopher", ""], ["Schirrmacher", "Franziska", ""], ["Nittka", "Mathias", ""], ["Pfeuffer", "Josef", ""], ["Meyer", "Heiko", ""], ["Maier", "Andreas", ""]]}, {"id": "1909.06399", "submitter": "Omar Elharrouss", "authors": "Omar Elharrouss, Noor Almaadeed, Somaya Al-Maadeed, Younes Akbari", "title": "Image inpainting: A review", "comments": null, "journal-ref": null, "doi": "10.1007/s11063-019-10163-0", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although image inpainting, or the art of repairing the old and deteriorated\nimages, has been around for many years, it has gained even more popularity\nbecause of the recent development in image processing techniques. With the\nimprovement of image processing tools and the flexibility of digital image\nediting, automatic image inpainting has found important applications in\ncomputer vision and has also become an important and challenging topic of\nresearch in image processing. This paper is a brief review of the existing\nimage inpainting approaches we first present a global vision on the existing\nmethods for image inpainting. We attempt to collect most of the existing\napproaches and classify them into three categories, namely, sequential-based,\nCNN-based and GAN-based methods. In addition, for each category, a list of\nmethods for the different types of distortion on the images is presented.\nFurthermore, collect a list of the available datasets and discuss these in our\npaper. This is a contribution for digital image inpainting researchers trying\nto look for the available datasets because there is a lack of datasets\navailable for image inpainting. As the final step in this overview, we present\nthe results of real evaluations of the three categories of image inpainting\nmethods performed on the datasets used, for the different types of image\ndistortion. In the end, we also present the evaluations metrics and discuss the\nperformance of these methods in terms of these metrics. This overview can be\nused as a reference for image inpainting researchers, and it can also\nfacilitate the comparison of the methods as well as the datasets used. The main\ncontribution of this paper is the presentation of the three categories of image\ninpainting methods along with a list of available datasets that the researchers\ncan use to evaluate their proposed methodology against.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 18:33:38 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Elharrouss", "Omar", ""], ["Almaadeed", "Noor", ""], ["Al-Maadeed", "Somaya", ""], ["Akbari", "Younes", ""]]}, {"id": "1909.06423", "submitter": "Valter Estevam", "authors": "Valter Estevam, Helio Pedrini, David Menotti", "title": "Zero-Shot Action Recognition in Videos: A Survey", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot Action Recognition has attracted attention in the last years and\nmany approaches have been proposed for recognition of objects, events and\nactions in images and videos. There is a demand for methods that can classify\ninstances from classes that are not present in the training of models,\nespecially in the complex problem of automatic video understanding, since\ncollecting, annotating and labeling videos are difficult and laborious tasks.\nWe have identified that there are many methods available in the literature,\nhowever, it is difficult to categorize which techniques can be considered state\nof the art. Despite the existence of some surveys about zero-shot action\nrecognition in still images and experimental protocol, there is no work focused\non videos. Therefore, we present a survey of the methods that comprise\ntechniques to perform visual feature extraction and semantic feature extraction\nas well to learn the mapping between these features considering specifically\nzero-shot action recognition in videos. We also provide a complete description\nof datasets, experiments and protocols, presenting open issues and directions\nfor future work, essential for the development of the computer vision research\nfield.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 19:57:27 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 17:10:24 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Estevam", "Valter", ""], ["Pedrini", "Helio", ""], ["Menotti", "David", ""]]}, {"id": "1909.06436", "submitter": "Isaac Gerg", "authors": "Albert Reed, Isaac Gerg, John McKay, Daniel Brown, David Williams, and\n  Suren Jayasuriya", "title": "Coupling Rendering and Generative Adversarial Networks for Artificial\n  SAS Image Generation", "comments": "10 pages, 9 figures. Submitted to IEEE OCEANS 2019 (Seattle). Updated\n  acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquisition of Synthetic Aperture Sonar (SAS) datasets is bottlenecked by the\ncostly deployment of SAS imaging systems, and even when data acquisition is\npossible,the data is often skewed towards containing barren seafloor rather\nthan objects of interest. We present a novel pipeline, called SAS GAN, which\ncouples an optical renderer with a generative adversarial network (GAN) to\nsynthesize realistic SAS images of targets on the seafloor. This coupling\nenables high levels of SAS image realism while enabling control over image\ngeometry and parameters. We demonstrate qualitative results by presenting\nexamples of images created with our pipeline. We also present quantitative\nresults through the use of t-SNE and the Fr\\'echet Inception Distance to argue\nthat our generated SAS imagery potentially augments SAS datasets more\neffectively than an off-the-shelf GAN.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 20:30:17 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 20:21:33 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Reed", "Albert", ""], ["Gerg", "Isaac", ""], ["McKay", "John", ""], ["Brown", "Daniel", ""], ["Williams", "David", ""], ["Jayasuriya", "Suren", ""]]}, {"id": "1909.06441", "submitter": "Nicolai H\\\"ani", "authors": "Nicolai H\\\"ani, Pravakar Roy and Volkan Isler", "title": "MinneApple: A Benchmark Dataset for Apple Detection and Segmentation", "comments": null, "journal-ref": null, "doi": "10.1109/LRA.2020.2965061", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a new dataset to advance the state-of-the-art in\nfruit detection, segmentation, and counting in orchard environments. While\nthere has been significant recent interest in solving these problems, the lack\nof a unified dataset has made it difficult to compare results. We hope to\nenable direct comparisons by providing a large variety of high-resolution\nimages acquired in orchards, together with human annotations of the fruit on\ntrees. The fruits are labeled using polygonal masks for each object instance to\naid in precise object detection, localization, and segmentation. Additionally,\nwe provide data for patch-based counting of clustered fruits. Our dataset\ncontains over 41, 000 annotated object instances in 1000 images. We present a\ndetailed overview of the dataset together with baseline performance analysis\nfor bounding box detection, segmentation, and fruit counting as well as\nrepresentative results for yield estimation. We make this dataset publicly\navailable and host a CodaLab challenge to encourage comparison of results on a\ncommon dataset. To download the data and learn more about MinneApple please see\nthe project website: http://rsn.cs.umn.edu/index.php/MinneApple. Up to date\ninformation is available online.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 20:47:31 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 21:15:05 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["H\u00e4ni", "Nicolai", ""], ["Roy", "Pravakar", ""], ["Isler", "Volkan", ""]]}, {"id": "1909.06446", "submitter": "Odemir Bruno PhD", "authors": "Leonardo F. S. Scabini, Lucas C. Ribas, Odemir M. Bruno", "title": "Spatio-spectral networks for color-texture analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture is one of the most-studied visual attribute for image\ncharacterization since the 1960s. However, most hand-crafted descriptors are\nmonochromatic, focusing on the gray scale images and discarding the color\ninformation. In this context, this work focus on a new method for color texture\nanalysis considering all color channels in a more intrinsic approach. Our\nproposal consists of modeling color images as directed complex networks that we\nnamed Spatio-Spectral Network (SSN). Its topology includes within-channel edges\nthat cover spatial patterns throughout individual image color channels, while\nbetween-channel edges tackle spectral properties of channel pairs in an\nopponent fashion. Image descriptors are obtained through a concise topological\ncharacterization of the modeled network in a multiscale approach with radially\nsymmetric neighborhoods. Experiments with four datasets cover several aspects\nof color-texture analysis, and results demonstrate that SSN overcomes all the\ncompared literature methods, including known deep convolutional networks, and\nalso has the most stable performance between datasets, achieving $98.5(\\pm1.1)$\nof average accuracy against $97.1(\\pm1.3)$ of MCND and $96.8(\\pm3.2)$ of\nAlexNet. Additionally, an experiment verifies the performance of the methods\nunder different color spaces, where results show that SSN also has higher\nperformance and robustness.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 20:54:59 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Scabini", "Leonardo F. S.", ""], ["Ribas", "Lucas C.", ""], ["Bruno", "Odemir M.", ""]]}, {"id": "1909.06459", "submitter": "Qi Chen", "authors": "Qi Chen", "title": "F-Cooper: Feature based Cooperative Perception for Autonomous Vehicle\n  Edge Computing System Using 3D Point Clouds", "comments": "Accepted by SEC2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles are heavily reliant upon their sensors to perfect the\nperception of surrounding environments, however, with the current state of\ntechnology, the data which a vehicle uses is confined to that from its own\nsensors. Data sharing between vehicles and/or edge servers is limited by the\navailable network bandwidth and the stringent real-time constraints of\nautonomous driving applications. To address these issues, we propose a point\ncloud feature based cooperative perception framework (F-Cooper) for connected\nautonomous vehicles to achieve a better object detection precision. Not only\nwill feature based data be sufficient for the training process, we also use the\nfeatures' intrinsically small size to achieve real-time edge computing, without\nrunning the risk of congesting the network. Our experiment results show that by\nfusing features, we are able to achieve a better object detection result,\naround 10% improvement for detection within 20 meters and 30% for further\ndistances, as well as achieve faster edge computing with a low communication\ndelay, requiring 71 milliseconds in certain feature selections. To the best of\nour knowledge, we are the first to introduce feature-level data fusion to\nconnected autonomous vehicles for the purpose of enhancing object detection and\nmaking real-time edge computing on inter-vehicle data feasible for autonomous\nvehicles.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 21:33:18 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Chen", "Qi", ""]]}, {"id": "1909.06485", "submitter": "Vahan Huroyan", "authors": "Md Iqbal Hossain and Vahan Huroyan and Stephen Kobourov and Raymundo\n  Navarrete", "title": "Multi-Perspective, Simultaneous Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe MPSE: a Multi-Perspective Simultaneous Embedding method for\nvisualizing high-dimensional data, based on multiple pairwise distances between\nthe data points. Specifically, MPSE computes positions for the points in 3D and\nprovides different views into the data by means of 2D projections (planes) that\npreserve each of the given distance matrices. We consider two versions of the\nproblem: fixed projections and variable projections. MPSE with fixed\nprojections takes as input a set of pairwise distance matrices defined on the\ndata points, along with the same number of projections and embeds the points in\n3D so that the pairwise distances are preserved in the given projections. MPSE\nwith variable projections takes as input a set of pairwise distance matrices\nand embeds the points in 3D while also computing the appropriate projections\nthat preserve the pairwise distances. The proposed approach can be useful in\nmultiple scenarios: from creating simultaneous embedding of multiple graphs on\nthe same set of vertices, to reconstructing a 3D object from multiple 2D\nsnapshots, to analyzing data from multiple points of view. We provide a\nfunctional prototype of MPSE that is based on an adaptive and stochastic\ngeneralization of multi-dimensional scaling to multiple distances and multiple\nvariable projections. We provide an extensive quantitative evaluation with\ndatasets of different sizes and using different number of projections, as well\nas several examples that illustrate the quality of the resulting solutions.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 23:20:17 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 10:41:39 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 00:55:29 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Hossain", "Md Iqbal", ""], ["Huroyan", "Vahan", ""], ["Kobourov", "Stephen", ""], ["Navarrete", "Raymundo", ""]]}, {"id": "1909.06500", "submitter": "Jian Liu", "authors": "Jian Liu, Naveed Akhtar, Ajmal Mian", "title": "Adversarial Attack on Skeleton-based Human Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning models achieve impressive performance for skeleton-based human\naction recognition. However, the robustness of these models to adversarial\nattacks remains largely unexplored due to their complex spatio-temporal nature\nthat must represent sparse and discrete skeleton joints. This work presents the\nfirst adversarial attack on skeleton-based action recognition with graph\nconvolutional networks. The proposed targeted attack, termed Constrained\nIterative Attack for Skeleton Actions (CIASA), perturbs joint locations in an\naction sequence such that the resulting adversarial sequence preserves the\ntemporal coherence, spatial integrity, and the anthropomorphic plausibility of\nthe skeletons. CIASA achieves this feat by satisfying multiple physical\nconstraints, and employing spatial skeleton realignments for the perturbed\nskeletons along with regularization of the adversarial skeletons with\nGenerative networks. We also explore the possibility of semantically\nimperceptible localized attacks with CIASA, and succeed in fooling the\nstate-of-the-art skeleton action recognition models with high confidence. CIASA\nperturbations show high transferability for black-box attacks. We also show\nthat the perturbed skeleton sequences are able to induce adversarial behavior\nin the RGB videos created with computer graphics. A comprehensive evaluation\nwith NTU and Kinetics datasets ascertains the effectiveness of CIASA for\ngraph-based skeleton action recognition and reveals the imminent threat to the\nspatio-temporal deep learning tasks in general.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 01:44:44 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Liu", "Jian", ""], ["Akhtar", "Naveed", ""], ["Mian", "Ajmal", ""]]}, {"id": "1909.06581", "submitter": "Sefi Bell-Kligler", "authors": "Sefi Bell-Kligler, Assaf Shocher, Michal Irani", "title": "Blind Super-Resolution Kernel Estimation using an Internal-GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Super resolution (SR) methods typically assume that the low-resolution (LR)\nimage was downscaled from the unknown high-resolution (HR) image by a fixed\n'ideal' downscaling kernel (e.g. Bicubic downscaling). However, this is rarely\nthe case in real LR images, in contrast to synthetically generated SR datasets.\nWhen the assumed downscaling kernel deviates from the true one, the performance\nof SR methods significantly deteriorates. This gave rise to Blind-SR - namely,\nSR when the downscaling kernel (\"SR-kernel\") is unknown. It was further shown\nthat the true SR-kernel is the one that maximizes the recurrence of patches\nacross scales of the LR image. In this paper we show how this powerful\ncross-scale recurrence property can be realized using Deep Internal Learning.\nWe introduce \"KernelGAN\", an image-specific Internal-GAN, which trains solely\non the LR test image at test time, and learns its internal distribution of\npatches. Its Generator is trained to produce a downscaled version of the LR\ntest image, such that its Discriminator cannot distinguish between the patch\ndistribution of the downscaled image, and the patch distribution of the\noriginal LR image. The Generator, once trained, constitutes the downscaling\noperation with the correct image-specific SR-kernel. KernelGAN is fully\nunsupervised, requires no training data other than the input image itself, and\nleads to state-of-the-art results in Blind-SR when plugged into existing SR\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 11:29:55 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 07:11:37 GMT"}, {"version": "v3", "created": "Wed, 18 Sep 2019 15:48:23 GMT"}, {"version": "v4", "created": "Thu, 24 Oct 2019 13:03:46 GMT"}, {"version": "v5", "created": "Sat, 4 Jan 2020 08:00:10 GMT"}, {"version": "v6", "created": "Tue, 7 Jan 2020 06:54:04 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Bell-Kligler", "Sefi", ""], ["Shocher", "Assaf", ""], ["Irani", "Michal", ""]]}, {"id": "1909.06585", "submitter": "Yaoxian Song", "authors": "Yaoxian Song, Jun Wen, Yuejiao Fei, Changbin Yu", "title": "Deep Robotic Prediction with hierarchical RGB-D Fusion", "comments": "8 pages, 8 figures, submit to ICRA2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robotic arm grasping is a fundamental operation in robotic control task\ngoals. Most current methods for robotic grasping focus on RGB-D policy in the\ntable surface scenario or 3D point cloud analysis and inference in the 3D\nspace. Comparing to these methods, we propose a novel real-time multimodal\nhierarchical encoder-decoder neural network that fuses RGB and depth data to\nrealize robotic humanoid grasping in 3D space with only partial observation.\nThe quantification of raw depth data's uncertainty and depth estimation fusing\nRGB is considered. We develop a general labeling method to label ground-truth\non common RGB-D datasets. We evaluate the effectiveness and performance of our\nmethod on a physical robot setup and our method achieves over 90\\% success rate\nin both table surface and 3D space scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 12:01:57 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 15:35:49 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Song", "Yaoxian", ""], ["Wen", "Jun", ""], ["Fei", "Yuejiao", ""], ["Yu", "Changbin", ""]]}, {"id": "1909.06591", "submitter": "Yi Sun", "authors": "Yi Sun, Xushen Han, Kai Sun, Boren Li, Yongjiang Chen, and Mingyang Li", "title": "Sem-LSD: A Learning-based Semantic Line Segment Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduces a new type of line-shaped image representation,\nnamed semantic line segment (Sem-LS) and focus on solving its detection\nproblem. Sem-LS contains high-level semantics and is a compact scene\nrepresentation where only visually salient line segments with stable semantics\nare preserved. Combined with high-level semantics, Sem-LS is more robust under\ncluttered environment compared with existing line-shaped representations. The\ncompactness of Sem-LS facilitates its use in large-scale applications, such as\ncity-scale SLAM (simultaneously localization and mapping) and LCD (loop closure\ndetection). Sem-LS detection is a challenging task due to its significantly\ndifferent appearance from existing learning-based image representations such as\nwireframes and objects. For further investigation, we first label Sem-LS on two\nwell-known datasets, KITTI and KAIST URBAN, as new benchmarks. Then, we propose\na learning-based Sem-LS detector (Sem-LSD) and devise new module as well as\nmetrics to address unique challenges in Sem-LS detection. Experimental results\nhave shown both the efficacy and efficiency of Sem-LSD. Finally, the\neffectiveness of the proposed Sem-LS is supported by two experiments on\ndetector repeatability and a city-scale LCD problem. Labeled datasets and code\nwill be released shortly.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 13:02:00 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 11:23:13 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Sun", "Yi", ""], ["Han", "Xushen", ""], ["Sun", "Kai", ""], ["Li", "Boren", ""], ["Chen", "Yongjiang", ""], ["Li", "Mingyang", ""]]}, {"id": "1909.06604", "submitter": "Kin Quan", "authors": "Kin Quan, Rebecca J. Shipley, Ryutaro Tanno, Graeme McPhillips,\n  Vasileios Vavourakis, David Edwards, Joseph Jacob, John R. Hurst, David J.\n  Hawkes", "title": "Tapering Analysis of Airways with Bronchiectasis", "comments": "12 pages, 7 figures. Previously submitted for SPIE Medical Imaging,\n  2018, Houston, Texas, United States", "journal-ref": "Proc. SPIE 10574, Medical Imaging 2018: Image Processing, 105742G\n  (2 March 2018)", "doi": "10.1117/12.2292306", "report-no": null, "categories": "eess.IV cs.CV physics.med-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bronchiectasis is the permanent dilation of airways. Patients with the\ndisease can suffer recurrent exacerbations, reducing their quality of life. The\ngold standard to diagnose and monitor bronchiectasis is accomplished by\ninspection of chest computed tomography (CT) scans. A clinician examines the\nbroncho-arterial ratio to determine if an airway is brochiectatic. The visual\nanalysis assumes the blood vessel diameter remains constant, although this\nassumption is disputed in the literature. We propose a simple measurement of\ntapering along the airways to diagnose and monitor bronchiectasis. To this end,\nwe constructed a pipeline to measure the cross-sectional area along the airways\nat contiguous intervals, starting from the carina to the most distal point\nobservable. Using a phantom with calibrated 3D printed structures, the\nprecision and accuracy of our algorithm extends to the sub voxel level. The\ntapering measurement is robust to bifurcations along the airway and was applied\nto chest CT images acquired in clinical practice. The result is a statistical\ndifference in tapering rate between airways with bronchiectasis and controls.\nOur code is available at https://github.com/quan14/AirwayTaperingInCT.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 14:46:53 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Quan", "Kin", ""], ["Shipley", "Rebecca J.", ""], ["Tanno", "Ryutaro", ""], ["McPhillips", "Graeme", ""], ["Vavourakis", "Vasileios", ""], ["Edwards", "David", ""], ["Jacob", "Joseph", ""], ["Hurst", "John R.", ""], ["Hawkes", "David J.", ""]]}, {"id": "1909.06629", "submitter": "Zhou He", "authors": "Zhou He, Siqi Bao, Albert Chung", "title": "3D Deep Affine-Invariant Shape Learning for Brain MR Image Segmentation", "comments": "Accepted to 2018 MICCAI DLMIA, published at Deep Learning in Medical\n  Image Analysis and Multimodal Learning for Clinical Decision Support", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in medical image segmentation techniques have achieved\ncompelling results. However, most of the widely used approaches do not take\ninto account any prior knowledge about the shape of the biomedical structures\nbeing segmented. More recently, some works have presented approaches to\nincorporate shape information. However, many of them are indeed introducing\nmore parameters to the segmentation network to learn the general features,\nwhich any segmentation network is able learn, instead of specifically shape\nfeatures. In this paper, we present a novel approach that seamlessly integrates\nthe shape information into the segmentation network. Experiments on human brain\nMRI segmentation demonstrate that our approach can achieve a lower Hausdorff\ndistance and higher Dice coefficient than the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 16:47:50 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 13:32:09 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["He", "Zhou", ""], ["Bao", "Siqi", ""], ["Chung", "Albert", ""]]}, {"id": "1909.06635", "submitter": "Shweta Mahajan", "authors": "Shweta Mahajan, Teresa Botschen, Iryna Gurevych, Stefan Roth", "title": "Joint Wasserstein Autoencoders for Aligning Multimodal Embeddings", "comments": "Accepted at ICCV 2019 Workshop on Cross-Modal Learning in Real World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges in learning joint embeddings of multiple\nmodalities, e.g. of images and text, is to ensure coherent cross-modal\nsemantics that generalize across datasets. We propose to address this through\njoint Gaussian regularization of the latent representations. Building on\nWasserstein autoencoders (WAEs) to encode the input in each domain, we enforce\nthe latent embeddings to be similar to a Gaussian prior that is shared across\nthe two domains, ensuring compatible continuity of the encoded semantic\nrepresentations of images and texts. Semantic alignment is achieved through\nsupervision from matching image-text pairs. To show the benefits of our\nsemi-supervised representation, we apply it to cross-modal retrieval and phrase\nlocalization. We not only achieve state-of-the-art accuracy, but significantly\nbetter generalization across datasets, owing to the semantic continuity of the\nlatent space.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 17:25:03 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Mahajan", "Shweta", ""], ["Botschen", "Teresa", ""], ["Gurevych", "Iryna", ""], ["Roth", "Stefan", ""]]}, {"id": "1909.06645", "submitter": "Kuan Huang", "authors": "Kuan Huang, Yingtao Zhang, H. D. Cheng, Ping Xing, Boyu Zhang", "title": "Fuzzy Semantic Segmentation of Breast Ultrasound Image with Breast\n  Anatomy Constraints", "comments": "15 pages, 19 figures, submitted to IEEE Transactions on Fuzzy Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is one of the most serious disease affecting women's health.\nDue to low cost, portable, no radiation, and high efficiency, breast ultrasound\n(BUS) imaging is the most popular approach for diagnosing early breast cancer.\nHowever, ultrasound images are low resolution and poor quality. Thus,\ndeveloping accurate detection system is a challenging task. In this paper, we\npropose a fully automatic segmentation algorithm consisting of two parts: fuzzy\nfully convolutional network and accurately fine-tuning post-processing based on\nbreast anatomy constraints. In the first part, the image is preprocessed by\ncontrast enhancement, and wavelet features are employed for image augmentation.\nA fuzzy membership function transforms the augmented BUS images into fuzzy\ndomain. The features from convolutional layers are processed using fuzzy logic\nas well. The conditional random fields (CRFs) post-process the segmentation\nresult. The location relation among the breast anatomy layers is utilized to\nimprove the performance. The proposed method is applied to the dataset with 325\nBUS images, and achieves state-of-art performance compared with that of\nexisting methods with true positive rate 90.33%, false positive rate 9.00%, and\nintersection over union (IoU) 81.29% on tumor category, and overall\nintersection over union (mIoU) 80.47% over five categories: fat layer, mammary\nlayer, muscle layer, background, and tumor.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 18:06:46 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 06:12:33 GMT"}, {"version": "v3", "created": "Wed, 23 Oct 2019 16:33:07 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Huang", "Kuan", ""], ["Zhang", "Yingtao", ""], ["Cheng", "H. D.", ""], ["Xing", "Ping", ""], ["Zhang", "Boyu", ""]]}, {"id": "1909.06672", "submitter": "Vikram Gupta", "authors": "Vikram Gupta, Sai Kumar Dwivedi, Rishabh Dabral, Arjun Jain", "title": "Progression Modelling for Online and Early Gesture Detection", "comments": "3DV 2019 Oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online and Early detection of gestures is crucial for building touchless\ngesture based interfaces. These interfaces should operate on a stream of video\nframes instead of the complete video and detect the presence of gestures at an\nearlier stage than post-completion for providing real time user experience. To\nachieve this, it is important to recognize the progression of the gesture\nacross different stages so that appropriate responses can be triggered on\nreaching the desired execution stage. To address this, we propose a simple yet\neffective multi-task learning framework which models the progression of the\ngesture along with frame level recognition. The proposed framework recognizes\nthe gestures at an early stage with high precision and also achieves\nstate-of-the-art recognition accuracy of 87.8% which is closer to human\naccuracy of 88.4% on the NVIDIA gesture dataset in the offline configuration\nand advances the state-of-the-art by more than 4%. We also introduce tightly\nsegmented annotations for the NVIDIA gesture dataset and setup a strong\nbaseline for gesture localization for this dataset. We also evaluate our\nframework on the Montalbano dataset and report competitive results.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 20:39:35 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Gupta", "Vikram", ""], ["Dwivedi", "Sai Kumar", ""], ["Dabral", "Rishabh", ""], ["Jain", "Arjun", ""]]}, {"id": "1909.06684", "submitter": "Ali Hatamizadeh", "authors": "Andriy Myronenko and Ali Hatamizadeh", "title": "3D Kidneys and Kidney Tumor Semantic Segmentation using Boundary-Aware\n  Networks", "comments": "Manuscript of MICCAI Kidney Tumor Segmentation Challenge 2019", "journal-ref": "MICCAI Kidney Tumor Segmentation Challenge 2019", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated segmentation of kidneys and kidney tumors is an important step in\nquantifying the tumor's morphometrical details to monitor the progression of\nthe disease and accurately compare decisions regarding the kidney tumor\ntreatment. Manual delineation techniques are often tedious, error-prone and\nrequire expert knowledge for creating unambiguous representation of kidneys and\nkidney tumors segmentation. In this work, we propose an end-to-end boundary\naware fully Convolutional Neural Networks (CNNs) for reliable kidney and kidney\ntumor semantic segmentation from arterial phase abdominal 3D CT scans. We\npropose a segmentation network consisting of an encoder-decoder architecture\nthat specifically accounts for organ and tumor edge information by devising a\ndedicated boundary branch supervised by edge-aware loss terms. We have\nevaluated our model on 2019 MICCAI KiTS Kidney Tumor Segmentation Challenge\ndataset and our method has achieved dice scores of 0.9742 and 0.8103 for kidney\nand tumor repetitively and an overall composite dice score of 0.8923.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 21:49:09 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Myronenko", "Andriy", ""], ["Hatamizadeh", "Ali", ""]]}, {"id": "1909.06685", "submitter": "Erik Gaasedelen", "authors": "Erik Gaasedelen, Alex Deakyne, Paul Iaizzo", "title": "Automated Multiclass Cardiac Volume Segmentation and Model Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many strides have been made in semantic segmentation of multiple classes\nwithin an image. This has been largely due to advancements in deep learning and\nconvolutional neural networks (CNNs). Features within a CNN are automatically\nlearned during training, which allows for the abstraction of semantic\ninformation within the images. These deep learning models are powerful enough\nto handle the segmentation of multiple classes without the need for multiple\nnetworks. Despite these advancements, few attempts have been made to\nautomatically segment multiple anatomical features within medical imaging\ndatasets obtained from CT or MRI scans. This offers a unique challenge because\nof the three dimensional nature of medical imaging data. In order to alleviate\nthe 3D modality problem, we propose a multi-axis ensemble method, applied to a\ndataset of 4-cardiac-chamber segmented CT scans. Inspired by the typical\nthree-axis view used by humans, this technique aims to maximize the 3D spatial\ninformation afforded to the model, while remaining efficient for consumer grade\ninference hardware. Multi-axis ensembling along with pragmatic voxel\npreprocessing have shown in our experiments to greatly increase the mean\nintersection over union of our predictions over the complete DICOM dataset.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 21:58:08 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Gaasedelen", "Erik", ""], ["Deakyne", "Alex", ""], ["Iaizzo", "Paul", ""]]}, {"id": "1909.06700", "submitter": "Lin Jiarong", "authors": "Jiarong Lin and Fu Zhang", "title": "Loam_livox: A fast, robust, high-precision LiDAR odometry and mapping\n  package for LiDARs of small FoV", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR odometry and mapping (LOAM) has been playing an important role in\nautonomous vehicles, due to its ability to simultaneously localize the robot's\npose and build high-precision, high-resolution maps of the surrounding\nenvironment. This enables autonomous navigation and safe path planning of\nautonomous vehicles. In this paper, we present a robust, real-time LOAM\nalgorithm for LiDARs with small FoV and irregular samplings. By taking effort\non both front-end and back-end, we address several fundamental challenges\narising from such LiDARs, and achieve better performance in both precision and\nefficiency compared to existing baselines. To share our findings and to make\ncontributions to the community, we open source our codes on Github\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 00:36:28 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Lin", "Jiarong", ""], ["Zhang", "Fu", ""]]}, {"id": "1909.06718", "submitter": "Rheeya Uppaal", "authors": "Rheeya Uppaal", "title": "LRS-DAG: Low Resource Supervised Domain Adaptation with Generalization\n  Across Domains", "comments": "10 pages, 3 figures. Accepted to NewInML Workshop at NeurIPS, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current state of the art methods in Domain Adaptation follow adversarial\napproaches, making training a challenge. Existing non-adversarial methods learn\nmappings between the source and target domains, to achieve reasonable\nperformance. However, even these methods do not focus on a key aspect:\nmaintaining performance on the source domain, even after optimizing over the\ntarget domain. Additionally, there exist very few methods in low resource\nsupervised domain adaptation. This work proposes a method, LRS-DAG, that aims\nto solve these current issues in the field. By adding a set of \"encoder layers\"\nwhich map the target domain to the source, and can be removed when dealing\ndirectly with the source data, the model learns to perform optimally on both\ndomains. LRS-DAG showcases its uniqueness by being a new algorithm for low\nresource domain adaptation which maintains performance over the source domain,\nwith a new metric for learning mappings between domains being introduced. We\nshow that, in the case of FCNs, when transferring from MNIST to SVHN, LRS-DAG\nperforms comparably to fine tuning, with the advantage of maintaining\nperformance over the source domain. LRS-DAG outperforms fine tuning when\ntransferring to a synthetic dataset similar to MNIST, which is a setting more\nrepresentative of low resource supervised domain adaptation.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 02:38:49 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 03:57:30 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Uppaal", "Rheeya", ""]]}, {"id": "1909.06720", "submitter": "Thang Vu", "authors": "Thang Vu, Hyunjun Jang, Trung X. Pham, Chang D. Yoo", "title": "Cascade RPN: Delving into High-Quality Region Proposal Network with\n  Adaptive Convolution", "comments": "To appear in NeurIPS 2019 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers an architecture referred to as Cascade Region Proposal\nNetwork (Cascade RPN) for improving the region-proposal quality and detection\nperformance by \\textit{systematically} addressing the limitation of the\nconventional RPN that \\textit{heuristically defines} the anchors and\n\\textit{aligns} the features to the anchors. First, instead of using multiple\nanchors with predefined scales and aspect ratios, Cascade RPN relies on a\n\\textit{single anchor} per location and performs multi-stage refinement. Each\nstage is progressively more stringent in defining positive samples by starting\nout with an anchor-free metric followed by anchor-based metrics in the ensuing\nstages. Second, to attain alignment between the features and the anchors\nthroughout the stages, \\textit{adaptive convolution} is proposed that takes the\nanchors in addition to the image features as its input and learns the sampled\nfeatures guided by the anchors. A simple implementation of a two-stage Cascade\nRPN achieves AR 13.4 points higher than that of the conventional RPN,\nsurpassing any existing region proposal methods. When adopting to Fast R-CNN\nand Faster R-CNN, Cascade RPN can improve the detection mAP by 3.1 and 3.5\npoints, respectively. The code is made publicly available at\n\\url{https://github.com/thangvubk/Cascade-RPN.git}.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 02:57:36 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 08:16:10 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Vu", "Thang", ""], ["Jang", "Hyunjun", ""], ["Pham", "Trung X.", ""], ["Yoo", "Chang D.", ""]]}, {"id": "1909.06726", "submitter": "Tianchen Wang", "authors": "Tianchen Wang, Jinjun Xiong, Xiaowei Xu, Meng Jiang, Yiyu Shi, Haiyun\n  Yuan, Meiping Huang, Jian Zhuang", "title": "MSU-Net: Multiscale Statistical U-Net for Real-time 3D Cardiac MRI Video\n  Segmentation", "comments": "MICCAI19", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac magnetic resonance imaging (MRI) is an essential tool for MRI-guided\nsurgery and real-time intervention. The MRI videos are expected to be segmented\non-the-fly in real practice. However, existing segmentation methods would\nsuffer from drastic accuracy loss when modified for speedup. In this work, we\npropose Multiscale Statistical U-Net (MSU-Net) for real-time 3D MRI video\nsegmentation in cardiac surgical guidance. Our idea is to model the input\nsamples as multiscale canonical form distributions for speedup, while the\nspatio-temporal correlation is still fully utilized. A parallel statistical\nU-Net is then designed to efficiently process these distributions. The fast\ndata sampling and efficient parallel structure of MSU-Net endorse the fast and\naccurate inference. Compared with vanilla U-Net and a modified state-of-the-art\nmethod GridNet, our method achieves up to 268% and 237% speedup with 1.6% and\n3.6% increased Dice scores.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 03:56:15 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Wang", "Tianchen", ""], ["Xiong", "Jinjun", ""], ["Xu", "Xiaowei", ""], ["Jiang", "Meng", ""], ["Shi", "Yiyu", ""], ["Yuan", "Haiyun", ""], ["Huang", "Meiping", ""], ["Zhuang", "Jian", ""]]}, {"id": "1909.06728", "submitter": "Jiayuan Wang", "authors": "Tamal K. Dey, Jiayuan Wang, Yusu Wang", "title": "Road Network Reconstruction from Satellite Images with Machine Learning\n  Supported by Topological Methods", "comments": "26 pages, 13 figures, ACM SIGSPATIAL 2019", "journal-ref": null, "doi": "10.1145/3347146.3359348", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic Extraction of road network from satellite images is a goal that can\nbenefit and even enable new technologies. Methods that combine machine learning\n(ML) and computer vision have been proposed in recent years which make the task\nsemi-automatic by requiring the user to provide curated training samples. The\nprocess can be fully automatized if training samples can be produced\nalgorithmically. Of course, this requires a robust algorithm that can\nreconstruct the road networks from satellite images reliably so that the output\ncan be fed as training samples. In this work, we develop such a technique by\ninfusing a persistence-guided discrete Morse based graph reconstruction\nalgorithm into ML framework.\n  We elucidate our contributions in two phases. First, in a semi-automatic\nframework, we combine a discrete-Morse based graph reconstruction algorithm\nwith an existing CNN framework to segment input satellite images. We show that\nthis leads to reconstructions with better connectivity and less noise. Next, in\na fully automatic framework, we leverage the power of the discrete-Morse based\ngraph reconstruction algorithm to train a CNN from a collection of images\nwithout labelled data and use the same algorithm to produce the final output\nfrom the segmented images created by the trained CNN. We apply the\ndiscrete-Morse based graph reconstruction algorithm iteratively to improve the\naccuracy of the CNN. We show promising experimental results of this new\nframework on datasets from SpaceNet Challenge.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 04:16:05 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Dey", "Tamal K.", ""], ["Wang", "Jiayuan", ""], ["Wang", "Yusu", ""]]}, {"id": "1909.06751", "submitter": "Diego Gragnaniello", "authors": "Francesco Marra, Diego Gragnaniello, Luisa Verdoliva and Giovanni\n  Poggi", "title": "A Full-Image Full-Resolution End-to-End-Trainable CNN Framework for\n  Image Forgery Detection", "comments": "13 pages, 12 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to limited computational and memory resources, current deep learning\nmodels accept only rather small images in input, calling for preliminary image\nresizing. This is not a problem for high-level vision problems, where\ndiscriminative features are barely affected by resizing. On the contrary, in\nimage forensics, resizing tends to destroy precious high-frequency details,\nimpacting heavily on performance. One can avoid resizing by means of patch-wise\nprocessing, at the cost of renouncing whole-image analysis. In this work, we\npropose a CNN-based image forgery detection framework which makes decisions\nbased on full-resolution information gathered from the whole image. Thanks to\ngradient checkpointing, the framework is trainable end-to-end with limited\nmemory resources and weak (image-level) supervision, allowing for the joint\noptimization of all parameters. Experiments on widespread image forensics\ndatasets prove the good performance of the proposed approach, which largely\noutperforms all baselines and all reference methods.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 07:08:37 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Marra", "Francesco", ""], ["Gragnaniello", "Diego", ""], ["Verdoliva", "Luisa", ""], ["Poggi", "Giovanni", ""]]}, {"id": "1909.06761", "submitter": "Georgios Kapidis", "authors": "Georgios Kapidis, Ronald Poppe, Elsbeth van Dam, Lucas Noldus, Remco\n  Veltkamp", "title": "Multitask Learning to Improve Egocentric Action Recognition", "comments": "10 pages, 3 figures, accepted at the 5th Egocentric Perception,\n  Interaction and Computing (EPIC) workshop at ICCV 2019, code repository:\n  https://github.com/georkap/hand_track_classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we employ multitask learning to capitalize on the structure that\nexists in related supervised tasks to train complex neural networks. It allows\ntraining a network for multiple objectives in parallel, in order to improve\nperformance on at least one of them by capitalizing on a shared representation\nthat is developed to accommodate more information than it otherwise would for a\nsingle task. We employ this idea to tackle action recognition in egocentric\nvideos by introducing additional supervised tasks. We consider learning the\nverbs and nouns from which action labels consist of and predict coordinates\nthat capture the hand locations and the gaze-based visual saliency for all the\nframes of the input video segments. This forces the network to explicitly focus\non cues from secondary tasks that it might otherwise have missed resulting in\nimproved inference. Our experiments on EPIC-Kitchens and EGTEA Gaze+ show\nconsistent improvements when training with multiple tasks over the single-task\nbaseline. Furthermore, in EGTEA Gaze+ we outperform the state-of-the-art in\naction recognition by 3.84%. Apart from actions, our method produces accurate\nhand and gaze estimations as side tasks, without requiring any additional input\nat test time other than the RGB video clips.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 08:29:46 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Kapidis", "Georgios", ""], ["Poppe", "Ronald", ""], ["van Dam", "Elsbeth", ""], ["Noldus", "Lucas", ""], ["Veltkamp", "Remco", ""]]}, {"id": "1909.06763", "submitter": "Hongxiang Lin PhD", "authors": "Hongxiang Lin, Matteo Figini, Ryutaro Tanno, Stefano B. Blumberg,\n  Enrico Kaden, Godwin Ogbole, Biobele J. Brown, Felice D'Arco, David W.\n  Carmichael, Ikeoluwa Lagunju, Helen J. Cross, Delmiro Fernandez-Reyes, Daniel\n  C. Alexander", "title": "Deep Learning for Low-Field to High-Field MR: Image Quality Transfer\n  with Probabilistic Decimation Simulator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  MR images scanned at low magnetic field ($<1$T) have lower resolution in the\nslice direction and lower contrast, due to a relatively small signal-to-noise\nratio (SNR) than those from high field (typically 1.5T and 3T). We adapt the\nrecent idea of Image Quality Transfer (IQT) to enhance very low-field\nstructural images aiming to estimate the resolution, spatial coverage, and\ncontrast of high-field images. Analogous to many learning-based image\nenhancement techniques, IQT generates training data from high-field scans alone\nby simulating low-field images through a pre-defined decimation model. However,\nthe ground truth decimation model is not well-known in practice, and lack of\nits specification can bias the trained model, aggravating performance on the\nreal low-field scans. In this paper we propose a probabilistic decimation\nsimulator to improve robustness of model training. It is used to generate and\naugment various low-field images whose parameters are random variables and\nsampled from an empirical distribution related to tissue-specific SNR on a\n0.36T scanner. The probabilistic decimation simulator is model-agnostic, that\nis, it can be used with any super-resolution networks. Furthermore we propose a\nvariant of U-Net architecture to improve its learning performance. We show\npromising qualitative results from clinical low-field images confirming the\nstrong efficacy of IQT in an important new application area: epilepsy diagnosis\nin sub-Saharan Africa where only low-field scanners are normally available.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 08:56:44 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Lin", "Hongxiang", ""], ["Figini", "Matteo", ""], ["Tanno", "Ryutaro", ""], ["Blumberg", "Stefano B.", ""], ["Kaden", "Enrico", ""], ["Ogbole", "Godwin", ""], ["Brown", "Biobele J.", ""], ["D'Arco", "Felice", ""], ["Carmichael", "David W.", ""], ["Lagunju", "Ikeoluwa", ""], ["Cross", "Helen J.", ""], ["Fernandez-Reyes", "Delmiro", ""], ["Alexander", "Daniel C.", ""]]}, {"id": "1909.06793", "submitter": "Peng Sun", "authors": "Peiwen Lin, Peng Sun, Guangliang Cheng, Sirui Xie, Xi Li, Jianping Shi", "title": "Graph-guided Architecture Search for Real-time Semantic Segmentation", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a lightweight semantic segmentation network often requires\nresearchers to find a trade-off between performance and speed, which is always\nempirical due to the limited interpretability of neural networks. In order to\nrelease researchers from these tedious mechanical trials, we propose a\nGraph-guided Architecture Search (GAS) pipeline to automatically search\nreal-time semantic segmentation networks. Unlike previous works that use a\nsimplified search space and stack a repeatable cell to form a network, we\nintroduce a novel search mechanism with new search space where a lightweight\nmodel can be effectively explored through the cell-level diversity and\nlatencyoriented constraint. Specifically, to produce the cell-level diversity,\nthe cell-sharing constraint is eliminated through the cell-independent manner.\nThen a graph convolution network (GCN) is seamlessly integrated as a\ncommunication mechanism between cells. Finally, a latency-oriented constraint\nis endowed into the search process to balance the speed and performance.\nExtensive experiments on Cityscapes and CamVid datasets demonstrate that GAS\nachieves the new state-of-the-art trade-off between accuracy and speed. In\nparticular, on Cityscapes dataset, GAS achieves the new best performance of\n73.5% mIoU with speed of 108.4 FPS on Titan Xp.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 12:45:55 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 04:38:55 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Lin", "Peiwen", ""], ["Sun", "Peng", ""], ["Cheng", "Guangliang", ""], ["Xie", "Sirui", ""], ["Li", "Xi", ""], ["Shi", "Jianping", ""]]}, {"id": "1909.06795", "submitter": "Ruiqi Cheng", "authors": "Ruiqi Cheng, Kaiwei Wang, Jian Bai, Zhijie Xu", "title": "OpenMPR: Recognize Places Using Multimodal Data for People with Visual\n  Impairments", "comments": "The paper is accepted by the special issue of Measurement Science and\n  Engineering", "journal-ref": null, "doi": "10.1088/1361-6501/ab2106", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Place recognition plays a crucial role in navigational assistance, and is\nalso a challenging issue of assistive technology. The place recognition is\nprone to erroneous localization owing to various changes between database and\nquery images. Aiming at the wearable assistive device for visually impaired\npeople, we propose an open-sourced place recognition algorithm OpenMPR, which\nutilizes the multimodal data to address the challenging issues of place\nrecognition. Compared with conventional place recognition, the proposed OpenMPR\nnot only leverages multiple effective descriptors, but also assigns different\nweights to those descriptors in image matching. Incorporating GNSS data into\nthe algorithm, the cone-based sequence searching is used for robust place\nrecognition. The experiments illustrate that the proposed algorithm manages to\nsolve the place recognition issue in the real-world scenarios and surpass the\nstate-of-the-art algorithms in terms of assistive navigation performance. On\nthe real-world testing dataset, the online OpenMPR achieves 88.7% precision at\n100% recall without illumination changes, and achieves 57.8% precision at 99.3%\nrecall with illumination changes. The OpenMPR is available at\nhttps://github.com/chengricky/OpenMultiPR.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 12:58:53 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Cheng", "Ruiqi", ""], ["Wang", "Kaiwei", ""], ["Bai", "Jian", ""], ["Xu", "Zhijie", ""]]}, {"id": "1909.06800", "submitter": "Peixia Li", "authors": "Peixia Li, Boyu Chen, Wanli Ouyang, Dong Wang, Xiaoyun Yang, Huchuan\n  Lu", "title": "GradNet: Gradient-Guided Network for Visual Object Tracking", "comments": "accepted by ICCV2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fully-convolutional siamese network based on template matching has shown\ngreat potentials in visual tracking. During testing, the template is fixed with\nthe initial target feature and the performance totally relies on the general\nmatching ability of the siamese network. However, this manner cannot capture\nthe temporal variations of targets or background clutter. In this work, we\npropose a novel gradient-guided network to exploit the discriminative\ninformation in gradients and update the template in the siamese network through\nfeed-forward and backward operations. Our algorithm performs feed-forward and\nbackward operations to exploit the discriminative informaiton in gradients and\ncapture the core attention of the target. To be specific, the algorithm can\nutilize the information from the gradient to update the template in the current\nframe. In addition, a template generalization training method is proposed to\nbetter use gradient information and avoid overfitting. To our knowledge, this\nwork is the first attempt to exploit the information in the gradient for\ntemplate update in siamese-based trackers. Extensive experiments on recent\nbenchmarks demonstrate that our method achieves better performance than other\nstate-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 13:39:49 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Li", "Peixia", ""], ["Chen", "Boyu", ""], ["Ouyang", "Wanli", ""], ["Wang", "Dong", ""], ["Yang", "Xiaoyun", ""], ["Lu", "Huchuan", ""]]}, {"id": "1909.06804", "submitter": "Jason Kuen", "authors": "Jason Kuen, Federico Perazzi, Zhe Lin, Jianming Zhang, Yap-Peng Tan", "title": "Scaling Object Detection by Transferring Classification Weights", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale object detection datasets are constantly increasing their size in\nterms of the number of classes and annotations count. Yet, the number of\nobject-level categories annotated in detection datasets is an order of\nmagnitude smaller than image-level classification labels. State-of-the art\nobject detection models are trained in a supervised fashion and this limits the\nnumber of object classes they can detect. In this paper, we propose a novel\nweight transfer network (WTN) to effectively and efficiently transfer knowledge\nfrom classification network's weights to detection network's weights to allow\ndetection of novel classes without box supervision. We first introduce input\nand feature normalization schemes to curb the under-fitting during training of\na vanilla WTN. We then propose autoencoder-WTN (AE-WTN) which uses\nreconstruction loss to preserve classification network's information over all\nclasses in the target latent space to ensure generalization to novel classes.\nCompared to vanilla WTN, AE-WTN obtains absolute performance gains of 6% on two\nOpen Images evaluation sets with 500 seen and 57 novel classes respectively,\nand 25% on a Visual Genome evaluation set with 200 novel classes. The code is\navailable at https://github.com/xternalz/AE-WTN.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 13:59:29 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Kuen", "Jason", ""], ["Perazzi", "Federico", ""], ["Lin", "Zhe", ""], ["Zhang", "Jianming", ""], ["Tan", "Yap-Peng", ""]]}, {"id": "1909.06826", "submitter": "Cheng Chi", "authors": "Cheng Chi, Shifeng Zhang, Junliang Xing, Zhen Lei, Stan Z. Li, Xudong\n  Zou", "title": "PedHunter: Occlusion Robust Pedestrian Detector in Crowded Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection in crowded scenes is a challenging problem, because\nocclusion happens frequently among different pedestrians. In this paper, we\npropose an effective and efficient detection network to hunt pedestrians in\ncrowd scenes. The proposed method, namely PedHunter, introduces strong\nocclusion handling ability to existing region-based detection networks without\nbringing extra computations in the inference stage. Specifically, we design a\nmask-guided module to leverage the head information to enhance the feature\nrepresentation learning of the backbone network. Moreover, we develop a strict\nclassification criterion by improving the quality of positive samples during\ntraining to eliminate common false positives of pedestrian detection in crowded\nscenes. Besides, we present an occlusion-simulated data augmentation to enrich\nthe pattern and quantity of occlusion samples to improve the occlusion\nrobustness. As a consequent, we achieve state-of-the-art results on three\npedestrian detection datasets including CityPersons, Caltech-USA and\nCrowdHuman. To facilitate further studies on the occluded pedestrian detection\nin surveillance scenes, we release a new pedestrian dataset, called SUR-PED,\nwith a total of over 162k high-quality manually labeled instances in 10k\nimages. The proposed dataset, source codes and trained models will be released.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 16:02:25 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Chi", "Cheng", ""], ["Zhang", "Shifeng", ""], ["Xing", "Junliang", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""], ["Zou", "Xudong", ""]]}, {"id": "1909.06840", "submitter": "Konstantin Ushenin", "authors": "Alexander Karimov, Artem Razumov, Ruslana Manbatchurina, Ksenia\n  Simonova, Irina Donets, Anastasia Vlasova, Yulia Khramtsova, Konstantin\n  Ushenin", "title": "Comparison of UNet, ENet, and BoxENet for Segmentation of Mast Cells in\n  Scans of Histological Slices", "comments": "4 pages, 5 figures, 1 table", "journal-ref": null, "doi": "10.1109/SIBIRCON48586.2019.8958121", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks show high accuracy in theproblem of semantic and\ninstance segmentation of biomedicaldata. However, this approach is\ncomputationally expensive. Thecomputational cost may be reduced with network\nsimplificationafter training or choosing the proper architecture, which\nprovidessegmentation with less accuracy but does it much faster. In thepresent\nstudy, we analyzed the accuracy and performance ofUNet and ENet architectures\nfor the problem of semantic imagesegmentation. In addition, we investigated the\nENet architecture by replacing of some convolution layers with\nbox-convolutionlayers. The analysis performed on the original dataset consisted\nof histology slices with mast cells. These cells provide a region\nforsegmentation with different types of borders, which vary fromclearly visible\nto ragged. ENet was less accurate than UNet byonly about 1-2%, but ENet\nperformance was 8-15 times faster than UNet one.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 17:26:56 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 00:31:19 GMT"}, {"version": "v3", "created": "Fri, 22 Nov 2019 16:14:21 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Karimov", "Alexander", ""], ["Razumov", "Artem", ""], ["Manbatchurina", "Ruslana", ""], ["Simonova", "Ksenia", ""], ["Donets", "Irina", ""], ["Vlasova", "Anastasia", ""], ["Khramtsova", "Yulia", ""], ["Ushenin", "Konstantin", ""]]}, {"id": "1909.06860", "submitter": "Guido F. Montufar", "authors": "Alex Tong Lin, Yonatan Dukler, Wuchen Li, Guido Montufar", "title": "Wasserstein Diffusion Tikhonov Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose regularization strategies for learning discriminative models that\nare robust to in-class variations of the input data. We use the Wasserstein-2\ngeometry to capture semantically meaningful neighborhoods in the space of\nimages, and define a corresponding input-dependent additive noise data\naugmentation model. Expanding and integrating the augmented loss yields an\neffective Tikhonov-type Wasserstein diffusion smoothness regularizer. This\napproach allows us to apply high levels of regularization and train functions\nthat have low variability within classes but remain flexible across classes. We\nprovide efficient methods for computing the regularizer at a negligible cost in\ncomparison to training with adversarial data augmentation. Initial experiments\ndemonstrate improvements in generalization performance under adversarial\nperturbations and also large in-class variations of the input data.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 19:10:16 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Lin", "Alex Tong", ""], ["Dukler", "Yonatan", ""], ["Li", "Wuchen", ""], ["Montufar", "Guido", ""]]}, {"id": "1909.06867", "submitter": "Isaac Weiss", "authors": "Isaac Weiss", "title": "A Dual-hierarchy Semantic Graph for Robust Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for object recognition based on a semantic graph\nrepresentation, which the system can learn from image examples. This graph is\nbased on intrinsic properties of objects such as structure and geometry, so it\nis more robust than the current machine learning methods that can be fooled by\nchanging a few pixels. Current methods have proved to be powerful but brittle\nbecause they ignore the structure and semantics of the objects. We define\nsemantics as a form of abstraction, in terms of the intrinsic properties of the\nobject, not in terms of human perception. Thus, it can be learned\nautomatically. This is facilitated by the graph having two distinct\nhierarchies: abstraction and parts, which also makes its representation of\nobjects more accurate and versatile. Previous semantic networks had only one\namorphous hierarchy and were difficult to build and traverse. Our system\nperforms both the learning and recognition by an algorithm that traverses both\nhierarchies at the same time, combining the advantages of top-down and\nbottom-up strategies. This reduces dimensionality and obviates the need for the\nbrute force of big data training.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 19:49:58 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 10:42:58 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2020 15:49:58 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Weiss", "Isaac", ""]]}, {"id": "1909.06884", "submitter": "Riccardo Spezialetti", "authors": "Riccardo Spezialetti and Samuele Salti and Luigi Di Stefano", "title": "Performance Evaluation of Learned 3D Features", "comments": null, "journal-ref": "International Conference on Image Analysis and Processing.\n  Springer, Cham, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching surfaces is a challenging 3D Computer Vision problem typically\naddressed by local features. Although a variety of 3D feature detectors and\ndescriptors has been proposed in literature, they have seldom been proposed\ntogether and it is yet not clear how to identify the most effective\ndetector-descriptor pair for a specific application. A promising solution is to\nleverage machine learning to learn the optimal 3D detector for any given 3D\ndescriptor [15]. In this paper, we report a performance evaluation of the\ndetector-descriptor pairs obtained by learning a paired 3D detector for the\nmost popular 3D descriptors. In particular, we address experimental settings\ndealing with object recognition and surface registration.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 21:05:49 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Spezialetti", "Riccardo", ""], ["Salti", "Samuele", ""], ["Di Stefano", "Luigi", ""]]}, {"id": "1909.06887", "submitter": "Riccardo Spezialetti", "authors": "Riccardo Spezialetti and Samuele Salti and Luigi Di Stefano", "title": "Learning an Effective Equivariant 3D Descriptor Without Supervision", "comments": "Accepted to International Conference on Computer Vision 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing correspondences between 3D shapes is a fundamental task in 3D\nComputer Vision, typically addressed by matching local descriptors. Recently, a\nfew attempts at applying the deep learning paradigm to the task have shown\npromising results. Yet, the only explored way to learn rotation invariant\ndescriptors has been to feed neural networks with highly engineered and\ninvariant representations provided by existing hand-crafted descriptors, a path\nthat goes in the opposite direction of end-to-end learning from raw data so\nsuccessfully deployed for 2D images. In this paper, we explore the benefits of\ntaking a step back in the direction of end-to-end learning of 3D descriptors by\ndisentangling the creation of a robust and distinctive rotation equivariant\nrepresentation, which can be learned from unoriented input data, and the\ndefinition of a good canonical orientation, required only at test time to\nobtain an invariant descriptor. To this end, we leverage two recent\ninnovations: spherical convolutional neural networks to learn an equivariant\ndescriptor and plane folding decoders to learn without supervision. The\neffectiveness of the proposed approach is experimentally validated by\noutperforming hand-crafted and learned descriptors on a standard benchmark.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 21:27:23 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Spezialetti", "Riccardo", ""], ["Salti", "Samuele", ""], ["Di Stefano", "Luigi", ""]]}, {"id": "1909.06892", "submitter": "Shubham Jain", "authors": "Shubham Jain, Sumeet Kumar Gupta, Anand Raghunathan", "title": "TiM-DNN: Ternary in-Memory accelerator for Deep Neural Networks", "comments": "12 pages, 18 figures, Accepted in IEEE Transactions on Very Large\n  Scale Integration (VLSI) Systems 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of lower precision has emerged as a popular technique to optimize the\ncompute and storage requirements of complex Deep Neural Networks (DNNs). In the\nquest for lower precision, recent studies have shown that ternary DNNs (which\nrepresent weights and activations by signed ternary values) represent a\npromising sweet spot, achieving accuracy close to full-precision networks on\ncomplex tasks. We propose TiM-DNN, a programmable in-memory accelerator that is\nspecifically designed to execute ternary DNNs. TiM-DNN supports various ternary\nrepresentations including unweighted {-1,0,1}, symmetric weighted {-a,0,a}, and\nasymmetric weighted {-a,0,b} ternary systems. The building blocks of TiM-DNN\nare TiM tiles -- specialized memory arrays that perform massively parallel\nsigned ternary vector-matrix multiplications with a single access. TiM tiles\nare in turn composed of Ternary Processing Cells (TPCs), bit-cells that\nfunction as both ternary storage units and signed ternary multiplication units.\nWe evaluate an implementation of TiM-DNN in 32nm technology using an\narchitectural simulator calibrated with SPICE simulations and RTL synthesis. We\nevaluate TiM-DNN across a suite of state-of-the-art DNN benchmarks including\nboth deep convolutional and recurrent neural networks. A 32-tile instance of\nTiM-DNN achieves a peak performance of 114 TOPs/s, consumes 0.9W power, and\noccupies 1.96mm2 chip area, representing a 300X and 388X improvement in TOPS/W\nand TOPS/mm2, respectively, compared to an NVIDIA Tesla V100 GPU. In comparison\nto specialized DNN accelerators, TiM-DNN achieves 55X-240X and 160X-291X\nimprovement in TOPS/W and TOPS/mm2, respectively. Finally, when compared to a\nwell-optimized near-memory accelerator for ternary DNNs, TiM-DNN demonstrates\n3.9x-4.7x improvement in system-level energy and 3.2x-4.2x speedup,\nunderscoring the potential of in-memory computing for ternary DNNs.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 21:43:19 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 03:59:26 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 02:42:18 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Jain", "Shubham", ""], ["Gupta", "Sumeet Kumar", ""], ["Raghunathan", "Anand", ""]]}, {"id": "1909.06894", "submitter": "Denys Rozumnyi", "authors": "Denys Rozumnyi, Jan Kotera, Filip \\v{S}roubek, Ji\\v{r}\\'i Matas", "title": "Non-Causal Tracking by Deblatting", "comments": "Published at GCPR 2019, oral presentation, Best Paper Honorable\n  Mention Award", "journal-ref": null, "doi": "10.1007/978-3-030-33676-9_9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking by Deblatting stands for solving an inverse problem of deblurring\nand image matting for tracking motion-blurred objects. We propose non-causal\nTracking by Deblatting which estimates continuous, complete and accurate object\ntrajectories. Energy minimization by dynamic programming is used to detect\nabrupt changes of motion, called bounces. High-order polynomials are fitted to\nsegments, which are parts of the trajectory separated by bounces. The output is\na continuous trajectory function which assigns location for every real-valued\ntime stamp from zero to the number of frames. Additionally, we show that from\nthe trajectory function precise physical calculations are possible, such as\nradius, gravity or sub-frame object velocity. Velocity estimation is compared\nto the high-speed camera measurements and radars. Results show high performance\nof the proposed method in terms of Trajectory-IoU, recall and velocity\nestimation.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 22:02:43 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Rozumnyi", "Denys", ""], ["Kotera", "Jan", ""], ["\u0160roubek", "Filip", ""], ["Matas", "Ji\u0159\u00ed", ""]]}, {"id": "1909.06897", "submitter": "Adam Ligocki", "authors": "Adam Ligocki, Ales Jelinek and Ludek Zalud", "title": "Brno Urban Dataset -- The New Data for Self-Driving Agents and Mapping\n  Tasks", "comments": "7 pages, ICRA 2020 submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving is a dynamically growing field of research, where quality\nand amount of experimental data is critical. Although several rich datasets are\navailable these days, the demands of researchers and technical possibilities\nare evolving. Through this paper, we bring a new dataset recorded in Brno,\nCzech Republic. It offers data from four WUXGA cameras, two 3D LiDARs, inertial\nmeasurement unit, infrared camera and especially differential RTK GNSS receiver\nwith centimetre accuracy which, to the best knowledge of the authors, is not\navailable from any other public dataset so far. In addition, all the data are\nprecisely timestamped with sub-millisecond precision to allow wider range of\napplications. At the time of publishing of this paper, recordings of more than\n350 km of rides in varying environment are shared at: https:\n//github.com/RoboticsBUT/Brno-Urban-Dataset.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 22:06:34 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Ligocki", "Adam", ""], ["Jelinek", "Ales", ""], ["Zalud", "Ludek", ""]]}, {"id": "1909.06904", "submitter": "Steve DiPaola", "authors": "Vanessa Utz and Steve DiPaola", "title": "Using an AI creativity system to explore how aesthetic experiences are\n  processed along the brains perceptual neural pathways", "comments": null, "journal-ref": "Cognitive Systems Research, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased sophistication of AI techniques, the application of these\nsystems has been expanding to ever newer fields. Increasingly, these systems\nare being used in modeling of human aesthetics and creativity, e.g. how humans\ncreate artworks and design products. Our lab has developed one such AI\ncreativity deep learning system that can be used to create artworks in the form\nof images and videos. In this paper, we describe this system and its use in\nstudying the human visual system and the formation of aesthetic experiences.\nSpecifically, we show how time-based AI created media can be used to explore\nthe nature of the dual-pathway neuro-architecture of the human visual system\nand how this relates to higher cognitive judgments such as aesthetic\nexperiences that rely on these divergent information streams. We propose a\ntheoretical framework for how the movement within percepts such as video clips,\ncauses the engagement of reflexive attention and a subsequent focus on visual\ninformation that are primarily processed via the dorsal stream, thereby\nmodulating aesthetic experiences that rely on information relayed via the\nventral stream. We outline our recent study in support of our proposed\nframework, which serves as the first study that investigates the relationship\nbetween the two visual streams and aesthetic experiences.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 22:49:41 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Utz", "Vanessa", ""], ["DiPaola", "Steve", ""]]}, {"id": "1909.06907", "submitter": "Arjun Akula", "authors": "Arjun R. Akula, Changsong Liu, Sari Saba-Sadiya, Hongjing Lu, Sinisa\n  Todorovic, Joyce Y. Chai, Song-Chun Zhu", "title": "X-ToM: Explaining with Theory-of-Mind for Gaining Justified Human Trust", "comments": "A short version of this was presented at CVPR 2019 Workshop on\n  Explainable AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new explainable AI (XAI) framework aimed at increasing justified\nhuman trust and reliance in the AI machine through explanations. We pose\nexplanation as an iterative communication process, i.e. dialog, between the\nmachine and human user. More concretely, the machine generates sequence of\nexplanations in a dialog which takes into account three important aspects at\neach dialog turn: (a) human's intention (or curiosity); (b) human's\nunderstanding of the machine; and (c) machine's understanding of the human\nuser. To do this, we use Theory of Mind (ToM) which helps us in explicitly\nmodeling human's intention, machine's mind as inferred by the human as well as\nhuman's mind as inferred by the machine. In other words, these explicit mental\nrepresentations in ToM are incorporated to learn an optimal explanation policy\nthat takes into account human's perception and beliefs. Furthermore, we also\nshow that ToM facilitates in quantitatively measuring justified human trust in\nthe machine by comparing all the three mental representations.\n  We applied our framework to three visual recognition tasks, namely, image\nclassification, action recognition, and human body pose estimation. We argue\nthat our ToM based explanations are practical and more natural for both expert\nand non-expert users to understand the internal workings of complex machine\nlearning models. To the best of our knowledge, this is the first work to derive\nexplanations using ToM. Extensive human study experiments verify our\nhypotheses, showing that the proposed explanations significantly outperform the\nstate-of-the-art XAI methods in terms of all the standard quantitative and\nqualitative XAI evaluation metrics including human trust, reliance, and\nexplanation satisfaction.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 23:24:32 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Akula", "Arjun R.", ""], ["Liu", "Changsong", ""], ["Saba-Sadiya", "Sari", ""], ["Lu", "Hongjing", ""], ["Todorovic", "Sinisa", ""], ["Chai", "Joyce Y.", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1909.06928", "submitter": "Tawfiq Salem", "authors": "Tawfiq Salem, Connor Greenwell, Hunter Blanton, Nathan Jacobs", "title": "Learning to Map Nearly Anything", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Looking at the world from above, it is possible to estimate many properties\nof a given location, including the type of land cover and the expected land\nuse. Historically, such tasks have relied on relatively coarse-grained\ncategories due to the difficulty of obtaining fine-grained annotations. In this\nwork, we propose an easily extensible approach that makes it possible to\nestimate fine-grained properties from overhead imagery. In particular, we\npropose a cross-modal distillation strategy to learn to predict the\ndistribution of fine-grained properties from overhead imagery, without\nrequiring any manual annotation of overhead imagery. We show that our learned\nmodels can be used directly for applications in mapping and image localization.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 01:40:44 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Salem", "Tawfiq", ""], ["Greenwell", "Connor", ""], ["Blanton", "Hunter", ""], ["Jacobs", "Nathan", ""]]}, {"id": "1909.06933", "submitter": "Peter Florence", "authors": "Peter Florence, Lucas Manuelli, Russ Tedrake", "title": "Self-Supervised Correspondence in Visuomotor Policy Learning", "comments": "Video at: https://sites.google.com/view/visuomotor-correspondence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore using self-supervised correspondence for improving\nthe generalization performance and sample efficiency of visuomotor policy\nlearning. Prior work has primarily used approaches such as autoencoding,\npose-based losses, and end-to-end policy optimization in order to train the\nvisual portion of visuomotor policies. We instead propose an approach using\nself-supervised dense visual correspondence training, and show this enables\nvisuomotor policy learning with surprisingly high generalization performance\nwith modest amounts of data: using imitation learning, we demonstrate extensive\nhardware validation on challenging manipulation tasks with as few as 50\ndemonstrations. Our learned policies can generalize across classes of objects,\nreact to deformable object configurations, and manipulate textureless\nsymmetrical objects in a variety of backgrounds, all with closed-loop,\nreal-time vision-based policies. Simulated imitation learning experiments\nsuggest that correspondence training offers sample complexity and\ngeneralization benefits compared to autoencoding and end-to-end training.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 01:52:39 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Florence", "Peter", ""], ["Manuelli", "Lucas", ""], ["Tedrake", "Russ", ""]]}, {"id": "1909.06940", "submitter": "Zhao Kang", "authors": "Zhao Kang and Guoxin Shi and Shudong Huang and Wenyu Chen and Xiaorong\n  Pu and Joey Tianyi Zhou and Zenglin Xu", "title": "Multi-graph Fusion for Multi-view Spectral Clustering", "comments": "submitted to Knowledge-based Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A panoply of multi-view clustering algorithms has been developed to deal with\nprevalent multi-view data. Among them, spectral clustering-based methods have\ndrawn much attention and demonstrated promising results recently. Despite\nprogress, there are still two fundamental questions that stay unanswered to\ndate. First, how to fuse different views into one graph. More often than not,\nthe similarities between samples may be manifested differently by different\nviews. Many existing algorithms either simply take the average of multiple\nviews or just learn a common graph. These simple approaches fail to consider\nthe flexible local manifold structures of all views. Hence, the rich\nheterogeneous information is not fully exploited. Second, how to learn the\nexplicit cluster structure. Most existing methods don't pay attention to the\nquality of the graphs and perform graph learning and spectral clustering\nseparately. Those unreliable graphs might lead to suboptimal clustering\nresults. To fill these gaps, in this paper, we propose a novel multi-view\nspectral clustering model which performs graph fusion and spectral clustering\nsimultaneously. The fusion graph approximates the original graph of each\nindividual view but maintains an explicit cluster structure. Experiments on\nfour widely used data sets confirm the superiority of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 02:22:02 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Kang", "Zhao", ""], ["Shi", "Guoxin", ""], ["Huang", "Shudong", ""], ["Chen", "Wenyu", ""], ["Pu", "Xiaorong", ""], ["Zhou", "Joey Tianyi", ""], ["Xu", "Zenglin", ""]]}, {"id": "1909.06956", "submitter": "Wentao Jiang", "authors": "Wentao Jiang, Si Liu, Chen Gao, Jie Cao, Ran He, Jiashi Feng,\n  Shuicheng Yan", "title": "PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable\n  Makeup Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the makeup transfer task, which aims to transfer\nthe makeup from a reference image to a source image. Existing methods have\nachieved promising progress in constrained scenarios, but transferring between\nimages with large pose and expression differences is still challenging.\nBesides, they cannot realize customizable transfer that allows a controllable\nshade of makeup or specifies the part to transfer, which limits their\napplications. To address these issues, we propose Pose and expression robust\nSpatial-aware GAN (PSGAN). It first utilizes Makeup Distill Network to\ndisentangle the makeup of the reference image as two spatial-aware makeup\nmatrices. Then, Attentive Makeup Morphing module is introduced to specify how\nthe makeup of a pixel in the source image is morphed from the reference image.\nWith the makeup matrices and the source image, Makeup Apply Network is used to\nperform makeup transfer. Our PSGAN not only achieves state-of-the-art results\neven when large pose and expression differences exist but also is able to\nperform partial and shade-controllable makeup transfer. We also collected a\ndataset containing facial images with various poses and expressions for\nevaluations.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 02:52:00 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 08:25:05 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Jiang", "Wentao", ""], ["Liu", "Si", ""], ["Gao", "Chen", ""], ["Cao", "Jie", ""], ["He", "Ran", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1909.06957", "submitter": "Dorien Herremans", "authors": "Ha Thi Phuong Thao, Dorien Herremans and Gemma Roig", "title": "Multimodal Deep Models for Predicting Affective Responses Evoked by\n  Movies", "comments": "10 pages, 7 figures, Preprint accepted for publication in the\n  Proceedings of the 2nd International Workshop on Computer Vision for\n  Physiological Measurement as part of ICCV. Seoul, South Korea. 2019", "journal-ref": "Proceedings of the 2nd International Workshop on Computer Vision\n  for Physiological Measurement as part of ICCV. Seoul, South Korea. 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this study is to develop and analyze multimodal models for\npredicting experienced affective responses of viewers watching movie clips. We\ndevelop hybrid multimodal prediction models based on both the video and audio\nof the clips. For the video content, we hypothesize that both image content and\nmotion are crucial features for evoked emotion prediction. To capture such\ninformation, we extract features from RGB frames and optical flow using\npre-trained neural networks. For the audio model, we compute an enhanced set of\nlow-level descriptors including intensity, loudness, cepstrum, linear predictor\ncoefficients, pitch and voice quality. Both visual and audio features are then\nconcatenated to create audio-visual features, which are used to predict the\nevoked emotion. To classify the movie clips into the corresponding affective\nresponse categories, we propose two approaches based on deep neural network\nmodels. The first one is based on fully connected layers without memory on the\ntime component, the second incorporates the sequential dependency with a long\nshort-term memory recurrent neural network (LSTM). We perform a thorough\nanalysis of the importance of each feature set. Our experiments reveal that in\nour set-up, predicting emotions at each time step independently gives slightly\nbetter accuracy performance than with the LSTM. Interestingly, we also observe\nthat the optical flow is more informative than the RGB in videos, and overall,\nmodels using audio features are more accurate than those based on video\nfeatures when making the final prediction of evoked emotions.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 02:55:11 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 05:24:05 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Thao", "Ha Thi Phuong", ""], ["Herremans", "Dorien", ""], ["Roig", "Gemma", ""]]}, {"id": "1909.06966", "submitter": "Zhaoyi Yan", "authors": "Zhaoyi Yan, Yuchen Yuan, Wangmeng Zuo, Xiao Tan, Yezhen Wang, Shilei\n  Wen, Errui Ding", "title": "Perspective-Guided Convolution Networks for Crowd Counting", "comments": "Accepted by ICCV 2019", "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel perspective-guided convolution (PGC) for\nconvolutional neural network (CNN) based crowd counting (i.e. PGCNet), which\naims to overcome the dramatic intra-scene scale variations of people due to the\nperspective effect. While most state-of-the-arts adopt multi-scale or\nmulti-column architectures to address such issue, they generally fail in\nmodeling continuous scale variations since only discrete representative scales\nare considered. PGCNet, on the other hand, utilizes perspective information to\nguide the spatially variant smoothing of feature maps before feeding them to\nthe successive convolutions. An effective perspective estimation branch is also\nintroduced to PGCNet, which can be trained in either supervised setting or\nweakly-supervised setting when the branch has been pre-trained. Our PGCNet is\nsingle-column with moderate increase in computation, and extensive experimental\nresults on four benchmark datasets show the improvements of our method against\nthe state-of-the-arts. Additionally, we also introduce Crowd Surveillance, a\nlarge scale dataset for crowd counting that contains 13,000+ high-resolution\nimages with challenging scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 03:16:17 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Yan", "Zhaoyi", ""], ["Yuan", "Yuchen", ""], ["Zuo", "Wangmeng", ""], ["Tan", "Xiao", ""], ["Wang", "Yezhen", ""], ["Wen", "Shilei", ""], ["Ding", "Errui", ""]]}, {"id": "1909.06970", "submitter": "Montserrat Alvarado", "authors": "Alicia Montserrat Alvarado-Gonzalez, Gibran Fuentes-Pineda and Jorge\n  Cervantes-Ojeda", "title": "A few filters are enough: Convolutional Neural Network for P300\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, convolutional neural networks (CNNs) have become the\ndriving force of an ever-increasing set of applications, achieving\nstate-of-the-art performance. Most of the modern CNN architectures are composed\nof many convolutional and fully connected layers and typically require\nthousands or millions of parameters to learn. CNNs have also been effective in\nthe detection of Event-Related Potentials from electroencephalogram (EEG)\nsignals, notably the P300 component which is frequently employed in\nBrain-Computer Interfaces (BCIs). However, for this task, the increase in\ndetection rates compared to approaches based on human-engineered features has\nnot been as impressive as in other areas and might not justify such a large\nnumber of parameters. In this paper, we study the performances of existing CNN\narchitectures with diverse complexities for single-trial within-subject and\ncross-subject P300 detection on four different datasets. We also proposed\nSepConv1D, a very simple CNN architecture consisting of a single depthwise\nseparable 1D convolutional layer followed by a fully connected Sigmoid\nclassification neuron. We found that with as few as four filters in its\nconvolutional layer and a small overall number of parameters, SepConv1D\nobtained competitive performances in the four datasets. We believe this may\nrepresent an important step towards building simpler, cheaper, faster, and more\nportable BCIs.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 03:48:28 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 01:27:09 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2020 19:38:37 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Alvarado-Gonzalez", "Alicia Montserrat", ""], ["Fuentes-Pineda", "Gibran", ""], ["Cervantes-Ojeda", "Jorge", ""]]}, {"id": "1909.06978", "submitter": "Chongzhi Zhang", "authors": "Chongzhi Zhang, Aishan Liu, Xianglong Liu, Yitao Xu, Hang Yu, Yuqing\n  Ma, Tianlin Li", "title": "Interpreting and Improving Adversarial Robustness of Deep Neural\n  Networks with Neuron Sensitivity", "comments": "Accepted by IEEE Transaction on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2020.3042083", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are vulnerable to adversarial examples where\ninputs with imperceptible perturbations mislead DNNs to incorrect results.\nDespite the potential risk they bring, adversarial examples are also valuable\nfor providing insights into the weakness and blind-spots of DNNs. Thus, the\ninterpretability of a DNN in the adversarial setting aims to explain the\nrationale behind its decision-making process and makes deeper understanding\nwhich results in better practical applications. To address this issue, we try\nto explain adversarial robustness for deep models from a new perspective of\nneuron sensitivity which is measured by neuron behavior variation intensity\nagainst benign and adversarial examples. In this paper, we first draw the close\nconnection between adversarial robustness and neuron sensitivities, as\nsensitive neurons make the most non-trivial contributions to model predictions\nin the adversarial setting. Based on that, we further propose to improve\nadversarial robustness by constraining the similarities of sensitive neurons\nbetween benign and adversarial examples which stabilizes the behaviors of\nsensitive neurons towards adversarial noises. Moreover, we demonstrate that\nstate-of-the-art adversarial training methods improve model robustness by\nreducing neuron sensitivities which in turn confirms the strong connections\nbetween adversarial robustness and neuron sensitivity as well as the\neffectiveness of using sensitive neurons to build robust models. Extensive\nexperiments on various datasets demonstrate that our algorithm effectively\nachieves excellent results.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 04:10:13 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 14:52:34 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2020 15:13:09 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zhang", "Chongzhi", ""], ["Liu", "Aishan", ""], ["Liu", "Xianglong", ""], ["Xu", "Yitao", ""], ["Yu", "Hang", ""], ["Ma", "Yuqing", ""], ["Li", "Tianlin", ""]]}, {"id": "1909.06979", "submitter": "Seokju Lee", "authors": "Seokju Lee, Junsik Kim, Tae-Hyun Oh, Yongseop Jeong, Donggeun Yoo,\n  Stephen Lin, In So Kweon", "title": "Visuomotor Understanding for Representation Learning of Driving Scenes", "comments": "BMVC 2019. Supplementary material:\n  https://bmvc2019.org/wp-content/uploads/papers/0002-supplementary.zip\n  Dataset: http://github.com/SeokjuLee/driving-dataset-doc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dashboard cameras capture a tremendous amount of driving scene video each\nday. These videos are purposefully coupled with vehicle sensing data, such as\nfrom the speedometer and inertial sensors, providing an additional sensing\nmodality for free. In this work, we leverage the large-scale unlabeled yet\nnaturally paired data for visual representation learning in the driving\nscenario. A representation is learned in an end-to-end self-supervised\nframework for predicting dense optical flow from a single frame with paired\nsensing data. We postulate that success on this task requires the network to\nlearn semantic and geometric knowledge in the ego-centric view. For example,\nforecasting a future view to be seen from a moving vehicle requires an\nunderstanding of scene depth, scale, and movement of objects. We demonstrate\nthat our learned representation can benefit other tasks that require detailed\nscene understanding and outperforms competing unsupervised representations on\nsemantic segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 04:12:37 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Lee", "Seokju", ""], ["Kim", "Junsik", ""], ["Oh", "Tae-Hyun", ""], ["Jeong", "Yongseop", ""], ["Yoo", "Donggeun", ""], ["Lin", "Stephen", ""], ["Kweon", "In So", ""]]}, {"id": "1909.06980", "submitter": "Wei Gao", "authors": "Wei Gao, Russ Tedrake", "title": "kPAM-SC: Generalizable Manipulation Planning using KeyPoint Affordance\n  and Shape Completion", "comments": "In submission. The video demo and source code are available on\n  https://sites.google.com/view/generalizable-manipulation/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manipulation planning is the task of computing robot trajectories that move a\nset of objects to their target configuration while satisfying physically\nfeasibility. In contrast to existing works that assume known object templates,\nwe are interested in manipulation planning for a category of objects with\npotentially unknown instances and large intra-category shape variation. To\nachieve it, we need an object representation with which the manipulation\nplanner can reason about both the physical feasibility and desired object\nconfiguration, while being generalizable to novel instances. The widely-used\npose representation is not suitable, as representing an object with a\nparameterized transformation from a fixed template cannot capture large\nintra-category shape variation. Hence, we propose a new hybrid object\nrepresentation consisting of semantic keypoint and dense geometry (a point\ncloud or mesh) as the interface between the perception module and motion\nplanner. Leveraging advances in learning-based keypoint detection and shape\ncompletion, both dense geometry and keypoints can be perceived from raw sensor\ninput. Using the proposed hybrid object representation, we formulate the\nmanipulation task as a motion planning problem which encodes both the object\ntarget configuration and physical feasibility for a category of objects. In\nthis way, many existing manipulation planners can be generalized to categories\nof objects, and the resulting perception-to-action manipulation pipeline is\nrobust to large intra-category shape variation. Extensive hardware experiments\ndemonstrate our pipeline can produce robot trajectories that accomplish tasks\nwith never-before-seen objects.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 04:15:12 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Gao", "Wei", ""], ["Tedrake", "Russ", ""]]}, {"id": "1909.06982", "submitter": "Tai-Xiang Jiang", "authors": "Tai-Xiang Jiang, Michael K. Ng, Xi-Le Zhao, and Ting-Zhu Huang", "title": "Framelet Representation of Tensor Nuclear Norm for Third-Order Tensor\n  Completion", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.3000349", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main aim of this paper is to develop a framelet representation of the\ntensor nuclear norm for third-order tensor completion. In the literature, the\ntensor nuclear norm can be computed by using tensor singular value\ndecomposition based on the discrete Fourier transform matrix, and tensor\ncompletion can be performed by the minimization of the tensor nuclear norm\nwhich is the relaxation of the sum of matrix ranks from all Fourier transformed\nmatrix frontal slices. These Fourier transformed matrix frontal slices are\nobtained by applying the discrete Fourier transform on the tubes of the\noriginal tensor. In this paper, we propose to employ the framelet\nrepresentation of each tube so that a framelet transformed tensor can be\nconstructed. Because of framelet basis redundancy, the representation of each\ntube is sparsely represented. When the matrix slices of the original tensor are\nhighly correlated, we expect the corresponding sum of matrix ranks from all\nframelet transformed matrix frontal slices would be small, and the resulting\ntensor completion can be performed much better. The proposed minimization model\nis convex and global minimizers can be obtained. Numerical results on several\ntypes of multi-dimensional data (videos, multispectral images, and magnetic\nresonance imaging data) have tested and shown that the proposed method\noutperformed the other testing methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 04:31:49 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2020 10:10:17 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Jiang", "Tai-Xiang", ""], ["Ng", "Michael K.", ""], ["Zhao", "Xi-Le", ""], ["Huang", "Ting-Zhu", ""]]}, {"id": "1909.06989", "submitter": "Bingwen Hu", "authors": "Bingwen Hu, Zhedong Zheng, Ping Liu, Wankou Yang, and Mingwu Ren", "title": "Unsupervised Eyeglasses Removal in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eyeglasses removal is challenging in removing different kinds of eyeglasses,\ne.g., rimless glasses, full-rim glasses and sunglasses, and recovering\nappropriate eyes. Due to the large visual variants, the conventional methods\nlack scalability. Most existing works focus on the frontal face images in the\ncontrolled environment, such as the laboratory, and need to design specific\nsystems for different eyeglass types. To address the limitation, we propose a\nunified eyeglass removal model called Eyeglasses Removal Generative Adversarial\nNetwork (ERGAN), which could handle different types of glasses in the wild. The\nproposed method does not depend on the dense annotation of eyeglasses location\nbut benefits from the large-scale face images with weak annotations.\nSpecifically, we study the two relevant tasks simultaneously, i.e., removing\nand wearing eyeglasses. Given two facial images with and without eyeglasses,\nthe proposed model learns to swap the eye area in two faces. The generation\nmechanism focuses on the eye area and invades the difficulty of generating a\nnew face. In the experiment, we show the proposed method achieves a competitive\nremoval quality in terms of realism and diversity. Furthermore, we evaluate\nERGAN on several subsequent tasks, such as face verification and facial\nexpression recognition. The experiment shows that our method could serve as a\npre-processing method for these tasks.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 05:13:47 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 10:34:20 GMT"}, {"version": "v3", "created": "Wed, 5 Feb 2020 00:37:17 GMT"}, {"version": "v4", "created": "Fri, 15 May 2020 15:08:18 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Hu", "Bingwen", ""], ["Zheng", "Zhedong", ""], ["Liu", "Ping", ""], ["Yang", "Wankou", ""], ["Ren", "Mingwu", ""]]}, {"id": "1909.06993", "submitter": "Rogerio Bonatti", "authors": "Rogerio Bonatti and Ratnesh Madaan and Vibhav Vineet and Sebastian\n  Scherer and Ashish Kapoor", "title": "Learning Visuomotor Policies for Aerial Navigation Using Cross-Modal\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machines are a long way from robustly solving open-world perception-control\ntasks, such as first-person view (FPV) aerial navigation. While recent advances\nin end-to-end Machine Learning, especially Imitation and Reinforcement Learning\nappear promising, they are constrained by the need of large amounts of\ndifficult-to-collect labeled real-world data. Simulated data, on the other\nhand, is easy to generate, but generally does not render safe behaviors in\ndiverse real-life scenarios. In this work we propose a novel method for\nlearning robust visuomotor policies for real-world deployment which can be\ntrained purely with simulated data. We develop rich state representations that\ncombine supervised and unsupervised environment data. Our approach takes a\ncross-modal perspective, where separate modalities correspond to the raw camera\ndata and the system states relevant to the task, such as the relative pose of\ngates to the drone in the case of drone racing. We feed both data modalities\ninto a novel factored architecture, which learns a joint low-dimensional\nembedding via Variational Auto Encoders. This compact representation is then\nfed into a control policy, which we trained using imitation learning with\nexpert trajectories in a simulator. We analyze the rich latent spaces learned\nwith our proposed representations, and show that the use of our cross-modal\narchitecture significantly improves control policy performance as compared to\nend-to-end learning or purely unsupervised feature extractors. We also present\nreal-world results for drone navigation through gates in different track\nconfigurations and environmental conditions. Our proposed method, which runs\nfully onboard, can successfully generalize the learned representations and\npolicies across simulation and reality, significantly outperforming baseline\napproaches.\n  Supplementary video: https://youtu.be/VKc3A5HlUU8\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 05:23:14 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 13:22:41 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Bonatti", "Rogerio", ""], ["Madaan", "Ratnesh", ""], ["Vineet", "Vibhav", ""], ["Scherer", "Sebastian", ""], ["Kapoor", "Ashish", ""]]}, {"id": "1909.06998", "submitter": "Taeyoung Kim", "authors": "Taeyoung Kim, Youngsun Kwon and Sung-eui Yoon", "title": "Real-time 3-D Mapping with Estimating Acoustic Materials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a real-time system integrating an acoustic material\nestimation from visual appearance and an on-the-fly mapping in the 3-dimension.\nThe proposed method estimates the acoustic materials of surroundings in indoor\nscenes and incorporates them to a 3-D occupancy map, as a robot moves around\nthe environment. To estimate the acoustic material from the visual cue, we\napply the state-of-the-art semantic segmentation CNN network based on the\nassumption that the visual appearance and the acoustic materials have a strong\nassociation. Furthermore, we introduce an update policy to handle the material\nestimations during the online mapping process. As a result, our environment map\nwith acoustic material can be used for sound-related robotics applications,\nsuch as sound source localization taking into account various acoustic\npropagation (e.g., reflection).\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 05:40:38 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Kim", "Taeyoung", ""], ["Kwon", "Youngsun", ""], ["Yoon", "Sung-eui", ""]]}, {"id": "1909.06999", "submitter": "Seokju Lee", "authors": "Seokju Lee, Sunghoon Im, Stephen Lin, In So Kweon", "title": "Learning Residual Flow as Dynamic Motion from Stereo Videos", "comments": "IROS 2019. https://sites.google.com/site/seokjucv/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for decomposing the 3D scene flow observed from a moving\nstereo rig into stationary scene elements and dynamic object motion. Our\nunsupervised learning framework jointly reasons about the camera motion,\noptical flow, and 3D motion of moving objects. Three cooperating networks\npredict stereo matching, camera motion, and residual flow, which represents the\nflow component due to object motion and not from camera motion. Based on rigid\nprojective geometry, the estimated stereo depth is used to guide the camera\nmotion estimation, and the depth and camera motion are used to guide the\nresidual flow estimation. We also explicitly estimate the 3D scene flow of\ndynamic objects based on the residual flow and scene depth. Experiments on the\nKITTI dataset demonstrate the effectiveness of our approach and show that our\nmethod outperforms other state-of-the-art algorithms on the optical flow and\nvisual odometry tasks.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 05:48:42 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Lee", "Seokju", ""], ["Im", "Sunghoon", ""], ["Lin", "Stephen", ""], ["Kweon", "In So", ""]]}, {"id": "1909.07031", "submitter": "Masashi Okada Dr", "authors": "Masashi Okada, Shinji Takenaka, Tadahiro Taniguchi", "title": "Multi-person Pose Tracking using Sequential Monte Carlo with\n  Probabilistic Neural Pose Predictor", "comments": "Accepted to ICRA2020; Camera-ready ver", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is an effective strategy for the multi-person pose tracking task in videos\nto employ prediction and pose matching in a frame-by-frame manner. For this\ntype of approach, uncertainty-aware modeling is essential because precise\nprediction is impossible. However, previous studies have relied on only a\nsingle prediction without incorporating uncertainty, which can cause critical\ntracking errors if the prediction is unreliable. This paper proposes an\nextension to this approach with Sequential Monte Carlo (SMC). This naturally\nreformulates the tracking scheme to handle multiple predictions (or hypotheses)\nof poses, thereby mitigating the negative effect of prediction errors. An\nimportant component of SMC, i.e., a proposal distribution, is designed as a\nprobabilistic neural pose predictor, which can propose diverse and plausible\nhypotheses by incorporating epistemic uncertainty and heteroscedastic aleatoric\nuncertainty. In addition, a recurrent architecture is introduced to our neural\nmodeling to utilize time-sequence information of poses to manage difficult\nsituations, such as the frequent disappearance and reappearances of poses.\nCompared to existing baselines, the proposed method achieves a state-of-the-art\nMOTA score on the PoseTrack2018 validation dataset by reducing approximately\n50% of tracking errors from a state-of-the art baseline method.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 07:25:28 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 07:54:14 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Okada", "Masashi", ""], ["Takenaka", "Shinji", ""], ["Taniguchi", "Tadahiro", ""]]}, {"id": "1909.07038", "submitter": "Zhenzhen Xiang", "authors": "Zhenzhen Xiang, Anbo Bao, Jie Li and Jianbo Su", "title": "Boosting Real-Time Driving Scene Parsing with Shared Semantics", "comments": null, "journal-ref": null, "doi": "10.1109/LRA.2020.2965075", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time scene parsing is a fundamental feature for autonomous driving\nvehicles with multiple cameras. In this letter we demonstrate that sharing\nsemantics between cameras with different perspectives and overlapped views can\nboost the parsing performance when compared with traditional methods, which\nindividually process the frames from each camera. Our framework is based on a\ndeep neural network for semantic segmentation but with two kinds of additional\nmodules for sharing and fusing semantics. On the one hand, a semantics sharing\nmodule is designed to establish the pixel-wise mapping between the input\nimages. Features as well as semantics are shared by the map to reduce\nduplicated workload which leads to more efficient computation. On the other\nhand, feature fusion modules are designed to combine different modal of\nsemantic features, which leverage the information from both inputs for better\naccuracy. To evaluate the effectiveness of the proposed framework, we have\napplied our network to a dual-camera vision system for driving scene parsing.\nExperimental results show that our network outperforms the baseline method on\nthe parsing accuracy with comparable computations.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 07:38:26 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 05:46:47 GMT"}, {"version": "v3", "created": "Thu, 28 Nov 2019 05:20:23 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Xiang", "Zhenzhen", ""], ["Bao", "Anbo", ""], ["Li", "Jie", ""], ["Su", "Jianbo", ""]]}, {"id": "1909.07043", "submitter": "Antonios Karakottas", "authors": "Antonis Karakottas, Nikolaos Zioulis, Stamatis Samaras, Dimitrios\n  Ataloglou, Vasileios Gkitsas, Dimitrios Zarpalas, Petros Daras", "title": "$360^o$ Surface Regression with a Hyper-Sphere Loss", "comments": "Pre-print of the main paper and supplementary material combined that\n  is going to be presented at 3DV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Omnidirectional vision is becoming increasingly relevant as more efficient\n$360^o$ image acquisition is now possible. However, the lack of annotated\n$360^o$ datasets has hindered the application of deep learning techniques on\nspherical content. This is further exaggerated on tasks where ground truth\nacquisition is difficult, such as monocular surface estimation. While recent\nresearch approaches on the 2D domain overcome this challenge by relying on\ngenerating normals from depth cues using RGB-D sensors, this is very difficult\nto apply on the spherical domain. In this work, we address the unavailability\nof sufficient $360^o$ ground truth normal data, by leveraging existing 3D\ndatasets and remodelling them via rendering. We present a dataset of $360^o$\nimages of indoor spaces with their corresponding ground truth surface normal,\nand train a deep convolutional neural network (CNN) on the task of monocular\n360 surface estimation. We achieve this by minimizing a novel angular loss\nfunction defined on the hyper-sphere using simple quaternion algebra. We put an\neffort to appropriately compare with other state of the art methods trained on\nplanar datasets and finally, present the practical applicability of our trained\nmodel on a spherical image re-lighting task using completely unseen data by\nqualitatively showing the promising generalization ability of our dataset and\nmodel. The dataset is available at:\nvcl3d.github.io/HyperSphereSurfaceRegression.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 07:55:49 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Karakottas", "Antonis", ""], ["Zioulis", "Nikolaos", ""], ["Samaras", "Stamatis", ""], ["Ataloglou", "Dimitrios", ""], ["Gkitsas", "Vasileios", ""], ["Zarpalas", "Dimitrios", ""], ["Daras", "Petros", ""]]}, {"id": "1909.07046", "submitter": "Justin Chan", "authors": "Justin Chan, Sharat Raju, Randall Bly, Jonathan A. Perkins, and\n  Shyamnath Gollakota", "title": "Identifying Pediatric Vascular Anomalies With Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vascular anomalies, more colloquially known as birthmarks, affect up to 1 in\n10 infants. Though many of these lesions self-resolve, some types can result in\nmedical complications or disfigurement without proper diagnosis or management.\nAccurately diagnosing vascular anomalies is challenging for pediatricians and\nprimary care physicians due to subtle visual differences and similarity to\nother pediatric dermatologic conditions. This can result in delayed or\nincorrect referrals for treatment. To address this problem, we developed a\nconvolutional neural network (CNN) to automatically classify images of vascular\nanomalies and other pediatric skin conditions to aid physicians with diagnosis.\nWe constructed a dataset of 21,681 clinical images, including data collected\nbetween 2002-2018 at Seattle Children's hospital as well as five\ndermatologist-curated online repositories, and built a taxonomy over vascular\nanomalies and other common pediatric skin lesions. The CNN achieved an average\nAUC of 0.9731 when ten-fold cross-validation was performed across a taxonomy of\n12 classes. The classifier's average AUC and weighted F1 score was 0.9889 and\n0.9732 respectively when evaluated on a previously unseen test set of six of\nthese classes. Further, when used as an aid by pediatricians (n = 7), the\nclassifier increased their average visual diagnostic accuracy from 73.10% to\n91.67%. The classifier runs in real-time on a smartphone and has the potential\nto improve diagnosis of these conditions, particularly in resource-limited\nareas.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 08:11:04 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Chan", "Justin", ""], ["Raju", "Sharat", ""], ["Bly", "Randall", ""], ["Perkins", "Jonathan A.", ""], ["Gollakota", "Shyamnath", ""]]}, {"id": "1909.07050", "submitter": "Se Young Chun", "authors": "Dongwon Park, Yonghyeok Seo, Dongju Shin, Jaesik Choi, Se Young Chun", "title": "A Single Multi-Task Deep Neural Network with Post-Processing for Object\n  Detection with Reasoning and Robotic Grasp Detection", "comments": "Dongwon Park and Yonghyeok Seo are equally contributed to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, robotic grasp detection (GD) and object detection (OD) with\nreasoning have been investigated using deep neural networks (DNNs). There have\nbeen works to combine these multi-tasks using separate networks so that robots\ncan deal with situations of grasping specific target objects in the cluttered,\nstacked, complex piles of novel objects from a single RGB-D camera. We propose\na single multi-task DNN that yields the information on GD, OD and relationship\nreasoning among objects with a simple post-processing. Our proposed methods\nyielded state-of-the-art performance with the accuracy of 98.6% and 74.2% and\nthe computation speed of 33 and 62 frame per second on VMRD and Cornell\ndatasets, respectively. Our methods also yielded 95.3% grasp success rate for\nsingle novel object grasping with a 4-axis robot arm and 86.7% grasp success\nrate in cluttered novel objects with a Baxter robot.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 08:24:23 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Park", "Dongwon", ""], ["Seo", "Yonghyeok", ""], ["Shin", "Dongju", ""], ["Choi", "Jaesik", ""], ["Chun", "Se Young", ""]]}, {"id": "1909.07057", "submitter": "Qi Dai", "authors": "Zhi-Qi Cheng, Jun-Xiu Li, Qi Dai, Xiao Wu, Alexander Hauptmann", "title": "Learning Spatial Awareness to Improve Crowd Counting", "comments": "ICCV 2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of crowd counting is to estimate the number of people in images by\nleveraging the annotation of center positions for pedestrians' heads. Promising\nprogresses have been made with the prevalence of deep Convolutional Neural\nNetworks. Existing methods widely employ the Euclidean distance (i.e., $L_2$\nloss) to optimize the model, which, however, has two main drawbacks: (1) the\nloss has difficulty in learning the spatial awareness (i.e., the position of\nhead) since it struggles to retain the high-frequency variation in the density\nmap, and (2) the loss is highly sensitive to various noises in crowd counting,\nsuch as the zero-mean noise, head size changes, and occlusions. Although the\nMaximum Excess over SubArrays (MESA) loss has been previously proposed to\naddress the above issues by finding the rectangular subregion whose predicted\ndensity map has the maximum difference from the ground truth, it cannot be\nsolved by gradient descent, thus can hardly be integrated into the deep\nlearning framework. In this paper, we present a novel architecture called\nSPatial Awareness Network (SPANet) to incorporate spatial context for crowd\ncounting. The Maximum Excess over Pixels (MEP) loss is proposed to achieve this\nby finding the pixel-level subregion with high discrepancy to the ground truth.\nTo this end, we devise a weakly supervised learning scheme to generate such\nregion with a multi-branch architecture. The proposed framework can be\nintegrated into existing deep crowd counting methods and is end-to-end\ntrainable. Extensive experiments on four challenging benchmarks show that our\nmethod can significantly improve the performance of baselines. More remarkably,\nour approach outperforms the state-of-the-art methods on all benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 08:39:01 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Cheng", "Zhi-Qi", ""], ["Li", "Jun-Xiu", ""], ["Dai", "Qi", ""], ["Wu", "Xiao", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1909.07061", "submitter": "Haofeng Li", "authors": "Haofeng Li, Guanqi Chen, Guanbin Li, Yizhou Yu", "title": "Motion Guided Attention for Video Salient Object Detection", "comments": "10 pages, 4 figures, ICCV 2019, code:\n  https://github.com/lhaof/Motion-Guided-Attention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video salient object detection aims at discovering the most visually\ndistinctive objects in a video. How to effectively take object motion into\nconsideration during video salient object detection is a critical issue.\nExisting state-of-the-art methods either do not explicitly model and harvest\nmotion cues or ignore spatial contexts within optical flow images. In this\npaper, we develop a multi-task motion guided video salient object detection\nnetwork, which learns to accomplish two sub-tasks using two sub-networks, one\nsub-network for salient object detection in still images and the other for\nmotion saliency detection in optical flow images. We further introduce a series\nof novel motion guided attention modules, which utilize the motion saliency\nsub-network to attend and enhance the sub-network for still images. These two\nsub-networks learn to adapt to each other by end-to-end training. Experimental\nresults demonstrate that the proposed method significantly outperforms existing\nstate-of-the-art algorithms on a wide range of benchmarks. We hope our simple\nand effective approach will serve as a solid baseline and help ease future\nresearch in video salient object detection. Code and models will be made\navailable.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 08:44:02 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 17:03:23 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Li", "Haofeng", ""], ["Chen", "Guanqi", ""], ["Li", "Guanbin", ""], ["Yu", "Yizhou", ""]]}, {"id": "1909.07068", "submitter": "Sen Yang", "authors": "Sen Yang, Wankou Yang and Zhen Cui", "title": "Pose Neural Fabrics Search", "comments": "11 pages, 8 figures, 7 tables. Code is available at\n  https://github.com/yangsenius/PoseNFS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) technologies have emerged in many domains to\njointly learn the architectures and weights of the neural network. However,\nmost existing NAS works claim they are task-specific and focus only on\noptimizing a single architecture to replace a human-designed neural network, in\nfact, their search processes are almost independent of domain knowledge of the\ntasks. In this paper, we propose Pose Neural Fabrics Search (PoseNFS). We\nexplore a new solution for NAS and human pose estimation task: part-specific\nneural architecture search, which can be seen as a variant of multi-task\nlearning. Firstly, we design a new neural architecture search space, Cell-based\nNeural Fabric (CNF), to learn micro as well as macro neural architecture using\na differentiable search strategy. Then, we view locating human keypoints as\nmultiple disentangled prediction sub-tasks, and then use prior knowledge of\nbody structure as guidance to search for multiple part-specific neural\narchitectures for different human parts. After search, all these part-specific\nCNFs have distinct micro and macro architecture parameters. The results show\nthat such knowledge-guided NAS-based architectures have obvious performance\nimprovements to a hand-designed part-based baseline model. The experiments on\nMPII and MS-COCO datasets demonstrate that PoseNFS\\footnote{Code is available\nat \\url{https://github.com/yangsenius/PoseNFS}} can achieve comparable\nperformance to some efficient and state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 08:56:14 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 12:14:31 GMT"}, {"version": "v3", "created": "Thu, 2 Jan 2020 01:56:06 GMT"}, {"version": "v4", "created": "Sat, 5 Dec 2020 05:40:24 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Yang", "Sen", ""], ["Yang", "Wankou", ""], ["Cui", "Zhen", ""]]}, {"id": "1909.07072", "submitter": "Yue Liao", "authors": "Yue Liao, Si Liu, Guanbin Li, Fei Wang, Yanjie Chen, Chen Qian, Bo Li", "title": "A Real-Time Cross-modality Correlation Filtering Method for Referring\n  Expression Comprehension", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring expression comprehension aims to localize the object instance\ndescribed by a natural language expression. Current referring expression\nmethods have achieved good performance. However, none of them is able to\nachieve real-time inference without accuracy drop. The reason for the\nrelatively slow inference speed is that these methods artificially split the\nreferring expression comprehension into two sequential stages including\nproposal generation and proposal ranking. It does not exactly conform to the\nhabit of human cognition. To this end, we propose a novel Realtime\nCross-modality Correlation Filtering method (RCCF). RCCF reformulates the\nreferring expression comprehension as a correlation filtering process. The\nexpression is first mapped from the language domain to the visual domain and\nthen treated as a template (kernel) to perform correlation filtering on the\nimage feature map. The peak value in the correlation heatmap indicates the\ncenter points of the target box. In addition, RCCF also regresses a 2-D object\nsize and 2-D offset. The center point coordinates, object size and center point\noffset together to form the target bounding box. Our method runs at 40 FPS\nwhile achieving leading performance in RefClef, RefCOCO, RefCOCO+ and RefCOCOg\nbenchmarks. In the challenging RefClef dataset, our methods almost double the\nstate-of-the-art performance (34.70% increased to 63.79%). We hope this work\ncan arouse more attention and studies to the new cross-modality correlation\nfiltering framework as well as the one-stage framework for referring expression\ncomprehension.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 09:01:45 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 02:23:22 GMT"}, {"version": "v3", "created": "Fri, 22 Nov 2019 08:29:40 GMT"}, {"version": "v4", "created": "Mon, 27 Apr 2020 03:50:23 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Liao", "Yue", ""], ["Liu", "Si", ""], ["Li", "Guanbin", ""], ["Wang", "Fei", ""], ["Chen", "Yanjie", ""], ["Qian", "Chen", ""], ["Li", "Bo", ""]]}, {"id": "1909.07074", "submitter": "Chanho Eom", "authors": "Chanho Eom, Hyunjong Park, and Bumsub Ham", "title": "Temporally Consistent Depth Prediction with Flow-Guided Memory Units", "comments": "IEEE Transactions on Intelligent Transportation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting depth from a monocular video sequence is an important task for\nautonomous driving. Although it has advanced considerably in the past few\nyears, recent methods based on convolutional neural networks (CNNs) discard\ntemporal coherence in the video sequence and estimate depth independently for\neach frame, which often leads to undesired inconsistent results over time. To\naddress this problem, we propose to memorize temporal consistency in the video\nsequence, and leverage it for the task of depth prediction. To this end, we\nintroduce a two-stream CNN with a flow-guided memory module, where each stream\nencodes visual and temporal features, respectively. The memory module,\nimplemented using convolutional gated recurrent units (ConvGRUs), inputs visual\nand temporal features sequentially together with optical flow tailored to our\ntask. It memorizes trajectories of individual features selectively and\npropagates spatial information over time, enforcing a long-term temporal\nconsistency to prediction results. We evaluate our method on the KITTI\nbenchmark dataset in terms of depth prediction accuracy, temporal consistency\nand runtime, and achieve a new state of the art. We also provide an extensive\nexperimental analysis, clearly demonstrating the effectiveness of our approach\nto memorizing temporal consistency for depth prediction.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 09:10:10 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Eom", "Chanho", ""], ["Park", "Hyunjong", ""], ["Ham", "Bumsub", ""]]}, {"id": "1909.07075", "submitter": "Dimitri Korsch", "authors": "Dimitri Korsch, Paul Bodesheim, Joachim Denzler", "title": "Classification-Specific Parts for Improving Fine-Grained Visual\n  Categorization", "comments": "Presented at the GCPR2019", "journal-ref": null, "doi": "10.1007/978-3-030-33676-9_5", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained visual categorization is a classification task for\ndistinguishing categories with high intra-class and small inter-class variance.\nWhile global approaches aim at using the whole image for performing the\nclassification, part-based solutions gather additional local information in\nterms of attentions or parts. We propose a novel classification-specific part\nestimation that uses an initial prediction as well as back-propagation of\nfeature importance via gradient computations in order to estimate relevant\nimage regions. The subsequently detected parts are then not only selected by\na-posteriori classification knowledge, but also have an intrinsic spatial\nextent that is determined automatically. This is in contrast to most part-based\napproaches and even to available ground-truth part annotations, which only\nprovide point coordinates and no additional scale information. We show in our\nexperiments on various widely-used fine-grained datasets the effectiveness of\nthe mentioned part selection method in conjunction with the extracted part\nfeatures.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 09:13:47 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Korsch", "Dimitri", ""], ["Bodesheim", "Paul", ""], ["Denzler", "Joachim", ""]]}, {"id": "1909.07083", "submitter": "Bowen Li", "authors": "Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, Philip H. S. Torr", "title": "Controllable Text-to-Image Generation", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel controllable text-to-image generative\nadversarial network (ControlGAN), which can effectively synthesise high-quality\nimages and also control parts of the image generation according to natural\nlanguage descriptions. To achieve this, we introduce a word-level spatial and\nchannel-wise attention-driven generator that can disentangle different visual\nattributes, and allow the model to focus on generating and manipulating\nsubregions corresponding to the most relevant words. Also, a word-level\ndiscriminator is proposed to provide fine-grained supervisory feedback by\ncorrelating words with image regions, facilitating training an effective\ngenerator which is able to manipulate specific visual attributes without\naffecting the generation of other content. Furthermore, perceptual loss is\nadopted to reduce the randomness involved in the image generation, and to\nencourage the generator to manipulate specific attributes required in the\nmodified text. Extensive experiments on benchmark datasets demonstrate that our\nmethod outperforms existing state of the art, and is able to effectively\nmanipulate synthetic images using natural language descriptions. Code is\navailable at https://github.com/mrlibw/ControlGAN.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 09:29:52 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 18:30:18 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Li", "Bowen", ""], ["Qi", "Xiaojuan", ""], ["Lukasiewicz", "Thomas", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1909.07097", "submitter": "Yongxiang Huang", "authors": "Yongxiang Huang and Albert C. S. Chung", "title": "CELNet: Evidence Localization for Pathology Images using Weakly\n  Supervised Learning", "comments": "Accepted for MICCAI 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32239-7_68", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite deep convolutional neural networks boost the performance of image\nclassification and segmentation in digital pathology analysis, they are usually\nweak in interpretability for clinical applications or require heavy annotations\nto achieve object localization. To overcome this problem, we propose a weakly\nsupervised learning-based approach that can effectively learn to localize the\ndiscriminative evidence for a diagnostic label from weakly labeled training\ndata. Experimental results show that our proposed method can reliably pinpoint\nthe location of cancerous evidence supporting the decision of interest, while\nstill achieving a competitive performance on glimpse-level and slide-level\nhistopathologic cancer detection tasks.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 10:02:51 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Huang", "Yongxiang", ""], ["Chung", "Albert C. S.", ""]]}, {"id": "1909.07113", "submitter": "Enze Xie", "authors": "Wenjia Wang, Enze Xie, Peize Sun, Wenhai Wang, Lixun Tian, Chunhua\n  Shen, Ping Luo", "title": "TextSR: Content-Aware Text Super-Resolution Guided by Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text recognition has witnessed rapid development with the advance of\nconvolutional neural networks. Nonetheless, most of the previous methods may\nnot work well in recognizing text with low resolution which is often seen in\nnatural scene images. An intuitive solution is to introduce super-resolution\ntechniques as pre-processing. However, conventional super-resolution methods in\nthe literature mainly focus on reconstructing the detailed texture of natural\nimages, which typically do not work well for text due to the unique\ncharacteristics of text. To tackle these problems, in this work, we propose a\ncontent-aware text super-resolution network to generate the information desired\nfor text recognition. In particular, we design an end-to-end network that can\nperform super-resolution and text recognition simultaneously. Different from\nprevious super-resolution methods, we use the loss of text recognition as the\nText Perceptual Loss to guide the training of the super-resolution network, and\nthus it pays more attention to the text content, rather than the irrelevant\nbackground area. Extensive experiments on several challenging benchmarks\ndemonstrate the effectiveness of our proposed method in restoring a sharp\nhigh-resolution image from a small blurred one, and show that the recognition\nperformance clearly boosts up the performance of text recognizer. To our\nknowledge, this is the first work focusing on text super-resolution. Code will\nbe released in https://github.com/xieenze/TextSR.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 10:46:06 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 02:43:31 GMT"}, {"version": "v3", "created": "Wed, 18 Sep 2019 00:27:54 GMT"}, {"version": "v4", "created": "Sun, 20 Oct 2019 03:30:58 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Wang", "Wenjia", ""], ["Xie", "Enze", ""], ["Sun", "Peize", ""], ["Wang", "Wenhai", ""], ["Tian", "Lixun", ""], ["Shen", "Chunhua", ""], ["Luo", "Ping", ""]]}, {"id": "1909.07137", "submitter": "Haojie Liu", "authors": "Haojie Liu, Kang Liao, Chunyu Lin, Yao Zhao and Yulan Guo", "title": "PLIN: A Network for Pseudo-LiDAR Point Cloud Interpolation", "comments": "7 pages, 5 figures, Submitted to ICRA2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR sensors can provide dependable 3D spatial information at a low\nfrequency (around 10Hz) and have been widely applied in the field of autonomous\ndriving and UAV. However, the camera with a higher frequency (around 20Hz) has\nto be decreased so as to match with LiDAR in a multi-sensor system. In this\npaper, we propose a novel Pseudo-LiDAR interpolation network (PLIN) to increase\nthe frequency of LiDAR sensors. PLIN can generate temporally and spatially\nhigh-quality point cloud sequences to match the high frequency of cameras. To\nachieve this goal, we design a coarse interpolation stage guided by consecutive\nsparse depth maps and motion relationship. We also propose a refined\ninterpolation stage guided by the realistic scene. Using this coarse-to-fine\ncascade structure, our method can progressively perceive multi-modal\ninformation and generate accurate intermediate point clouds. To the best of our\nknowledge, this is the first deep framework for Pseudo-LiDAR point cloud\ninterpolation, which shows appealing applications in navigation systems\nequipped with LiDAR and cameras. Experimental results demonstrate that PLIN\nachieves promising performance on the KITTI dataset, significantly\noutperforming the traditional interpolation method and the state-of-the-art\nvideo interpolation technique.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 11:50:04 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Liu", "Haojie", ""], ["Liao", "Kang", ""], ["Lin", "Chunyu", ""], ["Zhao", "Yao", ""], ["Guo", "Yulan", ""]]}, {"id": "1909.07145", "submitter": "Chee Seng Chan", "authors": "Chee-Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet Ng, Canjie Luo,\n  Zihan Ni, ChuanMing Fang, Shuaitao Zhang, Junyu Han, Errui Ding, Jingtuo Liu,\n  Dimosthenis Karatzas, Chee Seng Chan, Lianwen Jin", "title": "ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text (RRC-ArT)", "comments": "Technical report of ICDAR2019 Robust Reading Challenge on\n  Arbitrary-Shaped Text (RRC-ArT) Competition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports the ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped\nText (RRC-ArT) that consists of three major challenges: i) scene text\ndetection, ii) scene text recognition, and iii) scene text spotting. A total of\n78 submissions from 46 unique teams/individuals were received for this\ncompetition. The top performing score of each challenge is as follows: i) T1 -\n82.65%, ii) T2.1 - 74.3%, iii) T2.2 - 85.32%, iv) T3.1 - 53.86%, and v) T3.2 -\n54.91%. Apart from the results, this paper also details the ArT dataset, tasks\ndescription, evaluation metrics and participants methods. The dataset, the\nevaluation kit as well as the results are publicly available at\nhttps://rrc.cvc.uab.es/?ch=14\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 12:19:00 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Chng", "Chee-Kheng", ""], ["Liu", "Yuliang", ""], ["Sun", "Yipeng", ""], ["Ng", "Chun Chet", ""], ["Luo", "Canjie", ""], ["Ni", "Zihan", ""], ["Fang", "ChuanMing", ""], ["Zhang", "Shuaitao", ""], ["Han", "Junyu", ""], ["Ding", "Errui", ""], ["Liu", "Jingtuo", ""], ["Karatzas", "Dimosthenis", ""], ["Chan", "Chee Seng", ""], ["Jin", "Lianwen", ""]]}, {"id": "1909.07201", "submitter": "Oliver Struckmeier", "authors": "Oliver Struckmeier, Kshitij Tiwari, Shirin Dora, Martin J. Pearson,\n  Sander M. Bohte, Cyriel MA Pennartz and Ville Kyrki", "title": "MuPNet: Multi-modal Predictive Coding Network for Place Recognition by\n  Unsupervised Learning of Joint Visuo-Tactile Latent Representations", "comments": "Submitted to ICRA 2020. 6+1 Pages with 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extracting and binding salient information from different sensory modalities\nto determine common features in the environment is a significant challenge in\nrobotics. Here we present MuPNet (Multi-modal Predictive Coding Network), a\nbiologically plausible network architecture for extracting joint latent\nfeatures from visuo-tactile sensory data gathered from a biomimetic mobile\nrobot. In this study we evaluate MuPNet applied to place recognition as a\nsimulated biomimetic robot platform explores visually aliased environments. The\nF1 scores demonstrate that its performance over prior hand-crafted sensory\nfeature extraction techniques is equivalent under controlled conditions, with\nsignificant improvement when operating in novel environments.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 13:54:11 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Struckmeier", "Oliver", ""], ["Tiwari", "Kshitij", ""], ["Dora", "Shirin", ""], ["Pearson", "Martin J.", ""], ["Bohte", "Sander M.", ""], ["Pennartz", "Cyriel MA", ""], ["Kyrki", "Ville", ""]]}, {"id": "1909.07229", "submitter": "Xiangtai Li", "authors": "Xiangtai Li, Li Zhang, Ansheng You, Maoke Yang, Kuiyuan Yang, Yunhai\n  Tong", "title": "Global Aggregation then Local Distribution in Fully Convolutional\n  Networks", "comments": "accepted at BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been widely proven that modelling long-range dependencies in fully\nconvolutional networks (FCNs) via global aggregation modules is critical for\ncomplex scene understanding tasks such as semantic segmentation and object\ndetection. However, global aggregation is often dominated by features of large\npatterns and tends to oversmooth regions that contain small patterns (e.g.,\nboundaries and small objects). To resolve this problem, we propose to first use\n\\emph{Global Aggregation} and then \\emph{Local Distribution}, which is called\nGALD, where long-range dependencies are more confidently used inside large\npattern regions and vice versa. The size of each pattern at each position is\nestimated in the network as a per-channel mask map. GALD is end-to-end\ntrainable and can be easily plugged into existing FCNs with various global\naggregation modules for a wide range of vision tasks, and consistently improves\nthe performance of state-of-the-art object detection and instance segmentation\napproaches. In particular, GALD used in semantic segmentation achieves new\nstate-of-the-art performance on Cityscapes test set with mIoU 83.3\\%. Code is\navailable at: \\url{https://github.com/lxtGH/GALD-Net}\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 14:18:22 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Li", "Xiangtai", ""], ["Zhang", "Li", ""], ["You", "Ansheng", ""], ["Yang", "Maoke", ""], ["Yang", "Kuiyuan", ""], ["Tong", "Yunhai", ""]]}, {"id": "1909.07231", "submitter": "Muhamad Risqi U. Saputra", "authors": "Muhamad Risqi U. Saputra, Pedro P.B. de Gusmao, Chris Xiaoxuan Lu,\n  Yasin Almalioglu, Stefano Rosa, Changhao Chen, Johan Wahlstr\\\"om, Wei Wang,\n  Andrew Markham, Niki Trigoni", "title": "DeepTIO: A Deep Thermal-Inertial Odometry with Visual Hallucination", "comments": "Accepted to IEEE Robotics and Automation Letters (RAL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual odometry shows excellent performance in a wide range of environments.\nHowever, in visually-denied scenarios (e.g. heavy smoke or darkness), pose\nestimates degrade or even fail. Thermal cameras are commonly used for\nperception and inspection when the environment has low visibility. However,\ntheir use in odometry estimation is hampered by the lack of robust visual\nfeatures. In part, this is as a result of the sensor measuring the ambient\ntemperature profile rather than scene appearance and geometry. To overcome this\nissue, we propose a Deep Neural Network model for thermal-inertial odometry\n(DeepTIO) by incorporating a visual hallucination network to provide the\nthermal network with complementary information. The hallucination network is\ntaught to predict fake visual features from thermal images by using Huber loss.\nWe also employ selective fusion to attentively fuse the features from three\ndifferent modalities, i.e thermal, hallucination, and inertial features.\nExtensive experiments are performed in hand-held and mobile robot data in\nbenign and smoke-filled environments, showing the efficacy of the proposed\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 14:19:42 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2020 22:42:04 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Saputra", "Muhamad Risqi U.", ""], ["de Gusmao", "Pedro P. B.", ""], ["Lu", "Chris Xiaoxuan", ""], ["Almalioglu", "Yasin", ""], ["Rosa", "Stefano", ""], ["Chen", "Changhao", ""], ["Wahlstr\u00f6m", "Johan", ""], ["Wang", "Wei", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "1909.07245", "submitter": "Alun Preece", "authors": "Alun Preece", "title": "BMVC 2019: Workshop on Interpretable and Explainable Machine Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable\nMachine Vision, Cardiff, UK, September 12, 2019.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 14:44:19 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Preece", "Alun", ""]]}, {"id": "1909.07267", "submitter": "Jiawei Mo", "authors": "Jiawei Mo and Junaed Sattar", "title": "A Fast and Robust Place Recognition Approach for Stereo Visual Odometry\n  Using LiDAR Descriptors", "comments": "Accepted by IROS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Place recognition is a core component of Simultaneous Localization and\nMapping (SLAM) algorithms. Particularly in visual SLAM systems,\npreviously-visited places are recognized by measuring the appearance similarity\nbetween images representing these locations. However, such approaches are\nsensitive to visual appearance change and also can be computationally\nexpensive. In this paper, we propose an alternative approach adapting LiDAR\ndescriptors for 3D points obtained from stereo-visual odometry for place\nrecognition. 3D points are potentially more reliable than 2D visual cues (e.g.,\n2D features) against environmental changes (e.g., variable illumination) and\nthis may benefit visual SLAM systems in long-term deployment scenarios.\nStereo-visual odometry generates 3D points with an absolute scale, which\nenables us to use LiDAR descriptors for place recognition with high\ncomputational efficiency. Through extensive evaluations on standard benchmark\ndatasets, we demonstrate the accuracy, efficiency, and robustness of using 3D\npoints for place recognition over 2D methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 15:14:55 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 20:55:17 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 07:27:58 GMT"}, {"version": "v4", "created": "Sun, 26 Jul 2020 05:38:47 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Mo", "Jiawei", ""], ["Sattar", "Junaed", ""]]}, {"id": "1909.07273", "submitter": "Kai-Xuan Chen", "authors": "Kai-Xuan Chen, Xiao-Jun Wu, Jie-Yi Ren, Rui Wang and Josef Kittler", "title": "More About Covariance Descriptors for Image Set Coding: Log-Euclidean\n  Framework based Kernel Matrix Representation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a family of structural descriptors for visual data, namely\ncovariance descriptors (CovDs) that lie on a non-linear symmetric positive\ndefinite (SPD) manifold, a special type of Riemannian manifolds. We propose an\nimproved version of CovDs for image set coding by extending the traditional\nCovDs from Euclidean space to the SPD manifold. Specifically, the manifold of\nSPD matrices is a complete inner product space with the operations of\nlogarithmic multiplication and scalar logarithmic multiplication defined in the\nLog-Euclidean framework. In this framework, we characterise covariance\nstructure in terms of the arc-cosine kernel which satisfies Mercer's condition\nand propose the operation of mean centralization on SPD matrices. Furthermore,\nwe combine arc-cosine kernels of different orders using mixing parameters\nlearnt by kernel alignment in a supervised manner. Our proposed framework\nprovides a lower-dimensional and more discriminative data representation for\nthe task of image set classification. The experimental results demonstrate its\nsuperior performance, measured in terms of recognition accuracy, as compared\nwith the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 15:22:40 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 14:17:18 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Chen", "Kai-Xuan", ""], ["Wu", "Xiao-Jun", ""], ["Ren", "Jie-Yi", ""], ["Wang", "Rui", ""], ["Kittler", "Josef", ""]]}, {"id": "1909.07311", "submitter": "Artem Pavlov", "authors": "Artem L. Pavlov, Azat Davletshin, Alexey Kharlamov, Maksim S.\n  Koriukin, Artem Vasenin, Pavel Solovev, Pavel Ostyakov, Pavel A. Karpyshev,\n  George V. Ovchinnikov, Ivan V. Oseledets, and Dzmitry Tsetserukou", "title": "Recognition of Russian traffic signs in winter conditions. Solutions of\n  the \"Ice Vision\" competition winners", "comments": "Submitted to IEEE ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advancements of various autonomous car projects aiming to achieve\nSAE Level 5, real-time detection of traffic signs in real-life scenarios has\nbecome a highly relevant problem for the industry. Even though a great progress\nhas been achieved in this field, there is still no clear consensus on what the\nstate-of-the-art in this field is.\n  Moreover, it is important to develop and test systems in various regions and\nconditions. This is why the \"Ice Vision\" competition has focused on the\ndetection of Russian traffic signs in winter conditions. The IceVisionSet\ndataset used for this competition features real-world collection of lossless\nframe sequences with traffic sign annotations. The sequences were collected in\nvarying conditions, including: different weather, camera exposure, illumination\nand moving speeds.\n  In this work we describe the competition and present the solutions of the 3\ntop teams.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 16:15:51 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Pavlov", "Artem L.", ""], ["Davletshin", "Azat", ""], ["Kharlamov", "Alexey", ""], ["Koriukin", "Maksim S.", ""], ["Vasenin", "Artem", ""], ["Solovev", "Pavel", ""], ["Ostyakov", "Pavel", ""], ["Karpyshev", "Pavel A.", ""], ["Ovchinnikov", "George V.", ""], ["Oseledets", "Ivan V.", ""], ["Tsetserukou", "Dzmitry", ""]]}, {"id": "1909.07420", "submitter": "Marco Fiorucci", "authors": "Marco Fiorucci", "title": "Regular Partitions and Their Use in Structural Pattern Recognition", "comments": "PhD Thesis (Mar 2019), Ca Foscari University, Venice. arXiv admin\n  note: text overlap with arXiv:1704.07114 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years are characterized by an unprecedented quantity of available\nnetwork data which are produced at an astonishing rate by an heterogeneous\nvariety of interconnected sensors and devices. This high-throughput generation\ncalls for the development of new effective methods to store, retrieve,\nunderstand and process massive network data. In this thesis, we tackle this\nchallenge by introducing a framework to summarize large graphs based on\nSzemer\\'edi's Regularity Remma (RL), which roughly states that any sufficiently\nlarge graph can almost entirely be partitioned into a bounded number of\nrandom-like bipartite graphs. The partition resulting from the RL gives rise to\na summary, which inherits many of the essential structural properties of the\noriginal graph. We first extend an heuristic version of the RL to improve its\nefficiency and its robustness. We use the proposed algorithm to address\ngraph-based clustering and image segmentation tasks. In the second part of the\nthesis, we introduce a new heuristic algorithm which is characterized by an\nimprovement of the summary quality both in terms of reconstruction error and of\nnoise filtering. We use the proposed heuristic to address the graph search\nproblem defined under a similarity measure. Finally, we study the linkage among\nthe regularity lemma, the stochastic block model and the minimum description\nlength. This study provide us a principled way to develop a graph decomposition\nalgorithm based on stochastic block model which is fitted using likelihood\nmaximization.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 18:14:05 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 21:15:53 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Fiorucci", "Marco", ""]]}, {"id": "1909.07454", "submitter": "Kin Quan", "authors": "Kin Quan, Ryutaro Tanno, Rebecca J. Shipley, Jeremy S. Brown, Joseph\n  Jacob, John R. Hurst, David J. Hawkes", "title": "Reproducibility of an airway tapering measurement in CT with application\n  to bronchiectasis", "comments": "55 pages, 18 figures, The manuscript was originally published in\n  Journal of Medical Imaging", "journal-ref": "J. Med. Imag. 6(3), 034003 (2019)", "doi": "10.1117/1.JMI.6.3.034003", "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: This paper proposes a pipeline to acquire a scalar tapering\nmeasurement from the carina to the most distal point of an individual airway\nvisible on CT. We show the applicability of using tapering measurements on\nclinically acquired data by quantifying the reproducibility of the tapering\nmeasure. Methods: We generate a spline from the centreline of an airway to\nmeasure the area and arclength at contiguous intervals. The tapering\nmeasurement is the gradient of the linear regression between area in log space\nand arclength. The reproducibility of the measure was assessed by analysing\ndifferent radiation doses, voxel sizes and reconstruction kernel on single\ntimepoint and longitudinal CT scans and by evaluating the effct of airway\nbifurcations. Results: Using 74 airways from 10 CT scans, we show a statistical\ndifference, p = 3.4 $\\times$ 10$^{-4}$ in tapering between healthy airways (n =\n35) and those affected by bronchiectasis (n = 39). The difference between the\nmean of the two populations was 0.011mm$^{-1}$ and the difference between the\nmedians of the two populations was 0.006mm$^{-1}$. The tapering measurement\nretained a 95\\% confidence interval of $\\pm$0.005mm$^{-1}$ in a simulated 25\nmAs scan and retained a 95% confidence of $\\pm$0.005mm$^{-1}$ on simulated CTs\nup to 1.5 times the original voxel size. Conclusion: We have established an\nestimate of the precision of the tapering measurement and estimated the effect\non precision of simulated voxel size and CT scan dose. We recommend that the\nscanner calibration be undertaken with the phantoms as described, on the\nspecific CT scanner, radiation dose and reconstruction algorithm that is to be\nused in any quantitative studies. Our code is available at\nhttps://github.com/quan14/AirwayTaperingInCT\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 19:49:01 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Quan", "Kin", ""], ["Tanno", "Ryutaro", ""], ["Shipley", "Rebecca J.", ""], ["Brown", "Jeremy S.", ""], ["Jacob", "Joseph", ""], ["Hurst", "John R.", ""], ["Hawkes", "David J.", ""]]}, {"id": "1909.07459", "submitter": "Chen Jiang", "authors": "Chen Jiang, Martin Jagersand", "title": "Bridging Visual Perception with Contextual Semantics for Understanding\n  Robot Manipulation Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding manipulation scenarios allows intelligent robots to plan for\nappropriate actions to complete a manipulation task successfully. It is\nessential for intelligent robots to semantically interpret manipulation\nknowledge by describing entities, relations and attributes in a structural\nmanner. In this paper, we propose an implementing framework to generate\nhigh-level conceptual dynamic knowledge graphs from video clips. A combination\nof a Vision-Language model and an ontology system, in correspondence with\nvisual perception and contextual semantics, is used to represent robot\nmanipulation knowledge with Entity-Relation-Entity (E-R-E) and\nEntity-Attribute-Value (E-A-V) tuples. The proposed method is flexible and\nwell-versed. Using the framework, we present a case study where robot performs\nmanipulation actions in a kitchen environment, bridging visual perception with\ncontextual semantics using the generated dynamic knowledge graphs.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 20:06:54 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 11:15:04 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Jiang", "Chen", ""], ["Jagersand", "Martin", ""]]}, {"id": "1909.07474", "submitter": "Hoileong Lee", "authors": "Hoileong Lee, Tahreema Matin, Fergus Gleeson, and Vicente Grau", "title": "Efficient 3D Fully Convolutional Networks for Pulmonary Lobe\n  Segmentation in CT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human lung is a complex respiratory organ, consisting of five distinct\nanatomic compartments called lobes. Accurate and automatic segmentation of\nthese pulmonary lobes from computed tomography (CT) images is of clinical\nimportance for lung disease assessment and treatment planning. However, this\ntask is challenging due to ambiguous lobar boundaries, anatomical variations\nand pathological deformations. In this paper, we propose a high-resolution and\nefficient 3D fully convolutional network to automatically segment the lobes. We\nrefer to the network as Pulmonary Lobe Segmentation Network (PLS-Net), which is\ndesigned to efficiently exploit 3D spatial and contextual information from\nhigh-resolution volumetric CT images for effective volume-to-volume learning\nand inference. The PLS-Net is based on an asymmetric encoder-decoder\narchitecture with three novel components: (i) 3D depthwise separable\nconvolutions to improve the network efficiency by factorising each regular 3D\nconvolution into two simpler operations; (ii) dilated residual dense blocks to\nefficiently expand the receptive field of the network and aggregate multi-scale\ncontextual information for segmentation; and (iii) input reinforcement at each\ndownsampled resolution to compensate for the loss of spatial information due to\nconvolutional and downsampling operations. We evaluated the proposed PLS-Net on\na multi-institutional dataset that consists of 210 CT images acquired from\npatients with a wide range of lung abnormalities. Experimental results show\nthat our PLS-Net achieves state-of-the-art performance with better\ncomputational efficiency. Further experiments confirm the effectiveness of each\nnovel component of the PLS-Net.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 20:47:48 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Lee", "Hoileong", ""], ["Matin", "Tahreema", ""], ["Gleeson", "Fergus", ""], ["Grau", "Vicente", ""]]}, {"id": "1909.07480", "submitter": "Xiao-Yun Zhou", "authors": "Peichao Li, Xiao-Yun Zhou, Zhao-Yang Wang and Guang-Zhong Yang", "title": "Z-Net: an Anisotropic 3D DCNN for Medical CT Volume Segmentation", "comments": "8 pages, 9 figures, two tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate volume segmentation from the Computed Tomography (CT) scan is a\ncommon prerequisite for pre-operative planning, intra-operative guidance and\nquantitative assessment of therapeutic outcomes in robot-assisted Minimally\nInvasive Surgery (MIS). 3D Deep Convolutional Neural Network (DCNN) is a viable\nsolution for this task, but is memory intensive. Small isotropic patches are\ncropped from the original and large CT volume to mitigate this issue in\npractice, but it may cause discontinuities between the adjacent patches and\nsevere class-imbalances within individual sub-volumes. This paper presents a\nnew 3D DCNN framework, namely Z-Net, to tackle the discontinuity and\nclass-imbalance issue by preserving a full field-of-view of the objects in the\nXY planes using anisotropic spatial separable convolutions. The proposed Z-Net\ncan be seamlessly integrated into existing 3D DCNNs with isotropic convolutions\nsuch as 3D U-Net and V-Net, with improved volume segmentation Intersection over\nUnion (IoU) - up to $12.6\\%$. Detailed validation of Z-Net is provided for CT\naortic, liver and lung segmentation, demonstrating the effectiveness and\npractical value of Z-Net for intra-operative 3D navigation in robot-assisted\nMIS.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 20:56:13 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 16:12:39 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Li", "Peichao", ""], ["Zhou", "Xiao-Yun", ""], ["Wang", "Zhao-Yang", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "1909.07490", "submitter": "Rayan Mosli", "authors": "Rayan Mosli, Matthew Wright, Bo Yuan and Yin Pan", "title": "They Might NOT Be Giants: Crafting Black-Box Adversarial Examples with\n  Fewer Queries Using Particle Swarm Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models have been found to be susceptible to adversarial\nexamples that are often indistinguishable from the original inputs. These\nadversarial examples are created by applying adversarial perturbations to input\nsamples, which would cause them to be misclassified by the target models.\nAttacks that search and apply the perturbations to create adversarial examples\nare performed in both white-box and black-box settings, depending on the\ninformation available to the attacker about the target. For black-box attacks,\nthe only capability available to the attacker is the ability to query the\ntarget with specially crafted inputs and observing the labels returned by the\nmodel. Current black-box attacks either have low success rates, requires a high\nnumber of queries, or produce adversarial examples that are easily\ndistinguishable from their sources. In this paper, we present AdversarialPSO, a\nblack-box attack that uses fewer queries to create adversarial examples with\nhigh success rates. AdversarialPSO is based on the evolutionary search\nalgorithm Particle Swarm Optimization, a populationbased gradient-free\noptimization algorithm. It is flexible in balancing the number of queries\nsubmitted to the target vs the quality of imperceptible adversarial examples.\nThe attack has been evaluated using the image classification benchmark datasets\nCIFAR-10, MNIST, and Imagenet, achieving success rates of 99.6%, 96.3%, and\n82.0%, respectively, while submitting substantially fewer queries than the\nstate-of-the-art. We also present a black-box method for isolating salient\nfeatures used by models when making classifications. This method, called Swarms\nwith Individual Search Spaces or SWISS, creates adversarial examples by finding\nand modifying the most important features in the input.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 21:24:19 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Mosli", "Rayan", ""], ["Wright", "Matthew", ""], ["Yuan", "Bo", ""], ["Pan", "Yin", ""]]}, {"id": "1909.07499", "submitter": "Scott Workman", "authors": "Menghua Zhai and Tawfiq Salem and Connor Greenwell and Scott Workman\n  and Robert Pless and Nathan Jacobs", "title": "Learning Geo-Temporal Image Features", "comments": "British Machine Vision Conference (BMVC) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to implicitly learn to extract geo-temporal image features, which\nare mid-level features related to when and where an image was captured, by\nexplicitly optimizing for a set of location and time estimation tasks. To train\nour method, we take advantage of a large image dataset, captured by outdoor\nwebcams and cell phones. The only form of supervision we provide are the known\ncapture time and location of each image. We find that our approach learns\nfeatures that are related to natural appearance changes in outdoor scenes.\nAdditionally, we demonstrate the application of these geo-temporal features to\ntime and location estimation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 22:00:59 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Zhai", "Menghua", ""], ["Salem", "Tawfiq", ""], ["Greenwell", "Connor", ""], ["Workman", "Scott", ""], ["Pless", "Robert", ""], ["Jacobs", "Nathan", ""]]}, {"id": "1909.07507", "submitter": "Daniela Ridel", "authors": "Daniela Ridel, Nachiket Deo, Denis Wolf, Mohan Trivedi", "title": "Scene Compliant Trajectory Forecast with Agent-Centric Spatio-Temporal\n  Grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting long-term human motion is a challenging task due to the\nnon-linearity, multi-modality and inherent uncertainty in future trajectories.\nThe underlying scene and past motion of agents can provide useful cues to\npredict their future motion. However, the heterogeneity of the two inputs poses\na challenge for learning a joint representation of the scene and past\ntrajectories. To address this challenge, we propose a model based on grid\nrepresentations to forecast agent trajectories. We represent the past\ntrajectories of agents using binary 2-D grids, and the underlying scene as a\nRGB birds-eye view (BEV) image, with an agent-centric frame of reference. We\nencode the scene and past trajectories using convolutional layers and generate\ntrajectory forecasts using a Convolutional LSTM (ConvLSTM) decoder. Results on\nthe publicly available Stanford Drone Dataset (SDD) show that our model\noutperforms prior approaches and outputs realistic future trajectories that\ncomply with scene structure and past motion.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 22:33:27 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Ridel", "Daniela", ""], ["Deo", "Nachiket", ""], ["Wolf", "Denis", ""], ["Trivedi", "Mohan", ""]]}, {"id": "1909.07526", "submitter": "Dmitry Konovalov", "authors": "Dina B. Efremova and Mangalam Sankupellay and Dmitry A. Konovalov", "title": "Data-Efficient Classification of Birdcall Through Convolutional Neural\n  Networks Transfer Learning", "comments": "Accepted for IEEE Digital Image Computing: Techniques and\n  Applications, 2019 (DICTA 2019), 2-4 December 2019 in Perth, Australia,\n  http://dicta2019.dictaconference.org/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning Convolutional Neural Network (CNN) models are powerful\nclassification models but require a large amount of training data. In niche\ndomains such as bird acoustics, it is expensive and difficult to obtain a large\nnumber of training samples. One method of classifying data with a limited\nnumber of training samples is to employ transfer learning. In this research, we\nevaluated the effectiveness of birdcall classification using transfer learning\nfrom a larger base dataset (2814 samples in 46 classes) to a smaller target\ndataset (351 samples in 10 classes) using the ResNet-50 CNN. We obtained 79%\naverage validation accuracy on the target dataset in 5-fold cross-validation.\nThe methodology of transfer learning from an ImageNet-trained CNN to a\nproject-specific and a much smaller set of classes and images was extended to\nthe domain of spectrogram images, where the base dataset effectively played the\nrole of the ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 00:16:16 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Efremova", "Dina B.", ""], ["Sankupellay", "Mangalam", ""], ["Konovalov", "Dmitry A.", ""]]}, {"id": "1909.07541", "submitter": "Jie Lin", "authors": "Quang-Hieu Pham, Pierre Sevestre, Ramanpreet Singh Pahwa, Huijing\n  Zhan, Chun Ho Pang, Yuda Chen, Armin Mustafa, Vijay Chandrasekhar, Jie Lin", "title": "A*3D Dataset: Towards Autonomous Driving in Challenging Environments", "comments": "A new 3D dataset by I2R, A*STAR for autonomous driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing global popularity of self-driving cars, there is an\nimmediate need for challenging real-world datasets for benchmarking and\ntraining various computer vision tasks such as 3D object detection. Existing\ndatasets either represent simple scenarios or provide only day-time data. In\nthis paper, we introduce a new challenging A*3D dataset which consists of RGB\nimages and LiDAR data with significant diversity of scene, time, and weather.\nThe dataset consists of high-density images ($\\approx~10$ times more than the\npioneering KITTI dataset), heavy occlusions, a large number of night-time\nframes ($\\approx~3$ times the nuScenes dataset), addressing the gaps in the\nexisting datasets to push the boundaries of tasks in autonomous driving\nresearch to more challenging highly diverse environments. The dataset contains\n$39\\text{K}$ frames, $7$ classes, and $230\\text{K}$ 3D object annotations. An\nextensive 3D object detection benchmark evaluation on the A*3D dataset for\nvarious attributes such as high density, day-time/night-time, gives interesting\ninsights into the advantages and limitations of training and testing 3D object\ndetection in real-world setting.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 01:19:04 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Pham", "Quang-Hieu", ""], ["Sevestre", "Pierre", ""], ["Pahwa", "Ramanpreet Singh", ""], ["Zhan", "Huijing", ""], ["Pang", "Chun Ho", ""], ["Chen", "Yuda", ""], ["Mustafa", "Armin", ""], ["Chandrasekhar", "Vijay", ""], ["Lin", "Jie", ""]]}, {"id": "1909.07545", "submitter": "Menandro Roxas", "authors": "Menandro Roxas and Takeshi Oishi", "title": "Real-Time Variational Fisheye Stereo without Rectification and\n  Undistortion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense 3D maps from wide-angle cameras is beneficial to robotics applications\nsuch as navigation and autonomous driving. In this work, we propose a real-time\ndense 3D mapping method for fisheye cameras without explicit rectification and\nundistortion. We extend the conventional variational stereo method by\nconstraining the correspondence search along the epipolar curve using a\ntrajectory field induced by camera motion. We also propose a fast way of\ngenerating the trajectory field without increasing the processing time compared\nto conventional rectified methods. With our implementation, we were able to\nachieve real-time processing using modern GPUs. Our results show the advantages\nof our non-rectified dense mapping approach compared to rectified variational\nmethods and non-rectified discrete stereo matching methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 01:48:06 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Roxas", "Menandro", ""], ["Oishi", "Takeshi", ""]]}, {"id": "1909.07549", "submitter": "Linjie Deng", "authors": "Linjie Deng, Yanxiang Gong, Xinchen Lu, Yi Lin, Zheng Ma, Mei Xie", "title": "STELA: A Real-Time Scene Text Detector with Learned Anchor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To achieve high coverage of target boxes, a normal strategy of conventional\none-stage anchor-based detectors is to utilize multiple priors at each spatial\nposition, especially in scene text detection tasks. In this work, we present a\nsimple and intuitive method for multi-oriented text detection where each\nlocation of feature maps only associates with one reference box. The idea is\ninspired from the twostage R-CNN framework that can estimate the location of\nobjects with any shape by using learned proposals. The aim of our method is to\nintegrate this mechanism into a onestage detector and employ the learned anchor\nwhich is obtained through a regression operation to replace the original one\ninto the final predictions. Based on RetinaNet, our method achieves competitive\nperformances on several public benchmarks with a totally real-time efficiency\n(26:5fps at 800p), which surpasses all of anchor-based scene text detectors. In\naddition, with less attention on anchor design, we believe our method is easy\nto be applied on other analogous detection tasks. The code will publicly\navailable at https://github.com/xhzdeng/stela.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 02:01:30 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 01:30:52 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Deng", "Linjie", ""], ["Gong", "Yanxiang", ""], ["Lu", "Xinchen", ""], ["Lin", "Yi", ""], ["Ma", "Zheng", ""], ["Xie", "Mei", ""]]}, {"id": "1909.07558", "submitter": "Kai Qiao", "authors": "Wanting Yu, Hongyi Yu, Lingyun Jiang, Mengli Zhang, Kai Qiao, Linyuan\n  Wang, Bin Yan", "title": "HAD-GAN: A Human-perception Auxiliary Defense GAN to Defend Adversarial\n  Examples", "comments": "There is some error in our work. For example,\"Notably, we linked a\n  fully connected discriminant network in parallel at the penultimate level of\n  the target classifier.\" (section 3.2) Incorrect description of the network\n  structure can mislead readers. For example, \"associate GAN with human\n  imagination\" is not true.(section 1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples reveal the vulnerability and unexplained nature of\nneural networks. Studying the defense of adversarial examples is of\nconsiderable practical importance. Most adversarial examples that misclassify\nnetworks are often undetectable by humans. In this paper, we propose a defense\nmodel to train the classifier into a human-perception classification model with\nshape preference. The proposed model comprising a texture transfer network\n(TTN) and an auxiliary defense generative adversarial networks (GAN) is called\nHuman-perception Auxiliary Defense GAN (HAD-GAN). The TTN is used to extend the\ntexture samples of a clean image and helps classifiers focus on its shape. GAN\nis utilized to form a training framework for the model and generate the\nnecessary images. A series of experiments conducted on MNIST, Fashion-MNIST and\nCIFAR10 show that the proposed model outperforms the state-of-the-art defense\nmethods for network robustness. The model also demonstrates a significant\nimprovement on defense capability of adversarial examples.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 02:46:34 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 01:07:05 GMT"}, {"version": "v3", "created": "Sat, 25 Jul 2020 01:58:48 GMT"}, {"version": "v4", "created": "Sun, 9 May 2021 07:12:44 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yu", "Wanting", ""], ["Yu", "Hongyi", ""], ["Jiang", "Lingyun", ""], ["Zhang", "Mengli", ""], ["Qiao", "Kai", ""], ["Wang", "Linyuan", ""], ["Yan", "Bin", ""]]}, {"id": "1909.07566", "submitter": "Alexander Pon", "authors": "Alex D. Pon, Jason Ku, Chengyao Li, Steven L. Waslander", "title": "Object-Centric Stereo Matching for 3D Object Detection", "comments": "Accepted in ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Safe autonomous driving requires reliable 3D object detection-determining the\n6 DoF pose and dimensions of objects of interest. Using stereo cameras to solve\nthis task is a cost-effective alternative to the widely used LiDAR sensor. The\ncurrent state-of-the-art for stereo 3D object detection takes the existing\nPSMNet stereo matching network, with no modifications, and converts the\nestimated disparities into a 3D point cloud, and feeds this point cloud into a\nLiDAR-based 3D object detector. The issue with existing stereo matching\nnetworks is that they are designed for disparity estimation, not 3D object\ndetection; the shape and accuracy of object point clouds are not the focus.\nStereo matching networks commonly suffer from inaccurate depth estimates at\nobject boundaries, which we define as streaking, because background and\nforeground points are jointly estimated. Existing networks also penalize\ndisparity instead of the estimated position of object point clouds in their\nloss functions. We propose a novel 2D box association and object-centric stereo\nmatching method that only estimates the disparities of the objects of interest\nto address these two issues. Our method achieves state-of-the-art results on\nthe KITTI 3D and BEV benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 03:07:25 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 06:29:22 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Pon", "Alex D.", ""], ["Ku", "Jason", ""], ["Li", "Chengyao", ""], ["Waslander", "Steven L.", ""]]}, {"id": "1909.07572", "submitter": "Hongtao Wu", "authors": "Hongtao Wu, Deven Misra, and Gregory S. Chirikjian", "title": "Is That a Chair? Imagining Affordances Using Simulations of an\n  Articulated Human Body", "comments": "7 pages, 6 figures. Accepted to ICRA2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For robots to exhibit a high level of intelligence in the real world, they\nmust be able to assess objects for which they have no prior knowledge.\nTherefore, it is crucial for robots to perceive object affordances by reasoning\nabout physical interactions with the object. In this paper, we propose a novel\nmethod to provide robots with an ability to imagine object affordances using\nphysical simulations. The class of chair is chosen here as an initial category\nof objects to illustrate a more general paradigm. In our method, the robot\n\"imagines\" the affordance of an arbitrarily oriented object as a chair by\nsimulating a physical sitting interaction between an articulated human body and\nthe object. This object affordance reasoning is used as a cue for object\nclassification (chair vs non-chair). Moreover, if an object is classified as a\nchair, the affordance reasoning can also predict the upright pose of the object\nwhich allows the sitting interaction to take place. We call this type of poses\nthe functional pose. We demonstrate our method in chair classification on\nsynthetic 3D CAD models. Although our method uses only 30 models for training,\nit outperforms appearance-based deep learning methods, which require a large\namount of training data, when the upright orientation is not assumed to be\nknown a priori. In addition, we showcase that the functional pose predictions\nof our method align well with human judgments on both synthetic models and real\nobjects scanned by a depth camera.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 03:36:32 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 21:25:42 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Wu", "Hongtao", ""], ["Misra", "Deven", ""], ["Chirikjian", "Gregory S.", ""]]}, {"id": "1909.07577", "submitter": "Mehrdad Shoeiby", "authors": "Mehrdad Shoeiby, Sadegh Aliakbarian, Saeed Anwar, Lars Petersson", "title": "Multi-FAN: Multi-Spectral Mosaic Super-Resolution Via Multi-Scale\n  Feature Aggregation Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel method to super-resolve multi-spectral images\ncaptured by modern real-time single-shot mosaic image sensors, also known as\nmulti-spectral cameras. Our contribution is two-fold. Firstly, we super-resolve\nmulti-spectral images from mosaic images rather than image cubes, which helps\nto take into account the spatial offset of each wavelength. Secondly, we\nintroduce an external multi-scale feature aggregation network (Multi-FAN) which\nconcatenates the feature maps with different levels of semantic information\nthroughout a super-resolution (SR) network. A cascade of convolutional layers\nthen implicitly selects the most valuable feature maps to generate a mosaic\nimage. This mosaic image is then merged with the mosaic image generated by the\nSR network to produce a quantitatively superior image. We apply our Multi-FAN\nto RCAN (Residual Channel Attention Network), which is the state-of-the-art SR\nalgorithm. We show that Multi-FAN improves both quantitative results and well\nas inference time.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 03:53:20 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 02:22:59 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 05:52:55 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Shoeiby", "Mehrdad", ""], ["Aliakbarian", "Sadegh", ""], ["Anwar", "Saeed", ""], ["Petersson", "Lars", ""]]}, {"id": "1909.07581", "submitter": "Saima Rathore", "authors": "Saima Rathore, Muhammad A. Iftikhar, Metin N. Gurcan, Zissimos\n  Mourelatos", "title": "Radiopathomics: Integration of radiographic and histologic\n  characteristics for prognostication in glioblastoma", "comments": "10 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both radiographic (Rad) imaging, such as multi-parametric magnetic resonance\nimaging, and digital pathology (Path) images captured from tissue samples are\ncurrently acquired as standard clinical practice for glioblastoma tumors. Both\nthese data streams have been separately used for diagnosis and treatment\nplanning, despite the fact that they provide complementary information. In this\nresearch work, we aimed to assess the potential of both Rad and Path images in\ncombination and comparison. An extensive set of engineered features was\nextracted from delineated tumor regions in Rad images, comprising T1, T1-Gd,\nT2, T2-FLAIR, and 100 random patches extracted from Path images. Specifically,\nthe features comprised descriptors of intensity, histogram, and texture, mainly\nquantified via gray-level-co-occurrence matrix and gray-level-run-length\nmatrices. Features extracted from images of 107 glioblastoma patients,\ndownloaded from The Cancer Imaging Archive, were run through support vector\nmachine for classification using leave-one-out cross-validation mechanism, and\nthrough support vector regression for prediction of continuous survival\noutcome. The Pearson correlation coefficient was estimated to be 0.75, 0.74,\nand 0.78 for Rad, Path and RadPath data. The area-under the receiver operating\ncharacteristic curve was estimated to be 0.74, 0.76 and 0.80 for Rad, Path and\nRadPath data, when patients were discretized into long- and short-survival\ngroups based on average survival cutoff. Our results support the notion that\nsynergistically using Rad and Path images may lead to better prognosis at the\ninitial presentation of the disease, thereby facilitating the targeted\nenrollment of patients into clinical trials.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 04:32:09 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 17:58:20 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Rathore", "Saima", ""], ["Iftikhar", "Muhammad A.", ""], ["Gurcan", "Metin N.", ""], ["Mourelatos", "Zissimos", ""]]}, {"id": "1909.07583", "submitter": "Yuhong Guo", "authors": "Yaser Alwattar and Yuhong Guo", "title": "Inverse Visual Question Answering with Multi-Level Attentions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel deep multi-level attention model to address\ninverse visual question answering. The proposed model generates regional visual\nand semantic features at the object level and then enhances them with the\nanswer cue by using attention mechanisms. Two levels of multiple attentions are\nemployed in the model, including the dual attention at the partial question\nencoding step and the dynamic attention at the next question word generation\nstep. We evaluate the proposed model on the VQA V1 dataset. It demonstrates\nstate-of-the-art performance in terms of multiple commonly used metrics.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 04:41:12 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 00:13:21 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Alwattar", "Yaser", ""], ["Guo", "Yuhong", ""]]}, {"id": "1909.07594", "submitter": "Lalith Srikanth Chintalapati", "authors": "Lalith Srikanth Chintalapati, Raghunatha Sarma Rachakonda", "title": "Conformal Prediction based Spectral Clustering", "comments": "Under review in a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral Clustering(SC) is a prominent data clustering technique of recent\ntimes which has attracted much attention from researchers. It is a highly\ndata-driven method and makes no strict assumptions on the structure of the data\nto be clustered. One of the central pieces of spectral clustering is the\nconstruction of an affinity matrix based on a similarity measure between data\npoints. The way the similarity measure is defined between data points has a\ndirect impact on the performance of the SC technique. Several attempts have\nbeen made in the direction of strengthening the pairwise similarity measure to\nenhance the spectral clustering. In this work, we have defined a novel affinity\nmeasure by employing the concept of non-conformity used in Conformal\nPrediction(CP) framework. The non-conformity based affinity captures the\nrelationship between neighborhoods of data points and has the power to\ngeneralize the notion of contextual similarity. We have shown that this\nformulation of affinity measure gives good results and compares well with the\nstate of the art methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 05:09:01 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Chintalapati", "Lalith Srikanth", ""], ["Rachakonda", "Raghunatha Sarma", ""]]}, {"id": "1909.07600", "submitter": "Xiaobo Qu", "authors": "Xinlin Zhang, Hengfa Lu, Di Guo, Lijun Bao, Feng Huang, Qin Xu, Xiaobo\n  Qu", "title": "A Guaranteed Convergence Analysis for the Projected Fast Iterative\n  Soft-Thresholding Algorithm in Parallel MRI", "comments": "Main text: 13 pages, 10 figures. Supporting material: 5 pages, 5\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV math.OC physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The boom of non-uniform sampling and compressed sensing techniques\ndramatically alleviates the lengthy data acquisition problem of magnetic\nresonance imaging. Sparse reconstruction, thanks to its fast computation and\npromising performance, has attracted researchers to put numerous efforts on it\nand has been adopted in commercial scanners. To perform sparse reconstruction,\nchoosing a proper algorithm is essential in providing satisfying results and\nsaving time in tuning parameters. The pFISTA, a simple and efficient algorithm\nfor sparse reconstruction, has been successfully extended to parallel imaging.\nHowever, its convergence criterion is still an open question. And the existing\nconvergence criterion of single-coil pFISTA cannot be applied to the parallel\nimaging pFISTA, which, therefore, imposes confusions and difficulties on users\nabout determining the only parameter - step size. In this work, we provide the\nguaranteed convergence analysis of the parallel imaging version pFISTA to solve\nthe two well-known parallel imaging reconstruction models, SENSE and SPIRiT.\nAlong with the convergence analysis, we provide recommended step size values\nfor SENSE and SPIRiT reconstructions to obtain fast and promising\nreconstructions. Experiments on in vivo brain images demonstrate the validity\nof the convergence criterion. Besides, experimental results show that compared\nto using backtracking and power iteration to determine the step size, our\nrecommended step size achieves more than five times acceleration in\nreconstruction time in most tested cases.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 05:37:29 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 10:17:55 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Zhang", "Xinlin", ""], ["Lu", "Hengfa", ""], ["Guo", "Di", ""], ["Bao", "Lijun", ""], ["Huang", "Feng", ""], ["Xu", "Qin", ""], ["Qu", "Xiaobo", ""]]}, {"id": "1909.07608", "submitter": "Qi Dai", "authors": "Zhi-Qi Cheng, Jun-Xiu Li, Qi Dai, Xiao Wu, Jun-Yan He, Alexander\n  Hauptmann", "title": "Improving the Learning of Multi-column Convolutional Neural Network for\n  Crowd Counting", "comments": "ACM Multimedia 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tremendous variation in the scale of people/head size is a critical problem\nfor crowd counting. To improve the scale invariance of feature representation,\nrecent works extensively employ Convolutional Neural Networks with multi-column\nstructures to handle different scales and resolutions. However, due to the\nsubstantial redundant parameters in columns, existing multi-column networks\ninvariably exhibit almost the same scale features in different columns, which\nseverely affects counting accuracy and leads to overfitting. In this paper, we\nattack this problem by proposing a novel Multi-column Mutual Learning (McML)\nstrategy. It has two main innovations: 1) A statistical network is incorporated\ninto the multi-column framework to estimate the mutual information between\ncolumns, which can approximately indicate the scale correlation between\nfeatures from different columns. By minimizing the mutual information, each\ncolumn is guided to learn features with different image scales. 2) We devise a\nmutual learning scheme that can alternately optimize each column while keeping\nthe other columns fixed on each mini-batch training data. With such\nasynchronous parameter update process, each column is inclined to learn\ndifferent feature representation from others, which can efficiently reduce the\nparameter redundancy and improve generalization ability. More remarkably, McML\ncan be applied to all existing multi-column networks and is end-to-end\ntrainable. Extensive experiments on four challenging benchmarks show that McML\ncan significantly improve the original multi-column networks and outperform the\nother state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 06:34:47 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Cheng", "Zhi-Qi", ""], ["Li", "Jun-Xiu", ""], ["Dai", "Qi", ""], ["Wu", "Xiao", ""], ["He", "Jun-Yan", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1909.07615", "submitter": "Jingjing Li", "authors": "Jingjing Li and Mengmeng Jing and Ke Lu and Lei Zhu and Yang Yang and\n  Zi Huang", "title": "Alleviating Feature Confusion for Generative Zero-shot Learning", "comments": "Our codes can be found at github.com/lijin118/AFC-GAN", "journal-ref": "ACM Multimedia 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lately, generative adversarial networks (GANs) have been successfully applied\nto zero-shot learning (ZSL) and achieved state-of-the-art performance. By\nsynthesizing virtual unseen visual features, GAN-based methods convert the\nchallenging ZSL task into a supervised learning problem. However, GAN-based ZSL\nmethods have to train the generator on the seen categories and further apply it\nto unseen instances. An inevitable issue of such a paradigm is that the\nsynthesized unseen features are prone to seen references and incapable to\nreflect the novelty and diversity of real unseen instances. In a nutshell, the\nsynthesized features are confusing. One cannot tell unseen categories from seen\nones using the synthesized features. As a result, the synthesized features are\ntoo subtle to be classified in generalized zero-shot learning (GZSL) which\ninvolves both seen and unseen categories at the test stage. In this paper, we\nfirst introduce the feature confusion issue. Then, we propose a new feature\ngenerating network, named alleviating feature confusion GAN (AFC-GAN), to\nchallenge the issue. Specifically, we present a boundary loss which maximizes\nthe decision boundary of seen categories and unseen ones. Furthermore, a novel\nmetric named feature confusion score (FCS) is proposed to quantify the feature\nconfusion. Extensive experiments on five widely used datasets verify that our\nmethod is able to outperform previous state-of-the-arts under both ZSL and GZSL\nprotocols.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 06:59:48 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Li", "Jingjing", ""], ["Jing", "Mengmeng", ""], ["Lu", "Ke", ""], ["Zhu", "Lei", ""], ["Yang", "Yang", ""], ["Huang", "Zi", ""]]}, {"id": "1909.07618", "submitter": "Jingjing Li", "authors": "Jingjing Li and Erpeng Chen and Zhengming Ding and Lei Zhu and Ke Lu\n  and Zi Huang", "title": "Cycle-consistent Conditional Adversarial Transfer Networks", "comments": "Codes at github.com/lijin118/3CATN", "journal-ref": "ACM Multimedia 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation investigates the problem of cross-domain knowledge transfer\nwhere the labeled source domain and unlabeled target domain have distinctive\ndata distributions. Recently, adversarial training have been successfully\napplied to domain adaptation and achieved state-of-the-art performance.\nHowever, there is still a fatal weakness existing in current adversarial models\nwhich is raised from the equilibrium challenge of adversarial training.\nSpecifically, although most of existing methods are able to confuse the domain\ndiscriminator, they cannot guarantee that the source domain and target domain\nare sufficiently similar. In this paper, we propose a novel approach named {\\it\ncycle-consistent conditional adversarial transfer networks} (3CATN) to handle\nthis issue. Our approach takes care of the domain alignment by leveraging\nadversarial training. Specifically, we condition the adversarial networks with\nthe cross-covariance of learned features and classifier predictions to capture\nthe multimodal structures of data distributions. However, since the classifier\npredictions are not certainty information, a strong condition with the\npredictions is risky when the predictions are not accurate. We, therefore,\nfurther propose that the truly domain-invariant features should be able to be\ntranslated from one domain to the other. To this end, we introduce two feature\ntranslation losses and one cycle-consistent loss into the conditional\nadversarial domain adaptation networks. Extensive experiments on both classical\nand large-scale datasets verify that our model is able to outperform previous\nstate-of-the-arts with significant improvements.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 07:14:26 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Li", "Jingjing", ""], ["Chen", "Erpeng", ""], ["Ding", "Zhengming", ""], ["Zhu", "Lei", ""], ["Lu", "Ke", ""], ["Huang", "Zi", ""]]}, {"id": "1909.07623", "submitter": "Di Qiu", "authors": "Di Qiu, Jiahao Pang, Wenxiu Sun, Chengxi Yang", "title": "Deep End-to-End Alignment and Refinement for Time-of-Flight RGB-D Module", "comments": "ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it is increasingly popular to equip mobile RGB cameras with\nTime-of-Flight (ToF) sensors for active depth sensing. However, for\noff-the-shelf ToF sensors, one must tackle two problems in order to obtain\nhigh-quality depth with respect to the RGB camera, namely 1) online calibration\nand alignment; and 2) complicated error correction for ToF depth sensing. In\nthis work, we propose a framework for jointly alignment and refinement via deep\nlearning. First, a cross-modal optical flow between the RGB image and the ToF\namplitude image is estimated for alignment. The aligned depth is then refined\nvia an improved kernel predicting network that performs kernel normalization\nand applies the bias prior to the dynamic convolution. To enrich our data for\nend-to-end training, we have also synthesized a dataset using tools from\ncomputer graphics. Experimental results demonstrate the effectiveness of our\napproach, achieving state-of-the-art for ToF refinement.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 07:25:42 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Qiu", "Di", ""], ["Pang", "Jiahao", ""], ["Sun", "Wenxiu", ""], ["Yang", "Chengxi", ""]]}, {"id": "1909.07636", "submitter": "Gil Shomron", "authors": "Gil Shomron, Ron Banner, Moran Shkolnik, Uri Weiser", "title": "Thanks for Nothing: Predicting Zero-Valued Activations with Lightweight\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) introduce state-of-the-art results for\nvarious tasks with the price of high computational demands. Inspired by the\nobservation that spatial correlation exists in CNN output feature maps (ofms),\nwe propose a method to dynamically predict whether ofm activations are\nzero-valued or not according to their neighboring activation values, thereby\navoiding zero-valued activations and reducing the number of convolution\noperations. We implement the zero activation predictor (ZAP) with a lightweight\nCNN, which imposes negligible overheads and is easy to deploy on existing\nmodels. ZAPs are trained by mimicking hidden layer ouputs; thereby, enabling a\nparallel and label-free training. Furthermore, without retraining, each ZAP can\nbe tuned to a different operating point trading accuracy for MAC reduction.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 07:56:54 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 10:43:57 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 13:07:28 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Shomron", "Gil", ""], ["Banner", "Ron", ""], ["Shkolnik", "Moran", ""], ["Weiser", "Uri", ""]]}, {"id": "1909.07667", "submitter": "Andrea Pilzer", "authors": "Andrea Pilzer, St\\'ephane Lathuili\\`ere, Dan Xu, Mihai Marian Puscas,\n  Elisa Ricci, Nicu Sebe", "title": "Progressive Fusion for Unsupervised Binocular Depth Estimation using\n  Cycled Networks", "comments": "Accepted to TPAMI (SI RGB-D Vision), code\n  https://github.com/andrea-pilzer/PFN-depth", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep monocular depth estimation approaches based on supervised\nregression have achieved remarkable performance. However, they require costly\nground truth annotations during training. To cope with this issue, in this\npaper we present a novel unsupervised deep learning approach for predicting\ndepth maps. We introduce a new network architecture, named Progressive Fusion\nNetwork (PFN), that is specifically designed for binocular stereo depth\nestimation. This network is based on a multi-scale refinement strategy that\ncombines the information provided by both stereo views. In addition, we propose\nto stack twice this network in order to form a cycle. This cycle approach can\nbe interpreted as a form of data-augmentation since, at training time, the\nnetwork learns both from the training set images (in the forward half-cycle)\nbut also from the synthesized images (in the backward half-cycle). The\narchitecture is jointly trained with adversarial learning. Extensive\nexperiments on the publicly available datasets KITTI, Cityscapes and\nApolloScape demonstrate the effectiveness of the proposed model which is\ncompetitive with other unsupervised deep learning methods for depth prediction.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 09:21:02 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Pilzer", "Andrea", ""], ["Lathuili\u00e8re", "St\u00e9phane", ""], ["Xu", "Dan", ""], ["Puscas", "Mihai Marian", ""], ["Ricci", "Elisa", ""], ["Sebe", "Nicu", ""]]}, {"id": "1909.07671", "submitter": "Luis Gomez Camara", "authors": "Luis G. Camara and Libor P\\v{r}eu\\v{c}il", "title": "Spatio-Semantic ConvNet-Based Visual Place Recognition", "comments": "Accepted in Proceedings of the 2019 European Conference on Mobile\n  Robots (ECMR 2019), Prague, Czech Republic, September 4-6, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Visual Place Recognition system that follows the two-stage\nformat common to image retrieval pipelines. The system encodes images of places\nby employing the activations of different layers of a pre-trained,\noff-the-shelf, VGG16 Convolutional Neural Network (CNN) architecture. In the\nfirst stage of our method and given a query image of a place, a number of top\ncandidate images is retrieved from a previously stored database of places. In\nthe second stage, we propose an exhaustive comparison of the query image\nagainst these candidates by encoding semantic and spatial information in the\nform of CNN features. Results from our approach outperform by a large margin\nstate-of-the-art visual place recognition methods on five of the most commonly\nused benchmark datasets. The performance gain is especially remarkable on the\nmost challenging datasets, with more than a twofold recognition improvement\nwith respect to the latest published work.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 09:30:24 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Camara", "Luis G.", ""], ["P\u0159eu\u010dil", "Libor", ""]]}, {"id": "1909.07685", "submitter": "Allan Gr{\\o}nlund", "authors": "Lars Arge, Allan Gr{\\o}nlund, Svend Christian Svendsen, Jonas Tranberg", "title": "Learning to Find Hydrological Corrections", "comments": "27th ACM SIGSPATIAL International Conference on Advances in\n  Geographic Information Systems (ACM SIGSPATIAL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High resolution Digital Elevation models, such as the (Big) grid terrain\nmodel of Denmark with more than 200 billion measurements, is a basic\nrequirement for water flow modelling and flood risk analysis. However, a large\nnumber of modifications often need to be made to even very accurate terrain\nmodels, such as the Danish model, before they can be used in realistic flow\nmodeling. These modifications include removal of bridges, which otherwise will\nact as dams in flow modeling, and inclusion of culverts that transport water\nunderneath roads. In fact, the danish model is accompanied by a detailed set of\nhydrological corrections for the digital elevation model. However, producing\nthese hydrological corrections is a very slow an expensive process, since it is\nto a large extent done manually and often with local input. This also means\nthat corrections can be of varying quality. In this paper we propose a new\nalgorithmic apporach based on machine learning and convolutional neural\nnetworks for automatically detecting hydrological corrections for such large\nterrain data. Our model is able to detect most hydrological corrections known\nfor the danish model and quite a few more that should have been included in the\noriginal list.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 09:54:38 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Arge", "Lars", ""], ["Gr\u00f8nlund", "Allan", ""], ["Svendsen", "Svend Christian", ""], ["Tranberg", "Jonas", ""]]}, {"id": "1909.07697", "submitter": "Naif Alshammari PhD", "authors": "Naif Alshammari, Samet Ak\\c{c}ay, Toby P. Breckon", "title": "Multi-Task Learning for Automotive Foggy Scene Understanding via Domain\n  Adaptation to an Illumination-Invariant Representation", "comments": "Conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint scene understanding and segmentation for automotive applications is a\nchallenging problem in two key aspects:- (1) classifying every pixel in the\nentire scene and (2) performing this task under unstable weather and\nillumination changes (e.g. foggy weather), which results in poor outdoor scene\nvisibility. This poor outdoor scene visibility leads to a non-optimal\nperformance of deep convolutional neural network-based scene understanding and\nsegmentation. In this paper, we propose an efficient end-to-end contemporary\nautomotive semantic scene understanding approach under foggy weather\nconditions, employing domain adaptation and illumination-invariant image\nper-transformation. As a multi-task pipeline, our proposed model provides:- (1)\ntransferring images from extreme to clear-weather condition using domain\ntransfer approach and (2) semantically segmenting a scene using a competitive\nencoder-decoder convolutional neural network (CNN) with dense connectivity,\nskip connections and fusion-based techniques. We evaluate our approach on\nchallenging foggy datasets, including synthetic dataset (Foggy Cityscapes) as\nwell as real-world datasets (Foggy Zurich and Foggy Driving). By incorporating\nRGB, depth, and illumination-invariant information, our approach outperforms\nthe state-of-the-art within automotive scene understanding, under foggy weather\ncondition.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 10:18:14 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Alshammari", "Naif", ""], ["Ak\u00e7ay", "Samet", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1909.07701", "submitter": "Xinlong Wang", "authors": "Xinlong Wang, Wei Yin, Tao Kong, Yuning Jiang, Lei Li, Chunhua Shen", "title": "Task-Aware Monocular Depth Estimation for 3D Object Detection", "comments": "Accepted by AAAI2020 as Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth estimation enables 3D perception from a single 2D image, thus\nattracting much research attention for years. Almost all methods treat\nforeground and background regions (\"things and stuff\") in an image equally.\nHowever, not all pixels are equal. Depth of foreground objects plays a crucial\nrole in 3D object recognition and localization. To date how to boost the depth\nprediction accuracy of foreground objects is rarely discussed. In this paper,\nwe first analyse the data distributions and interaction of foreground and\nbackground, then propose the foreground-background separated monocular depth\nestimation (ForeSeE) method, to estimate the foreground depth and background\ndepth using separate optimization objectives and depth decoders. Our method\nsignificantly improves the depth estimation performance on foreground objects.\nApplying ForeSeE to 3D object detection, we achieve 7.5 AP gains and set new\nstate-of-the-art results among other monocular methods. Code will be available\nat: https://github.com/WXinlong/ForeSeE.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 10:24:17 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 10:15:56 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Wang", "Xinlong", ""], ["Yin", "Wei", ""], ["Kong", "Tao", ""], ["Jiang", "Yuning", ""], ["Li", "Lei", ""], ["Shen", "Chunhua", ""]]}, {"id": "1909.07704", "submitter": "Vaibhav Bansal", "authors": "Vaibhav Bansal, Stuart James and Alessio Del Bue", "title": "re-OBJ: Jointly Learning the Foreground and Background for Object\n  Instance Re-identification", "comments": "Accepted to ICIAP 2019 and awarded the Best Student Paper", "journal-ref": null, "doi": "10.1007/978-3-030-30645-8_37", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional approaches to object instance re-identification rely on matching\nappearances of the target objects among a set of frames. However, learning\nappearances of the objects alone might fail when there are multiple objects\nwith similar appearance or multiple instances of same object class present in\nthe scene. This paper proposes that partial observations of the background can\nbe utilized to aid in the object re-identification task for a rigid scene,\nespecially a rigid environment with a lot of reoccurring identical models of\nobjects. Using an extension to the Mask R-CNN architecture, we learn to encode\nthe important and distinct information in the background jointly with the\nforeground relevant to rigid real-world scenarios such as an indoor environment\nwhere objects are static and the camera moves around the scene. We demonstrate\nthe effectiveness of our joint visual feature in the re-identification of\nobjects in the ScanNet dataset and show a relative improvement of around 28.25%\nin the rank-1 accuracy over the deepSort method.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 10:36:12 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 17:21:55 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Bansal", "Vaibhav", ""], ["James", "Stuart", ""], ["Del Bue", "Alessio", ""]]}, {"id": "1909.07721", "submitter": "Kailun Yang", "authors": "Kailun Yang, Xinxin Hu, Hao Chen, Kaite Xiang, Kaiwei Wang and Rainer\n  Stiefelhagen", "title": "DS-PASS: Detail-Sensitive Panoramic Annular Semantic Segmentation\n  through SwaftNet for Surrounding Sensing", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantically interpreting the traffic scene is crucial for autonomous\ntransportation and robotics systems. However, state-of-the-art semantic\nsegmentation pipelines are dominantly designed to work with pinhole cameras and\ntrain with narrow Field-of-View (FoV) images. In this sense, the perception\ncapacity is severely limited to offer higher-level confidence for upstream\nnavigation tasks. In this paper, we propose a network adaptation framework to\nachieve Panoramic Annular Semantic Segmentation (PASS), which allows to re-use\nconventional pinhole-view image datasets, enabling modern segmentation networks\nto comfortably adapt to panoramic images. Specifically, we adapt our proposed\nSwaftNet to enhance the sensitivity to details by implementing attention-based\nlateral connections between the detail-critical encoder layers and the\ncontext-critical decoder layers. We benchmark the performance of efficient\nsegmenters on panoramic segmentation with our extended PASS dataset,\ndemonstrating that the proposed real-time SwaftNet outperforms state-of-the-art\nefficient networks. Furthermore, we assess real-world performance when\ndeploying the Detail-Sensitive PASS (DS-PASS) system on a mobile robot and an\ninstrumented vehicle, as well as the benefit of panoramic semantics for visual\nodometry, showing the robustness and potential to support diverse navigational\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 11:20:54 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 16:09:54 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Yang", "Kailun", ""], ["Hu", "Xinxin", ""], ["Chen", "Hao", ""], ["Xiang", "Kaite", ""], ["Wang", "Kaiwei", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1909.07725", "submitter": "Tao Kong", "authors": "Luxuan Li, Tao Kong, Fuchun Sun, Huaping Liu", "title": "Deep Point-wise Prediction for Action Temporal Proposal", "comments": "accepted by ICONIP2019 oral presentation (International Conference on\n  Neural Information Processing)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting actions in videos is an important yet challenging task. Previous\nworks usually utilize (a) sliding window paradigms, or (b) per-frame action\nscoring and grouping to enumerate the possible temporal locations. Their\nperformances are also limited to the designs of sliding windows or grouping\nstrategies. In this paper, we present a simple and effective method for\ntemporal action proposal generation, named Deep Point-wise Prediction (DPP).\nDPP simultaneously predicts the action existing possibility and the\ncorresponding temporal locations, without the utilization of any handcrafted\nsliding window or grouping. The whole system is end-to-end trained with joint\nloss of temporal action proposal classification and location prediction. We\nconduct extensive experiments to verify its effectiveness, generality and\nrobustness on standard THUMOS14 dataset. DPP runs more than 1000 frames per\nsecond, which largely satisfies the real-time requirement. The code is\navailable at https://github.com/liluxuan1997/DPP.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 11:30:52 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Li", "Luxuan", ""], ["Kong", "Tao", ""], ["Sun", "Fuchun", ""], ["Liu", "Huaping", ""]]}, {"id": "1909.07726", "submitter": "Chao Pang", "authors": "Yi Liu, Chao Pang, Zongqian Zhan, Xiaomeng Zhang, and Xue Yang", "title": "Building Change Detection for Remote Sensing Images Using a Dual Task\n  Constrained Deep Siamese Convolutional Network Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, building change detection methods have made great progress\nby introducing deep learning, but they still suffer from the problem of the\nextracted features not being discriminative enough, resulting in incomplete\nregions and irregular boundaries. To tackle this problem, we propose a dual\ntask constrained deep Siamese convolutional network (DTCDSCN) model, which\ncontains three sub-networks: a change detection network and two semantic\nsegmentation networks. DTCDSCN can accomplish both change detection and\nsemantic segmentation at the same time, which can help to learn more\ndiscriminative object-level features and obtain a complete change detection\nmap. Furthermore, we introduce a dual attention module (DAM) to exploit the\ninterdependencies between channels and spatial positions, which improves the\nfeature representation. We also improve the focal loss function to suppress the\nsample imbalance problem. The experimental results obtained with the WHU\nbuilding dataset show that the proposed method is effective for building change\ndetection and achieves a state-of-the-art performance in terms of four metrics:\nprecision, recall, F1-score, and intersection over union.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 11:34:19 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Liu", "Yi", ""], ["Pang", "Chao", ""], ["Zhan", "Zongqian", ""], ["Zhang", "Xiaomeng", ""], ["Yang", "Xue", ""]]}, {"id": "1909.07727", "submitter": "Jingshu Liu", "authors": "Jingshu Liu, Yuan Li", "title": "An Image Based Visual Servo Approach with Deep Learning for Robotic\n  Manipulation", "comments": "Accepted by The 6th International Workshop on Advanced Computational\n  Intelligence and Intelligent Informatics (IWACIII2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at the difficulty of extracting image features and estimating the\nJacobian matrix in image based visual servo, this paper proposes an image based\nvisual servo approach with deep learning. With the powerful learning\ncapabilities of convolutional neural networks(CNN), autonomous learning to\nextract features from images and fitting the nonlinear relationships from image\nspace to task space is achieved, which can greatly facilitate the image based\nvisual servo procedure. Based on the above ideas a two-stream network based on\nconvolutional neural network is designed and the corresponding control scheme\nis proposed to realize the four degrees of freedom visual servo of the robot\nmanipulator. Collecting images of observed target under different pose\nparameters of the manipulator as training samples for CNN, the trained network\ncan be used to estimate the nonlinear relationship from 2D image space to 3D\nCartesian space. The two-stream network takes the current image and the\ndesirable image as inputs and makes them equal to guide the manipulator to the\ndesirable pose. The effectiveness of the approach is verified with experimental\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 11:38:09 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Liu", "Jingshu", ""], ["Li", "Yuan", ""]]}, {"id": "1909.07741", "submitter": "Yipeng Sun", "authors": "Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Canjie Luo, Chun\n  Chet Ng, Junyu Han, Errui Ding, Jingtuo Liu, Dimosthenis Karatzas, Chee Seng\n  Chan, Lianwen Jin", "title": "ICDAR 2019 Competition on Large-scale Street View Text with Partial\n  Labeling -- RRC-LSVT", "comments": "ICDAR 2019 Robust Reading Challenge in IAPR International Conference\n  on Document Analysis and Recognition (ICDAR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust text reading from street view images provides valuable information for\nvarious applications. Performance improvement of existing methods in such a\nchallenging scenario heavily relies on the amount of fully annotated training\ndata, which is costly and in-efficient to obtain. To scale up the amount of\ntraining data while keeping the labeling procedure cost-effective, this\ncompetition introduces a new challenge on Large-scale Street View Text with\nPartial Labeling (LSVT), providing 50, 000 and 400, 000 images in full and weak\nannotations, respectively. This competition aims to explore the abilities of\nstate-of-the-art methods to detect and recognize text instances from\nlarge-scale street view images, closing the gap between research benchmarks and\nreal applications. During the competition period, a total of 41 teams\nparticipated in the two proposed tasks with 132 valid submissions, i.e., text\ndetection and end-to-end text spotting. This paper includes dataset\ndescriptions, task definitions, evaluation protocols and results summaries of\nthe ICDAR 2019-LSVT challenge.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 12:09:33 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Sun", "Yipeng", ""], ["Ni", "Zihan", ""], ["Chng", "Chee-Kheng", ""], ["Liu", "Yuliang", ""], ["Luo", "Canjie", ""], ["Ng", "Chun Chet", ""], ["Han", "Junyu", ""], ["Ding", "Errui", ""], ["Liu", "Jingtuo", ""], ["Karatzas", "Dimosthenis", ""], ["Chan", "Chee Seng", ""], ["Jin", "Lianwen", ""]]}, {"id": "1909.07745", "submitter": "Ali Ghadirzadeh", "authors": "Xi Chen, Ali Ghadirzadeh, M{\\aa}rten Bj\\\"orkman and Patric Jensfelt", "title": "Adversarial Feature Training for Generalizable Robotic Visuomotor\n  Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (RL) has enabled training action-selection\npolicies, end-to-end, by learning a function which maps image pixels to action\noutputs. However, it's application to visuomotor robotic policy training has\nbeen limited because of the challenge of large-scale data collection when\nworking with physical hardware. A suitable visuomotor policy should perform\nwell not just for the task-setup it has been trained for, but also for all\nvarieties of the task, including novel objects at different viewpoints\nsurrounded by task-irrelevant objects. However, it is impractical for a robotic\nsetup to sufficiently collect interactive samples in a RL framework to\ngeneralize well to novel aspects of a task. In this work, we demonstrate that\nby using adversarial training for domain transfer, it is possible to train\nvisuomotor policies based on RL frameworks, and then transfer the acquired\npolicy to other novel task domains. We propose to leverage the deep RL\ncapabilities to learn complex visuomotor skills for uncomplicated task setups,\nand then exploit transfer learning to generalize to new task domains provided\nonly still images of the task in the target domain. We evaluate our method on\ntwo real robotic tasks, picking and pouring, and compare it to a number of\nprior works, demonstrating its superiority.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 12:18:34 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Chen", "Xi", ""], ["Ghadirzadeh", "Ali", ""], ["Bj\u00f6rkman", "M\u00e5rten", ""], ["Jensfelt", "Patric", ""]]}, {"id": "1909.07763", "submitter": "Guillaume Labb\\'e-Morissette", "authors": "Guillaume Labbe-Morissette, Sylvain Gauthier", "title": "A machine vision meta-algorithm for automated recognition of underwater\n  objects using sidescan sonar imagery", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper details a new method to recognize and detect underwater objects in\nreal-time sidescan sonar data imagery streams, with case-studies of\napplications for underwater archeology, and ghost fishing gear retrieval. We\nfirst synthesize images from sidescan data, apply geometric and radiometric\ncorrections, then use 2D feature detection algorithms to identify point clouds\nof descriptive visual microfeatures such as corners and edges in the sonar\nimages. We then apply a clustering algorithm on the feature point clouds to\ngroup feature sets into regions of interest, reject false positives, yielding a\ngeoreferenced inventory of objects.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 13:15:11 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Labbe-Morissette", "Guillaume", ""], ["Gauthier", "Sylvain", ""]]}, {"id": "1909.07766", "submitter": "Hieu Nguyen", "authors": "Hieu Nguyen, Hui Li, Qiang Qiu, Yuzeng Wang, and Zhaoyang Wang", "title": "Single-shot 3D shape reconstruction using deep convolutional neural\n  networks", "comments": "6 pages, 4 figures, 1 dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust single-shot 3D shape reconstruction technique integrating the fringe\nprojection profilometry (FPP) technique with the deep convolutional neural\nnetworks (CNNs) is proposed in this letter. The input of the proposed technique\nis a single FPP image, and the training and validation data sets are prepared\nby using the conventional multi-frequency FPP technique. Unlike the\nconventional 3D shape reconstruction methods which involve complex algorithms\nand intensive computation, the proposed approach uses an end-to-end network\narchitecture to directly carry out the transformation of a 2D images to its\ncorresponding 3D shape. Experiments have been conducted to demonstrate the\nvalidity and robustness of the proposed technique. It is capable of satisfying\nvarious 3D shape reconstruction demands in scientific research and engineering\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 13:19:31 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Nguyen", "Hieu", ""], ["Li", "Hui", ""], ["Qiu", "Qiang", ""], ["Wang", "Yuzeng", ""], ["Wang", "Zhaoyang", ""]]}, {"id": "1909.07808", "submitter": "Yipeng Sun", "authors": "Yipeng Sun, Jiaming Liu, Wei Liu, Junyu Han, Errui Ding, Jingtuo Liu", "title": "Chinese Street View Text: Large-scale Chinese Text Reading with\n  Partially Supervised Learning", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing text reading benchmarks make it difficult to evaluate the\nperformance of more advanced deep learning models in large vocabularies due to\nthe limited amount of training data. To address this issue, we introduce a new\nlarge-scale text reading benchmark dataset named Chinese Street View Text\n(C-SVT) with 430,000 street view images, which is at least 14 times as large as\nthe existing Chinese text reading benchmarks. To recognize Chinese text in the\nwild while keeping large-scale datasets labeling cost-effective, we propose to\nannotate one part of the CSVT dataset (30,000 images) in locations and text\nlabels as full annotations and add 400,000 more images, where only the\ncorresponding text-of-interest in the regions is given as weak annotations. To\nexploit the rich information from the weakly annotated data, we design a text\nreading network in a partially supervised learning framework, which enables to\nlocalize and recognize text, learn from fully and weakly annotated data\nsimultaneously. To localize the best matched text proposals from weakly labeled\nimages, we propose an online proposal matching module incorporated in the whole\nmodel, spotting the keyword regions by sharing parameters for end-to-end\ntraining. Compared with fully supervised training algorithms, this model can\nimprove the end-to-end recognition performance remarkably by 4.03% in F-score\nat the same labeling cost. The proposed model can also achieve state-of-the-art\nresults on the ICDAR 2017-RCTW dataset, which demonstrates the effectiveness of\nthe proposed partially supervised learning framework.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 13:54:24 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 04:50:38 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Sun", "Yipeng", ""], ["Liu", "Jiaming", ""], ["Liu", "Wei", ""], ["Han", "Junyu", ""], ["Ding", "Errui", ""], ["Liu", "Jingtuo", ""]]}, {"id": "1909.07809", "submitter": "Shadi Albarqouni Ph.D.", "authors": "Abhijeet Parida, Arianne Tran, Nassir Navab, Shadi Albarqouni", "title": "Learn to Segment Organs with a Few Bounding Boxes", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is an import task in the medical field to identify the\nexact extent and orientation of significant structures like organs and\npathology. Deep neural networks can perform this task well by leveraging the\ninformation from a large well-labeled data-set. This paper aims to present a\nmethod that mitigates the necessity of an extensive well-labeled data-set. This\nmethod also addresses semi-supervision by enabling segmentation based on\nbounding box annotations, avoiding the need for full pixel-level annotations.\nThe network presented consists of a single U-Net based unbranched architecture\nthat generates a few-shot segmentation for an unseen human organ using just 4\nexample annotations of that specific organ. The network is trained by\nalternately minimizing the nearest neighbor loss for prototype learning and a\nweighted cross-entropy loss for segmentation learning to perform a fast 3D\nsegmentation with a median score of 54.64%.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 13:56:37 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Parida", "Abhijeet", ""], ["Tran", "Arianne", ""], ["Navab", "Nassir", ""], ["Albarqouni", "Shadi", ""]]}, {"id": "1909.07818", "submitter": "Lasse Hansen", "authors": "Lasse Hansen, Doris Dittmer, Mattias P. Heinrich", "title": "Learning Deformable Point Set Registration with Regularized Dynamic\n  Graph CNNs for Large Lung Motion in COPD Patients", "comments": "accepted for MICCAI 2019 Workshop Graph Learning in Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable registration continues to be one of the key challenges in medical\nimage analysis. While iconic registration methods have started to benefit from\nthe recent advances in medical deep learning, the same does not yet apply for\nthe registration of point sets, e.g. registration based on surfaces, keypoints\nor landmarks. This is mainly due to the restriction of the convolution operator\nin modern CNNs to densely gridded input. However, with the newly developed\nmethods from the field of geometric deep learning suitable tools are now\nemerging, which enable powerful analysis of medical data on irregular domains.\nIn this work, we present a new method that enables the learning of regularized\nfeature descriptors with dynamic graph CNNs. By incorporating the learned\ngeometric features as prior probabilities into the well-established coherent\npoint drift (CPD) algorithm, formulated as differentiable network layer, we\nestablish an end-to-end framework for robust registration of two point sets.\nOur approach is evaluated on the challenging task of aligning keypoints\nextracted from lung CT scans in inhale and exhale states with large\ndeformations and without any additional intensity information. Our results\nindicate that the inherent geometric structure of the extracted keypoints is\nsufficient to establish descriptive point features, which yield a significantly\nimproved performance and robustness of our registration framework.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 13:59:04 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Hansen", "Lasse", ""], ["Dittmer", "Doris", ""], ["Heinrich", "Mattias P.", ""]]}, {"id": "1909.07827", "submitter": "Guoqiang Zhong", "authors": "Qingyang Li, Guoqiang Zhong, Cui Xie", "title": "Weak Edge Identification Nets for Ocean Front Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ocean front has an important impact in many areas, it is meaningful to\nobtain accurate ocean front positioning, therefore, ocean front detection is a\nvery important task. However, the traditional edge detection algorithm does not\ndetect the weak edge information of the ocean front very well. In response to\nthis problem, we collected relevant ocean front gradient images and found\nrelevant experts to calibrate the ocean front data to obtain groundtruth, and\nproposed a weak edge identification nets(WEIN) for ocean front detection.\nWhether it is qualitative or quantitative, our methods perform best. The method\nuses a welltrained deep learning model to accurately extract the ocean front\nfrom the ocean front gradient image. The detection network is divided into\nmultiple stages, and the final output is a multi-stage output image fusion. The\nmethod uses the stochastic gradient descent and the correlation loss function\nto obtain a good ocean front image output.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 14:10:45 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Li", "Qingyang", ""], ["Zhong", "Guoqiang", ""], ["Xie", "Cui", ""]]}, {"id": "1909.07829", "submitter": "Konstantin Sofiiuk", "authors": "Konstantin Sofiiuk, Olga Barinova and Anton Konushin", "title": "AdaptIS: Adaptive Instance Selection Network", "comments": "Accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Adaptive Instance Selection network architecture for\nclass-agnostic instance segmentation. Given an input image and a point $(x,\ny)$, it generates a mask for the object located at $(x, y)$. The network adapts\nto the input point with a help of AdaIN layers, thus producing different masks\nfor different objects on the same image. AdaptIS generates pixel-accurate\nobject masks, therefore it accurately segments objects of complex shape or\nseverely occluded ones. AdaptIS can be easily combined with standard semantic\nsegmentation pipeline to perform panoptic segmentation. To illustrate the idea,\nwe perform experiments on a challenging toy problem with difficult occlusions.\nThen we extensively evaluate the method on panoptic segmentation benchmarks. We\nobtain state-of-the-art results on Cityscapes and Mapillary even without\npretraining on COCO, and show competitive results on a challenging COCO\ndataset. The source code of the method and the trained models are available at\nhttps://github.com/saic-vul/adaptis.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 14:11:42 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Sofiiuk", "Konstantin", ""], ["Barinova", "Olga", ""], ["Konushin", "Anton", ""]]}, {"id": "1909.07830", "submitter": "Chee Seng Chan", "authors": "Lixin Fan, Kam Woh Ng, Chee Seng Chan", "title": "[Extended version] Rethinking Deep Neural Network Ownership\n  Verification: Embedding Passports to Defeat Ambiguity Attacks", "comments": "This paper is accepted by NeurIPS 2019; Our code is available at\n  https://github.com/kamwoh/DeepIPR. This is the extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With substantial amount of time, resources and human (team) efforts invested\nto explore and develop successful deep neural networks (DNN), there emerges an\nurgent need to protect these inventions from being illegally copied,\nredistributed, or abused without respecting the intellectual properties of\nlegitimate owners. Following recent progresses along this line, we investigate\na number of watermark-based DNN ownership verification methods in the face of\nambiguity attacks, which aim to cast doubts on the ownership verification by\nforging counterfeit watermarks. It is shown that ambiguity attacks pose serious\nthreats to existing DNN watermarking methods. As remedies to the\nabove-mentioned loophole, this paper proposes novel passport-based DNN\nownership verification schemes which are both robust to network modifications\nand resilient to ambiguity attacks. The gist of embedding digital passports is\nto design and train DNN models in a way such that, the DNN inference\nperformance of an original task will be significantly deteriorated due to\nforged passports. In other words, genuine passports are not only verified by\nlooking for the predefined signatures, but also reasserted by the unyielding\nDNN model inference performances. Extensive experimental results justify the\neffectiveness of the proposed passport-based DNN ownership verification\nschemes. Code and models are available at https://github.com/kamwoh/DeepIPR\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 13:48:08 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 10:28:38 GMT"}, {"version": "v3", "created": "Sat, 2 Nov 2019 16:03:32 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Fan", "Lixin", ""], ["Ng", "Kam Woh", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1909.07846", "submitter": "Wei-Hung Weng", "authors": "Wei-Hung Weng, Yuannan Cai, Angela Lin, Fraser Tan, Po-Hsuan Cameron\n  Chen", "title": "Multimodal Multitask Representation Learning for Pathology Biobank\n  Metadata Prediction", "comments": "preprint version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metadata are general characteristics of the data in a well-curated and\ncondensed format, and have been proven to be useful for decision making,\nknowledge discovery, and also heterogeneous data organization of biobank. Among\nall data types in the biobank, pathology is the key component of the biobank\nand also serves as the gold standard of diagnosis. To maximize the utility of\nbiobank and allow the rapid progress of biomedical science, it is essential to\norganize the data with well-populated pathology metadata. However, manual\nannotation of such information is tedious and time-consuming. In the study, we\ndevelop a multimodal multitask learning framework to predict four major\nslide-level metadata of pathology images. The framework learns generalizable\nrepresentations across tissue slides, pathology reports, and case-level\nstructured data. We demonstrate improved performance across all four tasks with\nthe proposed method compared to a single modal single task baseline on two test\nsets, one external test set from a distinct data source (TCGA) and one internal\nheld-out test set (TTH). In the test sets, the performance improvements on the\naveraged area under receiver operating characteristic curve across the four\ntasks are 16.48% and 9.05% on TCGA and TTH, respectively. Such pathology\nmetadata prediction system may be adopted to mitigate the effort of expert\nannotation and ultimately accelerate the data-driven research by better\nutilization of the pathology biobank.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 14:34:37 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Weng", "Wei-Hung", ""], ["Cai", "Yuannan", ""], ["Lin", "Angela", ""], ["Tan", "Fraser", ""], ["Chen", "Po-Hsuan Cameron", ""]]}, {"id": "1909.07863", "submitter": "Aditya Kaushik Surikuchi", "authors": "Aditya Surikuchi, Jorma Laaksonen", "title": "Character-Centric Storytelling", "comments": "ACL Storytelling (StoryNLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential vision-to-language or visual storytelling has recently been one of\nthe areas of focus in computer vision and language modeling domains. Though\nexisting models generate narratives that read subjectively well, there could be\ncases when these models miss out on generating stories that account and address\nall prospective human and animal characters in the image sequences. Considering\nthis scenario, we propose a model that implicitly learns relationships between\nprovided characters and thereby generates stories with respective characters in\nscope. We use the VIST dataset for this purpose and report numerous statistics\non the dataset. Eventually, we describe the model, explain the experiment and\ndiscuss our current status and future work.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 14:51:59 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 13:12:52 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 10:11:42 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Surikuchi", "Aditya", ""], ["Laaksonen", "Jorma", ""]]}, {"id": "1909.07867", "submitter": "Braden Hurl", "authors": "Braden Hurl, Robin Cohen, Krzysztof Czarnecki, Steven Waslander", "title": "TruPercept: Trust Modelling for Autonomous Vehicle Cooperative\n  Perception from Synthetic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter-vehicle communication for autonomous vehicles (AVs) stands to provide\nsignificant benefits in terms of perception robustness. We propose a novel\napproach for AVs to communicate perceptual observations, tempered by trust\nmodelling of peers providing reports. Based on the accuracy of reported object\ndetections as verified locally, communicated messages can be fused to augment\nperception performance beyond line of sight and at great distance from the ego\nvehicle. Also presented is a new synthetic dataset which can be used to test\ncooperative perception. The TruPercept dataset includes unreliable and\nmalicious behaviour scenarios to experiment with some challenges cooperative\nperception introduces. The TruPercept runtime and evaluation framework allows\nmodular component replacement to facilitate ablation studies as well as the\ncreation of new trust scenarios we are able to show.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 14:57:45 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Hurl", "Braden", ""], ["Cohen", "Robin", ""], ["Czarnecki", "Krzysztof", ""], ["Waslander", "Steven", ""]]}, {"id": "1909.07877", "submitter": "Xiaoming Yu", "authors": "Xiaoming Yu, Yuanqi Chen, Thomas Li, Shan Liu, Ge Li", "title": "Multi-mapping Image-to-Image Translation via Learning Disentanglement", "comments": "Accepted by NeurIPS 2019. Code will be available at\n  https://github.com/Xiaoming-Yu/DMIT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances of image-to-image translation focus on learning the\none-to-many mapping from two aspects: multi-modal translation and multi-domain\ntranslation. However, the existing methods only consider one of the two\nperspectives, which makes them unable to solve each other's problem. To address\nthis issue, we propose a novel unified model, which bridges these two\nobjectives. First, we disentangle the input images into the latent\nrepresentations by an encoder-decoder architecture with a conditional\nadversarial training in the feature space. Then, we encourage the generator to\nlearn multi-mappings by a random cross-domain translation. As a result, we can\nmanipulate different parts of the latent representations to perform multi-modal\nand multi-domain translations simultaneously. Experiments demonstrate that our\nmethod outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 15:10:47 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 13:01:10 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Yu", "Xiaoming", ""], ["Chen", "Yuanqi", ""], ["Li", "Thomas", ""], ["Liu", "Shan", ""], ["Li", "Ge", ""]]}, {"id": "1909.07930", "submitter": "Piero Molino", "authors": "Piero Molino, Yaroslav Dudin, Sai Sumanth Miryala", "title": "Ludwig: a type-based declarative deep learning toolbox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present Ludwig, a flexible, extensible and easy to use\ntoolbox which allows users to train deep learning models and use them for\nobtaining predictions without writing code. Ludwig implements a novel approach\nto deep learning model building based on two main abstractions: data types and\ndeclarative configuration files. The data type abstraction allows for easier\ncode and sub-model reuse, and the standardized interfaces imposed by this\nabstraction allow for encapsulation and make the code easy to extend.\nDeclarative model definition configuration files enable inexperienced users to\nobtain effective models and increase the productivity of expert users.\nAlongside these two innovations, Ludwig introduces a general modularized deep\nlearning architecture called Encoder-Combiner-Decoder that can be instantiated\nto perform a vast amount of machine learning tasks. These innovations make it\npossible for engineers, scientists from other fields and, in general, a much\nbroader audience to adopt deep learning models for their tasks, concretely\nhelping in its democratization.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 16:54:29 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Molino", "Piero", ""], ["Dudin", "Yaroslav", ""], ["Miryala", "Sai Sumanth", ""]]}, {"id": "1909.07945", "submitter": "Sai Kumar Dwivedi", "authors": "Sai Kumar Dwivedi, Vikram Gupta, Rahul Mitra, Shuaib Ahmed, Arjun Jain", "title": "ProtoGAN: Towards Few Shot Learning for Action Recognition", "comments": "9 pages, 5 tables, 2 figures. To appear in the proceedings of ICCV\n  Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning (FSL) for action recognition is a challenging task of\nrecognizing novel action categories which are represented by few instances in\nthe training data. In a more generalized FSL setting (G-FSL), both seen as well\nas novel action categories need to be recognized. Conventional classifiers\nsuffer due to inadequate data in FSL setting and inherent bias towards seen\naction categories in G-FSL setting. In this paper, we address this problem by\nproposing a novel ProtoGAN framework which synthesizes additional examples for\nnovel categories by conditioning a conditional generative adversarial network\nwith class prototype vectors. These class prototype vectors are learnt using a\nClass Prototype Transfer Network (CPTN) from examples of seen categories. Our\nsynthesized examples for a novel class are semantically similar to real\nexamples belonging to that class and is used to train a model exhibiting better\ngeneralization towards novel classes. We support our claim by performing\nextensive experiments on three datasets: UCF101, HMDB51 and Olympic-Sports. To\nthe best of our knowledge, we are the first to report the results for G-FSL and\nprovide a strong benchmark for future research. We also outperform the\nstate-of-the-art method in FSL for all the aforementioned datasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 17:28:20 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Dwivedi", "Sai Kumar", ""], ["Gupta", "Vikram", ""], ["Mitra", "Rahul", ""], ["Ahmed", "Shuaib", ""], ["Jain", "Arjun", ""]]}, {"id": "1909.07950", "submitter": "Ahmed Sabir", "authors": "Ahmed Sabir, Francesc Moreno-Noguer and Llu\\'is Padr\\'o", "title": "Semantic Relatedness Based Re-ranker for Text Spotting", "comments": "Accepted by EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications such as textual entailment, plagiarism detection or document\nclustering rely on the notion of semantic similarity, and are usually\napproached with dimension reduction techniques like LDA or with embedding-based\nneural approaches. We present a scenario where semantic similarity is not\nenough, and we devise a neural approach to learn semantic relatedness. The\nscenario is text spotting in the wild, where a text in an image (e.g. street\nsign, advertisement or bus destination) must be identified and recognized. Our\ngoal is to improve the performance of vision systems by leveraging semantic\ninformation. Our rationale is that the text to be spotted is often related to\nthe image context in which it appears (word pairs such as Delta-airplane, or\nquarters-parking are not similar, but are clearly related). We show how\nlearning a word-to-word or word-to-sentence relatedness score can improve the\nperformance of text spotting systems up to 2.9 points, outperforming other\nmeasures in a benchmark dataset.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 17:31:37 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 15:29:27 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Sabir", "Ahmed", ""], ["Moreno-Noguer", "Francesc", ""], ["Padr\u00f3", "Llu\u00eds", ""]]}, {"id": "1909.07957", "submitter": "Haotian Zhang", "authors": "Haotian Zhang, Long Mai, Ning Xu, Zhaowen Wang, John Collomosse,\n  Hailin Jin", "title": "An Internal Learning Approach to Video Inpainting", "comments": "Accepted by ICCV 2019. Website:\n  https://cs.stanford.edu/~haotianz/publications/video_inpainting/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel video inpainting algorithm that simultaneously\nhallucinates missing appearance and motion (optical flow) information, building\nupon the recent 'Deep Image Prior' (DIP) that exploits convolutional network\narchitectures to enforce plausible texture in static images. In extending DIP\nto video we make two important contributions. First, we show that coherent\nvideo inpainting is possible without a priori training. We take a generative\napproach to inpainting based on internal (within-video) learning without\nreliance upon an external corpus of visual data to train a one-size-fits-all\nmodel for the large space of general videos. Second, we show that such a\nframework can jointly generate both appearance and flow, whilst exploiting\nthese complementary modalities to ensure mutual consistency. We show that\nleveraging appearance statistics specific to each video achieves visually\nplausible results whilst handling the challenging problem of long-term\nconsistency.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 17:47:53 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Zhang", "Haotian", ""], ["Mai", "Long", ""], ["Xu", "Ning", ""], ["Wang", "Zhaowen", ""], ["Collomosse", "John", ""], ["Jin", "Hailin", ""]]}, {"id": "1909.07973", "submitter": "Xiaoyu Yu", "authors": "Xiaoyu Yu, Yuwei Wang, Jie Miao, Ephrem Wu, Heng Zhang, Yu Meng, Bo\n  Zhang, Biao Min, Dewei Chen, Jianlin Gao", "title": "A Data-Center FPGA Acceleration Platform for Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1109/FPL.2019.00032", "report-no": null, "categories": "cs.CV cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intensive computation is entering data centers with multiple workloads of\ndeep learning. To balance the compute efficiency, performance, and total cost\nof ownership (TCO), the use of a field-programmable gate array (FPGA) with\nreconfigurable logic provides an acceptable acceleration capacity and is\ncompatible with diverse computation-sensitive tasks in the cloud. In this\npaper, we develop an FPGA acceleration platform that leverages a unified\nframework architecture for general-purpose convolutional neural network (CNN)\ninference acceleration at a data center. To overcome the computation bound,\n4,096 DSPs are assembled and shaped as supertile units (SUs) for different\ntypes of convolution, which provide up to 4.2 TOP/s 16-bit fixed-point\nperformance at 500 MHz. The interleaved-task-dispatching method is proposed to\nmap the computation across the SUs, and the memory bound is solved by a\ndispatching-assembling buffering model and broadcast caches. For various\nnon-convolution operators, a filter processing unit is designed for\ngeneral-purpose filter-like/pointwise operators. In the experiment, the\nperformances of CNN models running on server-class CPUs, a GPU, and an FPGA are\ncompared. The results show that our design achieves the best FPGA peak\nperformance and a throughput at the same level as that of the state-of-the-art\nGPU in data centers, with more than 50 times lower latency.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 03:32:21 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Yu", "Xiaoyu", ""], ["Wang", "Yuwei", ""], ["Miao", "Jie", ""], ["Wu", "Ephrem", ""], ["Zhang", "Heng", ""], ["Meng", "Yu", ""], ["Zhang", "Bo", ""], ["Min", "Biao", ""], ["Chen", "Dewei", ""], ["Gao", "Jianlin", ""]]}, {"id": "1909.08034", "submitter": "Wei Jiang", "authors": "Wei Jiang, Juan Camilo Gamboa Higuera, Baptiste Angles, Weiwei Sun,\n  Mehrsan Javan, Kwang Moo Yi", "title": "Optimizing Through Learned Errors for Accurate Sports Field Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an optimization-based framework to register sports field templates\nonto broadcast videos. For accurate registration we go beyond the prevalent\nfeed-forward paradigm. Instead, we propose to train a deep network that\nregresses the registration error, and then register images by finding the\nregistration parameters that minimize the regressed error. We demonstrate the\neffectiveness of our method by applying it to real-world sports broadcast\nvideos, outperforming the state of the art. We further apply our method on a\nsynthetic toy example and demonstrate that our method brings significant gains\neven when the problem is simplified and unlimited training data is available.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 19:07:00 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 04:15:47 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Jiang", "Wei", ""], ["Higuera", "Juan Camilo Gamboa", ""], ["Angles", "Baptiste", ""], ["Sun", "Weiwei", ""], ["Javan", "Mehrsan", ""], ["Yi", "Kwang Moo", ""]]}, {"id": "1909.08038", "submitter": "Bruno Alexandre Krinski", "authors": "Bruno A. Krinski, Daniel V. Ruiz, Guilherme Z. Machado and Eduardo\n  Todt", "title": "Masking Salient Object Detection, a Mask Region-based Convolutional\n  Neural Network Analysis for Segmentation of Salient Objects", "comments": "6 pages, 10 figures, Accepted for presentation at the Conference on\n  SBR 2019 7th Brazilian Robotics Symposium/IEEE LARS 2019 16th Latin American\n  Robotics Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a broad comparison between Fully Convolutional\nNetworks (FCNs) and Mask Region-based Convolutional Neural Networks\n(Mask-RCNNs) applied in the Salient Object Detection (SOD) context. Studies in\nthe SOD literature usually explore architectures based in FCNs to detect\nsalient regions and objects in visual scenes. However, besides the promising\nresults achieved, FCNs showed issues in some challenging scenarios. Fairly\nrecently studies in the SOD literature proposed the use of a Mask-RCNN approach\nto overcome such issues. However, there is no extensive comparison between the\ntwo networks in the SOD literature endorsing the effectiveness of Mask-RCNNs\nover FCN when segmenting salient objects. Aiming to effectively show the\nsuperiority of Mask-RCNNs over FCNs in the SOD context, we compare two\nvariations of Mask-RCNNs with two variations of FCNs in eight datasets widely\nused in the literature and in four metrics. Our findings show that in this\ncontext Mask-RCNNs achieved an improvement on the F-measure up to 47% over\nFCNs.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 19:14:33 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Krinski", "Bruno A.", ""], ["Ruiz", "Daniel V.", ""], ["Machado", "Guilherme Z.", ""], ["Todt", "Eduardo", ""]]}, {"id": "1909.08049", "submitter": "Shervin Minaee", "authors": "Amirhossein Khalilian-Gourtani, Shervin Minaee, Yao Wang", "title": "Masked-RPCA: Sparse and Low-rank Decomposition Under Overlaying Model\n  and Application to Moving Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Foreground detection in a given video sequence is a pivotal step in many\ncomputer vision applications such as video surveillance system. Robust\nPrincipal Component Analysis (RPCA) performs low-rank and sparse decomposition\nand accomplishes such a task when the background is stationary and the\nforeground is dynamic and relatively small. A fundamental issue with RPCA is\nthe assumption that the low-rank and sparse components are added at each\nelement, whereas in reality, the moving foreground is overlaid on the\nbackground. We propose the representation via masked decomposition (i.e. an\noverlaying model) where each element either belongs to the low-rank or the\nsparse component, decided by a mask. We propose the Masked-RPCA algorithm to\nrecover the mask and the low-rank components simultaneously, utilizing\nlinearizing and alternating direction techniques. We further extend our\nformulation to be robust to dynamic changes in the background and enforce\nspatial connectivity in the foreground component. Our study shows significant\nimprovement of the detected mask compared to post-processing on the sparse\ncomponent obtained by other frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 19:39:15 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Khalilian-Gourtani", "Amirhossein", ""], ["Minaee", "Shervin", ""], ["Wang", "Yao", ""]]}, {"id": "1909.08058", "submitter": "Shadi Albarqouni Ph.D.", "authors": "Agnieszka Tomczack, Nassir Navab, Shadi Albarqouni", "title": "Learn to Estimate Labels Uncertainty for Quality Assurance", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning sets the state-of-the-art in many challenging tasks showing\noutstanding performance in a broad range of applications. Despite its success,\nit still lacks robustness hindering its adoption in medical applications.\nModeling uncertainty, through Bayesian Inference and Monte-Carlo dropout, has\nbeen successfully introduced for better understanding the underlying deep\nlearning models. Yet, another important source of uncertainty, coming from the\ninter-observer variability, has not been thoroughly addressed in the\nliterature. In this paper, we introduce labels uncertainty which better suits\nmedical applications and show that modeling such uncertainty together with\nepistemic uncertainty is of high interest for quality control and referral\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 19:43:57 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Tomczack", "Agnieszka", ""], ["Navab", "Nassir", ""], ["Albarqouni", "Shadi", ""]]}, {"id": "1909.08097", "submitter": "Umar Asif", "authors": "Umar Asif, Jianbin Tang, and Stefan Harrer", "title": "Ensemble Knowledge Distillation for Learning Improved and Efficient\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble models comprising of deep Convolutional Neural Networks (CNN) have\nshown significant improvements in model generalization but at the cost of large\ncomputation and memory requirements. In this paper, we present a framework for\nlearning compact CNN models with improved classification performance and model\ngeneralization. For this, we propose a CNN architecture of a compact student\nmodel with parallel branches which are trained using ground truth labels and\ninformation from high capacity teacher networks in an ensemble learning\nfashion. Our framework provides two main benefits: i) Distilling knowledge from\ndifferent teachers into the student network promotes heterogeneity in feature\nlearning at different branches of the student network and enables the network\nto learn diverse solutions to the target problem. ii) Coupling the branches of\nthe student network through ensembling encourages collaboration and improves\nthe quality of the final predictions by reducing variance in the network\noutputs. Experiments on the well established CIFAR-10 and CIFAR-100 datasets\nshow that our Ensemble Knowledge Distillation (EKD) improves classification\naccuracy and model generalization especially in situations with limited\ntraining data. Experiments also show that our EKD based compact networks\noutperform in terms of mean accuracy on the test datasets compared to\nstate-of-the-art knowledge distillation based methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 21:03:19 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 08:03:56 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2020 03:41:59 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Asif", "Umar", ""], ["Tang", "Jianbin", ""], ["Harrer", "Stefan", ""]]}, {"id": "1909.08105", "submitter": "Iason Sarantopoulos", "authors": "Iason Sarantopoulos, Marios Kiatos, Zoe Doulgeri, and Sotiris\n  Malassiotis", "title": "Split Deep Q-Learning for Robust Object Singulation", "comments": "Accepted for presentation in 2020 International Conference on\n  Robotics and Automation (ICRA). Video attachment:\n  https://youtu.be/ef1MKgVkN0E", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting a known target object from a pile of other objects in a cluttered\nenvironment is a challenging robotic manipulation task encountered in many\nrobotic applications. In such conditions, the target object touches or is\ncovered by adjacent obstacle objects, thus rendering traditional grasping\ntechniques ineffective. In this paper, we propose a pushing policy aiming at\nsingulating the target object from its surrounding clutter, by means of lateral\npushing movements of both the neighboring objects and the target object until\nsufficient 'grasping room' has been achieved. To achieve the above goal we\nemploy reinforcement learning and particularly Deep Q-learning (DQN) to learn\noptimal push policies by trial and error. A novel Split DQN is proposed to\nimprove the learning rate and increase the modularity of the algorithm.\nExperiments show that although learning is performed in a simulated environment\nthe transfer of learned policies to a real environment is effective thanks to\nrobust feature selection. Finally, we demonstrate that the modularity of the\nalgorithm allows the addition of extra primitives without retraining the model\nfrom scratch.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 21:14:06 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 12:57:31 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Sarantopoulos", "Iason", ""], ["Kiatos", "Marios", ""], ["Doulgeri", "Zoe", ""], ["Malassiotis", "Sotiris", ""]]}, {"id": "1909.08112", "submitter": "Nikolaos Zioulis Mr.", "authors": "Nikolaos Zioulis, Antonis Karakottas, Dimitrios Zarpalas, Federico\n  Alvarez, Petros Daras", "title": "Spherical View Synthesis for Self-Supervised 360 Depth Estimation", "comments": "3DV19, code and data at\n  https://vcl3d.github.io/SphericalViewSynthesis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning based approaches for depth perception are limited by the\navailability of clean training data. This has led to the utilization of view\nsynthesis as an indirect objective for learning depth estimation using\nefficient data acquisition procedures. Nonetheless, most research focuses on\npinhole based monocular vision, with scarce works presenting results for\nomnidirectional input. In this work, we explore spherical view synthesis for\nlearning monocular 360 depth in a self-supervised manner and demonstrate its\nfeasibility. Under a purely geometrically derived formulation we present\nresults for horizontal and vertical baselines, as well as for the trinocular\ncase. Further, we show how to better exploit the expressiveness of traditional\nCNNs when applied to the equirectangular domain in an efficient manner.\nFinally, given the availability of ground truth depth data, our work is\nuniquely positioned to compare view synthesis against direct supervision in a\nconsistent and fair manner. The results indicate that alternative research\ndirections might be better suited to enable higher quality depth perception.\nOur data, models and code are publicly available at\nhttps://vcl3d.github.io/SphericalViewSynthesis/.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 21:25:35 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Zioulis", "Nikolaos", ""], ["Karakottas", "Antonis", ""], ["Zarpalas", "Dimitrios", ""], ["Alvarez", "Federico", ""], ["Daras", "Petros", ""]]}, {"id": "1909.08130", "submitter": "Hadi Kazemi", "authors": "Hadi Kazemi, Fariborz Taherkhani, Nasser M. Nasrabadi", "title": "Identity-Aware Deep Face Hallucination via Adversarial Face Verification", "comments": "BTAS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of face hallucination by proposing a\nnovel multi-scale generative adversarial network (GAN) architecture optimized\nfor face verification. First, we propose a multi-scale generator architecture\nfor face hallucination with a high up-scaling ratio factor, which has multiple\nintermediate outputs at different resolutions. The intermediate outputs have\nthe growing goal of synthesizing small to large images. Second, we incorporate\na face verifier with the original GAN discriminator and propose a novel\ndiscriminator which learns to discriminate different identities while\ndistinguishing fake generated HR face images from their ground truth images. In\nparticular, the learned generator cares for not only the visual quality of\nhallucinated face images but also preserving the discriminative features in the\nhallucination process. In addition, to capture perceptually relevant\ndifferences we employ a perceptual similarity loss, instead of similarity in\npixel space. We perform a quantitative and qualitative evaluation of our\nframework on the LFW and CelebA datasets. The experimental results show the\nadvantages of our proposed method against the state-of-the-art methods on the\n8x downsampled testing dataset.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 22:17:58 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Kazemi", "Hadi", ""], ["Taherkhani", "Fariborz", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1909.08150", "submitter": "Srikanth Malla", "authors": "Srikanth Malla and Isht Dwivedi and Behzad Dariush and Chiho Choi", "title": "NEMO: Future Object Localization Using Noisy Ego Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Predicting the future trajectory of agents from visual observations is an\nimportant problem for realization of safe and effective navigation of\nautonomous systems in dynamic environments. This paper focuses on two important\naspects of future trajectory forecast which are particularly relevant for\nmobile platforms: 1) modeling uncertainty of the predictions, particularly from\negocentric views, where uncertainty in the interactive reactions and behaviors\nof other agents must consider the uncertainty in the ego-motion, and 2)\nmodeling multi-modality nature of the problem, which are particularly prevalent\nat junctions in urban traffic scenes. To address these problems in a unified\napproach, we propose NEMO (Noisy Ego MOtion priors for future object\nlocalization) for future forecast of agents in the egocentric view. In the\nproposed approach, a predictive distribution of future forecast is jointly\nmodeled with the uncertainty of predictions. For this, we divide the problem\ninto two tasks: future ego-motion prediction and future object localization. We\nfirst model the multi-modal distribution of future ego-motion with uncertainty\nestimates. The resulting distribution of ego-behavior is used to sample\nmultiple modes of future ego-motion. Then, each modality is used as a prior to\nunderstand the interactions between the ego-vehicle and target agent. We\npredict the multi-modal future locations of the target from individual modes of\nthe ego-vehicle while modeling the uncertainty of the target's behavior. To\nthis end, we extensively evaluate the proposed framework using the publicly\navailable benchmark dataset (HEV-I) supplemented with odometry data from an\nInertial Measurement Unit (IMU).\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 23:52:17 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 07:56:40 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 18:22:29 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Malla", "Srikanth", ""], ["Dwivedi", "Isht", ""], ["Dariush", "Behzad", ""], ["Choi", "Chiho", ""]]}, {"id": "1909.08153", "submitter": "Ahmad Khaliq", "authors": "Ahmad Khaliq, Shoaib Ehsan, Michael Milford, Klaus McDonald-Maier", "title": "CAMAL: Context-Aware Multi-layer Attention framework for Lightweight\n  Environment Invariant Visual Place Recognition", "comments": "under-review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, Deep Convolutional Neural Networks (D-CNNs) have shown\nstate-of-the-art (SOTA) performance for Visual Place Recognition (VPR), a\npivotal component of long-term intelligent robotic vision (vision-aware\nlocalization and navigation systems). The prestigious generalization power of\nD-CNNs gained upon training on large scale places datasets and learned\npersistent image regions which are found to be robust for specific place\nrecognition under changing conditions and camera viewpoints. However, against\nthe computation and power intensive D-CNNs based VPR algorithms that are\nemployed to determine the approximate location of resource-constrained mobile\nrobots, lightweight VPR techniques are preferred. This paper presents a\ncomputation- and energy-efficient CAMAL framework that captures place-specific\nmulti-layer convolutional attentions efficient for environment invariant-VPR.\nAt 4x lesser power consumption, evaluating the proposed VPR framework on\nchallenging benchmark place recognition datasets reveal better and comparable\nArea under Precision-Recall (AUC-PR) curves with approximately 4x improved\nimage retrieval performance over the contemporary VPR methodologies.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 00:24:07 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 15:15:53 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Khaliq", "Ahmad", ""], ["Ehsan", "Shoaib", ""], ["Milford", "Michael", ""], ["McDonald-Maier", "Klaus", ""]]}, {"id": "1909.08164", "submitter": "Guanbin Li", "authors": "Sibei Yang, Guanbin Li, Yizhou Yu", "title": "Dynamic Graph Attention for Referring Expression Comprehension", "comments": "Accepted as an Oral presentation at ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring expression comprehension aims to locate the object instance\ndescribed by a natural language referring expression in an image. This task is\ncompositional and inherently requires visual reasoning on top of the\nrelationships among the objects in the image. Meanwhile, the visual reasoning\nprocess is guided by the linguistic structure of the referring expression.\nHowever, existing approaches treat the objects in isolation or only explore the\nfirst-order relationships between objects without being aligned with the\npotential complexity of the expression. Thus it is hard for them to adapt to\nthe grounding of complex referring expressions. In this paper, we explore the\nproblem of referring expression comprehension from the perspective of\nlanguage-driven visual reasoning, and propose a dynamic graph attention network\nto perform multi-step reasoning by modeling both the relationships among the\nobjects in the image and the linguistic structure of the expression. In\nparticular, we construct a graph for the image with the nodes and edges\ncorresponding to the objects and their relationships respectively, propose a\ndifferential analyzer to predict a language-guided visual reasoning process,\nand perform stepwise reasoning on top of the graph to update the compound\nobject representation at every node. Experimental results demonstrate that the\nproposed method can not only significantly surpass all existing\nstate-of-the-art algorithms across three common benchmark datasets, but also\ngenerate interpretable visual evidences for stepwisely locating the objects\nreferred to in complex language descriptions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 01:47:27 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Yang", "Sibei", ""], ["Li", "Guanbin", ""], ["Yu", "Yizhou", ""]]}, {"id": "1909.08171", "submitter": "Hitoshi Nishimura", "authors": "Hitoshi Nishimura, Kazuyuki Tasaka, Yasutomo Kawanishi, Hiroshi Murase", "title": "Multiple Human Tracking using Multi-Cues including Primitive Action\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Multiple Human Tracking method using multi-cues\nincluding Primitive Action Features (MHT-PAF). MHT-PAF can perform the accurate\nhuman tracking in dynamic aerial videos captured by a drone. PAF employs a\nglobal context, rich information by multi-label actions, and a middle level\nfeature. The accurate human tracking result using PAF helps multi-frame-based\naction recognition. In the experiments, we verified the effectiveness of the\nproposed method using the Okutama-Action dataset. Our code is available online.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 02:14:38 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Nishimura", "Hitoshi", ""], ["Tasaka", "Kazuyuki", ""], ["Kawanishi", "Yasutomo", ""], ["Murase", "Hiroshi", ""]]}, {"id": "1909.08174", "submitter": "Zhonghui You", "authors": "Zhonghui You, Kun Yan, Jinmian Ye, Meng Ma, Ping Wang", "title": "Gate Decorator: Global Filter Pruning Method for Accelerating Deep\n  Convolutional Neural Networks", "comments": "Accepted by NeurIPS'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filter pruning is one of the most effective ways to accelerate and compress\nconvolutional neural networks (CNNs). In this work, we propose a global filter\npruning algorithm called Gate Decorator, which transforms a vanilla CNN module\nby multiplying its output by the channel-wise scaling factors, i.e. gate. When\nthe scaling factor is set to zero, it is equivalent to removing the\ncorresponding filter. We use Taylor expansion to estimate the change in the\nloss function caused by setting the scaling factor to zero and use the\nestimation for the global filter importance ranking. Then we prune the network\nby removing those unimportant filters. After pruning, we merge all the scaling\nfactors into its original module, so no special operations or structures are\nintroduced. Moreover, we propose an iterative pruning framework called\nTick-Tock to improve pruning accuracy. The extensive experiments demonstrate\nthe effectiveness of our approaches. For example, we achieve the\nstate-of-the-art pruning ratio on ResNet-56 by reducing 70% FLOPs without\nnoticeable loss in accuracy. For ResNet-50 on ImageNet, our pruned model with\n40% FLOPs reduction outperforms the baseline model by 0.31% in top-1 accuracy.\nVarious datasets are used, including CIFAR-10, CIFAR-100, CUB-200, ImageNet\nILSVRC-12 and PASCAL VOC 2011. Code is available at\ngithub.com/youzhonghui/gate-decorator-pruning\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 02:28:56 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["You", "Zhonghui", ""], ["Yan", "Kun", ""], ["Ye", "Jinmian", ""], ["Ma", "Meng", ""], ["Wang", "Ping", ""]]}, {"id": "1909.08190", "submitter": "Yueru Chen", "authors": "Yueru Chen, C.-C. Jay Kuo", "title": "PixelHop: A Successive Subspace Learning (SSL) Method for Object\n  Classification", "comments": "17 pages, 11 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new machine learning methodology, called successive subspace learning\n(SSL), is introduced in this work. SSL contains four key ingredients: 1)\nsuccessive near-to-far neighborhood expansion; 2) unsupervised dimension\nreduction via subspace approximation; 3) supervised dimension reduction via\nlabel-assisted regression (LAG); and 4) feature concatenation and decision\nmaking. An image-based object classification method, called PixelHop, is\nproposed to illustrate the SSL design. It is shown by experimental results that\nthe PixelHop method outperforms the classic CNN model of similar model\ncomplexity in three benchmarking datasets (MNIST, Fashion MNIST and CIFAR-10).\nAlthough SSL and deep learning (DL) have some high-level concept in common,\nthey are fundamentally different in model formulation, the training process and\ntraining complexity. Extensive discussion on the comparison of SSL and DL is\nmade to provide further insights into the potential of SSL.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 15:14:19 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Chen", "Yueru", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1909.08205", "submitter": "Deying Kong", "authors": "Deying Kong, Yifei Chen, Haoyu Ma, Xiangyi Yan, Xiaohui Xie", "title": "Adaptive Graphical Model Network for 2D Handpose Estimation", "comments": "30th British Machine Vision Conference (BMVC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a new architecture called Adaptive Graphical Model\nNetwork (AGMN) to tackle the task of 2D hand pose estimation from a monocular\nRGB image. The AGMN consists of two branches of deep convolutional neural\nnetworks for calculating unary and pairwise potential functions, followed by a\ngraphical model inference module for integrating unary and pairwise potentials.\nUnlike existing architectures proposed to combine DCNNs with graphical models,\nour AGMN is novel in that the parameters of its graphical model are conditioned\non and fully adaptive to individual input images. Experiments show that our\napproach outperforms the state-of-the-art method used in 2D hand keypoints\nestimation by a notable margin on two public datasets.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 04:19:51 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Kong", "Deying", ""], ["Chen", "Yifei", ""], ["Ma", "Haoyu", ""], ["Yan", "Xiangyi", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1909.08213", "submitter": "Ying Dai", "authors": "Ying Dai", "title": "Sample-specific repetitive learning for photo aesthetic assessment and\n  highlight region extraction", "comments": "15 pages, 9 figures, 3 tables", "journal-ref": "Multimedia tools and application, 2020", "doi": "10.1007/s11042-020-09426-z", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aesthetic assessment is subjective, and the distribution of the aesthetic\nlevels is imbalanced. In order to realize the auto-assessment of photo\naesthetics, we focus on retraining the CNN-based aesthetic assessment model by\ndropping out the unavailable samples in the middle levels from the training\ndata set repetitively to overcome the effect of imbalanced aesthetic data on\nclassification. Further, the method of extracting aesthetics highlight region\nof the photo image by using the two repetitively trained models is presented.\nTherefore, the correlation of the extracted region with the aesthetic levels is\nanalyzed to illustrate what aesthetics features influence the aesthetic quality\nof the photo. Moreover, the testing data set is from the different data source\ncalled 500px. Experimental results show that the proposed method is effective.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 05:20:56 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Dai", "Ying", ""]]}, {"id": "1909.08216", "submitter": "Kaige Zhang", "authors": "Kaige Zhang, Yingtao Zhang, and Heng-Da Cheng", "title": "CrackGAN: Pavement Crack Detection Using Partially Accurate Ground\n  Truths Based on Generative Adversarial Learning", "comments": null, "journal-ref": null, "doi": "10.1109/TITS.2020.2990703", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional network is a powerful tool for per-pixel semantic\nsegmentation/detection. However, it is problematic when coping with crack\ndetection using partially accurate ground truths (GTs): the network may easily\nconverge to the status that treats all the pixels as background (BG) and still\nachieves a very good loss, named \"All Black\" phenomenon, due to the\nunavailability of accurate GTs and the data imbalance. To tackle this problem,\nwe propose crack-patch-only (CPO) supervised generative adversarial learning\nfor end-to-end training, which forces the network to always produce crack-GT\nimages while reserves both crack and BG-image translation abilities by feeding\na larger-size crack image into an asymmetric U-shape generator to overcome the\n\"All Black\" issue. The proposed approach is validated using four crack\ndatasets; and achieves state-of-the-art performance comparing with that of the\nrecently published works in efficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 05:52:08 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 16:38:28 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Zhang", "Kaige", ""], ["Zhang", "Yingtao", ""], ["Cheng", "Heng-Da", ""]]}, {"id": "1909.08223", "submitter": "Zhizhong Wang", "authors": "Zhizhong Wang, Lei Zhao, Haibo Chen, Lihong Qiu, Qihang Mo, Sihuan\n  Lin, Wei Xing, Dongming Lu", "title": "Diversified Arbitrary Style Transfer via Deep Feature Perturbation", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image style transfer is an underdetermined problem, where a large number of\nsolutions can satisfy the same constraint (the content and style). Although\nthere have been some efforts to improve the diversity of style transfer by\nintroducing an alternative diversity loss, they have restricted generalization,\nlimited diversity and poor scalability. In this paper, we tackle these\nlimitations and propose a simple yet effective method for diversified arbitrary\nstyle transfer. The key idea of our method is an operation called deep feature\nperturbation (DFP), which uses an orthogonal random noise matrix to perturb the\ndeep image feature maps while keeping the original style information unchanged.\nOur DFP operation can be easily integrated into many existing WCT (whitening\nand coloring transform)-based methods, and empower them to generate diverse\nresults for arbitrary styles. Experimental results demonstrate that this\nlearning-free and universal method can greatly increase the diversity while\nmaintaining the quality of stylization.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 06:24:42 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 13:24:40 GMT"}, {"version": "v3", "created": "Fri, 20 Mar 2020 10:21:57 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Wang", "Zhizhong", ""], ["Zhao", "Lei", ""], ["Chen", "Haibo", ""], ["Qiu", "Lihong", ""], ["Mo", "Qihang", ""], ["Lin", "Sihuan", ""], ["Xing", "Wei", ""], ["Lu", "Dongming", ""]]}, {"id": "1909.08228", "submitter": "Chunhua Shen", "authors": "Haokui Zhang, Ying Li, Hao Chen, Chunhua Shen", "title": "Memory-Efficient Hierarchical Neural Architecture Search for Image\n  Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, neural architecture search (NAS) methods have attracted much\nattention and outperformed manually designed architectures on a few high-level\nvision tasks. In this paper, we propose HiNAS (Hierarchical NAS), an effort\ntowards employing NAS to automatically design effective neural network\narchitectures for image denoising. HiNAS adopts gradient based search\nstrategies and employs operations with adaptive receptive field to build an\nflexible hierarchical search space. During the search stage, HiNAS shares cells\nacross different feature levels to save memory and employ an early stopping\nstrategy to avoid the collapse issue in NAS, and considerably accelerate the\nsearch speed. The proposed HiNAS is both memory and computation efficient,\nwhich takes only about 4.5 hours for searching using a single GPU. We evaluate\nthe effectiveness of our proposed HiNAS on two different datasets, namely an\nadditive white Gaussian noise dataset BSD500, and a realistic noise dataset\nSIM1800. Experimental results show that the architecture found by HiNAS has\nfewer parameters and enjoys a faster inference speed, while achieving highly\ncompetitive performance compared with state-of-the-art methods. We also present\nanalysis on the architectures found by NAS. HiNAS also shows good performance\non experiments for image de-raining.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 06:49:19 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 03:46:49 GMT"}, {"version": "v3", "created": "Fri, 7 Feb 2020 04:41:32 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Zhang", "Haokui", ""], ["Li", "Ying", ""], ["Chen", "Hao", ""], ["Shen", "Chunhua", ""]]}, {"id": "1909.08245", "submitter": "Nader Asadi", "authors": "Nader Asadi, Amir M. Sarfi, Mehrdad Hosseinzadeh, Zahra Karimpour,\n  Mahdi Eftekhari", "title": "Towards Shape Biased Unsupervised Representation Learning for Domain\n  Generalization", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that, without awareness of the process, our brain appears to\nfocus on the general shape of objects rather than superficial statistics of\ncontext. On the other hand, learning autonomously allows discovering invariant\nregularities which help generalization. In this work, we propose a learning\nframework to improve the shape bias property of self-supervised methods. Our\nmethod learns semantic and shape biased representations by integrating domain\ndiversification and jigsaw puzzles. The first module enables the model to\ncreate a dynamic environment across arbitrary domains and provides a domain\nexploration vs. exploitation trade-off, while the second module allows the\nmodel to explore this environment autonomously. This universal framework does\nnot require prior knowledge of the domain of interest. Extensive experiments\nare conducted on several domain generalization datasets, namely, PACS,\nOffice-Home, VLCS, and Digits. We show that our framework outperforms\nstate-of-the-art domain generalization methods by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:07:37 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 14:37:29 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Asadi", "Nader", ""], ["Sarfi", "Amir M.", ""], ["Hosseinzadeh", "Mehrdad", ""], ["Karimpour", "Zahra", ""], ["Eftekhari", "Mahdi", ""]]}, {"id": "1909.08269", "submitter": "Changqun Xia", "authors": "Changqun Xia, Jia Li, Jinming Su, Yonghong Tian", "title": "Exploring Reciprocal Attention for Salient Object Detection by\n  Cooperative Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, objects with the same semantics are not always prominent in images\ncontaining different backgrounds. Motivated by this observation that accurately\nsalient object detection is related to both foreground and background, we\nproposed a novel cooperative attention mechanism that jointly considers\nreciprocal relationships between background and foreground for efficient\nsalient object detection. Concretely, we first aggregate the features at each\nside-out of traditional dilated FCN to extract the initial foreground and\nbackground local responses respectively. Then taking these responses as input,\nreciprocal attention module adaptively models the nonlocal dependencies between\nany two pixels of the foreground and background features, which is then\naggregated with local features in a mutual reinforced way so as to enhance each\nbranch to generate more discriminative foreground and background saliency map.\nBesides, cooperative losses are particularly designed to guide the multi-task\nlearning of foreground and background branches, which encourages our network to\nobtain more complementary predictions with clear boundaries. At last, a simple\nbut effective fusion strategy is utilized to produce the final saliency map.\nComprehensive experimental results on five benchmark datasets demonstrate that\nour proposed method performs favorably against the state-of-the-art approaches\nin terms of all compared evaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:26:58 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Xia", "Changqun", ""], ["Li", "Jia", ""], ["Su", "Jinming", ""], ["Tian", "Yonghong", ""]]}, {"id": "1909.08287", "submitter": "Yang Liu", "authors": "Yang Liu, Zhaoyang Lu, Jing Li, Tao Yang, Chao Yao", "title": "Global Temporal Representation based CNNs for Infrared Action\n  Recognition", "comments": "Published in IEEE Signal Processing Letters, codes can be found at\n  https://yangliu9208.github.io/TSTDDs/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrared human action recognition has many advantages, i.e., it is\ninsensitive to illumination change, appearance variability, and shadows.\nExisting methods for infrared action recognition are either based on spatial or\nlocal temporal information, however, the global temporal information, which can\nbetter describe the movements of body parts across the whole video, is not\nconsidered. In this letter, we propose a novel global temporal representation\nnamed optical-flow stacked difference image (OFSDI) and extract robust and\ndiscriminative feature from the infrared action data by considering the local,\nglobal, and spatial temporal information together. Due to the small size of the\ninfrared action dataset, we first apply convolutional neural networks on local,\nspatial, and global temporal stream respectively to obtain efficient\nconvolutional feature maps from the raw data rather than train a classifier\ndirectly. Then these convolutional feature maps are aggregated into effective\ndescriptors named three-stream trajectory-pooled deep-convolutional descriptors\nby trajectory-constrained pooling. Furthermore, we improve the robustness of\nthese features by using the locality-constrained linear coding (LLC) method.\nWith these features, a linear support vector machine (SVM) is adopted to\nclassify the action data in our scheme. We conduct the experiments on infrared\naction recognition datasets InfAR and NTU RGB+D. The experimental results show\nthat the proposed approach outperforms the representative state-of-the-art\nhandcrafted features and deep learning features based methods for the infrared\naction recognition.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 08:52:35 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Liu", "Yang", ""], ["Lu", "Zhaoyang", ""], ["Li", "Jing", ""], ["Yang", "Tao", ""], ["Yao", "Chao", ""]]}, {"id": "1909.08288", "submitter": "Pedro Machado", "authors": "Pedro Machado, Georgina Cosma, T.M McGinnity", "title": "NatCSNN: A Convolutional Spiking Neural Network for recognition of\n  objects extracted from natural images", "comments": "12 pages", "journal-ref": "Artificial Neural Networks and Machine Learning - ICANN 2019:\n  Theoretical Neural Computation. ICANN 2019. Lecture Notes in Computer\n  Science, vol 11727", "doi": "10.1007/978-3-030-30487-4_28", "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological image processing is performed by complex neural networks composed\nof thousands of neurons interconnected via thousands of synapses, some of which\nare excitatory and others inhibitory. Spiking neural models are distinguished\nfrom classical neurons by being biological plausible and exhibiting the same\ndynamics as those observed in biological neurons. This paper proposes a Natural\nConvolutional Neural Network (NatCSNN) which is a 3-layer bio-inspired\nConvolutional Spiking Neural Network (CSNN), for classifying objects extracted\nfrom natural images. A two-stage training algorithm is proposed using\nunsupervised Spike Timing Dependent Plasticity (STDP) learning (phase 1) and\nReSuMe supervised learning (phase 2). The NatCSNN was trained and tested on the\nCIFAR-10 dataset and achieved an average testing accuracy of 84.7% which is an\nimprovement over the 2-layer neural networks previously applied to this\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 08:52:42 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Machado", "Pedro", ""], ["Cosma", "Georgina", ""], ["McGinnity", "T. M", ""]]}, {"id": "1909.08291", "submitter": "Eren Aksoy", "authors": "Eren Erdal Aksoy, Saimir Baci, Selcuk Cavdar", "title": "SalsaNet: Fast Road and Vehicle Segmentation in LiDAR Point Clouds for\n  Autonomous Driving", "comments": null, "journal-ref": "IEEE Intelligent Vehicles Symposium (IV) 2020. The code can be\n  found here: https://gitlab.com/aksoyeren/salsanet", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a deep encoder-decoder network, named SalsaNet,\nfor efficient semantic segmentation of 3D LiDAR point clouds. SalsaNet segments\nthe road, i.e. drivable free-space, and vehicles in the scene by employing the\nBird-Eye-View (BEV) image projection of the point cloud. To overcome the lack\nof annotated point cloud data, in particular for the road segments, we\nintroduce an auto-labeling process which transfers automatically generated\nlabels from the camera to LiDAR. We also explore the role of imagelike\nprojection of LiDAR data in semantic segmentation by comparing BEV with\nspherical-front-view projection and show that SalsaNet is projection-agnostic.\nWe perform quantitative and qualitative evaluations on the KITTI dataset, which\ndemonstrate that the proposed SalsaNet outperforms other state-of-the-art\nsemantic segmentation networks in terms of accuracy and computation time. Our\ncode and data are publicly available at\nhttps://gitlab.com/aksoyeren/salsanet.git.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 08:57:20 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Aksoy", "Eren Erdal", ""], ["Baci", "Saimir", ""], ["Cavdar", "Selcuk", ""]]}, {"id": "1909.08297", "submitter": "Yang Liu", "authors": "Yang Liu, Zhaoyang Lu, Jing Li, Chao Yao, Yanzi Deng", "title": "Transferable Feature Representation for Visible-to-Infrared\n  Cross-Dataset Human Action Recognition", "comments": "Published in Complexity(JCR Rank 1, IF:4.621),codes can be found at\n  https://yangliu9208.github.io/CDFAG/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, infrared human action recognition has attracted increasing\nattention for it has many advantages over visible light, that is, being robust\nto illumination change and shadows. However, the infrared action data is\nlimited until now, which degrades the performance of infrared action\nrecognition. Motivated by the idea of transfer learning, an infrared human\naction recognition framework using auxiliary data from visible light is\nproposed to solve the problem of limited infrared action data. In the proposed\nframework, we first construct a novel Cross-Dataset Feature Alignment and\nGeneralization (CDFAG) framework to map the infrared data and visible light\ndata into a common feature space, where Kernel Manifold Alignment (KEMA) and a\ndual alignedto-generalized encoders (AGE) model are employed to represent the\nfeature. Then, a support vector machine (SVM) is trained, using both the\ninfrared data and visible light data, and can classify the features derived\nfrom infrared data. The proposed method is evaluated on InfAR, which is a\npublicly available infrared human action dataset. To build up auxiliary data,\nwe set up a novel visible light action dataset XD145. Experimental results show\nthat the proposed method can achieve state-of-the-art performance compared with\nseveral transfer learning and domain adaptation methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 09:10:05 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Liu", "Yang", ""], ["Lu", "Zhaoyang", ""], ["Li", "Jing", ""], ["Yao", "Chao", ""], ["Deng", "Yanzi", ""]]}, {"id": "1909.08313", "submitter": "Qian Yu", "authors": "Runtao Liu, Qian Yu, Stella Yu", "title": "Unsupervised Sketch-to-Photo Synthesis", "comments": "23 pages, 15 figures. URL:\n  https://github.com/rt219/Unpaired-Sketch-to-Photo-Translation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can envision a realistic photo given a free-hand sketch that is not\nonly spatially imprecise and geometrically distorted but also without colors\nand visual details. We study unsupervised sketch-to-photo synthesis for the\nfirst time, learning from unpaired sketch-photo data where the target photo for\na sketch is unknown during training. Existing works only deal with style change\nor spatial deformation alone, synthesizing photos from edge-aligned line\ndrawings or transforming shapes within the same modality, e.g., color images.\nOur key insight is to decompose unsupervised sketch-to-photo synthesis into a\ntwo-stage translation task: First shape translation from sketches to grayscale\nphotos and then content enrichment from grayscale to color photos. We also\nincorporate a self-supervised denoising objective and an attention module to\nhandle abstraction and style variations that are inherent and specific to\nsketches. Our synthesis is sketch-faithful and photo-realistic to enable\nsketch-based image retrieval in practice. An exciting corollary product is a\nuniversal and promising sketch generator that captures human visual perception\nbeyond the edge map of a photo.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 09:33:10 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 22:33:24 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 15:06:33 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Liu", "Runtao", ""], ["Yu", "Qian", ""], ["Yu", "Stella", ""]]}, {"id": "1909.08315", "submitter": "Juan Maro\\~nas", "authors": "Daniel Ramos, Juan Maro\\~nas and Alicia Lozano-Diez", "title": "Bayesian Strategies for Likelihood Ratio Computation in Forensic Voice\n  Comparison with Automatic Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper explores several strategies for Forensic Voice Comparison (FVC),\naimed at improving the performance of the LRs when using generative Gaussian\nscore-to-LR models. First, different anchoring strategies are proposed, with\nthe objective of adapting the LR computation process to the case at hand,\nalways respecting the propositions defined for the particular case. Second, a\nfully-Bayesian Gaussian model is used to tackle the sparsity in the training\nscores that is often present when the proposed anchoring strategies are used.\nExperiments are performed using the 2014 i-Vector challenge set-up, which\npresents high variability in a telephone speech context. The results show that\nthe proposed fully-Bayesian model clearly outperforms a more common\nMaximum-Likelihood approach, leading to high robustness when the scores to\ntrain the model become sparse.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 09:39:41 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Ramos", "Daniel", ""], ["Maro\u00f1as", "Juan", ""], ["Lozano-Diez", "Alicia", ""]]}, {"id": "1909.08326", "submitter": "Hong Wang", "authors": "Hong Wang, Yichen Wu, Minghan Li, Qian Zhao, and Deyu Meng", "title": "A Survey on Rain Removal from Video and Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain streaks might severely degenerate the performance of video/image\nprocessing tasks. The investigations on rain removal from video or a single\nimage has thus been attracting much research attention in the field of computer\nvision and pattern recognition, and various methods have been proposed against\nthis task in the recent years. However, there is still not a comprehensive\nsurvey paper to summarize current rain removal methods and fairly compare their\ngeneralization performance, and especially, still not a off-the-shelf toolkit\nto accumulate recent representative methods for easy performance comparison and\ncapability evaluation. Aiming at this meaningful task, in this study we present\na comprehensive review for current rain removal methods for video and a single\nimage. Specifically, these methods are categorized into model-driven and\ndata-driven approaches, and more elaborate branches of each approach are\nfurther introduced. Intrinsic capabilities, especially generalization, of\nrepresentative state-of-the-art methods of each approach have been evaluated\nand analyzed by experiments implemented on synthetic and real data both\nvisually and quantitatively. Furthermore, we release a comprehensive\nrepository, including direct links to 74 rain removal papers, source codes of 9\nmethods for video rain removal and 20 ones for single image rain removal, 19\nrelated project pages, 6 synthetic datasets and 4 real ones, and 4 commonly\nused image quality metrics, to facilitate reproduction and performance\ncomparison of current existing methods for general users. Some limitations and\nresearch issues worthy to be further investigated have also been discussed for\nfuture research of this direction.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 10:05:24 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 16:18:32 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Wang", "Hong", ""], ["Wu", "Yichen", ""], ["Li", "Minghan", ""], ["Zhao", "Qian", ""], ["Meng", "Deyu", ""]]}, {"id": "1909.08330", "submitter": "Udaranga Wickramasinghe", "authors": "Udaranga Wickramasinghe, Graham Knott, Pascal Fua", "title": "Probabilistic Atlases to Enforce Topological Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic atlases (PAs) have long been used in standard segmentation\napproaches and, more recently, in conjunction with Convolutional Neural\nNetworks (CNNs). However, their use has been restricted to relatively\nstandardized structures such as the brain or heart which have limited or\npredictable range of deformations. Here we propose an encoding-decoding CNN\narchitecture that can exploit rough atlases that encode only the topology of\nthe target structures that can appear in any pose and have arbitrarily complex\nshapes to improve the segmentation results. It relies on the output of the\nencoder to compute both the pose parameters used to deform the atlas and the\nsegmentation mask itself, which makes it effective and end-to-end trainable.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 10:10:03 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Wickramasinghe", "Udaranga", ""], ["Knott", "Graham", ""], ["Fua", "Pascal", ""]]}, {"id": "1909.08383", "submitter": "Matthias De Lange", "authors": "Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia,\n  Ales Leonardis, Gregory Slabaugh and Tinne Tuytelaars", "title": "A continual learning survey: Defying forgetting in classification tasks", "comments": "Accepted TPAMI paper, including Appendix, code publicly available", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3057446", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks thrive in solving the classification problem for a\nparticular rigid task, acquiring knowledge through generalized learning\nbehaviour from a distinct training phase. The resulting network resembles a\nstatic entity of knowledge, with endeavours to extend this knowledge without\ntargeting the original task resulting in a catastrophic forgetting. Continual\nlearning shifts this paradigm towards networks that can continually accumulate\nknowledge over different tasks without the need to retrain from scratch. We\nfocus on task incremental classification, where tasks arrive sequentially and\nare delineated by clear boundaries. Our main contributions concern 1) a\ntaxonomy and extensive overview of the state-of-the-art, 2) a novel framework\nto continually determine the stability-plasticity trade-off of the continual\nlearner, 3) a comprehensive experimental comparison of 11 state-of-the-art\ncontinual learning methods and 4 baselines. We empirically scrutinize method\nstrengths and weaknesses on three benchmarks, considering Tiny Imagenet and\nlarge-scale unbalanced iNaturalist and a sequence of recognition datasets. We\nstudy the influence of model capacity, weight decay and dropout regularization,\nand the order in which the tasks are presented, and qualitatively compare\nmethods in terms of required memory, computation time, and storage.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 12:07:36 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 15:48:11 GMT"}, {"version": "v3", "created": "Fri, 16 Apr 2021 17:53:39 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["De Lange", "Matthias", ""], ["Aljundi", "Rahaf", ""], ["Masana", "Marc", ""], ["Parisot", "Sarah", ""], ["Jia", "Xu", ""], ["Leonardis", "Ales", ""], ["Slabaugh", "Gregory", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1909.08442", "submitter": "Diego Marcos", "authors": "Diego Marcos, Sylvain Lobry and Devis Tuia", "title": "Semantically Interpretable Activation Maps: what-where-how explanations\n  within CNNs", "comments": "2019 ICCV Workshop on Interpreting and Explaining Visual Artificial\n  Intelligence Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A main issue preventing the use of Convolutional Neural Networks (CNN) in end\nuser applications is the low level of transparency in the decision process.\nPrevious work on CNN interpretability has mostly focused either on localizing\nthe regions of the image that contribute to the result or on building an\nexternal model that generates plausible explanations. However, the former does\nnot provide any semantic information and the latter does not guarantee the\nfaithfulness of the explanation. We propose an intermediate representation\ncomposed of multiple Semantically Interpretable Activation Maps (SIAM)\nindicating the presence of predefined attributes at different locations of the\nimage. These attribute maps are then linearly combined to produce the final\noutput. This gives the user insight into what the model has seen, where, and a\nfinal output directly linked to this information in a comprehensive and\ninterpretable way. We test the method on the task of landscape scenicness\n(aesthetic value) estimation, using an intermediate representation of 33\nattributes from the SUN Attributes database. The results confirm that SIAM\nmakes it possible to understand what attributes in the image are contributing\nto the final score and where they are located. Since it is based on learning\nfrom multiple tasks and datasets, SIAM improve the explanability of the\nprediction without additional annotation efforts or computational overhead at\ninference time, while keeping good performances on both the final and\nintermediate tasks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 13:34:34 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Marcos", "Diego", ""], ["Lobry", "Sylvain", ""], ["Tuia", "Devis", ""]]}, {"id": "1909.08453", "submitter": "Bo Wan", "authors": "Bo Wan, Desen Zhou, Yongfei Liu, Rongjie Li and Xuming He", "title": "Pose-aware Multi-level Feature Network for Human Object Interaction\n  Detection", "comments": "International Conference on Computer Vision 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning human object interactions is a core problem in human-centric scene\nunderstanding and detecting such relations poses a unique challenge to vision\nsystems due to large variations in human-object configurations, multiple\nco-occurring relation instances and subtle visual difference between relation\ncategories. To address those challenges, we propose a multi-level relation\ndetection strategy that utilizes human pose cues to capture global spatial\nconfigurations of relations and as an attention mechanism to dynamically zoom\ninto relevant regions at human part level. Specifically, we develop a\nmulti-branch deep network to learn a pose-augmented relation representation at\nthree semantic levels, incorporating interaction context, object features and\ndetailed semantic part cues. As a result, our approach is capable of generating\nrobust predictions on fine-grained human object interactions with interpretable\noutputs. Extensive experimental evaluations on public benchmarks show that our\nmodel outperforms prior methods by a considerable margin, demonstrating its\nefficacy in handling complex scenes.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 13:47:37 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Wan", "Bo", ""], ["Zhou", "Desen", ""], ["Liu", "Yongfei", ""], ["Li", "Rongjie", ""], ["He", "Xuming", ""]]}, {"id": "1909.08473", "submitter": "Lei Kang", "authors": "Lei Kang, Mar\\c{c}al Rusi\\~nol, Alicia Forn\\'es, Pau Riba, Mauricio\n  Villegas", "title": "Unsupervised Adaptation for Synthetic-to-Real Handwritten Word\n  Recognition", "comments": "Accepted to WACV 2020", "journal-ref": null, "doi": "10.1109/WACV45572.2020.9093392", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Handwritten Text Recognition (HTR) is still a challenging problem because it\nmust deal with two important difficulties: the variability among writing\nstyles, and the scarcity of labelled data. To alleviate such problems,\nsynthetic data generation and data augmentation are typically used to train HTR\nsystems. However, training with such data produces encouraging but still\ninaccurate transcriptions in real words. In this paper, we propose an\nunsupervised writer adaptation approach that is able to automatically adjust a\ngeneric handwritten word recognizer, fully trained with synthetic fonts,\ntowards a new incoming writer. We have experimentally validated our proposal\nusing five different datasets, covering several challenges (i) the document\nsource: modern and historic samples, which may involve paper degradation\nproblems; (ii) different handwriting styles: single and multiple writer\ncollections; and (iii) language, which involves different character\ncombinations. Across these challenging collections, we show that our system is\nable to maintain its performance, thus, it provides a practical and generic\napproach to deal with new document collections without requiring any expensive\nand tedious manual annotation step.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 14:32:41 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 21:15:08 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Kang", "Lei", ""], ["Rusi\u00f1ol", "Mar\u00e7al", ""], ["Forn\u00e9s", "Alicia", ""], ["Riba", "Pau", ""], ["Villegas", "Mauricio", ""]]}, {"id": "1909.08487", "submitter": "Matteo Dunnhofer", "authors": "Matteo Dunnhofer, Niki Martinel, Gian Luca Foresti, Christian\n  Micheloni", "title": "Visual Tracking by means of Deep Reinforcement Learning and an Expert\n  Demonstrator", "comments": "in 2019 IEEE/CVF International Conference on Computer Vision\n  Workshops (ICCVW) - VOT2019 Challenge Workshop", "journal-ref": null, "doi": "10.1109/ICCVW.2019.00282", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade many different algorithms have been proposed to track a\ngeneric object in videos. Their execution on recent large-scale video datasets\ncan produce a great amount of various tracking behaviours. New trends in\nReinforcement Learning showed that demonstrations of an expert agent can be\nefficiently used to speed-up the process of policy learning. Taking inspiration\nfrom such works and from the recent applications of Reinforcement Learning to\nvisual tracking, we propose two novel trackers, A3CT, which exploits\ndemonstrations of a state-of-the-art tracker to learn an effective tracking\npolicy, and A3CTD, that takes advantage of the same expert tracker to correct\nits behaviour during tracking. Through an extensive experimental validation on\nthe GOT-10k, OTB-100, LaSOT, UAV123 and VOT benchmarks, we show that the\nproposed trackers achieve state-of-the-art performance while running in\nreal-time.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 14:55:02 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Dunnhofer", "Matteo", ""], ["Martinel", "Niki", ""], ["Foresti", "Gian Luca", ""], ["Micheloni", "Christian", ""]]}, {"id": "1909.08490", "submitter": "Md. Abu Bakr Siddique", "authors": "Fathma Siddique, Shadman Sakib, Md. Abu Bakr Siddique", "title": "Recognition of Handwritten Digit using Convolutional Neural Network in\n  Python with Tensorflow and Comparison of Performance for Various Hidden\n  Layers", "comments": "To be published in 5th International Conference on Advances in\n  Electrical Engineering (ICAEE-2019)", "journal-ref": "2019 5th International Conference on Advances in Electrical\n  Engineering (ICAEE)", "doi": "10.1109/ICAEE48663.2019.8975496", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In recent times, with the increase of Artificial Neural Network (ANN), deep\nlearning has brought a dramatic twist in the field of machine learning by\nmaking it more artificially intelligent. Deep learning is remarkably used in\nvast ranges of fields because of its diverse range of applications such as\nsurveillance, health, medicine, sports, robotics, drones, etc. In deep\nlearning, Convolutional Neural Network (CNN) is at the center of spectacular\nadvances that mixes Artificial Neural Network (ANN) and up to date deep\nlearning strategies. It has been used broadly in pattern recognition, sentence\nclassification, speech recognition, face recognition, text categorization,\ndocument analysis, scene, and handwritten digit recognition. The goal of this\npaper is to observe the variation of accuracies of CNN to classify handwritten\ndigits using various numbers of hidden layers and epochs and to make the\ncomparison between the accuracies. For this performance evaluation of CNN, we\nperformed our experiment using Modified National Institute of Standards and\nTechnology (MNIST) dataset. Further, the network is trained using stochastic\ngradient descent and the backpropagation algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 13:44:38 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Siddique", "Fathma", ""], ["Sakib", "Shadman", ""], ["Siddique", "Md. Abu Bakr", ""]]}, {"id": "1909.08508", "submitter": "Fu-Jen Chu", "authors": "Yunzhi Lin, Chao Tang, Fu-Jen Chu and Patricio A. Vela", "title": "Using Synthetic Data and Deep Networks to Recognize Primitive Shapes for\n  Object Grasping", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A segmentation-based architecture is proposed to decompose objects into\nmultiple primitive shapes from monocular depth input for robotic manipulation.\nThe backbone deep network is trained on synthetic data with 6 classes of\nprimitive shapes generated by a simulation engine. Each primitive shape is\ndesigned with parametrized grasp families, permitting the pipeline to identify\nmultiple grasp candidates per shape primitive region. The grasps are priority\nordered via proposed ranking algorithm, with the first feasible one chosen for\nexecution. On task-free grasping of individual objects, the method achieves a\n94% success rate. On task-oriented grasping, it achieves a 76% success rate.\nOverall, the method supports the hypothesis that shape primitives can support\ntask-free and task-relevant grasp prediction.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 20:05:57 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Lin", "Yunzhi", ""], ["Tang", "Chao", ""], ["Chu", "Fu-Jen", ""], ["Vela", "Patricio A.", ""]]}, {"id": "1909.08528", "submitter": "Peyman Hosseinzadeh Kassani", "authors": "Sara Hosseinzadeh Kassani, Farhood Rismanchian, Peyman Hosseinzadeh\n  Kassani", "title": "k-Relevance Vectors: Considering Relevancy Beside Nearness", "comments": "Will be submitted to Applied Soft Computing Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study combines two different learning paradigms, k-nearest neighbor\n(k-NN) rule, as memory-based learning paradigm and relevance vector machines\n(RVM), as statistical learning paradigm. This combination is performed in\nkernel space and is called k-relevance vector (k-RV). The purpose is to improve\nthe performance of k-NN rule. The proposed model significantly prunes\nirrelevant attributes. We also introduced a new parameter, responsible for\nearly stopping of iterations in RVM. We show that the new parameter improves\nthe classification accuracy of k-RV. Intensive experiments are conducted on\nseveral classification datasets from University of California Irvine (UCI)\nrepository and two real datasets from computer vision domain. The performance\nof k-RV is highly competitive compared to a few state-of-the-arts in terms of\nclassification accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 16:03:35 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 21:34:57 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Kassani", "Sara Hosseinzadeh", ""], ["Rismanchian", "Farhood", ""], ["Kassani", "Peyman Hosseinzadeh", ""]]}, {"id": "1909.08537", "submitter": "Chengyao Li", "authors": "Chengyao Li and Steven L. Waslander", "title": "Visual Measurement Integrity Monitoring for UAV Localization", "comments": "Published in Safety, Security, and Rescue Robotics 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned aerial vehicles (UAVs) have increasingly been adopted for safety,\nsecurity, and rescue missions, for which they need precise and reliable pose\nestimates relative to their environment. To ensure mission safety when relying\non visual perception, it is essential to have an approach to assess the\nintegrity of the visual localization solution. However, to the best of our\nknowledge, such an approach does not exist for optimization-based visual\nlocalization. Receiver autonomous integrity monitoring (RAIM) has been widely\nused in global navigation satellite systems (GNSS) applications such as\nautomated aircraft landing. In this paper, we propose a novel approach inspired\nby RAIM to monitor the integrity of optimization-based visual localization and\ncalculate the protection level of a state estimate, i.e. the largest possible\ntranslational error in each direction. We also propose a metric that\nquantitatively evaluates the performance of the error bounds. Finally, we\nvalidate the protection level using the EuRoC dataset and demonstrate that the\nproposed protection level provides a significantly more reliable bound than the\ncommonly used $3\\sigma$ method.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 16:08:41 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Li", "Chengyao", ""], ["Waslander", "Steven L.", ""]]}, {"id": "1909.08542", "submitter": "Samarth Shukla", "authors": "Samarth Shukla, Luc Van Gool, Radu Timofte", "title": "Extremely Weak Supervised Image-to-Image Translation for Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in generative models and adversarial training have led to a\nflourishing image-to-image (I2I) translation literature. The current I2I\ntranslation approaches require training images from the two domains that are\neither all paired (supervised) or all unpaired (unsupervised). In practice,\nobtaining paired training data in sufficient quantities is often very costly\nand cumbersome. Therefore solutions that employ unpaired data, while less\naccurate, are largely preferred. In this paper, we aim to bridge the gap\nbetween supervised and unsupervised I2I translation, with application to\nsemantic image segmentation. We build upon pix2pix and CycleGAN,\nstate-of-the-art seminal I2I translation techniques. We propose a method to\nselect (very few) paired training samples and achieve significant improvements\nin both supervised and unsupervised I2I translation settings over random\nselection. Further, we boost the performance by incorporating both (selected)\npaired and unpaired samples in the training process. Our experiments show that\nan extremely weak supervised I2I translation solution using only one paired\ntraining sample can achieve a quantitative performance much better than the\nunsupervised CycleGAN model, and comparable to that of the supervised pix2pix\nmodel trained on thousands of pairs.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 16:09:41 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Shukla", "Samarth", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "1909.08580", "submitter": "Chuchu Han", "authors": "Chuchu Han, Jiacheng Ye, Yunshan Zhong, Xin Tan, Chi Zhang, Changxin\n  Gao and Nong Sang", "title": "Re-ID Driven Localization Refinement for Person Search", "comments": "10 pages, 7 figures. Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person search aims at localizing and identifying a query person from a\ngallery of uncropped scene images. Different from person re-identification\n(re-ID), its performance also depends on the localization accuracy of a\npedestrian detector. The state-of-the-art methods train the detector\nindividually, and the detected bounding boxes may be sub-optimal for the\nfollowing re-ID task. To alleviate this issue, we propose a re-ID driven\nlocalization refinement framework for providing the refined detection boxes for\nperson search. Specifically, we develop a differentiable ROI transform layer to\neffectively transform the bounding boxes from the original images. Thus, the\nbox coordinates can be supervised by the re-ID training other than the original\ndetection task. With this supervision, the detector can generate more reliable\nbounding boxes, and the downstream re-ID model can produce more discriminative\nembeddings based on the refined person localizations. Extensive experimental\nresults on the widely used benchmarks demonstrate that our proposed method\nperforms favorably against the state-of-the-art person search methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 17:10:49 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Han", "Chuchu", ""], ["Ye", "Jiacheng", ""], ["Zhong", "Yunshan", ""], ["Tan", "Xin", ""], ["Zhang", "Chi", ""], ["Gao", "Changxin", ""], ["Sang", "Nong", ""]]}, {"id": "1909.08599", "submitter": "Mengyu Liu", "authors": "Mengyu Liu, Hujun Yin", "title": "Feature Pyramid Encoding Network for Real-time Semantic Segmentation", "comments": "Accepted to BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although current deep learning methods have achieved impressive results for\nsemantic segmentation, they incur high computational costs and have a huge\nnumber of parameters. For real-time applications, inference speed and memory\nusage are two important factors. To address the challenge, we propose a\nlightweight feature pyramid encoding network (FPENet) to make a good trade-off\nbetween accuracy and speed. Specifically, we use a feature pyramid encoding\nblock to encode multi-scale contextual features with depthwise dilated\nconvolutions in all stages of the encoder. A mutual embedding upsample module\nis introduced in the decoder to aggregate the high-level semantic features and\nlow-level spatial details efficiently. The proposed network outperforms\nexisting real-time methods with fewer parameters and improved inference speed\non the Cityscapes and CamVid benchmark datasets. Specifically, FPENet achieves\n68.0\\% mean IoU on the Cityscapes test set with only 0.4M parameters and 102\nFPS speed on an NVIDIA TITAN V GPU.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 17:40:06 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Liu", "Mengyu", ""], ["Yin", "Hujun", ""]]}, {"id": "1909.08605", "submitter": "Heng Yang", "authors": "Heng Yang, Pasquale Antonante, Vasileios Tzoumas, Luca Carlone", "title": "Graduated Non-Convexity for Robust Spatial Perception: From Non-Minimal\n  Solvers to Global Outlier Rejection", "comments": "10 pages, 5 figures, published at IEEE Robotics and Automation\n  Letters (RA-L), 2020, Best Paper Award in Robot Vision at ICRA 2020", "journal-ref": "IEEE Robotics and Automation Letters (RA-L), 2020", "doi": "10.1109/LRA.2020.2965893", "report-no": null, "categories": "cs.CV cs.RO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semidefinite Programming (SDP) and Sums-of-Squares (SOS) relaxations have led\nto certifiably optimal non-minimal solvers for several robotics and computer\nvision problems. However, most non-minimal solvers rely on least-squares\nformulations, and, as a result, are brittle against outliers. While a standard\napproach to regain robustness against outliers is to use robust cost functions,\nthe latter typically introduce other non-convexities, preventing the use of\nexisting non-minimal solvers. In this paper, we enable the simultaneous use of\nnon-minimal solvers and robust estimation by providing a general-purpose\napproach for robust global estimation, which can be applied to any problem\nwhere a non-minimal solver is available for the outlier-free case. To this end,\nwe leverage the Black-Rangarajan duality between robust estimation and outlier\nprocesses (which has been traditionally applied to early vision problems), and\nshow that graduated non-convexity (GNC) can be used in conjunction with\nnon-minimal solvers to compute robust solutions, without requiring an initial\nguess. Although GNC's global optimality cannot be guaranteed, we demonstrate\nthe empirical robustness of the resulting robust non-minimal solvers in\napplications, including point cloud and mesh registration, pose graph\noptimization, and image-based object pose estimation (also called shape\nalignment). Our solvers are robust to 70-80% of outliers, outperform RANSAC,\nare more accurate than specialized local solvers, and faster than specialized\nglobal solvers. We also propose the first certifiably optimal non-minimal\nsolver for shape alignment using SOS relaxation.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 17:50:04 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 01:36:34 GMT"}, {"version": "v3", "created": "Tue, 21 Jan 2020 19:20:41 GMT"}, {"version": "v4", "created": "Thu, 11 Jun 2020 20:09:58 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Yang", "Heng", ""], ["Antonante", "Pasquale", ""], ["Tzoumas", "Vasileios", ""], ["Carlone", "Luca", ""]]}, {"id": "1909.08606", "submitter": "Tejo Chalasani", "authors": "Tejo Chalasani and Aljosa Smolic", "title": "Simultaneous Segmentation and Recognition: Towards more accurate Ego\n  Gesture Recognition", "comments": "Accepted at ICCV Workshop for Egocentric Perception, Interaction and\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ego hand gestures can be used as an interface in AR and VR environments.\nWhile the context of an image is important for tasks like scene understanding,\nobject recognition, image caption generation and activity recognition, it plays\na minimal role in ego hand gesture recognition. An ego hand gesture used for AR\nand VR environments conveys the same information regardless of the background.\nWith this idea in mind, we present our work on ego hand gesture recognition\nthat produces embeddings from RBG images with ego hands, which are\nsimultaneously used for ego hand segmentation and ego gesture recognition. To\nthis extent, we achieved better recognition accuracy (96.9%) compared to the\nstate of the art (92.2%) on the biggest ego hand gesture dataset available\npublicly. We present a gesture recognition deep neural network which recognises\nego hand gestures from videos (videos containing a single gesture) by\ngenerating and recognising embeddings of ego hands from image sequences of\nvarying length. We introduce the concept of simultaneous segmentation and\nrecognition applied to ego hand gestures, present the network architecture, the\ntraining procedure and the results compared to the state of the art on the\nEgoGesture dataset\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 17:52:16 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Chalasani", "Tejo", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1909.08611", "submitter": "Alexandros Stergiou MSc", "authors": "Alexandros Stergiou, Georgios Kapidis, Grigorios Kalliatakis, Christos\n  Chrysoulas, Ronald Poppe, Remco Veltkamp", "title": "Class Feature Pyramids for Video Explanation", "comments": null, "journal-ref": null, "doi": "10.1109/ICCVW.2019.00524", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks are widely used in video action recognition. 3D\nconvolutions are one prominent approach to deal with the additional time\ndimension. While 3D convolutions typically lead to higher accuracies, the inner\nworkings of the trained models are more difficult to interpret. We focus on\ncreating human-understandable visual explanations that represent the\nhierarchical parts of spatio-temporal networks. We introduce Class Feature\nPyramids, a method that traverses the entire network structure and\nincrementally discovers kernels at different network depths that are\ninformative for a specific class. Our method does not depend on the network's\narchitecture or the type of 3D convolutions, supporting grouped and depth-wise\nconvolutions, convolutions in fibers, and convolutions in branches. We\ndemonstrate the method on six state-of-the-art 3D convolution neural networks\n(CNNs) on three action recognition (Kinetics-400, UCF-101, and HMDB-51) and two\negocentric action recognition datasets (EPIC-Kitchens and EGTEA Gaze+).\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:10:32 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Stergiou", "Alexandros", ""], ["Kapidis", "Georgios", ""], ["Kalliatakis", "Grigorios", ""], ["Chrysoulas", "Christos", ""], ["Poppe", "Ronald", ""], ["Veltkamp", "Remco", ""]]}, {"id": "1909.08612", "submitter": "Arnaud Belletoile PhD", "authors": "Arnaud Bell\\'etoile (for the Cdiscount datascience team)", "title": "Large e-retailer image dataset for visual search and product\n  classification", "comments": "5 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent results of deep convolutional networks in visual recognition\nchallenges open the path to a whole new set of disruptive user experiences such\nas visual search or recommendation. The list of companies offering this type of\nservice is growing everyday but the adoption rate and the relevancy of results\nmay vary a lot. We believe that the availability of large and diverse datasets\nis a necessary condition to improve the relevancy of such recommendation\nsystems and facilitate their adoption. For that purpose, we wish to share with\nthe community this dataset of more than 12M images of the 7M products of our\nonline store classified into 5K categories. This original dataset is introduced\nin this article and several features are described. We also present some\naspects of the winning solutions of our image classification challenge that was\norganized on the Kaggle platform around this set of images.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 09:21:53 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Bell\u00e9toile", "Arnaud", "", "for the Cdiscount datascience team"]]}, {"id": "1909.08675", "submitter": "Prudhvi Gurram", "authors": "Pengcheng Xu, Prudhvi Gurram, Gene Whipps, Rama Chellappa", "title": "Wasserstein Distance Based Domain Adaptation for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an adversarial unsupervised domain adaptation\nframework for object detection. Prior approaches utilize adversarial training\nbased on cross entropy between the source and target domain distributions to\nlearn a shared feature mapping that minimizes the domain gap. Here, we minimize\nthe Wasserstein distance between the two distributions instead of cross entropy\nor Jensen-Shannon divergence to improve the stability of domain adaptation in\nhigh-dimensional feature spaces that are inherent to object detection task.\nAdditionally, we remove the exact consistency constraint of the shared feature\nmapping between the source and target domains, so that the target feature\nmapping can be optimized independently, which is necessary in the case of\nsignificant domain gap. We empirically show that the proposed framework can\nmitigate domain shift in different scenarios, and provide improved target\ndomain object detection performance.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 20:00:31 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Xu", "Pengcheng", ""], ["Gurram", "Prudhvi", ""], ["Whipps", "Gene", ""], ["Chellappa", "Rama", ""]]}, {"id": "1909.08685", "submitter": "Muhammad Kamran Janjua", "authors": "Shah Nawaz, Muhammad Kamran Janjua, Ignazio Gallo, Arif Mahmood,\n  Alessandro Calefati", "title": "Deep Latent Space Learning for Cross-modal Mapping of Audio and Visual\n  Signals", "comments": "Accepted to DICTA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep training algorithm for joint representation of audio\nand visual information which consists of a single stream network (SSNet)\ncoupled with a novel loss function to learn a shared deep latent space\nrepresentation of multimodal information. The proposed framework characterizes\nthe shared latent space by leveraging the class centers which helps to\neliminate the need for pairwise or triplet supervision. We quantitatively and\nqualitatively evaluate the proposed approach on VoxCeleb, a benchmarks\naudio-visual dataset on a multitude of tasks including cross-modal\nverification, cross-modal matching, and cross-modal retrieval. State-of-the-art\nperformance is achieved on cross-modal verification and matching while\ncomparable results are observed on the remaining applications. Our experiments\ndemonstrate the effectiveness of the technique for cross-modal biometric\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 20:18:44 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Nawaz", "Shah", ""], ["Janjua", "Muhammad Kamran", ""], ["Gallo", "Ignazio", ""], ["Mahmood", "Arif", ""], ["Calefati", "Alessandro", ""]]}, {"id": "1909.08729", "submitter": "Han Wang", "authors": "Han Wang and Shangyu Xie and Yuan Hong", "title": "VideoDP: A Universal Platform for Video Analytics with Differential\n  Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive amounts of video data are ubiquitously generated in personal devices\nand dedicated video recording facilities. Analyzing such data would be\nextremely beneficial in real world (e.g., urban traffic analysis, pedestrian\nbehavior analysis, video surveillance). However, videos contain considerable\nsensitive information, such as human faces, identities and activities. Most of\nthe existing video sanitization techniques simply obfuscate the video by\ndetecting and blurring the region of interests (e.g., faces, vehicle plates,\nlocations and timestamps) without quantifying and bounding the privacy leakage\nin the sanitization. In this paper, to the best of our knowledge, we propose\nthe first differentially private video analytics platform (VideoDP) which\nflexibly supports different video analyses with rigorous privacy guarantee.\nDifferent from traditional noise-injection based differentially private\nmechanisms, given the input video, VideoDP randomly generates a utility-driven\nprivate video in which adding or removing any sensitive visual element (e.g.,\nhuman, object) does not significantly affect the output video. Then, different\nvideo analyses requested by untrusted video analysts can be flexibly performed\nover the utility-driven video while ensuring differential privacy. Finally, we\nconduct experiments on real videos, and the experimental results demonstrate\nthat our VideoDP effectively functions video analytics with good utility.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 22:36:40 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 16:47:47 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Wang", "Han", ""], ["Xie", "Shangyu", ""], ["Hong", "Yuan", ""]]}, {"id": "1909.08745", "submitter": "Giang Nguyen", "authors": "Giang Nguyen, Tae Joon Jun, Trung Tran, Tolcha Yalew, Daeyoung Kim", "title": "ContCap: A scalable framework for continual image captioning", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While advanced image captioning systems are increasingly describing images\ncoherently and exactly, recent progress in continual learning allows deep\nlearning models to avoid catastrophic forgetting. However, the domain where\nimage captioning working with continual learning has not yet been explored. We\ndefine the task in which we consolidate continual learning and image captioning\nas continual image captioning. In this work, we propose ContCap, a framework\ngenerating captions over a series of new tasks coming, seamlessly integrating\ncontinual learning into image captioning besides addressing catastrophic\nforgetting. After proving forgetting in image captioning, we propose various\ntechniques to overcome the forgetting dilemma by taking a simple fine-tuning\nschema as the baseline. We split MS-COCO 2014 dataset to perform experiments in\nclass-incremental settings without revisiting dataset of previously provided\ntasks. Experiments show remarkable improvements in the performance on the old\ntasks while the figures for the new surprisingly surpass fine-tuning. Our\nframework also offers a scalable solution for continual image or video\ncaptioning.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 00:31:17 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 02:56:33 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Nguyen", "Giang", ""], ["Jun", "Tae Joon", ""], ["Tran", "Trung", ""], ["Yalew", "Tolcha", ""], ["Kim", "Daeyoung", ""]]}, {"id": "1909.08754", "submitter": "Yuwei Yang", "authors": "Yuwei Yang, Fanman Meng, Hongliang Li, King N.Ngan, Qingbo Wu", "title": "A New Few-shot Segmentation Network Based on Class Representation", "comments": "accepted by VCIP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies few-shot segmentation, which is a task of predicting\nforeground mask of unseen classes by a few of annotations only, aided by a set\nof rich annotations already existed. The existing methods mainly focus the task\non \"\\textit{how to transfer segmentation cues from support images (labeled\nimages) to query images (unlabeled images)}\", and try to learn efficient and\ngeneral transfer module that can be easily extended to unseen classes. However,\nit is proved to be a challenging task to learn the transfer module that is\ngeneral to various classes. This paper solves few-shot segmentation in a new\nperspective of \"\\textit{how to represent unseen classes by existing classes}\",\nand formulates few-shot segmentation as the representation process that\nrepresents unseen classes (in terms of forming the foreground prior) by\nexisting classes precisely. Based on such idea, we propose a new class\nrepresentation based few-shot segmentation framework, which firstly generates\nclass activation map of unseen class based on the knowledge of existing\nclasses, and then uses the map as foreground probability map to extract the\nforegrounds from query image. A new two-branch based few-shot segmentation\nnetwork is proposed. Moreover, a new CAM generation module that extracts the\nCAM of unseen classes rather than the classical training classes is raised. We\nvalidate the effectiveness of our method on Pascal VOC 2012 dataset, the value\nFB-IoU of one-shot and five-shot arrives at 69.2\\% and 70.1\\% respectively,\nwhich outperforms the state-of-the-art method.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 00:59:56 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Yang", "Yuwei", ""], ["Meng", "Fanman", ""], ["Li", "Hongliang", ""], ["Ngan", "King N.", ""], ["Wu", "Qingbo", ""]]}, {"id": "1909.08766", "submitter": "Deepali Aneja", "authors": "Deepali Aneja, Daniel McDuff, Shital Shah", "title": "A High-Fidelity Open Embodied Avatar with Lip Syncing and Expression\n  Capabilities", "comments": "International Conference on Multimodal Interaction (ICMI 2019)", "journal-ref": null, "doi": "10.1145/3340555.3353744", "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodied avatars as virtual agents have many applications and provide\nbenefits over disembodied agents, allowing non-verbal social and interactional\ncues to be leveraged, in a similar manner to how humans interact with each\nother. We present an open embodied avatar built upon the Unreal Engine that can\nbe controlled via a simple python programming interface. The avatar has lip\nsyncing (phoneme control), head gesture and facial expression (using either\nfacial action units or cardinal emotion categories) capabilities. We release\ncode and models to illustrate how the avatar can be controlled like a puppet or\nused to create a simple conversational agent using public application\nprogramming interfaces (APIs). GITHUB link:\nhttps://github.com/danmcduff/AvatarSim\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 01:39:39 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 05:10:21 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Aneja", "Deepali", ""], ["McDuff", "Daniel", ""], ["Shah", "Shital", ""]]}, {"id": "1909.08774", "submitter": "Nagender Aneja", "authors": "Nagender Aneja and Sandhya Aneja", "title": "Transfer Learning using CNN for Handwritten Devanagari Character\n  Recognition", "comments": null, "journal-ref": "IEEE International Conference on Advances in Information\n  Technology (ICAIT), ICAIT - 2019", "doi": "10.1109/ICAIT47043.2019.8987286", "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an analysis of pre-trained models to recognize\nhandwritten Devanagari alphabets using transfer learning for Deep Convolution\nNeural Network (DCNN). This research implements AlexNet, DenseNet, Vgg, and\nInception ConvNet as a fixed feature extractor. We implemented 15 epochs for\neach of AlexNet, DenseNet 121, DenseNet 201, Vgg 11, Vgg 16, Vgg 19, and\nInception V3. Results show that Inception V3 performs better in terms of\naccuracy achieving 99% accuracy with average epoch time 16.3 minutes while\nAlexNet performs fastest with 2.2 minutes per epoch and achieving 98\\%\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 02:04:55 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Aneja", "Nagender", ""], ["Aneja", "Sandhya", ""]]}, {"id": "1909.08782", "submitter": "Gabriel Ilharco", "authors": "Gabriel Ilharco, Yuan Zhang, Jason Baldridge", "title": "Large-scale representation learning from visually grounded untranscribed\n  speech", "comments": null, "journal-ref": "The SIGNLL Conference on Computational Natural Language Learning\n  (CoNLL), 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems that can associate images with their spoken audio captions are an\nimportant step towards visually grounded language learning. We describe a\nscalable method to automatically generate diverse audio for image captioning\ndatasets. This supports pretraining deep networks for encoding both audio and\nimages, which we do via a dual encoder that learns to align latent\nrepresentations from both modalities. We show that a masked margin softmax loss\nfor such models is superior to the standard triplet loss. We fine-tune these\nmodels on the Flickr8k Audio Captions Corpus and obtain state-of-the-art\nresults---improving recall in the top 10 from 29.6% to 49.5%. We also obtain\nhuman ratings on retrieval outputs to better assess the impact of incidentally\nmatching image-caption pairs that were not associated in the data, finding that\nautomatic evaluation substantially underestimates the quality of the retrieved\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 02:50:23 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Ilharco", "Gabriel", ""], ["Zhang", "Yuan", ""], ["Baldridge", "Jason", ""]]}, {"id": "1909.08797", "submitter": "Cong Hu", "authors": "Cong Hu, Zhen-Hua Feng, Xiao-Jun Wu, Josef Kittler", "title": "Dual Encoder-Decoder based Generative Adversarial Networks for\n  Disentangled Facial Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To learn disentangled representations of facial images, we present a Dual\nEncoder-Decoder based Generative Adversarial Network (DED-GAN). In the proposed\nmethod, both the generator and discriminator are designed with deep\nencoder-decoder architectures as their backbones. To be more specific, the\nencoder-decoder structured generator is used to learn a pose disentangled face\nrepresentation, and the encoder-decoder structured discriminator is tasked to\nperform real/fake classification, face reconstruction, determining identity and\nestimating face pose. We further improve the proposed network architecture by\nminimising the additional pixel-wise loss defined by the Wasserstein distance\nat the output of the discriminator so that the adversarial framework can be\nbetter trained. Additionally, we consider face pose variation to be continuous,\nrather than discrete in existing literature, to inject richer pose information\ninto our model. The pose estimation task is formulated as a regression problem,\nwhich helps to disentangle identity information from pose variations. The\nproposed network is evaluated on the tasks of pose-invariant face recognition\n(PIFR) and face synthesis across poses. An extensive quantitative and\nqualitative evaluation carried out on several controlled and in-the-wild\nbenchmarking datasets demonstrates the superiority of the proposed DED-GAN\nmethod over the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 04:19:21 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Hu", "Cong", ""], ["Feng", "Zhen-Hua", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "1909.08830", "submitter": "Sekitoshi Kanai", "authors": "Sekitoshi Kanai, Yasutoshi Ida, Yasuhiro Fujiwara, Masanori Yamada,\n  Shuichi Adachi", "title": "Absum: Simple Regularization Method for Reducing Structural Sensitivity\n  of Convolutional Neural Networks", "comments": "16 pages, 39 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Absum, which is a regularization method for improving adversarial\nrobustness of convolutional neural networks (CNNs). Although CNNs can\naccurately recognize images, recent studies have shown that the convolution\noperations in CNNs commonly have structural sensitivity to specific noise\ncomposed of Fourier basis functions. By exploiting this sensitivity, they\nproposed a simple black-box adversarial attack: Single Fourier attack. To\nreduce structural sensitivity, we can use regularization of convolution filter\nweights since the sensitivity of linear transform can be assessed by the norm\nof the weights. However, standard regularization methods can prevent\nminimization of the loss function because they impose a tight constraint for\nobtaining high robustness. To solve this problem, Absum imposes a loose\nconstraint; it penalizes the absolute values of the summation of the parameters\nin the convolution layers. Absum can improve robustness against single Fourier\nattack while being as simple and efficient as standard regularization methods\n(e.g., weight decay and L1 regularization). Our experiments demonstrate that\nAbsum improves robustness against single Fourier attack more than standard\nregularization methods. Furthermore, we reveal that robust CNNs with Absum are\nmore robust against transferred attacks due to decreasing the common\nsensitivity and against high-frequency noise than standard regularization\nmethods. We also reveal that Absum can improve robustness against\ngradient-based attacks (projected gradient descent) when used with adversarial\ntraining.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 07:05:14 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Kanai", "Sekitoshi", ""], ["Ida", "Yasutoshi", ""], ["Fujiwara", "Yasuhiro", ""], ["Yamada", "Masanori", ""], ["Adachi", "Shuichi", ""]]}, {"id": "1909.08840", "submitter": "Lamberto Ballan", "authors": "Matteo Lisotto, Pasquale Coscia, Lamberto Ballan", "title": "Social and Scene-Aware Trajectory Prediction in Crowded Spaces", "comments": "Accepted to ICCV 2019 Workshop on Assistive Computer Vision and\n  Robotics (ACVR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mimicking human ability to forecast future positions or interpret complex\ninteractions in urban scenarios, such as streets, shopping malls or squares, is\nessential to develop socially compliant robots or self-driving cars. Autonomous\nsystems may gain advantage on anticipating human motion to avoid collisions or\nto naturally behave alongside people. To foresee plausible trajectories, we\nconstruct an LSTM (long short-term memory)-based model considering three\nfundamental factors: people interactions, past observations in terms of\npreviously crossed areas and semantics of surrounding space. Our model\nencompasses several pooling mechanisms to join the above elements defining\nmultiple tensors, namely social, navigation and semantic tensors. The network\nis tested in unstructured environments where complex paths emerge according to\nboth internal (intentions) and external (other people, not accessible areas)\nmotivations. As demonstrated, modeling paths unaware of social interactions or\ncontext information, is insufficient to correctly predict future positions.\nExperimental results corroborate the effectiveness of the proposed framework in\ncomparison to LSTM-based models for human path prediction.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 08:03:02 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Lisotto", "Matteo", ""], ["Coscia", "Pasquale", ""], ["Ballan", "Lamberto", ""]]}, {"id": "1909.08842", "submitter": "Eyal Rozenberg", "authors": "Eyal Rozenberg, Daniel Freedman, Alex Bronstein", "title": "Localization with Limited Annotation for Chest X-rays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Localization of an object within an image is a common task in medical\nimaging. Learning to localize or detect objects typically requires the\ncollection of data which has been labelled with bounding boxes or similar\nannotations, which can be very time consuming and expensive. A technique which\ncould perform such learning with much less annotation would, therefore, be\nquite valuable. We present such a technique for localization with limited\nannotation, in which the number of images with bounding boxes can be a small\nfraction of the total dataset (e.g. less than 1%); all other images only\npossess a whole image label and no bounding box. We propose a novel loss\nfunction for tackling this problem; the loss is a continuous relaxation of a\nwell-defined discrete formulation of weakly supervised learning and is\nnumerically well-posed. Furthermore, we propose a new architecture which\naccounts for both patch dependence and shift-invariance, through the inclusion\nof CRF layers and anti-aliasing filters, respectively. We apply our technique\nto the localization of thoracic diseases in chest X-ray images and demonstrate\nstate-of-the-art localization performance on the ChestX-ray14 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 08:07:28 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 14:05:44 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Rozenberg", "Eyal", ""], ["Freedman", "Daniel", ""], ["Bronstein", "Alex", ""]]}, {"id": "1909.08845", "submitter": "L\\'eo Hemamou", "authors": "L\\'eo Hemamou, Ghazi Felhi, Jean-Claude Martin and Chlo\\'e Clavel", "title": "Slices of Attention in Asynchronous Video Job Interviews", "comments": "Accepted at 2019 8th International Conference on Affective Computing\n  and Intelligent Interaction (ACII)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The impact of non verbal behaviour in a hiring decision remains an open\nquestion. Investigating this question is important, as it could provide a\nbetter understanding on how to train candidates for job interviews and make\nrecruiters be aware of influential non verbal behaviour. This research has\nrecently been accelerated due to the development of tools for the automatic\nanalysis of social signals, and the emergence of machine learning methods.\nHowever, these studies are still mainly based on hand engineered features,\nwhich imposes a limit to the discovery of influential social signals. On the\nother side, deep learning methods are a promising tool to discover complex\npatterns without the necessity of feature engineering. In this paper, we focus\non studying influential non verbal social signals in asynchronous job video\ninterviews that are discovered by deep learning methods. We use a previously\npublished deep learning system that aims at inferring the hirability of a\ncandidate with regard to a sequence of interview questions. One particularity\nof this system is the use of attention mechanisms, which aim at identifying the\nrelevant parts of an answer. Thus, information at a fine-grained temporal level\ncould be extracted using global (at the interview level) annotations on\nhirability. While most of the deep learning systems use attention mechanisms to\noffer a quick visualization of slices when a rise of attention occurs, we\nperform an in-depth analysis to understand what happens during these moments.\nFirst, we propose a methodology to automatically extract slices where there is\na rise of attention (attention slices). Second, we study the content of\nattention slices by comparing them with randomly sampled slices. Finally, we\nshow that they bear significantly more information for hirability than randomly\nsampled slices.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 08:15:46 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Hemamou", "L\u00e9o", ""], ["Felhi", "Ghazi", ""], ["Martin", "Jean-Claude", ""], ["Clavel", "Chlo\u00e9", ""]]}, {"id": "1909.08848", "submitter": "Anjith George", "authors": "Anjith George and Zohreh Mostaani and David Geissenbuhler and Olegs\n  Nikisins and Andre Anjos and Sebastien Marcel", "title": "Biometric Face Presentation Attack Detection with Multi-Channel\n  Convolutional Neural Network", "comments": "16 pages", "journal-ref": "IEEE Transactions on Information Forensics and Security, 2019", "doi": "10.1109/TIFS.2019.2916652", "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition is a mainstream biometric authentication method. However,\nvulnerability to presentation attacks (a.k.a spoofing) limits its usability in\nunsupervised applications. Even though there are many methods available for\ntackling presentation attacks (PA), most of them fail to detect sophisticated\nattacks such as silicone masks. As the quality of presentation attack\ninstruments improves over time, achieving reliable PA detection with visual\nspectra alone remains very challenging. We argue that analysis in multiple\nchannels might help to address this issue. In this context, we propose a\nmulti-channel Convolutional Neural Network based approach for presentation\nattack detection (PAD). We also introduce the new Wide Multi-Channel\npresentation Attack (WMCA) database for face PAD which contains a wide variety\nof 2D and 3D presentation attacks for both impersonation and obfuscation\nattacks. Data from different channels such as color, depth, near-infrared and\nthermal are available to advance the research in face PAD. The proposed method\nwas compared with feature-based approaches and found to outperform the\nbaselines achieving an ACER of 0.3% on the introduced dataset. The database and\nthe software to reproduce the results are made available publicly.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 08:16:35 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["George", "Anjith", ""], ["Mostaani", "Zohreh", ""], ["Geissenbuhler", "David", ""], ["Nikisins", "Olegs", ""], ["Anjos", "Andre", ""], ["Marcel", "Sebastien", ""]]}, {"id": "1909.08856", "submitter": "Fabian Eitel", "authors": "Fabian Eitel, Kerstin Ritter", "title": "Testing the robustness of attribution methods for convolutional neural\n  networks in MRI-based Alzheimer's disease classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Attribution methods are an easy to use tool for investigating and validating\nmachine learning models. Multiple methods have been suggested in the literature\nand it is not yet clear which method is most suitable for a given task. In this\nstudy, we tested the robustness of four attribution methods, namely\ngradient*input, guided backpropagation, layer-wise relevance propagation and\nocclusion, for the task of Alzheimer's disease classification. We have\nrepeatedly trained a convolutional neural network (CNN) with identical training\nsettings in order to separate structural MRI data of patients with Alzheimer's\ndisease and healthy controls. Afterwards, we produced attribution maps for each\nsubject in the test data and quantitatively compared them across models and\nattribution methods. We show that visual comparison is not sufficient and that\nsome widely used attribution methods produce highly inconsistent outcomes.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 08:30:31 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Eitel", "Fabian", ""], ["Ritter", "Kerstin", ""]]}, {"id": "1909.08859", "submitter": "Aykut Erdem", "authors": "Mustafa Sercan Amac, Semih Yagcioglu, Aykut Erdem, Erkut Erdem", "title": "Procedural Reasoning Networks for Understanding Multimodal Procedures", "comments": "Accepted to CoNLL 2019. The project website with code and demo is\n  available at https://hucvl.github.io/prn/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of comprehending procedural commonsense\nknowledge. This is a challenging task as it requires identifying key entities,\nkeeping track of their state changes, and understanding temporal and causal\nrelations. Contrary to most of the previous work, in this study, we do not rely\non strong inductive bias and explore the question of how multimodality can be\nexploited to provide a complementary semantic signal. Towards this end, we\nintroduce a new entity-aware neural comprehension model augmented with external\nrelational memory units. Our model learns to dynamically update entity states\nin relation to each other while reading the text instructions. Our experimental\nanalysis on the visual reasoning tasks in the recently proposed RecipeQA\ndataset reveals that our approach improves the accuracy of the previously\nreported models by a large margin. Moreover, we find that our model learns\neffective dynamic representations of entities even though we do not use any\nsupervision at the level of entity states.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 08:39:00 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Amac", "Mustafa Sercan", ""], ["Yagcioglu", "Semih", ""], ["Erdem", "Aykut", ""], ["Erdem", "Erkut", ""]]}, {"id": "1909.08862", "submitter": "Jie Zhao", "authors": "Jie Zhao, Xin Jiang, Xiaoman Wang, Shengfan Wang and Yunhui Liu", "title": "Assembly of randomly placed parts realized by using only one robot arm\n  with a general parallel-jaw gripper", "comments": "Submitted in ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In industry assembly lines, parts feeding machines are widely employed as the\nprologue of the whole procedure. They play the role of sorting the parts\nrandomly placed in bins to the state with specified pose. With the help of the\nparts feeding machines, the subsequent assembly processes by robot arm can\nalways start from the same condition. Thus it is expected that function of\nparting feeding machine and the robotic assembly can be integrated with one\nrobot arm. This scheme can provide great flexibility and can also contribute to\nreduce the cost. The difficulties involved in this scheme lie in the fact that\nin the part feeding phase, the pose of the part after grasping may be not\nproper for the subsequent assembly. Sometimes it can not even guarantee a\nstable grasp. In this paper, we proposed a method to integrate parts feeding\nand assembly within one robot arm. This proposal utilizes a specially designed\ngripper tip mounted on the jaws of a two-fingered gripper. With the modified\ngripper, in-hand manipulation of the grasped object is realized, which can\nensure the control of the orientation and offset position of the grasped\nobject. The proposal in this paper is verified by a simulated assembly in which\na robot arm completed the assembly process including parts picking from bin and\na subsequent peg-in-hole assembly.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 08:43:58 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Zhao", "Jie", ""], ["Jiang", "Xin", ""], ["Wang", "Xiaoman", ""], ["Wang", "Shengfan", ""], ["Liu", "Yunhui", ""]]}, {"id": "1909.08866", "submitter": "Dimitri Gominski", "authors": "Dimitri Gominski (LaSTIG), Martyna Poreba (LaSTIG), Val\\'erie\n  Gouet-Brunet (LaSTIG), Liming Chen (LaSTIG)", "title": "Challenging deep image descriptors for retrieval in heterogeneous\n  iconographic collections", "comments": "SUMAC '19, 2019", "journal-ref": null, "doi": "10.1145/3347317.3357246", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes to study the behavior of recent and efficient\nstate-of-the-art deep-learning based image descriptors for content-based image\nretrieval, facing a panel of complex variations appearing in heterogeneous\nimage datasets, in particular in cultural collections that may involve\nmulti-source, multi-date and multi-view Permission to make digital\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 08:54:51 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Gominski", "Dimitri", "", "LaSTIG"], ["Poreba", "Martyna", "", "LaSTIG"], ["Gouet-Brunet", "Val\u00e9rie", "", "LaSTIG"], ["Chen", "Liming", "", "LaSTIG"]]}, {"id": "1909.08868", "submitter": "Jan-Nico Zaech", "authors": "Jan-Nico Zaech, Cong Gao, Bastian Bier, Russell Taylor, Andreas Maier,\n  Nassir Navab, Mathias Unberath", "title": "Learning to Avoid Poor Images: Towards Task-aware C-arm Cone-beam CT\n  Trajectories", "comments": "Accepted for oral presentation at the International Conference on\n  Medical Image Computing and Computer Assisted Intervention (MICCAI) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metal artifacts in computed tomography (CT) arise from a mismatch between\nphysics of image formation and idealized assumptions during tomographic\nreconstruction. These artifacts are particularly strong around metal implants,\ninhibiting widespread adoption of 3D cone-beam CT (CBCT) despite clear\nopportunity for intra-operative verification of implant positioning, e.g. in\nspinal fusion surgery. On synthetic and real data, we demonstrate that much of\nthe artifact can be avoided by acquiring better data for reconstruction in a\ntask-aware and patient-specific manner, and describe the first step towards the\nenvisioned task-aware CBCT protocol. The traditional short-scan CBCT trajectory\nis planar, with little room for scene-specific adjustment. We extend this\ntrajectory by autonomously adjusting out-of-plane angulation. This enables\nC-arm source trajectories that are scene-specific in that they avoid acquiring\n\"poor images\", characterized by beam hardening, photon starvation, and noise.\nThe recommendation of ideal out-of-plane angulation is performed on-the-fly\nusing a deep convolutional neural network that regresses a detectability-rank\nderived from imaging physics.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 09:00:44 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Zaech", "Jan-Nico", ""], ["Gao", "Cong", ""], ["Bier", "Bastian", ""], ["Taylor", "Russell", ""], ["Maier", "Andreas", ""], ["Navab", "Nassir", ""], ["Unberath", "Mathias", ""]]}, {"id": "1909.08869", "submitter": "Yangzhe Liu", "authors": "Yongbing Zhang, Yangzhe Liu, Xiu Li, Shaowei Jiang, Krishna Dixit,\n  Xinfeng Zhang and Xiangyang Ji", "title": "PgNN: Physics-guided Neural Network for Fourier Ptychographic Microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fourier ptychography (FP) is a newly developed computational imaging approach\nthat achieves both high resolution and wide field of view by stitching a series\nof low-resolution images captured under angle-varied illumination. So far, many\nsupervised data-driven models have been applied to solve inverse imaging\nproblems. These models need massive amounts of data to train, and are limited\nby the dataset characteristics. In FP problems, generic datasets are always\nscarce, and the optical aberration varies greatly under different acquisition\nconditions. To address these dilemmas, we model the forward physical imaging\nprocess as an interpretable physics-guided neural network (PgNN), where the\nreconstructed image in the complex domain is considered as the learnable\nparameters of the neural network. Since the optimal parameters of the PgNN can\nbe derived by minimizing the difference between the model-generated images and\nreal captured angle-varied images corresponding to the same scene, the proposed\nPgNN can get rid of the problem of massive training data as in traditional\nsupervised methods. Applying the alternate updating mechanism and the total\nvariation regularization, PgNN can flexibly reconstruct images with improved\nperformance. In addition, the Zernike mode is incorporated to compensate for\noptical aberrations to enhance the robustness of FP reconstructions. As a\ndemonstration, we show our method can reconstruct images with smooth\nperformance and detailed information in both simulated and experimental\ndatasets. In particular, when validated in an extension of a high-defocus,\nhigh-exposure tissue section dataset, PgNN outperforms traditional FP methods\nwith fewer artifacts and distinguishable structures.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 09:02:25 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Zhang", "Yongbing", ""], ["Liu", "Yangzhe", ""], ["Li", "Xiu", ""], ["Jiang", "Shaowei", ""], ["Dixit", "Krishna", ""], ["Zhang", "Xinfeng", ""], ["Ji", "Xiangyang", ""]]}, {"id": "1909.08898", "submitter": "Hans Meine", "authors": "Hans Meine, Alessa Hering", "title": "Efficient Prealignment of CT Scans for Registration through a Bodypart\n  Regressor", "comments": "Extended Abstract accepted at MIDL 2019", "journal-ref": null, "doi": null, "report-no": "MIDL/2019/ExtendedAbstract/r1xYAvZXqN", "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Convolutional neural networks have not only been applied for classification\nof voxels, objects, or images, for instance, but have also been proposed as a\nbodypart regressor. We pick up this underexplored idea and evaluate its value\nfor registration: A CNN is trained to output the relative height within the\nhuman body in axial CT scans, and the resulting scores are used for quick\nalignment between different timepoints. Preliminary results confirm that this\nallows both fast and robust prealignment compared with iterative approaches.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 10:02:58 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Meine", "Hans", ""], ["Hering", "Alessa", ""]]}, {"id": "1909.08950", "submitter": "Max Bain", "authors": "Max Bain, Arsha Nagrani, Daniel Schofield, Andrew Zisserman", "title": "Count, Crop and Recognise: Fine-Grained Recognition in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to label all the animal individuals present in\nevery frame of a video. Unlike previous methods that have principally\nconcentrated on labelling face tracks, we aim to label individuals even when\ntheir faces are not visible. We make the following contributions: (i) we\nintroduce a 'Count, Crop and Recognise' (CCR) multistage recognition process\nfor frame level labelling. The Count and Recognise stages involve specialised\nCNNs for the task, and we show that this simple staging gives a substantial\nboost in performance; (ii) we compare the recall using frame based labelling to\nboth face and body track based labelling, and demonstrate the advantage of\nframe based with CCR for the specified goal; (iii) we introduce a new dataset\nfor chimpanzee recognition in the wild; and (iv) we apply a high-granularity\nvisualisation technique to further understand the learned CNN features for the\nrecognition of chimpanzee individuals.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 12:57:39 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 16:33:01 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Bain", "Max", ""], ["Nagrani", "Arsha", ""], ["Schofield", "Daniel", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1909.08959", "submitter": "Micha{\\l} Marcinkiewicz", "authors": "Micha{\\l} Marcinkiewicz, Grzegorz Mrukwa", "title": "Quantitative Impact of Label Noise on the Quality of Segmentation of\n  Brain Tumors on MRI scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, deep learning has proven to be a great solution to\nmany problems, such as image or text classification. Recently, deep\nlearning-based solutions have outperformed humans on selected benchmark\ndatasets, yielding a promising future for scientific and real-world\napplications. Training of deep learning models requires vast amounts of high\nquality data to achieve such supreme performance. In real-world scenarios,\nobtaining a large, coherent, and properly labeled dataset is a challenging\ntask. This is especially true in medical applications, where high-quality data\nand annotations are scarce and the number of expert annotators is limited. In\nthis paper, we investigate the impact of corrupted ground-truth masks on the\nperformance of a neural network for a brain tumor segmentation task. Our\nfindings suggest that a) the performance degrades about 8% less than it could\nbe expected from simulations, b) a neural network learns the simulated biases\nof annotators, c) biases can be partially mitigated by using an\ninversely-biased dice loss function.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 09:48:58 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Marcinkiewicz", "Micha\u0142", ""], ["Mrukwa", "Grzegorz", ""]]}, {"id": "1909.08986", "submitter": "Xiao-Yun Zhou", "authors": "Zhao-Yang Wang, Xiao-Yun Zhou, Peichao Li, and Celia Riga, and\n  Guang-Zhong Yang", "title": "Instantiation-Net: 3D Mesh Reconstruction from Single 2D Image for Right\n  Ventricle", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D shape instantiation which reconstructs the 3D shape of a target from\nlimited 2D images or projections is an emerging technique for surgical\nintervention. It improves the currently less-informative and insufficient 2D\nnavigation schemes for robot-assisted Minimally Invasive Surgery (MIS) to 3D\nnavigation. Previously, a general and registration-free framework was proposed\nfor 3D shape instantiation based on Kernel Partial Least Square Regression\n(KPLSR), requiring manually segmented anatomical structures as the\npre-requisite. Two hyper-parameters including the Gaussian width and component\nnumber also need to be carefully adjusted. Deep Convolutional Neural Network\n(DCNN) based framework has also been proposed to reconstruct a 3D point cloud\nfrom a single 2D image, with end-to-end and fully automatic learning. In this\npaper, an Instantiation-Net is proposed to reconstruct the 3D mesh of a target\nfrom its a single 2D image, by using DCNN to extract features from the 2D image\nand Graph Convolutional Network (GCN) to reconstruct the 3D mesh, and using\nFully Connected (FC) layers to connect the DCNN to GCN. Detailed validation was\nperformed to demonstrate the practical strength of the method and its potential\nclinical use.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 21:22:53 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Wang", "Zhao-Yang", ""], ["Zhou", "Xiao-Yun", ""], ["Li", "Peichao", ""], ["Riga", "Celia", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "1909.08987", "submitter": "Mohammed Zubair Mohammed Shamim", "authors": "Mohammed Zubair M. Shamim, Sadatullah Syed, Mohammad Shiblee, Mohammed\n  Usman and Syed Ali", "title": "Automated detection of oral pre-cancerous tongue lesions using deep\n  learning for early diagnosis of oral cavity cancer", "comments": "25 pages, 10 figures", "journal-ref": null, "doi": "10.13140/RG.2.2.28808.16643", "report-no": "01", "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering oral cavity cancer (OCC) at an early stage is an effective way to\nincrease patient survival rate. However, current initial screening process is\ndone manually and is expensive for the average individual, especially in\ndeveloping countries worldwide. This problem is further compounded due to the\nlack of specialists in such areas. Automating the initial screening process\nusing artificial intelligence (AI) to detect pre-cancerous lesions can prove to\nbe an effective and inexpensive technique that would allow patients to be\ntriaged accordingly to receive appropriate clinical management. In this study,\nwe have applied and evaluated the efficacy of six deep convolutional neural\nnetwork (DCNN) models using transfer learning, for identifying pre-cancerous\ntongue lesions directly using a small data set of clinically annotated\nphotographic images to diagnose early signs of OCC. DCNN model based on Vgg19\narchitecture was able to differentiate between benign and pre-cancerous tongue\nlesions with a mean classification accuracy of 0.98, sensitivity 0.89 and\nspecificity 0.97. Additionally, the ResNet50 DCNN model was able to distinguish\nbetween five types of tongue lesions i.e. hairy tongue, fissured tongue,\ngeographic tongue, strawberry tongue and oral hairy leukoplakia with a mean\nclassification accuracy of 0.97. Preliminary results using an (AI+Physician)\nensemble model demonstrate that an automated initial screening process of\ntongue lesions using DCNNs can achieve near-human level classification\nperformance for diagnosing early signs of OCC in patients.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 17:35:18 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Shamim", "Mohammed Zubair M.", ""], ["Syed", "Sadatullah", ""], ["Shiblee", "Mohammad", ""], ["Usman", "Mohammed", ""], ["Ali", "Syed", ""]]}, {"id": "1909.08989", "submitter": "Hui Zeng", "authors": "Hui Zeng, Lida Li, Zisheng Cao and Lei Zhang", "title": "Grid Anchor based Image Cropping: A New Benchmark and An Efficient Model", "comments": "Extension of a CVPR 2019 paper. Dataset and PyTorch Code are\n  released. arXiv admin note: substantial text overlap with arXiv:1904.04441", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image cropping aims to improve the composition as well as aesthetic quality\nof an image by removing extraneous content from it. Most of the existing image\ncropping databases provide only one or several human-annotated bounding boxes\nas the groundtruths, which can hardly reflect the non-uniqueness and\nflexibility of image cropping in practice. The employed evaluation metrics such\nas intersection-over-union cannot reliably reflect the real performance of a\ncropping model, either. This work revisits the problem of image cropping, and\npresents a grid anchor based formulation by considering the special properties\nand requirements (e.g., local redundancy, content preservation, aspect ratio)\nof image cropping. Our formulation reduces the searching space of candidate\ncrops from millions to no more than ninety. Consequently, a grid anchor based\ncropping benchmark is constructed, where all crops of each image are annotated\nand more reliable evaluation metrics are defined. To meet the practical demands\nof robust performance and high efficiency, we also design an effective and\nlightweight cropping model. By simultaneously considering the region of\ninterest and region of discard, and leveraging multi-scale information, our\nmodel can robustly output visually pleasing crops for images of different\nscenes. With less than 2.5M parameters, our model runs at a speed of 200 FPS on\none single GTX 1080Ti GPU and 12 FPS on one i7-6800K CPU. The code is available\nat: \\url{https://github.com/HuiZeng/Grid-Anchor-based-Image-Cropping-Pytorch}.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 14:41:42 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Zeng", "Hui", ""], ["Li", "Lida", ""], ["Cao", "Zisheng", ""], ["Zhang", "Lei", ""]]}, {"id": "1909.08991", "submitter": "Gilberto Ochoa-Ruiz", "authors": "A.A. Angulo, J.A. Vega-Fern\\'andez, L.M. Aguilar-Lobo, S. Natraj and G\n  Ochoa-Ruiz", "title": "Road Damage Detection Acquisition System based on Deep Neural Networks\n  for Physical Asset Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on damage detection of road surfaces has been an active area of\nre-search, but most studies have focused so far on the detection of the\npresence of damages. However, in real-world scenarios, road managers need to\nclearly understand the type of damage and its extent in order to take effective\naction in advance or to allocate the necessary resources. Moreover, currently\nthere are few uniform and openly available road damage datasets, leading to a\nlack of a common benchmark for road damage detection. Such dataset could be\nused in a great variety of applications; herein, it is intended to serve as the\nacquisition component of a physical asset management tool which can aid\ngovernments agencies for planning purposes, or by infrastructure mainte-nance\ncompanies. In this paper, we make two contributions to address these issues.\nFirst, we present a large-scale road damage dataset, which includes a more\nbalanced and representative set of damages. This dataset is composed of 18,034\nroad damage images captured with a smartphone, with 45,435 in-stances road\nsurface damages. Second, we trained different types of object detection\nmethods, both traditional (an LBP-cascaded classifier) and deep learning-based,\nspecifically, MobileNet and RetinaNet, which are amenable for embedded and\nmobile and implementations with an acceptable perfor-mance for many\napplications. We compare the accuracy and inference time of all these models\nwith others in the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 13:43:31 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Angulo", "A. A.", ""], ["Vega-Fern\u00e1ndez", "J. A.", ""], ["Aguilar-Lobo", "L. M.", ""], ["Natraj", "S.", ""], ["Ochoa-Ruiz", "G", ""]]}, {"id": "1909.09006", "submitter": "Chaoping Zhang", "authors": "Chaoping Zhang, Florian Dubost, Marleen de Bruijne, Stefan Klein, Dirk\n  H.J. Poot", "title": "APIR-Net: Autocalibrated Parallel Imaging Reconstruction using a Neural\n  Network", "comments": "To appear in the proceedings of MICCAI 2019 Workshop Machine Learning\n  for Medical Image Reconstruction", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV eess.IV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been successfully demonstrated in MRI reconstruction of\naccelerated acquisitions. However, its dependence on representative training\ndata limits the application across different contrasts, anatomies, or image\nsizes. To address this limitation, we propose an unsupervised, auto-calibrated\nk-space completion method, based on a uniquely designed neural network that\nreconstructs the full k-space from an undersampled k-space, exploiting the\nredundancy among the multiple channels in the receive coil in a parallel\nimaging acquisition. To achieve this, contrary to common convolutional network\napproaches, the proposed network has a decreasing number of feature maps of\nconstant size. In contrast to conventional parallel imaging methods such as\nGRAPPA that estimate the prediction kernel from the fully sampled\nautocalibration signals in a linear way, our method is able to learn nonlinear\nrelations between sampled and unsampled positions in k-space. The proposed\nmethod was compared to the start-of-the-art ESPIRiT and RAKI methods in terms\nof noise amplification and visual image quality in both phantom and in-vivo\nexperiments. The experiments indicate that APIR-Net provides a promising\nalternative to the conventional parallel imaging methods, and results in\nimproved image quality especially for low SNR acquisitions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 14:06:09 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Zhang", "Chaoping", ""], ["Dubost", "Florian", ""], ["de Bruijne", "Marleen", ""], ["Klein", "Stefan", ""], ["Poot", "Dirk H. J.", ""]]}, {"id": "1909.09018", "submitter": "Kuruparan Shanmugalingam", "authors": "Kuruparan Shanmugalingam, Nisal Chandrasekara, Calvin Hindle, Gihan\n  Fernando, Chanaka Gunawardhana", "title": "Corporate IT-support Help-Desk Process Hybrid-Automation Solution with\n  Machine Learning Approach", "comments": "7 pages, 8 Figures, 2 Tables", "journal-ref": "The International Conference on Digital Image Computing:\n  Techniques and Applications (DICTA) 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comprehensive IT support teams in large scale organizations require more man\npower for handling engagement and requests of employees from different channels\non a 24*7 basis. Automated email technical queries help desk is proposed to\nhave instant real-time quick solutions and email categorisation. Email topic\nmodelling with various machine learning, deep-learning approaches are compared\nwith different features for a scalable, generalised solution along with\nsure-shot static rules. Email's title, body, attachment, OCR text, and some\nfeature engineered custom features are given as input elements. XGBoost\ncascaded hierarchical models, Bi-LSTM model with word embeddings perform well\nshowing 77.3 overall accuracy For the real world corporate email data set. By\nintroducing the thresholding techniques, the overall automation system\narchitecture provides 85.6 percentage of accuracy for real world corporate\nemails. Combination of quick fixes, static rules, ML categorization as a low\ncost inference solution reduces 81 percentage of the human effort in the\nprocess of automation and real time implementation.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 10:07:01 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Shanmugalingam", "Kuruparan", ""], ["Chandrasekara", "Nisal", ""], ["Hindle", "Calvin", ""], ["Fernando", "Gihan", ""], ["Gunawardhana", "Chanaka", ""]]}, {"id": "1909.09034", "submitter": "Aishan Liu", "authors": "Aishan Liu, Xianglong Liu, Chongzhi Zhang, Hang Yu, Qiang Liu, Dacheng\n  Tao", "title": "Training Robust Deep Neural Networks via Adversarial Noise Propagation", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practice, deep neural networks have been found to be vulnerable to various\ntypes of noise, such as adversarial examples and corruption. Various\nadversarial defense methods have accordingly been developed to improve\nadversarial robustness for deep models. However, simply training on data mixed\nwith adversarial examples, most of these models still fail to defend against\nthe generalized types of noise. Motivated by the fact that hidden layers play a\nhighly important role in maintaining a robust model, this paper proposes a\nsimple yet powerful training algorithm, named \\emph{Adversarial Noise\nPropagation} (ANP), which injects noise into the hidden layers in a layer-wise\nmanner. ANP can be implemented efficiently by exploiting the nature of the\nbackward-forward training style. Through thorough investigations, we determine\nthat different hidden layers make different contributions to model robustness\nand clean accuracy, while shallow layers are comparatively more critical than\ndeep layers. Moreover, our framework can be easily combined with other\nadversarial training methods to further improve model robustness by exploiting\nthe potential of hidden layers. Extensive experiments on MNIST, CIFAR-10,\nCIFAR-10-C, CIFAR-10-P, and ImageNet demonstrate that ANP enables the strong\nrobustness for deep models against both adversarial and corrupted ones, and\nalso significantly outperforms various adversarial defense methods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 15:08:07 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 01:17:53 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Liu", "Aishan", ""], ["Liu", "Xianglong", ""], ["Zhang", "Chongzhi", ""], ["Yu", "Hang", ""], ["Liu", "Qiang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1909.09051", "submitter": "Michael Firman", "authors": "Jamie Watson, Michael Firman, Gabriel J. Brostow and Daniyar\n  Turmukhambetov", "title": "Self-Supervised Monocular Depth Hints", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth estimators can be trained with various forms of\nself-supervision from binocular-stereo data to circumvent the need for\nhigh-quality laser scans or other ground-truth data. The disadvantage, however,\nis that the photometric reprojection losses used with self-supervised learning\ntypically have multiple local minima. These plausible-looking alternatives to\nground truth can restrict what a regression network learns, causing it to\npredict depth maps of limited quality. As one prominent example, depth\ndiscontinuities around thin structures are often incorrectly estimated by\ncurrent state-of-the-art methods.\n  Here, we study the problem of ambiguous reprojections in depth prediction\nfrom stereo-based self-supervision, and introduce Depth Hints to alleviate\ntheir effects. Depth Hints are complementary depth suggestions obtained from\nsimple off-the-shelf stereo algorithms. These hints enhance an existing\nphotometric loss function, and are used to guide a network to learn better\nweights. They require no additional data, and are assumed to be right only\nsometimes. We show that using our Depth Hints gives a substantial boost when\ntraining several leading self-supervised-from-stereo models, not just our own.\nFurther, combined with other good practices, we produce state-of-the-art depth\npredictions on the KITTI benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 15:41:07 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Watson", "Jamie", ""], ["Firman", "Michael", ""], ["Brostow", "Gabriel J.", ""], ["Turmukhambetov", "Daniyar", ""]]}, {"id": "1909.09059", "submitter": "Titus Leistner", "authors": "Titus Leistner, Hendrik Schilling, Radek Mackowiak, Stefan Gumhold,\n  Carsten Rother", "title": "Learning to Think Outside the Box: Wide-Baseline Light Field Depth\n  Estimation with EPI-Shift", "comments": "Published at International Conference on 3D Vision (3DV) 2019", "journal-ref": null, "doi": "10.1109/3DV.2019.00036", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for depth estimation from light field data, based on a\nfully convolutional neural network architecture. Our goal is to design a\npipeline which achieves highly accurate results for small- and wide-baseline\nlight fields. Since light field training data is scarce, all learning-based\napproaches use a small receptive field and operate on small disparity ranges.\nIn order to work with wide-baseline light fields, we introduce the idea of\nEPI-Shift: To virtually shift the light field stack which enables to retain a\nsmall receptive field, independent of the disparity range. In this way, our\napproach \"learns to think outside the box of the receptive field\". Our network\nperforms joint classification of integer disparities and regression of\ndisparity-offsets. A U-Net component provides excellent long-range smoothing.\nEPI-Shift considerably outperforms the state-of-the-art learning-based\napproaches and is on par with hand-crafted methods. We demonstrate this on a\npublicly available, synthetic, small-baseline benchmark and on large-baseline\nreal-world recordings.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 15:57:17 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Leistner", "Titus", ""], ["Schilling", "Hendrik", ""], ["Mackowiak", "Radek", ""], ["Gumhold", "Stefan", ""], ["Rother", "Carsten", ""]]}, {"id": "1909.09060", "submitter": "Lun Huang", "authors": "Lun Huang and Wenmin Wang and Yaxian Xia and Jie Chen", "title": "Adaptively Aligned Image Captioning via Adaptive Attention Time", "comments": "Accepted to NeurIPS 2019. Code is available at\n  https://github.com/husthuaan/AAT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent neural models for image captioning usually employ an encoder-decoder\nframework with an attention mechanism. However, the attention mechanism in such\na framework aligns one single (attended) image feature vector to one caption\nword, assuming one-to-one mapping from source image regions and target caption\nwords, which is never possible. In this paper, we propose a novel attention\nmodel, namely Adaptive Attention Time (AAT), to align the source and the target\nadaptively for image captioning. AAT allows the framework to learn how many\nattention steps to take to output a caption word at each decoding step. With\nAAT, an image region can be mapped to an arbitrary number of caption words\nwhile a caption word can also attend to an arbitrary number of image regions.\nAAT is deterministic and differentiable, and doesn't introduce any noise to the\nparameter gradients. In this paper, we empirically show that AAT improves over\nstate-of-the-art methods on the task of image captioning. Code is available at\nhttps://github.com/husthuaan/AAT.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 15:59:33 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 04:04:38 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 09:26:01 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Huang", "Lun", ""], ["Wang", "Wenmin", ""], ["Xia", "Yaxian", ""], ["Chen", "Jie", ""]]}, {"id": "1909.09070", "submitter": "Jose Manuel Gomez-Perez", "authors": "Jose Manuel Gomez-Perez and Raul Ortega", "title": "Look, Read and Enrich. Learning from Scientific Figures and their\n  Captions", "comments": "Accepted in the 10th International Conference on Knowledge capture\n  (K-CAP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to natural images, understanding scientific figures is particularly\nhard for machines. However, there is a valuable source of information in\nscientific literature that until now has remained untapped: the correspondence\nbetween a figure and its caption. In this paper we investigate what can be\nlearnt by looking at a large number of figures and reading their captions, and\nintroduce a figure-caption correspondence learning task that makes use of our\nobservations. Training visual and language networks without supervision other\nthan pairs of unconstrained figures and captions is shown to successfully solve\nthis task. We also show that transferring lexical and semantic knowledge from a\nknowledge graph significantly enriches the resulting features. Finally, we\ndemonstrate the positive impact of such features in other tasks involving\nscientific text and figures, like multi-modal classification and machine\ncomprehension for question answering, outperforming supervised baselines and\nad-hoc approaches.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 16:10:15 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Gomez-Perez", "Jose Manuel", ""], ["Ortega", "Raul", ""]]}, {"id": "1909.09096", "submitter": "Peter Werner", "authors": "Peter Werner, Matthias Hofer, Carmelo Sferrazza and Raffaello D'Andrea", "title": "Vision-Based Proprioceptive Sensing for Soft Inflatable Actuators", "comments": "This work has been submitted to the 2020 IEEE International\n  Conference on Robotics and Automation (ICRA) for possible publication.\n  Accompanying video: https://youtu.be/1MJuhxVcTns", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a vision-based sensing approach for a soft linear\nactuator, which is equipped with an integrated camera. The proposed\nvision-based sensing pipeline predicts the three-dimensional position of a\npoint of interest on the actuator. To train and evaluate the algorithm,\npredictions are compared to ground truth data from an external motion capture\nsystem. An off-the-shelf distance sensor is integrated in a similar actuator\nand its performance is used as a baseline for comparison. The resulting sensing\npipeline runs at 40 Hz in real-time on a standard laptop and is additionally\nused for closed loop elongation control of the actuator. It is shown that the\napproach can achieve comparable accuracy to the distance sensor.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 17:13:03 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Werner", "Peter", ""], ["Hofer", "Matthias", ""], ["Sferrazza", "Carmelo", ""], ["D'Andrea", "Raffaello", ""]]}, {"id": "1909.09115", "submitter": "Tianwei Shen", "authors": "Tianwei Shen, Lei Zhou, Zixin Luo, Yao Yao, Shiwei Li, Jiahui Zhang,\n  Tian Fang, Long Quan", "title": "Self-Supervised Learning of Depth and Motion Under Photometric\n  Inconsistency", "comments": "International Conference on Computer Vision (ICCV) Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The self-supervised learning of depth and pose from monocular sequences\nprovides an attractive solution by using the photometric consistency of nearby\nframes as it depends much less on the ground-truth data. In this paper, we\naddress the issue when previous assumptions of the self-supervised approaches\nare violated due to the dynamic nature of real-world scenes. Different from\nhandling the noise as uncertainty, our key idea is to incorporate more robust\ngeometric quantities and enforce internal consistency in the temporal image\nsequence. As demonstrated on commonly used benchmark datasets, the proposed\nmethod substantially improves the state-of-the-art methods on both depth and\nrelative pose estimation for monocular image sequences, without adding\ninference overhead.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 17:41:28 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Shen", "Tianwei", ""], ["Zhou", "Lei", ""], ["Luo", "Zixin", ""], ["Yao", "Yao", ""], ["Li", "Shiwei", ""], ["Zhang", "Jiahui", ""], ["Fang", "Tian", ""], ["Quan", "Long", ""]]}, {"id": "1909.09124", "submitter": "Saima Rathore", "authors": "Saima Rathore, Muhammad Aksam Iftikhar, Zissimos Mourelatos", "title": "Prediction of overall survival and molecular markers in gliomas via\n  analysis of digital pathology images using deep learning", "comments": "4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cancer histology reveals disease progression and associated molecular\nprocesses, and contains rich phenotypic information that is predictive of\noutcome. In this paper, we developed a computational approach based on deep\nlearning to predict the overall survival and molecular subtypes of glioma\npatients from microscopic images of tissue biopsies, reflecting measures of\nmicrovascular proliferation, mitotic activity, nuclear atypia, and the presence\nof necrosis. Whole-slide images from 663 unique patients [IDH: 333\nIDH-wildtype, 330 IDH-mutants, 1p/19q: 201 1p/19q non-codeleted, 129 1p/19q\ncodeleted] were obtained from TCGA. Sub-images that were free of artifacts and\nthat contained viable tumor with descriptive histologic characteristics were\nextracted, which were further used for training and testing a deep neural\nnetwork. The output layer of the network was configured in two different ways:\n(i) a final Cox model layer to output a prediction of patient risk, and (ii) a\nfinal layer with sigmoid activation function, and stochastic gradient decent\nbased optimization with binary cross-entropy loss. Both survival prediction and\nmolecular subtype classification produced promising results using our model.\nThe c-statistic was estimated to be 0.82 (p-value=4.8x10-5) between the risk\nscores of the proposed deep learning model and overall survival, while\naccuracies of 88% (area under the curve [AUC]=0.86) were achieved in the\ndetection of IDH mutational status and 1p/19q codeletion. These findings\nsuggest that the deep learning techniques can be applied to microscopic images\nfor objective, accurate, and integrated prediction of outcome for glioma\npatients. The proposed marker may contribute to (i) stratification of patients\ninto clinical trials, (ii) patient selection for targeted therapy, and (iii)\npersonalized treatment planning.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 17:53:42 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Rathore", "Saima", ""], ["Iftikhar", "Muhammad Aksam", ""], ["Mourelatos", "Zissimos", ""]]}, {"id": "1909.09148", "submitter": "Zhuoxun He", "authors": "Zhuoxun He, Lingxi Xie, Xin Chen, Ya Zhang, Yanfeng Wang and Qi Tian", "title": "Data Augmentation Revisited: Rethinking the Distribution Gap between\n  Clean and Augmented Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation has been widely applied as an effective methodology to\nimprove generalization in particular when training deep neural networks.\nRecently, researchers proposed a few intensive data augmentation techniques,\nwhich indeed improved accuracy, yet we notice that these methods augment data\nhave also caused a considerable gap between clean and augmented data. In this\npaper, we revisit this problem from an analytical perspective, for which we\nestimate the upper-bound of expected risk using two terms, namely, empirical\nrisk and generalization error, respectively. We develop an understanding of\ndata augmentation as regularization, which highlights the major features. As a\nresult, data augmentation significantly reduces the generalization error, but\nmeanwhile leads to a slightly higher empirical risk. On the assumption that\ndata augmentation helps models converge to a better region, the model can\nbenefit from a lower empirical risk achieved by a simple method, i.e., using\nless-augmented data to refine the model trained on fully-augmented data. Our\napproach achieves consistent accuracy gain on a few standard image\nclassification benchmarks, and the gain transfers to object detection.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 08:36:45 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 15:56:49 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["He", "Zhuoxun", ""], ["Xie", "Lingxi", ""], ["Chen", "Xin", ""], ["Zhang", "Ya", ""], ["Wang", "Yanfeng", ""], ["Tian", "Qi", ""]]}, {"id": "1909.09156", "submitter": "Amos Azaria", "authors": "Moshe Hanukoglu, Nissan Goldberg, Aviv Rovshitz, Amos Azaria", "title": "Learning to Conceal: A Deep Learning Based Method for Preserving Privacy\n  and Avoiding Prejudice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a learning model able to conceals personal\ninformation (e.g. gender, age, ethnicity, etc.) from an image, while\nmaintaining any additional information present in the image (e.g. smile,\nhair-style, brightness). Our trained model is not provided the information that\nit is concealing, and does not try learning it either. Namely, we created a\nvariational autoencoder (VAE) model that is trained on a dataset including\nlabels of the information one would like to conceal (e.g. gender, ethnicity,\nage). These labels are directly added to the VAE's sampled latent vector. Due\nto the limited number of neurons in the latent vector and its appended noise,\nthe VAE avoids learning any relation between the given images and the given\nlabels, as those are given directly. Therefore, the encoded image lacks any of\nthe information one wishes to conceal. The encoding may be decoded back into an\nimage according to any provided properties (e.g. a 40 year old woman).\n  The proposed architecture can be used as a mean for privacy preserving and\ncan serve as an input to systems, which will become unbiased and not suffer\nfrom prejudice. We believe that privacy and discrimination are two of the most\nimportant aspects in which the community should try and develop methods to\nprevent misuse of technological advances.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 13:54:18 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Hanukoglu", "Moshe", ""], ["Goldberg", "Nissan", ""], ["Rovshitz", "Aviv", ""], ["Azaria", "Amos", ""]]}, {"id": "1909.09192", "submitter": "Vardaan Pahuja", "authors": "Vardaan Pahuja, Jie Fu, Christopher J. Pal", "title": "Learning Sparse Mixture of Experts for Visual Question Answering", "comments": "Accepted in Visual Question Answering and Dialog Workshop, CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a rapid progress in the task of Visual Question Answering with\nimproved model architectures. Unfortunately, these models are usually\ncomputationally intensive due to their sheer size which poses a serious\nchallenge for deployment. We aim to tackle this issue for the specific task of\nVisual Question Answering (VQA). A Convolutional Neural Network (CNN) is an\nintegral part of the visual processing pipeline of a VQA model (assuming the\nCNN is trained along with entire VQA model). In this project, we propose an\nefficient and modular neural architecture for the VQA task with focus on the\nCNN module. Our experiments demonstrate that a sparsely activated CNN based VQA\nmodel achieves comparable performance to a standard CNN based VQA model\narchitecture.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 18:55:54 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Pahuja", "Vardaan", ""], ["Fu", "Jie", ""], ["Pal", "Christopher J.", ""]]}, {"id": "1909.09225", "submitter": "Philipe A. Dias", "authors": "Philipe A. Dias, Damiano Malafronte, Henry Medeiros, Francesca Odone", "title": "Gaze Estimation for Assisted Living Environments", "comments": "Work to be published in its final version at WACV '20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective assisted living environments must be able to perform inferences on\nhow their occupants interact with one another as well as with surrounding\nobjects. To accomplish this goal using a vision-based automated approach,\nmultiple tasks such as pose estimation, object segmentation and gaze estimation\nmust be addressed. Gaze direction in particular provides some of the strongest\nindications of how a person interacts with the environment. In this paper, we\npropose a simple neural network regressor that estimates the gaze direction of\nindividuals in a multi-camera assisted living scenario, relying only on the\nrelative positions of facial keypoints collected from a single pose estimation\nmodel. To handle cases of keypoint occlusion, our model exploits a novel\nconfidence gated unit in its input layer. In addition to the gaze direction,\nour model also outputs an estimation of its own prediction uncertainty.\nExperimental results on a public benchmark demonstrate that our approach\nperforms on pair with a complex, dataset-specific baseline, while its\nuncertainty predictions are highly correlated to the actual angular error of\ncorresponding estimations. Finally, experiments on images from a real assisted\nliving environment demonstrate the higher suitability of our model for its\nfinal application.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 20:29:27 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Dias", "Philipe A.", ""], ["Malafronte", "Damiano", ""], ["Medeiros", "Henry", ""], ["Odone", "Francesca", ""]]}, {"id": "1909.09252", "submitter": "Devanshu Arya", "authors": "Devanshu Arya, Stevan Rudinac and Marcel Worring", "title": "HyperLearn: A Distributed Approach for Representation Learning in\n  Datasets With Many Modalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal datasets contain an enormous amount of relational information,\nwhich grows exponentially with the introduction of new modalities. Learning\nrepresentations in such a scenario is inherently complex due to the presence of\nmultiple heterogeneous information channels. These channels can encode both (a)\ninter-relations between the items of different modalities and (b)\nintra-relations between the items of the same modality. Encoding multimedia\nitems into a continuous low-dimensional semantic space such that both types of\nrelations are captured and preserved is extremely challenging, especially if\nthe goal is a unified end-to-end learning framework. The two key challenges\nthat need to be addressed are: 1) the framework must be able to merge complex\nintra and inter relations without losing any valuable information and 2) the\nlearning model should be invariant to the addition of new and potentially very\ndifferent modalities. In this paper, we propose a flexible framework which can\nscale to data streams from many modalities. To that end we introduce a\nhypergraph-based model for data representation and deploy Graph Convolutional\nNetworks to fuse relational information within and across modalities. Our\napproach provides an efficient solution for distributing otherwise extremely\ncomputationally expensive or even unfeasible training processes across\nmultiple-GPUs, without any sacrifices in accuracy. Moreover, adding new\nmodalities to our model requires only an additional GPU unit keeping the\ncomputational time unchanged, which brings representation learning to truly\nmultimodal datasets. We demonstrate the feasibility of our approach in the\nexperiments on multimedia datasets featuring second, third and fourth order\nrelations.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 22:45:21 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Arya", "Devanshu", ""], ["Rudinac", "Stevan", ""], ["Worring", "Marcel", ""]]}, {"id": "1909.09256", "submitter": "Subarna Tripathi", "authors": "Brigit Schroeder and Subarna Tripathi and Hanlin Tang", "title": "Triplet-Aware Scene Graph Embeddings", "comments": "Accepted to Scene Graph Representation Learning workshop at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene graphs have become an important form of structured knowledge for tasks\nsuch as for image generation, visual relation detection, visual question\nanswering, and image retrieval. While visualizing and interpreting word\nembeddings is well understood, scene graph embeddings have not been fully\nexplored. In this work, we train scene graph embeddings in a layout generation\ntask with different forms of supervision, specifically introducing triplet\nsuper-vision and data augmentation. We see a significant performance increase\nin both metrics that measure the goodness of layout prediction, mean\nintersection-over-union (mIoU)(52.3% vs. 49.2%) and relation score (61.7% vs.\n54.1%),after the addition of triplet supervision and data augmentation. To\nunderstand how these different methods affect the scene graph representation,\nwe apply several new visualization and evaluation methods to explore the\nevolution of the scene graph embedding. We find that triplet supervision\nsignificantly improves the embedding separability, which is highly correlated\nwith the performance of the layout prediction model.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 23:20:49 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Schroeder", "Brigit", ""], ["Tripathi", "Subarna", ""], ["Tang", "Hanlin", ""]]}, {"id": "1909.09263", "submitter": "Kyungyul Kim", "authors": "Jihyeun Yoon, Kyungyul Kim, Jongseong Jang", "title": "Propagated Perturbation of Adversarial Attack for well-known CNNs:\n  Empirical Study and its Explanation", "comments": null, "journal-ref": "ICCV 2019 Workshop on Interpreting and Explaining Visual\n  Artificial Intelligence Models", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network based classifiers are known to be vulnerable to\nperturbations of inputs constructed by an adversarial attack to force\nmisclassification. Most studies have focused on how to make vulnerable noise by\ngradient based attack methods or to defense model from adversarial attack. The\nuse of the denoiser model is one of a well-known solution to reduce the\nadversarial noise although classification performance had not significantly\nimproved. In this study, we aim to analyze the propagation of adversarial\nattack as an explainable AI(XAI) point of view. Specifically, we examine the\ntrend of adversarial perturbations through the CNN architectures. To analyze\nthe propagated perturbation, we measured normalized Euclidean Distance and\ncosine distance in each CNN layer between the feature map of the perturbed\nimage passed through denoiser and the non-perturbed original image. We used\nfive well-known CNN based classifiers and three gradient-based adversarial\nattacks. From the experimental results, we observed that in most cases,\nEuclidean Distance explosively increases in the final fully connected layer\nwhile cosine distance fluctuated and disappeared at the last layer. This means\nthat the use of denoiser can decrease the amount of noise. However, it failed\nto defense accuracy degradation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 23:51:07 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 07:18:24 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Yoon", "Jihyeun", ""], ["Kim", "Kyungyul", ""], ["Jang", "Jongseong", ""]]}, {"id": "1909.09269", "submitter": "Harshala Gammulle", "authors": "Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes", "title": "Fine-grained Action Segmentation using the Semi-Supervised Action GAN", "comments": "Published in Pattern Recognition Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of continuous fine-grained action\nsegmentation, in which multiple actions are present in an unsegmented video\nstream. The challenge for this task lies in the need to represent the\nhierarchical nature of the actions and to detect the transitions between\nactions, allowing us to localise the actions within the video effectively. We\npropose a novel recurrent semi-supervised Generative Adversarial Network (GAN)\nmodel for continuous fine-grained human action segmentation. Temporal context\ninformation is captured via a novel Gated Context Extractor (GCE) module,\ncomposed of gated attention units, that directs the queued context information\nthrough the generator model, for enhanced action segmentation. The GAN is made\nto learn features in a semi-supervised manner, enabling the model to perform\naction classification jointly with the standard, unsupervised, GAN learning\nprocedure. We perform extensive evaluations on different architectural variants\nto demonstrate the importance of the proposed network architecture, and show\nthat it is capable of outperforming current state-of-the-art on three\nchallenging datasets: 50 Salads, MERL Shopping and Georgia Tech Egocentric\nActivities dataset.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 00:38:05 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Gammulle", "Harshala", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1909.09272", "submitter": "Chengxi Li", "authors": "Chengxi Li, Yue Meng, Stanley H. Chan, Yi-Ting Chen", "title": "Learning 3D-aware Egocentric Spatial-Temporal Interaction via Graph\n  Convolutional Networks", "comments": "Accepted to the International Conference on Robotics and Automation\n  (ICRA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enable intelligent automated driving systems, a promising strategy is to\nunderstand how human drives and interacts with road users in complicated\ndriving situations. In this paper, we propose a 3D-aware egocentric\nspatial-temporal interaction framework for automated driving applications.\nGraph convolution networks (GCN) is devised for interaction modeling. We\nintroduce three novel concepts into GCN. First, we decompose egocentric\ninteractions into ego-thing and ego-stuff interaction, modeled by two GCNs. In\nboth GCNs, ego nodes are introduced to encode the interaction between thing\nobjects (e.g., car and pedestrian), and interaction between stuff objects\n(e.g., lane marking and traffic light). Second, objects' 3D locations are\nexplicitly incorporated into GCN to better model egocentric interactions.\nThird, to implement ego-stuff interaction in GCN, we propose a MaskAlign\noperation to extract features for irregular objects.\n  We validate the proposed framework on tactical driver behavior recognition.\nExtensive experiments are conducted using Honda Research Institute Driving\nDataset, the largest dataset with diverse tactical driver behavior annotations.\nOur framework demonstrates substantial performance boost over baselines on the\ntwo experimental settings by 3.9% and 6.0%, respectively. Furthermore, we\nvisualize the learned affinity matrices, which encode ego-thing and ego-stuff\ninteractions, to showcase the proposed framework can capture interactions\neffectively.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 00:43:09 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 17:31:35 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 05:19:04 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Li", "Chengxi", ""], ["Meng", "Yue", ""], ["Chan", "Stanley H.", ""], ["Chen", "Yi-Ting", ""]]}, {"id": "1909.09273", "submitter": "Matthew Tesfaldet", "authors": "Mattie Tesfaldet, Xavier Snelgrove, David Vazquez", "title": "Fourier-CPPNs for Image Synthesis", "comments": "Accepted at ICCV Workshops '19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional Pattern Producing Networks (CPPNs) are differentiable networks\nthat independently map (x, y) pixel coordinates to (r, g, b) colour values.\nRecently, CPPNs have been used for creating interesting imagery for creative\npurposes, e.g., neural art. However their architecture biases generated images\nto be overly smooth, lacking high-frequency detail. In this work, we extend\nCPPNs to explicitly model the frequency information for each pixel output,\ncapturing frequencies beyond the DC component. We show that our Fourier-CPPNs\n(F-CPPNs) provide improved visual detail for image synthesis.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 00:43:59 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Tesfaldet", "Mattie", ""], ["Snelgrove", "Xavier", ""], ["Vazquez", "David", ""]]}, {"id": "1909.09278", "submitter": "Harshala Gammulle", "authors": "Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes", "title": "Forecasting Future Action Sequences with Neural Memory Networks", "comments": "BMVC 2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel neural memory network based framework for future action\nsequence forecasting. This is a challenging task where we have to consider\nshort-term, within sequence relationships as well as relationships in between\nsequences, to understand how sequences of actions evolve over time. To capture\nthese relationships effectively, we introduce neural memory networks to our\nmodelling scheme. We show the significance of using two input streams, the\nobserved frames and the corresponding action labels, which provide different\ninformation cues for our prediction task. Furthermore, through the proposed\nmethod we effectively map the long-term relationships among individual input\nsequences through separate memory modules, which enables better fusion of the\nsalient features. Our method outperforms the state-of-the-art approaches by a\nlarge margin on two publicly available datasets: Breakfast and 50 Salads.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 01:04:38 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Gammulle", "Harshala", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1909.09283", "submitter": "Harshala Gammulle", "authors": "Harshala Gammulle, Tharindu Fernando, Simon Denman, Sridha Sridharan,\n  Clinton Fookes", "title": "Coupled Generative Adversarial Network for Continuous Fine-grained\n  Action Segmentation", "comments": "WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel conditional GAN (cGAN) model for continuous fine-grained\nhuman action segmentation, that utilises multi-modal data and learned scene\ncontext information. The proposed approach utilises two GANs: termed Action GAN\nand Auxiliary GAN, where the Action GAN is trained to operate over the current\nRGB frame while the Auxiliary GAN utilises supplementary information such as\ndepth or optical flow. The goal of both GANs is to generate similar `action\ncodes', a vector representation of the current action. To facilitate this\nprocess a context extractor that incorporates data and recent outputs from both\nmodes is used to extract context information to aid recognition. The result is\na recurrent GAN architecture which learns a task specific loss function from\nmultiple feature modalities. Extensive evaluations on variants of the proposed\nmodel to show the importance of utilising different information streams such as\ncontext and auxiliary information in the proposed network; and show that our\nmodel is capable of outperforming state-of-the-art methods for three widely\nused datasets: 50 Salads, MERL Shopping and Georgia Tech Egocentric Activities,\ncomprising both static and dynamic camera settings.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 01:17:00 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Gammulle", "Harshala", ""], ["Fernando", "Tharindu", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1909.09287", "submitter": "Huan Lei", "authors": "Huan Lei, Naveed Akhtar, and Ajmal Mian", "title": "Spherical Kernel for Efficient Graph Convolution on 3D Point Clouds", "comments": "Accepted to TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a spherical kernel for efficient graph convolution of 3D point\nclouds. Our metric-based kernels systematically quantize the local 3D space to\nidentify distinctive geometric relationships in the data. Similar to the\nregular grid CNN kernels, the spherical kernel maintains translation-invariance\nand asymmetry properties, where the former guarantees weight sharing among\nsimilar local structures in the data and the latter facilitates fine geometric\nlearning. The proposed kernel is applied to graph neural networks without\nedge-dependent filter generation, making it computationally attractive for\nlarge point clouds. In our graph networks, each vertex is associated with a\nsingle point location and edges connect the neighborhood points within a\ndefined range. The graph gets coarsened in the network with farthest point\nsampling. Analogous to the standard CNNs, we define pooling and unpooling\noperations for our network. We demonstrate the effectiveness of the proposed\nspherical kernel with graph neural networks for point cloud classification and\nsemantic segmentation using ModelNet, ShapeNet, RueMonge2014, ScanNet and S3DIS\ndatasets. The source code and the trained models can be downloaded from\nhttps://github.com/hlei-ziyan/SPH3D-GCN.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 01:27:43 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 09:48:19 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Lei", "Huan", ""], ["Akhtar", "Naveed", ""], ["Mian", "Ajmal", ""]]}, {"id": "1909.09295", "submitter": "Jingxi Xu", "authors": "David Watkins-Valls, Jingxi Xu, Nicholas Waytowich and Peter Allen", "title": "Learning Your Way Without Map or Compass: Panoramic Target Driven Visual\n  Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robot navigation system that uses an imitation learning\nframework to successfully navigate in complex environments. Our framework takes\na pre-built 3D scan of a real environment and trains an agent from\npre-generated expert trajectories to navigate to any position given a panoramic\nview of the goal and the current visual input without relying on map, compass,\nodometry, or relative position of the target at runtime. Our end-to-end trained\nagent uses RGB and depth (RGBD) information and can handle large environments\n(up to $1031m^2$) across multiple rooms (up to $40$) and generalizes to unseen\ntargets. We show that when compared to several baselines our method (1)\nrequires fewer training examples and less training time, (2) reaches the goal\nlocation with higher accuracy, and (3) produces better solutions with shorter\npaths for long-range navigation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 02:17:20 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 08:52:13 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Watkins-Valls", "David", ""], ["Xu", "Jingxi", ""], ["Waytowich", "Nicholas", ""], ["Allen", "Peter", ""]]}, {"id": "1909.09300", "submitter": "Tianhong Li", "authors": "Tianhong Li, Lijie Fan, Mingmin Zhao, Yingcheng Liu, Dina Katabi", "title": "Making the Invisible Visible: Action Recognition Through Walls and\n  Occlusions", "comments": "ICCV 2019. The first two authors contributed equally to this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding people's actions and interactions typically depends on seeing\nthem. Automating the process of action recognition from visual data has been\nthe topic of much research in the computer vision community. But what if it is\ntoo dark, or if the person is occluded or behind a wall? In this paper, we\nintroduce a neural network model that can detect human actions through walls\nand occlusions, and in poor lighting conditions. Our model takes radio\nfrequency (RF) signals as input, generates 3D human skeletons as an\nintermediate representation, and recognizes actions and interactions of\nmultiple people over time. By translating the input to an intermediate\nskeleton-based representation, our model can learn from both vision-based and\nRF-based datasets, and allow the two tasks to help each other. We show that our\nmodel achieves comparable accuracy to vision-based action recognition systems\nin visible scenarios, yet continues to work accurately when people are not\nvisible, hence addressing scenarios that are beyond the limit of today's\nvision-based action recognition.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 02:49:55 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Li", "Tianhong", ""], ["Fan", "Lijie", ""], ["Zhao", "Mingmin", ""], ["Liu", "Yingcheng", ""], ["Katabi", "Dina", ""]]}, {"id": "1909.09301", "submitter": "Viktor Reshniak", "authors": "Viktor Reshniak, Jeremy Trageser, Clayton G. Webster", "title": "A nonlocal feature-driven exemplar-based approach for image inpainting", "comments": null, "journal-ref": "SIAM J. Imaging Sci. 13(2020) 2140-2168", "doi": "10.1137/20M1317864", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a nonlocal variational image completion technique which admits\nsimultaneous inpainting of multiple structures and textures in a unified\nframework. The recovery of geometric structures is achieved by using general\nconvolution operators as a measure of behavior within an image. These are\ncombined with a nonlocal exemplar-based approach to exploit the self-similarity\nof an image in the selected feature domains and to ensure the inpainting of\ntextures. We also introduce an anisotropic patch distance metric to allow for\nbetter control of the feature selection within an image and present a nonlocal\nenergy functional based on this metric. Finally, we derive an optimization\nalgorithm for the proposed variational model and examine its validity\nexperimentally with various test images.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 03:01:24 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 21:54:40 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Reshniak", "Viktor", ""], ["Trageser", "Jeremy", ""], ["Webster", "Clayton G.", ""]]}, {"id": "1909.09309", "submitter": "Hao Chen", "authors": "Hao Chen and Youfu Li", "title": "CNN-based RGB-D Salient Object Detection: Learn, Select and Fuse", "comments": "submitted to a journal in 12-October-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to present a systematic solution for RGB-D salient\nobject detection, which addresses the following three aspects with a unified\nframework: modal-specific representation learning, complementary cue selection\nand cross-modal complement fusion. To learn discriminative modal-specific\nfeatures, we propose a hierarchical cross-modal distillation scheme, in which\nthe well-learned source modality provides supervisory signals to facilitate the\nlearning process for the new modality. To better extract the complementary\ncues, we formulate a residual function to incorporate complements from the\npaired modality adaptively. Furthermore, a top-down fusion structure is\nconstructed for sufficient cross-modal interactions and cross-level\ntransmissions. The experimental results demonstrate the effectiveness of the\nproposed cross-modal distillation scheme in zero-shot saliency detection and\npre-training on a new modality, as well as the advantages in selecting and\nfusing cross-modal/cross-level complements.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 03:53:53 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Chen", "Hao", ""], ["Li", "Youfu", ""]]}, {"id": "1909.09313", "submitter": "Ulugbek Kamilov", "authors": "Jiaming Liu, Yu Sun, and Ulugbek S. Kamilov", "title": "Infusing Learned Priors into Model-Based Multispectral Imaging", "comments": "arXiv admin note: text overlap with arXiv:1905.05113", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new algorithm for regularized reconstruction of multispectral\n(MS) images from noisy linear measurements. Unlike traditional approaches, the\nproposed algorithm regularizes the recovery problem by using a prior specified\n\\emph{only} through a learned denoising function. More specifically, we propose\na new accelerated gradient method (AGM) variant of regularization by denoising\n(RED) for model-based MS image reconstruction. The key ingredient of our\napproach is the three-dimensional (3D) deep neural net (DNN) denoiser that can\nfully leverage spationspectral correlations within MS images. Our results\nsuggest the generalizability of our MS-RED algorithm, where a single trained\nDNN can be used to solve several different MS imaging problems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 04:21:06 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Liu", "Jiaming", ""], ["Sun", "Yu", ""], ["Kamilov", "Ulugbek S.", ""]]}, {"id": "1909.09325", "submitter": "Rui Chen", "authors": "Rui Chen, Haizhou Ai, Chong Shang, Long Chen, Zijie Zhuang", "title": "Learning Lightweight Pedestrian Detector with Hierarchical Knowledge\n  Distillation", "comments": "Accepted at ICIP 2019 as Oral", "journal-ref": "2019 IEEE International Conference on Image Processing (ICIP), pp.\n  1645-1649", "doi": "10.1109/ICIP.2019.8803079", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It remains very challenging to build a pedestrian detection system for real\nworld applications, which demand for both accuracy and speed. This work\npresents a novel hierarchical knowledge distillation framework to learn a\nlightweight pedestrian detector, which significantly reduces the computational\ncost and still holds the high accuracy at the same time. Following the\n`teacher--student' diagram that a stronger, deeper neural network can teach a\nlightweight network to learn better representations, we explore multiple\nknowledge distillation architectures and reframe this approach as a unified,\nhierarchical distillation framework. In particular, the proposed distillation\nis performed at multiple hierarchies, multiple stages in a modern detector,\nwhich empowers the student detector to learn both low-level details and\nhigh-level abstractions simultaneously. Experiment result shows that a student\nmodel trained by our framework, with 6 times compression in number of\nparameters, still achieves competitive performance as the teacher model on the\nwidely used pedestrian detection benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 05:20:56 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Chen", "Rui", ""], ["Ai", "Haizhou", ""], ["Shang", "Chong", ""], ["Chen", "Long", ""], ["Zhuang", "Zijie", ""]]}, {"id": "1909.09349", "submitter": "Juan Luis Gonzalez Bello", "authors": "Juan Luis Gonzalez Bello and Munchurl Kim", "title": "Deep 3D-Zoom Net: Unsupervised Learning of Photo-Realistic 3D-Zoom", "comments": "Check our video at https://www.youtube.com/watch?v=Gz76VYwUzZ8", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3D-zoom operation is the positive translation of the camera in the\nZ-axis, perpendicular to the image plane. In contrast, the optical zoom changes\nthe focal length and the digital zoom is used to enlarge a certain region of an\nimage to the original image size. In this paper, we are the first to formulate\nan unsupervised 3D-zoom learning problem where images with an arbitrary zoom\nfactor can be generated from a given single image. An unsupervised framework is\nconvenient, as it is a challenging task to obtain a 3D-zoom dataset of natural\nscenes due to the need for special equipment to ensure camera movement is\nrestricted to the Z-axis. In addition, the objects in the scenes should not\nmove when being captured, which hinders the construction of a large dataset of\noutdoor scenes. We present a novel unsupervised framework to learn how to\ngenerate arbitrarily 3D-zoomed versions of a single image, not requiring a\n3D-zoom ground truth, called the Deep 3D-Zoom Net. The Deep 3D-Zoom Net\nincorporates the following features: (i) transfer learning from a pre-trained\ndisparity estimation network via a back re-projection reconstruction loss; (ii)\na fully convolutional network architecture that models depth-image-based\nrendering (DIBR), taking into account high-frequency details without the need\nfor estimating the intermediate disparity; and (iii) incorporating a\ndiscriminator network that acts as a no-reference penalty for unnaturally\nrendered areas. Even though there is no baseline to fairly compare our results,\nour method outperforms previous novel view synthesis research in terms of\nrealistic appearance on large camera baselines. We performed extensive\nexperiments to verify the effectiveness of our method on the KITTI and\nCityscapes datasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 07:18:39 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 17:23:17 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Bello", "Juan Luis Gonzalez", ""], ["Kim", "Munchurl", ""]]}, {"id": "1909.09380", "submitter": "Xiameng Qin", "authors": "He guo, Xiameng Qin, Jiaming Liu, Junyu Han, Jingtuo Liu, Errui Ding", "title": "EATEN: Entity-aware Attention for Single Shot Visual Text Extraction", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting entity from images is a crucial part of many OCR applications,\nsuch as entity recognition of cards, invoices, and receipts. Most of the\nexisting works employ classical detection and recognition paradigm. This paper\nproposes an Entity-aware Attention Text Extraction Network called EATEN, which\nis an end-to-end trainable system to extract the entities without any\npost-processing. In the proposed framework, each entity is parsed by its\ncorresponding entity-aware decoder, respectively. Moreover, we innovatively\nintroduce a state transition mechanism which further improves the robustness of\nentity extraction. In consideration of the absence of public benchmarks, we\nconstruct a dataset of almost 0.6 million images in three real-world scenarios\n(train ticket, passport and business card), which is publicly available at\nhttps://github.com/beacandler/EATEN. To the best of our knowledge, EATEN is the\nfirst single shot method to extract entities from images. Extensive experiments\non these benchmarks demonstrate the state-of-the-art performance of EATEN.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 09:12:59 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["guo", "He", ""], ["Qin", "Xiameng", ""], ["Liu", "Jiaming", ""], ["Han", "Junyu", ""], ["Liu", "Jingtuo", ""], ["Ding", "Errui", ""]]}, {"id": "1909.09399", "submitter": "Mehul S. Raval", "authors": "Rupal Agravat, Mehul S Raval", "title": "Brain Tumor Segmentation and Survival Prediction", "comments": "9 Pages", "journal-ref": "BraTS 2019", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper demonstrates the use of the fully convolutional neural network for\nglioma segmentation on the BraTS 2019 dataset. Three-layers deep\nencoder-decoder architecture is used along with dense connection at encoder\npart to propagate the information from coarse layer to deep layers. This\narchitecture is used to train three tumor sub-components separately.\nSubcomponent training weights are initialized with whole tumor weights to get\nthe localization of the tumor within the brain. At the end, three segmentation\nresults were merged to get the entire tumor segmentation. Dice Similarity of\ntraining dataset with focal loss implementation for whole tumor, tumor core and\nenhancing tumor is 0.92, 0.90 and 0.79 respectively. Radiomic features along\nwith segmentation results and age are used to predict the overall survival of\npatients using random forest regressor to classify survival of patients in\nlong, medium and short survival classes. 55.4% of classification accuracy is\nreported for training dataset with the scans whose resection status is\ngross-total resection.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 10:00:32 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Agravat", "Rupal", ""], ["Raval", "Mehul S", ""]]}, {"id": "1909.09408", "submitter": "Fan Zhang", "authors": "Fan Zhang, Yanqin Chen, Zhihang Li, Zhibin Hong, Jingtuo Liu, Feifei\n  Ma, Junyu Han, Errui Ding", "title": "ACFNet: Attentional Class Feature Network for Semantic Segmentation", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have made great progress in semantic segmentation by exploiting\nricher context, most of which are designed from a spatial perspective. In\ncontrast to previous works, we present the concept of class center which\nextracts the global context from a categorical perspective. This class-level\ncontext describes the overall representation of each class in an image. We\nfurther propose a novel module, named Attentional Class Feature (ACF) module,\nto calculate and adaptively combine different class centers according to each\npixel. Based on the ACF module, we introduce a coarse-to-fine segmentation\nnetwork, called Attentional Class Feature Network (ACFNet), which can be\ncomposed of an ACF module and any off-the-shell segmentation network (base\nnetwork). In this paper, we use two types of base networks to evaluate the\neffectiveness of ACFNet. We achieve new state-of-the-art performance of 81.85%\nmIoU on Cityscapes dataset with only finely annotated data used for training.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 10:19:17 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 11:03:24 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2019 02:01:35 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Zhang", "Fan", ""], ["Chen", "Yanqin", ""], ["Li", "Zhihang", ""], ["Hong", "Zhibin", ""], ["Liu", "Jingtuo", ""], ["Ma", "Feifei", ""], ["Han", "Junyu", ""], ["Ding", "Errui", ""]]}, {"id": "1909.09414", "submitter": "Sinem Aslan", "authors": "Sinem Aslan, Marcello Pelillo", "title": "Weakly Supervised Semantic Segmentation Using Constrained Dominant Sets", "comments": null, "journal-ref": "In: Image Analysis and Processing (ICIAP 2019). Lecture Notes in\n  Computer Science, vol 11752. Springer, Cham (2019)", "doi": "10.1007/978-3-030-30645-8_39", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large-scale data sets is an essential pre-requisite for\ndeep learning based semantic segmentation schemes. Since obtaining pixel-level\nlabels is extremely expensive, supervising deep semantic segmentation networks\nusing low-cost weak annotations has been an attractive research problem in\nrecent years. In this work, we explore the potential of Constrained Dominant\nSets (CDS) for generating multi-labeled full mask predictions to train a fully\nconvolutional network (FCN) for semantic segmentation. Our experimental results\nshow that using CDS's yields higher-quality mask predictions compared to\nmethods that have been adopted in the literature for the same purpose.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 10:32:48 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Aslan", "Sinem", ""], ["Pelillo", "Marcello", ""]]}, {"id": "1909.09420", "submitter": "Konstantin Schall", "authors": "Konstantin Schall, Kai Uwe Barthel, Nico Hezel, Klaus Jung", "title": "Deep Aggregation of Regional Convolutional Activations for Content Based\n  Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges of deep learning based image retrieval remains in\naggregating convolutional activations into one highly representative feature\nvector. Ideally, this descriptor should encode semantic, spatial and low level\ninformation. Even though off-the-shelf pre-trained neural networks can already\nproduce good representations in combination with aggregation methods,\nappropriate fine tuning for the task of image retrieval has shown to\nsignificantly boost retrieval performance. In this paper, we present a simple\nyet effective supervised aggregation method built on top of existing regional\npooling approaches. In addition to the maximum activation of a given region, we\ncalculate regional average activations of extracted feature maps. Subsequently,\nweights for each of the pooled feature vectors are learned to perform a\nweighted aggregation to a single feature vector. Furthermore, we apply our\nnewly proposed NRA loss function for deep metric learning to fine tune the\nbackbone neural network and to learn the aggregation weights. Our method\nachieves state-of-the-art results for the INRIA Holidays data set and\ncompetitive results for the Oxford Buildings and Paris data sets while reducing\nthe training time significantly.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 10:43:00 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 06:53:16 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Schall", "Konstantin", ""], ["Barthel", "Kai Uwe", ""], ["Hezel", "Nico", ""], ["Jung", "Klaus", ""]]}, {"id": "1909.09422", "submitter": "Will Price", "authors": "Will Price and Dima Damen", "title": "Retro-Actions: Learning 'Close' by Time-Reversing 'Open' Videos", "comments": "ICCVW 2019, 8 pages, 7 figures, 6 tables.\n  https://video-reversal.willprice.dev/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate video transforms that result in class-homogeneous\nlabel-transforms. These are video transforms that consistently maintain or\nmodify the labels of all videos in each class. We propose a general approach to\ndiscover invariant classes, whose transformed examples maintain their label;\npairs of equivariant classes, whose transformed examples exchange their labels;\nand novel-generating classes, whose transformed examples belong to a new class\noutside the dataset. Label transforms offer additional supervision previously\nunexplored in video recognition benefiting data augmentation and enabling\nzero-shot learning opportunities by learning a class from transformed videos of\nits counterpart.\n  Amongst such video transforms, we study horizontal-flipping, time-reversal,\nand their composition. We highlight errors in naively using horizontal-flipping\nas a form of data augmentation in video. Next, we validate the realism of\ntime-reversed videos through a human perception study where people exhibit\nequal preference for forward and time-reversed videos. Finally, we test our\napproach on two datasets, Jester and Something-Something, evaluating the three\nvideo transforms for zero-shot learning and data augmentation. Our results show\nthat gestures such as zooming in can be learnt from zooming out in a zero-shot\nsetting, as well as more complex actions with state transitions such as digging\nsomething out of something from burying something in something.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 10:52:31 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Price", "Will", ""], ["Damen", "Dima", ""]]}, {"id": "1909.09432", "submitter": "SeyedAbolghasem Mirroshandel", "authors": "Erfan Miahi, Seyed Abolghasem Mirroshandel, Alexis Nasr", "title": "Genetic Neural Architecture Search for automatic assessment of human\n  sperm images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Male infertility is a disease which affects approximately 7% of men. Sperm\nmorphology analysis (SMA) is one of the main diagnosis methods for this\nproblem. Manual SMA is an inexact, subjective, non-reproducible, and hard to\nteach process. As a result, in this paper, we introduce a novel automatic SMA\nbased on a neural architecture search algorithm termed Genetic Neural\nArchitecture Search (GeNAS). For this purpose, we used a collection of images\ncalled MHSMA dataset contains 1,540 sperm images which have been collected from\n235 patients with infertility problems. GeNAS is a genetic algorithm that acts\nas a meta-controller which explores the constrained search space of plain\nconvolutional neural network architectures. Every individual of the genetic\nalgorithm is a convolutional neural network trained to predict morphological\ndeformities in different segments of human sperm (head, vacuole, and acrosome),\nand its fitness is calculated by a novel proposed method named GeNAS-WF\nespecially designed for noisy, low resolution, and imbalanced datasets. Also, a\nhashing method is used to save each trained neural architecture fitness, so we\ncould reuse them during fitness evaluation and speed up the algorithm. Besides,\nin terms of running time and computation power, our proposed architecture\nsearch method is far more efficient than most of the other existing neural\narchitecture search algorithms. Additionally, other proposed methods have been\nevaluated on balanced datasets, whereas GeNAS is built specifically for noisy,\nlow quality, and imbalanced datasets which are common in the field of medical\nimaging. In our experiments, the best neural architecture found by GeNAS has\nreached an accuracy of 91.66%, 77.33%, and 77.66% in the vacuole, head, and\nacrosome abnormality detection, respectively. In comparison to other proposed\nalgorithms for MHSMA dataset, GeNAS achieved state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 11:25:05 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 11:12:35 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Miahi", "Erfan", ""], ["Mirroshandel", "Seyed Abolghasem", ""], ["Nasr", "Alexis", ""]]}, {"id": "1909.09437", "submitter": "Md Jahidul Islam", "authors": "Md Jahidul Islam, Sadman Sakib Enan, Peigen Luo, and Junaed Sattar", "title": "Underwater Image Super-Resolution using Deep Residual Multipliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep residual network-based generative model for single image\nsuper-resolution (SISR) of underwater imagery for use by autonomous underwater\nrobots. We also provide an adversarial training pipeline for learning SISR from\npaired data. In order to supervise the training, we formulate an objective\nfunction that evaluates the \\textit{perceptual quality} of an image based on\nits global content, color, and local style information. Additionally, we\npresent USR-248, a large-scale dataset of three sets of underwater images of\n'high' (640x480) and 'low' (80x60, 160x120, and 320x240) spatial resolution.\nUSR-248 contains paired instances for supervised training of 2x, 4x, or 8x SISR\nmodels. Furthermore, we validate the effectiveness of our proposed model\nthrough qualitative and quantitative experiments and compare the results with\nseveral state-of-the-art models' performances. We also analyze its practical\nfeasibility for applications such as scene understanding and attention modeling\nin noisy visual conditions.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 11:53:07 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 03:05:15 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2020 20:42:34 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Islam", "Md Jahidul", ""], ["Enan", "Sadman Sakib", ""], ["Luo", "Peigen", ""], ["Sattar", "Junaed", ""]]}, {"id": "1909.09470", "submitter": "Xiaoyu Li", "authors": "Xiaoyu Li, Bo Zhang, Jing Liao, Pedro V. Sander", "title": "Document Rectification and Illumination Correction using a Patch-based\n  CNN", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel learning method to rectify document images with various\ndistortion types from a single input image. As opposed to previous\nlearning-based methods, our approach seeks to first learn the distortion flow\non input image patches rather than the entire image. We then present a robust\ntechnique to stitch the patch results into the rectified document by processing\nin the gradient domain. Furthermore, we propose a second network to correct the\nuneven illumination, further improving the readability and OCR accuracy. Due to\nthe less complex distortion present on the smaller image patches, our\npatch-based approach followed by stitching and illumination correction can\nsignificantly improve the overall accuracy in both the synthetic and real\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 12:47:40 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Li", "Xiaoyu", ""], ["Zhang", "Bo", ""], ["Liao", "Jing", ""], ["Sander", "Pedro V.", ""]]}, {"id": "1909.09481", "submitter": "Yaoyao Zhong", "authors": "Yaoyao Zhong and Weihong Deng", "title": "Adversarial Learning with Margin-based Triplet Embedding Regularization", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Deep neural networks (DNNs) have achieved great success on a variety of\ncomputer vision tasks, however, they are highly vulnerable to adversarial\nattacks. To address this problem, we propose to improve the local smoothness of\nthe representation space, by integrating a margin-based triplet embedding\nregularization term into the classification objective, so that the obtained\nmodel learns to resist adversarial examples. The regularization term consists\nof two steps optimizations which find potential perturbations and punish them\nby a large margin in an iterative way. Experimental results on MNIST,\nCASIA-WebFace, VGGFace2 and MS-Celeb-1M reveal that our approach increases the\nrobustness of the network against both feature and label adversarial attacks in\nsimple object classification and deep face recognition.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 13:08:12 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Zhong", "Yaoyao", ""], ["Deng", "Weihong", ""]]}, {"id": "1909.09529", "submitter": "Mansi Sharma", "authors": "Kumar Mridul, M. Ramanathan, Kunal Ahirwar, Mansi Sharma", "title": "Multi-user Augmented Reality Application for Video Communication in\n  Virtual Space", "comments": "European Light Field Imaging Workshop (ELFI 2019), Borovets, Bulgaria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Communication is the most useful tool to impart knowledge, understand ideas,\nclarify thoughts and expressions, organize plan and manage every single\nday-to-day activity. Although there are different modes of communication,\nphysical barrier always affects the clarity of the message due to the absence\nof body language and facial expressions. These barriers are overcome by video\ncalling, which is technically the most advance mode of communication at\npresent. The proposed work concentrates around the concept of video calling in\na more natural and seamless way using Augmented Reality (AR). AR can be helpful\nin giving the users an experience of physical presence in each other's\nenvironment. Our work provides an entirely new platform for video calling,\nwherein the users can enjoy the privilege of their own virtual space to\ninteract with the individual's environment. Moreover, there is no limitation of\nsharing the same screen space. Any number of participants can be accommodated\nover a single conference without having to compromise the screen size.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 14:32:54 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Mridul", "Kumar", ""], ["Ramanathan", "M.", ""], ["Ahirwar", "Kunal", ""], ["Sharma", "Mansi", ""]]}, {"id": "1909.09541", "submitter": "Farzad Khalvati", "authors": "Saman Motamed, Isha Gujrathi, Dominik Deniffel, Anton Oentoro, Masoom\n  A. Haider, Farzad Khalvati", "title": "A Transfer Learning Approach for Automated Segmentation of Prostate\n  Whole Gland and Transition Zone in Diffusion Weighted MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of prostate whole gland and transition zone in Diffusion\nWeighted MRI (DWI) are the first step in designing computer-aided detection\nalgorithms for prostate cancer. However, variations in MRI acquisition\nparameters and scanner manufacturing result in different appearances of\nprostate tissue in the images. Convolutional neural networks (CNNs) which have\nshown to be successful in various medical image analysis tasks including\nsegmentation are typically sensitive to the variations in imaging parameters.\nThis sensitivity leads to poor segmentation performance of CNNs trained on a\nsource cohort and tested on a target cohort from a different scanner and hence,\nit limits the applicability of CNNs for cross-cohort training and testing.\nContouring prostate whole gland and transition zone in DWI images are\ntime-consuming and expensive. Thus, it is important to enable CNNs pretrained\non images of source domain, to segment images of target domain with minimum\nrequirement for manual segmentation of images from the target domain. In this\nwork, we propose a transfer learning method based on a modified U-net\narchitecture and loss function, for segmentation of prostate whole gland and\ntransition zone in DWIs using a CNN pretrained on a source dataset and tested\non the target dataset. We explore the effect of the size of subset of target\ndataset used for fine-tuning the pre-trained CNN on the overall segmentation\naccuracy. Our results show that with a fine-tuning data as few as 30 patients\nfrom the target domain, the proposed transfer learning-based algorithm can\nreach dice score coefficient of 0.80 for both prostate whole gland and\ntransition zone segmentation. Using a fine-tuning data of 115 patients from the\ntarget domain, dice score coefficient of 0.85 and 0.84 are achieved for\nsegmentation of whole gland and transition zone, respectively, in the target\ndomain.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 14:44:50 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 15:45:32 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Motamed", "Saman", ""], ["Gujrathi", "Isha", ""], ["Deniffel", "Dominik", ""], ["Oentoro", "Anton", ""], ["Haider", "Masoom A.", ""], ["Khalvati", "Farzad", ""]]}, {"id": "1909.09548", "submitter": "Lukas Schmid", "authors": "Lukas Schmid, Michael Pantic, Raghav Khanna, Lionel Ott, Roland\n  Siegwart and Juan Nieto", "title": "An Efficient Sampling-based Method for Online Informative Path Planning\n  in Unknown Environments", "comments": "8 pages, 6 figures, video: https://youtu.be/lEadqJ1_8Do, framework:\n  https://github.com/ethz-asl/mav_active_3d_planning", "journal-ref": "IEEE Robotics and Automation Letters, Vol. 5, Iss. 2, April 2020", "doi": "10.1109/LRA.2020.2969191", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to plan informative paths online is essential to robot autonomy.\nIn particular, sampling-based approaches are often used as they are capable of\nusing arbitrary information gain formulations. However, they are prone to local\nminima, resulting in sub-optimal trajectories, and sometimes do not reach\nglobal coverage. In this paper, we present a new RRT*-inspired online\ninformative path planning algorithm. Our method continuously expands a single\ntree of candidate trajectories and rewires segments to maintain the tree and\nrefine intermediate trajectories. This allows the algorithm to achieve global\ncoverage and maximize the utility of a path in a global context, using a single\nobjective function. We demonstrate the algorithm's capabilities in the\napplications of autonomous indoor exploration as well as accurate Truncated\nSigned Distance Field (TSDF)-based 3D reconstruction on-board a Micro Aerial\nvehicle (MAV). We study the impact of commonly used information gain and cost\nformulations in these scenarios and propose a novel TSDF-based 3D\nreconstruction gain and cost-utility formulation. Detailed evaluation in\nrealistic simulation environments show that our approach outperforms state of\nthe art methods in these tasks. Experiments on a real MAV demonstrate the\nability of our method to robustly plan in real-time, exploring an indoor\nenvironment solely with on-board sensing and computation. We make our framework\navailable for future research.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 15:07:14 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 15:09:43 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Schmid", "Lukas", ""], ["Pantic", "Michael", ""], ["Khanna", "Raghav", ""], ["Ott", "Lionel", ""], ["Siegwart", "Roland", ""], ["Nieto", "Juan", ""]]}, {"id": "1909.09552", "submitter": "Tong Wu", "authors": "Tong Wu, Liang Tong, Yevgeniy Vorobeychik", "title": "Defending Against Physically Realizable Attacks on Image Classification", "comments": "camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of defending deep neural network approaches for image\nclassification from physically realizable attacks. First, we demonstrate that\nthe two most scalable and effective methods for learning robust models,\nadversarial training with PGD attacks and randomized smoothing, exhibit very\nlimited effectiveness against three of the highest profile physical attacks.\nNext, we propose a new abstract adversarial model, rectangular occlusion\nattacks, in which an adversary places a small adversarially crafted rectangle\nin an image, and develop two approaches for efficiently computing the resulting\nadversarial examples. Finally, we demonstrate that adversarial training using\nour new attack yields image classification models that exhibit high robustness\nagainst the physically realizable attacks we study, offering the first\neffective generic defense against such attacks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 15:11:09 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 20:07:55 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Wu", "Tong", ""], ["Tong", "Liang", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "1909.09566", "submitter": "Behnaz Rezaei", "authors": "Behnaz Rezaei, Yiorgos Christakis, Bryan Ho, Kevin Thomas, Kelley Erb,\n  Sarah Ostadabbas and Shyamal Patel", "title": "Target-Specific Action Classification for Automated Assessment of Human\n  Motor Behavior from Video", "comments": "This manuscript is under submission to the Sensors journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective monitoring and assessment of human motor behavior can improve the\ndiagnosis and management of several medical conditions. Over the past decade,\nsignificant advances have been made in the use of wearable technology for\ncontinuously monitoring human motor behavior in free-living conditions.\nHowever, wearable technology remains ill-suited for applications which require\nmonitoring and interpretation of complex motor behaviors (e.g. involving\ninteractions with the environment). Recent advances in computer vision and deep\nlearning have opened up new possibilities for extracting information from video\nrecordings. In this paper, we present a hierarchical vision-based behavior\nphenotyping method for classification of basic human actions in video\nrecordings performed using a single RGB camera. Our method addresses challenges\nassociated with tracking multiple human actors and classification of actions in\nvideos recorded in changing environments with different fields of view. We\nimplement a cascaded pose tracker that uses temporal relationships between\ndetections for short-term tracking and appearance-based tracklet fusion for\nlong-term tracking. Furthermore, for action classification, we use pose\nevolution maps derived from the cascaded pose tracker as low-dimensional and\ninterpretable representations of the movement sequences for training a\nconvolutional neural network. The cascaded pose tracker achieves an average\naccuracy of 88\\% in tracking the target human actor in our video recordings,\nand overall system achieves average test accuracy of 84\\% for target-specific\naction classification in untrimmed video recordings.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 15:40:05 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Rezaei", "Behnaz", ""], ["Christakis", "Yiorgos", ""], ["Ho", "Bryan", ""], ["Thomas", "Kevin", ""], ["Erb", "Kelley", ""], ["Ostadabbas", "Sarah", ""], ["Patel", "Shyamal", ""]]}, {"id": "1909.09569", "submitter": "Yao Shu", "authors": "Yao Shu, Wei Wang and Shaofeng Cai", "title": "Understanding Architectures Learnt by Cell-based Neural Architecture\n  Search", "comments": "Published as a conference paper at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) searches architectures automatically for\ngiven tasks, e.g., image classification and language modeling. Improving the\nsearch efficiency and effectiveness have attracted increasing attention in\nrecent years. However, few efforts have been devoted to understanding the\ngenerated architectures. In this paper, we first reveal that existing NAS\nalgorithms (e.g., DARTS, ENAS) tend to favor architectures with wide and\nshallow cell structures. These favorable architectures consistently achieve\nfast convergence and are consequently selected by NAS algorithms. Our empirical\nand theoretical study further confirms that their fast convergence derives from\ntheir smooth loss landscape and accurate gradient information. Nonetheless,\nthese architectures may not necessarily lead to better generalization\nperformance compared with other candidate architectures in the same search\nspace, and therefore further improvement is possible by revising existing NAS\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 15:49:45 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 12:14:40 GMT"}, {"version": "v3", "created": "Wed, 1 Jan 2020 13:57:23 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Shu", "Yao", ""], ["Wang", "Wei", ""], ["Cai", "Shaofeng", ""]]}, {"id": "1909.09592", "submitter": "Kanji Tanaka", "authors": "Sugimoto Takuma, Yamaguchi Kousuke, Tanaka Kanji", "title": "Fault-Diagnosing SLAM for Varying Scale Change Detection", "comments": "7 pages, 4 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new fault diagnosis (FD) -based approach for\ndetection of imagery changes that can detect significant changes as\ninconsistencies between different sub-modules (e.g., self-localizaiton) of\nvisual SLAM. Unlike classical change detection approaches such as pairwise\nimage comparison (PC) and anomaly detection (AD), neither the memorization of\neach map image nor the maintenance of up-to-date place-specific anomaly\ndetectors are required in this FD approach. A significant challenge that is\nencountered when incorporating different SLAM sub-modules into FD involves\ndealing with the varying scales of objects that have changed (e.g., the\nappearance of small dangerous obstacles on the floor). To address this issue,\nwe reconsider the bag-of-words (BoW) image representation, by exploiting its\nrecent advances in terms of self-localization and change detection. As a key\nadvantage, BoW image representation can be reorganized into any different\nscaling by simply cropping the original BoW image. Furthermore, we propose to\ncombine different self-localization modules with strong and weak BoW features\nwith different discriminativity, and to treat inconsistency between strong and\nweak self-localization as an indicator of change. The efficacy of the proposed\napproach for FD with/without AD and/or PC was experimentally validated.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 08:52:45 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Takuma", "Sugimoto", ""], ["Kousuke", "Yamaguchi", ""], ["Kanji", "Tanaka", ""]]}, {"id": "1909.09594", "submitter": "Kanji Tanaka", "authors": "Tanaka Kanji", "title": "Mining Minimal Map-Segments for Visual Place Classifiers", "comments": "8 pages, 4 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In visual place recognition (VPR), map segmentation (MS) is a preprocessing\ntechnique used to partition a given view-sequence map into place classes (i.e.,\nmap segments) so that each class has good place-specific training images for a\nvisual place classifier (VPC). Existing approaches to MS implicitly/explicitly\nsuppose that map segments have a certain size, or individual map segments are\nbalanced in size. However, recent VPR systems showed that very small important\nmap segments (minimal map segments) often suffice for VPC, and the remaining\nlarge unimportant portion of the map should be discarded to minimize map\nmaintenance cost. Here, a new MS algorithm that can mine minimal map segments\nfrom a large view-sequence map is presented. To solve the inherently NP hard\nproblem, MS is formulated as a video-segmentation problem and the efficient\npoint-trajectory based paradigm of video segmentation is used. The proposed map\nrepresentation was implemented with three types of VPC: deep convolutional\nneural network, bag-of-words, and object class detector, and each was\nintegrated into a Monte Carlo localization algorithm (MCL) within a topometric\nVPR framework. Experiments using the publicly available NCLT dataset thoroughly\ninvestigate the efficacy of MS in terms of VPR performance.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 02:49:47 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Kanji", "Tanaka", ""]]}, {"id": "1909.09598", "submitter": "Heon Lee", "authors": "Samuel Yu, Heon Lee, Jung Hoon Kim", "title": "Street Crossing Aid Using Light-weight CNNs for the Visually Impaired", "comments": "10 pages, 5 figures, 7 tables, ICCV 2019 - 7th International Workshop\n  on Assistive Computer Vision and Robotics (ACVR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we address an issue that the visually impaired commonly face\nwhile crossing intersections and propose a solution that takes form as a mobile\napplication. The application utilizes a deep learning convolutional neural\nnetwork model, LytNetV2, to output necessary information that the visually\nimpaired may lack when without human companions or guide-dogs. A prototype of\nthe application runs on iOS devices of versions 11 or above. It is designed for\ncomprehensiveness, concision, accuracy, and computational efficiency through\ndelivering the two most important pieces of information, pedestrian traffic\nlight color and direction, required to cross the road in real-time.\nFurthermore, it is specifically aimed to support those facing financial burden\nas the solution takes the form of a free mobile application. Through the\nmodification and utilization of key principles in MobileNetV3 such as depthwise\nseperable convolutions and squeeze-excite layers, the deep neural network model\nachieves a classification accuracy of 96% and average angle error of 6.15\ndegrees, while running at a frame rate of 16.34 frames per second.\nAdditionally, the model is trained as an image classifier, allowing for a\nfaster and more accurate model. The network is able to outperform other methods\nsuch as object detection and non-deep learning algorithms in both accuracy and\nthoroughness. The information is delivered through both auditory signals and\nvibrations, and it has been tested on seven visually impaired and has received\nabove satisfactory responses.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 11:29:33 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Yu", "Samuel", ""], ["Lee", "Heon", ""], ["Kim", "Jung Hoon", ""]]}, {"id": "1909.09602", "submitter": "Brian Hutchinson", "authors": "Chris Careaga, Brian Hutchinson, Nathan Hodas and Lawrence Phillips", "title": "Metric-Based Few-Shot Learning for Video Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the few-shot scenario, a learner must effectively generalize to unseen\nclasses given a small support set of labeled examples. While a relatively large\namount of research has gone into few-shot learning for image classification,\nlittle work has been done on few-shot video classification. In this work, we\naddress the task of few-shot video action recognition with a set of two-stream\nmodels. We evaluate the performance of a set of convolutional and recurrent\nneural network video encoder architectures used in conjunction with three\npopular metric-based few-shot algorithms. We train and evaluate using a\nfew-shot split of the Kinetics 600 dataset. Our experiments confirm the\nimportance of the two-stream setup, and find prototypical networks and pooled\nlong short-term memory network embeddings to give the best performance as\nfew-shot method and video encoder, respectively. For a 5-shot 5-way task, this\nsetup obtains 84.2% accuracy on the test set and 59.4% on a special \"challenge\"\ntest set, composed of highly confusable classes.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 17:53:16 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Careaga", "Chris", ""], ["Hutchinson", "Brian", ""], ["Hodas", "Nathan", ""], ["Phillips", "Lawrence", ""]]}, {"id": "1909.09629", "submitter": "Andreas Lugmayr", "authors": "Andreas Lugmayr and Martin Danelljan and Radu Timofte", "title": "Unsupervised Learning for Real-World Super-Resolution", "comments": "To appear in the AIM 2019 workshop at ICCV. Includes supplementary\n  material", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most current super-resolution methods rely on low and high resolution image\npairs to train a network in a fully supervised manner. However, such image\npairs are not available in real-world applications. Instead of directly\naddressing this problem, most works employ the popular bicubic downsampling\nstrategy to artificially generate a corresponding low resolution image.\nUnfortunately, this strategy introduces significant artifacts, removing natural\nsensor noise and other real-world characteristics. Super-resolution networks\ntrained on such bicubic images therefore struggle to generalize to natural\nimages. In this work, we propose an unsupervised approach for image\nsuper-resolution. Given only unpaired data, we learn to invert the effects of\nbicubic downsampling in order to restore the natural image characteristics\npresent in the data. This allows us to generate realistic image pairs,\nfaithfully reflecting the distribution of real-world images. Our\nsuper-resolution network can therefore be trained with direct pixel-wise\nsupervision in the high resolution domain, while robustly generalizing to real\ninput. We demonstrate the effectiveness of our approach in quantitative and\nqualitative experiments.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 17:37:55 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Lugmayr", "Andreas", ""], ["Danelljan", "Martin", ""], ["Timofte", "Radu", ""]]}, {"id": "1909.09656", "submitter": "Arber Zela", "authors": "Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas\n  Brox, Frank Hutter", "title": "Understanding and Robustifying Differentiable Architecture Search", "comments": "In: International Conference on Learning Representations (ICLR 2020);\n  28 pages, 30 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable Architecture Search (DARTS) has attracted a lot of attention\ndue to its simplicity and small search costs achieved by a continuous\nrelaxation and an approximation of the resulting bi-level optimization problem.\nHowever, DARTS does not work robustly for new problems: we identify a wide\nrange of search spaces for which DARTS yields degenerate architectures with\nvery poor test performance. We study this failure mode and show that, while\nDARTS successfully minimizes validation loss, the found solutions generalize\npoorly when they coincide with high validation loss curvature in the\narchitecture space. We show that by adding one of various types of\nregularization we can robustify DARTS to find solutions with less curvature and\nbetter generalization properties. Based on these observations, we propose\nseveral simple variations of DARTS that perform substantially more robustly in\npractice. Our observations are robust across five search spaces on three image\nclassification tasks and also hold for the very different domains of disparity\nestimation (a dense regression task) and language modelling.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 18:03:06 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 14:14:05 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Zela", "Arber", ""], ["Elsken", "Thomas", ""], ["Saikia", "Tonmoy", ""], ["Marrakchi", "Yassine", ""], ["Brox", "Thomas", ""], ["Hutter", "Frank", ""]]}, {"id": "1909.09675", "submitter": "Yu-Jhe Li", "authors": "Yu-Jhe Li, Ci-Siang Lin, Yan-Bo Lin, Yu-Chiang Frank Wang", "title": "Cross-Dataset Person Re-Identification via Unsupervised Pose\n  Disentanglement and Adaptation", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) aims at recognizing the same person from\nimages taken across different cameras. To address this challenging task,\nexisting re-ID models typically rely on a large amount of labeled training\ndata, which is not practical for real-world applications. To alleviate this\nlimitation, researchers now targets at cross-dataset re-ID which focuses on\ngeneralizing the discriminative ability to the unlabeled target domain when\ngiven a labeled source domain dataset. To achieve this goal, our proposed Pose\nDisentanglement and Adaptation Network (PDA-Net) aims at learning deep image\nrepresentation with pose and domain information properly disentangled. With the\nlearned cross-domain pose invariant feature space, our proposed PDA-Net is able\nto perform pose disentanglement across domains without supervision in\nidentities, and the resulting features can be applied to cross-dataset re-ID.\nBoth of our qualitative and quantitative results on two benchmark datasets\nconfirm the effectiveness of our approach and its superiority over the\nstate-of-the-art cross-dataset Re-ID approaches.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 18:54:05 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Li", "Yu-Jhe", ""], ["Lin", "Ci-Siang", ""], ["Lin", "Yan-Bo", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "1909.09677", "submitter": "Zhe Huang", "authors": "Zhe Huang, Weijiang Yu, Wayne Zhang, Litong Feng, Nong Xiao", "title": "Gradual Network for Single Image De-raining", "comments": "In Proceedings of the 27th ACM International Conference on Multimedia\n  (MM 2019)", "journal-ref": null, "doi": "10.1145/3343031.3350883", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most advances in single image de-raining meet a key challenge, which is\nremoving rain streaks with different scales and shapes while preserving image\ndetails. Existing single image de-raining approaches treat rain-streak removal\nas a process of pixel-wise regression directly. However, they are lacking in\nmining the balance between over-de-raining (e.g. removing texture details in\nrain-free regions) and under-de-raining (e.g. leaving rain streaks). In this\npaper, we firstly propose a coarse-to-fine network called Gradual Network\n(GraNet) consisting of coarse stage and fine stage for delving into single\nimage de-raining with different granularities. Specifically, to reveal\ncoarse-grained rain-streak characteristics (e.g. long and thick rain\nstreaks/raindrops), we propose a coarse stage by utilizing local-global spatial\ndependencies via a local-global subnetwork composed of region-aware blocks.\nTaking the residual result (the coarse de-rained result) between the rainy\nimage sample (i.e. the input data) and the output of coarse stage (i.e. the\nlearnt rain mask) as input, the fine stage continues to de-rain by removing the\nfine-grained rain streaks (e.g. light rain streaks and water mist) to get a\nrain-free and well-reconstructed output image via a unified contextual merging\nsub-network with dense blocks and a merging block. Solid and comprehensive\nexperiments on synthetic and real data demonstrate that our GraNet can\nsignificantly outperform the state-of-the-art methods by removing rain streaks\nwith various densities, scales and shapes while keeping the image details of\nrain-free regions well-preserved.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 18:56:08 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Huang", "Zhe", ""], ["Yu", "Weijiang", ""], ["Zhang", "Wayne", ""], ["Feng", "Litong", ""], ["Xiao", "Nong", ""]]}, {"id": "1909.09709", "submitter": "Xiaofan Zhang", "authors": "Xiaofan Zhang, Haoming Lu, Cong Hao, Jiachen Li, Bowen Cheng, Yuhong\n  Li, Kyle Rupnow, Jinjun Xiong, Thomas Huang, Honghui Shi, Wen-mei Hwu, Deming\n  Chen", "title": "SkyNet: a Hardware-Efficient Method for Object Detection and Tracking on\n  Embedded Systems", "comments": "Published as a conference paper at Conference on Machine Learning and\n  Systems (MLSys) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection and tracking are challenging tasks for resource-constrained\nembedded systems. While these tasks are among the most compute-intensive tasks\nfrom the artificial intelligence domain, they are only allowed to use limited\ncomputation and memory resources on embedded devices. In the meanwhile, such\nresource-constrained implementations are often required to satisfy additional\ndemanding requirements such as real-time response, high-throughput performance,\nand reliable inference accuracy. To overcome these challenges, we propose\nSkyNet, a hardware-efficient neural network to deliver the state-of-the-art\ndetection accuracy and speed for embedded systems. Instead of following the\ncommon top-down flow for compact DNN (Deep Neural Network) design, SkyNet\nprovides a bottom-up DNN design approach with comprehensive understanding of\nthe hardware constraints at the very beginning to deliver hardware-efficient\nDNNs. The effectiveness of SkyNet is demonstrated by winning the competitive\nSystem Design Contest for low power object detection in the 56th IEEE/ACM\nDesign Automation Conference (DAC-SDC), where our SkyNet significantly\noutperforms all other 100+ competitors: it delivers 0.731 Intersection over\nUnion (IoU) and 67.33 frames per second (FPS) on a TX2 embedded GPU; and 0.716\nIoU and 25.05 FPS on an Ultra96 embedded FPGA. The evaluation of SkyNet is also\nextended to GOT-10K, a recent large-scale high-diversity benchmark for generic\nobject tracking in the wild. For state-of-the-art object trackers SiamRPN++ and\nSiamMask, where ResNet-50 is employed as the backbone, implementations using\nour SkyNet as the backbone DNN are 1.60X and 1.73X faster with better or\nsimilar accuracy when running on a 1080Ti GPU, and 37.20X smaller in terms of\nparameter size for significantly better memory and storage footprint.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 20:26:43 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 22:48:22 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Zhang", "Xiaofan", ""], ["Lu", "Haoming", ""], ["Hao", "Cong", ""], ["Li", "Jiachen", ""], ["Cheng", "Bowen", ""], ["Li", "Yuhong", ""], ["Rupnow", "Kyle", ""], ["Xiong", "Jinjun", ""], ["Huang", "Thomas", ""], ["Shi", "Honghui", ""], ["Hwu", "Wen-mei", ""], ["Chen", "Deming", ""]]}, {"id": "1909.09716", "submitter": "Chunwei Ma", "authors": "Chunwei Ma, Zhanghexuan Ji, Mingchen Gao", "title": "Neural Style Transfer Improves 3D Cardiovascular MR Image Segmentation\n  on Inconsistent Data", "comments": "22nd International Conference on Medical Image Computing and Computer\n  Assisted Intervention (MICCAI 2019) early accept", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional medical image segmentation is one of the most important\nproblems in medical image analysis and plays a key role in downstream diagnosis\nand treatment. Recent years, deep neural networks have made groundbreaking\nsuccess in medical image segmentation problem. However, due to the high\nvariance in instrumental parameters, experimental protocols, and subject\nappearances, the generalization of deep learning models is often hindered by\nthe inconsistency in medical images generated by different machines and\nhospitals. In this work, we present StyleSegor, an efficient and easy-to-use\nstrategy to alleviate this inconsistency issue. Specifically, neural style\ntransfer algorithm is applied to unlabeled data in order to minimize the\ndifferences in image properties including brightness, contrast, texture, etc.\nbetween the labeled and unlabeled data. We also apply probabilistic adjustment\non the network output and integrate multiple predictions through ensemble\nlearning. On a publicly available whole heart segmentation benchmarking dataset\nfrom MICCAI HVSMR 2016 challenge, we have demonstrated an elevated dice\naccuracy surpassing current state-of-the-art method and notably, an improvement\nof the total score by 29.91\\%. StyleSegor is thus corroborated to be an\naccurate tool for 3D whole heart segmentation especially on highly inconsistent\ndata, and is available at https://github.com/horsepurve/StyleSegor.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 20:58:45 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Ma", "Chunwei", ""], ["Ji", "Zhanghexuan", ""], ["Gao", "Mingchen", ""]]}, {"id": "1909.09720", "submitter": "Mohammad Rezaei", "authors": "Mohammad Rezaei, Nader Naderi", "title": "Persian Signature Verification using Fully Convolutional Networks", "comments": null, "journal-ref": "2nd National Conference on New Researches in Electrical and\n  Computer Engineering, 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional networks (FCNs) have been recently used for feature\nextraction and classification in image and speech recognition, where their\ninputs have been raw signal or other complicated features. Persian signature\nverification is done using conventional convolutional neural networks (CNNs).\nIn this paper, we propose to use FCN for learning a robust feature extraction\nfrom the raw signature images. FCN can be considered as a variant of CNN where\nits fully connected layers are replaced with a global pooling layer. In the\nproposed manner, FCN inputs are raw signature images and convolution filter\nsize is fixed. Recognition accuracy on UTSig database, shows that FCN with a\nglobal average pooling outperforms CNN.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 21:24:03 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Rezaei", "Mohammad", ""], ["Naderi", "Nader", ""]]}, {"id": "1909.09722", "submitter": "Mohammad Rezaei", "authors": "Mohammad Rezaei, Ali Ahmadi, Navid Naderi", "title": "Content-based image retrieval using Mix histogram", "comments": null, "journal-ref": "2d National Conference on New Research in Electrical and Computer\n  Engineering, 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method to extract image low-level features, namely\nmix histogram (MH), for content-based image retrieval. Since color and edge\norientation features are important visual information which help the human\nvisual system percept and discriminate different images, this method extracts\nand integrates color and edge orientation information in order to measure\nsimilarity between different images. Traditional color histograms merely focus\non the global distribution of color in the image and therefore fail to extract\nother visual features. The MH is attempting to overcome this problem by\nextracting edge orientations as well as color feature. The unique\ncharacteristic of the MH is that it takes into consideration both color and\nedge orientation information in an effective manner. Experimental results show\nthat it outperforms many existing methods which were originally developed for\nimage retrieval purposes.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 21:24:16 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Rezaei", "Mohammad", ""], ["Ahmadi", "Ali", ""], ["Naderi", "Navid", ""]]}, {"id": "1909.09725", "submitter": "Qiqi Hou", "authors": "Qiqi Hou, Feng Liu", "title": "Context-Aware Image Matting for Simultaneous Foreground and Alpha\n  Estimation", "comments": "This is the camera ready version of ICCV2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Natural image matting is an important problem in computer vision and\ngraphics. It is an ill-posed problem when only an input image is available\nwithout any external information. While the recent deep learning approaches\nhave shown promising results, they only estimate the alpha matte. This paper\npresents a context-aware natural image matting method for simultaneous\nforeground and alpha matte estimation. Our method employs two encoder networks\nto extract essential information for matting. Particularly, we use a matting\nencoder to learn local features and a context encoder to obtain more global\ncontext information. We concatenate the outputs from these two encoders and\nfeed them into decoder networks to simultaneously estimate the foreground and\nalpha matte. To train this whole deep neural network, we employ both the\nstandard Laplacian loss and the feature loss: the former helps to achieve high\nnumerical performance while the latter leads to more perceptually plausible\nresults. We also report several data augmentation strategies that greatly\nimprove the network's generalization performance. Our qualitative and\nquantitative experiments show that our method enables high-quality matting for\na single natural image. Our inference codes and models have been made publicly\navailable at https://github.com/hqqxyy/Context-Aware-Matting.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 21:36:30 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 23:24:17 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Hou", "Qiqi", ""], ["Liu", "Feng", ""]]}, {"id": "1909.09741", "submitter": "Ricardo Borsoi", "authors": "Ricardo Augusto Borsoi, Tales Imbiriba, Jos\\'e Carlos Moreira\n  Bermudez, C\\'edric Richard", "title": "Deep Generative Models for Library Augmentation in Multiple Endmember\n  Spectral Mixture Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Endmember Spectral Mixture Analysis (MESMA) is one of the leading\napproaches to perform spectral unmixing (SU) considering variability of the\nendmembers (EMs). It represents each EM in the image using libraries of\nspectral signatures acquired a priori. However, existing spectral libraries are\noften small and unable to properly capture the variability of each EM in\npractical scenes, which compromises the performance of MESMA. In this paper, we\npropose a library augmentation strategy to increase the diversity of existing\nspectral libraries, thus improving their ability to represent the materials in\nreal images. First, we leverage the power of deep generative models to learn\nthe statistical distribution of the EMs based on the spectral signatures\navailable in the existing libraries. Afterwards, new samples can be drawn from\nthe learned EM distributions and used to augment the spectral libraries,\nimproving the overall quality of the SU process. Experimental results using\nsynthetic and real data attest the superior performance of the proposed method\neven under library mismatch conditions.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 23:51:24 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 01:12:29 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Borsoi", "Ricardo Augusto", ""], ["Imbiriba", "Tales", ""], ["Bermudez", "Jos\u00e9 Carlos Moreira", ""], ["Richard", "C\u00e9dric", ""]]}, {"id": "1909.09767", "submitter": "Zehui Yao", "authors": "Zehui Yao, Boyan Zhang, Zhiyong Wang, Wanli Ouyang, Dong Xu, Dagan\n  Feng", "title": "IntersectGAN: Learning Domain Intersection for Generating Images with\n  Multiple Attributes", "comments": null, "journal-ref": null, "doi": "10.1145/3343031.3350908", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have demonstrated great success in\ngenerating various visual content. However, images generated by existing GANs\nare often of attributes (e.g., smiling expression) learned from one image\ndomain. As a result, generating images of multiple attributes requires many\nreal samples possessing multiple attributes which are very resource expensive\nto be collected. In this paper, we propose a novel GAN, namely IntersectGAN, to\nlearn multiple attributes from different image domains through an intersecting\narchitecture. For example, given two image domains $X_1$ and $X_2$ with certain\nattributes, the intersection $X_1 \\cap X_2$ denotes a new domain where images\npossess the attributes from both $X_1$ and $X_2$ domains. The proposed\nIntersectGAN consists of two discriminators $D_1$ and $D_2$ to distinguish\nbetween generated and real samples of different domains, and three generators\nwhere the intersection generator is trained against both discriminators. And an\noverall adversarial loss function is defined over three generators. As a\nresult, our proposed IntersectGAN can be trained on multiple domains of which\neach presents one specific attribute, and eventually eliminates the need of\nreal sample images simultaneously possessing multiple attributes. By using the\nCelebFaces Attributes dataset, our proposed IntersectGAN is able to produce\nhigh quality face images possessing multiple attributes (e.g., a face with\nblack hair and a smiling expression). Both qualitative and quantitative\nevaluations are conducted to compare our proposed IntersectGAN with other\nbaseline methods. Besides, several different applications of IntersectGAN have\nbeen explored with promising results.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 03:40:02 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 10:18:21 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Yao", "Zehui", ""], ["Zhang", "Boyan", ""], ["Wang", "Zhiyong", ""], ["Ouyang", "Wanli", ""], ["Xu", "Dong", ""], ["Feng", "Dagan", ""]]}, {"id": "1909.09777", "submitter": "Kemal Oksuz", "authors": "Kemal Oksuz, Baris Can Cam, Emre Akbas, Sinan Kalkan", "title": "Generating Positive Bounding Boxes for Balanced Training of Object\n  Detectors", "comments": "To appear in WACV 20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-stage deep object detectors generate a set of regions-of-interest (RoI)\nin the first stage, then, in the second stage, identify objects among the\nproposed RoIs that sufficiently overlap with a ground truth (GT) box. The\nsecond stage is known to suffer from a bias towards RoIs that have low\nintersection-over-union (IoU) with the associated GT boxes. To address this\nissue, we first propose a sampling method to generate bounding boxes (BB) that\noverlap with a given reference box more than a given IoU threshold. Then, we\nuse this BB generation method to develop a positive RoI (pRoI) generator that\nproduces RoIs following any desired spatial or IoU distribution, for the\nsecond-stage. We show that our pRoI generator is able to simulate other\nsampling methods for positive examples such as hard example mining and prime\nsampling. Using our generator as an analysis tool, we show that (i) IoU\nimbalance has an adverse effect on performance, (ii) hard positive example\nmining improves the performance only for certain input IoU distributions, and\n(iii) the imbalance among the foreground classes has an adverse effect on\nperformance and that it can be alleviated at the batch level. Finally, we train\nFaster R-CNN using our pRoI generator and, compared to conventional training,\nobtain better or on-par performance for low IoUs and significant improvements\nwhen trained for higher IoUs for Pascal VOC and MS COCO datasets. The code is\navailable at: https://github.com/kemaloksuz/BoundingBoxGenerator.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 05:27:15 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 14:51:01 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2020 07:50:03 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Oksuz", "Kemal", ""], ["Cam", "Baris Can", ""], ["Akbas", "Emre", ""], ["Kalkan", "Sinan", ""]]}, {"id": "1909.09801", "submitter": "Saypraseuth Mounsaveng", "authors": "Saypraseuth Mounsaveng, David Vazquez, Ismail Ben Ayed, Marco\n  Pedersoli", "title": "Adversarial Learning of General Transformations for Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation (DA) is fundamental against overfitting in large\nconvolutional neural networks, especially with a limited training dataset. In\nimages, DA is usually based on heuristic transformations, like geometric or\ncolor transformations. Instead of using predefined transformations, our work\nlearns data augmentation directly from the training data by learning to\ntransform images with an encoder-decoder architecture combined with a spatial\ntransformer network. The transformed images still belong to the same class but\nare new, more complex samples for the classifier. Our experiments show that our\napproach is better than previous generative data augmentation methods, and\ncomparable to predefined transformation methods when training an image\nclassifier.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 09:43:24 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Mounsaveng", "Saypraseuth", ""], ["Vazquez", "David", ""], ["Ayed", "Ismail Ben", ""], ["Pedersoli", "Marco", ""]]}, {"id": "1909.09803", "submitter": "Huangying Zhan", "authors": "Huangying Zhan, Chamara Saroj Weerasekera, Jiawang Bian, Ian Reid", "title": "Visual Odometry Revisited: What Should Be Learnt?", "comments": "ICRA2020. Demo video: https://youtu.be/Nl8mFU4SJKY Code:\n  https://github.com/Huangying-Zhan/DF-VO", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a monocular visual odometry (VO) algorithm which\nleverages geometry-based methods and deep learning. Most existing VO/SLAM\nsystems with superior performance are based on geometry and have to be\ncarefully designed for different application scenarios. Moreover, most\nmonocular systems suffer from scale-drift issue.Some recent deep learning works\nlearn VO in an end-to-end manner but the performance of these deep systems is\nstill not comparable to geometry-based methods. In this work, we revisit the\nbasics of VO and explore the right way for integrating deep learning with\nepipolar geometry and Perspective-n-Point (PnP) method. Specifically, we train\ntwo convolutional neural networks (CNNs) for estimating single-view depths and\ntwo-view optical flows as intermediate outputs. With the deep predictions, we\ndesign a simple but robust frame-to-frame VO algorithm (DF-VO) which\noutperforms pure deep learning-based and geometry-based methods. More\nimportantly, our system does not suffer from the scale-drift issue being aided\nby a scale consistent single-view depth CNN. Extensive experiments on KITTI\ndataset shows the robustness of our system and a detailed ablation study shows\nthe effect of different factors in our system.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 10:00:21 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 12:26:26 GMT"}, {"version": "v3", "created": "Mon, 17 Feb 2020 11:40:17 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2020 02:32:35 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Zhan", "Huangying", ""], ["Weerasekera", "Chamara Saroj", ""], ["Bian", "Jiawang", ""], ["Reid", "Ian", ""]]}, {"id": "1909.09816", "submitter": "Ioannis Ivrissimtzis", "authors": "Luma Omar and Ioannis Ivrissimtzis", "title": "Using theoretical ROC curves for analysing machine learning binary\n  classifiers", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most binary classifiers work by processing the input to produce a scalar\nresponse and comparing it to a threshold value. The various measures of\nclassifier performance assume, explicitly or implicitly, probability\ndistributions $P_s$ and $P_n$ of the response belonging to either class,\nprobability distributions for the cost of each type of misclassification, and\ncompute a performance score from the expected cost.\n  In machine learning, classifier responses are obtained experimentally and\nperformance scores are computed directly from them, without any assumptions on\n$P_s$ and $P_n$. Here, we argue that the omitted step of estimating theoretical\ndistributions for $P_s$ and $P_n$ can be useful. In a biometric security\nexample, we fit beta distributions to the responses of two classifiers, one\nbased on logistic regression and one on ANNs, and use them to establish a\ncategorisation into a small number of classes with different extremal\nbehaviours at the ends of the ROC curves.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 11:48:19 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Omar", "Luma", ""], ["Ivrissimtzis", "Ioannis", ""]]}, {"id": "1909.09822", "submitter": "Zhi Chen", "authors": "Zhi Chen, Jingjing Li, Yadan Luo, Zi Huang, Yang Yang", "title": "CANZSL: Cycle-Consistent Adversarial Networks for Zero-Shot Learning\n  from Natural Language", "comments": "WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods using generative adversarial approaches for Zero-Shot\nLearning (ZSL) aim to generate realistic visual features from class semantics\nby a single generative network, which is highly under-constrained. As a result,\nthe previous methods cannot guarantee that the generated visual features can\ntruthfully reflect the corresponding semantics. To address this issue, we\npropose a novel method named Cycle-consistent Adversarial Networks for\nZero-Shot Learning (CANZSL). It encourages a visual feature generator to\nsynthesize realistic visual features from semantics, and then inversely\ntranslate back synthesized the visual feature to corresponding semantic space\nby a semantic feature generator. Furthermore, in this paper a more challenging\nand practical ZSL problem is considered where the original semantics are from\nnatural language with irrelevant words instead of clean semantics that are\nwidely used in previous work. Specifically, a multi-modal consistent\nbidirectional generative adversarial network is trained to handle unseen\ninstances by leveraging noise in the natural language. A forward one-to-many\nmapping from one text description to multiple visual features is coupled with\nan inverse many-to-one mapping from the visual space to the semantic space.\nThus, a multi-modal cycle-consistency loss between the synthesized semantic\nrepresentations and the ground truth can be learned and leveraged to enforce\nthe generated semantic features to approximate to the real distribution in\nsemantic space. Extensive experiments are conducted to demonstrate that our\nmethod consistently outperforms state-of-the-art approaches on natural\nlanguage-based zero-shot learning tasks.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 13:19:15 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Chen", "Zhi", ""], ["Li", "Jingjing", ""], ["Luo", "Yadan", ""], ["Huang", "Zi", ""], ["Yang", "Yang", ""]]}, {"id": "1909.09823", "submitter": "Manu Airaksinen", "authors": "Manu Airaksinen, Okko R\\\"as\\\"anen, Elina Il\\'en, Taru H\\\"ayrinen, Anna\n  Kivi, Viviana Marchi, Anastasia Gallen, Sonja Blom, Anni Varhe, Nico\n  Kaartinen, Leena Haataja, Sampsa Vanhatalo", "title": "Automatic Posture and Movement Tracking of Infants with Wearable\n  Movement Sensors", "comments": "17 pages, 8 figures, preprint of manuscript accepted for publication\n  for publication in Nature Scientific Reports", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Infants' spontaneous and voluntary movements mirror developmental integrity\nof brain networks since they require coordinated activation of multiple sites\nin the central nervous system. Accordingly, early detection of infants with\natypical motor development holds promise for recognizing those infants who are\nat risk for a wide range of neurodevelopmental disorders (e.g., cerebral palsy,\nautism spectrum disorders). Previously, novel wearable technology has shown\npromise for offering efficient, scalable and automated methods for movement\nassessment in adults. Here, we describe the development of an infant wearable,\na multi-sensor smart jumpsuit that allows mobile accelerometer and gyroscope\ndata collection during movements. Using this suit, we first recorded play\nsessions of 22 typically developing infants of approximately 7 months of age.\nThese data were manually annotated for infant posture and movement based on\nvideo recordings of the sessions, and using a novel annotation scheme\nspecifically designed to assess the overall movement pattern of infants in the\ngiven age group. A machine learning algorithm, based on deep convolutional\nneural networks (CNNs) was then trained for automatic detection of posture and\nmovement classes using the data and annotations. Our experiments show that the\nsetup can be used for quantitative tracking of infant movement activities with\na human equivalent accuracy, i.e., it meets the human inter-rater agreement\nlevels in infant posture and movement classification. We also quantify the\nambiguity of human observers in analyzing infant movements, and propose a\nmethod for utilizing this uncertainty for performance improvements in training\nof the automated classifier. Comparison of different sensor configurations also\nshows that four-limb recording leads to the best performance in posture and\nmovement classification.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 13:37:28 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 13:42:39 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Airaksinen", "Manu", ""], ["R\u00e4s\u00e4nen", "Okko", ""], ["Il\u00e9n", "Elina", ""], ["H\u00e4yrinen", "Taru", ""], ["Kivi", "Anna", ""], ["Marchi", "Viviana", ""], ["Gallen", "Anastasia", ""], ["Blom", "Sonja", ""], ["Varhe", "Anni", ""], ["Kaartinen", "Nico", ""], ["Haataja", "Leena", ""], ["Vanhatalo", "Sampsa", ""]]}, {"id": "1909.09837", "submitter": "Jiechao Ma", "authors": "Xiang Li, Jiechao Ma, Hongwei Li", "title": "Invasiveness Prediction of Pulmonary Adenocarcinomas Using Deep Feature\n  Fusion Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early diagnosis of pathological invasiveness of pulmonary adenocarcinomas\nusing computed tomography (CT) imaging would alter the course of treatment of\nadenocarcinomas and subsequently improve the prognosis. Most of the existing\nsystems use either conventional radiomics features or deep-learning features\nalone to predict the invasiveness. In this study, we explore the fusion of the\ntwo kinds of features and claim that radiomics features can be complementary to\ndeep-learning features. An effective deep feature fusion network is proposed to\nexploit the complementarity between the two kinds of features, which improves\nthe invasiveness prediction results. We collected a private dataset that\ncontains lung CT scans of 676 patients categorized into four invasiveness types\nfrom a collaborating hospital. Evaluations on this dataset demonstrate the\neffectiveness of our proposal.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 14:54:10 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Li", "Xiang", ""], ["Ma", "Jiechao", ""], ["Li", "Hongwei", ""]]}, {"id": "1909.09839", "submitter": "Kaixu Huang", "authors": "Kaixu Huang, Fanman Meng, Hongliang Li, Shuai Chen, Qingbo Wu, King\n  N.Ngan", "title": "Class Activation Map generation by Multiple Level Class Grouping and\n  Orthogonal Constraint", "comments": "International Conference on Digital Image Computing: Techniques and\n  Applications(DICTA) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class activation map (CAM) highlights regions of classes based on\nclassification network, which is widely used in weakly supervised tasks.\nHowever, it faces the problem that the class activation regions are usually\nsmall and local. Although several efforts paid to the second step (the CAM\ngeneration step) have partially enhanced the generation, we believe such\nproblem is also caused by the first step (training step), because single\nclassification model trained on the entire classes contains finite discriminate\ninformation that limits the object region extraction. To this end, this paper\nsolves CAM generation by using multiple classification models. To form multiple\nclassification networks that carry different discriminative information, we try\nto capture the semantic relationships between classes to form different\nsemantic levels of classification models. Specifically, hierarchical clustering\nbased on class relationships is used to form hierarchical clustering results,\nwhere the clustering levels are treated as semantic levels to form the\nclassification models. Moreover, a new orthogonal module and a two-branch based\nCAM generation method are proposed to generate class regions that are\northogonal and complementary. We use the PASCAL VOC 2012 dataset to verify the\nproposed method. Experimental results show that our approach improves the CAM\ngeneration.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 14:59:09 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Huang", "Kaixu", ""], ["Meng", "Fanman", ""], ["Li", "Hongliang", ""], ["Chen", "Shuai", ""], ["Wu", "Qingbo", ""], ["Ngan", "King N.", ""]]}, {"id": "1909.09853", "submitter": "Saket Chaturvedi", "authors": "Saket S. Chaturvedi, Kajol Gupta, Vaishali Ninawe, Prakash S. Prasad", "title": "Advances in Computer-Aided Diagnosis of Diabetic Retinopathy", "comments": "6 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic Retinopathy is a critical health problem influences 100 million\nindividuals worldwide, and these figures are expected to rise, particularly in\nAsia. Diabetic Retinopathy is a chronic eye disease which can lead to\nirreversible vision loss. Considering the visual complexity of retinal images,\nthe early-stage diagnosis of Diabetic Retinopathy can be challenging for human\nexperts. However, Early detection of Diabetic Retinopathy can significantly\nhelp to avoid permanent vision loss. The capability of computer-aided detection\nsystems to accurately and efficiently detect the diabetic retinopathy had\npopularized them among researchers. In this review paper, the literature search\nwas conducted on PubMed, Google Scholar, IEEE Explorer with a focus on the\ncomputer-aided detection of Diabetic Retinopathy using either of Machine\nLearning or Deep Learning algorithms. Moreover, this study also explores the\ntypical methodology utilized for the computer-aided diagnosis of Diabetic\nRetinopathy. This review paper is aimed to direct the researchers about the\nlimitations of current methods and identify the specific areas in the field to\nboost future research.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 16:21:15 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Chaturvedi", "Saket S.", ""], ["Gupta", "Kajol", ""], ["Ninawe", "Vaishali", ""], ["Prasad", "Prakash S.", ""]]}, {"id": "1909.09872", "submitter": "Kisuk Lee", "authors": "Kisuk Lee, Ran Lu, Kyle Luther, H. Sebastian Seung", "title": "Learning Dense Voxel Embeddings for 3D Neuron Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show dense voxel embeddings learned via deep metric learning can be\nemployed to produce a highly accurate segmentation of neurons from 3D electron\nmicroscopy images. A metric graph on an arbitrary set of short and long-range\nedges can be constructed from the dense embeddings generated by a convolutional\nnetwork. Partitioning the metric graph with long-range affinities as repulsive\nconstraints can produce an initial segmentation with high precision, with\nsubstantial improvements on very thin objects. The convolutional embedding net\nis reused without any modification to agglomerate the systematic splits caused\nby complex \"self-touching\"' objects. Our proposed method achieves\nstate-of-the-art accuracy on the challenging problem of 3D neuron\nreconstruction from the brain images acquired by serial section electron\nmicroscopy. Our alternative, object-centered representation could be more\ngenerally useful for other computational tasks in automated neural circuit\nreconstruction.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 19:00:27 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Lee", "Kisuk", ""], ["Lu", "Ran", ""], ["Luther", "Kyle", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1909.09891", "submitter": "Boitumelo Ruf", "authors": "Boitumelo Ruf, Thomas Pollok, Martin Weinmann", "title": "Efficient Surface-Aware Semi-Global Matching with Multi-View Plane-Sweep\n  Sampling", "comments": null, "journal-ref": "ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., IV-2/W7,\n  137-144, 2019", "doi": "10.5194/isprs-annals-IV-2-W7-137-2019", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online augmentation of an oblique aerial image sequence with structural\ninformation is an essential aspect in the process of 3D scene interpretation\nand analysis. One key aspect in this is the efficient dense image matching and\ndepth estimation. Here, the Semi-Global Matching (SGM) approach has proven to\nbe one of the most widely used algorithms for efficient depth estimation,\nproviding a good trade-off between accuracy and computational complexity.\nHowever, SGM only models a first-order smoothness assumption, thus favoring\nfronto-parallel surfaces. In this work, we present a hierarchical algorithm\nthat allows for efficient depth and normal map estimation together with\nconfidence measures for each estimate. Our algorithm relies on a plane-sweep\nmulti-image matching followed by an extended SGM optimization that allows to\nincorporate local surface orientations, thus achieving more consistent and\naccurate estimates in areasmade up of slanted surfaces, inherent to oblique\naerial imagery. We evaluate numerous configurations of our algorithm on two\ndifferent datasets using an absolute and relative accuracy measure. In our\nevaluation, we show that the results of our approach are comparable to the ones\nachieved by refined Structure-from-Motion (SfM) pipelines, such as COLMAP,\nwhich are designed for offline processing. In contrast, however, our approach\nonly considers a confined image bundle of an input sequence, thus allowing to\nperform an online and incremental computation at 1Hz-2Hz.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 20:37:55 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Ruf", "Boitumelo", ""], ["Pollok", "Thomas", ""], ["Weinmann", "Martin", ""]]}, {"id": "1909.09901", "submitter": "Joshua Engelsma", "authors": "Joshua J. Engelsma, Kai Cao, Anil K. Jain", "title": "Learning a Fixed-Length Fingerprint Representation", "comments": "to appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeepPrint, a deep network, which learns to extract fixed-length\nfingerprint representations of only 200 bytes. DeepPrint incorporates\nfingerprint domain knowledge, including alignment and minutiae detection, into\nthe deep network architecture to maximize the discriminative power of its\nrepresentation. The compact, DeepPrint representation has several advantages\nover the prevailing variable length minutiae representation which (i) requires\ncomputationally expensive graph matching techniques, (ii) is difficult to\nsecure using strong encryption schemes (e.g. homomorphic encryption), and (iii)\nhas low discriminative power in poor quality fingerprints where minutiae\nextraction is unreliable. We benchmark DeepPrint against two top performing\nCOTS SDKs (Verifinger and Innovatrics) from the NIST and FVC evaluations.\nCoupled with a re-ranking scheme, the DeepPrint rank-1 search accuracy on the\nNIST SD4 dataset against a gallery of 1.1 million fingerprints is comparable to\nthe top COTS matcher, but it is significantly faster (DeepPrint: 98.80% in 0.3\nseconds vs. COTS A: 98.85% in 27 seconds). To the best of our knowledge, the\nDeepPrint representation is the most compact and discriminative fixed-length\nfingerprint representation reported in the academic literature.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 21:28:28 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 16:15:10 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Engelsma", "Joshua J.", ""], ["Cao", "Kai", ""], ["Jain", "Anil K.", ""]]}, {"id": "1909.09931", "submitter": "Jun Liu", "authors": "Haifeng Li, Jun Liu, Li Cui, Haiyang Huang, Xue-cheng Tai", "title": "Volume Preserving Image Segmentation with Entropic Regularization\n  Optimal Transport and Its Applications in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation with a volume constraint is an important prior for many\nreal applications. In this work, we present a novel volume preserving image\nsegmentation algorithm, which is based on the framework of entropic regularized\noptimal transport theory. The classical Total Variation (TV) regularizer and\nvolume preserving are integrated into a regularized optimal transport model,\nand the volume and classification constraints can be regarded as two measures\npreserving constraints in the optimal transport problem. By studying the dual\nproblem, we develop a simple and efficient dual algorithm for our model.\nMoreover, to be different from many variational based image segmentation\nalgorithms, the proposed algorithm can be directly unrolled to a new Volume\nPreserving and TV regularized softmax (VPTV-softmax) layer for semantic\nsegmentation in the popular Deep Convolution Neural Network (DCNN). The\nexperiment results show that our proposed model is very competitive and can\nimprove the performance of many semantic segmentation nets such as the popular\nU-net.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 02:56:09 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 18:53:24 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Li", "Haifeng", ""], ["Liu", "Jun", ""], ["Cui", "Li", ""], ["Huang", "Haiyang", ""], ["Tai", "Xue-cheng", ""]]}, {"id": "1909.09932", "submitter": "Jun Liu", "authors": "Wei Wan, Jun Liu", "title": "Nonlocal Patches based Gaussian Mixture Model for Image Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the inpainting problem for noisy images. It is very challenge to\nsuppress noise when image inpainting is processed. An image patches based\nnonlocal variational method is proposed to simultaneously inpainting and\ndenoising in this paper. Our approach is developed on an assumption that the\nsmall image patches should be obeyed a distribution which can be described by a\nhigh dimension Gaussian Mixture Model. By a maximum a posteriori (MAP)\nestimation, we formulate a new regularization term according to the\nlog-likelihood function of the mixture model. To optimize this regularization\nterm efficiently, we adopt the idea of the Expectation Maximum (EM) algorithm.\nIn which, the expectation step can give an adaptive weighting function which\ncan be regarded as a nonlocal connections among pixels. Using this fact, we\nbuilt a framework for non-local image inpainting under noise. Moreover, we\nmathematically prove the existence of minimizer for the proposed inpainting\nmodel. By using a spitting algorithm, the proposed model are able to realize\nimage inpainting and denoising simultaneously. Numerical results show that the\nproposed method can produce impressive reconstructed results when the\ninpainting region is rather large.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 03:28:05 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Wan", "Wei", ""], ["Liu", "Jun", ""]]}, {"id": "1909.09934", "submitter": "Chunhua Shen", "authors": "Bohan Zhuang, Chunhua Shen, Mingkui Tan, Peng Chen, Lingqiao Liu, Ian\n  Reid", "title": "Structured Binary Neural Networks for Image Recognition", "comments": "15 pages. Extended version of the conference version arXiv:1811.10413", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose methods to train convolutional neural networks (CNNs) with both\nbinarized weights and activations, leading to quantized models that are\nspecifically friendly to mobile devices with limited power capacity and\ncomputation resources. Previous works on quantizing CNNs often seek to\napproximate the floating-point information using a set of discrete values,\nwhich we call value approximation, typically assuming the same architecture as\nthe full-precision networks. Here we take a novel \"structure approximation\"\nview of quantization -- it is very likely that different architectures designed\nfor low-bit networks may be better for achieving good performance. In\nparticular, we propose a \"network decomposition\" strategy, termed Group-Net, in\nwhich we divide the network into groups. Thus, each full-precision group can be\neffectively reconstructed by aggregating a set of homogeneous binary branches.\nIn addition, we learn effective connections among groups to improve the\nrepresentation capability. Moreover, the proposed Group-Net shows strong\ngeneralization to other tasks. For instance, we extend Group-Net for accurate\nsemantic segmentation by embedding rich context into the binary structure.\nFurthermore, for the first time, we apply binary neural networks to object\ndetection. Experiments on both classification, semantic segmentation and object\ndetection tasks demonstrate the superior performance of the proposed methods\nover various quantized networks in the literature. Our methods outperform the\nprevious best binary neural networks in terms of accuracy and computation\nefficiency.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 03:45:49 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 13:40:20 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 07:52:44 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Zhuang", "Bohan", ""], ["Shen", "Chunhua", ""], ["Tan", "Mingkui", ""], ["Chen", "Peng", ""], ["Liu", "Lingqiao", ""], ["Reid", "Ian", ""]]}, {"id": "1909.09944", "submitter": "Tanzila Rahman", "authors": "Tanzila Rahman, Bicheng Xu, Leonid Sigal", "title": "Watch, Listen and Tell: Multi-modal Weakly Supervised Dense Event\n  Captioning", "comments": null, "journal-ref": "ICCV2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal learning, particularly among imaging and linguistic modalities,\nhas made amazing strides in many high-level fundamental visual understanding\nproblems, ranging from language grounding to dense event captioning. However,\nmuch of the research has been limited to approaches that either do not take\naudio corresponding to video into account at all, or those that model the\naudio-visual correlations in service of sound or sound source localization. In\nthis paper, we present the evidence, that audio signals can carry surprising\namount of information when it comes to high-level visual-lingual tasks.\nSpecifically, we focus on the problem of weakly-supervised dense event\ncaptioning in videos and show that audio on its own can nearly rival\nperformance of a state-of-the-art visual model and, combined with video, can\nimprove on the state-of-the-art performance. Extensive experiments on the\nActivityNet Captions dataset show that our proposed multi-modal approach\noutperforms state-of-the-art unimodal methods, as well as validate specific\nfeature representation and architecture design choices.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 06:12:25 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 08:42:41 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Rahman", "Tanzila", ""], ["Xu", "Bicheng", ""], ["Sigal", "Leonid", ""]]}, {"id": "1909.09945", "submitter": "Kun Zhao", "authors": "Can Peng, Kun Zhao, Arnold Wiliem, Teng Zhang, Peter Hobson, Anthony\n  Jennings, Brian C. Lovell", "title": "To What Extent Does Downsampling, Compression, and Data Scarcity Impact\n  Renal Image Analysis?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The condition of the Glomeruli, or filter sacks, in renal Direct\nImmunofluorescence (DIF) specimens is a critical indicator for diagnosing\nkidney diseases. A digital pathology system which digitizes a glass histology\nslide into a Whole Slide Image (WSI) and then automatically detects and zooms\nin on the glomeruli with a higher magnification objective will be extremely\nhelpful for pathologists. In this paper, using glomerulus detection as the\nstudy case, we provide analysis and observations on several important issues to\nhelp with the development of Computer Aided Diagnostic (CAD) systems to process\nWSIs. Large image resolution, large file size, and data scarcity are always\nchallenging to deal with. To this end, we first examine image downsampling\nrates in terms of their effect on detection accuracy. Second, we examine the\nimpact of image compression. Third, we examine the relationship between the\nsize of the training set and detection accuracy. To understand the above\nissues, experiments are performed on the state-of-the-art detectors: Faster\nR-CNN, R-FCN, Mask R-CNN and SSD. Critical findings are observed: (1) The best\nbalance between detection accuracy, detection speed and file size is achieved\nat 8 times downsampling captured with a $40\\times$ objective; (2) compression\nwhich reduces the file size dramatically, does not necessarily have an adverse\neffect on overall accuracy; (3) reducing the amount of training data to some\nextents causes a drop in precision but has a negligible impact on the recall;\n(4) in most cases, Faster R-CNN achieves the best accuracy in the glomerulus\ndetection task. We show that the image file size of $40\\times$ WSI images can\nbe reduced by a factor of over 6000 with negligible loss of glomerulus\ndetection accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 06:16:27 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Peng", "Can", ""], ["Zhao", "Kun", ""], ["Wiliem", "Arnold", ""], ["Zhang", "Teng", ""], ["Hobson", "Peter", ""], ["Jennings", "Anthony", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1909.09946", "submitter": "Ha Tran Hong Phan", "authors": "Ha Tran Hong Phan, Ashnil Kumar, David Feng, Michael Fulham, Jinman\n  Kim", "title": "Semi-supervised estimation of event temporal length for cell event\n  detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell event detection in cell videos is essential for monitoring of cellular\nbehavior over extended time periods. Deep learning methods have shown great\nsuccess in the detection of cell events for their ability to capture more\ndiscriminative features of cellular processes compared to traditional methods.\nIn particular, convolutional long short-term memory (LSTM) models, which\nexploits the changes in cell events observable in video sequences, is the\nstate-of-the-art for mitosis detection in cell videos. However, their\nlimitations are the determination of the input sequence length, which is often\nperformed empirically, and the need for a large annotated training dataset\nwhich is expensive to prepare. We propose a novel semi-supervised method of\noptimal length detection for mitosis detection with two key contributions: (i)\nan unsupervised step for learning the spatial and temporal locations of cells\nin their normal stage and approximating the distribution of temporal lengths of\ncell events and, (ii) a step of inferring, from that distribution, an optimal\ninput sequence length and a minimal number of annotated frames for training a\nLSTM model for each particular video. We evaluated our method in detecting\nmitosis in densely packed stem cells in a phase-contrast microscopy videos. Our\nexperimental data prove that increasing the input sequence length of LSTM can\nlead to a decrease in performance. Our results also show that by approximating\nthe optimal input sequence length of the tested video, a model trained with\nonly 18 annotated frames achieved F1-scores of 0.880-0.907, which are 10%\nhigher than those of other published methods with a full set of 110 training\nannotated frames.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 06:22:36 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Phan", "Ha Tran Hong", ""], ["Kumar", "Ashnil", ""], ["Feng", "David", ""], ["Fulham", "Michael", ""], ["Kim", "Jinman", ""]]}, {"id": "1909.09953", "submitter": "Kuang-Huei Lee", "authors": "Kuang-Huei Lee, Hamid Palangi, Xi Chen, Houdong Hu, Jianfeng Gao", "title": "Learning Visual Relation Priors for Image-Text Matching and Image\n  Captioning with Neural Scene Graph Generators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grounding language to visual relations is critical to various\nlanguage-and-vision applications. In this work, we tackle two fundamental\nlanguage-and-vision tasks: image-text matching and image captioning, and\ndemonstrate that neural scene graph generators can learn effective visual\nrelation features to facilitate grounding language to visual relations and\nsubsequently improve the two end applications. By combining relation features\nwith the state-of-the-art models, our experiments show significant improvement\non the standard Flickr30K and MSCOCO benchmarks. Our experimental results and\nanalysis show that relation features improve downstream models' capability of\ncapturing visual relations in end vision-and-language applications. We also\ndemonstrate the importance of learning scene graph generators with visually\nrelevant relations to the effectiveness of relation features.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 07:30:29 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Lee", "Kuang-Huei", ""], ["Palangi", "Hamid", ""], ["Chen", "Xi", ""], ["Hu", "Houdong", ""], ["Gao", "Jianfeng", ""]]}, {"id": "1909.09961", "submitter": "Xin Cai", "authors": "Xin Cai and Yi-Fei Pu", "title": "FlatteNet: A Simple Versatile Framework for Dense Pixelwise Prediction", "comments": null, "journal-ref": "IEEE Access, vol. 7, pp. 179985-179996, 2019", "doi": "10.1109/ACCESS.2019.2959640", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on devising a versatile framework for dense pixelwise\nprediction whose goal is to assign a discrete or continuous label to each pixel\nfor an image. It is well-known that the reduced feature resolution due to\nrepeated subsampling operations poses a serious challenge to Fully\nConvolutional Network (FCN) based models. In contrast to the commonly-used\nstrategies, such as dilated convolution and encoder-decoder structure, we\nintroduce the Flattening Module to produce high-resolution predictions without\neither removing any subsampling operations or building a complicated decoder\nmodule. In addition, the Flattening Module is lightweight and can be easily\ncombined with any existing FCNs, allowing the model builder to trade off among\nmodel size, computational cost and accuracy by simply choosing different\nbackbone networks. We empirically demonstrate the effectiveness of the proposed\nFlattening Module through competitive results in human pose estimation on MPII,\nsemantic segmentation on PASCAL-Context and object detection on PASCAL VOC. We\nhope that the proposed approach can serve as a simple and strong alternative of\ncurrent dominant dense pixelwise prediction frameworks.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 08:05:04 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 14:26:43 GMT"}, {"version": "v3", "created": "Fri, 8 Nov 2019 02:47:21 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Cai", "Xin", ""], ["Pu", "Yi-Fei", ""]]}, {"id": "1909.09974", "submitter": "Gerasimos Spanakis", "authors": "Cedric Oeldorf and Gerasimos Spanakis", "title": "LoGANv2: Conditional Style-Based Logo Generation with Generative\n  Adversarial Networks", "comments": "accepted for poster presentation at ICMLA 2019, data+code available:\n  https://github.com/cedricoeldorf/ConditionalStyleGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domains such as logo synthesis, in which the data has a high degree of\nmulti-modality, still pose a challenge for generative adversarial networks\n(GANs). Recent research shows that progressive training (ProGAN) and mapping\nnetwork extensions (StyleGAN) enable both increased training stability for\nhigher dimensional problems and better feature separation within the embedded\nlatent space. However, these architectures leave limited control over shaping\nthe output of the network, which is an undesirable trait in the case of logo\nsynthesis. This paper explores a conditional extension to the StyleGAN\narchitecture with the aim of firstly, improving on the low resolution results\nof previous research and, secondly, increasing the controllability of the\noutput through the use of synthetic class-conditions. Furthermore, methods of\nextracting such class conditions are explored with a focus on the human\ninterpretability, where the challenge lies in the fact that, by nature, visual\nlogo characteristics are hard to define. The introduced conditional style-based\ngenerator architecture is trained on the extracted class-conditions in two\nexperiments and studied relative to the performance of an unconditional model.\nResults show that, whilst the unconditional model more closely matches the\ntraining distribution, high quality conditions enabled the embedding of finer\ndetails onto the latent space, leading to more diverse output.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 10:29:19 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Oeldorf", "Cedric", ""], ["Spanakis", "Gerasimos", ""]]}, {"id": "1909.09979", "submitter": "Mingqi Hu", "authors": "Mingqi Hu, Deyu Zhou, Yulan He", "title": "Variational Conditional GAN for Fine-grained Controllable Image\n  Generation", "comments": "Accepted at the 11th Asian Conference on Machine Learning (ACML 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel variational generator framework for\nconditional GANs to catch semantic details for improving the generation quality\nand diversity. Traditional generators in conditional GANs simply concatenate\nthe conditional vector with the noise as the input representation, which is\ndirectly employed for upsampling operations. However, the hidden condition\ninformation is not fully exploited, especially when the input is a class label.\nTherefore, we introduce a variational inference into the generator to infer the\nposterior of latent variable only from the conditional input, which helps\nachieve a variable augmented representation for image generation. Qualitative\nand quantitative experimental results show that the proposed method outperforms\nthe state-of-the-art approaches and achieves the realistic controllable images.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 10:52:33 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Hu", "Mingqi", ""], ["Zhou", "Deyu", ""], ["He", "Yulan", ""]]}, {"id": "1909.09998", "submitter": "Feng Xiong", "authors": "Kevin Zhang, Feng Xiong, Peize Sun, Li Hu, Boxun Li, Gang Yu", "title": "Double Anchor R-CNN for Human Detection in a Crowd", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting human in a crowd is a challenging problem due to the uncertainties\nof occlusion patterns. In this paper, we propose to handle the crowd occlusion\nproblem in human detection by leveraging the head part. Double Anchor RPN is\ndeveloped to capture body and head parts in pairs. A proposal crossover\nstrategy is introduced to generate high-quality proposals for both parts as a\ntraining augmentation. Features of coupled proposals are then aggregated\nefficiently to exploit the inherent relationship. Finally, a Joint NMS module\nis developed for robust post-processing. The proposed framework, called Double\nAnchor R-CNN, is able to detect the body and head for each person\nsimultaneously in crowded scenarios. State-of-the-art results are reported on\nchallenging human detection datasets. Our model yields log-average miss rates\n(MR) of 51.79pp on CrowdHuman, 55.01pp on COCOPersons~(crowded sub-dataset) and\n40.02pp on CrowdPose~(crowded sub-dataset), which outperforms previous baseline\ndetectors by 3.57pp, 3.82pp, and 4.24pp, respectively. We hope our simple and\neffective approach will serve as a solid baseline and help ease future research\nin crowded human detection.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 13:01:08 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Zhang", "Kevin", ""], ["Xiong", "Feng", ""], ["Sun", "Peize", ""], ["Hu", "Li", ""], ["Li", "Boxun", ""], ["Yu", "Gang", ""]]}, {"id": "1909.09999", "submitter": "Chiranjibi Sitaula", "authors": "Chiranjibi Sitaula, Yong Xiang, Anish Basnet, Sunil Aryal, Xuequan Lu", "title": "Tag-based Semantic Features for Scene Image Classification", "comments": "Accepted by ICONIP2019 conference", "journal-ref": "In: Gedeon T., Wong K., Lee M. (eds) Neural Information\n  Processing. ICONIP 2019., vol 11955 (2019)", "doi": "10.1007/978-3-030-36718-3_8", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing image feature extraction methods are primarily based on the\ncontent and structure information of images, and rarely consider the contextual\nsemantic information. Regarding some types of images such as scenes and\nobjects, the annotations and descriptions of them available on the web may\nprovide reliable contextual semantic information for feature extraction. In\nthis paper, we introduce novel semantic features of an image based on the\nannotations and descriptions of its similar images available on the web.\nSpecifically, we propose a new method which consists of two consecutive steps\nto extract our semantic features. For each image in the training set, we\ninitially search the top $k$ most similar images from the internet and extract\ntheir annotations/descriptions (e.g., tags or keywords). The annotation\ninformation is employed to design a filter bank for each image category and\ngenerate filter words (codebook). Finally, each image is represented by the\nhistogram of the occurrences of filter words in all categories. We evaluate the\nperformance of the proposed features in scene image classification on three\ncommonly-used scene image datasets (i.e., MIT-67, Scene15 and Event8). Our\nmethod typically produces a lower feature dimension than existing feature\nextraction methods. Experimental results show that the proposed features\ngenerate better classification accuracies than vision based and tag based\nfeatures, and comparable results to deep learning based features.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 13:17:39 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Sitaula", "Chiranjibi", ""], ["Xiang", "Yong", ""], ["Basnet", "Anish", ""], ["Aryal", "Sunil", ""], ["Lu", "Xuequan", ""]]}, {"id": "1909.10084", "submitter": "Alessa Hering", "authors": "Alessa Hering and Bram van Ginneken and Stefan Heldmann", "title": "mlVIRNET: Multilevel Variational Image Registration Network", "comments": "accepted for publication at MICCAI 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32226-7_29", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel multilevel approach for deep learning based image\nregistration. Recently published deep learning based registration methods have\nshown promising results for a wide range of tasks. However, these algorithms\nare still limited to relatively small deformations. Our method addresses this\nshortcoming by introducing a multilevel framework, which computes deformation\nfields on different scales, similar to conventional methods. Thereby, a\ncoarse-level alignment is obtained first, which is subsequently improved on\nfiner levels. We demonstrate our method on the complex task of inhale-to-exhale\nlung registration. We show that the use of a deep learning multilevel approach\nleads to significantly better registration results.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 20:13:48 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Hering", "Alessa", ""], ["van Ginneken", "Bram", ""], ["Heldmann", "Stefan", ""]]}, {"id": "1909.10120", "submitter": "Ciprian Tomoiag\\u{a}", "authors": "Ciprian Tomoiaga (1), Paul Feng (1), Mathieu Salzmann (2), Patrick\n  Jayet (1) ((1) AXA REV Lausanne, (2) CVLab EPFL Switzerland)", "title": "Field typing for improved recognition on heterogeneous handwritten forms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline handwriting recognition has undergone continuous progress over the\npast decades. However, existing methods are typically benchmarked on free-form\ntext datasets that are biased towards good-quality images and handwriting\nstyles, and homogeneous content. In this paper, we show that state-of-the-art\nalgorithms, employing long short-term memory (LSTM) layers, do not readily\ngeneralize to real-world structured documents, such as forms, due to their\nhighly heterogeneous and out-of-vocabulary content, and to the inherent\nambiguities of this content. To address this, we propose to leverage the\ncontent type within an LSTM-based architecture. Furthermore, we introduce a\nprocedure to generate synthetic data to train this architecture without\nrequiring expensive manual annotations. We demonstrate the effectiveness of our\napproach at transcribing text on a challenging, real-world dataset of European\nAccident Statements.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 01:29:58 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Tomoiaga", "Ciprian", "", "AXA REV Lausanne"], ["Feng", "Paul", "", "AXA REV Lausanne"], ["Salzmann", "Mathieu", "", "CVLab EPFL Switzerland"], ["Jayet", "Patrick", "", "AXA REV Lausanne"]]}, {"id": "1909.10128", "submitter": "Qingxing Cao", "authors": "Qingxing Cao, Bailin Li, Xiaodan Liang, Liang Lin", "title": "Explainable High-order Visual Question Reasoning: A New Benchmark and\n  Knowledge-routed Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explanation and high-order reasoning capabilities are crucial for real-world\nvisual question answering with diverse levels of inference complexity (e.g.,\nwhat is the dog that is near the girl playing with?) and important for users to\nunderstand and diagnose the trustworthiness of the system. Current VQA\nbenchmarks on natural images with only an accuracy metric end up pushing the\nmodels to exploit the dataset biases and cannot provide any interpretable\njustification, which severally hinders advances in high-level question\nanswering. In this work, we propose a new HVQR benchmark for evaluating\nexplainable and high-order visual question reasoning ability with three\ndistinguishable merits: 1) the questions often contain one or two relationship\ntriplets, which requires the model to have the ability of multistep reasoning\nto predict plausible answers; 2) we provide an explicit evaluation on a\nmultistep reasoning process that is constructed with image scene graphs and\ncommonsense knowledge bases; and 3) each relationship triplet in a large-scale\nknowledge base only appears once among all questions, which poses challenges\nfor existing networks that often attempt to overfit the knowledge base that\nalready appears in the training set and enforces the models to handle unseen\nquestions and knowledge fact usage. We also propose a new knowledge-routed\nmodular network (KM-net) that incorporates the multistep reasoning process over\na large knowledge base into visual question reasoning. An extensive dataset\nanalysis and comparisons with existing models on the HVQR benchmark show that\nour benchmark provides explainable evaluations, comprehensive reasoning\nrequirements and realistic challenges of VQA systems, as well as our KM-net's\nsuperiority in terms of accuracy and explanation ability.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 02:38:56 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Cao", "Qingxing", ""], ["Li", "Bailin", ""], ["Liang", "Xiaodan", ""], ["Lin", "Liang", ""]]}, {"id": "1909.10137", "submitter": "Yiyuan Zhao", "authors": "Yiyuan Zhao, Jianing Wang, Rui Li, Robert F. Labadie, Benoit M.\n  Dawant, Jack H. Noble", "title": "Validation of image-guided cochlear implant programming techniques", "comments": "37 pages, 12 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GL eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cochlear implants (CIs) are a standard treatment for patients who experience\nsevere to profound hearing loss. Recent studies have shown that hearing outcome\nis correlated with intra-cochlear anatomy and electrode placement. Our group\nhas developed image-guided CI programming (IGCIP) techniques that use image\nanalysis methods to both segment the inner ear structures in pre- or\npost-implantation CT images and localize the CI electrodes in post-implantation\nCT images. This permits to assist audiologists with CI programming by\nsuggesting which among the contacts should be deactivated to reduce electrode\ninteraction that is known to affect outcomes. Clinical studies have shown that\nIGCIP can improve hearing outcomes for CI recipients. However, the sensitivity\nof IGCIP with respect to the accuracy of the two major steps: electrode\nlocalization and intra-cochlear anatomy segmentation, is unknown. In this\narticle, we create a ground truth dataset with conventional CT and micro-CT\nimages of 35 temporal bone specimens to both rigorously characterize the\naccuracy of these two steps and assess how inaccuracies in these steps affect\nthe overall results. Our study results show that when clinical pre- and\npost-implantation CTs are available, IGCIP produces results that are comparable\nto those obtained with the corresponding ground truth in 86.7% of the subjects\ntested. When only post-implantation CTs are available, this number is 83.3%.\nThese results suggest that our current method is robust to errors in\nsegmentation and localization but also that it can be improved upon.\n  Keywords: cochlear implant, ground truth, segmentation, validation\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 03:16:27 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 14:13:50 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Zhao", "Yiyuan", ""], ["Wang", "Jianing", ""], ["Li", "Rui", ""], ["Labadie", "Robert F.", ""], ["Dawant", "Benoit M.", ""], ["Noble", "Jack H.", ""]]}, {"id": "1909.10147", "submitter": "Chuanbiao Song", "authors": "Chuanbiao Song, Kun He, Jiadong Lin, Liwei Wang, John E. Hopcroft", "title": "Robust Local Features for Improving the Generalization of Adversarial\n  Training", "comments": "accepted by ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training has been demonstrated as one of the most effective\nmethods for training robust models to defend against adversarial examples.\nHowever, adversarially trained models often lack adversarially robust\ngeneralization on unseen testing data. Recent works show that adversarially\ntrained models are more biased towards global structure features. Instead, in\nthis work, we would like to investigate the relationship between the\ngeneralization of adversarial training and the robust local features, as the\nrobust local features generalize well for unseen shape variation. To learn the\nrobust local features, we develop a Random Block Shuffle (RBS) transformation\nto break up the global structure features on normal adversarial examples. We\ncontinue to propose a new approach called Robust Local Features for Adversarial\nTraining (RLFAT), which first learns the robust local features by adversarial\ntraining on the RBS-transformed adversarial examples, and then transfers the\nrobust local features into the training of normal adversarial examples. To\ndemonstrate the generality of our argument, we implement RLFAT in currently\nstate-of-the-art adversarial training frameworks. Extensive experiments on\nSTL-10, CIFAR-10 and CIFAR-100 show that RLFAT significantly improves both the\nadversarially robust generalization and the standard generalization of\nadversarial training. Additionally, we demonstrate that our models capture more\nlocal features of the object on the images, aligning better with human\nperception.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 04:19:34 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 07:45:21 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 07:56:04 GMT"}, {"version": "v4", "created": "Wed, 22 Jan 2020 04:31:16 GMT"}, {"version": "v5", "created": "Sun, 2 Feb 2020 13:54:45 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Song", "Chuanbiao", ""], ["He", "Kun", ""], ["Lin", "Jiadong", ""], ["Wang", "Liwei", ""], ["Hopcroft", "John E.", ""]]}, {"id": "1909.10153", "submitter": "Robert Grupp", "authors": "Robert Grupp, Hsin-Hong Chiang, Yoshito Otake, Ryan Murphy, Chad\n  Gordon, Mehran Armand, Russell Taylor", "title": "Smooth Extrapolation of Unknown Anatomy via Statistical Shape Models", "comments": "SPIE Medical Imaging Conference 2015 Paper", "journal-ref": "In Medical Imaging 2015: Image-Guided Procedures, Robotic\n  Interventions, and Modeling 2015 Mar 18 (Vol. 9415, p. 941524). International\n  Society for Optics and Photonics", "doi": "10.1117/12.2081310", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods to perform extrapolation of unknown anatomy were evaluated.\nThe primary application is to enhance surgical procedures that may use partial\nmedical images or medical images of incomplete anatomy. Le Fort-based,\nface-jaw-teeth transplant is one such procedure. From CT data of 36 skulls and\n21 mandibles separate Statistical Shape Models of the anatomical surfaces were\ncreated. Using the Statistical Shape Models, incomplete surfaces were projected\nto obtain complete surface estimates. The surface estimates exhibit non-zero\nerror in regions where the true surface is known; it is desirable to keep the\ntrue surface and seamlessly merge the estimated unknown surface. Existing\nextrapolation techniques produce non-smooth transitions from the true surface\nto the estimated surface, resulting in additional error and a less\naesthetically pleasing result. The three extrapolation techniques evaluated\nwere: copying and pasting of the surface estimate (non-smooth baseline), a\nfeathering between the patient surface and surface estimate, and an estimate\ngenerated via a Thin Plate Spline trained from displacements between the\nsurface estimate and corresponding vertices of the known patient surface.\nFeathering and Thin Plate Spline approaches both yielded smooth transitions.\nHowever, feathering corrupted known vertex values. Leave-one-out analyses were\nconducted, with 5% to 50% of known anatomy removed from the left-out patient\nand estimated via the proposed approaches. The Thin Plate Spline approach\nyielded smaller errors than the other two approaches, with an average vertex\nerror improvement of 1.46 mm and 1.38 mm for the skull and mandible\nrespectively, over the baseline approach.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 04:40:10 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Grupp", "Robert", ""], ["Chiang", "Hsin-Hong", ""], ["Otake", "Yoshito", ""], ["Murphy", "Ryan", ""], ["Gordon", "Chad", ""], ["Armand", "Mehran", ""], ["Taylor", "Russell", ""]]}, {"id": "1909.10169", "submitter": "Yuyu Guo", "authors": "Yuyu Guo", "title": "Deep Local Global Refinement Network for Stent Analysis in IVOCT Images", "comments": "8 pages,5 figures, MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implantation of stents into coronary arteries is a common treatment option\nfor patients with cardiovascular disease. Assessment of safety and efficacy of\nthe stent implantation occurs via manual visual inspection of the neointimal\ncoverage from intravascular optical coherence tomography (IVOCT) images.\nHowever, such manual assessment requires the detection of thousands of strut\npoints within the stent. This is a challenging, tedious, and time-consuming\ntask because the strut points usually appear as small, irregular shaped objects\nwith inhomogeneous textures, and are often occluded by shadows, artifacts, and\nvessel walls. Conventional methods based on textures, edge detection, or simple\nclassifiers for automated detection of strut points in IVOCT images have low\nrecall and precision as they are, unable to adequately represent the visual\nfeatures of the strut point for detection. In this study, we propose a\nlocal-global refinement network to integrate local-patch content with global\ncontent for strut points detection from IVOCT images. Our method densely\ndetects the potential strut points in local image patches and then refines them\naccording to global appearance constraints to reduce false positives. Our\nexperimental results on a clinical dataset of 7,000 IVOCT images demonstrated\nthat our method outperformed the state-of-the-art methods with a recall of 0.92\nand precision of 0.91 for strut points detection.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 05:50:38 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Guo", "Yuyu", ""]]}, {"id": "1909.10184", "submitter": "Hanjiang Hu", "authors": "Hanjiang Hu, Hesheng Wang, Zhe Liu, Chenguang Yang, Weidong Chen and\n  Le Xie", "title": "Retrieval-based Localization Based on Domain-invariant Feature Learning\n  under Changing Environments", "comments": "Accepted by 2019 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2019)", "journal-ref": null, "doi": "10.1109/IROS40897.2019.8968047", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual localization is a crucial problem in mobile robotics and autonomous\ndriving. One solution is to retrieve images with known pose from a database for\nthe localization of query images. However, in environments with drastically\nvarying conditions (e.g. illumination changes, seasons, occlusion, dynamic\nobjects), retrieval-based localization is severely hampered and becomes a\nchallenging problem. In this paper, a novel domain-invariant feature learning\nmethod (DIFL) is proposed based on ComboGAN, a multi-domain image translation\nnetwork architecture. By introducing a feature consistency loss (FCL) between\nthe encoded features of the original image and translated image in another\ndomain, we are able to train the encoders to generate domain-invariant features\nin a self-supervised manner. To retrieve a target image from the database, the\nquery image is first encoded using the encoder belonging to the query domain to\nobtain a domain-invariant feature vector. We then preform retrieval by\nselecting the database image with the most similar domain-invariant feature\nvector. We validate the proposed approach on the CMU-Seasons dataset, where we\noutperform state-of-the-art learning-based descriptors in retrieval-based\nlocalization for high and medium precision scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 06:47:37 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Hu", "Hanjiang", ""], ["Wang", "Hesheng", ""], ["Liu", "Zhe", ""], ["Yang", "Chenguang", ""], ["Chen", "Weidong", ""], ["Xie", "Le", ""]]}, {"id": "1909.10214", "submitter": "Jiayun Wang", "authors": "Jiayun Wang", "title": "Learning Coupled Spatial-temporal Attention for Skeleton-based Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a coupled spatial-temporal attention (CSTA) model\nfor skeleton-based action recognition, which aims to figure out the most\ndiscriminative joints and frames in spatial and temporal domains\nsimultaneously. Conventional approaches usually consider all the joints or\nframes in a skeletal sequence equally important, which are unrobust to\nambiguous and redundant information. To address this, we first learn two sets\nof weights for different joints and frames through two subnetworks\nrespectively, which enable the model to have the ability of \"paying attention\nto\" the relatively informative section. Then, we calculate the cross product\nbased on the weights of joints and frames for the coupled spatial-temporal\nattention. Moreover, our CSTA mechanisms can be easily plugged into existing\nhierarchical CNN models (CSTA-CNN) to realize their function. Extensive\nexperimental results on the recently collected UESTC dataset and the currently\nlargest NTU dataset have shown the effectiveness of our proposed method for\nskeleton-based action recognition.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 08:30:11 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Wang", "Jiayun", ""]]}, {"id": "1909.10225", "submitter": "Irene Amerini", "authors": "Irene Amerini, Elena Balashova, Sayna Ebrahimi, Kathryn Leonard, Arsha\n  Nagrani, Amaia Salvador", "title": "WiCV 2019: The Sixth Women In Computer Vision Workshop", "comments": "Report of the Sixth Women In Computer Vision Workshop", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR) Workshops, 2019, pp. 0-0", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the Women in Computer Vision Workshop - WiCV 2019,\norganized in conjunction with CVPR 2019. This event is meant for increasing the\nvisibility and inclusion of women researchers in the computer vision field.\nComputer vision and machine learning have made incredible progress over the\npast years, but the number of female researchers is still low both in academia\nand in industry. WiCV is organized especially for the following reason: to\nraise visibility of female researchers, to increase collaborations between\nthem, and to provide mentorship to female junior researchers in the field. In\nthis paper, we present a report of trends over the past years, along with a\nsummary of statistics regarding presenters, attendees, and sponsorship for the\ncurrent workshop.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 08:52:33 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Amerini", "Irene", ""], ["Balashova", "Elena", ""], ["Ebrahimi", "Sayna", ""], ["Leonard", "Kathryn", ""], ["Nagrani", "Arsha", ""], ["Salvador", "Amaia", ""]]}, {"id": "1909.10227", "submitter": "Evgeny Baraboshkin E.", "authors": "E.E. Baraboshkin, L.S. Ismailova, D.M. Orlov, E.A. Zhukovskaya, G.A.\n  Kalmykov, O.V. Khotylev, E.Yu. Baraboshkin, D.A. Koroteev", "title": "Deep Convolutions for In-Depth Automated Rock Typing", "comments": "25 pages, 9 figures, 3 tables, submitted to Computers and Geosciences\n  Journal. Keywords: Core Image; Description; Convolutional Neural Networks;\n  Representation; Geology; Lithotypes", "journal-ref": null, "doi": "10.1016/j.cageo.2019.104330", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The description of rocks is one of the most time-consuming tasks in the\neveryday work of a geologist, especially when very accurate description is\nrequired. We here present a method that reduces the time needed for accurate\ndescription of rocks, enabling the geologist to work more efficiently. We\ndescribe the application of methods based on color distribution analysis and\nfeature extraction. Then we focus on a new approach, used by us, which is based\non convolutional neural networks. We used several well-known neural network\narchitectures (AlexNet, VGG, GoogLeNet, ResNet) and made a comparison of their\nperformance. The precision of the algorithms is up to 95% on the validation set\nwith GoogLeNet architecture. The best of the proposed algorithms can describe\n50 m of full-size core in one minute.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 08:55:36 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 10:25:01 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 08:13:09 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Baraboshkin", "E. E.", ""], ["Ismailova", "L. S.", ""], ["Orlov", "D. M.", ""], ["Zhukovskaya", "E. A.", ""], ["Kalmykov", "G. A.", ""], ["Khotylev", "O. V.", ""], ["Baraboshkin", "E. Yu.", ""], ["Koroteev", "D. A.", ""]]}, {"id": "1909.10236", "submitter": "Ting Yao", "authors": "Zhaofan Qiu and Ting Yao and Yiheng Zhang and Yongdong Zhang and Tao\n  Mei", "title": "Scheduled Differentiable Architecture Search for Visual Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) have been regarded as a capable class of\nmodels for visual recognition problems. Nevertheless, it is not trivial to\ndevelop generic and powerful network architectures, which requires significant\nefforts of human experts. In this paper, we introduce a new idea for\nautomatically exploring architectures on a remould of Differentiable\nArchitecture Search (DAS), which possesses the efficient search via gradient\ndescent. Specifically, we present Scheduled Differentiable Architecture Search\n(SDAS) for both image and video recognition that nicely integrates the\nselection of operations during training with a schedule. Technically, an\narchitecture or a cell is represented as a directed graph. Our SDAS gradually\nfixes the operations on the edges in the graph in a progressive and scheduled\nmanner, as opposed to a one-step decision of operations for all the edges once\nthe training completes in existing DAS, which may make the architecture\nbrittle. Moreover, we enlarge the search space of SDAS particularly for video\nrecognition by devising several unique operations to encode spatio-temporal\ndynamics and demonstrate the impact in affecting the architecture search of\nSDAS. Extensive experiments of architecture learning are conducted on CIFAR10,\nKinetics10, UCF101 and HMDB51 datasets, and superior results are reported when\ncomparing to DAS method. More remarkably, the search by our SDAS is around\n2-fold faster than DAS. When transferring the learnt cells on CIFAR10 and\nKinetics10 respectively to large-scale ImageNet and Kinetics400 datasets, the\nconstructed network also outperforms several state-of-the-art hand-crafted\nstructures.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 09:19:57 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Qiu", "Zhaofan", ""], ["Yao", "Ting", ""], ["Zhang", "Yiheng", ""], ["Zhang", "Yongdong", ""], ["Mei", "Tao", ""]]}, {"id": "1909.10239", "submitter": "Ignas Budvytis", "authors": "Ignas Budvytis, Marvin Teichmann, Tomas Vojir, Roberto Cipolla", "title": "Large Scale Joint Semantic Re-Localisation and Scene Understanding via\n  Globally Unique Instance Coordinate Regression", "comments": "BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a novel approach to joint semantic localisation and\nscene understanding. Our work is motivated by the need for localisation\nalgorithms which not only predict 6-DoF camera pose but also simultaneously\nrecognise surrounding objects and estimate 3D geometry. Such capabilities are\ncrucial for computer vision guided systems which interact with the environment:\nautonomous driving, augmented reality and robotics. In particular, we propose a\ntwo step procedure. During the first step we train a convolutional neural\nnetwork to jointly predict per-pixel globally unique instance labels and\ncorresponding local coordinates for each instance of a static object (e.g. a\nbuilding). During the second step we obtain scene coordinates by combining\nobject center coordinates and local coordinates and use them to perform 6-DoF\ncamera pose estimation. We evaluate our approach on real world (CamVid-360) and\nartificial (SceneCity) autonomous driving datasets. We obtain smaller mean\ndistance and angular errors than state-of-the-art 6-DoF pose estimation\nalgorithms based on direct pose regression and pose estimation from scene\ncoordinates on all datasets. Our contributions include: (i) a novel formulation\nof scene coordinate regression as two separate tasks of object instance\nrecognition and local coordinate regression and a demonstration that our\nproposed solution allows to predict accurate 3D geometry of static objects and\nestimate 6-DoF pose of camera on (ii) maps larger by several orders of\nmagnitude than previously attempted by scene coordinate regression methods, as\nwell as on (iii) lightweight, approximate 3D maps built from 3D primitives such\nas building-aligned cuboids.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 09:26:27 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Budvytis", "Ignas", ""], ["Teichmann", "Marvin", ""], ["Vojir", "Tomas", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1909.10296", "submitter": "Christian Requena-Mesa", "authors": "Christian Requena-Mesa, Markus Reichstein, Miguel Mahecha, Basil\n  Kraft, Joachim Denzler", "title": "Predicting Landscapes from Environmental Conditions Using Generative\n  Networks", "comments": "Accepted conference paper at GCPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Landscapes are meaningful ecological units that strongly depend on the\nenvironmental conditions. Such dependencies between landscapes and the\nenvironment have been noted since the beginning of Earth sciences and cast into\nconceptual models describing the interdependencies of climate, geology,\nvegetation and geomorphology. Here, we ask whether landscapes, as seen from\nspace, can be statistically predicted from pertinent environmental conditions.\nTo this end we adapted a deep learning generative model in order to establish\nthe relationship between the environmental conditions and the view of\nlandscapes from the Sentinel-2 satellite. We trained a conditional generative\nadversarial network to generate multispectral imagery given a set of climatic,\nterrain and anthropogenic predictors. The generated imagery of the landscapes\nshare many characteristics with the real one. Results based on landscape patch\nmetrics, indicative of landscape composition and structure, show that the\nproposed generative model creates landscapes that are more similar to the\ntargets than the baseline models while overall reflectance and vegetation cover\nare predicted better. We demonstrate that for many purposes the generated\nlandscapes behave as real with immediate application for global change studies.\nWe envision the application of machine learning as a tool to forecast the\neffects of climate change on the spatial features of landscapes, while we\nassess its limitations and breaking points.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 11:24:52 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Requena-Mesa", "Christian", ""], ["Reichstein", "Markus", ""], ["Mahecha", "Miguel", ""], ["Kraft", "Basil", ""], ["Denzler", "Joachim", ""]]}, {"id": "1909.10304", "submitter": "Soroush Seifi", "authors": "Soroush Seifi, Tinne Tuytelaars", "title": "Where to Look Next: Unsupervised Active Visual Exploration on 360{\\deg}\n  Input", "comments": "Oral Presentation and best Paper Award at 360 Perception and\n  Interaction Workshop at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of active visual exploration of large 360{\\deg}\ninputs. In our setting an active agent with a limited camera bandwidth explores\nits 360{\\deg} environment by changing its viewing direction at limited discrete\ntime steps. As such, it observes the world as a sequence of narrow\nfield-of-view 'glimpses', deciding for itself where to look next. Our proposed\nmethod exceeds previous works' performance by a significant margin without the\nneed for deep reinforcement learning or training separate networks as\nsidekicks. A key component of our system are the spatial memory maps that make\nthe system aware of the glimpses' orientations (locations in the 360{\\deg}\nimage). Further, we stress the advantages of retina-like glimpses when the\nagent's sensor bandwidth and time-steps are limited. Finally, we use our\ntrained model to do classification of the whole scene using only the\ninformation observed in the glimpses.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 11:50:46 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 10:38:02 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Seifi", "Soroush", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1909.10305", "submitter": "Alice Othmani", "authors": "Amine Djerghri, Ahmed Rachid Hazourli, Alice Othmani", "title": "Deep Multi-Facial patches Aggregation Network for Expression\n  Classification from Face Images", "comments": "we have a new version of the paper arXiv:2002.09298", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Emotional Intelligence in Human-Computer Interaction has attracted increasing\nattention from researchers in multidisciplinary research fields including\npsychology, computer vision, neuroscience, artificial intelligence, and related\ndisciplines. Human prone to naturally interact with computers face-to-face.\nHuman Expressions is an important key to better link human and computers. Thus,\ndesigning interfaces able to understand human expressions and emotions can\nimprove Human-Computer Interaction (HCI) for better communication. In this\npaper, we investigate HCI via a deep multi-facial patches aggregation network\nfor Face Expression Recognition (FER). Deep features are extracted from facial\nparts and aggregated for expression classification. Several problems may affect\nthe performance of the proposed framework like the small size of FER datasets\nand the high number of parameters to learn. For That, two data augmentation\ntechniques are proposed for facial expression generation to expand the labeled\ntraining. The proposed framework is evaluated on the extended Cohn-Konade\ndataset (CK+) and promising results are achieved.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 11:52:27 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 11:52:19 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Djerghri", "Amine", ""], ["Hazourli", "Ahmed Rachid", ""], ["Othmani", "Alice", ""]]}, {"id": "1909.10307", "submitter": "Mihai Zanfir", "authors": "Mihai Zanfir, Elisabeta Oneata, Alin-Ionut Popa, Andrei Zanfir,\n  Cristian Sminchisescu", "title": "Human Synthesis and Scene Compositing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating good quality and geometrically plausible synthetic images of\nhumans with the ability to control appearance, pose and shape parameters, has\nbecome increasingly important for a variety of tasks ranging from photo\nediting, fashion virtual try-on, to special effects and image compression. In\nthis paper, we propose HUSC, a HUman Synthesis and Scene Compositing framework\nfor the realistic synthesis of humans with different appearance, in novel poses\nand scenes. Central to our formulation is 3d reasoning for both people and\nscenes, in order to produce realistic collages, by correctly modeling\nperspective effects and occlusion, by taking into account scene semantics and\nby adequately handling relative scales. Conceptually our framework consists of\nthree components: (1) a human image synthesis model with controllable pose and\nappearance, based on a parametric representation, (2) a person insertion\nprocedure that leverages the geometry and semantics of the 3d scene, and (3) an\nappearance compositing process to create a seamless blending between the colors\nof the scene and the generated human image, and avoid visual artifacts. The\nperformance of our framework is supported by both qualitative and quantitative\nresults, in particular state-of-the art synthesis scores for the DeepFashion\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 11:59:05 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 13:35:52 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Zanfir", "Mihai", ""], ["Oneata", "Elisabeta", ""], ["Popa", "Alin-Ionut", ""], ["Zanfir", "Andrei", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "1909.10312", "submitter": "Soroush Seifi", "authors": "Soroush Seifi, Tinne Tuytelaars", "title": "How to improve CNN-based 6-DoF camera pose estimation", "comments": "Accepted at Deep Learning for Visual SLAM workshop at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) and transfer learning have recently been\nused for 6 degrees of freedom (6-DoF) camera pose estimation. While they do not\nreach the same accuracy as visual SLAM-based approaches and are restricted to a\nspecific environment, they excel in robustness and can be applied even to a\nsingle image. In this paper, we study PoseNet [1] and investigate modifications\nbased on datasets' characteristics to improve the accuracy of the pose\nestimates. In particular, we emphasize the importance of field-of-view over\nimage resolution; we present a data augmentation scheme to reduce overfitting;\nwe study the effect of Long-Short-Term-Memory (LSTM) cells. Lastly, we combine\nthese modifications and improve PoseNet's performance for monocular CNN based\ncamera pose regression.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 12:12:17 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 10:38:42 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Seifi", "Soroush", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1909.10341", "submitter": "Ricard Durall Lopez", "authors": "Ricard Durall, Franz-Josef Pfreundt, Ullrich K\\\"othe and Janis Keuper", "title": "Object Segmentation using Pixel-wise Adversarial Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning based approaches have shown remarkable success on object\nsegmentation tasks. However, there is still room for further improvement.\nInspired by generative adversarial networks, we present a generic end-to-end\nadversarial approach, which can be combined with a wide range of existing\nsemantic segmentation networks to improve their segmentation performance. The\nkey element of our method is to replace the commonly used binary adversarial\nloss with a high resolution pixel-wise loss. In addition, we train our\ngenerator employing stochastic weight averaging fashion, which further enhances\nthe predicted output label maps leading to state-of-the-art results. We show,\nthat this combination of pixel-wise adversarial training and weight averaging\nleads to significant and consistent gains in segmentation performance, compared\nto the baseline models.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 12:52:54 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Durall", "Ricard", ""], ["Pfreundt", "Franz-Josef", ""], ["K\u00f6the", "Ullrich", ""], ["Keuper", "Janis", ""]]}, {"id": "1909.10360", "submitter": "ZhenLiang Ni", "authors": "Zhen-Liang Ni, Gui-Bin Bian, Xiao-Hu Zhou, Zeng-Guang Hou, Xiao-Liang\n  Xie, Chen Wang, Yan-Jie Zhou, Rui-Qi Li, and Zhen Li", "title": "RAUNet: Residual Attention U-Net for Semantic Segmentation of Cataract\n  Surgical Instruments", "comments": "Accepted by the 26th International Conference on Neural Information\n  Processing (ICONIP2019). arXiv admin note: cs.CV => eess.IV cs.CV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of surgical instruments plays a crucial role in\nrobot-assisted surgery. However, accurate segmentation of cataract surgical\ninstruments is still a challenge due to specular reflection and class imbalance\nissues. In this paper, an attention-guided network is proposed to segment the\ncataract surgical instrument. A new attention module is designed to learn\ndiscriminative features and address the specular reflection issue. It captures\nglobal context and encodes semantic dependencies to emphasize key semantic\nfeatures, boosting the feature representation. This attention module has very\nfew parameters, which helps to save memory. Thus, it can be flexibly plugged\ninto other networks. Besides, a hybrid loss is introduced to train our network\nfor addressing the class imbalance issue, which merges cross entropy and\nlogarithms of Dice loss. A new dataset named Cata7 is constructed to evaluate\nour network. To the best of our knowledge, this is the first cataract surgical\ninstrument dataset for semantic segmentation. Based on this dataset, RAUNet\nachieves state-of-the-art performance 97.71% mean Dice and 95.62% mean IOU.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 13:34:00 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 14:12:04 GMT"}, {"version": "v3", "created": "Wed, 2 Oct 2019 09:24:54 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Ni", "Zhen-Liang", ""], ["Bian", "Gui-Bin", ""], ["Zhou", "Xiao-Hu", ""], ["Hou", "Zeng-Guang", ""], ["Xie", "Xiao-Liang", ""], ["Wang", "Chen", ""], ["Zhou", "Yan-Jie", ""], ["Li", "Rui-Qi", ""], ["Li", "Zhen", ""]]}, {"id": "1909.10363", "submitter": "Alexandra Carlson", "authors": "Alexandra Carlson, Ram Vasudevan and Matthew Johnson-Roberson", "title": "Shadow Transfer: Single Image Relighting For Urban Road Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Illumination effects in images, specifically cast shadows and shading, have\nbeen shown to decrease the performance of deep neural networks on a large\nnumber of vision-based detection, recognition and segmentation tasks in urban\ndriving scenes. A key factor that contributes to this performance gap is the\nlack of `time-of-day' diversity within real, labeled datasets. There have been\nimpressive advances in the realm of image to image translation in transferring\npreviously unseen visual effects into a dataset, specifically in day to night\ntranslation. However, it is not easy to constrain what visual effects, let\nalone illumination effects, are transferred from one dataset to another during\nthe training process. To address this problem, we propose deep learning\nframework, called Shadow Transfer, that can relight complex outdoor scenes by\ntransferring realistic shadow, shading, and other lighting effects onto a\nsingle image. The novelty of the proposed framework is that it is both\nself-supervised, and is designed to operate on sensor and label information\nthat is easily available in autonomous vehicle datasets. We show the\neffectiveness of this method on both synthetic and real datasets, and we\nprovide experiments that demonstrate that the proposed method produces images\nof higher visual quality than state of the art image to image translation\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 13:47:38 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 17:08:30 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Carlson", "Alexandra", ""], ["Vasudevan", "Ram", ""], ["Johnson-Roberson", "Matthew", ""]]}, {"id": "1909.10364", "submitter": "Rahim Entezari", "authors": "Rahim Entezari, Olga Saukh", "title": "Class-dependent Compression of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's deep neural networks require substantial computation resources for\ntheir training, storage, and inference, which limits their effective use on\nresource-constrained devices. Many recent research activities explore different\noptions for compressing and optimizing deep models. On the one hand, in many\nreal-world applications, we face the data imbalance challenge, i.e. when the\nnumber of labeled instances of one class considerably outweighs the number of\nlabeled instances of the other class. On the other hand, applications may pose\na class imbalance problem, i.e. higher number of false positives produced when\ntraining a model and optimizing its performance may be tolerable, yet the\nnumber of false negatives must stay low. The problem originates from the fact\nthat some classes are more important for the application than others, e.g.\ndetection problems in medical and surveillance domains. Motivated by the\nsuccess of the lottery ticket hypothesis, in this paper we propose an iterative\ndeep model compression technique, which keeps the number of false negatives of\nthe compressed model close to the one of the original model at the price of\nincreasing the number of false positives if necessary. Our experimental\nevaluation using two benchmark data sets shows that the resulting compressed\nsub-networks 1) achieve up to 35% lower number of false negatives than the\ncompressed model without class optimization, 2) provide an overall higher\nAUC_ROC measure, and 3) use up to 99% fewer parameters compared to the original\nnetwork.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 13:47:51 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 14:24:12 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2020 15:47:42 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Entezari", "Rahim", ""], ["Saukh", "Olga", ""]]}, {"id": "1909.10391", "submitter": "Daniel Rueckert", "authors": "Daniel Rueckert and Julia A. Schnabel", "title": "Model-Based and Data-Driven Strategies in Medical Image Computing", "comments": "Accepted in IEEE Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based approaches for image reconstruction, analysis and interpretation\nhave made significant progress over the last decades. Many of these approaches\nare based on either mathematical, physical or biological models. A challenge\nfor these approaches is the modelling of the underlying processes (e.g. the\nphysics of image acquisition or the patho-physiology of a disease) with\nappropriate levels of detail and realism. With the availability of large\namounts of imaging data and machine learning (in particular deep learning)\ntechniques, data-driven approaches have become more widespread for use in\ndifferent tasks in reconstruction, analysis and interpretation. These\napproaches learn statistical models directly from labelled or unlabeled image\ndata and have been shown to be very powerful for extracting clinically useful\ninformation from medical imaging. While these data-driven approaches often\noutperform traditional model-based approaches, their clinical deployment often\nposes challenges in terms of robustness, generalization ability and\ninterpretability. In this article, we discuss what developments have motivated\nthe shift from model-based approaches towards data-driven strategies and what\npotential problems are associated with the move towards purely data-driven\napproaches, in particular deep learning. We also discuss some of the open\nchallenges for data-driven approaches, e.g. generalization to new unseen data\n(e.g. transfer learning), robustness to adversarial attacks and\ninterpretability. Finally, we conclude with a discussion on how these\napproaches may lead to the development of more closely coupled imaging\npipelines that are optimized in an end-to-end fashion.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 14:35:55 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 14:26:05 GMT"}, {"version": "v3", "created": "Mon, 30 Sep 2019 12:41:42 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Rueckert", "Daniel", ""], ["Schnabel", "Julia A.", ""]]}, {"id": "1909.10400", "submitter": "Yuying Chen", "authors": "Yuying Chen, Congcong Liu, Ming Liu, Bertram E. Shi", "title": "Robot Navigation in Crowds by Graph Convolutional Networks with\n  Attention Learned from Human Gaze", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safe and efficient crowd navigation for mobile robot is a crucial yet\nchallenging task. Previous work has shown the power of deep reinforcement\nlearning frameworks to train efficient policies. However, their performance\ndeteriorates when the crowd size grows. We suggest that this can be addressed\nby enabling the network to identify and pay attention to the humans in the\ncrowd that are most critical to navigation. We propose a novel network\nutilizing a graph representation to learn the policy. We first train a graph\nconvolutional network based on human gaze data that accurately predicts human\nattention to different agents in the crowd. Then we incorporate the learned\nattention into a graph-based reinforcement learning architecture. The proposed\nattention mechanism enables the assignment of meaningful weightings to the\nneighbors of the robot, and has the additional benefit of interpretability.\nExperiments on real-world dense pedestrian datasets with various crowd sizes\ndemonstrate that our model outperforms state-of-art methods by 18.4% in task\naccomplishment and by 16.4% in time efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 14:46:11 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Chen", "Yuying", ""], ["Liu", "Congcong", ""], ["Liu", "Ming", ""], ["Shi", "Bertram E.", ""]]}, {"id": "1909.10407", "submitter": "Mandar Gogate", "authors": "Mandar Gogate, Kia Dashtipour, Ahsan Adeel, Amir Hussain", "title": "CochleaNet: A Robust Language-independent Audio-Visual Model for Speech\n  Enhancement", "comments": "34 pages, 11 figures, Submitted to Information Fusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy situations cause huge problems for suffers of hearing loss as hearing\naids often make the signal more audible but do not always restore the\nintelligibility. In noisy settings, humans routinely exploit the audio-visual\n(AV) nature of the speech to selectively suppress the background noise and to\nfocus on the target speaker. In this paper, we present a causal, language,\nnoise and speaker independent AV deep neural network (DNN) architecture for\nspeech enhancement (SE). The model exploits the noisy acoustic cues and noise\nrobust visual cues to focus on the desired speaker and improve the speech\nintelligibility. To evaluate the proposed SE framework a first of its kind AV\nbinaural speech corpus, called ASPIRE, is recorded in real noisy environments\nincluding cafeteria and restaurant. We demonstrate superior performance of our\napproach in terms of objective measures and subjective listening tests over the\nstate-of-the-art SE approaches as well as recent DNN based SE models. In\naddition, our work challenges a popular belief that a scarcity of\nmulti-language large vocabulary AV corpus and wide variety of noises is a major\nbottleneck to build a robust language, speaker and noise independent SE\nsystems. We show that a model trained on synthetic mixture of Grid corpus (with\n33 speakers and a small English vocabulary) and ChiME 3 Noises (consisting of\nonly bus, pedestrian, cafeteria, and street noises) generalise well not only on\nlarge vocabulary corpora but also on completely unrelated languages (such as\nMandarin), wide variety of speakers and noises.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 14:59:47 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Gogate", "Mandar", ""], ["Dashtipour", "Kia", ""], ["Adeel", "Ahsan", ""], ["Hussain", "Amir", ""]]}, {"id": "1909.10411", "submitter": "Alane Suhr", "authors": "Alane Suhr, Yoav Artzi", "title": "NLVR2 Visual Bias Analysis", "comments": "Corresponding notebook available at\n  http://lil.nlp.cornell.edu/nlvr/NLVR2BiasAnalysis.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NLVR2 (Suhr et al., 2019) was designed to be robust for language bias through\na data collection process that resulted in each natural language sentence\nappearing with both true and false labels. The process did not provide a\nsimilar measure of control for visual bias. This technical report analyzes the\npotential for visual bias in NLVR2. We show that some amount of visual bias\nlikely exists. Finally, we identify a subset of the test data that allows to\ntest for model performance in a way that is robust to such potential biases. We\nshow that the performance of existing models (Li et al., 2019; Tan and Bansal\n2019) is relatively robust to this potential bias. We propose to add the\nevaluation on this subset of the data to the NLVR2 evaluation protocol, and\nupdate the official release to include it. A notebook including an\nimplementation of the code used to replicate this analysis is available at\nhttp://nlvr.ai/NLVR2BiasAnalysis.html.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 15:10:41 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Suhr", "Alane", ""], ["Artzi", "Yoav", ""]]}, {"id": "1909.10431", "submitter": "Can Chen", "authors": "Can Chen, Luca Zanotti Fragonara, Antonios Tsourdos", "title": "Go Wider: An Efficient Neural Network for Point Cloud Analysis via Group\n  Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to achieve better performance for point cloud analysis, many\nresearchers apply deeper neural networks using stacked Multi-Layer-Perceptron\n(MLP) convolutions over irregular point cloud. However, applying dense MLP\nconvolutions over large amount of points (e.g. autonomous driving application)\nleads to inefficiency in memory and computation. To achieve high performance\nbut less complexity, we propose a deep-wide neural network, called\nShufflePointNet, to exploit fine-grained local features and reduce redundancy\nin parallel using group convolution and channel shuffle operation. Unlike\nconventional operation that directly applies MLPs on high-dimensional features\nof point cloud, our model goes wider by splitting features into groups in\nadvance, and each group with certain smaller depth is only responsible for\nrespective MLP operation, which can reduce complexity and allows to encode more\nuseful information. Meanwhile, we connect communication between groups by\nshuffling groups in feature channel to capture fine-grained features. We claim\nthat, multi-branch method for wider neural networks is also beneficial to\nfeature extraction for point cloud. We present extensive experiments for shape\nclassification task on ModelNet40 dataset and semantic segmentation task on\nlarge scale datasets ShapeNet part, S3DIS and KITTI. We further perform\nablation study and compare our model to other state-of-the-art algorithms in\nterms of complexity and accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 15:39:46 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Chen", "Can", ""], ["Fragonara", "Luca Zanotti", ""], ["Tsourdos", "Antonios", ""]]}, {"id": "1909.10443", "submitter": "Robert Grupp", "authors": "Robert Grupp, Mehran Armand, Russell Taylor", "title": "Patch-Based Image Similarity for Intraoperative 2D/3D Pelvis\n  Registration During Periacetabular Osteotomy", "comments": "Presented at MICCAI CLIP Workshop 2018", "journal-ref": "In OR 2.0 Context-Aware Operating Theaters, Computer Assisted\n  Robotic Endoscopy, Clinical Image-Based Procedures, and Skin Image Analysis\n  2018 Sep 16 (pp. 153-163). Springer, Cham", "doi": "10.1007/978-3-030-01201-4_17", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Periacetabular osteotomy is a challenging surgical procedure for treating\ndevelopmental hip dysplasia, providing greater coverage of the femoral head via\nrelocation of a patient's acetabulum. Since fluoroscopic imaging is frequently\nused in the surgical workflow, computer-assisted X-Ray navigation of osteotomes\nand the relocated acetabular fragment should be feasible. We use\nintensity-based 2D/3D registration to estimate the pelvis pose with respect to\nfluoroscopic images, recover relative poses of multiple views, and triangulate\nlandmarks which may be used for navigation. Existing similarity metrics are\nunable to consistently account for the inherent mismatch between the\npreoperative intact pelvis, and the intraoperative reality of a fractured\npelvis. To mitigate the effect of this mismatch, we continuously estimate the\nrelevance of each pixel to solving the registration and use these values as\nweightings in a patch-based similarity metric. Limiting computation to randomly\nselected subsets of patches results in faster runtimes than existing\npatch-based methods. A simulation study was conducted with random fragment\nshapes, relocations, and fluoroscopic views, and the proposed method achieved a\n1.7 mm mean triangulation error over all landmarks, compared to mean errors of\n3 mm and 2.8 mm for the non-patched and image-intensity-variance-weighted patch\nsimilarity metrics, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 15:53:49 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Grupp", "Robert", ""], ["Armand", "Mehran", ""], ["Taylor", "Russell", ""]]}, {"id": "1909.10452", "submitter": "Robert Grupp", "authors": "Robert Grupp, Yoshito Otake, Ryan Murphy, Javad Parvizi, Mehran\n  Armand, Russell Taylor", "title": "Pelvis Surface Estimation From Partial CT for Computer-Aided Pelvic\n  Osteotomies", "comments": "CAOS 2015 Extended Paper", "journal-ref": "In Orthopaedic Proceedings 2016 Feb (Vol. 98, No. SUPP_5, pp.\n  55-55). The British Editorial Society of Bone & Joint Surgery", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-aided surgical systems commonly use preoperative CT scans when\nperforming pelvic osteotomies for intraoperative navigation. These systems have\nthe potential to improve the safety and accuracy of pelvic osteotomies,\nhowever, exposing the patient to radiation is a significant drawback. In order\nto reduce radiation exposure, we propose a new smooth extrapolation method\nleveraging a partial pelvis CT and a statistical shape model (SSM) of the full\npelvis in order to estimate a patient's complete pelvis. A SSM of normal,\ncomplete, female pelvis anatomy was created and evaluated from 42 subjects. A\nleave-one-out test was performed to characterise the inherent generalisation\ncapability of the SSM. An additional leave-one-out test was conducted to\nmeasure performance of the smooth extrapolation method and an existing\n\"cut-and-paste\" extrapolation method. Unknown anatomy was simulated by keeping\nthe axial slices of the patient's acetabulum intact and varying the amount of\nthe superior iliac crest retained; from 0% to 15% of the total pelvis extent.\nThe smooth technique showed an average improvement over the cut-and-paste\nmethod of 1.31 mm and 3.61 mm, in RMS and maximum surface error, respectively.\nWith 5% of the iliac crest retained, the smoothly estimated surface had an RMS\nsurface error of 2.21 mm, an improvement of 1.25 mm when retaining none of the\niliac crest. This anatomical estimation method creates the possibility of a\npatient and surgeon benefiting from the use of a CAS system and simultaneously\nreducing the patient's radiation exposure.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 16:11:11 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Grupp", "Robert", ""], ["Otake", "Yoshito", ""], ["Murphy", "Ryan", ""], ["Parvizi", "Javad", ""], ["Armand", "Mehran", ""], ["Taylor", "Russell", ""]]}, {"id": "1909.10469", "submitter": "Li Jiang", "authors": "Li Jiang, Hengshuang Zhao, Shu Liu, Xiaoyong Shen, Chi-Wing Fu, Jiaya\n  Jia", "title": "Hierarchical Point-Edge Interaction Network for Point Cloud Semantic\n  Segmentation", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We achieve 3D semantic scene labeling by exploring semantic relation between\neach point and its contextual neighbors through edges. Besides an\nencoder-decoder branch for predicting point labels, we construct an edge branch\nto hierarchically integrate point features and generate edge features. To\nincorporate point features in the edge branch, we establish a hierarchical\ngraph framework, where the graph is initialized from a coarse layer and\ngradually enriched along the point decoding process. For each edge in the final\ngraph, we predict a label to indicate the semantic consistency of the two\nconnected points to enhance point prediction. At different layers, edge\nfeatures are also fed into the corresponding point module to integrate\ncontextual information for message passing enhancement in local regions. The\ntwo branches interact with each other and cooperate in segmentation. Decent\nexperimental results on several 3D semantic labeling datasets demonstrate the\neffectiveness of our work.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 16:45:42 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Jiang", "Li", ""], ["Zhao", "Hengshuang", ""], ["Liu", "Shu", ""], ["Shen", "Xiaoyong", ""], ["Fu", "Chi-Wing", ""], ["Jia", "Jiaya", ""]]}, {"id": "1909.10470", "submitter": "Vishvak Murahari", "authors": "Vishvak Murahari, Prithvijit Chattopadhyay, Dhruv Batra, Devi Parikh,\n  Abhishek Das", "title": "Improving Generative Visual Dialog by Answering Diverse Questions", "comments": "EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work on training generative Visual Dialog models with reinforcement\nlearning(Das et al.) has explored a Qbot-Abot image-guessing game and shown\nthat this 'self-talk' approach can lead to improved performance at the\ndownstream dialog-conditioned image-guessing task. However, this improvement\nsaturates and starts degrading after a few rounds of interaction, and does not\nlead to a better Visual Dialog model. We find that this is due in part to\nrepeated interactions between Qbot and Abot during self-talk, which are not\ninformative with respect to the image. To improve this, we devise a simple\nauxiliary objective that incentivizes Qbot to ask diverse questions, thus\nreducing repetitions and in turn enabling Abot to explore a larger state space\nduring RL ie. be exposed to more visual concepts to talk about, and varied\nquestions to answer. We evaluate our approach via a host of automatic metrics\nand human studies, and demonstrate that it leads to better dialog, ie. dialog\nthat is more diverse (ie. less repetitive), consistent (ie. has fewer\nconflicting exchanges), fluent (ie. more human-like),and detailed, while still\nbeing comparably image-relevant as prior work and ablations.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 16:47:15 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 03:01:48 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Murahari", "Vishvak", ""], ["Chattopadhyay", "Prithvijit", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Das", "Abhishek", ""]]}, {"id": "1909.10473", "submitter": "Alexey Demyanchuk", "authors": "Alexey Demyanchuk, Ekaterina Pushkina, Nikolay Russkikh, Dmitry\n  Shtokalo, Sergey Mishinov", "title": "Hydrocephalus verification on brain magnetic resonance images with deep\n  convolutional neural networks and \"transfer learning\" technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hydrocephalus can be either an independent disease or a concomitant\nsymptom of a number of pathologies, therefore representing an urgent issue in\nthe present-day clinical practice. Deep Learning is an evolving technology and\nthe part of a broader field of Machine Learning. Deep learning is currently\nactively researched in the field of radiology. The aim of this study was to\nevaluate deep learning applicability to the diagnostics of hydrocephalus with\nthe use of MRI images. We retrospectively collected, annotated, and\npreprocessed the brain MRI data of 200 patients with and without radiological\nsigns of hydrocephalus. We applied a state-of-the-art deep convolutional neural\nnetwork in conjunction with transfer learning method to train a hydrocephalus\nclassifier model. Using deep convolutional neural networks, we achieved a high\nquality of machine learning model. Accuracy, sensitivity, and specificity of\nhydrocephalus signs identification was 97%, 98%, and 96% respectively. In this\nstudy, we demonstrated the capacity of deep neural networks to identify\nhydrocephalus syndrome using brain MRI images. Applying transfer learning\ntechnique, the high quality of classification was achieved although trained on\nrather limited data.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 16:54:04 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Demyanchuk", "Alexey", ""], ["Pushkina", "Ekaterina", ""], ["Russkikh", "Nikolay", ""], ["Shtokalo", "Dmitry", ""], ["Mishinov", "Sergey", ""]]}, {"id": "1909.10476", "submitter": "Binil Starly", "authors": "Binil Starly, Atin Angrish, Paul Cohen", "title": "Research Directions in Democratizing Innovation through Design\n  Automation, One-Click Manufacturing Services and Intelligent Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CV cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The digitalization of manufacturing has created opportunities for consumers\nto customize products that fit their individualized needs which in turn would\ndrive demand for manufacturing services. However, this pull-based manufacturing\nsystem production of extremely low quantity and limitless variety for products\nis expensive to implement. New emerging technology in design automation driven\nby data-driven computational design, manufacturing-as-a-service marketplaces\nand digitally enabled micro-factories holds promise towards democratization of\ninnovation. In this paper, scientific, technology and infrastructure challenges\nare identified and if solved, the impact of these emerging technologies on\nproduct innovation and future factory organization is discussed.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 16:56:08 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Starly", "Binil", ""], ["Angrish", "Atin", ""], ["Cohen", "Paul", ""]]}, {"id": "1909.10555", "submitter": "Ziming Qiu", "authors": "Ziming Qiu, Nitin Nair, Jack Langerman, Orlando Aristizabal, Jonathan\n  Mamou, Daniel H. Turnbull, Jeffrey A. Ketterling, Yao Wang", "title": "Automatic Mouse Embryo Brain Ventricle & Body Segmentation and Mutant\n  Classification From Ultrasound Data Using Deep Learning", "comments": "4 pages, 6 figures, the 2019 IEEE International Ultrasonics Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-frequency ultrasound (HFU) is well suited for imaging embryonic mice in\nvivo because it is non-invasive and real-time. Manual segmentation of the brain\nventricles (BVs) and whole body from 3D HFU images is time-consuming and\nrequires specialized training. This paper presents a deep-learning-based\nsegmentation pipeline which automates several time-consuming, repetitive tasks\ncurrently performed to study genetic mutations in developing mouse embryos.\nNamely, the pipeline accurately segments the BV and body regions in 3D HFU\nimages of mouse embryos, despite significant challenges due to position and\nshape variation of the embryos, as well as imaging artifacts. Based on the BV\nsegmentation, a 3D convolutional neural network (CNN) is further trained to\ndetect embryos with the Engrailed-1 (En1) mutation. The algorithms achieve\n0.896 and 0.925 Dice Similarity Coefficient (DSC) for BV and body segmentation,\nrespectively, and 95.8% accuracy on mutant classification. Through gradient\nbased interrogation and visualization of the trained classifier, it is\ndemonstrated that the model focuses on the morphological structures known to be\naffected by the En1 mutation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 18:24:52 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Qiu", "Ziming", ""], ["Nair", "Nitin", ""], ["Langerman", "Jack", ""], ["Aristizabal", "Orlando", ""], ["Mamou", "Jonathan", ""], ["Turnbull", "Daniel H.", ""], ["Ketterling", "Jeffrey A.", ""], ["Wang", "Yao", ""]]}, {"id": "1909.10650", "submitter": "Mohan Sridharan", "authors": "Heather Riley, Mohan Sridharan", "title": "Non-monotonic Logical Reasoning Guiding Deep Learning for Explainable\n  Visual Question Answering", "comments": "28 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art algorithms for many pattern recognition problems rely on\ndeep network models. Training these models requires a large labeled dataset and\nconsiderable computational resources. Also, it is difficult to understand the\nworking of these learned models, limiting their use in some critical\napplications. Towards addressing these limitations, our architecture draws\ninspiration from research in cognitive systems, and integrates the principles\nof commonsense logical reasoning, inductive learning, and deep learning. In the\ncontext of answering explanatory questions about scenes and the underlying\nclassification problems, the architecture uses deep networks for extracting\nfeatures from images and for generating answers to queries. Between these deep\nnetworks, it embeds components for non-monotonic logical reasoning with\nincomplete commonsense domain knowledge, and for decision tree induction. It\nalso incrementally learns and reasons with previously unknown constraints\ngoverning the domain's states. We evaluated the architecture in the context of\ndatasets of simulated and real-world images, and a simulated robot computing,\nexecuting, and providing explanatory descriptions of plans. Experimental\nresults indicate that in comparison with an ``end to end'' architecture of deep\nnetworks, our architecture provides better accuracy on classification problems\nwhen the training dataset is small, comparable accuracy with larger datasets,\nand more accurate answers to explanatory questions. Furthermore, incremental\nacquisition of previously unknown constraints improves the ability to answer\nexplanatory questions, and extending non-monotonic logical reasoning to support\nplanning and diagnostics improves the reliability and efficiency of computing\nand executing plans on a simulated robot.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 23:34:32 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Riley", "Heather", ""], ["Sridharan", "Mohan", ""]]}, {"id": "1909.10674", "submitter": "Cheng Chi", "authors": "Cheng Chi, Shifeng Zhang, Junliang Xing, Zhen Lei, Stan Z. Li, Xudong\n  Zou", "title": "Relational Learning for Joint Head and Human Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Head and human detection have been rapidly improved with the development of\ndeep convolutional neural networks. However, these two tasks are often studied\nseparately without considering their inherent correlation, leading to that 1)\nhead detection is often trapped in more false positives, and 2) the performance\nof human detector frequently drops dramatically in crowd scenes. To handle\nthese two issues, we present a novel joint head and human detection network,\nnamely JointDet, which effectively detects head and human body simultaneously.\nMoreover, we design a head-body relationship discriminating module to perform\nrelational learning between heads and human bodies, and leverage this learned\nrelationship to regain the suppressed human detections and reduce head false\npositives. To verify the effectiveness of the proposed method, we annotate head\nbounding boxes of the CityPersons and Caltech-USA datasets, and conduct\nextensive experiments on the CrowdHuman, CityPersons and Caltech-USA datasets.\nAs a consequence, the proposed JointDet detector achieves state-of-the-art\nperformance on these three benchmarks. To facilitate further studies on the\nhead and human detection problem, all new annotations, source codes and trained\nmodels will be public.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 01:40:50 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Chi", "Cheng", ""], ["Zhang", "Shifeng", ""], ["Xing", "Junliang", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""], ["Zou", "Xudong", ""]]}, {"id": "1909.10682", "submitter": "Hongxuan Ma", "authors": "Hongxuan Ma, Wei Zou, Zheng Zhu, Siyang Sun, Zhaobing Kang", "title": "The Field-of-View Constraint of Markers for Mobile Robot with Pan-Tilt\n  Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of navigation and visual servo, it is common to calculate\nrelative pose by feature points on markers, so keeping markers in camera's view\nis an important problem. In this paper, we propose a novel approach to\ncalculate field-of-view (FOV) constraint of markers for camera. Our method can\nmake the camera maintain the visibility of all feature points during the motion\nof mobile robot. According to the angular aperture of camera, the mobile robot\ncan obtain the FOV constraint region where the camera cannot keep all feature\npoints in an image. Based on the FOV constraint region, the mobile robot can be\nguided to move from the initial position to destination. Finally simulations\nand experiments are conducted based on a mobile robot equipped with a pan-tilt\ncamera, which validates the effectiveness of the method to obtain the FOV\nconstraints.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 02:13:41 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Ma", "Hongxuan", ""], ["Zou", "Wei", ""], ["Zhu", "Zheng", ""], ["Sun", "Siyang", ""], ["Kang", "Zhaobing", ""]]}, {"id": "1909.10690", "submitter": "Vinicius Vianna", "authors": "Vinicius Pavanelli Vianna, Luiz Otavio Murta Junior", "title": "Analysis of Generalized Entropies in Mutual Information Medical Image\n  Registration", "comments": "20 pages, 14 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutual information (MI) is the standard method used in image registration and\nthe most studied one but can diverge and produce wrong results when used in an\nautomated manner. In this study we compared the results of the ITK Mattes MI\nfunction, used in 3D Slicer and ITK derived software solutions, and our own\nMICUDA Shannon and Tsallis MI functions under the translation, rotation and\nscale transforms in a 3D mathematical space. This comparison allows to\nunderstand why registration fails in some circumstances and how to produce a\nmore robust automated algorithm to register medical images. Since our\nalgorithms were designed to use GPU computations we also have a huge gain in\nspeed while improving the quality of registration.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 03:11:38 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Vianna", "Vinicius Pavanelli", ""], ["Junior", "Luiz Otavio Murta", ""]]}, {"id": "1909.10692", "submitter": "Hua Wang", "authors": "Hua Wang, Dewei Su, Chuangchuang Liu, Longcun Jin, Xianfang Sun and\n  Xinyi Peng", "title": "Deformable Non-local Network for Video Super-Resolution", "comments": null, "journal-ref": "IEEE Access, vol. 7, pp. 177734-177744, 2019", "doi": "10.1109/ACCESS.2019.2958030", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The video super-resolution (VSR) task aims to restore a high-resolution (HR)\nvideo frame by using its corresponding low-resolution (LR) frame and multiple\nneighboring frames. At present, many deep learning-based VSR methods rely on\noptical flow to perform frame alignment. The final recovery results will be\ngreatly affected by the accuracy of optical flow. However, optical flow\nestimation cannot be completely accurate, and there are always some errors. In\nthis paper, we propose a novel deformable non-local network (DNLN) which is a\nnon-optical-flow-based method. Specifically, we apply the deformable\nconvolution and improve its ability of adaptive alignment at the feature level.\nFurthermore, we utilize a non-local structure to capture the global correlation\nbetween the reference frame and the aligned neighboring frames, and\nsimultaneously enhance desired fine details in the aligned frames. To\nreconstruct the final high-quality HR video frames, we use residual in residual\ndense blocks to take full advantage of the hierarchical features. Experimental\nresults on benchmark datasets demonstrate that the proposed DNLN can achieve\nstate-of-the-art performance on VSR task.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 03:20:09 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 13:40:34 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Wang", "Hua", ""], ["Su", "Dewei", ""], ["Liu", "Chuangchuang", ""], ["Jin", "Longcun", ""], ["Sun", "Xianfang", ""], ["Peng", "Xinyi", ""]]}, {"id": "1909.10695", "submitter": "Philipp V. Rouast", "authors": "Philipp V. Rouast, Marc T. P. Adam", "title": "Learning deep representations for video-based intake gesture detection", "comments": "To be published in IEEE Journal of Biomedical and Health Informatics", "journal-ref": null, "doi": "10.1109/JBHI.2019.2942845", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic detection of individual intake gestures during eating occasions has\nthe potential to improve dietary monitoring and support dietary\nrecommendations. Existing studies typically make use of on-body solutions such\nas inertial and audio sensors, while video is used as ground truth. Intake\ngesture detection directly based on video has rarely been attempted. In this\nstudy, we address this gap and show that deep learning architectures can\nsuccessfully be applied to the problem of video-based detection of intake\ngestures. For this purpose, we collect and label video data of eating occasions\nusing 360-degree video of 102 participants. Applying state-of-the-art\napproaches from video action recognition, our results show that (1) the best\nmodel achieves an $F_1$ score of 0.858, (2) appearance features contribute more\nthan motion features, and (3) temporal context in form of multiple video frames\nis essential for top model performance.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 03:29:53 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Rouast", "Philipp V.", ""], ["Adam", "Marc T. P.", ""]]}, {"id": "1909.10698", "submitter": "Pei Lv", "authors": "Pei Lv, Haiyu Yu, Junxiao Xue, Junjin Cheng, Lisha Cui, Bing Zhou,\n  Mingliang Xu, and Yi Yang", "title": "Multi-scale discriminative Region Discovery for Weakly-Supervised Object\n  Localization", "comments": "12 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localizing objects with weak supervision in an image is a key problem of the\nresearch in computer vision community. Many existing Weakly-Supervised Object\nLocalization (WSOL) approaches tackle this problem by estimating the most\ndiscriminative regions with feature maps (activation maps) obtained by Deep\nConvolutional Neural Network, that is, only the objects or parts of them with\nthe most discriminative response will be located. However, the activation maps\noften display different local maximum responses or relatively weak response\nwhen one image contains multiple objects with the same type or small objects.\nIn this paper, we propose a simple yet effective multi-scale discriminative\nregion discovery method to localize not only more integral objects but also as\nmany as possible with only image-level class labels. The gradient weights\nflowing into different convolutional layers of CNN are taken as the input of\nour method, which is different from previous methods only considering that of\nthe final convolutional layer. To mine more discriminative regions for the task\nof object localization, the multiple local maximum from the gradient weight\nmaps are leveraged to generate the localization map with a parallel sliding\nwindow. Furthermore, multi-scale localization maps from different convolutional\nlayers are fused to produce the final result. We evaluate the proposed method\nwith the foundation of VGGnet on the ILSVRC 2016, CUB-200-2011 and PASCAL VOC\n2012 datasets. On ILSVRC 2016, the proposed method yields the Top-1\nlocalization error of 48.65\\%, which outperforms previous results by 2.75\\%. On\nPASCAL VOC 2012, our approach achieve the highest localization accuracy of\n0.43. Even for CUB-200-2011 dataset, our method still achieves competitive\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 03:54:38 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Lv", "Pei", ""], ["Yu", "Haiyu", ""], ["Xue", "Junxiao", ""], ["Cheng", "Junjin", ""], ["Cui", "Lisha", ""], ["Zhou", "Bing", ""], ["Xu", "Mingliang", ""], ["Yang", "Yi", ""]]}, {"id": "1909.10702", "submitter": "Nitish Bahadur", "authors": "Nitish Bahadur and Randy Paffenroth", "title": "Dimension Estimation Using Autoencoders", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dimension Estimation (DE) and Dimension Reduction (DR) are two closely\nrelated topics, but with quite different goals. In DE, one attempts to estimate\nthe intrinsic dimensionality or number of latent variables in a set of\nmeasurements of a random vector. However, in DR, one attempts to project a\nrandom vector, either linearly or non-linearly, to a lower dimensional space\nthat preserves the information contained in the original higher dimensional\nspace. Of course, these two ideas are quite closely linked since, for example,\ndoing DR to a dimension smaller than suggested by DE will likely lead to\ninformation loss. Accordingly, in this paper we will focus on a particular\nclass of deep neural networks called autoencoders which are used extensively\nfor DR but are less well studied for DE. We show that several important\nquestions arise when using autoencoders for DE, above and beyond those that\narise for more classic DR/DE techniques such as Principal Component Analysis.\nWe address autoencoder architectural choices and regularization techniques that\nallow one to transform autoencoder latent layer representations into estimates\nof intrinsic dimension.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 04:09:48 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Bahadur", "Nitish", ""], ["Paffenroth", "Randy", ""]]}, {"id": "1909.10708", "submitter": "Chiranjibi Sitaula", "authors": "Chiranjibi Sitaula, Yong Xiang, Sunil Aryal, Xuequan Lu", "title": "Unsupervised Deep Features for Privacy Image Classification", "comments": "Accepted in PSIVT2019 Conference", "journal-ref": "PSIVT 2019. Lecture Notes in Computer Science, vol 11854", "doi": "10.1007/978-3-030-34879-3_31", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sharing images online poses security threats to a wide range of users due to\nthe unawareness of privacy information. Deep features have been demonstrated to\nbe a powerful representation for images. However, deep features usually suffer\nfrom the issues of a large size and requiring a huge amount of data for\nfine-tuning. In contrast to normal images (e.g., scene images), privacy images\nare often limited because of sensitive information. In this paper, we propose a\nnovel approach that can work on limited data and generate deep features of\nsmaller size. For training images, we first extract the initial deep features\nfrom the pre-trained model and then employ the K-means clustering algorithm to\nlearn the centroids of these initial deep features. We use the learned\ncentroids from training features to extract the final features for each testing\nimage and encode our final features with the triangle encoding. To improve the\ndiscriminability of the features, we further perform the fusion of two proposed\nunsupervised deep features obtained from different layers. Experimental results\nshow that the proposed features outperform state-of-the-art deep features, in\nterms of both classification accuracy and testing time.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 04:38:15 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Sitaula", "Chiranjibi", ""], ["Xiang", "Yong", ""], ["Aryal", "Sunil", ""], ["Lu", "Xuequan", ""]]}, {"id": "1909.10726", "submitter": "R\\\"udiger Schmitz", "authors": "R\\\"udiger Schmitz, Frederic Madesta, Maximilian Nielsen, Jenny Krause,\n  Ren\\'e Werner, and Thomas R\\\"osch", "title": "Multi-scale fully convolutional neural networks for histopathology image\n  segmentation: from nuclear aberrations to the global tissue architecture", "comments": "Accepted for Medical Image Analysis", "journal-ref": null, "doi": "10.1016/j.media.2021.101996", "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.TO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Histopathologic diagnosis relies on simultaneous integration of information\nfrom a broad range of scales, ranging from nuclear aberrations ($\\approx\n\\mathcal{O}(0.1{\\mu m})$) through cellular structures ($\\approx\n\\mathcal{O}(10{\\mu m})$) to the global tissue architecture ($\\gtrapprox\n\\mathcal{O}(1{mm})$). To explicitly mimic how human pathologists combine\nmulti-scale information, we introduce a family of multi-encoder FCNs with deep\nfusion. We present a simple block for merging model paths with differing\nspatial scales in a spatial relationship-preserving fashion, which can readily\nbe included in standard encoder-decoder networks. Additionally, a context\nclassification gate block is proposed as an alternative for the incorporation\nof global context.\n  Our experiments were performed on three publicly available whole-slide images\nof recent challenges (PAIP 2019, BACH 2020, CAMELYON 2016). The multi-scale\narchitectures consistently outperformed the baseline single-scale U-Nets by a\nlarge margin. They benefit from local as well as global context and\nparticularly a combination of both. If feature maps from different scales are\nfused, doing so in a manner preserving spatial relationships was found to be\nbeneficial. Deep guidance by a context classification loss appeared to improve\nmodel training at low computational costs. All multi-scale models had a reduced\nGPU memory footprint compared to ensembles of individual U-Nets trained on\ndifferent image scales. Additional path fusions were shown to be possible at\nlow computational cost, opening up possibilities for further, systematic and\ntask-specific architecture optimization.\n  The findings demonstrate the potential of the presented family of\nhuman-inspired, end-to-end trainable, multi-scale multi-encoder FCNs to improve\ndeep histopathologic diagnosis by extensive integration of largely different\nspatial scales.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 06:25:29 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 13:09:26 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2021 21:07:46 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Schmitz", "R\u00fcdiger", ""], ["Madesta", "Frederic", ""], ["Nielsen", "Maximilian", ""], ["Krause", "Jenny", ""], ["Werner", "Ren\u00e9", ""], ["R\u00f6sch", "Thomas", ""]]}, {"id": "1909.10754", "submitter": "Seonguk Park", "authors": "SeongUk Park, Nojun Kwak", "title": "FEED: Feature-level Ensemble for Knowledge Distillation", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Distillation (KD) aims to transfer knowledge in a teacher-student\nframework, by providing the predictions of the teacher network to the student\nnetwork in the training stage to help the student network generalize better. It\ncan use either a teacher with high capacity or {an} ensemble of multiple\nteachers. However, the latter is not convenient when one wants to use\nfeature-map-based distillation methods. For a solution, this paper proposes a\nversatile and powerful training algorithm named FEature-level Ensemble for\nknowledge Distillation (FEED), which aims to transfer the ensemble knowledge\nusing multiple teacher networks. We introduce a couple of training algorithms\nthat transfer ensemble knowledge to the student at the feature map level. Among\nthe feature-map-based distillation methods, using several non-linear\ntransformations in parallel for transferring the knowledge of the multiple\nteacher{s} helps the student find more generalized solutions. We name this\nmethod as parallel FEED, andexperimental results on CIFAR-100 and ImageNet show\nthat our method has clear performance enhancements, without introducing any\nadditional parameters or computations at test time. We also show the\nexperimental results of sequentially feeding teacher's information to the\nstudent, hence the name sequential FEED, and discuss the lessons obtained.\nAdditionally, the empirical results on measuring the reconstruction errors at\nthe feature map give hints for the enhancements.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 08:14:40 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Park", "SeongUk", ""], ["Kwak", "Nojun", ""]]}, {"id": "1909.10774", "submitter": "Biao Li", "authors": "Biao Li, Jiabin Liu, Bo Wang, Zhiquan Qi, and Yong Shi", "title": "s-LWSR: Super Lightweight Super-Resolution Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) architectures for superresolution (SR) normally contain\ntremendous parameters, which has been regarded as the crucial advantage for\nobtaining satisfying performance. However, with the widespread use of mobile\nphones for taking and retouching photos, this character greatly hampers the\ndeployment of DL-SR models on the mobile devices. To address this problem, in\nthis paper, we propose a super lightweight SR network: s-LWSR. There are mainly\nthree contributions in our work. Firstly, in order to efficiently abstract\nfeatures from the low resolution image, we build an information pool to mix\nmulti-level information from the first half part of the pipeline. Accordingly,\nthe information pool feeds the second half part with the combination of\nhierarchical features from the previous layers. Secondly, we employ a\ncompression module to further decrease the size of parameters. Intensive\nanalysis confirms its capacity of trade-off between model complexity and\naccuracy. Thirdly, by revealing the specific role of activation in deep models,\nwe remove several activation layers in our SR model to retain more information\nfor performance improvement. Extensive experiments show that our s-LWSR, with\nlimited parameters and operations, can achieve similar performance to other\ncumbersome DL-SR methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 09:34:21 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Li", "Biao", ""], ["Liu", "Jiabin", ""], ["Wang", "Bo", ""], ["Qi", "Zhiquan", ""], ["Shi", "Yong", ""]]}, {"id": "1909.10783", "submitter": "Dongling Xiao", "authors": "Dongling Xiao, Chang Liu, Qi Wang, Chao Wang, Xin Zhang", "title": "PolSAR Image Classification Based on Dilated Convolution and\n  Pixel-Refining Parallel Mapping network in the Complex Domain", "comments": "15 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and accurate polarimetric synthetic aperture radar (PolSAR) image\nclassification with a limited number of prior labels is always full of\nchallenges. For general supervised deep learning classification algorithms, the\npixel-by-pixel algorithm achieves precise yet inefficient classification with a\nsmall number of labeled pixels, whereas the pixel mapping algorithm achieves\nefficient yet edge-rough classification with more prior labels required. To\ntake efficiency, accuracy and prior labels into account, we propose a novel\npixel-refining parallel mapping network in the complex domain named CRPM-Net\nand the corresponding training algorithm for PolSAR image classification.\nCRPM-Net consists of two parallel sub-networks: a) A transfer dilated\nconvolution mapping network in the complex domain (C-Dilated CNN) activated by\na complex cross-convolution neural network (Cs-CNN), which is aiming at precise\nlocalization, high efficiency and the full use of phase information; b) A\ncomplex domain encoder-decoder network connected parallelly with C-Dilated CNN,\nwhich is to extract more contextual semantic features. Finally, we design a\ntwo-step algorithm to train the Cs-CNN and CRPM-Net with a small number of\nlabeled pixels for higher accuracy by refining misclassified labeled pixels. We\nverify the proposed method on AIRSAR and E-SAR datasets. The experimental\nresults demonstrate that CRPM-Net achieves the best classification results and\nsubstantially outperforms some latest state-of-the-art approaches in both\nefficiency and accuracy for PolSAR image classification. The source code and\ntrained models for CRPM-Net is available at:\nhttps://github.com/PROoshio/CRPM-Net.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 09:59:47 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 15:34:26 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Xiao", "Dongling", ""], ["Liu", "Chang", ""], ["Wang", "Qi", ""], ["Wang", "Chao", ""], ["Zhang", "Xin", ""]]}, {"id": "1909.10788", "submitter": "Haotong Qin", "authors": "Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei,\n  Fengwei Yu, Jingkuan Song", "title": "Forward and Backward Information Retention for Accurate Binary Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weight and activation binarization is an effective approach to deep neural\nnetwork compression and can accelerate the inference by leveraging bitwise\noperations. Although many binarization methods have improved the accuracy of\nthe model by minimizing the quantization error in forward propagation, there\nremains a noticeable performance gap between the binarized model and the\nfull-precision one. Our empirical study indicates that the quantization brings\ninformation loss in both forward and backward propagation, which is the\nbottleneck of training accurate binary neural networks. To address these\nissues, we propose an Information Retention Network (IR-Net) to retain the\ninformation that consists in the forward activations and backward gradients.\nIR-Net mainly relies on two technical contributions: (1) Libra Parameter\nBinarization (Libra-PB): simultaneously minimizing both quantization error and\ninformation loss of parameters by balanced and standardized weights in forward\npropagation; (2) Error Decay Estimator (EDE): minimizing the information loss\nof gradients by gradually approximating the sign function in backward\npropagation, jointly considering the updating ability and accurate gradients.\nWe are the first to investigate both forward and backward processes of binary\nnetworks from the unified information perspective, which provides new insight\ninto the mechanism of network binarization. Comprehensive experiments with\nvarious network structures on CIFAR-10 and ImageNet datasets manifest that the\nproposed IR-Net can consistently outperform state-of-the-art quantization\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 10:12:36 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 10:04:19 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 19:47:59 GMT"}, {"version": "v4", "created": "Mon, 9 Mar 2020 16:31:03 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Qin", "Haotong", ""], ["Gong", "Ruihao", ""], ["Liu", "Xianglong", ""], ["Shen", "Mingzhu", ""], ["Wei", "Ziran", ""], ["Yu", "Fengwei", ""], ["Song", "Jingkuan", ""]]}, {"id": "1909.10790", "submitter": "Arnaud Huaulm\\'e", "authors": "Arnaud Huaulm\\'e, Pierre Jannin, Fabian Reche, Jean-Luc Faucheron,\n  Alexandre Moreau-Gaudry, Sandrine Voros", "title": "Offline identification of surgical deviations in laparoscopic rectopexy", "comments": null, "journal-ref": null, "doi": "10.1016/j.artmed.2020.101837", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: A median of 14.4% of patient undergone at least one adverse event\nduring surgery and a third of them are preventable. The occurrence of adverse\nevents forces surgeons to implement corrective strategies and, thus, deviate\nfrom the standard surgical process. Therefore, it is clear that the automatic\nidentification of adverse events is a major challenge for patient safety. In\nthis paper, we have proposed a method enabling us to identify such deviations.\nWe have focused on identifying surgeons' deviations from standard surgical\nprocesses due to surgical events rather than anatomic specificities. This is\nparticularly challenging, given the high variability in typical surgical\nprocedure workflows. Methods: We have introduced a new approach designed to\nautomatically detect and distinguish surgical process deviations based on\nmulti-dimensional non-linear temporal scaling with a hidden semi-Markov model\nusing manual annotation of surgical processes. The approach was then evaluated\nusing cross-validation. Results: The best results have over 90% accuracy.\nRecall and precision were superior at 70%. We have provided a detailed analysis\nof the incorrectly-detected observations. Conclusion: Multi-dimensional\nnon-linear temporal scaling with a hidden semi-Markov model provides promising\nresults for detecting deviations. Our error analysis of the\nincorrectly-detected observations offers different leads in order to further\nimprove our method. Significance: Our method demonstrated the feasibility of\nautomatically detecting surgical deviations that could be implemented for both\nskill analysis and developing situation awareness-based computer-assisted\nsurgical systems.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 10:17:44 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 10:30:25 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Huaulm\u00e9", "Arnaud", ""], ["Jannin", "Pierre", ""], ["Reche", "Fabian", ""], ["Faucheron", "Jean-Luc", ""], ["Moreau-Gaudry", "Alexandre", ""], ["Voros", "Sandrine", ""]]}, {"id": "1909.10798", "submitter": "Min-Kook Choi", "authors": "Min-Kook Choi, Heechul Jung", "title": "Development of Fast Refinement Detectors on AI Edge Platforms", "comments": "7 pages, 2 figures, IML@ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the improvements in the object detection networks, several variations of\nobject detection networks have been achieved impressive performance. However,\nthe performance evaluation of most models has focused on detection accuracy,\nand performance verification is mostly based on high-end GPU hardware. In this\npaper, we propose real-time object detectors that guarantee balanced\nperformance for real-time systems on embedded platforms. The proposed model\nutilizes the basic head structure of the RefineDet model, which is a variant of\nthe single-shot object detector (SSD). In order to ensure real-time\nperformance, CNN models with relatively shallow layers or fewer parameters have\nbeen used as the backbone structure. In addition to the basic VGGNet and ResNet\nstructures, various backbone structures such as MobileNet, Xception, ResNeXt,\nInception-SENet, and SE-ResNeXt have been used for this purpose. Successful\ntraining of object detection networks was achieved through an appropriate\ncombination of intermediate layers. The accuracy of the proposed detector was\nestimated by the evaluation of the MS-COCO 2017 object detection dataset and\nthe inference speed on the NVIDIA Drive PX2 and Jetson Xavier boards were\ntested to verify real-time performance in the embedded systems. The experiments\nshow that the proposed models ensure balanced performance in terms of accuracy\nand inference speed in the embedded system environments. In addition, unlike\nthe high-end GPUs, the use of embedded GPUs involves several additional\nconcerns for efficient inference, which have been identified in this work. The\ncodes and models are publicly available on the web (link).\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 10:29:43 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 03:46:53 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Choi", "Min-Kook", ""], ["Jung", "Heechul", ""]]}, {"id": "1909.10811", "submitter": "Kieran Greer Dr", "authors": "Kieran Greer", "title": "Image Recognition using Region Creep", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new type of image classifier that uses a shallow\narchitecture with a very quick learning phase. The image is parsed into smaller\nareas and each area is saved directly for a region, along with the related\noutput category. When a new image is presented, a direct match with each part\nis made and the best matching areas returned. These areas can overlap with each\nother and when moving from a region to its neighbours, there is likely to be\nonly small changes in the area image part. It would therefore be possible to\nguess what the best image part is for one region by cumulating the results of\nits neighbours. This is in fact an associative feature of the classifier that\ncan re-construct missing or noisy input by substituting the direct match with\nwhat the region match suggests and is being called 'Region Creep'. As each area\nstores the categories it belongs to, the image classification process sums this\nto return a preferred category for the whole image. The classifier works mostly\nat a local level and so to give it some type of global picture, rules are\nadded. These rules work at the whole image level and basically state that if\none set of pixels are present, another set should be removed or should also be\npresent. While the rules appear to be very specific, most of the construction\ncan be done automatically. Tests on a set of hand-written numbers have produced\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 10:59:44 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Greer", "Kieran", ""]]}, {"id": "1909.10819", "submitter": "Risheng Liu", "authors": "Risheng Liu, Pan Mu and Jin Zhang", "title": "On the Convergence of ADMM with Task Adaption and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with the development of learning and vision, Alternating Direction\nMethod of Multiplier (ADMM) has become a popular algorithm for separable\noptimization model with linear constraint. However, the ADMM and its numerical\nvariants (e.g., inexact, proximal or linearized) are awkward to obtain\nstate-of-the-art performance when dealing with complex learning and vision\ntasks due to their weak task-adaption ability. Recently, there has been an\nincreasing interest in incorporating task-specific computational modules (e.g.,\ndesigned filters or learned architectures) into ADMM iterations. Unfortunately,\nthese task-related modules introduce uncontrolled and unstable iterative flows,\nthey also break the structures of the original optimization model. Therefore,\nexisting theoretical investigations are invalid for these resulted\ntask-specific iterations. In this paper, we develop a simple and generic\nproximal ADMM framework to incorporate flexible task-specific module for\nlearning and vision problems. We rigorously prove the convergence both in\nobjective function values and the constraint violation and provide the\nworst-case convergence rate measured by the iteration complexity. Our\ninvestigations not only develop new perspectives for analyzing task-adaptive\nADMM but also supply meaningful guidelines on designing practical optimization\nmethods for real-world applications. Numerical experiments are conducted to\nverify the theoretical results and demonstrate the efficiency of our\nalgorithmic framework.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 11:29:13 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Liu", "Risheng", ""], ["Mu", "Pan", ""], ["Zhang", "Jin", ""]]}, {"id": "1909.10820", "submitter": "Szabolcs Pavel", "authors": "Szabolcs P\\'avel, Csan\\'ad S\\'andor, Lehel Csat\\'o", "title": "Distortion Estimation Through Explicit Modeling of the Refractive\n  Surface", "comments": "Accepted to ICANN 2019", "journal-ref": "LNCS 11729, pp. 17-28, 2019", "doi": "10.1007/978-3-030-30508-6_2", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise calibration is a must for high reliance 3D computer vision\nalgorithms. A challenging case is when the camera is behind a protective glass\nor transparent object: due to refraction, the image is heavily distorted; the\npinhole camera model alone can not be used and a distortion correction step is\nrequired. By directly modeling the geometry of the refractive media, we build\nthe image generation process by tracing individual light rays from the camera\nto a target. Comparing the generated images to their distorted - observed -\ncounterparts, we estimate the geometry parameters of the refractive surface via\nmodel inversion by employing an RBF neural network. We present an image\ncollection methodology that produces data suited for finding the distortion\nparameters and test our algorithm on synthetic and real-world data. We analyze\nthe results of the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 11:31:09 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["P\u00e1vel", "Szabolcs", ""], ["S\u00e1ndor", "Csan\u00e1d", ""], ["Csat\u00f3", "Lehel", ""]]}, {"id": "1909.10833", "submitter": "Sandra Aigner", "authors": "Peter K\\\"onig and Sandra Aigner and Marco K\\\"orner", "title": "Enhancing Traffic Scene Predictions with Generative Adversarial Networks", "comments": "Accepted for presentation at the IEEE Intelligent Transportation\n  Systems Conference -- ITSC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new two-stage pipeline for predicting frames of traffic scenes\nwhere relevant objects can still reliably be detected. Using a recent video\nprediction network, we first generate a sequence of future frames based on past\nframes. A second network then enhances these frames in order to make them\nappear more realistic. This ensures the quality of the predicted frames to be\nsufficient to enable accurate detection of objects, which is especially\nimportant for autonomously driving cars. To verify this two-stage approach, we\nconducted experiments on the Cityscapes dataset. For enhancing, we trained two\nimage-to-image translation methods based on generative adversarial networks,\none for blind motion deblurring and one for image super-resolution. All\nresulting predictions were quantitatively evaluated using both traditional\nmetrics and a state-of-the-art object detection network showing that the\nenhanced frames appear qualitatively improved. While the traditional image\ncomparison metrics, i.e., MSE, PSNR, and SSIM, failed to confirm this visual\nimpression, the object detection evaluation resembles it well. The best\nperforming prediction-enhancement pipeline is able to increase the average\nprecision values for detecting cars by about 9% for each prediction step,\ncompared to the non-enhanced predictions.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 12:14:48 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["K\u00f6nig", "Peter", ""], ["Aigner", "Sandra", ""], ["K\u00f6rner", "Marco", ""]]}, {"id": "1909.10837", "submitter": "Ying Chen", "authors": "Shibo Zhou, Xiaohua LI, Ying Chen, Sanjeev T. Chandrasekaran, Arindam\n  Sanyal", "title": "Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust\n  Performance", "comments": null, "journal-ref": "Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21),\n  2021: 35(12),11143-11151", "doi": null, "report-no": null, "categories": "cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural network (SNN) is interesting both theoretically and\npractically because of its strong bio-inspiration nature and potentially\noutstanding energy efficiency. Unfortunately, its development has fallen far\nbehind the conventional deep neural network (DNN), mainly because of difficult\ntraining and lack of widely accepted hardware experiment platforms. In this\npaper, we show that a deep temporal-coded SNN can be trained easily and\ndirectly over the benchmark datasets CIFAR10 and ImageNet, with testing\naccuracy within 1% of the DNN of equivalent size and architecture. Training\nbecomes similar to DNN thanks to the closed-form solution to the spiking\nwaveform dynamics. Considering that SNNs should be implemented in practical\nneuromorphic hardwares, we train the deep SNN with weights quantized to 8, 4, 2\nbits and with weights perturbed by random noise to demonstrate its robustness\nin practical applications. In addition, we develop a phase-domain signal\nprocessing circuit schematic to implement our spiking neuron with 90% gain of\nenergy efficiency over existing work. This paper demonstrates that the\ntemporal-coded deep SNN is feasible for applications with high performance and\nhigh energy efficient.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 12:28:11 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 06:28:18 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2020 05:31:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhou", "Shibo", ""], ["LI", "Xiaohua", ""], ["Chen", "Ying", ""], ["Chandrasekaran", "Sanjeev T.", ""], ["Sanyal", "Arindam", ""]]}, {"id": "1909.10848", "submitter": "Tianyu Zhang", "authors": "Tianyu Zhang, Lingxi Xie, Longhui Wei, Yongfei Zhang, Bo Li, Qi Tian", "title": "Single Camera Training for Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (ReID) aims at finding the same person in different\ncameras. Training such systems usually requires a large amount of cross-camera\npedestrians to be annotated from surveillance videos, which is labor-consuming\nespecially when the number of cameras is large. Differently, this paper\ninvestigates ReID in an unexplored single-camera-training (SCT) setting, where\neach person in the training set appears in only one camera. To the best of our\nknowledge, this setting was never studied before. SCT enjoys the advantage of\nlow-cost data collection and annotation, and thus eases ReID systems to be\ntrained in a brand new environment. However, it raises major challenges due to\nthe lack of cross-camera person occurrences, which conventional approaches\nheavily rely on to extract discriminative features. The key to dealing with the\nchallenges in the SCT setting lies in designing an effective mechanism to\ncomplement cross-camera annotation. We start with a regular deep network for\nfeature extraction, upon which we propose a novel loss function named\nmulti-camera negative loss (MCNL). This is a metric learning loss motivated by\nprobability, suggesting that in a multi-camera system, one image is more likely\nto be closer to the most similar negative sample in other cameras than to the\nmost similar negative sample in the same camera. In experiments, MCNL\nsignificantly boosts ReID accuracy in the SCT setting, which paves the way of\nfast deployment of ReID systems with good performance on new target scenes.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 12:50:54 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Zhang", "Tianyu", ""], ["Xie", "Lingxi", ""], ["Wei", "Longhui", ""], ["Zhang", "Yongfei", ""], ["Li", "Bo", ""], ["Tian", "Qi", ""]]}, {"id": "1909.10854", "submitter": "Rishabh Dabral", "authors": "Rishabh Dabral, Nitesh B Gundavarapu, Rahul Mitra, Abhishek Sharma,\n  Ganesh Ramakrishnan, Arjun Jain", "title": "Multi-Person 3D Human Pose Estimation from Monocular Images", "comments": "3DV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-person 3D human pose estimation from a single image is a challenging\nproblem, especially for in-the-wild settings due to the lack of 3D annotated\ndata. We propose HG-RCNN, a Mask-RCNN based network that also leverages the\nbenefits of the Hourglass architecture for multi-person 3D Human Pose\nEstimation. A two-staged approach is presented that first estimates the 2D\nkeypoints in every Region of Interest (RoI) and then lifts the estimated\nkeypoints to 3D. Finally, the estimated 3D poses are placed in\ncamera-coordinates using weak-perspective projection assumption and joint\noptimization of focal length and root translations. The result is a simple and\nmodular network for multi-person 3D human pose estimation that does not require\nany multi-person 3D pose dataset. Despite its simple formulation, HG-RCNN\nachieves the state-of-the-art results on MuPoTS-3D while also approximating the\n3D pose in the camera-coordinate system.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 12:55:56 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Dabral", "Rishabh", ""], ["Gundavarapu", "Nitesh B", ""], ["Mitra", "Rahul", ""], ["Sharma", "Abhishek", ""], ["Ramakrishnan", "Ganesh", ""], ["Jain", "Arjun", ""]]}, {"id": "1909.10900", "submitter": "Vasileios Gkitsas", "authors": "Vasileios Gkitsas, Antonis Karakottas, Nikolaos Zioulis, Dimitrios\n  Zarpalas, Petros Daras", "title": "Restyling Data: Application to Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is driven by data, yet while their availability is\nconstantly increasing, training data require laborious, time consuming and\nerror-prone labelling or ground truth acquisition, which in some cases is very\ndifficult or even impossible. Recent works have resorted to synthetic data\ngeneration, but the inferior performance of models trained on synthetic data\nwhen applied to the real world, introduced the challenge of unsupervised domain\nadaptation. In this work we investigate an unsupervised domain adaptation\ntechnique that descends from another perspective, in order to avoid the\ncomplexity of adversarial training and cycle consistencies. We exploit the\nrecent advances in photorealistic style transfer and take a fully data driven\napproach. While this concept is already implicitly formulated within the\nintricate objectives of domain adaptation GANs, we take an explicit approach\nand apply it directly as data pre-processing. The resulting technique is\nscalable, efficient and easy to implement, offers competitive performance to\nthe complex state-of-the-art alternatives and can open up new pathways for\ndomain adaptation.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 13:30:58 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Gkitsas", "Vasileios", ""], ["Karakottas", "Antonis", ""], ["Zioulis", "Nikolaos", ""], ["Zarpalas", "Dimitrios", ""], ["Daras", "Petros", ""]]}, {"id": "1909.10922", "submitter": "Yiyuan Zhao", "authors": "Yiyuan Zhao", "title": "Automatic techniques for cochlear implant CT image analysis", "comments": "This is a preprint of Yiyuan Zhao's Ph.D. dissertation from\n  Vanderbilt University, Nashville, TN, USA. Trivial formatting modifications\n  have been made in the arxiv version for readability. Vanderbilt University\n  Electronic These & Dissertation (https://etd.library.vanderbilt.edu/) has the\n  original submission on May 11 2018, and will be released on May 11 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.GL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goals of this dissertation are to fully automate the image processing\ntechniques needed in the post-operative stage of IGCIP and to perform a\nthorough analysis of (a) the robustness of the automatic image processing\ntechniques used in IGCIP and (b) assess the sensitivity of the IGCIP process as\na whole to individual components. The automatic methods that have been\ndeveloped include the automatic localization of both closely- and\ndistantly-spaced CI electrode arrays in post-implantation CTs and the automatic\nselection of electrode configurations based on the stimulation patterns.\nTogether with the existing automatic techniques developed for IGCIP, the\nproposed automatic methods enable an end-to-end IGCIP process that takes pre-\nand post-implantation CT images as input and produces a patient-customized\nelectrode configuration as output.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 16:05:26 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Zhao", "Yiyuan", ""]]}, {"id": "1909.10939", "submitter": "Frederic Borne", "authors": "Philippe Borianne (UMR AMAP), Frederic Borne (UMR AMAP), Julien\n  Sarron, Emile Faye (EGCE)", "title": "Deep Mangoes: from fruit detection to cultivar identification in colour\n  images of mango trees", "comments": null, "journal-ref": "DISP'19 International Conference on Digital Image and Signal\n  Processing, Apr 2019, Oxford, United Kingdom", "doi": null, "report-no": null, "categories": "cs.CV eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents results on the detection and identification mango fruits\nfrom colour images of trees. We evaluate the behaviour and the performances of\nthe Faster R-CNN network to determine whether it is robust enough to \"detect\nand classify\" fruits under particularly heterogeneous conditions in terms of\nplant cultivars, plantation scheme, and visual information acquisition\ncontexts. The network is trained to distinguish the 'Kent', 'Keitt', and\n\"Boucodiekhal\" mango cultivars from 3,000 representative labelled fruit\nannotations. The validation set composed of about 7,000 annotations was then\ntested with a confidence threshold of 0.7 and a Non-Maximal-Suppression\nthreshold of 0.25. With a F1-score of 0.90, the Faster R-CNN is well suitable\nto the simple fruit detection in tiles of 500x500 pixels. We then combine a\nmulti-tiling approach with a Jaccard matrix to merge the different parts of\nobjects detected several times, and thus report the detections made at the tile\nscale to the native 6,000x4,000 pixel size images. Nonetheless with a F1-score\nof 0.56, the cultivar identification Faster R-CNN network presents some\nlimitations for simultaneously detecting the mango fruits and identifying their\nrespective cultivars. Despite the proven errors in fruit detection, the\ncultivar identification rates of the detected mango fruits are in the order of\n80%. The ideal solution could combine a Mask R-CNN for the image\npre-segmentation of trees and a double-stream Faster R-CNN for detecting the\nmango fruits and identifying their respective cultivar to provide predictions\nmore relevant to users' expectations.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 14:05:54 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Borianne", "Philippe", "", "UMR AMAP"], ["Borne", "Frederic", "", "UMR AMAP"], ["Sarron", "Julien", "", "EGCE"], ["Faye", "Emile", "", "EGCE"]]}, {"id": "1909.10964", "submitter": "Fanrong Li", "authors": "Fanrong Li, Zitao Mo, Peisong Wang, Zejian Liu, Jiayun Zhang, Gang Li,\n  Qinghao Hu, Xiangyu He, Cong Leng, Yang Zhang, Jian Cheng", "title": "A System-Level Solution for Low-Power Object Detection", "comments": "Accepted by ICCV 2019 Low-Power Computer Vision Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection has made impressive progress in recent years with the help\nof deep learning. However, state-of-the-art algorithms are both computation and\nmemory intensive. Though many lightweight networks are developed for a\ntrade-off between accuracy and efficiency, it is still a challenge to make it\npractical on an embedded device. In this paper, we present a system-level\nsolution for efficient object detection on a heterogeneous embedded device. The\ndetection network is quantized to low bits and allows efficient implementation\nwith shift operators. In order to make the most of the benefits of low-bit\nquantization, we design a dedicated accelerator with programmable logic. Inside\nthe accelerator, a hybrid dataflow is exploited according to the heterogeneous\nproperty of different convolutional layers. We adopt a straightforward but\nresource-friendly column-prior tiling strategy to map the computation-intensive\nconvolutional layers to the accelerator that can support arbitrary feature\nsize. Other operations can be performed on the low-power CPU cores, and the\nentire system is executed in a pipelined manner. As a case study, we evaluate\nour object detection system on a real-world surveillance video with input size\nof 512x512, and it turns out that the system can achieve an inference speed of\n18 fps at the cost of 6.9W (with display) with an mAP of 66.4 verified on the\nPASCAL VOC 2012 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 14:45:43 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 13:57:12 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Li", "Fanrong", ""], ["Mo", "Zitao", ""], ["Wang", "Peisong", ""], ["Liu", "Zejian", ""], ["Zhang", "Jiayun", ""], ["Li", "Gang", ""], ["Hu", "Qinghao", ""], ["He", "Xiangyu", ""], ["Leng", "Cong", ""], ["Zhang", "Yang", ""], ["Cheng", "Jian", ""]]}, {"id": "1909.10970", "submitter": "Chenchen Zhao", "authors": "Chenchen Zhao, Yeqiang Qian, Ming Yang", "title": "Monocular Pedestrian Orientation Estimation Based on Deep 2D-3D\n  Feedforward", "comments": "29 pages, 12 figures", "journal-ref": null, "doi": "10.1016/j.patcog.2019.107182", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate pedestrian orientation estimation of autonomous driving helps the\nego vehicle obtain the intentions of pedestrians in the related environment,\nwhich are the base of safety measures such as collision avoidance and\nprewarning. However, because of relatively small sizes and high-level\ndeformation of pedestrians, common pedestrian orientation estimation models\nfail to extract sufficient and comprehensive information from them, thus having\ntheir performance restricted, especially monocular ones which fail to obtain\ndepth information of objects and related environment. In this paper, a novel\nmonocular pedestrian orientation estimation model, called FFNet, is proposed.\nApart from camera captures, the model adds the 2D and 3D dimensions of\npedestrians as two other inputs according to the logic relationship between\norientation and them. The 2D and 3D dimensions of pedestrians are determined\nfrom the camera captures and further utilized through two feedforward links\nconnected to the orientation estimator. The feedforward links strengthen the\nlogicality and interpretability of the network structure of the proposed model.\nExperiments show that the proposed model has at least 1.72% AOS increase than\nmost state-of-the-art models after identical training processes. The model also\nhas competitive results in orientation estimation evaluation on KITTI dataset.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 14:54:07 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 15:29:40 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhao", "Chenchen", ""], ["Qian", "Yeqiang", ""], ["Yang", "Ming", ""]]}, {"id": "1909.10976", "submitter": "Matthew Z Wong", "authors": "Matthew Z. Wong, Kiyohito Kunii, Max Baylis, Wai Hong Ong, Pavel\n  Kroupa, Swen Koller", "title": "Synthetic dataset generation for object-to-model deep learning in\n  industrial applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large image data sets has been a crucial factor in the\nsuccess of deep learning-based classification and detection methods. While data\nsets for everyday objects are widely available, data for specific industrial\nuse-cases (e.g. identifying packaged products in a warehouse) remains scarce.\nIn such cases, the data sets have to be created from scratch, placing a crucial\nbottleneck on the deployment of deep learning techniques in industrial\napplications.\n  We present work carried out in collaboration with a leading UK online\nsupermarket, with the aim of creating a computer vision system capable of\ndetecting and identifying unique supermarket products in a warehouse setting.\nTo this end, we demonstrate a framework for using synthetic data to create an\nend-to-end deep learning pipeline, beginning with real-world objects and\nculminating in a trained model.\n  Our method is based on the generation of a synthetic dataset from 3D models\nobtained by applying photogrammetry techniques to real-world objects. Using\n100k synthetic images generated from 60 real images per class, an InceptionV3\nconvolutional neural network (CNN) was trained, which achieved classification\naccuracy of 95.8% on a separately acquired test set of real supermarket product\nimages. The image generation process supports automatic pixel annotation. This\neliminates the prohibitively expensive manual annotation typically required for\ndetection tasks. Based on this readily available data, a one-stage RetinaNet\ndetector was trained on the synthetic, annotated images to produce a detector\nthat can accurately localize and classify the specimen products in real-time.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 14:58:07 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Wong", "Matthew Z.", ""], ["Kunii", "Kiyohito", ""], ["Baylis", "Max", ""], ["Ong", "Wai Hong", ""], ["Kroupa", "Pavel", ""], ["Koller", "Swen", ""]]}, {"id": "1909.10980", "submitter": "Shreyas Skandan Shivakumar", "authors": "Shreyas S. Shivakumar, Neil Rodrigues, Alex Zhou, Ian D. Miller, Vijay\n  Kumar and Camillo J. Taylor", "title": "PST900: RGB-Thermal Calibration, Dataset and Segmentation Network", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose long wave infrared (LWIR) imagery as a viable\nsupporting modality for semantic segmentation using learning-based techniques.\nWe first address the problem of RGB-thermal camera calibration by proposing a\npassive calibration target and procedure that is both portable and easy to use.\nSecond, we present PST900, a dataset of 894 synchronized and calibrated RGB and\nThermal image pairs with per pixel human annotations across four distinct\nclasses from the DARPA Subterranean Challenge. Lastly, we propose a CNN\narchitecture for fast semantic segmentation that combines both RGB and Thermal\nimagery in a way that leverages RGB imagery independently. We compare our\nmethod against the state-of-the-art and show that our method outperforms them\nin our dataset.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 15:59:20 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Shivakumar", "Shreyas S.", ""], ["Rodrigues", "Neil", ""], ["Zhou", "Alex", ""], ["Miller", "Ian D.", ""], ["Kumar", "Vijay", ""], ["Taylor", "Camillo J.", ""]]}, {"id": "1909.10989", "submitter": "Yiming Li", "authors": "Yiming Li, Changhong Fu, Fangqiang Ding, Ziyuan Huang, Jia Pan", "title": "Augmented Memory for Correlation Filters in Real-Time UAV Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outstanding computational efficiency of discriminative correlation filter\n(DCF) fades away with various complicated improvements. Previous appearances\nare also gradually forgotten due to the exponential decay of historical views\nin traditional appearance updating scheme of DCF framework, reducing the\nmodel's robustness. In this work, a novel tracker based on DCF framework is\nproposed to augment memory of previously appeared views while running at\nreal-time speed. Several historical views and the current view are\nsimultaneously introduced in training to allow the tracker to adapt to new\nappearances as well as memorize previous ones. A novel rapid compressed context\nlearning is proposed to increase the discriminative ability of the filter\nefficiently. Substantial experiments on UAVDT and UAV123 datasets have\nvalidated that the proposed tracker performs competitively against other 26 top\nDCF and deep-based trackers with over 40 FPS on CPU.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 15:12:13 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Li", "Yiming", ""], ["Fu", "Changhong", ""], ["Ding", "Fangqiang", ""], ["Huang", "Ziyuan", ""], ["Pan", "Jia", ""]]}, {"id": "1909.11004", "submitter": "Mehdi Ghayoumi", "authors": "Mehdi Ghayoumi, Maryam Pourebadi", "title": "Fuzzy Knowledge-Based Architecture for Learning and Interaction in\n  Social Robots", "comments": "7 pages, AI-HRI 2019", "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/21", "categories": "cs.RO cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an extension of our presented cognitive-based\nemotion model [27][28]and [30], where we enhance our knowledge-based emotion\nunit of the architecture by embedding a fuzzy rule-based system to it. The\nmodel utilizes the cognitive parameters dependency and their corresponding\nweights to regulate the robot's behavior and fuse their behavior data to\nachieve the final decision in their interaction with the environment. Using\nthis fuzzy system, our previous model can simulate linguistic parameters for\nbetter controlling and generating understandable and flexible behaviors in the\nrobots. We implement our model on an assistive healthcare robot, named Robot\nNurse Assistant (RNA) and test it with human subjects. Our model records all\nthe emotion states and essential information based on its predefined rules and\nlearning system. Our results show that our robot interacts with patients in a\nreasonable, faithful way in special conditions which are defined by rules. This\nwork has the potential to provide better on-demand service for clinical experts\nto monitor the patients' emotion states and help them make better decisions\naccordingly.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 07:15:37 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Ghayoumi", "Mehdi", ""], ["Pourebadi", "Maryam", ""]]}, {"id": "1909.11015", "submitter": "Shiv Ram Dubey", "authors": "Shiv Ram Dubey, Soumendu Chakraborty, Swalpa Kumar Roy, Snehasis\n  Mukherjee, Satish Kumar Singh, Bidyut Baran Chaudhuri", "title": "diffGrad: An Optimization Method for Convolutional Neural Networks", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Decent (SGD) is one of the core techniques behind the\nsuccess of deep neural networks. The gradient provides information on the\ndirection in which a function has the steepest rate of change. The main problem\nwith basic SGD is to change by equal sized steps for all parameters,\nirrespective of gradient behavior. Hence, an efficient way of deep network\noptimization is to make adaptive step sizes for each parameter. Recently,\nseveral attempts have been made to improve gradient descent methods such as\nAdaGrad, AdaDelta, RMSProp and Adam. These methods rely on the square roots of\nexponential moving averages of squared past gradients. Thus, these methods do\nnot take advantage of local change in gradients. In this paper, a novel\noptimizer is proposed based on the difference between the present and the\nimmediate past gradient (i.e., diffGrad). In the proposed diffGrad optimization\ntechnique, the step size is adjusted for each parameter in such a way that it\nshould have a larger step size for faster gradient changing parameters and a\nlower step size for lower gradient changing parameters. The convergence\nanalysis is done using the regret bound approach of online learning framework.\nRigorous analysis is made in this paper over three synthetic complex non-convex\nfunctions. The image categorization experiments are also conducted over the\nCIFAR10 and CIFAR100 datasets to observe the performance of diffGrad with\nrespect to the state-of-the-art optimizers such as SGDM, AdaGrad, AdaDelta,\nRMSProp, AMSGrad, and Adam. The residual unit (ResNet) based Convolutional\nNeural Networks (CNN) architecture is used in the experiments. The experiments\nshow that diffGrad outperforms other optimizers. Also, we show that diffGrad\nperforms uniformly well for training CNN using different activation functions.\nThe source code is made publicly available at\nhttps://github.com/shivram1987/diffGrad.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 06:20:05 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 06:11:50 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2020 06:51:39 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Dubey", "Shiv Ram", ""], ["Chakraborty", "Soumendu", ""], ["Roy", "Swalpa Kumar", ""], ["Mukherjee", "Snehasis", ""], ["Singh", "Satish Kumar", ""], ["Chaudhuri", "Bidyut Baran", ""]]}, {"id": "1909.11023", "submitter": "Tanwi Mallick", "authors": "Tanwi Mallick, Partha Pratim Das, and Arun Kumar Majumdar", "title": "Posture and sequence recognition for Bharatanatyam dance performances\n  using machine learning approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the underlying semantics of performing arts like dance is a\nchallenging task. Dance is multimedia in nature and spans over time as well as\nspace. Capturing and analyzing the multimedia content of the dance is useful\nfor the preservation of cultural heritage, to build video recommendation\nsystems, to assist learners to use tutoring systems. To develop an application\nfor dance, three aspects of dance analysis need to be addressed: 1)\nSegmentation of the dance video to find the representative action elements, 2)\nMatching or recognition of the detected action elements, and 3) Recognition of\nthe dance sequences formed by combining a number of action elements under\ncertain rules. This paper attempts to solve three fundamental problems of dance\nanalysis for understanding the underlying semantics of dance forms. Our focus\nis on an Indian Classical Dance (ICD) form known as Bharatanatyam. As dance is\ndriven by music, we use the music as well as motion information for key posture\nextraction. Next, we recognize the key postures using machine learning as well\nas deep learning techniques. Finally, the dance sequence is recognized using\nthe Hidden Markov Model (HMM). We capture the multi-modal data of Bharatanatyam\ndance using Kinect and build an annotated data set for research in ICD.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 16:18:01 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Mallick", "Tanwi", ""], ["Das", "Partha Pratim", ""], ["Majumdar", "Arun Kumar", ""]]}, {"id": "1909.11048", "submitter": "Piotr Koniusz", "authors": "Arian Prabowo, Piotr Koniusz, Wei Shao, Flora D. Salim", "title": "COLTRANE: ConvolutiOnaL TRAjectory NEtwork for Deep Map Inference", "comments": "BuildSys 2019", "journal-ref": "BuildSys 2019", "doi": "10.1145/3360322.3360853", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of automatic generation of a road map from GPS trajectories,\ncalled map inference, remains a challenging task to perform on a geospatial\ndata from a variety of domains as the majority of existing studies focus on\nroad maps in cities. Inherently, existing algorithms are not guaranteed to work\non unusual geospatial sites, such as an airport tarmac, pedestrianized paths\nand shortcuts, or animal migration routes, etc. Moreover, deep learning has not\nbeen explored well enough for such tasks. This paper introduces COLTRANE,\nConvolutiOnaL TRAjectory NEtwork, a novel deep map inference framework which\noperates on GPS trajectories collected in various environments. This framework\nincludes an Iterated Trajectory Mean Shift (ITMS) module to localize road\ncenterlines, which copes with noisy GPS data points. Convolutional Neural\nNetwork trained on our novel trajectory descriptor is then introduced into our\nframework to detect and accurately classify junctions for refinement of the\nroad maps. COLTRANE yields up to 37% improvement in F1 scores over existing\nmethods on two distinct real-world datasets: city roads and airport tarmac.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 16:59:33 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Prabowo", "Arian", ""], ["Koniusz", "Piotr", ""], ["Shao", "Wei", ""], ["Salim", "Flora D.", ""]]}, {"id": "1909.11059", "submitter": "Luowei Zhou", "authors": "Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso,\n  Jianfeng Gao", "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA", "comments": "AAAI 2020 camera-ready version. The code and the pre-trained models\n  are available at https://github.com/LuoweiZhou/VLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a unified Vision-Language Pre-training (VLP) model. The\nmodel is unified in that (1) it can be fine-tuned for either vision-language\ngeneration (e.g., image captioning) or understanding (e.g., visual question\nanswering) tasks, and (2) it uses a shared multi-layer transformer network for\nboth encoding and decoding, which differs from many existing methods where the\nencoder and decoder are implemented using separate models. The unified VLP\nmodel is pre-trained on a large amount of image-text pairs using the\nunsupervised learning objectives of two tasks: bidirectional and\nsequence-to-sequence (seq2seq) masked vision-language prediction. The two tasks\ndiffer solely in what context the prediction conditions on. This is controlled\nby utilizing specific self-attention masks for the shared transformer network.\nTo the best of our knowledge, VLP is the first reported model that achieves\nstate-of-the-art results on both vision-language generation and understanding\ntasks, as disparate as image captioning and visual question answering, across\nthree challenging benchmark datasets: COCO Captions, Flickr30k Captions, and\nVQA 2.0. The code and the pre-trained models are available at\nhttps://github.com/LuoweiZhou/VLP.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 17:17:26 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 02:16:27 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 18:48:15 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Zhou", "Luowei", ""], ["Palangi", "Hamid", ""], ["Zhang", "Lei", ""], ["Hu", "Houdong", ""], ["Corso", "Jason J.", ""], ["Gao", "Jianfeng", ""]]}, {"id": "1909.11065", "submitter": "Yuhui Yuan", "authors": "Yuhui Yuan, Xiaokang Chen, Xilin Chen, Jingdong Wang", "title": "Segmentation Transformer: Object-Contextual Representations for Semantic\n  Segmentation", "comments": "We rephrase the object-contextual representation scheme using the\n  Transformer encoder-decoder framework. ECCV 2020 Spotlight. Project Page:\n  https://github.com/openseg-group/openseg.pytorch\n  https://github.com/HRNet/HRNet-Semantic-Segmentation/tree/HRNet-OCR", "journal-ref": "ECCV 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the semantic segmentation problem with a focus on\nthe context aggregation strategy. Our motivation is that the label of a pixel\nis the category of the object that the pixel belongs to. We present a simple\nyet effective approach, object-contextual representations, characterizing a\npixel by exploiting the representation of the corresponding object class.\nFirst, we learn object regions under the supervision of ground-truth\nsegmentation. Second, we compute the object region representation by\naggregating the representations of the pixels lying in the object region. Last,\n% the representation similarity we compute the relation between each pixel and\neach object region and augment the representation of each pixel with the\nobject-contextual representation which is a weighted aggregation of all the\nobject region representations according to their relations with the pixel. We\nempirically demonstrate that the proposed approach achieves competitive\nperformance on various challenging semantic segmentation benchmarks:\nCityscapes, ADE20K, LIP, PASCAL-Context, and COCO-Stuff. Cityscapes, ADE20K,\nLIP, PASCAL-Context, and COCO-Stuff. Our submission \"HRNet + OCR + SegFix\"\nachieves 1-st place on the Cityscapes leaderboard by the time of submission.\nCode is available at: https://git.io/openseg and https://git.io/HRNet.OCR. We\nrephrase the object-contextual representation scheme using the Transformer\nencoder-decoder framework. The details are presented in~Section3.3.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 17:39:23 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 13:35:53 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 17:09:29 GMT"}, {"version": "v4", "created": "Thu, 9 Jul 2020 16:06:28 GMT"}, {"version": "v5", "created": "Sat, 25 Jul 2020 08:00:33 GMT"}, {"version": "v6", "created": "Fri, 30 Apr 2021 16:42:15 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Yuan", "Yuhui", ""], ["Chen", "Xiaokang", ""], ["Chen", "Xilin", ""], ["Wang", "Jingdong", ""]]}, {"id": "1909.11081", "submitter": "Arnab Ghosh", "authors": "Arnab Ghosh and Richard Zhang and Puneet K. Dokania and Oliver Wang\n  and Alexei A. Efros and Philip H.S. Torr and Eli Shechtman", "title": "Interactive Sketch & Fill: Multiclass Sketch-to-Image Translation", "comments": "ICCV 2019, Video Avaiable at https://youtu.be/T9xtpAMUDps", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an interactive GAN-based sketch-to-image translation method that\nhelps novice users create images of simple objects. As the user starts to draw\na sketch of a desired object type, the network interactively recommends\nplausible completions, and shows a corresponding synthesized image to the user.\nThis enables a feedback loop, where the user can edit their sketch based on the\nnetwork's recommendations, visualizing both the completed shape and final\nrendered image while they draw. In order to use a single trained model across a\nwide array of object classes, we introduce a gating-based approach for class\nconditioning, which allows us to generate distinct classes without feature\nmixing, from a single generator network. Video available at our website:\nhttps://arnabgho.github.io/iSketchNFill/.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 17:56:37 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 18:16:30 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Ghosh", "Arnab", ""], ["Zhang", "Richard", ""], ["Dokania", "Puneet K.", ""], ["Wang", "Oliver", ""], ["Efros", "Alexei A.", ""], ["Torr", "Philip H. S.", ""], ["Shechtman", "Eli", ""]]}, {"id": "1909.11128", "submitter": "Pooya Abolghasemi", "authors": "Pooya Abolghasemi, Ladislau B\\\"ol\\\"oni", "title": "Accept Synthetic Objects as Real: End-to-End Training of Attentive Deep\n  Visuomotor Policies for Manipulation in Clutter", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research demonstrated that it is feasible to end-to-end train\nmulti-task deep visuomotor policies for robotic manipulation using variations\nof learning from demonstration (LfD) and reinforcement learning (RL). In this\npaper, we extend the capabilities of end-to-end LfD architectures to object\nmanipulation in clutter. We start by introducing a data augmentation procedure\ncalled Accept Synthetic Objects as Real (ASOR). Using ASOR we develop two\nnetwork architectures: implicit attention ASOR-IA and explicit attention\nASOR-EA. Both architectures use the same training data (demonstrations in\nuncluttered environments) as previous approaches. Experimental results show\nthat ASOR-IA and ASOR-EA succeed ina significant fraction of trials in\ncluttered environments where previous approaches never succeed. In addition, we\nfind that both ASOR-IA and ASOR-EA outperform previous approaches even in\nuncluttered environments, with ASOR-EA performing better even in clutter\ncompared to the previous best baseline in an uncluttered environment.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 19:01:40 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 22:45:55 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Abolghasemi", "Pooya", ""], ["B\u00f6l\u00f6ni", "Ladislau", ""]]}, {"id": "1909.11155", "submitter": "Serim Ryou", "authors": "Serim Ryou, Seong-Gyun Jeong, Pietro Perona", "title": "Anchor Loss: Modulating Loss Scale based on Prediction Difficulty", "comments": "To appear in Proceedings of IEEE International Conference on Computer\n  Vision (ICCV), 2019. (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel loss function that dynamically rescales the cross entropy\nbased on prediction difficulty regarding a sample. Deep neural network\narchitectures in image classification tasks struggle to disambiguate visually\nsimilar objects. Likewise, in human pose estimation symmetric body parts often\nconfuse the network with assigning indiscriminative scores to them. This is due\nto the output prediction, in which only the highest confidence label is\nselected without taking into consideration a measure of uncertainty. In this\nwork, we define the prediction difficulty as a relative property coming from\nthe confidence score gap between positive and negative labels. More precisely,\nthe proposed loss function penalizes the network to avoid the score of a false\nprediction being significant. To demonstrate the efficacy of our loss function,\nwe evaluate it on two different domains: image classification and human pose\nestimation. We find improvements in both applications by achieving higher\naccuracy compared to the baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 20:01:36 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Ryou", "Serim", ""], ["Jeong", "Seong-Gyun", ""], ["Perona", "Pietro", ""]]}, {"id": "1909.11167", "submitter": "Liang Chen", "authors": "Liang Chen, Paul Bentley, Kensaku Mori, Kazunari Misawa, Michitaka\n  Fujiwara, Daniel Rueckert", "title": "Intelligent image synthesis to attack a segmentation CNN using\n  adversarial learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches based on convolutional neural networks (CNNs) have\nbeen successful in solving a number of problems in medical imaging, including\nimage segmentation. In recent years, it has been shown that CNNs are vulnerable\nto attacks in which the input image is perturbed by relatively small amounts of\nnoise so that the CNN is no longer able to perform a segmentation of the\nperturbed image with sufficient accuracy. Therefore, exploring methods on how\nto attack CNN-based models as well as how to defend models against attacks have\nbecome a popular topic as this also provides insights into the performance and\ngeneralization abilities of CNNs. However, most of the existing work assumes\nunrealistic attack models, i.e. the resulting attacks were specified in\nadvance. In this paper, we propose a novel approach for generating adversarial\nexamples to attack CNN-based segmentation models for medical images. Our\napproach has three key features: 1) The generated adversarial examples exhibit\nanatomical variations (in form of deformations) as well as appearance\nperturbations; 2) The adversarial examples attack segmentation models so that\nthe Dice scores decrease by a pre-specified amount; 3) The attack is not\nrequired to be specified beforehand. We have evaluated our approach on\nCNN-based approaches for the multi-organ segmentation problem in 2D CT images.\nWe show that the proposed approach can be used to attack different CNN-based\nsegmentation models.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 20:48:55 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Chen", "Liang", ""], ["Bentley", "Paul", ""], ["Mori", "Kensaku", ""], ["Misawa", "Kazunari", ""], ["Fujiwara", "Michitaka", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1909.11212", "submitter": "Rajath Soans", "authors": "Julianna D. Ianni, Rajath E. Soans, Sivaramakrishnan Sankarapandian,\n  Ramachandra Vikas Chamarthi, Devi Ayyagari, Thomas G. Olsen, Michael J.\n  Bonham, Coleman C. Stavish, Kiran Motaparthi, Clay J. Cockerell, Theresa A.\n  Feeser, Jason B. Lee", "title": "Augmenting the Pathology Lab: An Intelligent Whole Slide Image\n  Classification System for the Real World", "comments": "23 pages, 5 figures", "journal-ref": "Sci Rep 10, 3217 (2020)", "doi": "10.1038/s41598-020-59985-2", "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard of care diagnostic procedure for suspected skin cancer is\nmicroscopic examination of hematoxylin \\& eosin stained tissue by a\npathologist. Areas of high inter-pathologist discordance and rising biopsy\nrates necessitate higher efficiency and diagnostic reproducibility. We present\nand validate a deep learning system which classifies digitized dermatopathology\nslides into 4 categories. The system is developed using 5,070 images from a\nsingle lab, and tested on an uncurated set of 13,537 images from 3 test labs,\nusing whole slide scanners manufactured by 3 different vendors. The system's\nuse of deep-learning-based confidence scoring as a criterion to consider the\nresult as accurate yields an accuracy of up to 98\\%, and makes it adoptable in\na real-world setting. Without confidence scoring, the system achieved an\naccuracy of 78\\%. We anticipate that our deep learning system will serve as a\nfoundation enabling faster diagnosis of skin cancer, identification of cases\nfor specialist review, and targeted diagnostic classifications.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 22:26:44 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Ianni", "Julianna D.", ""], ["Soans", "Rajath E.", ""], ["Sankarapandian", "Sivaramakrishnan", ""], ["Chamarthi", "Ramachandra Vikas", ""], ["Ayyagari", "Devi", ""], ["Olsen", "Thomas G.", ""], ["Bonham", "Michael J.", ""], ["Stavish", "Coleman C.", ""], ["Motaparthi", "Kiran", ""], ["Cockerell", "Clay J.", ""], ["Feeser", "Theresa A.", ""], ["Lee", "Jason B.", ""]]}, {"id": "1909.11229", "submitter": "Alexander Mathis", "authors": "Alexander Mathis, Thomas Biasi, Steffen Schneider, Mert\n  Y\\\"uksekg\\\"on\\\"ul, Byron Rogers, Matthias Bethge, Mackenzie W. Mathis", "title": "Pretraining boosts out-of-domain robustness for pose estimation", "comments": "A.M. and T.B. co-first authors. Dataset available at http://horse10.\n  deeplabcut.org . WACV 2021 conference", "journal-ref": "https://openaccess.thecvf.com/content/WACV2021/html/Mathis_Pretraining_Boosts_Out-of-Domain_Robustness_for_Pose_Estimation_WACV_2021_paper.html", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are highly effective tools for pose estimation. However, as\nin other computer vision tasks, robustness to out-of-domain data remains a\nchallenge, especially for small training sets that are common for real-world\napplications. Here, we probe the generalization ability with three architecture\nclasses (MobileNetV2s, ResNets, and EfficientNets) for pose estimation. We\ndeveloped a dataset of 30 horses that allowed for both \"within-domain\" and\n\"out-of-domain\" (unseen horse) benchmarking - this is a crucial test for\nrobustness that current human pose estimation benchmarks do not directly\naddress. We show that better ImageNet-performing architectures perform better\non both within- and out-of-domain data if they are first pretrained on\nImageNet. We additionally show that better ImageNet models generalize better\nacross animal species. Furthermore, we introduce Horse-C, a new benchmark for\ncommon corruptions for pose estimation, and confirm that pretraining increases\nperformance in this domain shift context as well. Overall, our results\ndemonstrate that transfer learning is beneficial for out-of-domain robustness.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 23:40:39 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 18:46:51 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Mathis", "Alexander", ""], ["Biasi", "Thomas", ""], ["Schneider", "Steffen", ""], ["Y\u00fcksekg\u00f6n\u00fcl", "Mert", ""], ["Rogers", "Byron", ""], ["Bethge", "Matthias", ""], ["Mathis", "Mackenzie W.", ""]]}, {"id": "1909.11230", "submitter": "Mohammad Akhlaghi", "authors": "Mohammad Akhlaghi", "title": "Carving out the low surface brightness universe with NoiseChisel", "comments": "Invited talk at IAU Symposium 355 (The Realm of the Low Surface\n  Brightness Universe). The downloadable source (on arXiv) includes the full\n  reproduction info (scripts, config files and input data links) and can\n  reproduce the paper automatically. It is also available with its Git history\n  in https://gitlab.com/makhlaghi/iau-symposium-355 , and in Zenodo at\n  https://doi.org/10.5281/zenodo.3408481", "journal-ref": "Proceedings of the International Astronomical Union (S355), 2020", "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  NoiseChisel is a program to detect very low signal-to-noise ratio (S/N)\nfeatures with minimal assumptions on their morphology. It was introduced in\n2015 and released within a collection of data analysis programs and libraries\nknown as GNU Astronomy Utilities (Gnuastro). Over the last ten stable releases\nof Gnuastro, NoiseChisel has significantly improved: detecting even fainter\nsignal, enabling better user control over its inner workings, and many bug\nfixes. The most important change may be that NoiseChisel's segmentation\nfeatures have been moved into a new program called Segment. Another major\nchange is the final growth strategy of its true detections, for example\nNoiseChisel is able to detect the outer wings of M51 down to S/N of 0.25, or\n28.27 mag/arcsec2 on a single-exposure SDSS image (r-band). Segment is also\nable to detect the localized HII regions as \"clumps\" much more successfully.\nFinally, to orchestrate a controlled analysis, the concept of a \"reproducible\npaper\" is discussed: this paper itself is exactly reproducible (snapshot\nv4-0-g8505cfd).\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 23:42:19 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Akhlaghi", "Mohammad", ""]]}, {"id": "1909.11232", "submitter": "Al Amin Hosain", "authors": "Al Amin Hosain, Panneer Selvam Santhalingam, Parth Pathak, Jana\n  Kosecka and Huzefa Rangwala", "title": "Sign Language Recognition Analysis using Multimodal Data", "comments": "conference : IEEE DSAA, 2019, Washington DC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice-controlled personal and home assistants (such as the Amazon Echo and\nApple Siri) are becoming increasingly popular for a variety of applications.\nHowever, the benefits of these technologies are not readily accessible to Deaf\nor Hard-ofHearing (DHH) users. The objective of this study is to develop and\nevaluate a sign recognition system using multiple modalities that can be used\nby DHH signers to interact with voice-controlled devices. With the advancement\nof depth sensors, skeletal data is used for applications like video analysis\nand activity recognition. Despite having similarity with the well-studied human\nactivity recognition, the use of 3D skeleton data in sign language recognition\nis rare. This is because unlike activity recognition, sign language is mostly\ndependent on hand shape pattern. In this work, we investigate the feasibility\nof using skeletal and RGB video data for sign language recognition using a\ncombination of different deep learning architectures. We validate our results\non a large-scale American Sign Language (ASL) dataset of 12 users and 13107\nsamples across 51 signs. It is named as GMUASL51. We collected the dataset over\n6 months and it will be publicly released in the hope of spurring further\nmachine learning research towards providing improved accessibility for digital\nassistants.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 23:44:49 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Hosain", "Al Amin", ""], ["Santhalingam", "Panneer Selvam", ""], ["Pathak", "Parth", ""], ["Kosecka", "Jana", ""], ["Rangwala", "Huzefa", ""]]}, {"id": "1909.11237", "submitter": "Sifei Liu", "authors": "Sifei Liu, Xueting Li, Varun Jampani, Shalini De Mello, Jan Kautz", "title": "Learning Propagation for Arbitrarily-structured Data", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing an input signal that contains arbitrary structures, e.g.,\nsuperpixels and point clouds, remains a big challenge in computer vision.\nLinear diffusion, an effective model for image processing, has been recently\nintegrated with deep learning algorithms. In this paper, we propose to learn\npairwise relations among data points in a global fashion to improve semantic\nsegmentation with arbitrarily-structured data, through spatial generalized\npropagation networks (SGPN). The network propagates information on a group of\ngraphs, which represent the arbitrarily-structured data, through a learned,\nlinear diffusion process. The module is flexible to be embedded and jointly\ntrained with many types of networks, e.g., CNNs. We experiment with semantic\nsegmentation networks, where we use our propagation module to jointly train on\ndifferent data -- images, superpixels and point clouds. We show that SGPN\nconsistently improves the performance of both pixel and point cloud\nsegmentation, compared to networks that do not contain this module. Our method\nsuggests an effective way to model the global pairwise relations for\narbitrarily-structured data.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 00:13:10 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Liu", "Sifei", ""], ["Li", "Xueting", ""], ["Jampani", "Varun", ""], ["De Mello", "Shalini", ""], ["Kautz", "Jan", ""]]}, {"id": "1909.11268", "submitter": "Maciej Halber", "authors": "Maciej Halber and Yifei Shi and Kai Xu and Thomas Funkhouser", "title": "Rescan: Inductive Instance Segmentation for Indoor RGBD Scans", "comments": "IEEE International Conference on Computer Vision 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In depth-sensing applications ranging from home robotics to AR/VR, it will be\ncommon to acquire 3D scans of interior spaces repeatedly at sparse time\nintervals (e.g., as part of regular daily use). We propose an algorithm that\nanalyzes these \"rescans\" to infer a temporal model of a scene with semantic\ninstance information. Our algorithm operates inductively by using the temporal\nmodel resulting from past observations to infer an instance segmentation of a\nnew scan, which is then used to update the temporal model. The model contains\nobject instance associations across time and thus can be used to track\nindividual objects, even though there are only sparse observations. During\nexperiments with a new benchmark for the new task, our algorithm outperforms\nalternate approaches based on state-of-the-art networks for semantic instance\nsegmentation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 03:20:42 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Halber", "Maciej", ""], ["Shi", "Yifei", ""], ["Xu", "Kai", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1909.11277", "submitter": "Irwandi Hipiny", "authors": "Irwandi Hipiny, Hamimah Ujir, Aazani Mujahid, Nurhartini Kamalia Yahya", "title": "Towards Automated Biometric Identification of Sea Turtles (Chelonia\n  mydas)", "comments": "Published in Journal of ICT Research and Applications, [S.l.], v. 12,\n  n. 3, p. 256-266, dec. 2018", "journal-ref": "Journal of ICT Research and Applications, [S.l.], v. 12, n. 3, p.\n  256-266, dec. 2018. ISSN 2338-5499", "doi": "10.5614/itbj.ict.res.appl.2018.12.3.4.", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Passive biometric identification enables wildlife monitoring with minimal\ndisturbance. Using a motion-activated camera placed at an elevated position and\nfacing downwards, we collected images of sea turtle carapace, each belonging to\none of sixteen Chelonia mydas juveniles. We then learned co-variant and robust\nimage descriptors from these images, enabling indexing and retrieval. In this\nwork, we presented several classification results of sea turtle carapaces using\nthe learned image descriptors. We found that a template-based descriptor, i.e.,\nHistogram of Oriented Gradients (HOG) performed exceedingly better during\nclassification than keypoint-based descriptors. For our dataset, a\nhigh-dimensional descriptor is a must due to the minimal gradient and color\ninformation inside the carapace images. Using HOG, we obtained an average\nclassification accuracy of 65%.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 04:02:25 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 10:55:49 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Hipiny", "Irwandi", ""], ["Ujir", "Hamimah", ""], ["Mujahid", "Aazani", ""], ["Yahya", "Nurhartini Kamalia", ""]]}, {"id": "1909.11285", "submitter": "Ze Wang", "authors": "Ze Wang, Xiuyuan Cheng, Guillermo Sapiro, and Qiang Qiu", "title": "A Dictionary Approach to Domain-Invariant Learning in Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider domain-invariant deep learning by explicitly\nmodeling domain shifts with only a small amount of domain-specific parameters\nin a Convolutional Neural Network (CNN). By exploiting the observation that a\nconvolutional filter can be well approximated as a linear combination of a\nsmall set of dictionary atoms, we show for the first time, both empirically and\ntheoretically, that domain shifts can be effectively handled by decomposing a\nconvolutional layer into a domain-specific atom layer and a domain-shared\ncoefficient layer, while both remain convolutional. An input channel will now\nfirst convolve spatially only with each respective domain-specific dictionary\natom to \"absorb\" domain variations, and then output channels are linearly\ncombined using common decomposition coefficients trained to promote shared\nsemantics across domains. We use toy examples, rigorous analysis, and\nreal-world examples with diverse datasets and architectures, to show the\nproposed plug-in framework's effectiveness in cross and joint domain\nperformance and domain adaptation. With the proposed architecture, we need only\na small set of dictionary atoms to model each additional domain, which brings a\nnegligible amount of additional parameters, typically a few hundred.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 04:35:04 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 23:31:44 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Wang", "Ze", ""], ["Cheng", "Xiuyuan", ""], ["Sapiro", "Guillermo", ""], ["Qiu", "Qiang", ""]]}, {"id": "1909.11286", "submitter": "Ze Wang", "authors": "Ze Wang, Xiuyuan Cheng, Guillermo Sapiro, and Qiang Qiu", "title": "Stochastic Conditional Generative Networks with Basis Decomposition", "comments": "Published as a conference paper at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While generative adversarial networks (GANs) have revolutionized machine\nlearning, a number of open questions remain to fully understand them and\nexploit their power. One of these questions is how to efficiently achieve\nproper diversity and sampling of the multi-mode data space. To address this, we\nintroduce BasisGAN, a stochastic conditional multi-mode image generator. By\nexploiting the observation that a convolutional filter can be well approximated\nas a linear combination of a small set of basis elements, we learn a\nplug-and-played basis generator to stochastically generate basis elements, with\njust a few hundred of parameters, to fully embed stochasticity into\nconvolutional filters. By sampling basis elements instead of filters, we\ndramatically reduce the cost of modeling the parameter space with no sacrifice\non either image diversity or fidelity. To illustrate this proposed\nplug-and-play framework, we construct variants of BasisGAN based on\nstate-of-the-art conditional image generation networks, and train the networks\nby simply plugging in a basis generator, without additional auxiliary\ncomponents, hyperparameters, or training objectives. The experimental success\nis complemented with theoretical results indicating how the perturbations\nintroduced by the proposed sampling of basis elements can propagate to the\nappearance of generated images.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 04:37:38 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 19:35:47 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Wang", "Ze", ""], ["Cheng", "Xiuyuan", ""], ["Sapiro", "Guillermo", ""], ["Qiu", "Qiang", ""]]}, {"id": "1909.11289", "submitter": "Morgan Heisler", "authors": "Morgan Heisler, Forson Chan, Zaid Mammo, Chandrakumar Balaratnasingam,\n  Pavle Prentasic, Gavin Docherty, MyeongJin Ju, Sanjeeva Rajapakse, Sieun Lee,\n  Andrew Merkur, Andrew Kirker, David Albiani, David Maberley, K. Bailey\n  Freund, Mirza Faisal Beg, Sven Loncaric, Marinko V. Sarunic and Eduardo V.\n  Navajas", "title": "Deep learning vessel segmentation and quantification of the foveal\n  avascular zone using commercial and prototype OCT-A platforms", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic quantification of perifoveal vessel densities in optical coherence\ntomography angiography (OCT-A) images face challenges such as variable intra-\nand inter-image signal to noise ratios, projection artefacts from outer\nvasculature layers, and motion artefacts. This study demonstrates the utility\nof deep neural networks for automatic quantification of foveal avascular zone\n(FAZ) parameters and perifoveal vessel density of OCT-A images in healthy and\ndiabetic eyes. OCT-A images of the foveal region were acquired using three\nOCT-A systems: a 1060nm Swept Source (SS)-OCT prototype, RTVue XR Avanti\n(Optovue Inc., Fremont, CA), and the ZEISS Angioplex (Carl Zeiss Meditec,\nDublin, CA). Automated segmentation was then performed using a deep neural\nnetwork. Four FAZ morphometric parameters (area, min/max diameter, and\neccentricity) and perifoveal vessel density were used as outcome measures. The\naccuracy, sensitivity and specificity of the DNN vessel segmentations were\ncomparable across all three device platforms. No significant difference between\nthe means of the measurements from automated and manual segmentations were\nfound for any of the outcome measures on any system. The intraclass correlation\ncoefficient (ICC) was also good (> 0.51) for all measurements. Automated deep\nlearning vessel segmentation of OCT-A may be suitable for both commercial and\nresearch purposes for better quantification of the retinal circulation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 05:04:20 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Heisler", "Morgan", ""], ["Chan", "Forson", ""], ["Mammo", "Zaid", ""], ["Balaratnasingam", "Chandrakumar", ""], ["Prentasic", "Pavle", ""], ["Docherty", "Gavin", ""], ["Ju", "MyeongJin", ""], ["Rajapakse", "Sanjeeva", ""], ["Lee", "Sieun", ""], ["Merkur", "Andrew", ""], ["Kirker", "Andrew", ""], ["Albiani", "David", ""], ["Maberley", "David", ""], ["Freund", "K. Bailey", ""], ["Beg", "Mirza Faisal", ""], ["Loncaric", "Sven", ""], ["Sarunic", "Marinko V.", ""], ["Navajas", "Eduardo V.", ""]]}, {"id": "1909.11307", "submitter": "Yuanqiang Cai", "authors": "Yuanqiang Cai, Dawei Du, Libo Zhang, Longyin Wen, Weiqiang Wang,\n  Yanjun Wu, Siwei Lyu", "title": "Guided Attention Network for Object Detection and Counting on Drones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection and counting are related but challenging problems,\nespecially for drone based scenes with small objects and cluttered background.\nIn this paper, we propose a new Guided Attention Network (GANet) to deal with\nboth object detection and counting tasks based on the feature pyramid.\nDifferent from the previous methods relying on unsupervised attention modules,\nwe fuse different scales of feature maps by using the proposed\nweakly-supervised Background Attention (BA) between the background and objects\nfor more semantic feature representation. Then, the Foreground Attention (FA)\nmodule is developed to consider both global and local appearance of the object\nto facilitate accurate localization. Moreover, the new data argumentation\nstrategy is designed to train a robust model in various complex scenes.\nExtensive experiments on three challenging benchmarks (i.e., UAVDT, CARPK and\nPUCPR+) show the state-of-the-art detection and counting performance of the\nproposed method compared with existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 06:37:49 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Cai", "Yuanqiang", ""], ["Du", "Dawei", ""], ["Zhang", "Libo", ""], ["Wen", "Longyin", ""], ["Wang", "Weiqiang", ""], ["Wu", "Yanjun", ""], ["Lyu", "Siwei", ""]]}, {"id": "1909.11308", "submitter": "Chunpeng Wu", "authors": "Chunpeng Wu, Wei Wen, Yiran Chen, and Hai Li", "title": "Conditional Transferring Features: Scaling GANs to Thousands of Classes\n  with 30% Less High-quality Data for Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial network (GAN) has greatly improved the quality of\nunsupervised image generation. Previous GAN-based methods often require a large\namount of high-quality training data while producing a small number (e.g.,\ntens) of classes. This work aims to scale up GANs to thousands of classes\nmeanwhile reducing the use of high-quality data in training. We propose an\nimage generation method based on conditional transferring features, which can\ncapture pixel-level semantic changes when transforming low-quality images into\nhigh-quality ones. Moreover, self-supervision learning is integrated into our\nGAN architecture to provide more label-free semantic supervisory information\nobserved from the training data. As such, training our GAN architecture\nrequires much fewer high-quality images with a small number of additional\nlow-quality images. The experiments on CIFAR-10 and STL-10 show that even\nremoving 30% high-quality images from the training set, our method can still\noutperform previous ones. The scalability on object classes has been\nexperimentally validated: our method with 30% fewer high-quality images obtains\nthe best quality in generating 1,000 ImageNet classes, as well as generating\nall 3,755 classes of CASIA-HWDB1.0 Chinese handwriting characters.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 06:45:39 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Wu", "Chunpeng", ""], ["Wen", "Wei", ""], ["Chen", "Yiran", ""], ["Li", "Hai", ""]]}, {"id": "1909.11316", "submitter": "Feroz Ali T M", "authors": "T M Feroz Ali and Subhasis Chaudhuri", "title": "Cross-View Kernel Similarity Metric Learning Using Pairwise Constraints\n  for Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is the task of matching pedestrian images across\nnon-overlapping cameras. In this paper, we propose a non-linear cross-view\nsimilarity metric learning for handling small size training data in practical\nre-ID systems. The method employs non-linear mappings combined with cross-view\ndiscriminative subspace learning and cross-view distance metric learning based\non pairwise similarity constraints. It is a natural extension of XQDA from\nlinear to non-linear mappings using kernels, and learns non-linear\ntransformations for efficiently handling complex non-linearity of person\nappearance across camera views. Importantly, the proposed method is very\ncomputationally efficient. Extensive experiments on four challenging datasets\nshows that our method attains competitive performance against state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 07:30:23 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Ali", "T M Feroz", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "1909.11321", "submitter": "Jun-Gi Jang", "authors": "Jun-Gi Jang, Chun Quan, Hyun Dong Lee, U Kang", "title": "FALCON: Lightweight and Accurate Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we efficiently compress Convolutional Neural Network (CNN) while\nretaining their accuracy on classification tasks? Depthwise Separable\nConvolution (DSConv), which replaces a standard convolution with a depthwise\nconvolution and a pointwise convolution, has been used for building lightweight\narchitectures. However, previous works based on depthwise separable convolution\nare limited when compressing a trained CNN model since 1) they are mostly\nheuristic approaches without a precise understanding of their relations to\nstandard convolution, and 2) their accuracies do not match that of the standard\nconvolution. In this paper, we propose FALCON, an accurate and lightweight\nmethod to compress CNN. FALCON uses GEP, our proposed mathematical formulation\nto approximate the standard convolution kernel, to interpret existing\nconvolution methods based on depthwise separable convolution. By exploiting the\nknowledge of a trained standard model and carefully determining the order of\ndepthwise separable convolution via GEP, FALCON achieves sufficient accuracy\nclose to that of the trained standard model. Furthermore, this interpretation\nleads to developing a generalized version rank-k FALCON which performs k\nindependent FALCON operations and sums up the result. Experiments show that\nFALCON 1) provides higher accuracy than existing methods based on depthwise\nseparable convolution and tensor decomposition, and 2) reduces the number of\nparameters and FLOPs of standard convolution by up to a factor of 8 while\nensuring similar accuracy. We also demonstrate that rank-k FALCON further\nimproves the accuracy while sacrificing a bit of compression and computation\nreduction rates.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 07:48:31 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 04:31:56 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Jang", "Jun-Gi", ""], ["Quan", "Chun", ""], ["Lee", "Hyun Dong", ""], ["Kang", "U", ""]]}, {"id": "1909.11348", "submitter": "Koby Bibas", "authors": "Dotan Kaufman, Koby Bibas, Eran Borenstein, Michael Chertok and Tal\n  Hassner", "title": "Balancing Specialization, Generalization, and Compression for Detection\n  and Tracking", "comments": "Accepted to BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for specializing deep detectors and trackers to\nrestricted settings. Our approach is designed with the following goals in mind:\n(a) Improving accuracy in restricted domains; (b) preventing overfitting to new\ndomains and forgetting of generalized capabilities; (c) aggressive model\ncompression and acceleration. To this end, we propose a novel loss that\nbalances compression and acceleration of a deep learning model vs. loss of\ngeneralization capabilities. We apply our method to the existing tracker and\ndetector models. We report detection results on the VIRAT and CAVIAR data sets.\nThese results show our method to offer unprecedented compression rates along\nwith improved detection. We apply our loss for tracker compression at test\ntime, as it processes each video. Our tests on the OTB2015 benchmark show that\napplying compression during test time actually improves tracking performance.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 08:59:06 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Kaufman", "Dotan", ""], ["Bibas", "Koby", ""], ["Borenstein", "Eran", ""], ["Chertok", "Michael", ""], ["Hassner", "Tal", ""]]}, {"id": "1909.11362", "submitter": "Xiaolong Wu", "authors": "Xiaolong Wu, Patricio Vela, and Cedric Pradalier", "title": "Robust Monocular Edge Visual Odometry through Coarse-to-Fine Data\n  Association", "comments": "6 pages, 7 figures, 2 tables, submitted to iros2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a monocular visual odometry framework, which allows\nexploiting the best attributes of edge feature for illumination-robust camera\ntracking, while at the same time ameliorating the performance degradation of\nedge mapping. In the front-end, an ICP-based edge registration can provide\nrobust motion estimation and coarse data association under lighting changes. In\nthe back-end, a novel edge-guided data association pipeline searches for the\nbest photometrically matched points along geometrically possible edges through\ntemplate matching, so that the matches can be further refined in later bundle\nadjustment. The core of our proposed data association strategy lies in a\npoint-to-edge geometric uncertainty analysis, which analytically derives (1)\nthe probabilistic search length formula that significantly reduces the search\nspace for system speed-up and (2) the geometrical confidence metric for mapping\ndegradation detection based on the predicted depth uncertainty. Moreover, match\nconfidence based patch size adaption strategy is integrated into our pipeline,\nconnecting with other components, to reduce the matching ambiguity. We present\nextensive analysis and evaluation of our proposed system on synthetic and\nreal-world benchmark datasets under the influence of illumination changes and\nlarge camera motions, where our proposed system outperforms current\nstate-of-art algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 09:21:47 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 23:28:23 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Wu", "Xiaolong", ""], ["Vela", "Patricio", ""], ["Pradalier", "Cedric", ""]]}, {"id": "1909.11366", "submitter": "Zhe Xu", "authors": "Zhe Xu, Ray C. C. Cheung", "title": "Accurate and Compact Convolutional Neural Networks with Trained\n  Binarization", "comments": "Accepted as an Oral presentation in British Machine Vision Conference\n  (BMVC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although convolutional neural networks (CNNs) are now widely used in various\ncomputer vision applications, its huge resource demanding on parameter storage\nand computation makes the deployment on mobile and embedded devices difficult.\nRecently, binary convolutional neural networks are explored to help alleviate\nthis issue by quantizing both weights and activations with only 1 single bit.\nHowever, there may exist a noticeable accuracy degradation when compared with\nfull-precision models. In this paper, we propose an improved training approach\ntowards compact binary CNNs with higher accuracy. Trainable scaling factors for\nboth weights and activations are introduced to increase the value range. These\nscaling factors will be trained jointly with other parameters via\nbackpropagation. Besides, a specific training algorithm is developed including\ntight approximation for derivative of discontinuous binarization function and\n$L_2$ regularization acting on weight scaling factors. With these improvements,\nthe binary CNN achieves 92.3% accuracy on CIFAR-10 with VGG-Small network. On\nImageNet, our method also obtains 46.1% top-1 accuracy with AlexNet and 54.2%\nwith Resnet-18 surpassing previous works.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 09:29:50 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Xu", "Zhe", ""], ["Cheung", "Ray C. C.", ""]]}, {"id": "1909.11378", "submitter": "Ruyi Ji", "authors": "Ruyi Ji, Longyin Wen, Libo Zhang, Dawei Du, Yanjun Wu, Chen Zhao,\n  Xianglong Liu, Feiyue Huang", "title": "Attention Convolutional Binary Neural Tree for Fine-Grained Visual\n  Categorization", "comments": "accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained visual categorization (FGVC) is an important but challenging\ntask due to high intra-class variances and low inter-class variances caused by\ndeformation, occlusion, illumination, etc. An attention convolutional binary\nneural tree architecture is presented to address those problems for weakly\nsupervised FGVC. Specifically, we incorporate convolutional operations along\nedges of the tree structure, and use the routing functions in each node to\ndetermine the root-to-leaf computational paths within the tree. The final\ndecision is computed as the summation of the predictions from leaf nodes. The\ndeep convolutional operations learn to capture the representations of objects,\nand the tree structure characterizes the coarse-to-fine hierarchical feature\nlearning process. In addition, we use the attention transformer module to\nenforce the network to capture discriminative features. The negative\nlog-likelihood loss is used to train the entire network in an end-to-end\nfashion by SGD with back-propagation. Several experiments on the CUB-200-2011,\nStanford Cars and Aircraft datasets demonstrate that the proposed method\nperforms favorably against the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 10:03:48 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 04:59:49 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Ji", "Ruyi", ""], ["Wen", "Longyin", ""], ["Zhang", "Libo", ""], ["Du", "Dawei", ""], ["Wu", "Yanjun", ""], ["Zhao", "Chen", ""], ["Liu", "Xianglong", ""], ["Huang", "Feiyue", ""]]}, {"id": "1909.11380", "submitter": "Ketil Malde", "authors": "Ketil Malde and Hyeongji Kim", "title": "Beyond image classification: zooplankton identification with deep vector\n  space embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Zooplankton images, like many other real world data types, have intrinsic\nproperties that make the design of effective classification systems difficult.\nFor instance, the number of classes encountered in practical settings is\npotentially very large, and classes can be ambiguous or overlap. In addition,\nthe choice of taxonomy often differs between researchers and between\ninstitutions. Although high accuracy has been achieved in benchmarks using\nstandard classifier architectures, biases caused by an inflexible\nclassification scheme can have profound effects when the output is used in\necosystem assessments and monitoring.\n  Here, we propose using a deep convolutional network to construct a vector\nembedding of zooplankton images. The system maps (embeds) each image into a\nhigh-dimensional Euclidean space so that distances between vectors reflect\nsemantic relationships between images. We show that the embedding can be used\nto derive classifications with comparable accuracy to a specific classifier,\nbut that it simultaneously reveals important structures in the data.\nFurthermore, we apply the embedding to new classes previously unseen by the\nsystem, and evaluate its classification performance in such cases.\n  Traditional neural network classifiers perform well when the classes are\nclearly defined a priori and have sufficiently large labeled data sets\navailable. For practical cases in ecology as well as in many other fields this\nis not the case, and we argue that the vector embedding method presented here\nis a more appropriate approach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 10:12:05 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Malde", "Ketil", ""], ["Kim", "Hyeongji", ""]]}, {"id": "1909.11409", "submitter": "Dehua Song", "authors": "Dehua Song, Chang Xu, Xu Jia, Yiyi Chen, Chunjing Xu, Yunhe Wang", "title": "Efficient Residual Dense Block Search for Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although remarkable progress has been made on single image super-resolution\ndue to the revival of deep convolutional neural networks, deep learning methods\nare confronted with the challenges of computation and memory consumption in\npractice, especially for mobile devices. Focusing on this issue, we propose an\nefficient residual dense block search algorithm with multiple objectives to\nhunt for fast, lightweight and accurate networks for image super-resolution.\nFirstly, to accelerate super-resolution network, we exploit the variation of\nfeature scale adequately with the proposed efficient residual dense blocks. In\nthe proposed evolutionary algorithm, the locations of pooling and upsampling\noperator are searched automatically. Secondly, network architecture is evolved\nwith the guidance of block credits to acquire accurate super-resolution\nnetwork. The block credit reflects the effect of current block and is earned\nduring model evaluation process. It guides the evolution by weighing the\nsampling probability of mutation to favor admirable blocks. Extensive\nexperimental results demonstrate the effectiveness of the proposed searching\nmethod and the found efficient super-resolution models achieve better\nperformance than the state-of-the-art methods with limited number of parameters\nand FLOPs.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 11:19:49 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 12:32:48 GMT"}, {"version": "v3", "created": "Mon, 30 Dec 2019 08:04:18 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Song", "Dehua", ""], ["Xu", "Chang", ""], ["Jia", "Xu", ""], ["Chen", "Yiyi", ""], ["Xu", "Chunjing", ""], ["Wang", "Yunhe", ""]]}, {"id": "1909.11464", "submitter": "Karin Van Garderen", "authors": "Karin van Garderen, Marion Smits, Stefan Klein", "title": "Multi-modal segmentation with missing MR sequences using pre-trained\n  fusion networks", "comments": "Accepted at MICCAI MIL3ID workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data is a common problem in machine learning and in retrospective\nimaging research it is often encountered in the form of missing imaging\nmodalities. We propose to take into account missing modalities in the design\nand training of neural networks, to ensure that they are capable of providing\nthe best possible prediction even when multiple images are not available. The\nproposed network combines three modifications to the standard 3D UNet\narchitecture: a training scheme with dropout of modalities, a multi-pathway\narchitecture with fusion layer in the final stage, and the separate\npre-training of these pathways. These modifications are evaluated incrementally\nin terms of performance on full and missing data, using the BraTS multi-modal\nsegmentation challenge. The final model shows significant improvement with\nrespect to the state of the art on missing data and requires less memory during\ntraining.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 13:04:28 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["van Garderen", "Karin", ""], ["Smits", "Marion", ""], ["Klein", "Stefan", ""]]}, {"id": "1909.11469", "submitter": "Vinu Joseph", "authors": "Mark Van der Merwe, Vinu Joseph, Ganesh Gopalakrishnan", "title": "Message Scheduling for Performant, Many-Core Belief Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.CV cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Belief Propagation (BP) is a message-passing algorithm for approximate\ninference over Probabilistic Graphical Models (PGMs), finding many applications\nsuch as computer vision, error-correcting codes, and protein-folding. While\ngeneral, the convergence and speed of the algorithm has limited its practical\nuse on difficult inference problems. As an algorithm that is highly amenable to\nparallelization, many-core Graphical Processing Units (GPUs) could\nsignificantly improve BP performance. Improving BP through many-core systems is\nnon-trivial: the scheduling of messages in the algorithm strongly affects\nperformance. We present a study of message scheduling for BP on GPUs. We\ndemonstrate that BP exhibits a tradeoff between speed and convergence based on\nparallelism and show that existing message schedulings are not able to utilize\nthis tradeoff. To this end, we present a novel randomized message scheduling\napproach, Randomized BP (RnBP), which outperforms existing methods on the GPU.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 05:19:33 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Van der Merwe", "Mark", ""], ["Joseph", "Vinu", ""], ["Gopalakrishnan", "Ganesh", ""]]}, {"id": "1909.11479", "submitter": "Karin Van Garderen", "authors": "Karin van Garderen, Sebastian van der Voort, Fatih Incekara, Marion\n  Smits, Stefan Klein", "title": "Towards continuous learning for glioma segmentation with elastic weight\n  consolidation", "comments": null, "journal-ref": null, "doi": null, "report-no": "MIDL/2019/ExtendedAbstract/Hkx_ry0NcN", "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  When finetuning a convolutional neural network (CNN) on data from a new\ndomain, catastrophic forgetting will reduce performance on the original\ntraining data. Elastic Weight Consolidation (EWC) is a recent technique to\nprevent this, which we evaluated while training and re-training a CNN to\nsegment glioma on two different datasets. The network was trained on the public\nBraTS dataset and finetuned on an in-house dataset with non-enhancing low-grade\nglioma. EWC was found to decrease catastrophic forgetting in this case, but was\nalso found to restrict adaptation to the new domain.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 13:27:23 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["van Garderen", "Karin", ""], ["van der Voort", "Sebastian", ""], ["Incekara", "Fatih", ""], ["Smits", "Marion", ""], ["Klein", "Stefan", ""]]}, {"id": "1909.11481", "submitter": "Evgenii Zheltonozhskii", "authors": "Chaim Baskin, Brian Chmiel, Evgenii Zheltonozhskii, Ron Banner, Alex\n  M. Bronstein, Avi Mendelson", "title": "CAT: Compression-Aware Training for bandwidth reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional neural networks (CNNs) have become the dominant neural network\narchitecture for solving visual processing tasks. One of the major obstacles\nhindering the ubiquitous use of CNNs for inference is their relatively high\nmemory bandwidth requirements, which can be a main energy consumer and\nthroughput bottleneck in hardware accelerators. Accordingly, an efficient\nfeature map compression method can result in substantial performance gains.\nInspired by quantization-aware training approaches, we propose a\ncompression-aware training (CAT) method that involves training the model in a\nway that allows better compression of feature maps during inference. Our method\ntrains the model to achieve low-entropy feature maps, which enables efficient\ncompression at inference time using classical transform coding methods. CAT\nsignificantly improves the state-of-the-art results reported for quantization.\nFor example, on ResNet-34 we achieve 73.1% accuracy (0.2% degradation from the\nbaseline) with an average representation of only 1.79 bits per value. Reference\nimplementation accompanies the paper at https://github.com/CAT-teams/CAT\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 13:29:58 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Baskin", "Chaim", ""], ["Chmiel", "Brian", ""], ["Zheltonozhskii", "Evgenii", ""], ["Banner", "Ron", ""], ["Bronstein", "Alex M.", ""], ["Mendelson", "Avi", ""]]}, {"id": "1909.11498", "submitter": "Liheng Bian", "authors": "Hao Fu, Liheng Bian and Jun Zhang", "title": "Non-imaging single-pixel sensing with optimized binary modulation", "comments": null, "journal-ref": null, "doi": "10.1364/OL.395150", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional high-level sensing techniques require high-fidelity images\nas input to extract target features, which are produced by either complex\nimaging hardware or high-complexity reconstruction algorithms. In this letter,\nwe propose single-pixel sensing (SPS) that performs high-level sensing directly\nfrom coupled measurements of a single-pixel detector, without the conventional\nimage acquisition and reconstruction process. The technique consists of three\nsteps including binary light modulation that can be physically implemented at\n$\\sim$22kHz, single-pixel coupled detection owning wide working spectrum and\nhigh signal-to-noise ratio, and end-to-end deep-learning based sensing that\nreduces both hardware and software complexity. Besides, the binary modulation\nis trained and optimized together with the sensing network, which ensures least\nrequired measurements and optimal sensing accuracy. The effectiveness of SPS is\ndemonstrated on the classification task of handwritten MNIST dataset, and\n96.68% classification accuracy at $\\sim$1kHz is achieved. The reported\nsingle-pixel sensing technique is a novel framework for highly efficient\nmachine intelligence.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 13:52:06 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 15:27:10 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Fu", "Hao", ""], ["Bian", "Liheng", ""], ["Zhang", "Jun", ""]]}, {"id": "1909.11504", "submitter": "Mahmut Yurt", "authors": "Mahmut Yurt, Salman Ul Hassan Dar, Aykut Erdem, Erkut Erdem and Tolga\n  \\c{C}ukur", "title": "mustGAN: Multi-Stream Generative Adversarial Networks for MR Image\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-contrast MRI protocols increase the level of morphological information\navailable for diagnosis. Yet, the number and quality of contrasts is limited in\npractice by various factors including scan time and patient motion. Synthesis\nof missing or corrupted contrasts can alleviate this limitation to improve\nclinical utility. Common approaches for multi-contrast MRI involve either\none-to-one and many-to-one synthesis methods. One-to-one methods take as input\na single source contrast, and they learn a latent representation sensitive to\nunique features of the source. Meanwhile, many-to-one methods receive multiple\ndistinct sources, and they learn a shared latent representation more sensitive\nto common features across sources. For enhanced image synthesis, here we\npropose a multi-stream approach that aggregates information across multiple\nsource images via a mixture of multiple one-to-one streams and a joint\nmany-to-one stream. The shared feature maps generated in the many-to-one stream\nand the complementary feature maps generated in the one-to-one streams are\ncombined with a fusion block. The location of the fusion block is adaptively\nmodified to maximize task-specific performance. Qualitative and quantitative\nassessments on T1-, T2-, PD-weighted and FLAIR images clearly demonstrate the\nsuperior performance of the proposed method compared to previous\nstate-of-the-art one-to-one and many-to-one methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 14:11:06 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Yurt", "Mahmut", ""], ["Dar", "Salman Ul Hassan", ""], ["Erdem", "Aykut", ""], ["Erdem", "Erkut", ""], ["\u00c7ukur", "Tolga", ""]]}, {"id": "1909.11508", "submitter": "Neelanjan Bhowmik", "authors": "Neelanjan Bhowmik, Qian Wang, Yona Falinie A. Gaus, Marcin Szarek,\n  Toby P. Breckon", "title": "The Good, the Bad and the Ugly: Evaluating Convolutional Neural Networks\n  for Prohibited Item Detection Using Real and Synthetically Composited X-ray\n  Imagery", "comments": null, "journal-ref": "In Proc. British Machine Vision Conference Workshops, BMVA, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting prohibited items in X-ray security imagery is pivotal in\nmaintaining border and transport security against a wide range of threat\nprofiles. Convolutional Neural Networks (CNN) with the support of a significant\nvolume of data have brought advancement in such automated prohibited object\ndetection and classification. However, collating such large volumes of X-ray\nsecurity imagery remains a significant challenge. This work opens up the\npossibility of using synthetically composed imagery, avoiding the need to\ncollate such large volumes of hand-annotated real-world imagery. Here we\ninvestigate the difference in detection performance achieved using real and\nsynthetic X-ray training imagery for CNN architecture detecting three exemplar\nprohibited items, {Firearm, Firearm Parts, Knives}, within cluttered and\ncomplex X-ray security baggage imagery. We achieve 0.88 of mean average\nprecision (mAP) with a Faster R-CNN and ResNet-101 CNN architecture for this\n3-class object detection using real X-ray imagery. While the performance is\ncomparable with synthetically composited X-ray imagery (0.78 mAP), our extended\nevaluation demonstrates both challenge and promise of using synthetically\ncomposed images to diversify the X-ray security training imagery for automated\ndetection algorithm training.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 14:16:16 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Bhowmik", "Neelanjan", ""], ["Wang", "Qian", ""], ["Gaus", "Yona Falinie A.", ""], ["Szarek", "Marcin", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1909.11512", "submitter": "Sergey Nikolenko", "authors": "Sergey I. Nikolenko", "title": "Synthetic Data for Deep Learning", "comments": "156 pages, 24 figures, 719 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic data is an increasingly popular tool for training deep learning\nmodels, especially in computer vision but also in other areas. In this work, we\nattempt to provide a comprehensive survey of the various directions in the\ndevelopment and application of synthetic data. First, we discuss synthetic\ndatasets for basic computer vision problems, both low-level (e.g., optical flow\nestimation) and high-level (e.g., semantic segmentation), synthetic\nenvironments and datasets for outdoor and urban scenes (autonomous driving),\nindoor scenes (indoor navigation), aerial navigation, simulation environments\nfor robotics, applications of synthetic data outside computer vision (in neural\nprogramming, bioinformatics, NLP, and more); we also survey the work on\nimproving synthetic data development and alternative ways to produce it such as\nGANs. Second, we discuss in detail the synthetic-to-real domain adaptation\nproblem that inevitably arises in applications of synthetic data, including\nsynthetic-to-real refinement with GAN-based models and domain adaptation at the\nfeature/model level without explicit data transformations. Third, we turn to\nprivacy-related applications of synthetic data and review the work on\ngenerating synthetic datasets with differential privacy guarantees. We conclude\nby highlighting the most promising directions for further work in synthetic\ndata studies.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 14:20:57 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Nikolenko", "Sergey I.", ""]]}, {"id": "1909.11515", "submitter": "Tianyu Pang", "authors": "Tianyu Pang, Kun Xu, Jun Zhu", "title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been widely recognized that adversarial examples can be easily crafted\nto fool deep networks, which mainly root from the locally non-linear behavior\nnearby input examples. Applying mixup in training provides an effective\nmechanism to improve generalization performance and model robustness against\nadversarial perturbations, which introduces the globally linear behavior\nin-between training examples. However, in previous work, the mixup-trained\nmodels only passively defend adversarial attacks in inference by directly\nclassifying the inputs, where the induced global linearity is not well\nexploited. Namely, since the locality of the adversarial perturbations, it\nwould be more efficient to actively break the locality via the globality of the\nmodel predictions. Inspired by simple geometric intuition, we develop an\ninference principle, named mixup inference (MI), for mixup-trained models. MI\nmixups the input with other random clean samples, which can shrink and transfer\nthe equivalent perturbation if the input is adversarial. Our experiments on\nCIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial\nrobustness for the models trained by mixup and its variants.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 14:21:55 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 08:54:57 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Pang", "Tianyu", ""], ["Xu", "Kun", ""], ["Zhu", "Jun", ""]]}, {"id": "1909.11519", "submitter": "Yu Wu", "authors": "Zongxin Yang, Linchao Zhu, Yu Wu, Yi Yang", "title": "Gated Channel Transformation for Visual Recognition", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a generally applicable transformation unit for\nvisual recognition with deep convolutional neural networks. This transformation\nexplicitly models channel relationships with explainable control variables.\nThese variables determine the neuron behaviors of competition or cooperation,\nand they are jointly optimized with the convolutional weight towards more\naccurate recognition. In Squeeze-and-Excitation (SE) Networks, the channel\nrelationships are implicitly learned by fully connected layers, and the SE\nblock is integrated at the block-level. We instead introduce a channel\nnormalization layer to reduce the number of parameters and computational\ncomplexity. This lightweight layer incorporates a simple l2 normalization,\nenabling our transformation unit applicable to operator-level without much\nincrease of additional parameters. Extensive experiments demonstrate the\neffectiveness of our unit with clear margins on many vision tasks, i.e., image\nclassification on ImageNet, object detection and instance segmentation on COCO,\nvideo classification on Kinetics.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 14:26:32 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 10:08:39 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Yang", "Zongxin", ""], ["Zhu", "Linchao", ""], ["Wu", "Yu", ""], ["Yang", "Yi", ""]]}, {"id": "1909.11524", "submitter": "Xianxu Hou", "authors": "Xianxu Hou, Jingxin Liu, Bolei Xu, Bozhi Liu, Xin Chen, Mohammad\n  Ilyas, Ian Ellis, Jon Garibaldi, Guoping Qiu", "title": "Dual Adaptive Pyramid Network for Cross-Stain Histopathology Image\n  Segmentation", "comments": "MICCAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised semantic segmentation normally assumes the test data being in a\nsimilar data domain as the training data. However, in practice, the domain\nmismatch between the training and unseen data could lead to a significant\nperformance drop. Obtaining accurate pixel-wise label for images in different\ndomains is tedious and labor intensive, especially for histopathology images.\nIn this paper, we propose a dual adaptive pyramid network (DAPNet) for\nhistopathological gland segmentation adapting from one stain domain to another.\nWe tackle the domain adaptation problem on two levels: 1) the image-level\nconsiders the differences of image color and style; 2) the feature-level\naddresses the spatial inconsistency between two domains. The two components are\nimplemented as domain classifiers with adversarial training. We evaluate our\nnew approach using two gland segmentation datasets with H&E and DAB-H stains\nrespectively. The extensive experiments and ablation study demonstrate the\neffectiveness of our approach on the domain adaptive segmentation task. We show\nthat the proposed approach performs favorably against other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 14:31:02 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Hou", "Xianxu", ""], ["Liu", "Jingxin", ""], ["Xu", "Bolei", ""], ["Liu", "Bozhi", ""], ["Chen", "Xin", ""], ["Ilyas", "Mohammad", ""], ["Ellis", "Ian", ""], ["Garibaldi", "Jon", ""], ["Qiu", "Guoping", ""]]}, {"id": "1909.11573", "submitter": "Thanh Thi Nguyen", "authors": "Thanh Thi Nguyen, Quoc Viet Hung Nguyen, Cuong M. Nguyen, Dung Nguyen,\n  Duc Thanh Nguyen and Saeid Nahavandi", "title": "Deep Learning for Deepfakes Creation and Detection: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning has been successfully applied to solve various complex problems\nranging from big data analytics to computer vision and human-level control.\nDeep learning advances however have also been employed to create software that\ncan cause threats to privacy, democracy and national security. One of those\ndeep learning-powered applications recently emerged is deepfake. Deepfake\nalgorithms can create fake images and videos that humans cannot distinguish\nthem from authentic ones. The proposal of technologies that can automatically\ndetect and assess the integrity of digital visual media is therefore\nindispensable. This paper presents a survey of algorithms used to create\ndeepfakes and, more importantly, methods proposed to detect deepfakes in the\nliterature to date. We present extensive discussions on challenges, research\ntrends and directions related to deepfake technologies. By reviewing the\nbackground of deepfakes and state-of-the-art deepfake detection methods, this\nstudy provides a comprehensive overview of deepfake techniques and facilitates\nthe development of new and more robust methods to deal with the increasingly\nchallenging deepfakes.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 16:03:45 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 17:54:11 GMT"}, {"version": "v3", "created": "Mon, 26 Apr 2021 05:23:48 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Nguyen", "Thanh Thi", ""], ["Nguyen", "Quoc Viet Hung", ""], ["Nguyen", "Cuong M.", ""], ["Nguyen", "Dung", ""], ["Nguyen", "Duc Thanh", ""], ["Nahavandi", "Saeid", ""]]}, {"id": "1909.11574", "submitter": "Karsten Roth", "authors": "Karsten Roth and Biagio Brattoli and Bj\\\"orn Ommer", "title": "MIC: Mining Interclass Characteristics for Improved Metric Learning", "comments": "8 pages, 10 figures, accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning seeks to embed images of objects suchthat class-defined\nrelations are captured by the embeddingspace. However, variability in images is\nnot just due to different depicted object classes, but also depends on other\nlatent characteristics such as viewpoint or illumination. In addition to these\nstructured properties, random noise further obstructs the visual relations of\ninterest. The common approach to metric learning is to enforce a representation\nthat is invariant under all factors but the ones of interest. In contrast, we\npropose to explicitly learn the latent characteristics that are shared by and\ngo across object classes. We can then directly explain away structured visual\nvariability, rather than assuming it to be unknown random noise. We propose a\nnovel surrogate task to learn visual characteristics shared across classes with\na separate encoder. This encoder is trained jointly with the encoder for class\ninformation by reducing their mutual information. On five standard image\nretrieval benchmarks the approach significantly improves upon the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 16:04:27 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Roth", "Karsten", ""], ["Brattoli", "Biagio", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "1909.11575", "submitter": "Karin Stacke", "authors": "Karin Stacke, Gabriel Eilertsen, Jonas Unger, Claes Lundstr\\\"om", "title": "A Closer Look at Domain Shift for Deep Learning in Histopathology", "comments": "8 pages, 4 figures. Accepted to COMPAY2019: Second MICCAI Workshop on\n  Computational Pathology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain shift is a significant problem in histopathology. There can be large\ndifferences in data characteristics of whole-slide images between medical\ncenters and scanners, making generalization of deep learning to unseen data\ndifficult. To gain a better understanding of the problem, we present a study on\nconvolutional neural networks trained for tumor classification of H&E stained\nwhole-slide images. We analyze how augmentation and normalization strategies\naffect performance and learned representations, and what features a trained\nmodel respond to. Most centrally, we present a novel measure for evaluating the\ndistance between domains in the context of the learned representation of a\nparticular model. This measure can reveal how sensitive a model is to domain\nvariations, and can be used to detect new data that a model will have problems\ngeneralizing to. The results show how learning is heavily influenced by the\npreparation of training data, and that the latent representation used to do\nclassification is sensitive to changes in data distribution, especially when\ntraining without augmentation or normalization.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 16:06:05 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 03:20:50 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Stacke", "Karin", ""], ["Eilertsen", "Gabriel", ""], ["Unger", "Jonas", ""], ["Lundstr\u00f6m", "Claes", ""]]}, {"id": "1909.11625", "submitter": "Ayush Singh", "authors": "Ayush Singh, Seyed Sadegh Mohseni Salehi, Ali Gholipour", "title": "Deep Predictive Motion Tracking in Magnetic Resonance Imaging:\n  Application to Fetal Imaging", "comments": "The article has been published in IEEE TMI: 14 pages, 11 figures, 2\n  tables and 1 supplementary\n  https://github.com/bchimagine/DeepPredictiveMotionTracking", "journal-ref": null, "doi": "10.1109/TMI.2020.2998600", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fetal magnetic resonance imaging (MRI) is challenged by uncontrollable,\nlarge, and irregular fetal movements. It is, therefore, performed through\nvisual monitoring of fetal motion and repeated acquisitions to ensure\ndiagnostic-quality images are acquired. Nevertheless, visual monitoring of\nfetal motion based on displayed slices, and navigation at the level of\nstacks-of-slices is inefficient. The current process is highly\noperator-dependent, increases scanner usage and cost, and significantly\nincreases the length of fetal MRI scans which makes them hard to tolerate for\npregnant women. To help build automatic MRI motion tracking and navigation\nsystems to overcome the limitations of the current process and improve fetal\nimaging, we have developed a new real time image-based motion tracking method\nbased on deep learning that learns to predict fetal motion directly from\nacquired images. Our method is based on a recurrent neural network, composed of\nspatial and temporal encoder-decoders, that infers motion parameters from\nanatomical features extracted from sequences of acquired slices. We compared\nour trained network on held out test sets (including data with different\ncharacteristics, e.g. different fetuses scanned at different ages, and motion\ntrajectories recorded from volunteer subjects) with networks designed for\nestimation as well as methods adopted to make predictions. The results show\nthat our method outperformed alternative techniques, and achieved real-time\nperformance with average errors of 3.5 and 8 degrees for the estimation and\nprediction tasks, respectively. Our real-time deep predictive motion tracking\ntechnique can be used to assess fetal movements, to guide slice acquisitions,\nand to build navigation systems for fetal MRI.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 17:12:40 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 14:03:27 GMT"}, {"version": "v3", "created": "Sat, 6 Jun 2020 23:15:28 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Singh", "Ayush", ""], ["Salehi", "Seyed Sadegh Mohseni", ""], ["Gholipour", "Ali", ""]]}, {"id": "1909.11663", "submitter": "Tristan Bepler", "authors": "Tristan Bepler, Ellen D. Zhong, Kotaro Kelley, Edward Brignole, and\n  Bonnie Berger", "title": "Explicitly disentangling image content from translation and rotation\n  with spatial-VAE", "comments": "11 pages, 6 figures, to appear in the 33rd Conference on Neural\n  Information Processing Systems (NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an image dataset, we are often interested in finding data generative\nfactors that encode semantic content independently from pose variables such as\nrotation and translation. However, current disentanglement approaches do not\nimpose any specific structure on the learned latent representations. We propose\na method for explicitly disentangling image rotation and translation from other\nunstructured latent factors in a variational autoencoder (VAE) framework. By\nformulating the generative model as a function of the spatial coordinate, we\nmake the reconstruction error differentiable with respect to latent translation\nand rotation parameters. This formulation allows us to train a neural network\nto perform approximate inference on these latent variables while explicitly\nconstraining them to only represent rotation and translation. We demonstrate\nthat this framework, termed spatial-VAE, effectively learns latent\nrepresentations that disentangle image rotation and translation from content\nand improves reconstruction over standard VAEs on several benchmark datasets,\nincluding applications to modeling continuous 2-D views of proteins from single\nparticle electron microscopy and galaxies in astronomical images.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 17:17:30 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Bepler", "Tristan", ""], ["Zhong", "Ellen D.", ""], ["Kelley", "Kotaro", ""], ["Brignole", "Edward", ""], ["Berger", "Bonnie", ""]]}, {"id": "1909.11721", "submitter": "Hongming Shan", "authors": "Wenxiang Cong, Hongming Shan, Xiaohua Zhang, Shaohua Liu, Ruola Ning,\n  Ge Wang", "title": "Deep-learning-based Breast CT for Radiation Dose Reduction", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": "10.1117/12.2530234", "report-no": null, "categories": "physics.med-ph cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cone-beam breast computed tomography (CT) provides true 3D breast images with\nisotropic resolution and high-contrast information, detecting calcifications as\nsmall as a few hundred microns and revealing subtle tissue differences.\nHowever, breast is highly sensitive to x-ray radiation. It is critically\nimportant for healthcare to reduce radiation dose. Few-view cone-beam CT only\nuses a fraction of x-ray projection data acquired by standard cone-beam breast\nCT, enabling significant reduction of the radiation dose. However, insufficient\nsampling data would cause severe streak artifacts in CT images reconstructed\nusing conventional methods. In this study, we propose a deep-learning-based\nmethod to establish a residual neural network model for the image\nreconstruction, which is applied for few-view breast CT to produce high quality\nbreast CT images. We respectively evaluate the deep-learning-based image\nreconstruction using one third and one quarter of x-ray projection views of the\nstandard cone-beam breast CT. Based on clinical breast imaging dataset, we\nperform a supervised learning to train the neural network from few-view CT\nimages to corresponding full-view CT images. Experimental results show that the\ndeep learning-based image reconstruction method allows few-view breast CT to\nachieve a radiation dose <6 mGy per cone-beam CT scan, which is a threshold set\nby FDA for mammographic screening.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 19:30:08 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Cong", "Wenxiang", ""], ["Shan", "Hongming", ""], ["Zhang", "Xiaohua", ""], ["Liu", "Shaohua", ""], ["Ning", "Ruola", ""], ["Wang", "Ge", ""]]}, {"id": "1909.11723", "submitter": "Li Yuan", "authors": "Li Yuan, Francis E.H.Tay, Guilin Li, Tao Wang, Jiashi Feng", "title": "Revisiting Knowledge Distillation via Label Smoothing Regularization", "comments": "CVPR2020 Oral, codes:\n  https://github.com/yuanli2333/Teacher-free-Knowledge-Distillation", "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Distillation (KD) aims to distill the knowledge of a cumbersome\nteacher model into a lightweight student model. Its success is generally\nattributed to the privileged information on similarities among categories\nprovided by the teacher model, and in this sense, only strong teacher models\nare deployed to teach weaker students in practice. In this work, we challenge\nthis common belief by following experimental observations: 1) beyond the\nacknowledgment that the teacher can improve the student, the student can also\nenhance the teacher significantly by reversing the KD procedure; 2) a\npoorly-trained teacher with much lower accuracy than the student can still\nimprove the latter significantly. To explain these observations, we provide a\ntheoretical analysis of the relationships between KD and label smoothing\nregularization. We prove that 1) KD is a type of learned label smoothing\nregularization and 2) label smoothing regularization provides a virtual teacher\nmodel for KD. From these results, we argue that the success of KD is not fully\ndue to the similarity information between categories from teachers, but also to\nthe regularization of soft targets, which is equally or even more important.\n  Based on these analyses, we further propose a novel Teacher-free Knowledge\nDistillation (Tf-KD) framework, where a student model learns from itself or\nmanuallydesigned regularization distribution. The Tf-KD achieves comparable\nperformance with normal KD from a superior teacher, which is well applied when\na stronger teacher model is unavailable. Meanwhile, Tf-KD is generic and can be\ndirectly deployed for training deep neural networks. Without any extra\ncomputation cost, Tf-KD achieves up to 0.65\\% improvement on ImageNet over\nwell-established baseline models, which is superior to label smoothing\nregularization.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 19:33:43 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 03:53:49 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 08:02:53 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Yuan", "Li", ""], ["Tay", "Francis E. H.", ""], ["Li", "Guilin", ""], ["Wang", "Tao", ""], ["Feng", "Jiashi", ""]]}, {"id": "1909.11730", "submitter": "Andrew Hundt", "authors": "Andrew Hundt and Benjamin Killeen and Nicholas Greene and Hongtao Wu\n  and Heeyeon Kwon and Chris Paxton and Gregory D. Hager", "title": "\"Good Robot!\": Efficient Reinforcement Learning for Multi-Step Visual\n  Tasks with Sim to Real Transfer", "comments": "Accepted to the journal IEEE Robotics and Automation Letters (RA-L)\n  and to be presented at IROS 2020. This is a minor update to v3. 8 pages, 6\n  figures, 3 tables, 1 algorithm. Code is available at\n  https://github.com/jhu-lcsr/good_robot and a video overview is at\n  https://youtu.be/MbCuEZadkIw", "journal-ref": null, "doi": "10.1109/LRA.2020.3015448", "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current Reinforcement Learning (RL) algorithms struggle with long-horizon\ntasks where time can be wasted exploring dead ends and task progress may be\neasily reversed. We develop the SPOT framework, which explores within action\nsafety zones, learns about unsafe regions without exploring them, and\nprioritizes experiences that reverse earlier progress to learn with remarkable\nefficiency.\n  The SPOT framework successfully completes simulated trials of a variety of\ntasks, improving a baseline trial success rate from 13% to 100% when stacking 4\ncubes, from 13% to 99% when creating rows of 4 cubes, and from 84% to 95% when\nclearing toys arranged in adversarial patterns. Efficiency with respect to\nactions per trial typically improves by 30% or more, while training takes just\n1-20k actions, depending on the task.\n  Furthermore, we demonstrate direct sim to real transfer. We are able to\ncreate real stacks in 100% of trials with 61% efficiency and real rows in 100%\nof trials with 59% efficiency by directly loading the simulation-trained model\non the real robot with no additional real-world fine-tuning. To our knowledge,\nthis is the first instance of reinforcement learning with successful sim to\nreal transfer applied to long term multi-step tasks such as block-stacking and\nrow-making with consideration of progress reversal. Code is available at\nhttps://github.com/jhu-lcsr/good_robot .\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 19:50:36 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 21:05:14 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2020 21:42:30 GMT"}, {"version": "v4", "created": "Sat, 15 Aug 2020 18:10:40 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Hundt", "Andrew", ""], ["Killeen", "Benjamin", ""], ["Greene", "Nicholas", ""], ["Wu", "Hongtao", ""], ["Kwon", "Heeyeon", ""], ["Paxton", "Chris", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1909.11735", "submitter": "Oran Shayer", "authors": "Oran Shayer, Michael Lindenbaum", "title": "Learning Pixel Representations for Generic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches to generic (non-semantic) segmentation have so far\nbeen indirect and relied on edge detection. This is in contrast to semantic\nsegmentation, where DNNs are applied directly. We propose an alternative\napproach called Deep Generic Segmentation (DGS) and try to follow the path used\nfor semantic segmentation. Our main contribution is a new method for learning a\npixel-wise representation that reflects segment relatedness. This\nrepresentation is combined with a CRF to yield the segmentation algorithm. We\nshow that we are able to learn meaningful representations that improve\nsegmentation quality and that the representations themselves achieve\nstate-of-the-art segment similarity scores. The segmentation results are\ncompetitive and promising.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 19:56:29 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Shayer", "Oran", ""], ["Lindenbaum", "Michael", ""]]}, {"id": "1909.11740", "submitter": "Yen-Chun Chen", "authors": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed,\n  Zhe Gan, Yu Cheng, Jingjing Liu", "title": "UNITER: UNiversal Image-TExt Representation Learning", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint image-text embedding is the bedrock for most Vision-and-Language (V+L)\ntasks, where multimodality inputs are simultaneously processed for joint visual\nand textual understanding. In this paper, we introduce UNITER, a UNiversal\nImage-TExt Representation, learned through large-scale pre-training over four\nimage-text datasets (COCO, Visual Genome, Conceptual Captions, and SBU\nCaptions), which can power heterogeneous downstream V+L tasks with joint\nmultimodal embeddings. We design four pre-training tasks: Masked Language\nModeling (MLM), Masked Region Modeling (MRM, with three variants), Image-Text\nMatching (ITM), and Word-Region Alignment (WRA). Different from previous work\nthat applies joint random masking to both modalities, we use conditional\nmasking on pre-training tasks (i.e., masked language/region modeling is\nconditioned on full observation of image/text). In addition to ITM for global\nimage-text alignment, we also propose WRA via the use of Optimal Transport (OT)\nto explicitly encourage fine-grained alignment between words and image regions\nduring pre-training. Comprehensive analysis shows that both conditional masking\nand OT-based WRA contribute to better pre-training. We also conduct a thorough\nablation study to find an optimal combination of pre-training tasks. Extensive\nexperiments show that UNITER achieves new state of the art across six V+L tasks\n(over nine datasets), including Visual Question Answering, Image-Text\nRetrieval, Referring Expression Comprehension, Visual Commonsense Reasoning,\nVisual Entailment, and NLVR$^2$. Code is available at\nhttps://github.com/ChenRocks/UNITER.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 20:02:54 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 05:03:12 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 22:19:59 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Chen", "Yen-Chun", ""], ["Li", "Linjie", ""], ["Yu", "Licheng", ""], ["Kholy", "Ahmed El", ""], ["Ahmed", "Faisal", ""], ["Gan", "Zhe", ""], ["Cheng", "Yu", ""], ["Liu", "Jingjing", ""]]}, {"id": "1909.11795", "submitter": "Jo Schlemper", "authors": "Jo Schlemper, Jinming Duan, Cheng Ouyang, Chen Qin, Jose Caballero,\n  Joseph V. Hajnal, Daniel Rueckert", "title": "Data consistency networks for (calibration-less) accelerated parallel MR\n  image reconstruction", "comments": "Presented at ISMRM 27th Annual Meeting & Exhibition (Abstract #4663)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present simple reconstruction networks for multi-coil data by extending\ndeep cascade of CNN's and exploiting the data consistency layer. In particular,\nwe propose two variants, where one is inspired by POCSENSE and the other is\ncalibration-less. We show that the proposed approaches are competitive relative\nto the state of the art both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 22:15:56 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Schlemper", "Jo", ""], ["Duan", "Jinming", ""], ["Ouyang", "Cheng", ""], ["Qin", "Chen", ""], ["Caballero", "Jose", ""], ["Hajnal", "Joseph V.", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1909.11811", "submitter": "Lin Jiarong", "authors": "Jiarong Lin and Fu Zhang", "title": "A fast, complete, point cloud based loop closure for LiDAR odometry and\n  mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a loop closure method to correct the long-term drift in\nLiDAR odometry and mapping (LOAM). Our proposed method computes the 2D\nhistogram of keyframes, a local map patch, and uses the normalized\ncross-correlation of the 2D histograms as the similarity metric between the\ncurrent keyframe and those in the map. We show that this method is fast,\ninvariant to rotation, and produces reliable and accurate loop detection. The\nproposed method is implemented with careful engineering and integrated into the\nLOAM algorithm, forming a complete and practical system ready to use. To\nbenefit the community by serving a benchmark for loop closure, the entire\nsystem is made open source on Github\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 23:25:23 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Lin", "Jiarong", ""], ["Zhang", "Fu", ""]]}, {"id": "1909.11813", "submitter": "Andrea Dittadi", "authors": "Andrea Dittadi, Ole Winther", "title": "LAVAE: Disentangling Location and Appearance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a probabilistic generative model for unsupervised learning of\nstructured, interpretable, object-based representations of visual scenes. We\nuse amortized variational inference to train the generative model end-to-end.\nThe learned representations of object location and appearance are fully\ndisentangled, and objects are represented independently of each other in the\nlatent space. Unlike previous approaches that disentangle location and\nappearance, ours generalizes seamlessly to scenes with many more objects than\nencountered in the training regime. We evaluate the proposed model on\nmulti-MNIST and multi-dSprites data sets.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 23:33:14 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 00:10:09 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Dittadi", "Andrea", ""], ["Winther", "Ole", ""]]}, {"id": "1909.11825", "submitter": "Yu Sun", "authors": "Yu Sun, Eric Tzeng, Trevor Darrell, Alexei A. Efros", "title": "Unsupervised Domain Adaptation through Self-Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper addresses unsupervised domain adaptation, the setting where\nlabeled training data is available on a source domain, but the goal is to have\ngood performance on a target domain with only unlabeled data. Like much of\nprevious work, we seek to align the learned representations of the source and\ntarget domains while preserving discriminability. The way we accomplish\nalignment is by learning to perform auxiliary self-supervised task(s) on both\ndomains simultaneously. Each self-supervised task brings the two domains closer\ntogether along the direction relevant to that task. Training this jointly with\nthe main task classifier on the source domain is shown to successfully\ngeneralize to the unlabeled target domain. The presented objective is\nstraightforward to implement and easy to optimize. We achieve state-of-the-art\nresults on four out of seven standard benchmarks, and competitive results on\nsegmentation adaptation. We also demonstrate that our method composes well with\nanother popular pixel-level adaptation method.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 00:21:16 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 08:09:29 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Sun", "Yu", ""], ["Tzeng", "Eric", ""], ["Darrell", "Trevor", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1909.11839", "submitter": "Sara Hosseinzadeh Kassani", "authors": "Sara Hosseinzadeh Kassani, Peyman Hosseinzadeh Kassani, Michal J.\n  Wesolowski, Kevin A. Schneider, Ralph Deters", "title": "Breast Cancer Diagnosis with Transfer Learning and Global Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is one of the most common causes of cancer-related death in\nwomen worldwide. Early and accurate diagnosis of breast cancer may\nsignificantly increase the survival rate of patients. In this study, we aim to\ndevelop a fully automatic, deep learning-based, method using descriptor\nfeatures extracted by Deep Convolutional Neural Network (DCNN) models and\npooling operation for the classification of hematoxylin and eosin stain (H&E)\nhistological breast cancer images provided as a part of the International\nConference on Image Analysis and Recognition (ICIAR) 2018 Grand Challenge on\nBreAst Cancer Histology (BACH) Images. Different data augmentation methods are\napplied to optimize the DCNN performance. We also investigated the efficacy of\ndifferent stain normalization methods as a pre-processing step. The proposed\nnetwork architecture using a pre-trained Xception model yields 92.50% average\nclassification accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 01:29:59 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Kassani", "Sara Hosseinzadeh", ""], ["Kassani", "Peyman Hosseinzadeh", ""], ["Wesolowski", "Michal J.", ""], ["Schneider", "Kevin A.", ""], ["Deters", "Ralph", ""]]}, {"id": "1909.11855", "submitter": "Dai Quoc Nguyen", "authors": "Dai Quoc Nguyen and Tu Dinh Nguyen and Dinh Phung", "title": "Universal Graph Transformer Self-Attention Networks", "comments": "We have updated the Pytorch and Tensorflow implementation at:\n  https://github.com/daiquocnguyen/Graph-Transformer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transformer self-attention network has been extensively used in research\ndomains such as computer vision, image processing, and natural language\nprocessing. But it has not been actively used in graph neural networks (GNNs)\nwhere constructing an advanced aggregation function is essential. To this end,\nwe present U2GNN, an effective GNN model leveraging a transformer\nself-attention mechanism followed by a recurrent transition, to induce a\npowerful aggregation function to learn graph representations. Experimental\nresults show that the proposed U2GNN achieves state-of-the-art accuracies on\nwell-known benchmark datasets for graph classification. Our code is available\nat: https://github.com/daiquocnguyen/Graph-Transformer\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 02:39:59 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 13:27:35 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 16:47:35 GMT"}, {"version": "v4", "created": "Sat, 29 Feb 2020 02:05:59 GMT"}, {"version": "v5", "created": "Wed, 8 Apr 2020 15:15:35 GMT"}, {"version": "v6", "created": "Thu, 16 Apr 2020 14:46:21 GMT"}, {"version": "v7", "created": "Mon, 29 Jun 2020 10:15:50 GMT"}, {"version": "v8", "created": "Mon, 3 Aug 2020 15:13:44 GMT"}, {"version": "v9", "created": "Fri, 23 Oct 2020 17:39:40 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Nguyen", "Dai Quoc", ""], ["Nguyen", "Tu Dinh", ""], ["Phung", "Dinh", ""]]}, {"id": "1909.11856", "submitter": "Zheng Hui", "authors": "Zheng Hui, Xinbo Gao, Yunchu Yang, and Xiumei Wang", "title": "Lightweight Image Super-Resolution with Information Multi-distillation\n  Network", "comments": "To be appear in ACM Multimedia 2019, https://github.com/Zheng222/IMDN", "journal-ref": null, "doi": "10.1145/3343031.3351084", "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, single image super-resolution (SISR) methods using deep\nconvolution neural network (CNN) have achieved impressive results. Thanks to\nthe powerful representation capabilities of the deep networks, numerous\nprevious ways can learn the complex non-linear mapping between low-resolution\n(LR) image patches and their high-resolution (HR) versions. However, excessive\nconvolutions will limit the application of super-resolution technology in low\ncomputing power devices. Besides, super-resolution of any arbitrary scale\nfactor is a critical issue in practical applications, which has not been well\nsolved in the previous approaches. To address these issues, we propose a\nlightweight information multi-distillation network (IMDN) by constructing the\ncascaded information multi-distillation blocks (IMDB), which contains\ndistillation and selective fusion parts. Specifically, the distillation module\nextracts hierarchical features step-by-step, and fusion module aggregates them\naccording to the importance of candidate features, which is evaluated by the\nproposed contrast-aware channel attention mechanism. To process real images\nwith any sizes, we develop an adaptive cropping strategy (ACS) to super-resolve\nblock-wise image patches using the same well-trained model. Extensive\nexperiments suggest that the proposed method performs favorably against the\nstate-of-the-art SR algorithms in term of visual quality, memory footprint, and\ninference time. Code is available at \\url{https://github.com/Zheng222/IMDN}.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 02:40:32 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Hui", "Zheng", ""], ["Gao", "Xinbo", ""], ["Yang", "Yunchu", ""], ["Wang", "Xiumei", ""]]}, {"id": "1909.11862", "submitter": "Yi Wang", "authors": "Yi Wang, Zhen-Peng Bian, Junhui Hou and Lap-Pui Chau", "title": "Convolutional Neural Networks with Dynamic Regularization", "comments": "7 pages. Accepted for Publication at IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is commonly used for alleviating overfitting in machine\nlearning. For convolutional neural networks (CNNs), regularization methods,\nsuch as DropBlock and Shake-Shake, have illustrated the improvement in the\ngeneralization performance. However, these methods lack a self-adaptive ability\nthroughout training. That is, the regularization strength is fixed to a\npredefined schedule, and manual adjustments are required to adapt to various\nnetwork architectures. In this paper, we propose a dynamic regularization\nmethod for CNNs. Specifically, we model the regularization strength as a\nfunction of the training loss. According to the change of the training loss,\nour method can dynamically adjust the regularization strength in the training\nprocedure, thereby balancing the underfitting and overfitting of CNNs. With\ndynamic regularization, a large-scale model is automatically regularized by the\nstrong perturbation, and vice versa. Experimental results show that the\nproposed method can improve the generalization capability on off-the-shelf\nnetwork architectures and outperform state-of-the-art regularization methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 03:06:49 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 07:59:04 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2020 03:14:07 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Wang", "Yi", ""], ["Bian", "Zhen-Peng", ""], ["Hou", "Junhui", ""], ["Chau", "Lap-Pui", ""]]}, {"id": "1909.11866", "submitter": "Sara Hosseinzadeh Kassani", "authors": "Sara Hosseinzadeh Kassani, Peyman Hosseinzadeh kassani, Michal J.\n  Wesolowski, Kevin A. Schneider, Ralph Deters", "title": "A Hybrid Deep Learning Architecture for Leukemic B-lymphoblast\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection of leukemic B-lymphoblast cancer in microscopic images is\nvery challenging due to the complicated nature of histopathological structures.\nTo tackle this issue, an automatic and robust diagnostic system is required for\nearly detection and treatment. In this paper, an automated deep learning-based\nmethod is proposed to distinguish between immature leukemic blasts and normal\ncells. The proposed deep learning based hybrid method, which is enriched by\ndifferent data augmentation techniques, is able to extract high-level features\nfrom input images. Results demonstrate that the proposed model yields better\nprediction than individual models for Leukemic B-lymphoblast classification\nwith 96.17% overall accuracy, 95.17% sensitivity and 98.58% specificity. Fusing\nthe features extracted from intermediate layers, our approach has the potential\nto improve the overall classification performance.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 03:34:24 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Kassani", "Sara Hosseinzadeh", ""], ["kassani", "Peyman Hosseinzadeh", ""], ["Wesolowski", "Michal J.", ""], ["Schneider", "Kevin A.", ""], ["Deters", "Ralph", ""]]}, {"id": "1909.11867", "submitter": "Binh Nguyen Xuan", "authors": "Binh D. Nguyen, Thanh-Toan Do, Binh X. Nguyen, Tuong Do, Erman\n  Tjiputra, and Quang D. Tran", "title": "Overcoming Data Limitation in Medical Visual Question Answering", "comments": "Accepted in MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional approaches for Visual Question Answering (VQA) require large\namount of labeled data for training. Unfortunately, such large scale data is\nusually not available for medical domain. In this paper, we propose a novel\nmedical VQA framework that overcomes the labeled data limitation. The proposed\nframework explores the use of the unsupervised Denoising Auto-Encoder (DAE) and\nthe supervised Meta-Learning. The advantage of DAE is to leverage the large\namount of unlabeled images while the advantage of Meta-Learning is to learn\nmeta-weights that quickly adapt to VQA problem with limited labeled data. By\nleveraging the advantages of these techniques, it allows the proposed framework\nto be efficiently trained using a small labeled training set. The experimental\nresults show that our proposed method significantly outperforms the\nstate-of-the-art medical VQA.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 03:40:31 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Nguyen", "Binh D.", ""], ["Do", "Thanh-Toan", ""], ["Nguyen", "Binh X.", ""], ["Do", "Tuong", ""], ["Tjiputra", "Erman", ""], ["Tran", "Quang D.", ""]]}, {"id": "1909.11870", "submitter": "Sara Hosseinzadeh Kassani", "authors": "Sara Hosseinzadeh Kassani, Peyman Hosseinzadeh Kassani, Michal J.\n  Wesolowski, Kevin A. Schneider, Ralph Deters", "title": "Classification of Histopathological Biopsy Images Using Ensemble of Deep\n  Learning Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is one of the leading causes of death across the world in\nwomen. Early diagnosis of this type of cancer is critical for treatment and\npatient care. Computer-aided detection (CAD) systems using convolutional neural\nnetworks (CNN) could assist in the classification of abnormalities. In this\nstudy, we proposed an ensemble deep learning-based approach for automatic\nbinary classification of breast histology images. The proposed ensemble model\nadapts three pre-trained CNNs, namely VGG19, MobileNet, and DenseNet. The\nensemble model is used for the feature representation and extraction steps. The\nextracted features are then fed into a multi-layer perceptron classifier to\ncarry out the classification task. Various pre-processing and CNN tuning\ntechniques such as stain-normalization, data augmentation, hyperparameter\ntuning, and fine-tuning are used to train the model. The proposed method is\nvalidated on four publicly available benchmark datasets, i.e., ICIAR, BreakHis,\nPatchCamelyon, and Bioimaging. The proposed multi-model ensemble method obtains\nbetter predictions than single classifiers and machine learning algorithms with\naccuracies of 98.13%, 95.00%, 94.64% and 83.10% for BreakHis, ICIAR,\nPatchCamelyon and Bioimaging datasets, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 03:57:32 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Kassani", "Sara Hosseinzadeh", ""], ["Kassani", "Peyman Hosseinzadeh", ""], ["Wesolowski", "Michal J.", ""], ["Schneider", "Kevin A.", ""], ["Deters", "Ralph", ""]]}, {"id": "1909.11874", "submitter": "Tuong Do Khanh Long", "authors": "Tuong Do, Thanh-Toan Do, Huy Tran, Erman Tjiputra, Quang D. Tran", "title": "Compact Trilinear Interaction for Visual Question Answering", "comments": "Accepted in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Visual Question Answering (VQA), answers have a great correlation with\nquestion meaning and visual contents. Thus, to selectively utilize image,\nquestion and answer information, we propose a novel trilinear interaction model\nwhich simultaneously learns high level associations between these three inputs.\nIn addition, to overcome the interaction complexity, we introduce a multimodal\ntensor-based PARALIND decomposition which efficiently parameterizes trilinear\ninteraction between the three inputs. Moreover, knowledge distillation is first\ntime applied in Free-form Opened-ended VQA. It is not only for reducing the\ncomputational cost and required memory but also for transferring knowledge from\ntrilinear interaction model to bilinear interaction model. The extensive\nexperiments on benchmarking datasets TDIUC, VQA-2.0, and Visual7W show that the\nproposed compact trilinear interaction model achieves state-of-the-art results\nwhen using a single model on all three datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 04:02:38 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Do", "Tuong", ""], ["Do", "Thanh-Toan", ""], ["Tran", "Huy", ""], ["Tjiputra", "Erman", ""], ["Tran", "Quang D.", ""]]}, {"id": "1909.11888", "submitter": "Shin-Fang Chng", "authors": "Shin-Fang Ch'ng, Naoya Sogi, Pulak Purkait, Tat-Jun Chin and Kazuhiro\n  Fukui", "title": "Resolving Marker Pose Ambiguity by Robust Rotation Averaging with Clique\n  Constraints", "comments": "7 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planar markers are useful in robotics and computer vision for mapping and\nlocalisation. Given a detected marker in an image, a frequent task is to\nestimate the 6DOF pose of the marker relative to the camera, which is an\ninstance of planar pose estimation (PPE). Although there are mature techniques,\nPPE suffers from a fundamental ambiguity problem, in that there can be more\nthan one plausible pose solutions for a PPE instance. Especially when\nlocalisation of the marker corners is noisy, it is often difficult to\ndisambiguate the pose solutions based on reprojection error alone. Previous\nmethods choose between the possible solutions using a heuristic criteria, or\nsimply ignore ambiguous markers.\n  We propose to resolve the ambiguities by examining the consistencies of a set\nof markers across multiple views. Our specific contributions include a novel\nrotation averaging formulation that incorporates long-range dependencies\nbetween possible marker orientation solutions that arise from PPE ambiguities.\nWe analyse the combinatorial complexity of the problem, and develop a novel\nlifted algorithm to effectively resolve marker pose ambiguities, without\ndiscarding any marker observations. Results on real and synthetic data show\nthat our method is able to handle highly ambiguous inputs, and provides more\naccurate and/or complete marker-based mapping and localisation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 04:44:16 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Ch'ng", "Shin-Fang", ""], ["Sogi", "Naoya", ""], ["Purkait", "Pulak", ""], ["Chin", "Tat-Jun", ""], ["Fukui", "Kazuhiro", ""]]}, {"id": "1909.11895", "submitter": "Sifei Liu", "authors": "Xueting Li, Sifei Liu, Shalini De Mello, Xiaolong Wang, Jan Kautz,\n  Ming-Hsuan Yang", "title": "Joint-task Self-supervised Learning for Temporal Correspondence", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to learn reliable dense correspondence from videos in a\nself-supervised manner. Our learning process integrates two highly related\ntasks: tracking large image regions \\emph{and} establishing fine-grained\npixel-level associations between consecutive video frames. We exploit the\nsynergy between both tasks through a shared inter-frame affinity matrix, which\nsimultaneously models transitions between video frames at both the region- and\npixel-levels. While region-level localization helps reduce ambiguities in\nfine-grained matching by narrowing down search regions; fine-grained matching\nprovides bottom-up features to facilitate region-level localization. Our method\noutperforms the state-of-the-art self-supervised methods on a variety of visual\ncorrespondence tasks, including video-object and part-segmentation propagation,\nkeypoint tracking, and object tracking. Our self-supervised method even\nsurpasses the fully-supervised affinity feature representation obtained from a\nResNet-18 pre-trained on the ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 05:11:26 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Li", "Xueting", ""], ["Liu", "Sifei", ""], ["De Mello", "Shalini", ""], ["Wang", "Xiaolong", ""], ["Kautz", "Jan", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1909.11902", "submitter": "Jie Song", "authors": "Jie Song, Yixin Chen, Xinchao Wang, Chengchao Shen, Mingli Song", "title": "Deep Model Transferability from Attribution Maps", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploring the transferability between heterogeneous tasks sheds light on\ntheir intrinsic interconnections, and consequently enables knowledge transfer\nfrom one task to another so as to reduce the training effort of the latter. In\nthis paper, we propose an embarrassingly simple yet very efficacious approach\nto estimating the transferability of deep networks, especially those handling\nvision tasks. Unlike the seminal work of taskonomy that relies on a large\nnumber of annotations as supervision and is thus computationally cumbersome,\nthe proposed approach requires no human annotations and imposes no constraints\non the architectures of the networks. This is achieved, specifically, via\nprojecting deep networks into a model space, wherein each network is treated as\na point and the distances between two points are measured by deviations of\ntheir produced attribution maps. The proposed approach is several-magnitude\ntimes faster than taskonomy, and meanwhile preserves a task-wise topological\nstructure highly similar to the one obtained by taskonomy. Code is available at\nhttps://github.com/zju-vipa/TransferbilityFromAttributionMaps.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 05:36:38 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 06:21:00 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Song", "Jie", ""], ["Chen", "Yixin", ""], ["Wang", "Xinchao", ""], ["Shen", "Chengchao", ""], ["Song", "Mingli", ""]]}, {"id": "1909.11903", "submitter": "Catalin Stoean", "authors": "Ruxandra Stoean, Dominic Iliescu, Catalin Stoean", "title": "Segmentation of points of interest during fetal cardiac assesment in the\n  first trimester from color Doppler ultrasound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper puts forward an incipient study that uses a traditional\nsegmentation method based on Zernike moments for extracting significant\nfeatures from frames of fetal echocardiograms from first trimester color\nDoppler examinations. A distance based approach is then used on the obtained\nindicators to classify frames of three given categories that should be present\nin a normal heart condition. The computational tool shows promise in supporting\nthe obstetrician in a rapid recognition of heart views during screening.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 05:36:58 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Stoean", "Ruxandra", ""], ["Iliescu", "Dominic", ""], ["Stoean", "Catalin", ""]]}, {"id": "1909.11915", "submitter": "Haseeb Nazki", "authors": "Haseeb Nazki, Sook Yoon, Alvaro Fuentes, Dong Sun Park", "title": "Unsupervised Image Translation using Adversarial Networks for Improved\n  Plant Disease Recognition", "comments": "20 pages, 11 figures, 3 tables, article under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquisition of data in task-specific applications of machine learning like\nplant disease recognition is a costly endeavor owing to the requirements of\nprofessional human diligence and time constraints. In this paper, we present a\nsimple pipeline that uses GANs in an unsupervised image translation environment\nto improve learning with respect to the data distribution in a plant disease\ndataset, reducing the partiality introduced by acute class imbalance and hence\nshifting the classification decision boundary towards better performance. The\nempirical analysis of our method is demonstrated on a limited dataset of 2789\ntomato plant disease images, highly corrupted with an imbalance in the 9\ndisease categories. First, we extend the state of the art for the GAN-based\nimage-to-image translation method by enhancing the perceptual quality of the\ngenerated images and preserving the semantics. We introduce AR-GAN, where in\naddition to the adversarial loss, our synthetic image generator optimizes on\nActivation Reconstruction loss (ARL) function that optimizes feature\nactivations against the natural image. We present visually more compelling\nsynthetic images in comparison to most prominent existing models and evaluate\nthe performance of our GAN framework in terms of various datasets and metrics.\nSecond, we evaluate the performance of a baseline convolutional neural network\nclassifier for improved recognition using the resulting synthetic samples to\naugment our training set and compare it with the classical data augmentation\nscheme. We observe a significant improvement in classification accuracy (+5.2%)\nusing generated synthetic samples as compared to (+0.8%) increase using classic\naugmentation in an equal class distribution environment.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 06:01:42 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Nazki", "Haseeb", ""], ["Yoon", "Sook", ""], ["Fuentes", "Alvaro", ""], ["Park", "Dong Sun", ""]]}, {"id": "1909.11926", "submitter": "Guilin Li", "authors": "Guilin Li, Xing Zhang, Zitong Wang, Matthias Tan, Jiashi Feng, Zhenguo\n  Li, Tong Zhang", "title": "Hierarchical Neural Architecture Search via Operator Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, the efficiency of automatic neural architecture design has been\nsignificantly improved by gradient-based search methods such as DARTS. However,\nrecent literature has brought doubt to the generalization ability of DARTS,\narguing that DARTS performs poorly when the search space is changed, i.e, when\ndifferent set of candidate operators are used. Regularization techniques such\nas early stopping have been proposed to partially solve this problem. In this\npaper, we tackle this problem from a different perspective by identifying two\ncontributing factors to the collapse of DARTS when the search space changes:\n(1) the correlation of similar operators incurs unfavorable competition among\nthem and makes their relative importance score unreliable and (2) the\noptimization complexity gap between the proxy search stage and the final\ntraining. Based on these findings, we propose a new hierarchical search\nalgorithm. With its operator clustering and optimization complexity match, the\nalgorithm can consistently find high-performance architecture across various\nsearch spaces. For all the five variants of the popular cell-based search\nspaces, the proposed algorithm always obtains state-of-the-art architecture\nwith best accuracy on the CIFAR-10, CIFAR-100 and ImageNet over other\nwell-established DARTS-alike algorithms. Code is available at\nhttps://github.com/susan0199/StacNAS.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 06:26:58 GMT"}, {"version": "v2", "created": "Sat, 28 Sep 2019 15:29:40 GMT"}, {"version": "v3", "created": "Tue, 1 Oct 2019 03:20:17 GMT"}, {"version": "v4", "created": "Wed, 11 Dec 2019 03:53:31 GMT"}, {"version": "v5", "created": "Mon, 25 Jan 2021 10:03:07 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Li", "Guilin", ""], ["Zhang", "Xing", ""], ["Wang", "Zitong", ""], ["Tan", "Matthias", ""], ["Feng", "Jiashi", ""], ["Li", "Zhenguo", ""], ["Zhang", "Tong", ""]]}, {"id": "1909.11932", "submitter": "Md Sazzad Hossain", "authors": "Md Sazzad Hossain, Andrew P Paplinski, John M Betts", "title": "Adaptive Class Weight based Dual Focal Loss for Improved Semantic\n  Segmentation", "comments": "We, the authors, are withdrawing this preprint due to a number of\n  errors pointed out by the reviewers. Based on the reviewers' feedback, the\n  paper has gone through an extensive revision, which significantly differs\n  from this preprint version by methodologically as well as experimentally. We\n  acknowledge the reviewers for their scrutinized review which guided our study\n  in the right direction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Dual Focal Loss (DFL) function, as a replacement\nfor the standard cross entropy (CE) function to achieve a better treatment of\nthe unbalanced classes in a dataset. Our DFL method is an improvement on the\nrecently reported Focal Loss (FL) cross-entropy function, which proposes a\nscaling method that puts more weight on the examples that are difficult to\nclassify over those that are easy. However, the scaling parameter of FL is\nempirically set, which is problem-dependent. In addition, like other CE\nvariants, FL only focuses on the loss of true classes. Therefore, no loss\nfeedback is gained from the false classes. Although focusing only on true\nexamples increases probability on true classes and correspondingly reduces\nprobability on false classes due to the nature of the softmax function, it does\nnot achieve the best convergence due to avoidance of the loss on false classes.\nOur DFL method improves on the simple FL in two ways. Firstly, it takes the\nidea of FL to focus more on difficult examples than the easy ones, but\nevaluates loss on both true and negative classes with equal importance.\nSecondly, the scaling parameter of DFL has been made learnable so that it can\ntune itself by backpropagation rather than being dependent on manual tuning. In\nthis way, our proposed DFL method offers an auto-tunable loss function that can\nreduce the class imbalance effect as well as put more focus on both true\ndifficult examples and negative easy examples.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 06:36:21 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2019 13:54:05 GMT"}, {"version": "v3", "created": "Thu, 26 Nov 2020 05:20:15 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Hossain", "Md Sazzad", ""], ["Paplinski", "Andrew P", ""], ["Betts", "John M", ""]]}, {"id": "1909.11936", "submitter": "Yukun Zhou", "authors": "Yukun Zhou, Zailiang Chen, Hailan Shen, Xianxian Zheng, Rongchang\n  Zhao, Xuanchu Duan", "title": "A Refined Equilibrium Generative Adversarial Network for Retinal Vessel\n  Segmentation", "comments": "12 pages, 8 figures, and 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Recognizing retinal vessel abnormity is vital to early diagnosis\nof ophthalmological diseases and cardiovascular events. However, segmentation\nresults are highly influenced by elusive vessels, especially in low-contrast\nbackground and lesion region. In this work, we present an end-to-end synthetic\nneural network, containing a symmetric equilibrium generative adversarial\nnetwork (SEGAN), multi-scale features refine blocks (MSFRB), and attention\nmechanism (AM) to enhance the performance on vessel segmentation. Method: The\nproposed network is granted powerful multi-scale representation capability to\nextract detail information. First, SEGAN constructs a symmetric adversarial\narchitecture, which forces generator to produce more realistic images with\nlocal details. Second, MSFRB are devised to prevent high-resolution features\nfrom being obscured, thereby merging multi-scale features better. Finally, the\nAM is employed to encourage the network to concentrate on discriminative\nfeatures. Results: On public dataset DRIVE, STARE, CHASEDB1, and HRF, we\nevaluate our network quantitatively and compare it with state-of-the-art works.\nThe ablation experiment shows that SEGAN, MSFRB, and AM both contribute to the\ndesirable performance. Conclusion: The proposed network outperforms the mature\nmethods and effectively functions in elusive vessels segmentation, achieving\nhighest scores in Sensitivity, G-Mean, Precision, and F1-Score while\nmaintaining the top level in other metrics. Significance: The appreciable\nperformance and computational efficiency offer great potential in clinical\nretinal vessel segmentation application. Meanwhile, the network could be\nutilized to extract detail information in other biomedical issues\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 06:50:03 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 13:09:50 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Zhou", "Yukun", ""], ["Chen", "Zailiang", ""], ["Shen", "Hailan", ""], ["Zheng", "Xianxian", ""], ["Zhao", "Rongchang", ""], ["Duan", "Xuanchu", ""]]}, {"id": "1909.11937", "submitter": "Huapeng Wu", "authors": "Huapeng Wu, Zhengxia Zou, Jie Gui, Wen-Jun Zeng, Jieping Ye, Jun\n  Zhang, Hongyi Liu, Zhihui Wei", "title": "Multi-grained Attention Networks for Single Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNN) have drawn great attention in image\nsuper-resolution (SR). Recently, visual attention mechanism, which exploits\nboth of the feature importance and contextual cues, has been introduced to\nimage SR and proves to be effective to improve CNN-based SR performance. In\nthis paper, we make a thorough investigation on the attention mechanisms in a\nSR model and shed light on how simple and effective improvements on these ideas\nimprove the state-of-the-arts. We further propose a unified approach called\n\"multi-grained attention networks (MGAN)\" which fully exploits the advantages\nof multi-scale and attention mechanisms in SR tasks. In our method, the\nimportance of each neuron is computed according to its surrounding regions in a\nmulti-grained fashion and then is used to adaptively re-scale the feature\nresponses. More importantly, the \"channel attention\" and \"spatial attention\"\nstrategies in previous methods can be essentially considered as two special\ncases of our method. We also introduce multi-scale dense connections to extract\nthe image features at multiple scales and capture the features of different\nlayers through dense skip connections. Ablation studies on benchmark datasets\ndemonstrate the effectiveness of our method. In comparison with other\nstate-of-the-art SR methods, our method shows the superiority in terms of both\naccuracy and model size.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 06:54:00 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 09:09:30 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Wu", "Huapeng", ""], ["Zou", "Zhengxia", ""], ["Gui", "Jie", ""], ["Zeng", "Wen-Jun", ""], ["Ye", "Jieping", ""], ["Zhang", "Jun", ""], ["Liu", "Hongyi", ""], ["Wei", "Zhihui", ""]]}, {"id": "1909.11944", "submitter": "Olly Styles", "authors": "Olly Styles, Tanaya Guha, Victor Sanchez", "title": "Multiple Object Forecasting: Predicting Future Object Locations in\n  Diverse Environments", "comments": "WACV 2020. Code & dataset:\n  https://github.com/olly-styles/Multiple-Object-Forecasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces the problem of multiple object forecasting (MOF), in\nwhich the goal is to predict future bounding boxes of tracked objects. In\ncontrast to existing works on object trajectory forecasting which primarily\nconsider the problem from a birds-eye perspective, we formulate the problem\nfrom an object-level perspective and call for the prediction of full object\nbounding boxes, rather than trajectories alone. Towards solving this task, we\nintroduce the Citywalks dataset, which consists of over 200k high-resolution\nvideo frames. Citywalks comprises of footage recorded in 21 cities from 10\nEuropean countries in a variety of weather conditions and over 3.5k unique\npedestrian trajectories. For evaluation, we adapt existing trajectory\nforecasting methods for MOF and confirm cross-dataset generalizability on the\nMOT-17 dataset without fine-tuning. Finally, we present STED, a novel\nencoder-decoder architecture for MOF. STED combines visual and temporal\nfeatures to model both object-motion and ego-motion, and outperforms existing\napproaches for MOF. Code & dataset link:\nhttps://github.com/olly-styles/Multiple-Object-Forecasting\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 07:11:50 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 12:19:53 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Styles", "Olly", ""], ["Guha", "Tanaya", ""], ["Sanchez", "Victor", ""]]}, {"id": "1909.11946", "submitter": "Palakorn Achananuparp", "authors": "Doyen Sahoo, Wang Hao, Shu Ke, Wu Xiongwei, Hung Le, Palakorn\n  Achananuparp, Ee-Peng Lim, Steven C. H. Hoi", "title": "FoodAI: Food Image Recognition via Deep Learning for Smart Food Logging", "comments": "Published at KDD 2019 (Applied Data Science track). Demo is\n  accessible at https://foodai.org/", "journal-ref": null, "doi": "10.1145/3292500.3330734", "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important aspect of health monitoring is effective logging of food\nconsumption. This can help management of diet-related diseases like obesity,\ndiabetes, and even cardiovascular diseases. Moreover, food logging can help\nfitness enthusiasts, and people who wanting to achieve a target weight.\nHowever, food-logging is cumbersome, and requires not only taking additional\neffort to note down the food item consumed regularly, but also sufficient\nknowledge of the food item consumed (which is difficult due to the availability\nof a wide variety of cuisines). With increasing reliance on smart devices, we\nexploit the convenience offered through the use of smart phones and propose a\nsmart-food logging system: FoodAI, which offers state-of-the-art deep-learning\nbased image recognition capabilities. FoodAI has been developed in Singapore\nand is particularly focused on food items commonly consumed in Singapore.\nFoodAI models were trained on a corpus of 400,000 food images from 756\ndifferent classes. In this paper we present extensive analysis and insights\ninto the development of this system. FoodAI has been deployed as an API service\nand is one of the components powering Healthy 365, a mobile app developed by\nSingapore's Heath Promotion Board. We have over 100 registered organizations\n(universities, companies, start-ups) subscribing to this service and actively\nreceive several API requests a day. FoodAI has made food logging convenient,\naiding smart consumption and a healthy lifestyle.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 07:15:46 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Sahoo", "Doyen", ""], ["Hao", "Wang", ""], ["Ke", "Shu", ""], ["Xiongwei", "Wu", ""], ["Le", "Hung", ""], ["Achananuparp", "Palakorn", ""], ["Lim", "Ee-Peng", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "1909.11947", "submitter": "Xi Cheng", "authors": "Xi Cheng, Zhenyong Fu, Jian Yang", "title": "Multi-scale Dynamic Feature Encoding Network for Image Demoireing", "comments": "Accepted in Advances in Image Manipulation workshop and challenges at\n  ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The prevalence of digital sensors, such as digital cameras and mobile phones,\nsimplifies the acquisition of photos. Digital sensors, however, suffer from\nproducing Moire when photographing objects having complex textures, which\ndeteriorates the quality of photos. Moire spreads across various frequency\nbands of images and is a dynamic texture with varying colors and shapes, which\npose two main challenges in demoireing---an important task in image\nrestoration. In this paper, towards addressing the first challenge, we design a\nmulti-scale network to process images at different spatial resolutions,\nobtaining features in different frequency bands, and thus our method can\njointly remove moire in different frequency bands. Towards solving the second\nchallenge, we propose a dynamic feature encoding module (DFE), embedded in each\nscale, for dynamic texture. Moire pattern can be eliminated more effectively\nvia DFE.Our proposed method, termed Multi-scale convolutional network with\nDynamic feature encoding for image DeMoireing (MDDM), can outperform the state\nof the arts in fidelity as well as perceptual on benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 07:23:43 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Cheng", "Xi", ""], ["Fu", "Zhenyong", ""], ["Yang", "Jian", ""]]}, {"id": "1909.11953", "submitter": "Sheng Wan", "authors": "Sheng Wan and Chen Gong and Ping Zhong and Shirui Pan and Guangyu Li\n  and Jian Yang", "title": "Hyperspectral Image Classification With Context-Aware Dynamic Graph\n  Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In hyperspectral image (HSI) classification, spatial context has demonstrated\nits significance in achieving promising performance. However, conventional\nspatial context-based methods simply assume that spatially neighboring pixels\nshould correspond to the same land-cover class, so they often fail to correctly\ndiscover the contextual relations among pixels in complex situations, and thus\nleading to imperfect classification results on some irregular or inhomogeneous\nregions such as class boundaries. To address this deficiency, we develop a new\nHSI classification method based on the recently proposed Graph Convolutional\nNetwork (GCN), as it can flexibly encode the relations among arbitrarily\nstructured non-Euclidean data. Different from traditional GCN, there are two\nnovel strategies adopted by our method to further exploit the contextual\nrelations for accurate HSI classification. First, since the receptive field of\ntraditional GCN is often limited to fairly small neighborhood, we proposed to\ncapture long range contextual relations in HSI by performing successive graph\nconvolutions on a learned region-induced graph which is transformed from the\noriginal 2D image grids. Second, we refine the graph edge weight and the\nconnective relationships among image regions by learning the improved adjacency\nmatrix and the 'edge filter', so that the graph can be gradually refined to\nadapt to the representations generated by each graph convolutional layer. Such\nupdated graph will in turn result in accurate region representations, and vice\nversa. The experiments carried out on three real-world benchmark datasets\ndemonstrate that the proposed method yields significant improvement in the\nclassification performance when compared with some state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 07:37:37 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Wan", "Sheng", ""], ["Gong", "Chen", ""], ["Zhong", "Ping", ""], ["Pan", "Shirui", ""], ["Li", "Guangyu", ""], ["Yang", "Jian", ""]]}, {"id": "1909.11966", "submitter": "Weilin Huang", "authors": "Xiaojun Hu and Miao Kang and Weilin Huang and Matthew R. Scott and\n  Roland Wiest and Mauricio Reyes", "title": "Dual-Stream Pyramid Registration Network", "comments": "To appear in MICCAI 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Dual-Stream Pyramid Registration Network (referred as\nDual-PRNet) for unsupervised 3D medical image registration. Unlike recent\nCNN-based registration approaches, such as VoxelMorph, which explores a\nsingle-stream encoder-decoder network to compute a registration fields from a\npair of 3D volumes, we design a two-stream architecture able to compute\nmulti-scale registration fields from convolutional feature pyramids. Our\ncontributions are two-fold: (i) we design a two-stream 3D encoder-decoder\nnetwork which computes two convolutional feature pyramids separately for a pair\nof input volumes, resulting in strong deep representations that are meaningful\nfor deformation estimation; (ii) we propose a pyramid registration module able\nto predict multi-scale registration fields directly from the decoding feature\npyramids. This allows it to refine the registration fields gradually in a\ncoarse-to-fine manner via sequential warping, and enable the model with the\ncapability for handling significant deformations between two volumes, such as\nlarge displacements in spatial domain or slice space. The proposed Dual-PRNet\nis evaluated on two standard benchmarks for brain MRI registration, where it\noutperforms the state-of-the-art approaches by a large margin, e.g., having\nimprovements over recent VoxelMorph [2] with 0.683->0.778 on the LPBA40, and\n0.511->0.631 on the Mindboggle101, in term of average Dice score.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 08:17:01 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Hu", "Xiaojun", ""], ["Kang", "Miao", ""], ["Huang", "Weilin", ""], ["Scott", "Matthew R.", ""], ["Wiest", "Roland", ""], ["Reyes", "Mauricio", ""]]}, {"id": "1909.11972", "submitter": "Woo-Han Yun", "authors": "Woo-han Yun, Taewoo Kim, Jaeyeon Lee, Jaehong Kim, Junmo Kim", "title": "Cut-and-Paste Dataset Generation for Balancing Domain Gaps in Object\n  Instance Detection", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3051964", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Training an object instance detector where only a few training object images\nare available is a challenging task. One solution is a cut-and-paste method\nthat generates a training dataset by cutting object areas out of training\nimages and pasting them onto other background images. A detector trained on a\ndataset generated with a cut-and-paste method suffers from the conventional\ndomain shift problem, which stems from a discrepancy between the source domain\n(generated training dataset) and the target domain (real test dataset). Though\nstate-of-the-art domain adaptation methods are able to reduce this gap, it is\nlimited because they do not consider the difference of domain gaps of\nforeground and background. In this study, we present that the conventional\ndomain gap can be divided into two sub-domain gaps for foreground and\nbackground. Then, we show that the original cut-and-paste approach suffers from\na new domain gap problem, an unbalanced domain gaps, because it has two\nseparate source domains for foreground and background, unlike the conventional\ndomain shift problem. Then, we introduce an advanced cut-and-paste method to\nbalance the unbalanced domain gaps by diversifying the foreground with GAN\n(generative adversarial network)-generated seed images and simplifying the\nbackground using image processing techniques. Experimental results show that\nour method is effective for balancing domain gaps and improving the accuracy of\nobject instance detection in a cluttered indoor environment using only a few\nseed images. Furthermore, we show that balancing domain gaps can improve the\ndetection accuracy of state-of-the-art domain adaptation methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 08:30:36 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 03:23:28 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Yun", "Woo-han", ""], ["Kim", "Taewoo", ""], ["Lee", "Jaeyeon", ""], ["Kim", "Jaehong", ""], ["Kim", "Junmo", ""]]}, {"id": "1909.11975", "submitter": "Jianwen Xie", "authors": "Jianwen Xie, Song-Chun Zhu, Ying Nian Wu", "title": "Learning Energy-based Spatial-Temporal Generative ConvNets for Dynamic\n  Patterns", "comments": null, "journal-ref": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n  2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video sequences contain rich dynamic patterns, such as dynamic texture\npatterns that exhibit stationarity in the temporal domain, and action patterns\nthat are non-stationary in either spatial or temporal domain. We show that an\nenergy-based spatial-temporal generative ConvNet can be used to model and\nsynthesize dynamic patterns. The model defines a probability distribution on\nthe video sequence, and the log probability is defined by a spatial-temporal\nConvNet that consists of multiple layers of spatial-temporal filters to capture\nspatial-temporal patterns of different scales. The model can be learned from\nthe training video sequences by an \"analysis by synthesis\" learning algorithm\nthat iterates the following two steps. Step 1 synthesizes video sequences from\nthe currently learned model. Step 2 then updates the model parameters based on\nthe difference between the synthesized video sequences and the observed\ntraining sequences. We show that the learning algorithm can synthesize\nrealistic dynamic patterns. We also show that it is possible to learn the model\nfrom incomplete training sequences with either occluded pixels or missing\nframes, so that model learning and pattern completion can be accomplished\nsimultaneously.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 08:36:15 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Xie", "Jianwen", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1909.11983", "submitter": "Qingbo Wu", "authors": "Qingbo Wu and Lei Wang and King N. Ngan and Hongliang Li and Fanman\n  Meng and Linfeng Xu", "title": "Subjective and Objective De-raining Quality Assessment Towards Authentic\n  Rain Image", "comments": "In this revision, we add the comparison with our previous exploration\n  towards the de-raining quality assessment in Ref. [16]. Some typos in Tables\n  III and IV are corrected, where the missed minus signs are added back for\n  some OU metrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images acquired by outdoor vision systems easily suffer poor visibility and\nannoying interference due to the rainy weather, which brings great challenge\nfor accurately understanding and describing the visual contents. Recent\nresearches have devoted great efforts on the task of rain removal for improving\nthe image visibility. However, there is very few exploration about the quality\nassessment of de-rained image, even it is crucial for accurately measuring the\nperformance of various de-raining algorithms. In this paper, we first create a\nde-raining quality assessment (DQA) database that collects 206 authentic rain\nimages and their de-rained versions produced by 6 representative single image\nrain removal algorithms. Then, a subjective study is conducted on our DQA\ndatabase, which collects the subject-rated scores of all de-rained images. To\nquantitatively measure the quality of de-rained image with non-uniform\nartifacts, we propose a bi-directional feature embedding network (B-FEN) which\nintegrates the features of global perception and local difference together.\nExperiments confirm that the proposed method significantly outperforms many\nexisting universal blind image quality assessment models. To help the research\ntowards perceptually preferred de-raining algorithm, we will publicly release\nour DQA database and B-FEN source code on https://github.com/wqb-uestc.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 08:57:34 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 04:56:02 GMT"}, {"version": "v3", "created": "Sun, 6 Oct 2019 03:55:12 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Wu", "Qingbo", ""], ["Wang", "Lei", ""], ["Ngan", "King N.", ""], ["Li", "Hongliang", ""], ["Meng", "Fanman", ""], ["Xu", "Linfeng", ""]]}, {"id": "1909.11995", "submitter": "Dominik Klein", "authors": "Dominik Klein", "title": "The Stroke Correspondence Problem, Revisited", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We revisit the stroke correspondence problem [13,14]. We optimize this\nalgorithm by 1) evaluating suitable preprocessing (normalization) methods 2)\nextending the algorithm with an additional distance measure to handle Hiragana,\nKatakana and Kanji characters with a low number of strokes and c) simplify the\nstroke linking algorithms. Our contributions are implemented in the free,\nopen-source library ctegaki and in the demo-tools jTegaki and Kanjicanvas.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 09:26:12 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 11:04:47 GMT"}, {"version": "v3", "created": "Sat, 6 Mar 2021 22:25:53 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Klein", "Dominik", ""]]}, {"id": "1909.12000", "submitter": "Fabien Baradel", "authors": "Fabien Baradel, Natalia Neverova, Julien Mille, Greg Mori, Christian\n  Wolf", "title": "CoPhy: Counterfactual Learning of Physical Dynamics", "comments": "ICLR 2020 -Spotlight presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding causes and effects in mechanical systems is an essential\ncomponent of reasoning in the physical world. This work poses a new problem of\ncounterfactual learning of object mechanics from visual input. We develop the\nCoPhy benchmark to assess the capacity of the state-of-the-art models for\ncausal physical reasoning in a synthetic 3D environment and propose a model for\nlearning the physical dynamics in a counterfactual setting. Having observed a\nmechanical experiment that involves, for example, a falling tower of blocks, a\nset of bouncing balls or colliding objects, we learn to predict how its outcome\nis affected by an arbitrary intervention on its initial conditions, such as\ndisplacing one of the objects in the scene. The alternative future is predicted\ngiven the altered past and a latent representation of the confounders learned\nby the model in an end-to-end fashion with no supervision. We compare against\nfeedforward video prediction baselines and show how observing alternative\nexperiences allows the network to capture latent physical properties of the\nenvironment, which results in significantly more accurate predictions at the\nlevel of super human performance.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 09:34:48 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 12:48:26 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Baradel", "Fabien", ""], ["Neverova", "Natalia", ""], ["Mille", "Julien", ""], ["Mori", "Greg", ""], ["Wolf", "Christian", ""]]}, {"id": "1909.12034", "submitter": "Thomas Probst", "authors": "Thomas Probst, Danda Pani Paudel, Ajad Chhatkuli, Luc Van Gool", "title": "Convex Relaxations for Consensus and Non-Minimal Problems in 3D Vision", "comments": "Accepted to ICCV'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we formulate a generic non-minimal solver using the existing\ntools of Polynomials Optimization Problems (POP) from computational algebraic\ngeometry. The proposed method exploits the well known Shor's or Lasserre's\nrelaxations, whose theoretical aspects are also discussed. Notably, we further\nexploit the POP formulation of non-minimal solver also for the generic\nconsensus maximization problems in 3D vision. Our framework is simple and\nstraightforward to implement, which is also supported by three diverse\napplications in 3D vision, namely rigid body transformation estimation,\nNon-Rigid Structure-from-Motion (NRSfM), and camera autocalibration. In all\nthree cases, both non-minimal and consensus maximization are tested, which are\nalso compared against the state-of-the-art methods. Our results are competitive\nto the compared methods, and are also coherent with our theoretical analysis.\nThe main contribution of this paper is the claim that a good approximate\nsolution for many polynomial problems involved in 3D vision can be obtained\nusing the existing theory of numerical computational algebra. This claim leads\nus to reason about why many relaxed methods in 3D vision behave so well? And\nalso allows us to offer a generic relaxed solver in a rather straightforward\nway. We further show that the convex relaxation of these polynomials can easily\nbe used for maximizing consensus in a deterministic manner. We support our\nclaim using several experiments for aforementioned three diverse problems in 3D\nvision.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 11:32:02 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Probst", "Thomas", ""], ["Paudel", "Danda Pani", ""], ["Chhatkuli", "Ajad", ""], ["Van Gool", "Luc", ""]]}, {"id": "1909.12037", "submitter": "Jianqiang Wang", "authors": "Jianqiang Wang, Hao Zhu, Zhan Ma, Tong Chen, Haojie Liu, Qiu Shen", "title": "Learned Point Cloud Geometry Compression", "comments": "13 pages", "journal-ref": null, "doi": "10.1109/TCSVT.2021.3051377", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel end-to-end Learned Point Cloud Geometry\nCompression (a.k.a., Learned-PCGC) framework, to efficiently compress the point\ncloud geometry (PCG) using deep neural networks (DNN) based variational\nautoencoders (VAE). In our approach, PCG is first voxelized, scaled and\npartitioned into non-overlapped 3D cubes, which is then fed into stacked 3D\nconvolutions for compact latent feature and hyperprior generation. Hyperpriors\nare used to improve the conditional probability modeling of latent features. A\nweighted binary cross-entropy (WBCE) loss is applied in training while an\nadaptive thresholding is used in inference to remove unnecessary voxels and\nreduce the distortion. Objectively, our method exceeds the geometry-based point\ncloud compression (G-PCC) algorithm standardized by well-known Moving Picture\nExperts Group (MPEG) with a significant performance margin, e.g., at least 60%\nBD-Rate (Bjontegaard Delta Rate) gains, using common test datasets.\nSubjectively, our method has presented better visual quality with smoother\nsurface reconstruction and appealing details, in comparison to all existing\nMPEG standard compliant PCC methods. Our method requires about 2.5MB parameters\nin total, which is a fairly small size for practical implementation, even on\nembedded platform. Additional ablation studies analyze a variety of aspects\n(e.g., cube size, kernels, etc) to explore the application potentials of our\nlearned-PCGC.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 11:40:50 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Jianqiang", ""], ["Zhu", "Hao", ""], ["Ma", "Zhan", ""], ["Chen", "Tong", ""], ["Liu", "Haojie", ""], ["Shen", "Qiu", ""]]}, {"id": "1909.12047", "submitter": "Max Argus", "authors": "Max Argus, Cornelia Schaefer-Prokop, David A. Lynch, Bram van Ginneken", "title": "Function Follows Form: Regression from Complete Thoracic Computed\n  Tomography Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chronic Obstructive Pulmonary Disease (COPD) is a leading cause of morbidity\nand mortality. While COPD diagnosis is based on lung function tests, early\nstages and progression of different aspects of the disease can be visible and\nquantitatively assessed on computed tomography (CT) scans. Many studies have\nbeen published that quantify imaging biomarkers related to COPD. In this paper\nwe present a convolutional neural network that directly computes visual\nemphysema scores and predicts the outcome of lung function tests for 195 CT\nscans from the COPDGene study. Contrary to previous work, the proposed method\ndoes not encode any specific prior knowledge about what to quantify, but it is\ntrained end-to-end with a set of 1424 CT scans for which the output parameters\nwere available. The network provided state-of-the-art results for these tasks:\nVisual emphysema scores are comparable to those assessed by trained human\nobservers; COPD diagnosis from estimated lung function reaches an area under\nthe ROC curve of 0.94, outperforming prior art. The method is easily\ngeneralizable to other situations where information from whole scans needs to\nbe summarized in single quantities.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 12:27:52 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 07:13:36 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Argus", "Max", ""], ["Schaefer-Prokop", "Cornelia", ""], ["Lynch", "David A.", ""], ["van Ginneken", "Bram", ""]]}, {"id": "1909.12083", "submitter": "Marco Cristoforetti", "authors": "L. Coviello, M. Cristoforetti, G. Jurman and C. Furlanello", "title": "In-field grape berries counting for yield estimation using dilated CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital technologies ignited a revolution in the agrifood domain known as\nprecision agriculture: a main question for enabling precision agriculture at\nscale is if accurate product quality control can be made available at minimal\ncost, leveraging existing technologies and agronomists' skills. As a\ncontribution along this direction we demonstrate a tool for accurate fruit\nyield estimation from smartphone cameras, by adapting Deep Learning algorithms\noriginally developed for crowd counting.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 13:28:53 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Coviello", "L.", ""], ["Cristoforetti", "M.", ""], ["Jurman", "G.", ""], ["Furlanello", "C.", ""]]}, {"id": "1909.12111", "submitter": "Jianhang Zhou", "authors": "Jianhang Zhou, Shaoning Zeng, Bob Zhang", "title": "Two-stage Image Classification Supervised by a Single Teacher Single\n  Student Model", "comments": "Accepted by 30th British Machine Vision Conference (BMVC2019)", "journal-ref": null, "doi": null, "report-no": "#155", "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two-stage strategy has been widely used in image classification. However,\nthese methods barely take the classification criteria of the first stage into\nconsideration in the second prediction stage. In this paper, we propose a novel\ntwo-stage representation method (TSR), and convert it to a Single-Teacher\nSingle-Student (STSS) problem in our two-stage image classification framework.\nWe seek the nearest neighbours of the test sample to choose candidate target\nclasses. Meanwhile, the first stage classifier is formulated as the teacher,\nwhich holds the classification scores. The samples of the candidate classes are\nutilized to learn a student classifier based on L2-minimization in the second\nstage. The student will be supervised by the teacher classifier, which approves\nthe student only if it obtains a higher score. In actuality, the proposed\nframework generates a stronger classifier by staging two weaker classifiers in\na novel way. The experiments conducted on several face and object databases\nshow that our proposed framework is effective and outperforms multiple popular\nclassification methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 13:59:34 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Zhou", "Jianhang", ""], ["Zeng", "Shaoning", ""], ["Zhang", "Bob", ""]]}, {"id": "1909.12116", "submitter": "Jong Chul Ye", "authors": "Byeongsu Sim, Gyutaek Oh, Jeongsol Kim, Chanyong Jung, Jong Chul Ye", "title": "Optimal Transport driven CycleGAN for Unsupervised Learning in Inverse\n  Problems", "comments": "accepted for publication in the SIAM Journal on Imaging Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the performance of classical generative adversarial network (GAN),\nWasserstein generative adversarial networks (W-GAN) was developed as a\nKantorovich dual formulation of the optimal transport (OT) problem using\nWasserstein-1 distance. However, it was not clear how cycleGAN-type generative\nmodels can be derived from the optimal transport theory. Here we show that a\nnovel cycleGAN architecture can be derived as a Kantorovich dual OT formulation\nif a penalized least square (PLS) cost with deep learning-based inverse path\npenalty is used as a transportation cost. One of the most important advantages\nof this formulation is that depending on the knowledge of the forward problem,\ndistinct variations of cycleGAN architecture can be derived: for example, one\nwith two pairs of generators and discriminators, and the other with only a\nsingle pair of generator and discriminator. Even for the two generator cases,\nwe show that the structural knowledge of the forward operator can lead to a\nsimpler generator architecture which significantly simplifies the neural\nnetwork training. The new cycleGAN formulation, what we call the OT-cycleGAN,\nhave been applied for various biomedical imaging problems, such as accelerated\nmagnetic resonance imaging (MRI), super-resolution microscopy, and low-dose\nx-ray computed tomography (CT). Experimental results confirm the efficacy and\nflexibility of the theory.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 11:28:49 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 13:59:39 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 16:28:14 GMT"}, {"version": "v4", "created": "Sun, 30 Aug 2020 12:14:48 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Sim", "Byeongsu", ""], ["Oh", "Gyutaek", ""], ["Kim", "Jeongsol", ""], ["Jung", "Chanyong", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1909.12117", "submitter": "Mingzhu Shen", "authors": "Mingzhu Shen and Xianglong Liu and Ruihao Gong and Kai Han", "title": "Balanced Binary Neural Networks with Gated Residual", "comments": "Accepted by ICASSP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary neural networks have attracted numerous attention in recent years.\nHowever, mainly due to the information loss stemming from the biased\nbinarization, how to preserve the accuracy of networks still remains a critical\nissue. In this paper, we attempt to maintain the information propagated in the\nforward process and propose a Balanced Binary Neural Networks with Gated\nResidual (BBG for short). First, a weight balanced binarization is introduced\nto maximize information entropy of binary weights, and thus the informative\nbinary weights can capture more information contained in the activations.\nSecond, for binary activations, a gated residual is further appended to\ncompensate their information loss during the forward process, with a slight\noverhead. Both techniques can be wrapped as a generic network module that\nsupports various network architectures for different tasks including\nclassification and detection. We evaluate our BBG on image classification tasks\nover CIFAR-10/100 and ImageNet and on detection task over Pascal VOC. The\nexperimental results show that BBG-Net performs remarkably well across various\nnetwork architectures such as VGG, ResNet and SSD with the superior performance\nover state-of-the-art methods in terms of memory consumption, inference speed\nand accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 14:03:10 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 10:51:56 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Shen", "Mingzhu", ""], ["Liu", "Xianglong", ""], ["Gong", "Ruihao", ""], ["Han", "Kai", ""]]}, {"id": "1909.12118", "submitter": "Shifeng Zhang", "authors": "Shifeng Zhang, Yiliang Xie, Jun Wan, Hansheng Xia, Stan Z. Li, Guodong\n  Guo", "title": "WiderPerson: A Diverse Dataset for Dense Pedestrian Detection in the\n  Wild", "comments": "TMM: submitted on 2018.07.17, accepted on 2019.07.01. arXiv admin\n  note: text overlap with arXiv:1805.07193, arXiv:1805.00123 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection has achieved significant progress with the availability\nof existing benchmark datasets. However, there is a gap in the diversity and\ndensity between real world requirements and current pedestrian detection\nbenchmarks: 1) most of existing datasets are taken from a vehicle driving\nthrough the regular traffic scenario, usually leading to insufficient\ndiversity; 2) crowd scenarios with highly occluded pedestrians are still under\nrepresented, resulting in low density. To narrow this gap and facilitate future\npedestrian detection research, we introduce a large and diverse dataset named\nWiderPerson for dense pedestrian detection in the wild. This dataset involves\nfive types of annotations in a wide range of scenarios, no longer limited to\nthe traffic scenario. There are a total of $13,382$ images with $399,786$\nannotations, i.e., $29.87$ annotations per image, which means this dataset\ncontains dense pedestrians with various kinds of occlusions. Hence, pedestrians\nin the proposed dataset are extremely challenging due to large variations in\nthe scenario and occlusion, which is suitable to evaluate pedestrian detectors\nin the wild. We introduce an improved Faster R-CNN and the vanilla RetinaNet to\nserve as baselines for the new pedestrian detection benchmark. Several\nexperiments are conducted on previous datasets including Caltech-USA and\nCityPersons to analyze the generalization capabilities of the proposed dataset\nand we achieve state-of-the-art performances on these previous datasets without\nbells and whistles. Finally, we analyze common failure cases and find the\nclassification ability of pedestrian detector needs to be improved to reduce\nfalse alarm and miss detection rates. The proposed dataset is available at\nhttp://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 02:42:07 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Zhang", "Shifeng", ""], ["Xie", "Yiliang", ""], ["Wan", "Jun", ""], ["Xia", "Hansheng", ""], ["Li", "Stan Z.", ""], ["Guo", "Guodong", ""]]}, {"id": "1909.12146", "submitter": "Pavel Kirsanov", "authors": "Pavel Kirsanov, Airat Gaskarov, Filipp Konokhov, Konstantin Sofiiuk,\n  Anna Vorontsova, Igor Slinko, Dmitry Zhukov, Sergey Bykov, Olga Barinova,\n  Anton Konushin", "title": "DISCOMAN: Dataset of Indoor SCenes for Odometry, Mapping And Navigation", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel dataset for training and benchmarking semantic SLAM\nmethods. The dataset consists of 200 long sequences, each one containing\n3000-5000 data frames. We generate the sequences using realistic home layouts.\nFor that we sample trajectories that simulate motions of a simple home robot,\nand then render the frames along the trajectories. Each data frame contains a)\nRGB images generated using physically-based rendering, b) simulated depth\nmeasurements, c) simulated IMU readings and d) ground truth occupancy grid of a\nhouse. Our dataset serves a wider range of purposes compared to existing\ndatasets and is the first large-scale benchmark focused on the mapping\ncomponent of SLAM. The dataset is split into train/validation/test parts\nsampled from different sets of virtual houses. We present benchmarking results\nforboth classical geometry-based and recent learning-based SLAM algorithms, a\nbaseline mapping method, semantic segmentation and panoptic segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 14:33:31 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Kirsanov", "Pavel", ""], ["Gaskarov", "Airat", ""], ["Konokhov", "Filipp", ""], ["Sofiiuk", "Konstantin", ""], ["Vorontsova", "Anna", ""], ["Slinko", "Igor", ""], ["Zhukov", "Dmitry", ""], ["Bykov", "Sergey", ""], ["Barinova", "Olga", ""], ["Konushin", "Anton", ""]]}, {"id": "1909.12158", "submitter": "Mihee Lee", "authors": "Mihee Lee, Ognjen Rudovic, Vladimir Pavlovic, and Maja Pantic", "title": "Fast and Effective Adaptation of Facial Action Unit Detection Deep Model", "comments": "Presented at 2019 IJCAI Affective Computing Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting facial action units (AU) is one of the fundamental steps in\nautomatic recognition of facial expression of emotions and cognitive states.\nThough there have been a variety of approaches proposed for this task, most of\nthese models are trained only for the specific target AUs, and as such they\nfail to easily adapt to the task of recognition of new AUs (i.e., those not\ninitially used to train the target models). In this paper, we propose a deep\nlearning approach for facial AU detection that can easily and in a fast manner\nadapt to a new AU or target subject by leveraging only a few labeled samples\nfrom the new task (either an AU or subject). To this end, we propose a modeling\napproach based on the notion of the model-agnostic meta-learning, originally\nproposed for the general image recognition/detection tasks (e.g., the character\nrecognition from the Omniglot dataset). Specifically, each subject and/or AU is\ntreated as a new learning task and the model learns to adapt based on the\nknowledge of the previous tasks (the AUs and subjects used to pre-train the\ntarget models). Thus, given a new subject or AU, this meta-knowledge (that is\nshared among training and test tasks) is used to adapt the model to the new\ntask using the notion of deep learning and model-agnostic meta-learning. We\nshow on two benchmark datasets (BP4D and DISFA) for facial AU detection that\nthe proposed approach can be easily adapted to new tasks (AUs/subjects). Using\nonly a few labeled examples from these tasks, the model achieves large\nimprovements over the baselines (i.e., non-adapted models).\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 14:42:27 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 14:13:31 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Lee", "Mihee", ""], ["Rudovic", "Ognjen", ""], ["Pavlovic", "Vladimir", ""], ["Pantic", "Maja", ""]]}, {"id": "1909.12180", "submitter": "Alexander Meinke", "authors": "Alexander Meinke, Matthias Hein", "title": "Towards neural networks that provably know when they don't know", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has recently been shown that ReLU networks produce arbitrarily\nover-confident predictions far away from the training data. Thus, ReLU networks\ndo not know when they don't know. However, this is a highly important property\nin safety critical applications. In the context of out-of-distribution\ndetection (OOD) there have been a number of proposals to mitigate this problem\nbut none of them are able to make any mathematical guarantees. In this paper we\npropose a new approach to OOD which overcomes both problems. Our approach can\nbe used with ReLU networks and provides provably low confidence predictions far\naway from the training data as well as the first certificates for low\nconfidence predictions in a neighborhood of an out-distribution point. In the\nexperiments we show that state-of-the-art methods fail in this worst-case\nsetting whereas our model can guarantee its performance while retaining\nstate-of-the-art OOD performance.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 15:20:08 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 09:27:11 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Meinke", "Alexander", ""], ["Hein", "Matthias", ""]]}, {"id": "1909.12196", "submitter": "Jochen Gast", "authors": "Jochen Gast and Stefan Roth", "title": "Deep Video Deblurring: The Devil is in the Details", "comments": "To appear at ICCVW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video deblurring for hand-held cameras is a challenging task, since the\nunderlying blur is caused by both camera shake and object motion.\nState-of-the-art deep networks exploit temporal information from neighboring\nframes, either by means of spatio-temporal transformers or by recurrent\narchitectures. In contrast to these involved models, we found that a simple\nbaseline CNN can perform astonishingly well when particular care is taken\nw.r.t. the details of model and training procedure. To that end, we conduct a\ncomprehensive study regarding these crucial details, uncovering extreme\ndifferences in quantitative and qualitative performance. Exploiting these\ndetails allows us to boost the architecture and training procedure of a simple\nbaseline CNN by a staggering 3.15dB, such that it becomes highly competitive\nw.r.t. cutting-edge networks. This raises the question whether the reported\naccuracy difference between models is always due to technical contributions or\nalso subject to such orthogonal, but crucial details.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 15:35:29 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Gast", "Jochen", ""], ["Roth", "Stefan", ""]]}, {"id": "1909.12205", "submitter": "Vahid Partovi Nia", "authors": "Gr\\'egoire Morin, Ryan Razani, Vahid Partovi Nia, and Eyy\\\"ub Sari", "title": "Smart Ternary Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network models are resource hungry. Low bit quantization such as\nbinary and ternary quantization is a common approach to alleviate this resource\nrequirements. Ternary quantization provides a more flexible model and often\nbeats binary quantization in terms of accuracy, but doubles memory and\nincreases computation cost. Mixed quantization depth models, on another hand,\nallows a trade-off between accuracy and memory footprint. In such models,\nquantization depth is often chosen manually (which is a tiring task), or is\ntuned using a separate optimization routine (which requires training a\nquantized network multiple times). Here, we propose Smart Ternary Quantization\n(STQ) in which we modify the quantization depth directly through an adaptive\nregularization function, so that we train a model only once. This method jumps\nbetween binary and ternary quantization while training. We show its application\non image classification.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 15:49:08 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Morin", "Gr\u00e9goire", ""], ["Razani", "Ryan", ""], ["Nia", "Vahid Partovi", ""], ["Sari", "Eyy\u00fcb", ""]]}, {"id": "1909.12220", "submitter": "Yulin Wang", "authors": "Yulin Wang, Xuran Pan, Shiji Song, Hong Zhang, Cheng Wu, Gao Huang", "title": "Implicit Semantic Data Augmentation for Deep Networks", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel implicit semantic data augmentation (ISDA)\napproach to complement traditional augmentation techniques like flipping,\ntranslation or rotation. Our work is motivated by the intriguing property that\ndeep networks are surprisingly good at linearizing features, such that certain\ndirections in the deep feature space correspond to meaningful semantic\ntransformations, e.g., adding sunglasses or changing backgrounds. As a\nconsequence, translating training samples along many semantic directions in the\nfeature space can effectively augment the dataset to improve generalization. To\nimplement this idea effectively and efficiently, we first perform an online\nestimate of the covariance matrix of deep features for each class, which\ncaptures the intra-class semantic variations. Then random vectors are drawn\nfrom a zero-mean normal distribution with the estimated covariance to augment\nthe training data in that class. Importantly, instead of augmenting the samples\nexplicitly, we can directly minimize an upper bound of the expected\ncross-entropy (CE) loss on the augmented training set, leading to a highly\nefficient algorithm. In fact, we show that the proposed ISDA amounts to\nminimizing a novel robust CE loss, which adds negligible extra computational\ncost to a normal training procedure. Although being simple, ISDA consistently\nimproves the generalization performance of popular deep models (ResNets and\nDenseNets) on a variety of datasets, e.g., CIFAR-10, CIFAR-100 and ImageNet.\nCode for reproducing our results is available at\nhttps://github.com/blackfeather-wang/ISDA-for-Deep-Networks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 16:17:45 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 04:57:35 GMT"}, {"version": "v3", "created": "Sun, 24 Nov 2019 13:56:17 GMT"}, {"version": "v4", "created": "Fri, 20 Dec 2019 10:11:01 GMT"}, {"version": "v5", "created": "Sat, 25 Apr 2020 03:13:03 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Wang", "Yulin", ""], ["Pan", "Xuran", ""], ["Song", "Shiji", ""], ["Zhang", "Hong", ""], ["Wu", "Cheng", ""], ["Huang", "Gao", ""]]}, {"id": "1909.12224", "submitter": "Zhixin Piao", "authors": "Wen Liu, Zhixin Piao, Jie Min, Wenhan Luo, Lin Ma, Shenghua Gao", "title": "Liquid Warping GAN: A Unified Framework for Human Motion Imitation,\n  Appearance Transfer and Novel View Synthesis", "comments": "accepted by ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the human motion imitation, appearance transfer, and novel view\nsynthesis within a unified framework, which means that the model once being\ntrained can be used to handle all these tasks. The existing task-specific\nmethods mainly use 2D keypoints (pose) to estimate the human body structure.\nHowever, they only expresses the position information with no abilities to\ncharacterize the personalized shape of the individual person and model the\nlimbs rotations. In this paper, we propose to use a 3D body mesh recovery\nmodule to disentangle the pose and shape, which can not only model the joint\nlocation and rotation but also characterize the personalized body shape. To\npreserve the source information, such as texture, style, color, and face\nidentity, we propose a Liquid Warping GAN with Liquid Warping Block (LWB) that\npropagates the source information in both image and feature spaces, and\nsynthesizes an image with respect to the reference. Specifically, the source\nfeatures are extracted by a denoising convolutional auto-encoder for\ncharacterizing the source identity well. Furthermore, our proposed method is\nable to support a more flexible warping from multiple sources. In addition, we\nbuild a new dataset, namely Impersonator (iPER) dataset, for the evaluation of\nhuman motion imitation, appearance transfer, and novel view synthesis.\nExtensive experiments demonstrate the effectiveness of our method in several\naspects, such as robustness in occlusion case and preserving face identity,\nshape consistency and clothes details. All codes and datasets are available on\nhttps://svip-lab.github.io/project/impersonator.html\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 16:23:59 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 14:52:32 GMT"}, {"version": "v3", "created": "Tue, 1 Oct 2019 16:42:27 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Liu", "Wen", ""], ["Piao", "Zhixin", ""], ["Min", "Jie", ""], ["Luo", "Wenhan", ""], ["Ma", "Lin", ""], ["Gao", "Shenghua", ""]]}, {"id": "1909.12235", "submitter": "Matteo Tiezzi", "authors": "Matteo Tiezzi, Stefano Melacci, Marco Maggini, Angelo Frosini", "title": "Video Surveillance of Highway Traffic Events by Deep Learning\n  Architectures", "comments": null, "journal-ref": "Lecture Notes in Computer Science, vol 11141, (2018) pp 584-593", "doi": "10.1007/978-3-030-01424-7_57", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a video surveillance system able to detect traffic\nevents in videos acquired by fixed videocameras on highways. The events of\ninterest consist in a specific sequence of situations that occur in the video,\nas for instance a vehicle stopping on the emergency lane. Hence, the detection\nof these events requires to analyze a temporal sequence in the video stream. We\ncompare different approaches that exploit architectures based on Recurrent\nNeural Networks (RNNs) and Convolutional Neural Networks (CNNs). A first\napproach extracts vectors of features, mostly related to motion, from each\nvideo frame and exploits a RNN fed with the resulting sequence of vectors. The\nother approaches are based directly on the sequence of frames, that are\neventually enriched with pixel-wise motion information. The obtained stream is\nprocessed by an architecture that stacks a CNN and a RNN, and we also\ninvestigate a transfer-learning-based model. The results are very promising and\nthe best architecture will be tested online in real operative conditions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 15:36:02 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Tiezzi", "Matteo", ""], ["Melacci", "Stefano", ""], ["Maggini", "Marco", ""], ["Frosini", "Angelo", ""]]}, {"id": "1909.12249", "submitter": "Ze Wang", "authors": "Ze Wang, Sihao Ding, Ying Li, Minming Zhao, Sohini Roychowdhury,\n  Andreas Wallin, Guillermo Sapiro, and Qiang Qiu", "title": "Range Adaptation for 3D Object Detection in LiDAR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR-based 3D object detection plays a crucial role in modern autonomous\ndriving systems. LiDAR data often exhibit severe changes in properties across\ndifferent observation ranges. In this paper, we explore cross-range adaptation\nfor 3D object detection using LiDAR, i.e., far-range observations are adapted\nto near-range. This way, far-range detection is optimized for similar\nperformance to near-range one. We adopt a bird-eyes view (BEV) detection\nframework to perform the proposed model adaptation. Our model adaptation\nconsists of an adversarial global adaptation, and a fine-grained local\nadaptation. The proposed cross range adaptation framework is validated on three\nstate-of-the-art LiDAR based object detection networks, and we consistently\nobserve performance improvement on the far-range objects, without adding any\nauxiliary parameters to the model. To the best of our knowledge, this paper is\nthe first attempt to study cross-range LiDAR adaptation for object detection in\npoint clouds. To demonstrate the generality of the proposed adaptation\nframework, experiments on more challenging cross-device adaptation are further\nconducted, and a new LiDAR dataset with high-quality annotated point clouds is\nreleased to promote future research.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 16:56:59 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Wang", "Ze", ""], ["Ding", "Sihao", ""], ["Li", "Ying", ""], ["Zhao", "Minming", ""], ["Roychowdhury", "Sohini", ""], ["Wallin", "Andreas", ""], ["Sapiro", "Guillermo", ""], ["Qiu", "Qiang", ""]]}, {"id": "1909.12271", "submitter": "Stephen James", "authors": "Stephen James, Zicong Ma, David Rovick Arrojo, Andrew J. Davison", "title": "RLBench: The Robot Learning Benchmark & Learning Environment", "comments": "Videos and code: https://sites.google.com/view/rlbench", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a challenging new benchmark and learning-environment for robot\nlearning: RLBench. The benchmark features 100 completely unique, hand-designed\ntasks ranging in difficulty, from simple target reaching and door opening, to\nlonger multi-stage tasks, such as opening an oven and placing a tray in it. We\nprovide an array of both proprioceptive observations and visual observations,\nwhich include rgb, depth, and segmentation masks from an over-the-shoulder\nstereo camera and an eye-in-hand monocular camera. Uniquely, each task comes\nwith an infinite supply of demos through the use of motion planners operating\non a series of waypoints given during task creation time; enabling an exciting\nflurry of demonstration-based learning. RLBench has been designed with\nscalability in mind; new tasks, along with their motion-planned demos, can be\neasily created and then verified by a series of tools, allowing users to submit\ntheir own tasks to the RLBench task repository. This large-scale benchmark aims\nto accelerate progress in a number of vision-guided manipulation research\nareas, including: reinforcement learning, imitation learning, multi-task\nlearning, geometric computer vision, and in particular, few-shot learning. With\nthe benchmark's breadth of tasks and demonstrations, we propose the first\nlarge-scale few-shot challenge in robotics. We hope that the scale and\ndiversity of RLBench offers unparalleled research opportunities in the robot\nlearning community and beyond.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 17:26:18 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["James", "Stephen", ""], ["Ma", "Zicong", ""], ["Arrojo", "David Rovick", ""], ["Davison", "Andrew J.", ""]]}, {"id": "1909.12297", "submitter": "Fredrik K. Gustafsson", "authors": "Fredrik K. Gustafsson, Martin Danelljan, Goutam Bhat, Thomas B.\n  Sch\\\"on", "title": "Energy-Based Models for Deep Probabilistic Regression", "comments": "ECCV 2020. Code is available at\n  https://github.com/fregu856/ebms_regression", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning-based classification is generally tackled using\nstandardized approaches, a wide variety of techniques are employed for\nregression. In computer vision, one particularly popular such technique is that\nof confidence-based regression, which entails predicting a confidence value for\neach input-target pair (x,y). While this approach has demonstrated impressive\nresults, it requires important task-dependent design choices, and the predicted\nconfidences lack a natural probabilistic meaning. We address these issues by\nproposing a general and conceptually simple regression method with a clear\nprobabilistic interpretation. In our proposed approach, we create an\nenergy-based model of the conditional target density p(y|x), using a deep\nneural network to predict the un-normalized density from (x,y). This model of\np(y|x) is trained by directly minimizing the associated negative\nlog-likelihood, approximated using Monte Carlo sampling. We perform\ncomprehensive experiments on four computer vision regression tasks. Our\napproach outperforms direct regression, as well as other probabilistic and\nconfidence-based methods. Notably, our model achieves a 2.2% AP improvement\nover Faster-RCNN for object detection on the COCO dataset, and sets a new\nstate-of-the-art on visual tracking when applied for bounding box estimation.\nIn contrast to confidence-based methods, our approach is also shown to be\ndirectly applicable to more general tasks such as age and head-pose estimation.\nCode is available at https://github.com/fregu856/ebms_regression.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 17:58:43 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 15:35:36 GMT"}, {"version": "v3", "created": "Wed, 8 Apr 2020 11:19:02 GMT"}, {"version": "v4", "created": "Sun, 19 Jul 2020 12:47:37 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Gustafsson", "Fredrik K.", ""], ["Danelljan", "Martin", ""], ["Bhat", "Goutam", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1909.12342", "submitter": "Han-Wen Kuo", "authors": "Han-Wen Kuo, Anna E. Dorfi, Daniel V. Esposito, John N. Wright", "title": "Compressed Sensing Microscopy with Scanning Line Probes", "comments": "15 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications of scanning probe microscopy, images are acquired by raster\nscanning a point probe across a sample. Viewed from the perspective of\ncompressed sensing (CS), this pointwise sampling scheme is inefficient,\nespecially when the target image is structured. While replacing point\nmeasurements with delocalized, incoherent measurements has the potential to\nyield order-of-magnitude improvements in scan time, implementing the\ndelocalized measurements of CS theory is challenging. In this paper we study a\npartially delocalized probe construction, in which the point probe is replaced\nwith a continuous line, creating a sensor which essentially acquires line\nintegrals of the target image. We show through simulations, rudimentary\ntheoretical analysis, and experiments, that these line measurements can image\nsparse samples far more efficiently than traditional point measurements,\nprovided the local features in the sample are enough separated. Despite this\npromise, practical reconstruction from line measurements poses additional\ndifficulties: the measurements are partially coherent, and real measurements\nexhibit nonidealities. We show how to overcome these limitations using natural\nstrategies (reweighting to cope with coherence, blind calibration for\nnonidealities), culminating in an end-to-end demonstration.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 19:13:05 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Kuo", "Han-Wen", ""], ["Dorfi", "Anna E.", ""], ["Esposito", "Daniel V.", ""], ["Wright", "John N.", ""]]}, {"id": "1909.12354", "submitter": "Qingyang Tan", "authors": "Qingyang Tan, Zherong Pan, Lin Gao, Dinesh Manocha", "title": "Realtime Simulation of Thin-Shell Deformable Materials using CNN-Based\n  Mesh Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of accelerating thin-shell deformable object\nsimulations by dimension reduction. We present a new algorithm to embed a\nhigh-dimensional configuration space of deformable objects in a low-dimensional\nfeature space, where the configurations of objects and feature points have\napproximate one-to-one mapping. Our key technique is a graph-based\nconvolutional neural network (CNN) defined on meshes with arbitrary topologies\nand a new mesh embedding approach based on physics-inspired loss term. We have\napplied our approach to accelerate high-resolution thin shell simulations\ncorresponding to cloth-like materials, where the configuration space has tens\nof thousands of degrees of freedom. We show that our physics-inspired embedding\napproach leads to higher accuracy compared with prior mesh embedding methods.\nFinally, we show that the temporal evolution of the mesh in the feature space\ncan also be learned using a recurrent neural network (RNN) leading to fully\nlearnable physics simulators. After training our learned simulator runs\n$500-10000\\times$ faster and the accuracy is high enough for robot manipulation\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 19:38:58 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 14:28:37 GMT"}, {"version": "v3", "created": "Mon, 16 Dec 2019 04:16:33 GMT"}, {"version": "v4", "created": "Fri, 28 Feb 2020 04:45:12 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Tan", "Qingyang", ""], ["Pan", "Zherong", ""], ["Gao", "Lin", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1909.12358", "submitter": "Di Feng", "authors": "Di Feng, Lars Rosenbaum, Claudius Glaeser, Fabian Timm, Klaus\n  Dietmayer", "title": "Can We Trust You? On Calibration of a Probabilistic Object Detector for\n  Autonomous Driving", "comments": "To appear in IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable uncertainty estimation is crucial for perception systems in safe\nautonomous driving. Recently, many methods have been proposed to model\nuncertainties in deep learning based object detectors. However, the estimated\nprobabilities are often uncalibrated, which may lead to severe problems in\nsafety critical scenarios. In this work, we identify such uncertainty\nmiscalibration problems in a probabilistic LiDAR 3D object detection network,\nand propose three practical methods to significantly reduce errors in\nuncertainty calibration. Extensive experiments on several datasets show that\nour methods produce well-calibrated uncertainties, and generalize well between\ndifferent datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 19:48:16 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Feng", "Di", ""], ["Rosenbaum", "Lars", ""], ["Glaeser", "Claudius", ""], ["Timm", "Fabian", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1909.12366", "submitter": "Pritish Sahu", "authors": "Behnam Gholami, Pritish Sahu, Minyoung Kim, Vladimir Pavlovic", "title": "Task-Discriminative Domain Alignment for Unsupervised Domain Adaptation", "comments": "This paper is accepted for ORAL presentation at the ICCV 2019 MDALC\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain Adaptation (DA), the process of effectively adapting task models\nlearned on one domain, the source, to other related but distinct domains, the\ntargets, with no or minimal retraining, is typically accomplished using the\nprocess of source-to-target manifold alignment. However, this process often\nleads to unsatisfactory adaptation performance, in part because it ignores the\ntask-specific structure of the data. In this paper, we improve the performance\nof DA by introducing a discriminative discrepancy measure which takes advantage\nof auxiliary information available in the source and the target domains to\nbetter align the source and target distributions. Specifically, we leverage the\ncohesive clustering structure within individual data manifolds, associated with\ndifferent tasks, to improve the alignment. This structure is explicit in the\nsource, where the task labels are available, but is implicit in the target,\nmaking the problem challenging. We address the challenge by devising a deep DA\nframework, which combines a new task-driven domain alignment discriminator with\ndomain regularizers that encourage the shared features as task-specific and\ndomain invariant, and prompt the task model to be data structure preserving,\nguiding its decision boundaries through the low density data regions. We\nvalidate our framework on standard benchmarks, including Digits (MNIST, USPS,\nSVHN, MNIST-M), PACS, and VisDA. Our results show that our proposed model\nconsistently outperforms the state-of-the-art in unsupervised domain\nadaptation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 20:04:50 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Gholami", "Behnam", ""], ["Sahu", "Pritish", ""], ["Kim", "Minyoung", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "1909.12383", "submitter": "Ziming Zhang", "authors": "Yecheng Lyu and Xinming Huang and Ziming Zhang", "title": "Graph-Preserving Grid Layout: A Simple Graph Drawing Method for Graph\n  Classification using CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional networks (GCNs) suffer from the irregularity of graphs,\nwhile more widely-used convolutional neural networks (CNNs) benefit from\nregular grids. To bridge the gap between GCN and CNN, in contrast to previous\nworks on generalizing the basic operations in CNNs to graph data, in this paper\nwe address the problem of how to project undirected graphs onto the grid in a\n{\\em principled} way where CNNs can be used as backbone for geometric deep\nlearning. To this end, inspired by the literature of graph drawing we propose a\nnovel graph-preserving grid layout (GPGL), an integer programming that\nminimizes the topological loss on the grid. Technically we propose solving GPGL\napproximately using a {\\em regularized} Kamada-Kawai algorithm, a well-known\nnonconvex optimization technique in graph drawing, with a vertex separation\npenalty that improves the rounding performance on top of the solutions from\nrelaxation. Using GPGL we can easily conduct data augmentation as every local\nminimum will lead to a grid layout for the same graph. Together with the help\nof multi-scale maxout CNNs, we demonstrate the empirical success of our method\nfor graph classification.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 20:53:12 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Lyu", "Yecheng", ""], ["Huang", "Xinming", ""], ["Zhang", "Ziming", ""]]}, {"id": "1909.12398", "submitter": "Sathya N. Ravi", "authors": "Sathya N. Ravi, Abhay Venkatesh, Glenn Moo Fung, Vikas Singh", "title": "Optimizing Nondecomposable Data Dependent Regularizers via Lagrangian\n  Reparameterization offers Significant Performance and Efficiency Gains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data dependent regularization is known to benefit a wide variety of problems\nin machine learning. Often, these regularizers cannot be easily decomposed into\na sum over a finite number of terms, e.g., a sum over individual example-wise\nterms. The $F_\\beta$ measure, Area under the ROC curve (AUCROC) and Precision\nat a fixed recall (P@R) are some prominent examples that are used in many\napplications. We find that for most medium to large sized datasets, scalability\nissues severely limit our ability in leveraging the benefits of such\nregularizers. Importantly, the key technical impediment despite some recent\nprogress is that, such objectives remain difficult to optimize via\nbackpropapagation procedures. While an efficient general-purpose strategy for\nthis problem still remains elusive, in this paper, we show that for many\ndata-dependent nondecomposable regularizers that are relevant in applications,\nsizable gains in efficiency are possible with minimal code-level changes; in\nother words, no specialized tools or numerical schemes are needed. Our\nprocedure involves a reparameterization followed by a partial dualization --\nthis leads to a formulation that has provably cheap projection operators. We\npresent a detailed analysis of runtime and convergence properties of our\nalgorithm. On the experimental side, we show that a direct use of our scheme\nsignificantly improves the state of the art IOU measures reported for MSCOCO\nStuff segmentation dataset.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 21:19:30 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Ravi", "Sathya N.", ""], ["Venkatesh", "Abhay", ""], ["Fung", "Glenn Moo", ""], ["Singh", "Vikas", ""]]}, {"id": "1909.12400", "submitter": "Nikita Araslanov", "authors": "Vladyslav Yushchenko, Nikita Araslanov, Stefan Roth", "title": "Markov Decision Process for Video Generation", "comments": "To appear at 2019 ICCV Workshop on Large Scale Holistic Video\n  Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify two pathological cases of temporal inconsistencies in video\ngeneration: video freezing and video looping. To better quantify the temporal\ndiversity, we propose a class of complementary metrics that are effective, easy\nto implement, data agnostic, and interpretable. Further, we observe that\ncurrent state-of-the-art models are trained on video samples of fixed length\nthereby inhibiting long-term modeling. To address this, we reformulate the\nproblem of video generation as a Markov Decision Process (MDP). The underlying\nidea is to represent motion as a stochastic process with an infinite forecast\nhorizon to overcome the fixed length limitation and to mitigate the presence of\ntemporal artifacts. We show that our formulation is easy to integrate into the\nstate-of-the-art MoCoGAN framework. Our experiments on the Human Actions and\nUCF-101 datasets demonstrate that our MDP-based model is more memory efficient\nand improves the video quality both in terms of the new and established\nmetrics.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 21:23:16 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Yushchenko", "Vladyslav", ""], ["Araslanov", "Nikita", ""], ["Roth", "Stefan", ""]]}, {"id": "1909.12401", "submitter": "Brent Harrison", "authors": "Md Sultan Al Nahian, Tasmia Tasrin, Sagar Gandhi, Ryan Gaines, and\n  Brent Harrison", "title": "A Hierarchical Approach for Visual Storytelling Using Image Description", "comments": "Accepted at the 2019 International Conference on Interactive Digital\n  Storytelling (ICIDS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the primary challenges of visual storytelling is developing techniques\nthat can maintain the context of the story over long event sequences to\ngenerate human-like stories. In this paper, we propose a hierarchical deep\nlearning architecture based on encoder-decoder networks to address this\nproblem. To better help our network maintain this context while also generating\nlong and diverse sentences, we incorporate natural language image descriptions\nalong with the images themselves to generate each story sentence. We evaluate\nour system on the Visual Storytelling (VIST) dataset and show that our method\noutperforms state-of-the-art techniques on a suite of different automatic\nevaluation metrics. The empirical results from this evaluation demonstrate the\nnecessities of different components of our proposed architecture and shows the\neffectiveness of the architecture for visual storytelling.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 21:25:41 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Nahian", "Md Sultan Al", ""], ["Tasrin", "Tasmia", ""], ["Gandhi", "Sagar", ""], ["Gaines", "Ryan", ""], ["Harrison", "Brent", ""]]}, {"id": "1909.12446", "submitter": "Sin-Han Kang", "authors": "Sin-Han Kang, Hong-Gyu Jung, and Seong-Whan Lee", "title": "Interpreting Undesirable Pixels for Image Classification on Black-Box\n  Models", "comments": "Accepted to 2019 ICCV Workshop on Interpreting and Explaining Visual\n  Artificial Intelligence Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an effort to interpret black-box models, researches for developing\nexplanation methods have proceeded in recent years. Most studies have tried to\nidentify input pixels that are crucial to the prediction of a classifier. While\nthis approach is meaningful to analyse the characteristic of blackbox models,\nit is also important to investigate pixels that interfere with the prediction.\nTo tackle this issue, in this paper, we propose an explanation method that\nvisualizes undesirable regions to classify an image as a target class. To be\nspecific, we divide the concept of undesirable regions into two terms: (1)\nfactors for a target class, which hinder that black-box models identify\nintrinsic characteristics of a target class and (2) factors for non-target\nclasses that are important regions for an image to be classified as other\nclasses. We visualize such undesirable regions on heatmaps to qualitatively\nvalidate the proposed method. Furthermore, we present an evaluation metric to\nprovide quantitative results on ImageNet.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 00:25:43 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 11:04:15 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Kang", "Sin-Han", ""], ["Jung", "Hong-Gyu", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "1909.12456", "submitter": "Jingru Yi", "authors": "Jingru Yi, Pengxiang Wu, Dimitris N. Metaxas", "title": "ASSD: Attentive Single Shot Multibox Detector", "comments": "accepted by Computer Vision and Image Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new deep neural network for object detection. The\nproposed network, termed ASSD, builds feature relations in the spatial space of\nthe feature map. With the global relation information, ASSD learns to highlight\nuseful regions on the feature maps while suppressing the irrelevant\ninformation, thereby providing reliable guidance for object detection. Compared\nto methods that rely on complicated CNN layers to refine the feature maps, ASSD\nis simple in design and is computationally efficient. Experimental results show\nthat ASSD competes favorably with the state-of-the-arts, including SSD, DSSD,\nFSSD and RetinaNet.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 01:36:41 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Yi", "Jingru", ""], ["Wu", "Pengxiang", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "1909.12460", "submitter": "Mohit Sharma", "authors": "Kevin Zhang, Mohit Sharma, Manuela Veloso, and Oliver Kroemer", "title": "Leveraging Multimodal Haptic Sensory Data for Robust Cutting", "comments": "Accepted as conference paper at Humanoids'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cutting is a common form of manipulation when working with divisible objects\nsuch as food, rope, or clay. Cooking in particular relies heavily on cutting to\ndivide food items into desired shapes. However, cutting food is a challenging\ntask due to the wide range of material properties exhibited by food items. Due\nto this variability, the same cutting motions cannot be used for all food\nitems. Sensations from contact events, e.g., when placing the knife on the food\nitem, will also vary depending on the material properties, and the robot will\nneed to adapt accordingly. In this paper, we propose using vibrations and\nforce-torque feedback from the interactions to adapt the slicing motions and\nmonitor for contact events. The robot learns neural networks for performing\neach of these tasks and generalizing across different material properties. By\nadapting and monitoring the skill executions, the robot is able to reliably cut\nthrough more than 20 different types of food items and even detect whether\ncertain food items are fresh or old.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 01:54:17 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Zhang", "Kevin", ""], ["Sharma", "Mohit", ""], ["Veloso", "Manuela", ""], ["Kroemer", "Oliver", ""]]}, {"id": "1909.12471", "submitter": "Xiaohui Zeng", "authors": "Xiaohui Zeng, Renjie Liao, Li Gu, Yuwen Xiong, Sanja Fidler, Raquel\n  Urtasun", "title": "DMM-Net: Differentiable Mask-Matching Network for Video Object\n  Segmentation", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the differentiable mask-matching network (DMM-Net)\nfor solving the video object segmentation problem where the initial object\nmasks are provided. Relying on the Mask R-CNN backbone, we extract mask\nproposals per frame and formulate the matching between object templates and\nproposals at one time step as a linear assignment problem where the cost matrix\nis predicted by a CNN. We propose a differentiable matching layer by unrolling\na projected gradient descent algorithm in which the projection exploits the\nDykstra's algorithm. We prove that under mild conditions, the matching is\nguaranteed to converge to the optimum. In practice, it performs similarly to\nthe Hungarian algorithm during inference. Meanwhile, we can back-propagate\nthrough it to learn the cost matrix. After matching, a refinement head is\nleveraged to improve the quality of the matched mask. Our DMM-Net achieves\ncompetitive results on the largest video object segmentation dataset\nYouTube-VOS. On DAVIS 2017, DMM-Net achieves the best performance without\nonline learning on the first frames. Without any fine-tuning, DMM-Net performs\ncomparably to state-of-the-art methods on SegTrack v2 dataset. At last, our\nmatching layer is very simple to implement; we attach the PyTorch code ($<50$\nlines) in the supplementary material. Our code is released at\nhttps://github.com/ZENGXH/DMM_Net.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 02:25:59 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Zeng", "Xiaohui", ""], ["Liao", "Renjie", ""], ["Gu", "Li", ""], ["Xiong", "Yuwen", ""], ["Fidler", "Sanja", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1909.12472", "submitter": "Ruisen Luo", "authors": "Ruisen Luo, Tao Hu, Zuodong Tang, Chen Wang, Xiaofeng Gong, and Haiyan\n  Tu", "title": "A Radio Signal Modulation Recognition Algorithm Based on Residual\n  Networks and Attention Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To solve the problem of inaccurate recognition of types of communication\nsignal modulation, a RNN neural network recognition algorithm combining\nresidual block network with attention mechanism is proposed. In this method, 10\nkinds of communication signals with Gaussian white noise are generated from\nstandard data sets, such as MASK, MPSK, MFSK, OFDM, 16QAM, AM and FM. Based on\nthe original RNN neural network, residual block network is added to solve the\nproblem of gradient disappearance caused by deep network layers. Attention\nmechanism is added to the network to accelerate the gradient descent. In the\nexperiment, 16QAM, 2FSK and 4FSK are used as actual samples, IQ data frames of\nsignals are used as input, and the RNN neural network combined with residual\nblock network and attention mechanism is trained. The final recognition results\nshow that the average recognition rate of real-time signals is over 93%. The\nnetwork has high robustness and good use value.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 02:26:19 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Luo", "Ruisen", ""], ["Hu", "Tao", ""], ["Tang", "Zuodong", ""], ["Wang", "Chen", ""], ["Gong", "Xiaofeng", ""], ["Tu", "Haiyan", ""]]}, {"id": "1909.12482", "submitter": "Mingjie Sun", "authors": "Mingjie Sun, Jimin Xiao, Eng Gee Lim, Yanchu Xie, Jiashi Feng", "title": "Adaptive ROI Generation for Video Object Segmentation Using\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to tackle the task of semi-supervised video object\nsegmentation across a sequence of frames where only the ground-truth\nsegmentation of the first frame is provided. The challenges lie in how to\nonline update the segmentation model initialized from the first frame\nadaptively and accurately, even in presence of multiple confusing instances or\nlarge object motion. The existing approaches rely on selecting the region of\ninterest for model update, which however, is rough and inflexible, leading to\nperformance degradation. To overcome this limitation, we propose a novel\napproach which utilizes reinforcement learning to select optimal adaptation\nareas for each frame, based on the historical segmentation information. The RL\nmodel learns to take optimal actions to adjust the region of interest inferred\nfrom the previous frame for online model updating. To speed up the model\nadaption, we further design a novel multi-branch tree based exploration method\nto fast select the best state action pairs. Our experiments show that our work\nimproves the state-of-the-art of the mean region similarity on DAVIS 2016\ndataset to 87.1%.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 03:39:05 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Sun", "Mingjie", ""], ["Xiao", "Jimin", ""], ["Lim", "Eng Gee", ""], ["Xie", "Yanchu", ""], ["Feng", "Jiashi", ""]]}, {"id": "1909.12493", "submitter": "Kuniyuki Takahashi", "authors": "Kuniyuki Takahashi, Kenta Yonekura", "title": "Invisible Marker: Automatic Annotation of Segmentation Masks for Object\n  Manipulation", "comments": "8 pages. Accepted to IROS 2020. An accompanying video is available at\n  https://youtu.be/fnpyDYUvDA4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to annotate segmentation masks accurately and\nautomatically using invisible marker for object manipulation. Invisible marker\nis invisible under visible (regular) light conditions, but becomes visible\nunder invisible light, such as ultraviolet (UV) light. By painting objects with\nthe invisible marker, and by capturing images while alternately switching\nbetween regular and UV light at high speed, massive annotated datasets are\ncreated quickly and inexpensively. We show a comparison between our proposed\nmethod and manual annotations. We demonstrate semantic segmentation for\ndeformable objects including clothes, liquids, and powders under controlled\nenvironmental light conditions. In addition, we show demonstrations of liquid\npouring tasks under uncontrolled environmental light conditions in complex\nenvironments such as inside the office, house, and outdoors. Furthermore, it is\npossible to capture data while the camera is in motion so it becomes easier to\ncapture large datasets, as shown in our demonstration.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 04:49:47 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 07:24:42 GMT"}, {"version": "v3", "created": "Sun, 2 Aug 2020 02:59:33 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Takahashi", "Kuniyuki", ""], ["Yonekura", "Kenta", ""]]}, {"id": "1909.12507", "submitter": "Yuqing Ma", "authors": "Yuqing Ma, Xianglong Liu, Shihao Bai, Lei Wang, Aishan Liu, Dacheng\n  Tao, Edwin Hancock", "title": "Region-wise Generative Adversarial ImageInpainting for Large Missing\n  Areas", "comments": "13 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep neutral networks have achieved promising performance for\nfilling large missing regions in image inpainting tasks. They usually adopted\nthe standard convolutional architecture over the corrupted image, leading to\nmeaningless contents, such as color discrepancy, blur and artifacts. Moreover,\nmost inpainting approaches cannot well handle the large continuous missing area\ncases. To address these problems, we propose a generic inpainting framework\ncapable of handling with incomplete images on both continuous and discontinuous\nlarge missing areas, in an adversarial manner. From which, region-wise\nconvolution is deployed in both generator and discriminator to separately\nhandle with the different regions, namely existing regions and missing ones.\nMoreover, a correlation loss is introduced to capture the non-local\ncorrelations between different patches, and thus guides the generator to obtain\nmore information during inference. With the help of our proposed framework, we\ncan restore semantically reasonable and visually realistic images. Extensive\nexperiments on three widely-used datasets for image inpainting tasks have been\nconducted, and both qualitative and quantitative experimental results\ndemonstrate that the proposed model significantly outperforms the\nstate-of-the-art approaches, both on the large continuous and discontinuous\nmissing areas.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 06:07:08 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Ma", "Yuqing", ""], ["Liu", "Xianglong", ""], ["Bai", "Shihao", ""], ["Wang", "Lei", ""], ["Liu", "Aishan", ""], ["Tao", "Dacheng", ""], ["Hancock", "Edwin", ""]]}, {"id": "1909.12513", "submitter": "Lin Song", "authors": "Lin Song, Yanwei Li, Zeming Li, Gang Yu, Hongbin Sun, Jian Sun,\n  Nanning Zheng", "title": "Learnable Tree Filter for Structure-preserving Feature Transform", "comments": "Accepted by NeurIPS-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning discriminative global features plays a vital role in semantic\nsegmentation. And most of the existing methods adopt stacks of local\nconvolutions or non-local blocks to capture long-range context. However, due to\nthe absence of spatial structure preservation, these operators ignore the\nobject details when enlarging receptive fields. In this paper, we propose the\nlearnable tree filter to form a generic tree filtering module that leverages\nthe structural property of minimal spanning tree to model long-range\ndependencies while preserving the details. Furthermore, we propose a highly\nefficient linear-time algorithm to reduce resource consumption. Thus, the\ndesigned modules can be plugged into existing deep neural networks\nconveniently. To this end, tree filtering modules are embedded to formulate a\nunified framework for semantic segmentation. We conduct extensive ablation\nstudies to elaborate on the effectiveness and efficiency of the proposed\nmethod. Specifically, it attains better performance with much less overhead\ncompared with the classic PSP block and Non-local operation under the same\nbackbone. Our approach is proved to achieve consistent improvements on several\nbenchmarks without bells-and-whistles. Code and models are available at\nhttps://github.com/StevenGrove/TreeFilter-Torch.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 06:36:39 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Song", "Lin", ""], ["Li", "Yanwei", ""], ["Li", "Zeming", ""], ["Yu", "Gang", ""], ["Sun", "Hongbin", ""], ["Sun", "Jian", ""], ["Zheng", "Nanning", ""]]}, {"id": "1909.12525", "submitter": "Ashish Sinha", "authors": "Ashish Sinha, Yohei Sugawara, Yuichiro Hirano", "title": "GA-GAN: CT reconstruction from Biplanar DRRs using GAN with Guided\n  Attention", "comments": "4 pages, 4 figures, NeurIPS workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work investigates the use of guided attention in the reconstruction of\nCTvolumes from biplanar DRRs. We try to improve the visual image quality of the\nCT reconstruction using Guided Attention based GANs (GA-GAN). We also consider\nthe use of Vector Quantization (VQ) for the CT reconstruction so that the\nmemory usage can be reduced, maintaining the same visual image quality. To the\nbest of our knowledge no work has been done before that explores the Vector\nQuantization for this purpose. Although our findings show that our approaches\noutperform the previous works, still there is a lot of room for improvement.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 07:25:03 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2019 12:19:55 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Sinha", "Ashish", ""], ["Sugawara", "Yohei", ""], ["Hirano", "Yuichiro", ""]]}, {"id": "1909.12537", "submitter": "Hugo Richard", "authors": "Hugo Richard, Lucas Martin, Ana Lu{\\i}sa Pinho, Jonathan Pillow,\n  Bertrand Thirion", "title": "Fast shared response model for fMRI data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shared response model provides a simple but effective framework to\nanalyse fMRI data of subjects exposed to naturalistic stimuli. However when the\nnumber of subjects or runs is large, fitting the model requires a large amount\nof memory and computational power, which limits its use in practice. In this\nwork, we introduce the FastSRM algorithm that relies on an intermediate\natlas-based representation. It provides considerable speed-up in time and\nmemory usage, hence it allows easy and fast large-scale analysis of\nnaturalistic-stimulus fMRI data. Using four different datasets, we show that\nour method matches the performance of the original SRM algorithm while being\nabout 5x faster and 20x to 40x more memory efficient. Based on this\ncontribution, we use FastSRM to predict age from movie watching data on the\nCamCAN sample. Besides delivering accurate predictions (mean absolute error of\n7.5 years), FastSRM extracts topographic patterns that are predictive of age,\ndemonstrating that brain activity during free perception reflects age.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 07:46:28 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 16:20:24 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Richard", "Hugo", ""], ["Martin", "Lucas", ""], ["Pinho", "Ana Lu\u0131sa", ""], ["Pillow", "Jonathan", ""], ["Thirion", "Bertrand", ""]]}, {"id": "1909.12573", "submitter": "Atsuhiro Noguchi", "authors": "Atsuhiro Noguchi, Tatsuya Harada", "title": "RGBD-GAN: Unsupervised 3D Representation Learning From Natural Image\n  Datasets via RGBD Image Synthesis", "comments": "21 pages, ICLR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding three-dimensional (3D) geometries from two-dimensional (2D)\nimages without any labeled information is promising for understanding the real\nworld without incurring annotation cost. We herein propose a novel generative\nmodel, RGBD-GAN, which achieves unsupervised 3D representation learning from 2D\nimages. The proposed method enables camera parameter-conditional image\ngeneration and depth image generation without any 3D annotations, such as\ncamera poses or depth. We use an explicit 3D consistency loss for two RGBD\nimages generated from different camera parameters, in addition to the ordinal\nGAN objective. The loss is simple yet effective for any type of image generator\nsuch as DCGAN and StyleGAN to be conditioned on camera parameters. Through\nexperiments, we demonstrated that the proposed method could learn 3D\nrepresentations from 2D images with various generator architectures.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 09:10:12 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 00:18:28 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Noguchi", "Atsuhiro", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1909.12579", "submitter": "Yulong Wang", "authors": "Yulong Wang, Xiaolu Zhang, Lingxi Xie, Jun Zhou, Hang Su, Bo Zhang,\n  Xiaolin Hu", "title": "Pruning from Scratch", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Network pruning is an important research field aiming at reducing\ncomputational costs of neural networks. Conventional approaches follow a fixed\nparadigm which first trains a large and redundant network, and then determines\nwhich units (e.g., channels) are less important and thus can be removed. In\nthis work, we find that pre-training an over-parameterized model is not\nnecessary for obtaining the target pruned structure. In fact, a fully-trained\nover-parameterized model will reduce the search space for the pruned structure.\nWe empirically show that more diverse pruned structures can be directly pruned\nfrom randomly initialized weights, including potential models with better\nperformance. Therefore, we propose a novel network pruning pipeline which\nallows pruning from scratch. In the experiments for compressing classification\nmodels on CIFAR10 and ImageNet datasets, our approach not only greatly reduces\nthe pre-training burden of traditional pruning methods, but also achieves\nsimilar or even higher accuracy under the same computation budgets. Our results\nfacilitate the community to rethink the effectiveness of existing techniques\nused for network pruning.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 09:38:31 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Wang", "Yulong", ""], ["Zhang", "Xiaolu", ""], ["Xie", "Lingxi", ""], ["Zhou", "Jun", ""], ["Su", "Hang", ""], ["Zhang", "Bo", ""], ["Hu", "Xiaolin", ""]]}, {"id": "1909.12601", "submitter": "Kashif Ahmad Dr", "authors": "Naina Said, Kashif Ahmad, Nicola Conci, Ala Al-Fuqaha", "title": "Active Learning for Event Detection in Support of Disaster Analysis\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disaster analysis in social media content is one of the interesting research\ndomains having abundance of data. However, there is a lack of labeled data that\ncan be used to train machine learning models for disaster analysis\napplications. Active learning is one of the possible solutions to such problem.\nTo this aim, in this paper we propose and assess the efficacy of an active\nlearning based framework for disaster analysis using images shared on social\nmedia outlets. Specifically, we analyze the performance of different active\nlearning techniques employing several sampling and disagreement strategies.\nMoreover, we collect a large-scale dataset covering images from eight common\ntypes of natural disasters. The experimental results show that the use of\nactive learning techniques for disaster analysis using images results in a\nperformance comparable to that obtained using human annotated images, and could\nbe used in frameworks for disaster analysis in images without tedious job of\nmanual annotation.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 10:28:10 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Said", "Naina", ""], ["Ahmad", "Kashif", ""], ["Conci", "Nicola", ""], ["Al-Fuqaha", "Ala", ""]]}, {"id": "1909.12605", "submitter": "Zhongdao Wang", "authors": "Zhongdao Wang, Liang Zheng, Yixuan Liu, Yali Li, Shengjin Wang", "title": "Towards Real-Time Multi-Object Tracking", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern multiple object tracking (MOT) systems usually follow the\n\\emph{tracking-by-detection} paradigm. It has 1) a detection model for target\nlocalization and 2) an appearance embedding model for data association. Having\nthe two models separately executed might lead to efficiency problems, as the\nrunning time is simply a sum of the two steps without investigating potential\nstructures that can be shared between them. Existing research efforts on\nreal-time MOT usually focus on the association step, so they are essentially\nreal-time association methods but not real-time MOT system. In this paper, we\npropose an MOT system that allows target detection and appearance embedding to\nbe learned in a shared model. Specifically, we incorporate the appearance\nembedding model into a single-shot detector, such that the model can\nsimultaneously output detections and the corresponding embeddings. We further\npropose a simple and fast association method that works in conjunction with the\njoint model. In both components the computation cost is significantly reduced\ncompared with former MOT systems, resulting in a neat and fast baseline for\nfuture follow-ups on real-time MOT algorithm design. To our knowledge, this\nwork reports the first (near) real-time MOT system, with a running speed of 22\nto 40 FPS depending on the input resolution. Meanwhile, its tracking accuracy\nis comparable to the state-of-the-art trackers embodying separate detection and\nembedding (SDE) learning ($64.4\\%$ MOTA \\vs $66.1\\%$ MOTA on MOT-16 challenge).\nCode and models are available at\n\\url{https://github.com/Zhongdao/Towards-Realtime-MOT}.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 10:35:23 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 08:22:46 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Wang", "Zhongdao", ""], ["Zheng", "Liang", ""], ["Liu", "Yixuan", ""], ["Li", "Yali", ""], ["Wang", "Shengjin", ""]]}, {"id": "1909.12612", "submitter": "Bisser Raytchev", "authors": "Shohei Hayashi and Bisser Raytchev and Toru Tamaki and Kazufumi Kaneda", "title": "Biomedical Image Segmentation by Retina-like Sequential Attention\n  Mechanism Using Only A Few Training Images", "comments": "Submitted to MLMI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel deep learning-based algorithm for biomedical\nimage segmentation which uses a sequential attention mechanism able to shift\nthe focus of attention across the image in a selective way, allowing subareas\nwhich are more difficult to classify to be processed at increased resolution.\nThe spatial distribution of class information in each subarea is learned using\na retina-like representation where resolution decreases with distance from the\ncenter of attention. The final segmentation is achieved by averaging class\npredictions over overlapping subareas, utilizing the power of ensemble learning\nto increase segmentation accuracy. Experimental results for semantic\nsegmentation task for which only a few training images are available show that\na CNN using the proposed method outperforms both a patch-based classification\nCNN and a fully convolutional-based method.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 10:55:24 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Hayashi", "Shohei", ""], ["Raytchev", "Bisser", ""], ["Tamaki", "Toru", ""], ["Kaneda", "Kazufumi", ""]]}, {"id": "1909.12638", "submitter": "Jinchen Xuan", "authors": "Jinchen Xuan, Yunchang Yang, Ze Yang, Di He, Liwei Wang", "title": "On the Anomalous Generalization of GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models, especially Generative Adversarial Networks (GANs), have\nreceived significant attention recently. However, it has been observed that in\nterms of some attributes, e.g. the number of simple geometric primitives in an\nimage, GANs are not able to learn the target distribution in practice.\nMotivated by this observation, we discover two specific problems of GANs\nleading to anomalous generalization behaviour, which we refer to as the sample\ninsufficiency and the pixel-wise combination. For the first problem of sample\ninsufficiency, we show theoretically and empirically that the batchsize of the\ntraining samples in practice may be insufficient for the discriminator to learn\nan accurate discrimination function. It could result in unstable training\ndynamics for the generator, leading to anomalous generalization. For the second\nproblem of pixel-wise combination, we find that besides recognizing the\npositive training samples as real, under certain circumstances, the\ndiscriminator could be fooled to recognize the pixel-wise combinations (e.g.\npixel-wise average) of the positive training samples as real. However, those\ncombinations could be visually different from the real samples in the target\ndistribution. With the fooled discriminator as reference, the generator would\nobtain biased supervision further, leading to the anomalous generalization\nbehaviour. Additionally, in this paper, we propose methods to mitigate the\nanomalous generalization of GANs. Extensive experiments on benchmark show our\nproposed methods improve the FID score up to 30\\% on natural image dataset.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 12:00:41 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 06:27:06 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Xuan", "Jinchen", ""], ["Yang", "Yunchang", ""], ["Yang", "Ze", ""], ["He", "Di", ""], ["Wang", "Liwei", ""]]}, {"id": "1909.12663", "submitter": "Mingtao Feng", "authors": "Mingtao Feng, Liang Zhang, Xuefei Lin, Syed Zulqarnain Gilani and\n  Ajmal Mian", "title": "Point Attention Network for Semantic Segmentation of 3D Point Clouds", "comments": "Submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have performed extremely well on data\nrepresented by regularly arranged grids such as images. However, directly\nleveraging the classic convolution kernels or parameter sharing mechanisms on\nsparse 3D point clouds is inefficient due to their irregular and unordered\nnature. We propose a point attention network that learns rich local shape\nfeatures and their contextual correlations for 3D point cloud semantic\nsegmentation. Since the geometric distribution of the neighboring points is\ninvariant to the point ordering, we propose a Local Attention-Edge Convolution\n(LAE Conv) to construct a local graph based on the neighborhood points searched\nin multi-directions. We assign attention coefficients to each edge and then\naggregate the point features as a weighted sum of its neighbors. The learned\nLAE-Conv layer features are then given to a point-wise spatial attention module\nto generate an interdependency matrix of all points regardless of their\ndistances, which captures long-range spatial contextual features contributing\nto more precise semantic information. The proposed point attention network\nconsists of an encoder and decoder which, together with the LAE-Conv layers and\nthe point-wise spatial attention modules, make it an end-to-end trainable\nnetwork for predicting dense labels for 3D point cloud segmentation.\nExperiments on challenging benchmarks of 3D point clouds show that our\nalgorithm can perform at par or better than the existing state of the art\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 13:00:03 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Feng", "Mingtao", ""], ["Zhang", "Liang", ""], ["Lin", "Xuefei", ""], ["Gilani", "Syed Zulqarnain", ""], ["Mian", "Ajmal", ""]]}, {"id": "1909.12673", "submitter": "Jonathan Rosenfeld", "authors": "Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, Nir Shavit", "title": "A Constructive Prediction of the Generalization Error Across Scales", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dependency of the generalization error of neural networks on model and\ndataset size is of critical importance both in practice and for understanding\nthe theory of neural networks. Nevertheless, the functional form of this\ndependency remains elusive. In this work, we present a functional form which\napproximates well the generalization error in practice. Capitalizing on the\nsuccessful concept of model scaling (e.g., width, depth), we are able to\nsimultaneously construct such a form and specify the exact models which can\nattain it across model/data scales. Our construction follows insights obtained\nfrom observations conducted over a range of model/data scales, in various model\ntypes and datasets, in vision and language tasks. We show that the form both\nfits the observations well across scales, and provides accurate predictions\nfrom small- to large-scale models and data.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 13:27:53 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 18:20:34 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Rosenfeld", "Jonathan S.", ""], ["Rosenfeld", "Amir", ""], ["Belinkov", "Yonatan", ""], ["Shavit", "Nir", ""]]}, {"id": "1909.12734", "submitter": "Praneeth Vepakomma", "authors": "Indu Ilanchezian, Praneeth Vepakomma, Abhishek Singh, Otkrist Gupta,\n  G. N. Srinivasa Prasanna, Ramesh Raskar", "title": "Maximal adversarial perturbations for obfuscation: Hiding certain\n  attributes while preserving rest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the usage of adversarial perturbations for the\npurpose of privacy from human perception and model (machine) based detection.\nWe employ adversarial perturbations for obfuscating certain variables in raw\ndata while preserving the rest. Current adversarial perturbation methods are\nused for data poisoning with minimal perturbations of the raw data such that\nthe machine learning model's performance is adversely impacted while the human\nvision cannot perceive the difference in the poisoned dataset due to minimal\nnature of perturbations. We instead apply relatively maximal perturbations of\nraw data to conditionally damage model's classification of one attribute while\npreserving the model performance over another attribute. In addition, the\nmaximal nature of perturbation helps adversely impact human perception in\nclassifying hidden attribute apart from impacting model performance. We\nvalidate our result qualitatively by showing the obfuscated dataset and\nquantitatively by showing the inability of models trained on clean data to\npredict the hidden attribute from the perturbed dataset while being able to\npredict the rest of attributes.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 15:08:46 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Ilanchezian", "Indu", ""], ["Vepakomma", "Praneeth", ""], ["Singh", "Abhishek", ""], ["Gupta", "Otkrist", ""], ["Prasanna", "G. N. Srinivasa", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1909.12743", "submitter": "Reza Bahmanyar", "authors": "Reza Bahmanyar, Elenora Vig, and Peter Reinartz", "title": "MRCNet: Crowd Counting and Density Map Estimation in Aerial and Ground\n  Imagery", "comments": null, "journal-ref": "BMVC Workshop on Object Detection and Recognition for Security\n  Screenin (BMVC-ODRSS) 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the many advantages of aerial imagery for crowd monitoring and\nmanagement at mass events, datasets of aerial images of crowds are still\nlacking in the field. As a remedy, in this work we introduce a novel crowd\ndataset, the DLR Aerial Crowd Dataset (DLR-ACD), which is composed of 33 large\naerial images acquired from 16 flight campaigns over mass events with 226,291\npersons annotated. To the best of our knowledge, DLR-ACD is the first aerial\ncrowd dataset and will be released publicly. To tackle the problem of accurate\ncrowd counting and density map estimation in aerial images of crowds, this work\nalso proposes a new encoder-decoder convolutional neural network, the so-called\nMulti-Resolution Crowd Network MRCNet. The encoder is based on the VGG-16\nnetwork and the decoder is composed of a set of bilinear upsampling and\nconvolutional layers. Using two losses, one at an earlier level and another at\nthe last level of the decoder, MRCNet estimates crowd counts and\nhigh-resolution crowd density maps as two different but interrelated tasks. In\naddition, MRCNet utilizes contextual and detailed local information by\ncombining high- and low-level features through a number of lateral connections\ninspired by the Feature Pyramid Network (FPN) technique. We evaluated MRCNet on\nthe proposed DLR-ACD dataset as well as on the ShanghaiTech dataset, a\nCCTV-based crowd counting benchmark. The results demonstrate that MRCNet\noutperforms the state-of-the-art crowd counting methods in estimating the crowd\ncounts and density maps for both aerial and CCTV-based images.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 15:22:23 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Bahmanyar", "Reza", ""], ["Vig", "Elenora", ""], ["Reinartz", "Peter", ""]]}, {"id": "1909.12761", "submitter": "Yaadhav Raaj", "authors": "Yaadhav Raaj", "title": "Exploring Pose Priors for Human Pose Estimation with Joint Angle\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose Priors are critical in human pose estimation, since they are able to\nenforce constraints that prevent estimated poses from tending to physically\nimpossible positions. Human pose generally consists of up to 22 Joint Angles of\nvarious segments, and their respective bone lengths, but the way these various\nsegments interact can affect the validity of a pose. Looking at the Knee-Ankle\nsegment alone, we can observe that clearly, the Knee cannot bend forward beyond\nit's roughly 90 degree point, amongst various other impossible poses below.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 16:01:10 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Raaj", "Yaadhav", ""]]}, {"id": "1909.12775", "submitter": "Ester Hait-Fraenkel", "authors": "Ester Hait-Fraenkel and Guy Gilboa", "title": "Revealing Stable and Unstable Modes of Generic Denoisers through\n  Nonlinear Eigenvalue Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to analyze stable and unstable modes of generic\nimage denoisers through nonlinear eigenvalue analysis. We attempt to find input\nimages for which the output of a black-box denoiser is proportional to the\ninput. We treat this as a nonlinear eigenvalue problem. This has potentially\nwide implications, since most image processing algorithms can be viewed as\ngeneric nonlinear operators. We introduce a generalized nonlinear power-method\nto solve eigenproblems for such black-box operators. Using this method we\nreveal stable modes of nonlinear denoisers. These modes are optimal inputs for\nthe denoiser, achieving superior PSNR in noise removal. Analogously to the\nlinear case (low-pass-filter), such stable modes are eigenfunctions\ncorresponding to large eigenvalues, characterized by large piece-wise-smooth\nstructures. We also provide a method to generate the complementary, most\nunstable modes, which the denoiser suppresses strongly. These modes are\ntextures with small eigenvalues. We validate the method using total-variation\n(TV) and demonstrate it on the EPLL denoiser (Zoran-Weiss). Finally, we suggest\nan encryption-decryption application.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 15:01:48 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 08:34:42 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 08:56:03 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Hait-Fraenkel", "Ester", ""], ["Gilboa", "Guy", ""]]}, {"id": "1909.12778", "submitter": "Xiaohan Ding", "authors": "Xiaohan Ding, Guiguang Ding, Xiangxin Zhou, Yuchen Guo, Jungong Han,\n  Ji Liu", "title": "Global Sparse Momentum SGD for Pruning Very Deep Neural Networks", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network (DNN) is powerful but computationally expensive and\nmemory intensive, thus impeding its practical usage on resource-constrained\nfront-end devices. DNN pruning is an approach for deep model compression, which\naims at eliminating some parameters with tolerable performance degradation. In\nthis paper, we propose a novel momentum-SGD-based optimization method to reduce\nthe network complexity by on-the-fly pruning. Concretely, given a global\ncompression ratio, we categorize all the parameters into two parts at each\ntraining iteration which are updated using different rules. In this way, we\ngradually zero out the redundant parameters, as we update them using only the\nordinary weight decay but no gradients derived from the objective function. As\na departure from prior methods that require heavy human works to tune the\nlayer-wise sparsity ratios, prune by solving complicated non-differentiable\nproblems or finetune the model after pruning, our method is characterized by 1)\nglobal compression that automatically finds the appropriate per-layer sparsity\nratios; 2) end-to-end training; 3) no need for a time-consuming re-training\nprocess after pruning; and 4) superior capability to find better winning\ntickets which have won the initialization lottery.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 16:24:19 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 17:21:53 GMT"}, {"version": "v3", "created": "Fri, 25 Oct 2019 15:39:02 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Ding", "Xiaohan", ""], ["Ding", "Guiguang", ""], ["Zhou", "Xiangxin", ""], ["Guo", "Yuchen", ""], ["Han", "Jungong", ""], ["Liu", "Ji", ""]]}, {"id": "1909.12780", "submitter": "Givi Meishvili", "authors": "Givi Meishvili, Simon Jenni, Paolo Favaro", "title": "Learning to Have an Ear for Face Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method to use both audio and a low-resolution image to\nperform extreme face super-resolution (a 16x increase of the input size). When\nthe resolution of the input image is very low (e.g., 8x8 pixels), the loss of\ninformation is so dire that important details of the original identity have\nbeen lost and audio can aid the recovery of a plausible high-resolution image.\nIn fact, audio carries information about facial attributes, such as gender and\nage. To combine the aural and visual modalities, we propose a method to first\nbuild the latent representations of a face from the lone audio track and then\nfrom the lone low-resolution image. We then train a network to fuse these two\nrepresentations. We show experimentally that audio can assist in recovering\nattributes such as the gender, the age and the identity, and thus improve the\ncorrectness of the high-resolution image reconstruction process. Our procedure\ndoes not make use of human annotation and thus can be easily trained with\nexisting video datasets. Moreover, we show that our model builds a factorized\nrepresentation of images and audio as it allows one to mix low-resolution\nimages and audio from different videos and to generate realistic faces with\nsemantically meaningful combinations.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 16:28:55 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 16:00:13 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2020 16:14:12 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Meishvili", "Givi", ""], ["Jenni", "Simon", ""], ["Favaro", "Paolo", ""]]}, {"id": "1909.12828", "submitter": "Nikos Kolotouros", "authors": "Nikos Kolotouros, Georgios Pavlakos, Michael J. Black, Kostas\n  Daniilidis", "title": "Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the\n  Loop", "comments": "To appear at ICCV 2019. Project page:\n  https://seas.upenn.edu/~nkolot/projects/spin", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based human pose estimation is currently approached through two\ndifferent paradigms. Optimization-based methods fit a parametric body model to\n2D observations in an iterative manner, leading to accurate image-model\nalignments, but are often slow and sensitive to the initialization. In\ncontrast, regression-based methods, that use a deep network to directly\nestimate the model parameters from pixels, tend to provide reasonable, but not\npixel accurate, results while requiring huge amounts of supervision. In this\nwork, instead of investigating which approach is better, our key insight is\nthat the two paradigms can form a strong collaboration. A reasonable, directly\nregressed estimate from the network can initialize the iterative optimization\nmaking the fitting faster and more accurate. Similarly, a pixel accurate fit\nfrom iterative optimization can act as strong supervision for the network. This\nis the core of our proposed approach SPIN (SMPL oPtimization IN the loop). The\ndeep network initializes an iterative optimization routine that fits the body\nmodel to 2D joints within the training loop, and the fitted estimate is\nsubsequently used to supervise the network. Our approach is self-improving by\nnature, since better network estimates can lead the optimization to better\nsolutions, while more accurate optimization fits provide better supervision for\nthe network. We demonstrate the effectiveness of our approach in different\nsettings, where 3D ground truth is scarce, or not available, and we\nconsistently outperform the state-of-the-art model-based pose estimation\napproaches by significant margins. The project website with videos, results,\nand code can be found at https://seas.upenn.edu/~nkolot/projects/spin.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 17:56:35 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Kolotouros", "Nikos", ""], ["Pavlakos", "Georgios", ""], ["Black", "Michael J.", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1909.12837", "submitter": "Andrei Cramariuc", "authors": "Renaud Dub\\'e, Andrei Cramariuc, Daniel Dugas, Hannes Sommer, Marcin\n  Dymczyk, Juan Nieto, Roland Siegwart, and Cesar Cadena", "title": "SegMap: Segment-based mapping and localization using data-driven\n  descriptors", "comments": "arXiv admin note: substantial text overlap with arXiv:1804.09557", "journal-ref": null, "doi": "10.1177/0278364919863090", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precisely estimating a robot's pose in a prior, global map is a fundamental\ncapability for mobile robotics, e.g. autonomous driving or exploration in\ndisaster zones. This task, however, remains challenging in unstructured,\ndynamic environments, where local features are not discriminative enough and\nglobal scene descriptors only provide coarse information. We therefore present\nSegMap: a map representation solution for localization and mapping based on the\nextraction of segments in 3D point clouds. Working at the level of segments\noffers increased invariance to view-point and local structural changes, and\nfacilitates real-time processing of large-scale 3D data. SegMap exploits a\nsingle compact data-driven descriptor for performing multiple tasks: global\nlocalization, 3D dense map reconstruction, and semantic information extraction.\nThe performance of SegMap is evaluated in multiple urban driving and search and\nrescue experiments. We show that the learned SegMap descriptor has superior\nsegment retrieval capabilities, compared to state-of-the-art handcrafted\ndescriptors. In consequence, we achieve a higher localization accuracy and a 6%\nincrease in recall over state-of-the-art. These segment-based localizations\nallow us to reduce the open-loop odometry drift by up to 50%. SegMap is\nopen-source available along with easy to run demonstrations.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 16:02:02 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Dub\u00e9", "Renaud", ""], ["Cramariuc", "Andrei", ""], ["Dugas", "Daniel", ""], ["Sommer", "Hannes", ""], ["Dymczyk", "Marcin", ""], ["Nieto", "Juan", ""], ["Siegwart", "Roland", ""], ["Cadena", "Cesar", ""]]}, {"id": "1909.12887", "submitter": "Donglai Wei Mr.", "authors": "Abhimanyu Talwar, Zudi Lin, Donglai Wei, Yuesong Wu, Bowen Zheng,\n  Jinglin Zhao, Won-Dong Jang, Xueying Wang, Jeff W. Lichtman, and Hanspeter\n  Pfister", "title": "A Topological Nomenclature for 3D Shape Analysis in Connectomics", "comments": "Technical report", "journal-ref": "Computer Vision for Microscopy Image Analysis: CVPR2020 workshop", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the essential tasks in connectomics is the morphology analysis of\nneurons and organelles like mitochondria to shed light on their biological\nproperties. However, these biological objects often have tangled parts or\ncomplex branching patterns, which make it hard to abstract, categorize, and\nmanipulate their morphology. In this paper, we develop a novel topological\nnomenclature system to name these objects like the appellation for chemical\ncompounds to promote neuroscience analysis based on their skeletal structures.\nWe first convert the volumetric representation into the topology-preserving\nreduced graph to untangle the objects. Next, we develop nomenclature rules for\npyramidal neurons and mitochondria from the reduced graph and finally learn the\nfeature embedding for shape manipulation. In ablation studies, we\nquantitatively show that graphs generated by our proposed method align with the\nperception of experts. On 3D shape retrieval and decomposition tasks, we\nqualitatively demonstrate that the encoded topological nomenclature features\nachieve better results than state-of-the-art shape descriptors. To advance\nneuroscience, we will release a 3D segmentation dataset of mitochondria and\npyramidal neurons reconstructed from a 100um cube electron microscopy volume\nwith their reduced graph and topological nomenclature annotations. Code is\npublicly available at https://github.com/donglaiw/ibexHelper.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 19:42:20 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 21:15:18 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Talwar", "Abhimanyu", ""], ["Lin", "Zudi", ""], ["Wei", "Donglai", ""], ["Wu", "Yuesong", ""], ["Zheng", "Bowen", ""], ["Zhao", "Jinglin", ""], ["Jang", "Won-Dong", ""], ["Wang", "Xueying", ""], ["Lichtman", "Jeff W.", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "1909.12901", "submitter": "Feifan Wang", "authors": "Feifan Wang, Runzhou Jiang, Liqin Zheng, Chun Meng, and Bharat Biswal", "title": "3D U-Net Based Brain Tumor Segmentation and Survival Days Prediction", "comments": "Third place award of the 2019 MICCAI BraTS challenge survival task\n  [BraTS 2019](https://www.med.upenn.edu/cbica/brats2019.html)", "journal-ref": null, "doi": "10.1007/978-3-030-46640-4_13", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Past few years have witnessed the prevalence of deep learning in many\napplication scenarios, among which is medical image processing. Diagnosis and\ntreatment of brain tumors requires an accurate and reliable segmentation of\nbrain tumors as a prerequisite. However, such work conventionally requires\nbrain surgeons significant amount of time. Computer vision techniques could\nprovide surgeons a relief from the tedious marking procedure. In this paper, a\n3D U-net based deep learning model has been trained with the help of brain-wise\nnormalization and patching strategies for the brain tumor segmentation task in\nthe BraTS 2019 competition. Dice coefficients for enhancing tumor, tumor core,\nand the whole tumor are 0.737, 0.807 and 0.894 respectively on the validation\ndataset. These three values on the test dataset are 0.778, 0.798 and 0.852.\nFurthermore, numerical features including ratio of tumor size to brain size and\nthe area of tumor surface as well as age of subjects are extracted from\npredicted tumor labels and have been used for the overall survival days\nprediction task. The accuracy could be 0.448 on the validation dataset, and\n0.551 on the final test dataset.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 15:59:37 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 14:16:41 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Wang", "Feifan", ""], ["Jiang", "Runzhou", ""], ["Zheng", "Liqin", ""], ["Meng", "Chun", ""], ["Biswal", "Bharat", ""]]}, {"id": "1909.12902", "submitter": "Denys Dutykh", "authors": "Beno\\^it Colange and Laurent Vuillon and Sylvain Lespinats and Denys\n  Dutykh", "title": "Interpreting Distortions in Dimensionality Reduction by Superimposing\n  Neighbourhood Graphs", "comments": "5 pages, 6 figures, 22 references. Paper presented at IEEE VIS 2019\n  Conference. Other author's papers can be downloaded at\n  http://www.denys-dutykh.com/", "journal-ref": "Paper presented at IEEE Vis 2019 conference at Vancouver, Canada", "doi": "10.1109/VISUAL.2019.8933568", "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To perform visual data exploration, many dimensionality reduction methods\nhave been developed. These tools allow data analysts to represent\nmultidimensional data in a 2D or 3D space, while preserving as much relevant\ninformation as possible. Yet, they cannot preserve all structures\nsimultaneously and they induce some unavoidable distortions. Hence, many\ncriteria have been introduced to evaluate a map's overall quality, mostly based\non the preservation of neighbourhoods. Such global indicators are currently\nused to compare several maps, which helps to choose the most appropriate\nmapping method and its hyperparameters. However, those aggregated indicators\ntend to hide the local repartition of distortions. Thereby, they need to be\nsupplemented by local evaluation to ensure correct interpretation of maps. In\nthis paper, we describe a new method, called MING, for `Map Interpretation\nusing Neighbourhood Graphs'. It offers a graphical interpretation of pairs of\nmap quality indicators, as well as local evaluation of the distortions. This is\ndone by displaying on the map the nearest neighbours graphs computed in the\ndata space and in the embedding. Shared and unshared edges exhibit reliable and\nunreliable neighbourhood information conveyed by the mapping. By this mean,\nanalysts may determine whether proximity (or remoteness) of points on the map\nfaithfully represents similarity (or dissimilarity) of original data, within\nthe meaning of a chosen map quality criteria. We apply this approach to two\npairs of widespread indicators: precision/recall and\ntrustworthiness/continuity, chosen for their wide use in the community, which\nwill allow an easy handling by users.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 07:48:26 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Colange", "Beno\u00eet", ""], ["Vuillon", "Laurent", ""], ["Lespinats", "Sylvain", ""], ["Dutykh", "Denys", ""]]}, {"id": "1909.12906", "submitter": "Karol Arndt", "authors": "Karol Arndt, Murtaza Hazara, Ali Ghadirzadeh, Ville Kyrki", "title": "Meta Reinforcement Learning for Sim-to-real Domain Adaptation", "comments": "Submitted to ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern reinforcement learning methods suffer from low sample efficiency and\nunsafe exploration, making it infeasible to train robotic policies entirely on\nreal hardware. In this work, we propose to address the problem of sim-to-real\ndomain transfer by using meta learning to train a policy that can adapt to a\nvariety of dynamic conditions, and using a task-specific trajectory generation\nmodel to provide an action space that facilitates quick exploration. We\nevaluate the method by performing domain adaptation in simulation and analyzing\nthe structure of the latent space during adaptation. We then deploy this policy\non a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a\nhockey puck to a target. Our method shows more consistent and stable domain\nadaptation than the baseline, resulting in better overall performance.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 11:59:40 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Arndt", "Karol", ""], ["Hazara", "Murtaza", ""], ["Ghadirzadeh", "Ali", ""], ["Kyrki", "Ville", ""]]}, {"id": "1909.12907", "submitter": "Xiaoyang Guo", "authors": "Xiaoyang Guo, Anuj Srivastava, Sudeep Sarkar", "title": "A Quotient Space Formulation for Generative Statistical Analysis of\n  Graphical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complex analyses involving multiple, dependent random quantities often lead\nto graphical models - a set of nodes denoting variables of interest, and\ncorresponding edges denoting statistical interactions between nodes. To develop\nstatistical analyses for graphical data, especially towards generative\nmodeling, one needs mathematical representations and metrics for matching and\ncomparing graphs, and subsequent tools, such as geodesics, means, and\ncovariances. This paper utilizes a quotient structure to develop efficient\nalgorithms for computing these quantities, leading to useful statistical tools,\nincluding principal component analysis, statistical testing, and modeling. We\ndemonstrate the efficacy of this framework using datasets taken from several\nproblem areas, including letters, biochemical structures, and social networks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 13:29:04 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 01:33:17 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Guo", "Xiaoyang", ""], ["Srivastava", "Anuj", ""], ["Sarkar", "Sudeep", ""]]}, {"id": "1909.12908", "submitter": "Jens Lundell", "authors": "Jens Lundell, Francesco Verdoja, Ville Kyrki", "title": "Beyond Top-Grasps Through Scene Completion", "comments": "Accepted to 2020 IEEE Conference on Robotics and Automation (ICRA)", "journal-ref": "2020 IEEE International Conference on Robotics and Automation\n  (ICRA), Paris, France, 2020, pp. 545-551", "doi": "10.1109/ICRA40945.2020.9197320", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current end-to-end grasp planning methods propose grasps in the order of\nseconds that attain high grasp success rates on a diverse set of objects, but\noften by constraining the workspace to top-grasps. In this work, we present a\nmethod that allows end-to-end top-grasp planning methods to generate full\nsix-degree-of-freedom grasps using a single RGB-D view as input. This is\nachieved by estimating the complete shape of the object to be grasped, then\nsimulating different viewpoints of the object, passing the simulated viewpoints\nto an end-to-end grasp generation method, and finally executing the overall\nbest grasp. The method was experimentally validated on a Franka Emika Panda by\ncomparing 429 grasps generated by the state-of-the-art Fully Convolutional\nGrasp Quality CNN, both on simulated and real camera images. The results show\nstatistically significant improvements in terms of grasp success rate when\nusing simulated images over real camera images, especially when the real camera\nviewpoint is angled. Code and video are available at\nhttps://irobotics.aalto.fi/beyond-top-grasps-through-scene-completion/.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 15:12:14 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 18:31:03 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Lundell", "Jens", ""], ["Verdoja", "Francesco", ""], ["Kyrki", "Ville", ""]]}, {"id": "1909.12909", "submitter": "Yubao Liu", "authors": "Yubao Liu, Kai Lin", "title": "Deeply Matting-based Dual Generative Adversarial Network for Image and\n  Document Label Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many methods have been proposed to deal with nature image\nsuper-resolution (SR) and get impressive performance, the text images SR is not\ngood due to their ignorance of document images. In this paper, we propose a\nmatting-based dual generative adversarial network (mdGAN) for document image\nSR. Firstly, the input image is decomposed into document text, foreground and\nbackground layers using deep image matting. Then two parallel branches are\nconstructed to recover text boundary information and color information\nrespectively. Furthermore, in order to improve the restoration accuracy of\ncharacters in output image, we use the input image's corresponding ground truth\ntext label as extra supervise information to refine the two-branch networks\nduring training. Experiments on real text images demonstrate that our method\noutperforms several state-of-the-art methods quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 18:15:10 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Liu", "Yubao", ""], ["Lin", "Kai", ""]]}, {"id": "1909.12911", "submitter": "Luisa Polania", "authors": "Xin Guo, Luisa F. Polania, Bin Zhu, Charles Boncelet, Kenneth E.\n  Barner", "title": "Graph Neural Networks for Image Understanding Based on Multiple Cues:\n  Group Emotion Recognition and Event Recognition as Use Cases", "comments": "Paper accepted for publication at the 2020 IEEE Winter Conference on\n  Applications of Computer Vision (WACV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph neural network (GNN) for image understanding based on multiple cues\nis proposed in this paper. Compared to traditional feature and decision fusion\napproaches that neglect the fact that features can interact and exchange\ninformation, the proposed GNN is able to pass information among features\nextracted from different models. Two image understanding tasks, namely\ngroup-level emotion recognition (GER) and event recognition, which are highly\nsemantic and require the interaction of several deep models to synthesize\nmultiple cues, were selected to validate the performance of the proposed\nmethod. It is shown through experiments that the proposed method achieves\nstate-of-the-art performance on the selected image understanding tasks. In\naddition, a new group-level emotion recognition database is introduced and\nshared in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 00:22:36 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 05:34:17 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Guo", "Xin", ""], ["Polania", "Luisa F.", ""], ["Zhu", "Bin", ""], ["Boncelet", "Charles", ""], ["Barner", "Kenneth E.", ""]]}, {"id": "1909.12912", "submitter": "Andre Pacheco", "authors": "Andre G. C. Pacheco and Renato A. Krohling", "title": "The impact of patient clinical information on automated skin cancer\n  detection", "comments": null, "journal-ref": null, "doi": "10.1016/j.compbiomed.2019.103545", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin cancer is one of the most common types of cancer around the world. For\nthis reason, over the past years, different approaches have been proposed to\nassist detect it. Nonetheless, most of them are based only on dermoscopy images\nand do not take into account the patient clinical information. In this work,\nfirst, we present a new dataset that contains clinical images, acquired from\nsmartphones, and patient clinical information of the skin lesions. Next, we\nintroduce a straightforward approach to combine the clinical data and the\nimages using different well-known deep learning models. These models are\napplied to the presented dataset using only the images and combining them with\nthe patient clinical information. We present a comprehensive study to show the\nimpact of the clinical data on the final predictions. The results obtained by\ncombining both sets of information show a general improvement of around 7% in\nthe balanced accuracy for all models. In addition, the statistical test\nindicates significant differences between the models with and without\nconsidering both data. The improvement achieved shows the potential of using\npatient clinical information in skin cancer detection and indicates that this\npiece of information is important to leverage skin cancer detection systems.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 14:27:12 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Pacheco", "Andre G. C.", ""], ["Krohling", "Renato A.", ""]]}, {"id": "1909.12913", "submitter": "Prabin Sharma", "authors": "Prabin Sharma, Shubham Joshi, Subash Gautam, Sneha Maharjan, Vitor\n  Filipe, Manuel J. C. S. Reis", "title": "Student Engagement Detection Using Emotion Analysis, Eye Tracking and\n  Head Movement with Machine Learning", "comments": "9 pages, 9 Figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the increase of distance learning, in general, and e-learning, in\nparticular, having a system capable of determining the engagement of students\nis of primordial importance, and one of the biggest challenges, both for\nteachers, researchers and policy makers. Here, we present a system to detect\nthe engagement level of the students. It uses only information provided by the\ntypical built-in web-camera present in a laptop computer, and was designed to\nwork in real time. We combine information about the movements of the eyes and\nhead, and facial emotions to produce a concentration index with three classes\nof engagement: \"very engaged\", \"nominally engaged\" and \"not engaged at all\".\nThe system was tested in a typical e-learning scenario, and the results show\nthat it correctly identifies each period of time where students were \"very\nengaged\", \"nominally engaged\" and \"not engaged at all\". Additionally, the\nresults also show that the students with best scores also have higher\nconcentration indexes.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 15:46:48 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 09:28:54 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 16:56:12 GMT"}, {"version": "v4", "created": "Sat, 26 Dec 2020 19:05:15 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Sharma", "Prabin", ""], ["Joshi", "Shubham", ""], ["Gautam", "Subash", ""], ["Maharjan", "Sneha", ""], ["Filipe", "Vitor", ""], ["Reis", "Manuel J. C. S.", ""]]}, {"id": "1909.12916", "submitter": "Lucas Pascal", "authors": "Lucas Pascal, Xavier Bost (LIA), Beno\\^it Huet", "title": "Semantic and Visual Similarities for Efficient Knowledge Transfer in CNN\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, representation learning approaches have disrupted many\nmultimedia computing tasks. Among those approaches, deep convolutional neural\nnetworks (CNNs) have notably reached human level expertise on some constrained\nimage classification tasks. Nonetheless, training CNNs from scratch for new\ntask or simply new data turns out to be complex and time-consuming. Recently,\ntransfer learning has emerged as an effective methodology for adapting\npre-trained CNNs to new data and classes, by only retraining the last\nclassification layer. This paper focuses on improving this process, in order to\nbetter transfer knowledge between CNN architectures for faster trainings in the\ncase of fine tuning for image classification. This is achieved by combining and\ntransfering supplementary weights, based on similarity considerations between\nsource and target classes. The study includes a comparison between semantic and\ncontent-based similarities, and highlights increased initial performances and\ntraining speed, along with superior long term performances when limited\ntraining samples are available.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 14:49:38 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Pascal", "Lucas", "", "LIA"], ["Bost", "Xavier", "", "LIA"], ["Huet", "Beno\u00eet", ""]]}, {"id": "1909.12917", "submitter": "Mansaf Alam Dr", "authors": "Preeti Agarwal and Mansaf Alam", "title": "A Lightweight Deep Learning Model for Human Activity Recognition on Edge\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Activity Recognition (HAR) using wearable and mobile sensors has gained\nmomentum in last few years, in various fields, such as, healthcare,\nsurveillance, education, entertainment. Nowadays, Edge Computing has emerged to\nreduce communication latency and network traffic.Edge devices are resource\nconstrained devices and cannot support high computation. In literature, various\nmodels have been developed for HAR. In recent years, deep learning algorithms\nhave shown high performance in HAR, but these algorithms require lot of\ncomputation making them inefficient to be deployed on edge devices. This paper,\nproposes a Lightweight Deep Learning Model for HAR requiring less computational\npower, making it suitable to be deployed on edge devices. The performance of\nproposed model is tested on the participants six daily activities data. Results\nshow that the proposed model outperforms many of the existing machine learning\nand deep learning techniques.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 10:16:34 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Agarwal", "Preeti", ""], ["Alam", "Mansaf", ""]]}, {"id": "1909.12919", "submitter": "Madhura Ingalhalikar", "authors": "Sumeet Shinde, Tanay Chougule, Jitender Saini and Madhura Ingalhalikar", "title": "HR-CAM: Precise Localization of Pathology Using Multi-level Learning in\n  CNNs", "comments": "Medical Image Computing and Computer Assisted Intervention, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a CNN based technique that aggregates feature maps from its\nmultiple layers that can localize abnormalities with greater details as well as\npredict pathology under consideration. Existing class activation mapping (CAM)\ntechniques extract feature maps from either the final layer or a single\nintermediate layer to create the discriminative maps and then interpolate to\nupsample to the original image resolution. In this case, the subject specific\nlocalization is coarse and is unable to capture subtle abnormalities. To\nmitigate this, our method builds a novel CNN based discriminative localization\nmodel that we call high resolution CAM (HR-CAM), which accounts for layers from\neach resolution, therefore facilitating a comprehensive map that can delineate\nthe pathology for each subject by combining low-level, intermediate as well as\nhigh-level features from the CNN. Moreover, our model directly provides the\ndiscriminative map in the resolution of the original image facilitating finer\ndelineation of abnormalities. We demonstrate the working of our model on a\nsimulated abnormalities data where we illustrate how the model captures finer\ndetails in the final discriminative maps as compared to current techniques. We\nthen apply this technique: (1) to classify ependymomas from grade IV\nglioblastoma on T1-weighted contrast enhanced (T1-CE) MRI and (2) to predict\nParkinson's disease from neuromelanin sensitive MRI. In all these cases we\ndemonstrate that our model not only predicts pathologies with high accuracies,\nbut also creates clinically interpretable subject specific high resolution\ndiscriminative localizations. Overall, the technique can be generalized to any\nCNN and carries high relevance in a clinical setting.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 13:47:12 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Shinde", "Sumeet", ""], ["Chougule", "Tanay", ""], ["Saini", "Jitender", ""], ["Ingalhalikar", "Madhura", ""]]}, {"id": "1909.12921", "submitter": "Benjamin Renoust", "authors": "Benjamin Renoust, Matheus Oliveira Franca, Jacob Chan, Noa Garcia, Van\n  Le, Ayaka Uesaka, Yuta Nakashima, Hajime Nagahara, Jueren Wang, Yutaka\n  Fujioka", "title": "Historical and Modern Features for Buddha Statue Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Buddhism has spread along the Silk Roads, many pieces of art have been\ndisplaced. Only a few experts may identify these works, subjectively to their\nexperience. The construction of Buddha statues was taught through the\ndefinition of canon rules, but the applications of those rules greatly varies\nacross time and space. Automatic art analysis aims at supporting these\nchallenges. We propose to automatically recover the proportions induced by the\nconstruction guidelines, in order to use them and compare between different\ndeep learning features for several classification tasks, in a medium size but\nrich dataset of Buddha statues, collected with experts of Buddhism art history.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 06:22:32 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 14:07:58 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Renoust", "Benjamin", ""], ["Franca", "Matheus Oliveira", ""], ["Chan", "Jacob", ""], ["Garcia", "Noa", ""], ["Le", "Van", ""], ["Uesaka", "Ayaka", ""], ["Nakashima", "Yuta", ""], ["Nagahara", "Hajime", ""], ["Wang", "Jueren", ""], ["Fujioka", "Yutaka", ""]]}, {"id": "1909.12922", "submitter": "Han Li", "authors": "Zeju Li, Han Li, Hu Han, Gonglei Shi, Jiannan Wang, and S. Kevin Zhou", "title": "Encoding CT Anatomy Knowledge for Unpaired Chest X-ray Image\n  Decomposition", "comments": "9 pages with 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although chest X-ray (CXR) offers a 2D projection with overlapped anatomies,\nit is widely used for clinical diagnosis. There is clinical evidence supporting\nthat decomposing an X-ray image into different components (e.g., bone, lung and\nsoft tissue) improves diagnostic value. We hereby propose a decomposition\ngenerative adversarial network (DecGAN) to anatomically decompose a CXR image\nbut with unpaired data. We leverage the anatomy knowledge embedded in CT, which\nfeatures a 3D volume with clearly visible anatomies. Our key idea is to embed\nCT priori decomposition knowledge into the latent space of unpaired CXR\nautoencoder. Specifically, we train DecGAN with a decomposition loss,\nadversarial losses, cycle-consistency losses and a mask loss to guarantee that\nthe decomposed results of the latent space preserve realistic body structures.\nExtensive experiments demonstrate that DecGAN provides superior unsupervised\nCXR bone suppression results and the feasibility of modulating CXR components\nby latent space disentanglement. Furthermore, we illustrate the diagnostic\nvalue of DecGAN and demonstrate that it outperforms the state-of-the-art\napproaches in terms of predicting 11 out of 14 common lung diseases.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 09:47:47 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Li", "Zeju", ""], ["Li", "Han", ""], ["Han", "Hu", ""], ["Shi", "Gonglei", ""], ["Wang", "Jiannan", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "1909.12923", "submitter": "Iv\\'an L\\'opez-Espejo", "authors": "Iv\\'an L\\'opez-Espejo", "title": "End-to-End Deep Residual Learning with Dilated Convolutions for\n  Myocardial Infarction Detection and Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, I investigate the use of end-to-end deep residual learning\nwith dilated convolutions for myocardial infarction (MI) detection and\nlocalization from electrocardiogram (ECG) signals. Although deep residual\nlearning has already been applied to MI detection and localization, I propose a\nmore accurate system that distinguishes among a higher number (i.e., six) of MI\nlocations. Inspired by speech waveform processing with neural networks, I found\na more robust front-end than directly arranging the multi-lead ECG signal into\nan input matrix consisting of the use of a single one-dimensional convolutional\nlayer per ECG lead to extract a pseudo-time-frequency representation and create\na compact and discriminative input feature volume. As a result, I end up with a\nsystem achieving an MI detection and localization accuracy of 99.99% on the\nwell-known Physikalisch-Technische Bundesanstalt (PTB) database.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 15:48:29 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["L\u00f3pez-Espejo", "Iv\u00e1n", ""]]}, {"id": "1909.12927", "submitter": "Basemah Alshemali", "authors": "Basemah Alshemali, Alta Graham, Jugal Kalita", "title": "Toward Robust Image Classification", "comments": "2019 Intelligent Systems Conference, pp 483-489", "journal-ref": "Intelligent Systems and Applications. IntelliSys 2019. Advances in\n  Intelligent Systems and Computing, vol 1038. Springer, Cham", "doi": "10.1007/978-3-030-29513-4", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are frequently used for image classification, but can be\nvulnerable to misclassification caused by adversarial images. Attempts to make\nneural network image classification more robust have included variations on\npreprocessing (cropping, applying noise, blurring), adversarial training, and\ndropout randomization. In this paper, we implemented a model for adversarial\ndetection based on a combination of two of these techniques: dropout\nrandomization with preprocessing applied to images within a given Bayesian\nuncertainty. We evaluated our model on the MNIST dataset, using adversarial\nimages generated using Fast Gradient Sign Method (FGSM), Jacobian-based\nSaliency Map Attack (JSMA) and Basic Iterative Method (BIM) attacks. Our model\nachieved an average adversarial image detection accuracy of 97%, with an\naverage image classification accuracy, after discarding images flagged as\nadversarial, of 99%. Our average detection accuracy exceeded that of recent\npapers using similar techniques.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 21:08:13 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Alshemali", "Basemah", ""], ["Graham", "Alta", ""], ["Kalita", "Jugal", ""]]}, {"id": "1909.12929", "submitter": "Yumeng Zhang", "authors": "Yumeng Zhang, Gaoguo Jia, Li Chen, Mingrui Zhang, Junhai Yong", "title": "Self-Paced Video Data Augmentation with Dynamic Images Generated by\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an urgent need for an effective video classification method by means\nof a small number of samples. The deficiency of samples could be effectively\nalleviated by generating samples through Generative Adversarial Networks (GAN),\nbut the generation of videos on a typical category remains to be underexplored\nsince the complex actions and the changeable viewpoints are difficult to\nsimulate. In this paper, we propose a generative data augmentation method for\ntemporal stream of the Temporal Segment Networks with the dynamic image. The\ndynamic image compresses the motion information of video into a still image,\nremoving the interference factors such as the background. Thus it is easier to\ngenerate images with categorical motion information using GAN. We use the\ngenerated dynamic images to enhance the features, with regularization achieved\nas well, thereby to achieve the effect of video augmentation. In order to deal\nwith the uneven quality of generated images, we propose a Self-Paced Selection\n(SPS) method, which automatically selects the high-quality generated samples to\nbe added to the network training. Our method is verified on two benchmark\ndatasets, HMDB51 and UCF101. The experimental results show that the method can\nimprove the accuracy of video classification under the circumstance of sample\ninsufficiency and sample imbalance.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 16:03:08 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Zhang", "Yumeng", ""], ["Jia", "Gaoguo", ""], ["Chen", "Li", ""], ["Zhang", "Mingrui", ""], ["Yong", "Junhai", ""]]}, {"id": "1909.12932", "submitter": "Benjamin Renoust", "authors": "Benjamin Renoust, Matheus Oliveira Franca, Jacob Chan, Van Le, Ayaka\n  Uesaka, Yuta Nakashima, Hajime Nagahara, Jueren Wang, Yutaka Fujioka", "title": "BUDA.ART: A Multimodal Content-Based Analysis and Retrieval System for\n  Buddha Statues", "comments": "Demo video at: https://www.youtube.com/watch?v=3XJvLjSWieY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce BUDA.ART, a system designed to assist researchers in Art\nHistory, to explore and analyze an archive of pictures of Buddha statues. The\nsystem combines different CBIR and classical retrieval techniques to assemble\n2D pictures, 3D statue scans and meta-data, that is focused on the Buddha\nfacial characteristics. We build the system from an archive of 50,000 Buddhism\npictures, identify unique Buddha statues, extract contextual information, and\nprovide specific facial embedding to first index the archive. The system allows\nfor mobile, on-site search, and to explore similarities of statues in the\narchive. In addition, we provide search visualization and 3D analysis of the\nstatues\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 06:35:24 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Renoust", "Benjamin", ""], ["Franca", "Matheus Oliveira", ""], ["Chan", "Jacob", ""], ["Le", "Van", ""], ["Uesaka", "Ayaka", ""], ["Nakashima", "Yuta", ""], ["Nagahara", "Hajime", ""], ["Wang", "Jueren", ""], ["Fujioka", "Yutaka", ""]]}, {"id": "1909.12933", "submitter": "Hamid Tizhoosh", "authors": "H.R.Tizhoosh, Shivam Kalra, Shalev Lifshitz, Morteza Babaie", "title": "Subtractive Perceptrons for Learning Images: A Preliminary Report", "comments": "To appear in the 9th Intern. Conf. on Image Processing Theory, Tools\n  and Applications (IPTA 2019), Istanbul, Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, artificial neural networks have achieved tremendous success\nfor many vision-based tasks. However, this success remains within the paradigm\nof \\emph{weak AI} where networks, among others, are specialized for just one\ngiven task. The path toward \\emph{strong AI}, or Artificial General\nIntelligence, remains rather obscure. One factor, however, is clear, namely\nthat the feed-forward structure of current networks is not a realistic\nabstraction of the human brain. In this preliminary work, some ideas are\nproposed to define a \\textit{subtractive Perceptron} (s-Perceptron), a\ngraph-based neural network that delivers a more compact topology to learn one\nspecific task. In this preliminary study, we test the s-Perceptron with the\nMNIST dataset, a commonly used image archive for digit recognition. The\nproposed network achieves excellent results compared to the benchmark networks\nthat rely on more complex topologies.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 01:13:41 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Tizhoosh", "H. R.", ""], ["Kalra", "Shivam", ""], ["Lifshitz", "Shalev", ""], ["Babaie", "Morteza", ""]]}, {"id": "1909.12935", "submitter": "Yi Zeng", "authors": "Yi Zeng, Enmeng Lu, Yinqian Sun, Ruochen Tian", "title": "Responsible Facial Recognition and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial recognition is changing the way we live in and interact with our\nsociety. Here we discuss the two sides of facial recognition, summarizing\npotential risks and current concerns. We introduce current policies and\nregulations in different countries. Very importantly, we point out that the\nrisks and concerns are not only from facial recognition, but also realistically\nvery similar to other biometric recognition technology, including but not\nlimited to gait recognition, iris recognition, fingerprint recognition, voice\nrecognition, etc. To create a responsible future, we discuss possible\ntechnological moves and efforts that should be made to keep facial recognition\n(and biometric recognition in general) developing for social good.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 15:27:06 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Zeng", "Yi", ""], ["Lu", "Enmeng", ""], ["Sun", "Yinqian", ""], ["Tian", "Ruochen", ""]]}, {"id": "1909.12936", "submitter": "Yi Cheng", "authors": "Yi Cheng, Hongyuan Zhu, Ying Sun, Cihan Acar, Wei Jing, Yan Wu, Liyuan\n  Li, Cheston Tan, Joo-Hwee Lim", "title": "6D Pose Estimation with Correlation Fusion", "comments": "Accepted by ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  6D object pose estimation is widely applied in robotic tasks such as grasping\nand manipulation. Prior methods using RGB-only images are vulnerable to heavy\nocclusion and poor illumination, so it is important to complement them with\ndepth information. However, existing methods using RGB-D data cannot adequately\nexploit consistent and complementary information between RGB and depth\nmodalities. In this paper, we present a novel method to effectively consider\nthe correlation within and across both modalities with attention mechanism to\nlearn discriminative and compact multi-modal features. Then, effective fusion\nstrategies for intra- and inter-correlation modules are explored to ensure\nefficient information flow between RGB and depth. To our best knowledge, this\nis the first work to explore effective intra- and inter-modality fusion in 6D\npose estimation. The experimental results show that our method can achieve the\nstate-of-the-art performance on LineMOD and YCB-Video dataset. We also\ndemonstrate that the proposed method can benefit a real-world robot grasping\ntask by providing accurate object pose estimation.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 04:12:50 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 06:49:06 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Cheng", "Yi", ""], ["Zhu", "Hongyuan", ""], ["Sun", "Ying", ""], ["Acar", "Cihan", ""], ["Jing", "Wei", ""], ["Wu", "Yan", ""], ["Li", "Liyuan", ""], ["Tan", "Cheston", ""], ["Lim", "Joo-Hwee", ""]]}, {"id": "1909.12937", "submitter": "Manel Mart\\'inez-Ram\\'on", "authors": "Meenu Ajith and Manel Mart\\'inez-Ram\\'on", "title": "Unsupervised Segmentation of Fire and Smoke from Infra-Red Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a vision-based fire and smoke segmentation system which\nuse spatial, temporal and motion information to extract the desired regions\nfrom the video frames. The fusion of information is done using multiple\nfeatures such as optical flow, divergence and intensity values. These features\nextracted from the images are used to segment the pixels into different classes\nin an unsupervised way. A comparative analysis is done by using multiple\nclustering algorithms for segmentation. Here the Markov Random Field performs\nmore accurately than other segmentation algorithms since it characterizes the\nspatial interactions of pixels using a finite number of parameters. It builds a\nprobabilistic image model that selects the most likely labeling using the\nmaximum a posteriori (MAP) estimation. This unsupervised approach is tested on\nvarious images and achieves a frame-wise fire detection rate of 95.39%. Hence\nthis method can be used for early detection of fire in real-time and it can be\nincorporated into an indoor or outdoor surveillance system.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 17:19:17 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Ajith", "Meenu", ""], ["Mart\u00ednez-Ram\u00f3n", "Manel", ""]]}, {"id": "1909.12939", "submitter": "Xiaonan Zhao", "authors": "Xiaonan Zhao, Huan Qi, Rui Luo and Larry Davis", "title": "A weakly supervised adaptive triplet loss for deep metric learning", "comments": "4 pages, ICCV Fashion Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of distance metric learning in visual similarity\nsearch, defined as learning an image embedding model which projects images into\nEuclidean space where semantically and visually similar images are closer and\ndissimilar images are further from one another. We present a weakly supervised\nadaptive triplet loss (ATL) capable of capturing fine-grained semantic\nsimilarity that encourages the learned image embedding models to generalize\nwell on cross-domain data. The method uses weakly labeled product description\ndata to implicitly determine fine grained semantic classes, avoiding the need\nto annotate large amounts of training data. We evaluate on the Amazon fashion\nretrieval benchmark and DeepFashion in-shop retrieval data. The method boosts\nthe performance of triplet loss baseline by 10.6% on cross-domain data and\nout-performs the state-of-art model on all evaluation metrics.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 20:54:42 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Zhao", "Xiaonan", ""], ["Qi", "Huan", ""], ["Luo", "Rui", ""], ["Davis", "Larry", ""]]}, {"id": "1909.12942", "submitter": "Zheyu Yang", "authors": "Zheyu Yang, Yujie Wu, Guanrui Wang, Yukuan Yang, Guoqi Li, Lei Deng,\n  Jun Zhu, Luping Shi", "title": "DashNet: A Hybrid Artificial and Spiking Neural Network for High-speed\n  Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-science-oriented artificial neural networks (ANNs) have achieved\ntremendous success in a variety of scenarios via powerful feature extraction\nand high-precision data operations. It is well known, however, that ANNs\nusually suffer from expensive processing resources and costs. In contrast,\nneuroscience-oriented spiking neural networks (SNNs) are promising for\nenergy-efficient information processing benefit from the event-driven spike\nactivities, whereas, they are yet be evidenced to achieve impressive\neffectiveness on real complicated tasks. How to combine the advantage of these\ntwo model families is an open question of great interest. Two significant\nchallenges need to be addressed: (1) lack of benchmark datasets including both\nANN-oriented (frames) and SNN-oriented (spikes) signal resources; (2) the\ndifficulty in jointly processing the synchronous activation from ANNs and\nevent-driven spikes from SNNs. In this work, we proposed a hybrid paradigm,\nnamed as DashNet, to demonstrate the advantages of combining ANNs and SNNs in a\nsingle model. A simulator and benchmark dataset NFS-DAVIS is built, and a\ntemporal complementary filter (TCF) and attention module are designed to\naddress the two mentioned challenges, respectively. In this way, it is shown\nthat DashNet achieves the record-breaking speed of 2083FPS on neuromorphic\nchips and the best tracking performance on NFS-DAVIS and PRED18 datasets. To\nthe best of our knowledge, DashNet is the first framework that can integrate\nand process ANNs and SNNs in a hybrid paradigm, which provides a novel solution\nto achieve both effectiveness and efficiency for high-speed object tracking.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 14:59:53 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Yang", "Zheyu", ""], ["Wu", "Yujie", ""], ["Wang", "Guanrui", ""], ["Yang", "Yukuan", ""], ["Li", "Guoqi", ""], ["Deng", "Lei", ""], ["Zhu", "Jun", ""], ["Shi", "Luping", ""]]}, {"id": "1909.12943", "submitter": "Mesay Samuel", "authors": "Mesay Samuel Gondere, Lars Schmidt-Thieme, Abiot Sinamo Boltena, Hadi\n  Samer Jomaa", "title": "Handwritten Amharic Character Recognition Using a Convolutional Neural\n  Network", "comments": "ECDA2019 Conference Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amharic is the official language of the Federal Democratic Republic of\nEthiopia. There are lots of historic Amharic and Ethiopic handwritten documents\naddressing various relevant issues including governance, science, religious,\nsocial rules, cultures and art works which are very reach indigenous knowledge.\nThe Amharic language has its own alphabet derived from Ge'ez which is currently\nthe liturgical language in Ethiopia. Handwritten character recognition for non\nLatin scripts like Amharic is not addressed especially using the advantages of\nthe state of the art techniques. This research work designs for the first time\na model for Amharic handwritten character recognition using a convolutional\nneural network. The dataset was organized from collected sample handwritten\ndocuments and data augmentation was applied for machine learning. The model was\nfurther enhanced using multi-task learning from the relationships of the\ncharacters. Promising results are observed from the later model which can\nfurther be applied to word prediction.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 21:12:22 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Gondere", "Mesay Samuel", ""], ["Schmidt-Thieme", "Lars", ""], ["Boltena", "Abiot Sinamo", ""], ["Jomaa", "Hadi Samer", ""]]}, {"id": "1909.12945", "submitter": "Zhaobing Kang", "authors": "Zhaobing Kang, Wei Zou, Zheng Zhu, Chi Zhang and Hongxuan Ma", "title": "EPOSIT: An Absolute Pose Estimation Method for Pinhole and Fish-Eye\n  Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a generic 6DOF camera pose estimation method, which can\nbe used for both the pinhole camera and the fish-eye camera. Different from\nexisting methods, relative positions of 3D points rather than absolute\ncoordinates in the world coordinate system are employed in our method, and it\nhas a unique solution. The application scope of POSIT (Pose from Orthography\nand Scaling with Iteration) algorithm is generalized to fish-eye cameras by\ncombining with the radially symmetric projection model. The image point\nrelationship between the pinhole camera and the fish-eye camera is derived\nbased on their projection model. The general pose expression which fits for\ndifferent cameras can be acquired by four noncoplanar object points and their\ncorresponding image points. Accurate estimation results are calculated\niteratively. Experimental results on synthetic and real data show that the pose\nestimation results of our method are more stable and accurate than\nstate-of-the-art methods. The source code is available at\nhttps://github.com/k032131/EPOSIT.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 01:11:43 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Kang", "Zhaobing", ""], ["Zou", "Wei", ""], ["Zhu", "Zheng", ""], ["Zhang", "Chi", ""], ["Ma", "Hongxuan", ""]]}, {"id": "1909.12947", "submitter": "Zhenzhen Xiang", "authors": "Zhenzhen Xiang, Jingrui Yu, Jie Li and Jianbo Su", "title": "ViLiVO: Virtual LiDAR-Visual Odometry for an Autonomous Vehicle with a\n  Multi-Camera System", "comments": "IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a multi-camera visual odometry (VO) system for an\nautonomous vehicle. Our system mainly consists of a virtual LiDAR and a pose\ntracker. We use a perspective transformation method to synthesize a\nsurround-view image from undistorted fisheye camera images. With a semantic\nsegmentation model, the free space can be extracted. The scans of the virtual\nLiDAR are generated by discretizing the contours of the free space. As for the\npose tracker, we propose a visual odometry system fusing both the feature\nmatching and the virtual LiDAR scan matching results. Only those feature points\nlocated in the free space area are utilized to ensure the 2D-2D matching for\npose estimation. Furthermore, bundle adjustment (BA) is performed to minimize\nthe feature points reprojection error and scan matching error. We apply our\nsystem to an autonomous vehicle equipped with four fisheye cameras. The testing\nscenarios include an outdoor parking lot as well as an indoor garage.\nExperimental results demonstrate that our system achieves a more robust and\naccurate performance comparing with a fisheye camera based monocular visual\nodometry system.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 04:50:33 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Xiang", "Zhenzhen", ""], ["Yu", "Jingrui", ""], ["Li", "Jie", ""], ["Su", "Jianbo", ""]]}, {"id": "1909.12948", "submitter": "ArtEye Lab", "authors": "Vivekraj V. K., Debashis Sen and Balasubramanian Raman", "title": "Video Skimming: Taxonomy and Comprehensive Survey", "comments": null, "journal-ref": "ACM Computing Surveys (CSUR), Volume 52, Issue 5, 2019", "doi": "10.1145/3347712", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Video skimming, also known as dynamic video summarization, generates a\ntemporally abridged version of a given video. Skimming can be achieved by\nidentifying significant components either in uni-modal or multi-modal features\nextracted from the video. Being dynamic in nature, video skimming, through\ntemporal connectivity, allows better understanding of the video from its\nsummary. Having this obvious advantage, recently, video skimming has drawn the\nfocus of many researchers benefiting from the easy availability of the required\ncomputing resources. In this paper, we provide a comprehensive survey on video\nskimming focusing on the substantial amount of literature from the past decade.\nWe present a taxonomy of video skimming approaches, and discuss their evolution\nhighlighting key advances. We also provide a study on the components required\nfor the evaluation of a video skimming performance.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 11:21:09 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["K.", "Vivekraj V.", ""], ["Sen", "Debashis", ""], ["Raman", "Balasubramanian", ""]]}, {"id": "1909.12950", "submitter": "Rico Jonschkowski", "authors": "Rico Jonschkowski and Austin Stone", "title": "Towards Object Detection from Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel approach to weakly supervised object detection. Instead of\nannotated images, our method only requires two short videos to learn to detect\na new object: 1) a video of a moving object and 2) one or more \"negative\"\nvideos of the scene without the object. The key idea of our algorithm is to\ntrain the object detector to produce physically plausible object motion when\napplied to the first video and to not detect anything in the second video. With\nthis approach, our method learns to locate objects without any object location\nannotations. Once the model is trained, it performs object detection on single\nimages. We evaluate our method in three robotics settings that afford learning\nobjects from motion: observing moving objects, watching demonstrations of\nobject manipulation, and physically interacting with objects (see a video\nsummary at https://youtu.be/BH0Hv3zZG_4).\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 18:00:14 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Jonschkowski", "Rico", ""], ["Stone", "Austin", ""]]}, {"id": "1909.12959", "submitter": "Jason Wei", "authors": "Jason W. Wei, Arief A. Suriawinata, Louis J. Vaickus, Bing Ren,\n  Xiaoying Liu, Mikhail Lisovsky, Naofumi Tomita, Behnaz Abdollahi, Adam S.\n  Kim, Dale C. Snover, John A. Baron, Elizabeth L. Barry, Saeed Hassanpour", "title": "Deep neural networks for automated classification of colorectal polyps\n  on histopathology slides: A multi-institutional evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histological classification of colorectal polyps plays a critical role in\nboth screening for colorectal cancer and care of affected patients. An accurate\nand automated algorithm for the classification of colorectal polyps on\ndigitized histopathology slides could benefit clinicians and patients. Evaluate\nthe performance and assess the generalizability of a deep neural network for\ncolorectal polyp classification on histopathology slide images using a\nmulti-institutional dataset. In this study, we developed a deep neural network\nfor classification of four major colorectal polyp types, tubular adenoma,\ntubulovillous/villous adenoma, hyperplastic polyp, and sessile serrated\nadenoma, based on digitized histopathology slides from our institution,\nDartmouth-Hitchcock Medical Center (DHMC), in New Hampshire. We evaluated the\ndeep neural network on an internal dataset of 157 histopathology slide images\nfrom DHMC, as well as on an external dataset of 238 histopathology slide images\nfrom 24 different institutions spanning 13 states in the United States. We\nmeasured accuracy, sensitivity, and specificity of our model in this evaluation\nand compared its performance to local pathologists' diagnoses at the\npoint-of-care retrieved from corresponding pathology laboratories. For the\ninternal evaluation, the deep neural network had a mean accuracy of 93.5% (95%\nCI 89.6%-97.4%), compared with local pathologists' accuracy of 91.4% (95% CI\n87.0%-95.8%). On the external test set, the deep neural network achieved an\naccuracy of 87.0% (95% CI 82.7%-91.3%), comparable with local pathologists'\naccuracy of 86.6% (95% CI 82.3%-90.9%). If confirmed in clinical settings, our\nmodel could assist pathologists by improving the diagnostic efficiency,\nreproducibility, and accuracy of colorectal cancer screenings.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 21:18:38 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 23:03:40 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Wei", "Jason W.", ""], ["Suriawinata", "Arief A.", ""], ["Vaickus", "Louis J.", ""], ["Ren", "Bing", ""], ["Liu", "Xiaoying", ""], ["Lisovsky", "Mikhail", ""], ["Tomita", "Naofumi", ""], ["Abdollahi", "Behnaz", ""], ["Kim", "Adam S.", ""], ["Snover", "Dale C.", ""], ["Baron", "John A.", ""], ["Barry", "Elizabeth L.", ""], ["Hassanpour", "Saeed", ""]]}, {"id": "1909.12962", "submitter": "Yuezun Li", "authors": "Yuezun Li, Xin Yang, Pu Sun, Honggang Qi and Siwei Lyu", "title": "Celeb-DF: A Large-scale Challenging Dataset for DeepFake Forensics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI-synthesized face-swapping videos, commonly known as DeepFakes, is an\nemerging problem threatening the trustworthiness of online information. The\nneed to develop and evaluate DeepFake detection algorithms calls for\nlarge-scale datasets. However, current DeepFake datasets suffer from low visual\nquality and do not resemble DeepFake videos circulated on the Internet. We\npresent a new large-scale challenging DeepFake video dataset, Celeb-DF, which\ncontains 5,639 high-quality DeepFake videos of celebrities generated using\nimproved synthesis process. We conduct a comprehensive evaluation of DeepFake\ndetection methods and datasets to demonstrate the escalated level of challenges\nposed by Celeb-DF.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 21:26:34 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 00:23:11 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2019 03:46:36 GMT"}, {"version": "v4", "created": "Mon, 16 Mar 2020 16:20:16 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Li", "Yuezun", ""], ["Yang", "Xin", ""], ["Sun", "Pu", ""], ["Qi", "Honggang", ""], ["Lyu", "Siwei", ""]]}, {"id": "1909.12977", "submitter": "Sijie Zhu", "authors": "Sijie Zhu, Taojiannan Yang, Chen Chen", "title": "Visual Explanation for Deep Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work explores the visual explanation for deep metric learning and its\napplications. As an important problem for learning representation, metric\nlearning has attracted much attention recently, while the interpretation of\nsuch model is not as well studied as classification. To this end, we propose an\nintuitive idea to show where contributes the most to the overall similarity of\ntwo input images by decomposing the final activation. Instead of only providing\nthe overall activation map of each image, we propose to generate point-to-point\nactivation intensity between two images so that the relationship between\ndifferent regions is uncovered. We show that the proposed framework can be\ndirectly deployed to a large range of metric learning applications and provides\nvaluable information for understanding the model. Furthermore, our experiments\nshow its effectiveness on two potential applications, i.e. cross-view pattern\ndiscovery and interactive retrieval. The source code is available at\n\\url{https://github.com/Jeff-Zilence/Explain_Metric_Learning}.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 22:30:58 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 04:10:53 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2020 21:16:50 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Zhu", "Sijie", ""], ["Yang", "Taojiannan", ""], ["Chen", "Chen", ""]]}, {"id": "1909.12978", "submitter": "Taojiannan Yang", "authors": "Taojiannan Yang, Sijie Zhu, Chen Chen, Shen Yan, Mi Zhang, Andrew\n  Willis", "title": "MutualNet: Adaptive ConvNet via Mutual Learning from Network Width and\n  Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the width-resolution mutual learning method (MutualNet) to train a\nnetwork that is executable at dynamic resource constraints to achieve adaptive\naccuracy-efficiency trade-offs at runtime. Our method trains a cohort of\nsub-networks with different widths using different input resolutions to\nmutually learn multi-scale representations for each sub-network. It achieves\nconsistently better ImageNet top-1 accuracy over the state-of-the-art adaptive\nnetwork US-Net under different computation constraints, and outperforms the\nbest compound scaled MobileNet in EfficientNet by 1.5%. The superiority of our\nmethod is also validated on COCO object detection and instance segmentation as\nwell as transfer learning. Surprisingly, the training strategy of MutualNet can\nalso boost the performance of a single network, which substantially outperforms\nthe powerful AutoAugmentation in both efficiency (GPU search hours: 15000 vs.\n0) and accuracy (ImageNet: 77.6% vs. 78.6%). Code is available at\n\\url{https://github.com/taoyang1122/MutualNet}.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 22:33:15 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 16:09:34 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 18:15:15 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Yang", "Taojiannan", ""], ["Zhu", "Sijie", ""], ["Chen", "Chen", ""], ["Yan", "Shen", ""], ["Zhang", "Mi", ""], ["Willis", "Andrew", ""]]}, {"id": "1909.12983", "submitter": "Pablo Navarrete Michelini", "authors": "Pablo Navarrete Michelini, Wenbin Chen, Hanwen Liu, Dan Zhu", "title": "MGBPv2: Scaling Up Multi-Grid Back-Projection Networks", "comments": "In ICCV 2019 Workshops. Winner of Perceptual track in AIM Extreme\n  Super-Resolution Challenge 2019. Code available at\n  https://github.com/pnavarre/mgbpv2", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Here, we describe our solution for the AIM-2019 Extreme Super-Resolution\nChallenge, where we won the 1st place in terms of perceptual quality (MOS)\nsimilar to the ground truth and achieved the 5th place in terms of\nhigh-fidelity (PSNR). To tackle this challenge, we introduce the second\ngeneration of MultiGrid BackProjection networks (MGBPv2) whose major\nmodifications make the system scalable and more general than its predecessor.\nIt combines the scalability of the multigrid algorithm and the performance of\niterative backprojections. In its original form, MGBP is limited to a small\nnumber of parameters due to a strongly recursive structure. In MGBPv2, we make\nfull use of the multigrid recursion from the beginning of the network; we allow\ndifferent parameters in every module of the network; we simplify the main\nmodules; and finally, we allow adjustments of the number of network features\nbased on the scale of operation. For inference tasks, we introduce an\noverlapping patch approach to further allow processing of very large images\n(e.g. 8K). Our training strategies make use of a multiscale loss, combining\ndistortion and/or perception losses on the output as well as downscaled output\nimages. The final system can balance between high quality and high performance.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 23:27:10 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Michelini", "Pablo Navarrete", ""], ["Chen", "Wenbin", ""], ["Liu", "Hanwen", ""], ["Zhu", "Dan", ""]]}, {"id": "1909.12996", "submitter": "Md Amirul Islam", "authors": "Rezaul Karim, Md Amirul Islam, Neil D. B. Bruce", "title": "Distributed Iterative Gating Networks for Semantic Segmentation", "comments": "WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a canonical structure for controlling information\nflow in neural networks with an efficient feedback routing mechanism based on a\nstrategy of Distributed Iterative Gating (DIGNet). The structure of this\nmechanism derives from a strong conceptual foundation and presents a\nlight-weight mechanism for adaptive control of computation similar to recurrent\nconvolutional neural networks by integrating feedback signals with a\nfeed-forward architecture. In contrast to other RNN formulations, DIGNet\ngenerates feedback signals in a cascaded manner that implicitly carries\ninformation from all the layers above. This cascaded feedback propagation by\nmeans of the propagator gates is found to be more effective compared to other\nfeedback mechanisms that use feedback from the output of either the\ncorresponding stage or from the previous stage. Experiments reveal the high\ndegree of capability that this recurrent approach with cascaded feedback\npresents over feed-forward baselines and other recurrent models for pixel-wise\nlabeling problems on three challenging datasets, PASCAL VOC 2012, COCO-Stuff,\nand ADE20K.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 01:06:49 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Karim", "Rezaul", ""], ["Islam", "Md Amirul", ""], ["Bruce", "Neil D. B.", ""]]}, {"id": "1909.13005", "submitter": "Xiaojiang Peng", "authors": "Qing Li, Xiaojiang Peng, Yu Qiao, Qiang Peng", "title": "Learning Category Correlations for Multi-label Image Recognition with\n  Graph Networks", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label image recognition is a task that predicts a set of object labels\nin an image. As the objects co-occur in the physical world, it is desirable to\nmodel label dependencies. Previous existing methods resort to either recurrent\nnetworks or pre-defined label correlation graphs for this purpose. In this\npaper, instead of using a pre-defined graph which is inflexible and may be\nsub-optimal for multi-label classification, we propose the A-GCN, which\nleverages the popular Graph Convolutional Networks with an Adaptive label\ncorrelation graph to model label dependencies. Specifically, we introduce a\nplug-and-play Label Graph (LG) module to learn label correlations with word\nembeddings, and then utilize traditional GCN to map this graph into\nlabel-dependent object classifiers which are further applied to image features.\nThe basic LG module incorporates two 1x1 convolutional layers and uses the dot\nproduct to generate label graphs. In addition, we propose a sparse correlation\nconstraint to enhance the LG module and also explore different LG\narchitectures. We validate our method on two diverse multi-label datasets:\nMS-COCO and Fashion550K. Experimental results show that our A-GCN significantly\nimproves baseline methods and achieves performance superior or comparable to\nthe state of the art.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 02:03:25 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Li", "Qing", ""], ["Peng", "Xiaojiang", ""], ["Qiao", "Yu", ""], ["Peng", "Qiang", ""]]}, {"id": "1909.13028", "submitter": "Jialu Huang", "authors": "Jialu Huang, Jing Liao, Tak Wu Sam Kwong", "title": "Semantic Example Guided Image-to-Image Translation", "comments": "2020 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many image-to-image (I2I) translation problems are in nature of high\ndiversity that a single input may have various counterparts. Prior works\nproposed the multi-modal network that can build a many-to-many mapping between\ntwo visual domains. However, most of them are guided by sampled noises. Some\nothers encode the reference images into a latent vector, by which the semantic\ninformation of the reference image will be washed away. In this work, we aim to\nprovide a solution to control the output based on references semantically.\nGiven a reference image and an input in another domain, a semantic matching is\nfirst performed between the two visual contents and generates the auxiliary\nimage, which is explicitly encouraged to preserve semantic characteristics of\nthe reference. A deep network then is used for I2I translation and the final\noutputs are expected to be semantically similar to both the input and the\nreference; however, no such paired data can satisfy that dual-similarity in a\nsupervised fashion, so we build up a self-supervised framework to serve the\ntraining purpose. We improve the quality and diversity of the outputs by\nemploying non-local blocks and a multi-task architecture. We assess the\nproposed method through extensive qualitative and quantitative evaluations and\nalso presented comparisons with several state-of-art models.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 05:36:19 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 01:56:17 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Huang", "Jialu", ""], ["Liao", "Jing", ""], ["Kwong", "Tak Wu Sam", ""]]}, {"id": "1909.13030", "submitter": "Benjamin Evans", "authors": "Benjamin Patrick Evans, Harith Al-Sahaf, Bing Xue, Mengjie Zhang", "title": "Genetic Programming and Gradient Descent: A Memetic Approach to Binary\n  Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification is an essential task in computer vision, which aims to\ncategorise a set of images into different groups based on some visual criteria.\nExisting methods, such as convolutional neural networks, have been successfully\nutilised to perform image classification. However, such methods often require\nhuman intervention to design a model. Furthermore, such models are difficult to\ninterpret and it is challenging to analyse the patterns of different classes.\nThis paper presents a hybrid (memetic) approach combining genetic programming\n(GP) and Gradient-based optimisation for image classification to overcome the\nlimitations mentioned. The performance of the proposed method is compared to a\nbaseline version (without local search) on four binary classification image\ndatasets to provide an insight into the usefulness of local search mechanisms\nfor enhancing the performance of GP.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 05:42:22 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Evans", "Benjamin Patrick", ""], ["Al-Sahaf", "Harith", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1909.13032", "submitter": "Xiao Peng Yan", "authors": "Xiaopeng Yan, Ziliang Chen, Anni Xu, Xiaoxi Wang, Xiaodan Liang, Liang\n  Lin", "title": "Meta R-CNN : Towards General Solver for Instance-level Few-shot Learning", "comments": "Published in ICCV-2019. Project:\n  https://yanxp.github.io/metarcnn.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resembling the rapid learning capability of human, few-shot learning empowers\nvision systems to understand new concepts by training with few samples. Leading\napproaches derived from meta-learning on images with a single visual object.\nObfuscated by a complex background and multiple objects in one image, they are\nhard to promote the research of few-shot object detection/segmentation. In this\nwork, we present a flexible and general methodology to achieve these tasks. Our\nwork extends Faster /Mask R-CNN by proposing meta-learning over RoI\n(Region-of-Interest) features instead of a full image feature. This simple\nspirit disentangles multi-object information merged with the background,\nwithout bells and whistles, enabling Faster /Mask R-CNN turn into a\nmeta-learner to achieve the tasks. Specifically, we introduce a Predictor-head\nRemodeling Network (PRN) that shares its main backbone with Faster /Mask R-CNN.\nPRN receives images containing few-shot objects with their bounding boxes or\nmasks to infer their class attentive vectors. The vectors take channel-wise\nsoft-attention on RoI features, remodeling those R-CNN predictor heads to\ndetect or segment the objects that are consistent with the classes these\nvectors represent. In our experiments, Meta R-CNN yields the state of the art\nin few-shot object detection and improves few-shot object segmentation by Mask\nR-CNN.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 05:46:49 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 03:10:42 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Yan", "Xiaopeng", ""], ["Chen", "Ziliang", ""], ["Xu", "Anni", ""], ["Wang", "Xiaoxi", ""], ["Liang", "Xiaodan", ""], ["Lin", "Liang", ""]]}, {"id": "1909.13046", "submitter": "Yu Liu", "authors": "Yu Liu, Lingqiao Liu, Haokui Zhang, Hamid Rezatofighi and Ian Reid", "title": "Meta Learning with Differentiable Closed-form Solver for Fast Video\n  Object Segmentation", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper tackles the problem of video object segmentation. We are\nspecifically concerned with the task of segmenting all pixels of a target\nobject in all frames, given the annotation mask in the first frame. Even when\nsuch annotation is available this remains a challenging problem because of the\nchanging appearance and shape of the object over time. In this paper, we tackle\nthis task by formulating it as a meta-learning problem, where the base learner\ngrasping the semantic scene understanding for a general type of objects, and\nthe meta learner quickly adapting the appearance of the target object with a\nfew examples. Our proposed meta-learning method uses a closed form optimizer,\nthe so-called \"ridge regression\", which has been shown to be conducive for fast\nand better training convergence. Moreover, we propose a mechanism, named \"block\nsplitting\", to further speed up the training process as well as to reduce the\nnumber of learning parameters. In comparison with the-state-of-the art methods,\nour proposed framework achieves significant boost up in processing speed, while\nhaving very competitive performance compared to the best performing methods on\nthe widely used datasets.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 08:20:04 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Liu", "Yu", ""], ["Liu", "Lingqiao", ""], ["Zhang", "Haokui", ""], ["Rezatofighi", "Hamid", ""], ["Reid", "Ian", ""]]}, {"id": "1909.13047", "submitter": "Wei Zhou", "authors": "Wei Zhou, Yiying Li", "title": "Feature Fusion Detector for Semantic Cognition of Remote Sensing", "comments": "12 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The value of remote sensing images is of vital importance in many areas and\nneeds to be refined by some cognitive approaches. The remote sensing detection\nis an appropriate way to achieve the semantic cognition. However, such\ndetection is a challenging issue for scale diversity, diversity of views, small\nobjects, sophisticated light and shadow backgrounds. In this article, inspired\nby the state-of-the-art detection framework FPN, we propose a novel approach\nfor constructing a feature fusion module that optimizes feature context\nutilization in detection, calling our system LFFN for Layer-weakening Feature\nFusion Network. We explore the inherent relevance of different layers to the\nfinal decision, and the incentives of higher-level features to lower-level\nfeatures. More importantly, we explore the characteristics of different\nbackbone networks in the mining of basic features and the correlation\nutilization of convolutional channels, and call our upgraded version as\nadvanced LFFN. Based on experiments on the remote sensing dataset from Google\nEarth, our LFFN has proved effective and practical for the semantic cognition\nof remote sensing, achieving 89% mAP which is 4.1% higher than that of FPN.\nMoreover, in terms of the generalization performance, LFFN achieves 79.9% mAP\non VOC 2007 and achieves 73.0% mAP on VOC 2012 test, and advacned LFFN obtains\nthe mAP values of 80.7% and 74.4% on VOC 2007 and 2012 respectively,\noutperforming the comparable state-of-the-art SSD and Faster R-CNN models.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 08:30:03 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Zhou", "Wei", ""], ["Li", "Yiying", ""]]}, {"id": "1909.13051", "submitter": "Ming Cheng", "authors": "Ming Cheng, Zhan Ma, M. Salman Asif, Yiling Xu, Haojie Liu, Wenbo Bao,\n  and Jun Sun", "title": "A Dual Camera System for High Spatiotemporal Resolution Video\n  Acquisition", "comments": "To appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a dual camera system for high spatiotemporal resolution\n(HSTR) video acquisition, where one camera shoots a video with high spatial\nresolution and low frame rate (HSR-LFR) and another one captures a low spatial\nresolution and high frame rate (LSR-HFR) video. Our main goal is to combine\nvideos from LSR-HFR and HSR-LFR cameras to create an HSTR video. We propose an\nend-to-end learning framework, AWnet, mainly consisting of a FlowNet and a\nFusionNet that learn an adaptive weighting function in pixel domain to combine\ninputs in a frame recurrent fashion. To improve the reconstruction quality for\ncameras used in reality, we also introduce noise regularization under the same\nframework. Our method has demonstrated noticeable performance gains in terms of\nboth objective PSNR measurement in simulation with different publicly available\nvideo and light-field datasets and subjective evaluation with real data\ncaptured by dual iPhone 7 and Grasshopper3 cameras. Ablation studies are\nfurther conducted to investigate and explore various aspects (such as reference\nstructure, camera parallax, exposure time, etc) of our system to fully\nunderstand its capability for potential applications.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 09:10:21 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 04:08:34 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Cheng", "Ming", ""], ["Ma", "Zhan", ""], ["Asif", "M. Salman", ""], ["Xu", "Yiling", ""], ["Liu", "Haojie", ""], ["Bao", "Wenbo", ""], ["Sun", "Jun", ""]]}, {"id": "1909.13055", "submitter": "Duc Tam Nguyen", "authors": "Duc Tam Nguyen, Maximilian Dax, Chaithanya Kumar Mummadi, Thi Phuong\n  Nhung Ngo, Thi Hoai Phuong Nguyen, Zhongyu Lou, Thomas Brox", "title": "DeepUSPS: Deep Robust Unsupervised Saliency Prediction With\n  Self-Supervision", "comments": "NeuRIPS-2019 (Vancouver, Canada): camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) based salient object detection in images based on\nhigh-quality labels is expensive. Alternative unsupervised approaches rely on\ncareful selection of multiple handcrafted saliency methods to generate noisy\npseudo-ground-truth labels. In this work, we propose a two-stage mechanism for\nrobust unsupervised object saliency prediction, where the first stage involves\nrefinement of the noisy pseudo labels generated from different handcrafted\nmethods. Each handcrafted method is substituted by a deep network that learns\nto generate the pseudo labels. These labels are refined incrementally in\nmultiple iterations via our proposed self-supervision technique. In the second\nstage, the refined labels produced from multiple networks representing multiple\nsaliency methods are used to train the actual saliency detection network. We\nshow that this self-learning procedure outperforms all the existing\nunsupervised methods over different datasets. Results are even comparable to\nthose of fully-supervised state-of-the-art approaches. The code is available at\nhttps://tinyurl.com/wtlhgo3 .\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 09:23:14 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 14:11:58 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 14:32:36 GMT"}, {"version": "v4", "created": "Mon, 15 Mar 2021 13:28:46 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Nguyen", "Duc Tam", ""], ["Dax", "Maximilian", ""], ["Mummadi", "Chaithanya Kumar", ""], ["Ngo", "Thi Phuong Nhung", ""], ["Nguyen", "Thi Hoai Phuong", ""], ["Lou", "Zhongyu", ""], ["Brox", "Thomas", ""]]}, {"id": "1909.13057", "submitter": "Chuming Lin", "authors": "Bo Yan, Chuming Lin, Weimin Tan", "title": "Frame and Feature-Context Video Super-Resolution", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  For video super-resolution, current state-of-the-art approaches either\nprocess multiple low-resolution (LR) frames to produce each output\nhigh-resolution (HR) frame separately in a sliding window fashion or\nrecurrently exploit the previously estimated HR frames to super-resolve the\nfollowing frame. The main weaknesses of these approaches are: 1) separately\ngenerating each output frame may obtain high-quality HR estimates while\nresulting in unsatisfactory flickering artifacts, and 2) combining previously\ngenerated HR frames can produce temporally consistent results in the case of\nshort information flow, but it will cause significant jitter and jagged\nartifacts because the previous super-resolving errors are constantly\naccumulated to the subsequent frames. In this paper, we propose a fully\nend-to-end trainable frame and feature-context video super-resolution (FFCVSR)\nnetwork that consists of two key sub-networks: local network and context\nnetwork, where the first one explicitly utilizes a sequence of consecutive LR\nframes to generate local feature and local SR frame, and the other combines the\noutputs of local network and the previously estimated HR frames and features to\nsuper-resolve the subsequent frame. Our approach takes full advantage of the\ninter-frame information from multiple LR frames and the context information\nfrom previously predicted HR frames, producing temporally consistent\nhigh-quality results while maintaining real-time speed by directly reusing\nprevious features and frames. Extensive evaluations and comparisons demonstrate\nthat our approach produces state-of-the-art results on a standard benchmark\ndataset, with advantages in terms of accuracy, efficiency, and visual quality\nover the existing approaches.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 09:40:31 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Yan", "Bo", ""], ["Lin", "Chuming", ""], ["Tan", "Weimin", ""]]}, {"id": "1909.13062", "submitter": "Prateek Munjal", "authors": "Prateek Munjal, Akanksha Paul, Narayanan C. Krishnan", "title": "Implicit Discriminator in Variational Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently generative models have focused on combining the advantages of\nvariational autoencoders (VAE) and generative adversarial networks (GAN) for\ngood reconstruction and generative abilities. In this work we introduce a novel\nhybrid architecture, Implicit Discriminator in Variational Autoencoder (IDVAE),\nthat combines a VAE and a GAN, which does not need an explicit discriminator\nnetwork. The fundamental premise of the IDVAE architecture is that the encoder\nof a VAE and the discriminator of a GAN utilize common features and therefore\ncan be trained as a shared network, while the decoder of the VAE and the\ngenerator of the GAN can be combined to learn a single network. This results in\na simple two-tier architecture that has the properties of both a VAE and a GAN.\nThe qualitative and quantitative experiments on real-world benchmark datasets\ndemonstrates that IDVAE perform better than the state of the art hybrid\napproaches. We experimentally validate that IDVAE can be easily extended to\nwork in a conditional setting and demonstrate its performance on complex\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 10:12:28 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Munjal", "Prateek", ""], ["Paul", "Akanksha", ""], ["Krishnan", "Narayanan C.", ""]]}, {"id": "1909.13063", "submitter": "Jiao Xie", "authors": "Jiao Xie, Shaohui Lin, Yichen Zhang, Linkai Luo", "title": "Training convolutional neural networks with cheap convolutions and\n  online distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large memory and computation consumption in convolutional neural networks\n(CNNs) has been one of the main barriers for deploying them on resource-limited\nsystems. To this end, most cheap convolutions (e.g., group convolution,\ndepth-wise convolution, and shift convolution) have recently been used for\nmemory and computation reduction but with the specific architecture designing.\nFurthermore, it results in a low discriminability of the compressed networks by\ndirectly replacing the standard convolution with these cheap ones. In this\npaper, we propose to use knowledge distillation to improve the performance of\nthe compact student networks with cheap convolutions. In our case, the teacher\nis a network with the standard convolution, while the student is a simple\ntransformation of the teacher architecture without complicated redesigning. In\nparticular, we propose a novel online distillation method, which online\nconstructs the teacher network without pre-training and conducts mutual\nlearning between the teacher and student network, to improve the performance of\nthe student model. Extensive experiments demonstrate that the proposed approach\nachieves superior performance to simultaneously reduce memory and computation\noverhead of cutting-edge CNNs on different datasets, including CIFAR-10/100 and\nImageNet ILSVRC 2012, compared to the state-of-the-art CNN compression and\nacceleration methods. The codes are publicly available at\nhttps://github.com/EthanZhangYC/OD-cheap-convolution.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 10:16:17 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 12:56:21 GMT"}, {"version": "v3", "created": "Thu, 10 Oct 2019 07:47:43 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Xie", "Jiao", ""], ["Lin", "Shaohui", ""], ["Zhang", "Yichen", ""], ["Luo", "Linkai", ""]]}, {"id": "1909.13072", "submitter": "Danfei Xu", "authors": "Danfei Xu, Roberto Mart\\'in-Mart\\'in, De-An Huang, Yuke Zhu, Silvio\n  Savarese, Li Fei-Fei", "title": "Regression Planning Networks", "comments": "Accepted at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent learning-to-plan methods have shown promising results on planning\ndirectly from observation space. Yet, their ability to plan for long-horizon\ntasks is limited by the accuracy of the prediction model. On the other hand,\nclassical symbolic planners show remarkable capabilities in solving\nlong-horizon tasks, but they require predefined symbolic rules and symbolic\nstates, restricting their real-world applicability. In this work, we combine\nthe benefits of these two paradigms and propose a learning-to-plan method that\ncan directly generate a long-term symbolic plan conditioned on high-dimensional\nobservations. We borrow the idea of regression (backward) planning from\nclassical planning literature and introduce Regression Planning Networks (RPN),\na neural network architecture that plans backward starting at a task goal and\ngenerates a sequence of intermediate goals that reaches the current\nobservation. We show that our model not only inherits many favorable traits\nfrom symbolic planning, e.g., the ability to solve previously unseen tasks but\nalso can learn from visual inputs in an end-to-end manner. We evaluate the\ncapabilities of RPN in a grid world environment and a simulated 3D kitchen\nenvironment featuring complex visual scenes and long task horizons, and show\nthat it achieves near-optimal performance in completely new task instances.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 11:30:24 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Xu", "Danfei", ""], ["Mart\u00edn-Mart\u00edn", "Roberto", ""], ["Huang", "De-An", ""], ["Zhu", "Yuke", ""], ["Savarese", "Silvio", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1909.13080", "submitter": "Prajjwal Bhargava", "authors": "Prajjwal Bhargava", "title": "On Generalizing Detection Models for Unconstrained Environments", "comments": "In Proceedings of the 2019 IEEE International Conference on Computer\n  Vision workshop (ICCV Workshops)", "journal-ref": null, "doi": "10.1109/ICCVW.2019.00529", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection has seen tremendous progress in recent years. However,\ncurrent algorithms don't generalize well when tested on diverse data\ndistributions. We address the problem of incremental learning in object\ndetection on the India Driving Dataset (IDD). Our approach involves using\nmultiple domain-specific classifiers and effective transfer learning techniques\nfocussed on avoiding catastrophic forgetting. We evaluate our approach on the\nIDD and BDD100K dataset. Results show the effectiveness of our domain adaptive\napproach in the case of domain shifts in environments.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 12:35:56 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Bhargava", "Prajjwal", ""]]}, {"id": "1909.13082", "submitter": "Alexander Korotin", "authors": "Alexander Korotin and Vage Egiazarian and Arip Asadulaev and Alexander\n  Safin and Evgeny Burnaev", "title": "Wasserstein-2 Generative Networks", "comments": "30 pages, 21 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel end-to-end non-minimax algorithm for training optimal\ntransport mappings for the quadratic cost (Wasserstein-2 distance). The\nalgorithm uses input convex neural networks and a cycle-consistency\nregularization to approximate Wasserstein-2 distance. In contrast to popular\nentropic and quadratic regularizers, cycle-consistency does not introduce bias\nand scales well to high dimensions. From the theoretical side, we estimate the\nproperties of the generative mapping fitted by our algorithm. From the\npractical side, we evaluate our algorithm on a wide range of tasks:\nimage-to-image color transfer, latent space optimal transport, image-to-image\nstyle transfer, and domain adaptation.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 12:42:12 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 09:42:03 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 18:04:14 GMT"}, {"version": "v4", "created": "Thu, 10 Dec 2020 10:53:46 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Korotin", "Alexander", ""], ["Egiazarian", "Vage", ""], ["Asadulaev", "Arip", ""], ["Safin", "Alexander", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1909.13092", "submitter": "Zhi Chen", "authors": "Zhi Chen, Fan Yang, Wenbing Tao", "title": "GLA-Net: An Attention Network with Guided Loss for Mismatch Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mismatch removal is a critical prerequisite in many feature-based tasks.\nRecent attempts cast the mismatch removal task as a binary classification\nproblem and solve it through deep learning based methods. In these methods, the\nimbalance between positive and negative classes is important, which affects\nnetwork performance, i.e., Fn-score. To establish the link between Fn-score and\nloss, we propose to guide the loss with the Fn-score directly. We theoretically\ndemonstrate the direct link between our Guided Loss and Fn-score during\ntraining. Moreover, we discover that outliers often impair global context in\nmismatch removal networks. To address this issue, we introduce the attention\nmechanism to mismatch removal task and propose a novel Inlier Attention Block\n(IA Block). To evaluate the effectiveness of our loss and IA Block, we design\nan end-to-end network for mismatch removal, called GLA-Net \\footnote{Our code\nwill be available in Github later.}. Experiments have shown that our network\nachieves the state-of-the-art performance on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 13:19:13 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Chen", "Zhi", ""], ["Yang", "Fan", ""], ["Tao", "Wenbing", ""]]}, {"id": "1909.13101", "submitter": "Julisa Bana Abraham", "authors": "Julisa Bana Abraham", "title": "Plasmodium Detection Using Simple CNN and Clustered GLCM Features", "comments": "5 Pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malaria is a serious disease caused by the Plasmodium parasite that\ntransmitted through the bite of a female Anopheles mosquito and invades human\nerythrocytes. Malaria must be recognized precisely in order to treat the\npatient in time and to prevent further spread of infection. The standard\ndiagnostic technique using microscopic examination is inefficient, the quality\nof the diagnosis depends on the quality of blood smears and experience of\nmicroscopists in classifying and counting infected and non-infected cells.\nConvolutional Neural Networks (CNN) is one of deep learning class that able to\nautomate feature engineering and learn effective features that could be very\neffective in diagnosing malaria. This study proposes an intelligent system\nbased on simple CNN for detecting malaria parasites through images of thin\nblood smears. The CNN model obtained high sensitivity of 97% and relatively\nhigh PPV of 81%. This study also proposes a false positive reduction method\nusing feature clustering extracted from the gray level co-occurrence matrix\n(GLCM) from the Region of Interests (ROIs). Adding the GLCM feature can\nsignificantly reduce false positives. However, this technique requires manual\nset up of silhouette and euclidean distance limits to ensure cluster quality,\nso it does not adversely affect sensitivity.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 14:01:39 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Abraham", "Julisa Bana", ""]]}, {"id": "1909.13126", "submitter": "Mohammad Rasool Izadi", "authors": "Mohammad Rasool Izadi", "title": "Feature Level Fusion from Facial Attributes for Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a deep convolutional neural networks (CNN) architecture to\nclassify facial attributes and recognize face images simultaneously via a\nshared learning paradigm to improve the accuracy for facial attribute\nprediction and face recognition performance. In this method, we use facial\nattributes as an auxiliary source of information to assist CNN features\nextracted from the face images to improve the face recognition performance.\nSpecifically, we use a shared CNN architecture that jointly predicts facial\nattributes and recognize face images simultaneously via a shared learning\nparameters, and then we use facial attribute features an an auxiliary source of\ninformation concatenated by face features to increase the discrimination of the\nCNN for face recognition. This process assists the CNN classifier to better\nrecognize face images. The experimental results show that our model increases\nboth the face recognition and facial attribute prediction performance,\nespecially for the identity attributes such as gender and race. We evaluated\nour method on several standard datasets labeled by identities and face\nattributes and the results show that the proposed method outperforms\nstate-of-the-art face recognition models.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 18:25:41 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Izadi", "Mohammad Rasool", ""]]}, {"id": "1909.13130", "submitter": "Chenxu Luo", "authors": "Chenxu Luo, Alan Yuille", "title": "Grouped Spatial-Temporal Aggregation for Efficient Action Recognition", "comments": "ICCV 2019", "journal-ref": "IEEE International Conference on Computer Vision (ICCV), 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal reasoning is an important aspect of video analysis. 3D CNN shows\ngood performance by exploring spatial-temporal features jointly in an\nunconstrained way, but it also increases the computational cost a lot. Previous\nworks try to reduce the complexity by decoupling the spatial and temporal\nfilters. In this paper, we propose a novel decomposition method that decomposes\nthe feature channels into spatial and temporal groups in parallel. This\ndecomposition can make two groups focus on static and dynamic cues separately.\nWe call this grouped spatial-temporal aggregation (GST). This decomposition is\nmore parameter-efficient and enables us to quantitatively analyze the\ncontributions of spatial and temporal features in different layers. We verify\nour model on several action recognition tasks that require temporal reasoning\nand show its effectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 19:03:02 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Luo", "Chenxu", ""], ["Yuille", "Alan", ""]]}, {"id": "1909.13135", "submitter": "Kamran Ali", "authors": "Kamran Ali, Charles E. Hughes", "title": "Facial Expression Recognition Using Disentangled Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The representation used for Facial Expression Recognition (FER) usually\ncontain expression information along with other variations such as identity and\nillumination. In this paper, we propose a novel Disentangled Expression\nlearning-Generative Adversarial Network (DE-GAN) to explicitly disentangle\nfacial expression representation from identity information. In this learning by\nreconstruction method, facial expression representation is learned by\nreconstructing an expression image employing an encoder-decoder based\ngenerator. This expression representation is disentangled from identity\ncomponent by explicitly providing the identity code to the decoder part of\nDE-GAN. The process of expression image reconstruction and disentangled\nexpression representation learning is improved by performing expression and\nidentity classification in the discriminator of DE-GAN. The disentangled facial\nexpression representation is then used for facial expression recognition\nemploying simple classifiers like SVM or MLP. The experiments are performed on\npublicly available and widely used face expression databases (CK+, MMI,\nOulu-CASIA). The experimental results show that the proposed technique produces\ncomparable results with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 19:18:16 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Ali", "Kamran", ""], ["Hughes", "Charles E.", ""]]}, {"id": "1909.13140", "submitter": "Khoi Nguyen", "authors": "Khoi Nguyen, Sinisa Todorovic", "title": "Feature Weighting and Boosting for Few-Shot Segmentation", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about few-shot segmentation of foreground objects in images. We\ntrain a CNN on small subsets of training images, each mimicking the few-shot\nsetting. In each subset, one image serves as the query and the other(s) as\nsupport image(s) with ground-truth segmentation. The CNN first extracts feature\nmaps from the query and support images. Then, a class feature vector is\ncomputed as an average of the support's feature maps over the known foreground.\nFinally, the target object is segmented in the query image by using a cosine\nsimilarity between the class feature vector and the query's feature map. We\nmake two contributions by: (1) Improving discriminativeness of features so\ntheir activations are high on the foreground and low elsewhere; and (2)\nBoosting inference with an ensemble of experts guided with the gradient of loss\nincurred when segmenting the support images in testing. Our evaluations on the\nPASCAL-$5^i$ and COCO-$20^i$ datasets demonstrate that we significantly\noutperform existing approaches.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 19:35:26 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Nguyen", "Khoi", ""], ["Todorovic", "Sinisa", ""]]}, {"id": "1909.13155", "submitter": "Jun Li", "authors": "Jun Li, Peng Lei, Sinisa Todorovic", "title": "Weakly Supervised Energy-Based Learning for Action Segmentation", "comments": "ICCV 2019 Oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about labeling video frames with action classes under weak\nsupervision in training, where we have access to a temporal ordering of\nactions, but their start and end frames in training videos are unknown.\nFollowing prior work, we use an HMM grounded on a Gated Recurrent Unit (GRU)\nfor frame labeling. Our key contribution is a new constrained discriminative\nforward loss (CDFL) that we use for training the HMM and GRU under weak\nsupervision. While prior work typically estimates the loss on a single,\ninferred video segmentation, our CDFL discriminates between the energy of all\nvalid and invalid frame labelings of a training video. A valid frame labeling\nsatisfies the ground-truth temporal ordering of actions, whereas an invalid one\nviolates the ground truth. We specify an efficient recursive algorithm for\ncomputing the CDFL in terms of the logadd function of the segmentation energy.\nOur evaluation on action segmentation and alignment gives superior results to\nthose of the state of the art on the benchmark Breakfast Action, Hollywood\nExtended, and 50Salads datasets.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 21:34:40 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Li", "Jun", ""], ["Lei", "Peng", ""], ["Todorovic", "Sinisa", ""]]}, {"id": "1909.13163", "submitter": "Yi Fang", "authors": "Yunxiao Shi, Jing Zhu, Yi Fang, Kuochin Lien, Junli Gu", "title": "Self-Supervised Learning of Depth and Ego-motion with Differentiable\n  Bundle Adjustment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to predict scene depth and camera motion from RGB inputs only is a\nchallenging task. Most existing learning based methods deal with this task in a\nsupervised manner which require ground-truth data that is expensive to acquire.\nMore recent approaches explore the possibility of estimating scene depth and\ncamera pose in a self-supervised learning framework. Despite encouraging\nresults are shown, current methods either learn from monocular videos for depth\nand pose and typically do so without enforcing multi-view geometry constraints\nbetween scene structure and camera motion, or require stereo sequences as input\nwhere the ground-truth between-frame motion parameters need to be known. In\nthis paper we propose to jointly optimize the scene depth and camera motion via\nincorporating differentiable Bundle Adjustment (BA) layer by minimizing the\nfeature-metric error, and then form the photometric consistency loss with view\nsynthesis as the final supervisory signal. The proposed approach only needs\nunlabeled monocular videos as input, and extensive experiments on the KITTI and\nCityscapes dataset show that our method achieves state-of-the-art results in\nself-supervised approaches using monocular videos as input, and even gains\nadvantage to the line of methods that learns from calibrated stereo sequences\n(i.e. with pose supervision).\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 22:23:32 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Shi", "Yunxiao", ""], ["Zhu", "Jing", ""], ["Fang", "Yi", ""], ["Lien", "Kuochin", ""], ["Gu", "Junli", ""]]}, {"id": "1909.13196", "submitter": "Zhiwei Deng", "authors": "Zhiwei Deng, Greg Mori", "title": "Policy Message Passing: A New Algorithm for Probabilistic Graph\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general graph-structured neural network architecture operates on graphs\nthrough two core components: (1) complex enough message functions; (2) a fixed\ninformation aggregation process. In this paper, we present the Policy Message\nPassing algorithm, which takes a probabilistic perspective and reformulates the\nwhole information aggregation as stochastic sequential processes. The algorithm\nworks on a much larger search space, utilizes reasoning history to perform\ninference, and is robust to noisy edges. We apply our algorithm to multiple\ncomplex graph reasoning and prediction tasks and show that our algorithm\nconsistently outperforms state-of-the-art graph-structured models by a\nsignificant margin.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 03:23:17 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Deng", "Zhiwei", ""], ["Mori", "Greg", ""]]}, {"id": "1909.13226", "submitter": "Enze Xie", "authors": "Enze Xie, Peize Sun, Xiaoge Song, Wenhai Wang, Ding Liang, Chunhua\n  Shen, Ping Luo", "title": "PolarMask: Single Shot Instance Segmentation with Polar Representation", "comments": "Accepted to Proc. IEEE Conf. Computer Vision and Pattern Recognition\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an anchor-box free and single shot instance\nsegmentation method, which is conceptually simple, fully convolutional and can\nbe used as a mask prediction module for instance segmentation, by easily\nembedding it into most off-the-shelf detection methods. Our method, termed\nPolarMask, formulates the instance segmentation problem as instance center\nclassification and dense distance regression in a polar coordinate. Moreover,\nwe propose two effective approaches to deal with sampling high-quality center\nexamples and optimization for dense distance regression, respectively, which\ncan significantly improve the performance and simplify the training process.\nWithout any bells and whistles, PolarMask achieves 32.9% in mask mAP with\nsingle-model and single-scale training/testing on challenging COCO dataset. For\nthe first time, we demonstrate a much simpler and flexible instance\nsegmentation framework achieving competitive accuracy. We hope that the\nproposed PolarMask framework can serve as a fundamental and strong baseline for\nsingle shot instance segmentation tasks. Code is available at:\ngithub.com/xieenze/PolarMask.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 07:52:49 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 11:49:03 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2020 09:21:42 GMT"}, {"version": "v4", "created": "Wed, 26 Feb 2020 01:49:49 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Xie", "Enze", ""], ["Sun", "Peize", ""], ["Song", "Xiaoge", ""], ["Wang", "Wenhai", ""], ["Liang", "Ding", ""], ["Shen", "Chunhua", ""], ["Luo", "Ping", ""]]}, {"id": "1909.13231", "submitter": "Yu Sun", "authors": "Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros,\n  Moritz Hardt", "title": "Test-Time Training with Self-Supervision for Generalization under\n  Distribution Shifts", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we propose Test-Time Training, a general approach for\nimproving the performance of predictive models when training and test data come\nfrom different distributions. We turn a single unlabeled test sample into a\nself-supervised learning problem, on which we update the model parameters\nbefore making a prediction. This also extends naturally to data in an online\nstream. Our simple approach leads to improvements on diverse image\nclassification benchmarks aimed at evaluating robustness to distribution\nshifts.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 08:09:15 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 06:34:47 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 18:09:39 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Sun", "Yu", ""], ["Wang", "Xiaolong", ""], ["Liu", "Zhuang", ""], ["Miller", "John", ""], ["Efros", "Alexei A.", ""], ["Hardt", "Moritz", ""]]}, {"id": "1909.13239", "submitter": "Weiyu Guo", "authors": "Weiyu Guo, Jiabin Ma, Liang Wang, Yongzhen Huang", "title": "Learning Efficient Convolutional Networks through Irregular\n  Convolutional Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep neural networks are increasingly used in applications suited for\nlow-power devices, a fundamental dilemma becomes apparent: the trend is to grow\nmodels to absorb increasing data that gives rise to memory intensive; however\nlow-power devices are designed with very limited memory that can not store\nlarge models. Parameters pruning is critical for deep model deployment on\nlow-power devices. Existing efforts mainly focus on designing highly efficient\nstructures or pruning redundant connections for networks. They are usually\nsensitive to the tasks or relay on dedicated and expensive hashing storage\nstrategies. In this work, we introduce a novel approach for achieving a\nlightweight model from the views of reconstructing the structure of\nconvolutional kernels and efficient storage. Our approach transforms a\ntraditional square convolution kernel to line segments, and automatically learn\na proper strategy for equipping these line segments to model diverse features.\nThe experimental results indicate that our approach can massively reduce the\nnumber of parameters (pruned 69% on DenseNet-40) and calculations (pruned 59%\non DenseNet-40) while maintaining acceptable performance (only lose less than\n2% accuracy).\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 08:50:52 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Guo", "Weiyu", ""], ["Ma", "Jiabin", ""], ["Wang", "Liang", ""], ["Huang", "Yongzhen", ""]]}, {"id": "1909.13240", "submitter": "Jialun Pei", "authors": "Jialun Pei, He Tang, Chao Liu, and Chuanbo Chen", "title": "Salient Instance Segmentation via Subitizing and Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of salient region detection is to identify the regions of an image\nthat attract the most attention. Many methods have achieved state-of-the-art\nperformance levels on this task. Recently, salient instance segmentation has\nbecome an even more challenging task than traditional salient region detection;\nhowever, few of the existing methods have concentrated on this underexplored\nproblem. Unlike the existing methods, which usually employ object proposals to\nroughly count and locate object instances, our method applies salient objects\nsubitizing to predict an accurate number of instances for salient instance\nsegmentation. In this paper, we propose a multitask densely connected neural\nnetwork (MDNN) to segment salient instances in an image. In contrast to\nexisting approaches, our framework is proposal-free and category-independent.\nThe MDNN contains two parallel branches: the first is a densely connected\nsubitizing network (DSN) used for subitizing prediction; the second is a\ndensely connected fully convolutional network (DFCN) used for salient region\ndetection. The MDNN simultaneously outputs saliency maps and salient object\nsubitizing. Then, an adaptive deep feature-based spectral clustering operation\nsegments the salient regions into instances based on the subitizing and\nsaliency maps. The experimental results on both salient region detection and\nsalient instance segmentation datasets demonstrate the satisfactory performance\nof our framework. Notably, its APr@0.5 and Apr@0.7 reaches 73.46% and 60.14% in\nthe salient instance dataset, substantially higher than the results achieved by\nthe state-of-the-art algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 09:02:03 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Pei", "Jialun", ""], ["Tang", "He", ""], ["Liu", "Chao", ""], ["Chen", "Chuanbo", ""]]}, {"id": "1909.13245", "submitter": "Xiangbo Shu", "authors": "Xiangbo Shu, Liyan Zhang, Guo-Jun Qi, Wei Liu, and Jinhui Tang", "title": "Spatiotemporal Co-attention Recurrent Neural Networks for Human-Skeleton\n  Motion Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion prediction aims to generate future motions based on the observed\nhuman motions. Witnessing the success of Recurrent Neural Networks (RNN) in\nmodeling the sequential data, recent works utilize RNN to model human-skeleton\nmotion on the observed motion sequence and predict future human motions.\nHowever, these methods did not consider the existence of the spatial coherence\namong joints and the temporal evolution among skeletons, which reflects the\ncrucial characteristics of human motion in spatiotemporal space. To this end,\nwe propose a novel Skeleton-joint Co-attention Recurrent Neural Networks\n(SC-RNN) to capture the spatial coherence among joints, and the temporal\nevolution among skeletons simultaneously on a skeleton-joint co-attention\nfeature map in spatiotemporal space. First, a skeleton-joint feature map is\nconstructed as the representation of the observed motion sequence. Second, we\ndesign a new Skeleton-joint Co-Attention (SCA) mechanism to dynamically learn a\nskeleton-joint co-attention feature map of this skeleton-joint feature map,\nwhich can refine the useful observed motion information to predict one future\nmotion. Third, a variant of GRU embedded with SCA collaboratively models the\nhuman-skeleton motion and human-joint motion in spatiotemporal space by\nregarding the skeleton-joint co-attention feature map as the motion context.\nExperimental results on human motion prediction demonstrate the proposed method\noutperforms the related methods.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 09:50:24 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 15:30:51 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Shu", "Xiangbo", ""], ["Zhang", "Liyan", ""], ["Qi", "Guo-Jun", ""], ["Liu", "Wei", ""], ["Tang", "Jinhui", ""]]}, {"id": "1909.13247", "submitter": "Youngeun Kim", "authors": "Youngeun Kim, Seokeon Choi, Hankyeol Lee, Taekyung Kim and Changick\n  Kim", "title": "RPM-Net: Robust Pixel-Level Matching Networks for Self-Supervised Video\n  Object Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a self-supervised approach for video object\nsegmentation without human labeled data.Specifically, we present Robust\nPixel-level Matching Net-works (RPM-Net), a novel deep architecture that\nmatches pixels between adjacent frames, using only color information from\nunlabeled videos for training. Technically, RPM-Net can be separated in two\nmain modules. The embed-ding module first projects input images into high\ndimensional embedding space. Then the matching module with deformable\nconvolution layers matches pixels between reference and target frames based on\nthe embedding features.Unlike previous methods using deformable convolution,\nour matching module adopts deformable convolution to focus on similar features\nin spatio-temporally neighboring pixels.Our experiments show that the selective\nfeature sampling improves the robustness to challenging problems in video\nobject segmentation such as camera shake, fast motion, deformation, and\nocclusion. Also, we carry out comprehensive experiments on three public\ndatasets (i.e., DAVIS-2017,SegTrack-v2, and Youtube-Objects) and achieve\nstate-of-the-art performance on self-supervised video object seg-mentation.\nMoreover, we significantly reduce the performance gap between self-supervised\nand fully-supervised video object segmentation (41.0% vs. 52.5% on DAVIS-2017\nvalidation set)\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 10:07:02 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 12:02:26 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Kim", "Youngeun", ""], ["Choi", "Seokeon", ""], ["Lee", "Hankyeol", ""], ["Kim", "Taekyung", ""], ["Kim", "Changick", ""]]}, {"id": "1909.13248", "submitter": "Youngeun Kim", "authors": "Youngeun Kim, Seokeon Choi, Taekyung Kim, Sumin Lee and Changick Kim", "title": "Learning to Align Multi-Camera Domains using Part-Aware Clustering for\n  Unsupervised Video Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most video person re-identification (re-ID) methods are mainly based on\nsupervised learning, which requires cross-camera ID labeling. Since the cost of\nlabeling increases dramatically as the number of cameras increases, it is\ndifficult to apply the re-identification algorithm to a large camera network.\nIn this paper, we address the scalability issue by presenting deep\nrepresentation learning without ID information across multiple cameras.\nTechnically, we train neural networks to generate both ID-discriminative and\ncamera-invariant features. To achieve the ID discrimination ability of the\nembedding features, we maximize feature distances between different person IDs\nwithin a camera by using a metric learning approach. At the same time,\nconsidering each camera as a different domain, we apply adversarial learning\nacross multiple camera domains for generating camera-invariant features. We\nalso propose a part-aware adaptation module, which effectively performs\nmulti-camera domain invariant feature learning in different spatial regions. We\ncarry out comprehensive experiments on three public re-ID datasets (i.e.,\nPRID-2011, iLIDS-VID, and MARS). Our method outperforms state-of-the-art\nmethods by a large margin of about 20\\% in terms of rank-1 accuracy on the\nlarge-scale MARS dataset.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 10:07:05 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 03:04:50 GMT"}, {"version": "v3", "created": "Thu, 28 Nov 2019 06:10:33 GMT"}, {"version": "v4", "created": "Wed, 13 May 2020 04:11:20 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Kim", "Youngeun", ""], ["Choi", "Seokeon", ""], ["Kim", "Taekyung", ""], ["Lee", "Sumin", ""], ["Kim", "Changick", ""]]}, {"id": "1909.13258", "submitter": "Muhammad Faisal", "authors": "Muhammad Faisal, Ijaz Akhter, Mohsen Ali and Richard Hartley", "title": "EpO-Net: Exploiting Geometric Constraints on Dense Trajectories for\n  Motion Saliency", "comments": "Accepted in IEEE Winter Conference on Applications of Computer\n  Vision(WACV-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing approaches for salient motion segmentation are unable to\nexplicitly learn geometric cues and often give false detections on prominent\nstatic objects. We exploit multiview geometric constraints to avoid such\nshortcomings. To handle the nonrigid background like a sea, we also propose a\nrobust fusion mechanism between motion and appearance-based features. We find\ndense trajectories, covering every pixel in the video, and propose\ntrajectory-based epipolar distances to distinguish between background and\nforeground regions. Trajectory epipolar distances are data-independent and can\nbe readily computed given a few features' correspondences between the images.\nWe show that by combining epipolar distances with optical flow, a powerful\nmotion network can be learned. Enabling the network to leverage both of these\nfeatures, we propose a simple mechanism, we call input-dropout. Comparing the\nmotion-only networks, we outperform the previous state of the art on DAVIS-2016\ndataset by 5.2% in the mean IoU score. By robustly fusing our motion network\nwith an appearance network using the input-dropout mechanism, we also\noutperform the previous methods on DAVIS-2016, 2017 and Segtrackv2 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 11:17:01 GMT"}, {"version": "v2", "created": "Sat, 18 Jan 2020 10:40:58 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Faisal", "Muhammad", ""], ["Akhter", "Ijaz", ""], ["Ali", "Mohsen", ""], ["Hartley", "Richard", ""]]}, {"id": "1909.13299", "submitter": "Yice Cao", "authors": "Yice Cao, Yan Wu, Peng Zhang, Wenkai Liang, Ming Li", "title": "Pixel-Wise PolSAR Image Classification via a Novel Complex-Valued Deep\n  Fully Convolutional Network", "comments": "17 pages, 12 figures, first submission on May 20th, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although complex-valued (CV) neural networks have shown better classification\nresults compared to their real-valued (RV) counterparts for polarimetric\nsynthetic aperture radar (PolSAR) classification, the extension of pixel-level\nRV networks to the complex domain has not yet thoroughly examined. This paper\npresents a novel complex-valued deep fully convolutional neural network\n(CV-FCN) designed for PolSAR image classification. Specifically, CV-FCN uses\nPolSAR CV data that includes the phase information and utilizes the deep FCN\narchitecture that performs pixel-level labeling. It integrates the feature\nextraction module and the classification module in a united framework.\nTechnically, for the particularity of PolSAR data, a dedicated complex-valued\nweight initialization scheme is defined to initialize CV-FCN. It considers the\ndistribution of polarization data to conduct CV-FCN training from scratch in an\nefficient and fast manner. CV-FCN employs a complex\ndownsampling-then-upsampling scheme to extract dense features. To enrich\ndiscriminative information, multi-level CV features that retain more\npolarization information are extracted via the complex downsampling scheme.\nThen, a complex upsampling scheme is proposed to predict dense CV labeling. It\nemploys complex max-unpooling layers to greatly capture more spatial\ninformation for better robustness to speckle noise. In addition, to achieve\nfaster convergence and obtain more precise classification results, a novel\naverage cross-entropy loss function is derived for CV-FCN optimization.\nExperiments on real PolSAR datasets demonstrate that CV-FCN achieves better\nclassification performance than other state-of-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 15:03:38 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Cao", "Yice", ""], ["Wu", "Yan", ""], ["Zhang", "Peng", ""], ["Liang", "Wenkai", ""], ["Li", "Ming", ""]]}, {"id": "1909.13359", "submitter": "Ali Hatamizadeh", "authors": "Ali Hatamizadeh, Debleena Sengupta and Demetri Terzopoulos", "title": "End-to-End Deep Convolutional Active Contours for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Active Contour Model (ACM) is a standard image analysis technique whose\nnumerous variants have attracted an enormous amount of research attention\nacross multiple fields. Incorrectly, however, the ACM's\ndifferential-equation-based formulation and prototypical dependence on user\ninitialization have been regarded as being largely incompatible with the\nrecently popular deep learning approaches to image segmentation. This paper\nintroduces the first tight unification of these two paradigms. In particular,\nwe devise Deep Convolutional Active Contours (DCAC), a truly end-to-end\ntrainable image segmentation framework comprising a Convolutional Neural\nNetwork (CNN) and an ACM with learnable parameters. The ACM's Eulerian energy\nfunctional includes per-pixel parameter maps predicted by the backbone CNN,\nwhich also initializes the ACM. Importantly, both the CNN and ACM components\nare fully implemented in TensorFlow, and the entire DCAC architecture is\nend-to-end automatically differentiable and backpropagation trainable without\nuser intervention. As a challenging test case, we tackle the problem of\nbuilding instance segmentation in aerial images and evaluate DCAC on two\npublicly available datasets, Vaihingen and Bing Huts. Our reseults demonstrate\nthat, for building segmentation, the DCAC establishes a new state-of-the-art\nperformance by a wide margin.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 20:23:07 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 22:30:20 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Hatamizadeh", "Ali", ""], ["Sengupta", "Debleena", ""], ["Terzopoulos", "Demetri", ""]]}, {"id": "1909.13374", "submitter": "Neehar Peri", "authors": "Neehar Peri, Neal Gupta, W. Ronny Huang, Liam Fowl, Chen Zhu, Soheil\n  Feizi, Tom Goldstein, John P. Dickerson", "title": "Deep k-NN Defense against Clean-label Data Poisoning Attacks", "comments": "Accepted to ECCV 2020 Workshop - Adversarial Robustness in the Real\n  World (AROW). First three authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Targeted clean-label data poisoning is a type of adversarial attack on\nmachine learning systems in which an adversary injects a few correctly-labeled,\nminimally-perturbed samples into the training data, causing a model to\nmisclassify a particular test sample during inference. Although defenses have\nbeen proposed for general poisoning attacks, no reliable defense for\nclean-label attacks has been demonstrated, despite the attacks' effectiveness\nand realistic applications. In this work, we propose a simple, yet\nhighly-effective Deep k-NN defense against both feature collision and convex\npolytope clean-label attacks on the CIFAR-10 dataset. We demonstrate that our\nproposed strategy is able to detect over 99% of poisoned examples in both\nattacks and remove them without compromising model performance. Additionally,\nthrough ablation studies, we discover simple guidelines for selecting the value\nof k as well as for implementing the Deep k-NN defense on real-world datasets\nwith class imbalance. Our proposed defense shows that current clean-label\npoisoning attack strategies can be annulled, and serves as a strong yet\nsimple-to-implement baseline defense to test future clean-label poisoning\nattacks. Our code is available at https://github.com/neeharperi/DeepKNNDefense\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 21:47:14 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 02:38:02 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2020 05:47:23 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Peri", "Neehar", ""], ["Gupta", "Neal", ""], ["Huang", "W. Ronny", ""], ["Fowl", "Liam", ""], ["Zhu", "Chen", ""], ["Feizi", "Soheil", ""], ["Goldstein", "Tom", ""], ["Dickerson", "John P.", ""]]}, {"id": "1909.13377", "submitter": "Jiacheng Pan", "authors": "Jiacheng Pan, Hongyi Sun, Kecheng Xu, Yifei Jiang, Xiangquan Xiao,\n  Jiangtao Hu, Jinghao Miao", "title": "Lane Attention: Predicting Vehicles' Moving Trajectories by Learning\n  Their Attention over Lanes", "comments": "IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately forecasting the future movements of surrounding vehicles is\nessential for safe and efficient operations of autonomous driving cars. This\ntask is difficult because a vehicle's moving trajectory is greatly determined\nby its driver's intention, which is often hard to estimate. By leveraging\nattention mechanisms along with long short-term memory (LSTM) networks, this\nwork learns the relation between a driver's intention and the vehicle's\nchanging positions relative to road infrastructures, and uses it to guide the\nprediction. Different from other state-of-the-art solutions, our work treats\nthe on-road lanes as non-Euclidean structures, unfolds the vehicle's moving\nhistory to form a spatio-temporal graph, and uses methods from Graph Neural\nNetworks to solve the problem. Not only is our approach a pioneering attempt in\nusing non-Euclidean methods to process static environmental features around a\npredicted object, our model also outperforms other state-of-the-art models in\nseveral metrics. The practicability and interpretability analysis of the model\nshows great potential for large-scale deployment in various autonomous driving\nsystems in addition to our own.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 21:50:39 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 05:44:24 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Pan", "Jiacheng", ""], ["Sun", "Hongyi", ""], ["Xu", "Kecheng", ""], ["Jiang", "Yifei", ""], ["Xiao", "Xiangquan", ""], ["Hu", "Jiangtao", ""], ["Miao", "Jinghao", ""]]}, {"id": "1909.13395", "submitter": "Benjamin Busam", "authors": "Benjamin Busam and Matthieu Hog and Steven McDonagh and Gregory\n  Slabaugh", "title": "SteReFo: Efficient Image Refocusing with Stereo Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whether to attract viewer attention to a particular object, give the\nimpression of depth or simply reproduce human-like scene perception, shallow\ndepth of field images are used extensively by professional and amateur\nphotographers alike. To this end, high quality optical systems are used in DSLR\ncameras to focus on a specific depth plane while producing visually pleasing\nbokeh. We propose a physically motivated pipeline to mimic this effect from\nall-in-focus stereo images, typically retrieved by mobile cameras. It is\ncapable to change the focal plane a posteriori at 76 FPS on KITTI images to\nenable real-time applications. As our portmanteau suggests, SteReFo\ninterrelates stereo-based depth estimation and refocusing efficiently. In\ncontrast to other approaches, our pipeline is simultaneously fully\ndifferentiable, physically motivated, and agnostic to scene content. It also\nenables computational video focus tracking for moving objects in addition to\nrefocusing of static images. We evaluate our approach on the publicly available\ndatasets SceneFlow, KITTI, CityScapes and quantify the quality of architectural\nchanges.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 23:12:16 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Busam", "Benjamin", ""], ["Hog", "Matthieu", ""], ["McDonagh", "Steven", ""], ["Slabaugh", "Gregory", ""]]}, {"id": "1909.13396", "submitter": "Caiwen Ding", "authors": "Caiwen Ding, Shuo Wang, Ning Liu, Kaidi Xu, Yanzhi Wang and Yun Liang", "title": "REQ-YOLO: A Resource-Aware, Efficient Quantization Framework for Object\n  Detection on FPGAs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs), as the basis of object detection, will play a\nkey role in the development of future autonomous systems with full autonomy.\nThe autonomous systems have special requirements of real-time, energy-efficient\nimplementations of DNNs on a power-constrained system. Two research thrusts are\ndedicated to performance and energy efficiency enhancement of the inference\nphase of DNNs. The first one is model compression techniques while the second\nis efficient hardware implementation. Recent works on extremely-low-bit CNNs\nsuch as the binary neural network (BNN) and XNOR-Net replace the traditional\nfloating-point operations with binary bit operations which significantly\nreduces the memory bandwidth and storage requirement. However, it suffers from\nnon-negligible accuracy loss and underutilized digital signal processing (DSP)\nblocks of FPGAs. To overcome these limitations, this paper proposes REQ-YOLO, a\nresource-aware, systematic weight quantization framework for object detection,\nconsidering both algorithm and hardware resource aspects in object detection.\nWe adopt the block-circulant matrix method and propose a heterogeneous weight\nquantization using the Alternating Direction Method of Multipliers (ADMM), an\neffective optimization technique for general, non-convex optimization problems.\nTo achieve real-time, highly-efficient implementations on FPGA, we present the\ndetailed hardware implementation of block circulant matrices on CONV layers and\ndevelop an efficient processing element (PE) structure supporting the\nheterogeneous weight quantization, CONV dataflow and pipelining techniques,\ndesign optimization, and a template-based automatic synthesis framework to\noptimally exploit hardware resource. Experimental results show that our\nproposed REQ-YOLO framework can significantly compress the YOLO model while\nintroducing very small accuracy degradation.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 23:21:05 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Ding", "Caiwen", ""], ["Wang", "Shuo", ""], ["Liu", "Ning", ""], ["Xu", "Kaidi", ""], ["Wang", "Yanzhi", ""], ["Liang", "Yun", ""]]}, {"id": "1909.13411", "submitter": "Guoqiang Zhong", "authors": "Zhenlin Fan, Guoqiang Zhong", "title": "SymmetricNet: A mesoscale eddy detection method based on multivariate\n  fusion data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mesoscale eddies play a significant role in marine energy transport, marine\nbiological environment and marine climate. Due to their huge impact on the\nocean, mesoscale eddy detection has become a hot research area in recent years.\nTherefore, more and more people are entering the field of mesoscale eddy\ndetection. However, the existing detection methods mainly based on traditional\ndetection methods typically only use Sea Surface Height (SSH) as a variable to\ndetect, resulting in inaccurate performance. In this paper, we propose a\nmesoscale eddy detection method based on multivariate fusion data to solve this\nproblem. We not only use the SSH variable, but also add the two variables: Sea\nSurface Temperature (SST) and velocity of flow, achieving a multivariate\ninformation fusion input. We design a novel symmetric network, which merges\nlow-level feature maps from the downsampling pathway and high-level feature\nmaps from the upsampling pathway by lateral connection. In addition, we apply\ndilated convolutions to network structure to increase the receptive field and\nobtain more contextual information in the case of constant parameter. In the\nend, we demonstrate the effectiveness of our method on dataset provided by us,\nachieving the test set performance of 97.06% , greatly improved the performance\nof previous methods of mesoscale eddy detection.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 00:58:04 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Fan", "Zhenlin", ""], ["Zhong", "Guoqiang", ""]]}, {"id": "1909.13423", "submitter": "Gines Hidalgo Martinez", "authors": "Gines Hidalgo and Yaadhav Raaj and Haroon Idrees and Donglai Xiang and\n  Hanbyul Joo and Tomas Simon and Yaser Sheikh", "title": "Single-Network Whole-Body Pose Estimation", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first single-network approach for 2D~whole-body pose\nestimation, which entails simultaneous localization of body, face, hands, and\nfeet keypoints. Due to the bottom-up formulation, our method maintains constant\nreal-time performance regardless of the number of people in the image. The\nnetwork is trained in a single stage using multi-task learning, through an\nimproved architecture which can handle scale differences between body/foot and\nface/hand keypoints. Our approach considerably improves upon\nOpenPose~\\cite{cao2018openpose}, the only work so far capable of whole-body\npose estimation, both in terms of speed and global accuracy. Unlike OpenPose,\nour method does not need to run an additional network for each hand and face\ncandidate, making it substantially faster for multi-person scenarios. This work\ndirectly results in a reduction of computational complexity for applications\nthat require 2D whole-body information (e.g., VR/AR, re-targeting). In\naddition, it yields higher accuracy, especially for occluded, blurry, and low\nresolution faces and hands. For code, trained models, and validation\nbenchmarks, visit our project page:\nhttps://github.com/CMU-Perceptual-Computing-Lab/openpose_train.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 02:00:53 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Hidalgo", "Gines", ""], ["Raaj", "Yaadhav", ""], ["Idrees", "Haroon", ""], ["Xiang", "Donglai", ""], ["Joo", "Hanbyul", ""], ["Simon", "Tomas", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1909.13446", "submitter": "Xinlin Li", "authors": "Xinlin Li, Vahid Partovi Nia", "title": "Random Bias Initialization Improves Quantized Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary neural networks improve computationally efficiency of deep models with\na large margin. However, there is still a performance gap between a successful\nfull-precision training and binary training. We bring some insights about why\nthis accuracy drop exists and call for a better understanding of binary network\ngeometry. We start with analyzing full-precision neural networks with ReLU\nactivation and compare it with its binarized version. This comparison suggests\nto initialize networks with random bias, a counter-intuitive remedy.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 04:01:13 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 19:50:23 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Li", "Xinlin", ""], ["Nia", "Vahid Partovi", ""]]}, {"id": "1909.13465", "submitter": "Takumi Ichimura", "authors": "Shin Kamada and Takumi Ichimura", "title": "An Object Detection by using Adaptive Structural Learning of Deep Belief\n  Network", "comments": "8 pages, 14 figures, The International Joint Conference on Neural\n  Networks (IJCNN 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning forms a hierarchical network structure for representation of\nmultiple input features. The adaptive structural learning method of Deep Belief\nNetwork (DBN) can realize a high classification capability while searching the\noptimal network structure during the training. The method can find the optimal\nnumber of hidden neurons for given input data in a Restricted Boltzmann Machine\n(RBM) by neuron generation-annihilation algorithm. Moreover, it can generate a\nnew hidden layer in DBN by the layer generation algorithm to actualize a deep\ndata representation. The proposed method showed higher classification accuracy\nfor image benchmark data sets than several deep learning methods including\nwell-known CNN methods. In this paper, a new object detection method for the\nDBN architecture is proposed for localization and category of objects. The\nmethod is a task for finding semantic objects in images as Bounding Box\n(B-Box). To investigate the effectiveness of the proposed method, the adaptive\nstructural learning of DBN and the object detection were evaluated on the Chest\nX-ray image benchmark data set (CXR8), which is one of the most commonly\naccessible radio-logical examination for many lung diseases. The proposed\nmethod showed higher performance for both classification (more than 94.5%\nclassification for test data) and localization (more than 90.4% detection for\ntest data) than the other CNN methods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 05:49:02 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Kamada", "Shin", ""], ["Ichimura", "Takumi", ""]]}, {"id": "1909.13470", "submitter": "Albert Mosella-Montoro", "authors": "Albert Mosella-Montoro, Javier Ruiz-Hidalgo", "title": "Residual Attention Graph Convolutional Network for Geometric 3D Scene\n  Classification", "comments": null, "journal-ref": null, "doi": "10.1109/ICCVW.2019.00507", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric 3D scene classification is a very challenging task. Current\nmethodologies extract the geometric information using only a depth channel\nprovided by an RGB-D sensor. These kinds of methodologies introduce possible\nerrors due to missing local geometric context in the depth channel. This work\nproposes a novel Residual Attention Graph Convolutional Network that exploits\nthe intrinsic geometric context inside a 3D space without using any kind of\npoint features, allowing the use of organized or unorganized 3D data.\nExperiments are done in NYU Depth v1 and SUN-RGBD datasets to study the\ndifferent configurations and to demonstrate the effectiveness of the proposed\nmethod. Experimental results show that the proposed method outperforms current\nstate-of-the-art in geometric 3D scene classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 06:22:47 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Mosella-Montoro", "Albert", ""], ["Ruiz-Hidalgo", "Javier", ""]]}, {"id": "1909.13471", "submitter": "Damien Teney", "authors": "Damien Teney, Ehsan Abbasnejad, Anton van den Hengel", "title": "On Incorporating Semantic Prior Knowledge in Deep Learning Through\n  Embedding-Space Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The knowledge that humans hold about a problem often extends far beyond a set\nof training data and output labels. While the success of deep learning mostly\nrelies on supervised training, important properties cannot be inferred\nefficiently from end-to-end annotations alone, for example causal relations or\ndomain-specific invariances. We present a general technique to supplement\nsupervised training with prior knowledge expressed as relations between\ntraining instances. We illustrate the method on the task of visual question\nanswering to exploit various auxiliary annotations, including relations of\nequivalence and of logical entailment between questions. Existing methods to\nuse these annotations, including auxiliary losses and data augmentation, cannot\nguarantee the strict inclusion of these relations into the model since they\nrequire a careful balancing against the end-to-end objective. Our method uses\nthese relations to shape the embedding space of the model, and treats them as\nstrict constraints on its learned representations. In the context of VQA, this\napproach brings significant improvements in accuracy and robustness, in\nparticular over the common practice of incorporating the constraints as a soft\nregularizer. We also show that incorporating this type of prior knowledge with\nour method brings consistent improvements, independently from the amount of\nsupervised data used. It demonstrates the value of an additional training\nsignal that is otherwise difficult to extract from end-to-end annotations\nalone.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 06:26:09 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 04:07:47 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Teney", "Damien", ""], ["Abbasnejad", "Ehsan", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1909.13474", "submitter": "Alexandros Stergiou MSc", "authors": "Alexandros Stergiou and Ronald Poppe", "title": "Spatio-Temporal FAST 3D Convolutions for Human Action Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/ICMLA.2019.00036", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective processing of video input is essential for the recognition of\ntemporally varying events such as human actions. Motivated by the often\ndistinctive temporal characteristics of actions in either horizontal or\nvertical direction, we introduce a novel convolution block for CNN\narchitectures with video input. Our proposed Fractioned Adjacent Spatial and\nTemporal (FAST) 3D convolutions are a natural decomposition of a regular 3D\nconvolution. Each convolution block consist of three sequential convolution\noperations: a 2D spatial convolution followed by spatio-temporal convolutions\nin the horizontal and vertical direction, respectively. Additionally, we\nintroduce a FAST variant that treats horizontal and vertical motion in\nparallel. Experiments on benchmark action recognition datasets UCF-101 and\nHMDB-51 with ResNet architectures demonstrate consistent increased performance\nof FAST 3D convolution blocks over traditional 3D convolutions. The lower\nvalidation loss indicates better generalization, especially for deeper\nnetworks. We also evaluate the performance of CNN architectures with similar\nmemory requirements, based either on Two-stream networks or with 3D convolution\nblocks. DenseNet-121 with FAST 3D convolutions was shown to perform best,\ngiving further evidence of the merits of the decoupled spatio-temporal\nconvolutions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 06:34:59 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 13:30:21 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Stergiou", "Alexandros", ""], ["Poppe", "Ronald", ""]]}, {"id": "1909.13476", "submitter": "Kartik Gupta", "authors": "Kartik Gupta, Lars Petersson and Richard Hartley", "title": "CullNet: Calibrated and Pose Aware Confidence Scores for Object Pose\n  Estimation", "comments": "ICCV Workshop on Recovering 6D Object Pose, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for a single view, image-based object pose\nestimation. Specifically, the problem of culling false positives among several\npose proposal estimates is addressed in this paper. Our proposed approach\ntargets the problem of inaccurate confidence values predicted by CNNs which is\nused by many current methods to choose a final object pose prediction. We\npresent a network called CullNet, solving this task. CullNet takes pairs of\npose masks rendered from a 3D model and cropped regions in the original image\nas input. This is then used to calibrate the confidence scores of the pose\nproposals. This new set of confidence scores is found to be significantly more\nreliable for accurate object pose estimation as shown by our results. Our\nexperimental results on multiple challenging datasets (LINEMOD and Occlusion\nLINEMOD) reflects the utility of our proposed method. Our overall pose\nestimation pipeline outperforms state-of-the-art object pose estimation methods\non these standard object pose estimation datasets. Our code is publicly\navailable on https://github.com/kartikgupta-at-anu/CullNet.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 06:39:05 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Gupta", "Kartik", ""], ["Petersson", "Lars", ""], ["Hartley", "Richard", ""]]}, {"id": "1909.13480", "submitter": "Takumi Ichimura", "authors": "Shin Kamada and Takumi Ichimura", "title": "A Video Recognition Method by using Adaptive Structural Learning of Long\n  Short Term Memory based Deep Belief Network", "comments": "6 pages, 7 figures, IEEE 11th International Workshop on Computational\n  Intelligence and Applications (IWCIA2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning builds deep architectures such as multi-layered artificial\nneural networks to effectively represent multiple features of input patterns.\nThe adaptive structural learning method of Deep Belief Network (DBN) can\nrealize a high classification capability while searching the optimal network\nstructure during the training. The method can find the optimal number of hidden\nneurons of a Restricted Boltzmann Machine (RBM) by neuron\ngeneration-annihilation algorithm to train the given input data, and then it\ncan make a new layer in DBN by the layer generation algorithm to actualize a\ndeep data representation. Moreover, the learning algorithm of Adaptive RBM and\nAdaptive DBN was extended to the time-series analysis by using the idea of LSTM\n(Long Short Term Memory). In this paper, our proposed prediction method was\napplied to Moving MNIST, which is a benchmark data set for video recognition.\nWe challenge to reveal the power of our proposed method in the video\nrecognition research field, since video includes rich source of visual\ninformation. Compared with the LSTM model, our method showed higher prediction\nperformance (more than 90% predication accuracy for test data).\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 06:57:55 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Kamada", "Shin", ""], ["Ichimura", "Takumi", ""]]}, {"id": "1909.13481", "submitter": "Takumi Ichimura", "authors": "Takumi Ichimura and Shin Kamada", "title": "Re-learning of Child Model for Misclassified data by using KL Divergence\n  in AffectNet: A Database for Facial Expression", "comments": "6 pages, 7 figures, IEEE 11th International Workshop on Computational\n  Intelligence and Applications (IWCIA2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AffectNet contains more than 1,000,000 facial images which manually annotated\nfor the presence of eight discrete facial expressions and the intensity of\nvalence and arousal. Adaptive structural learning method of DBN (Adaptive DBN)\nis positioned as a top Deep learning model of classification capability for\nsome large image benchmark databases. The Convolutional Neural Network and\nAdaptive DBN were trained for AffectNet and classification capability was\ncompared. Adaptive DBN showed higher classification ratio. However, the model\nwas not able to classify some test cases correctly because human emotions\ncontain many ambiguous features or patterns leading wrong answer which includes\nthe possibility of being a factor of adversarial examples, due to two or more\nannotators answer different subjective judgment for an image. In order to\ndistinguish such cases, this paper investigated a re-learning model of Adaptive\nDBN with two or more child models, where the original trained model can be seen\nas a parent model and then new child models are generated for some\nmisclassified cases. In addition, an appropriate child model was generated\naccording to difference between two models by using KL divergence. The\ngenerated child models showed better performance to classify two emotion\ncategories: `Disgust' and `Anger'.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 06:58:27 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Ichimura", "Takumi", ""], ["Kamada", "Shin", ""]]}, {"id": "1909.13486", "submitter": "Stuart Eiffert", "authors": "Stuart Eiffert and Salah Sukkarieh", "title": "Predicting Responses to a Robot's Future Motion using Generative\n  Recurrent Neural Networks", "comments": "Accepted at Australasian Conference on Robotics and Automation (ACRA)\n  2019", "journal-ref": "Proceedings of the Australasian Conference on Robotics and\n  Automation (ACRA) 2019", "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic navigation through crowds or herds requires the ability to both\npredict the future motion of nearby individuals and understand how these\npredictions might change in response to a robot's future action. State of the\nart trajectory prediction models using Recurrent Neural Networks (RNNs) do not\ncurrently account for a planned future action of a robot, and so cannot predict\nhow an individual will move in response to a robot's planned path. We propose\nan approach that adapts RNNs to use a robot's next planned action as an input\nalongside the current position of nearby individuals. This allows the model to\nlearn the response of individuals with regards to a robot's motion from real\nworld observations. By linking a robot's actions to the response of those\naround it in training, we show that we are able to not only improve prediction\naccuracy in close range interactions, but also to predict the likely response\nof surrounding individuals to simulated actions. This allows the use of the\nmodel to simulate state transitions, without requiring any assumptions on agent\ninteraction. We apply this model to varied datasets, including crowds of\npedestrians interacting with vehicles and bicycles, and livestock interacting\nwith a robotic vehicle.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 07:15:29 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 00:36:01 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Eiffert", "Stuart", ""], ["Sukkarieh", "Salah", ""]]}, {"id": "1909.13493", "submitter": "Xueyang Kang Mr.", "authors": "Xueyang Kang and Shunying Yuan", "title": "Robust Data Association for Object-level Semantic SLAM", "comments": "8 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous mapping and localization (SLAM) in an real indoor environment is\nstill a challenging task. Traditional SLAM approaches rely heavily on low-level\ngeometric constraints like corners or lines, which may lead to tracking failure\nin textureless surroundings or cluttered world with dynamic objects. In this\npaper, a compact semantic SLAM framework is proposed, with utilization of both\ngeometric and object-level semantic constraints jointly, a more consistent\nmapping result, and more accurate pose estimation can be obtained. Two main\ncontributions are presented int the paper, a) a robust and efficient SLAM data\nassociation and optimization framework is proposed, it models both discrete\nsemantic labeling and continuous pose. b) a compact map representation,\ncombining 2D Lidar map with object detection is presented. Experiments on\npublic indoor datasets, TUM-RGBD, ICL-NUIM, and our own collected datasets\nprove the improving of SLAM robustness and accuracy compared to other popular\nSLAM systems, meanwhile a map maintenance efficiency can be achieved.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 07:37:54 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Kang", "Xueyang", ""], ["Yuan", "Shunying", ""]]}, {"id": "1909.13501", "submitter": "Guang-Yuan Hao", "authors": "Guang-Yuan Hao, Hong-Xing Yu, Wei-Shi Zheng", "title": "DSRGAN: Explicitly Learning Disentangled Representation of Underlying\n  Structure and Rendering for Image Generation without Tuple Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on explicitly learning disentangled representation for natural image\ngeneration, where the underlying spatial structure and the rendering on the\nstructure can be independently controlled respectively, yet using no tuple\nsupervision. The setting is significant since tuple supervision is costly and\nsometimes even unavailable. However, the task is highly unconstrained and thus\nill-posed. To address this problem, we propose to introduce an auxiliary domain\nwhich shares a common underlying-structure space with the target domain, and we\nmake a partially shared latent space assumption. The key idea is to encourage\nthe partially shared latent variable to represent the similar underlying\nspatial structures in both domains, while the two domain-specific latent\nvariables will be unavoidably arranged to present renderings of two domains\nrespectively. This is achieved by designing two parallel generative networks\nwith a common Progressive Rendering Architecture (PRA), which constrains both\ngenerative networks' behaviors to model shared underlying structure and to\nmodel spatially dependent relation between rendering and underlying structure.\nThus, we propose DSRGAN (GANs for Disentangling Underlying Structure and\nRendering) to instantiate our method. We also propose a quantitative criterion\n(the Normalized Disentanglability) to quantify disentanglability. Comparison to\nthe state-of-the-art methods shows that DSRGAN can significantly outperform\nthem in disentanglability.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 08:07:11 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Hao", "Guang-Yuan", ""], ["Yu", "Hong-Xing", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1909.13522", "submitter": "Yang Shunzhi South China Normal University", "authors": "Shunzhi Yang, Zheng Gong, Kai Ye, Yungen Wei, Zheng Huang, Zhenhua\n  Huang", "title": "EdgeCNN: Convolutional Neural Network Classification Model with small\n  inputs for Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of Internet of Things (IoT), data is increasingly\nappearing on the edge of the network. Processing tasks on the edge of the\nnetwork can effectively solve the problems of personal privacy leaks and server\noverload. As a result, it has attracted a great deal of attention and made\nsubstantial progress. This progress includes efficient convolutional neural\nnetwork (CNN) models such as MobileNet and ShuffleNet. However, all of these\nnetworks appear as a common network model and they usually need to identify\nmultiple targets when applied. So the size of the input is very large. In some\nspecific cases, only the target needs to be classified. Therefore, a small\ninput network can be designed to reduce computation. In addition, other\nefficient neural network models are primarily designed for mobile phones.\nMobile phones have faster memory access, which allows them to use group\nconvolution. In particular, this paper finds that the recently widely used\ngroup convolution is not suitable for devices with very slow memory access.\nTherefore, the EdgeCNN of this paper is designed for edge computing devices\nwith low memory access speed and low computing resources. EdgeCNN has been run\nsuccessfully on the Raspberry Pi 3B+ at a speed of 1.37 frames per second. The\naccuracy of facial expression classification for the FER-2013 and RAF-DB\ndatasets outperforms other proposed networks that are compatible with the\nRaspberry Pi 3B+. The implementation of EdgeCNN is available at\nhttps://github.com/yangshunzhi1994/EdgeCNN\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 08:45:03 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Yang", "Shunzhi", ""], ["Gong", "Zheng", ""], ["Ye", "Kai", ""], ["Wei", "Yungen", ""], ["Huang", "Zheng", ""], ["Huang", "Zhenhua", ""]]}, {"id": "1909.13551", "submitter": "Kshitij Agrawal", "authors": "Kshitij Agrawal and Anbumani Subramanian", "title": "Enhancing Object Detection in Adverse Conditions using Thermal Imaging", "comments": "IROS 2019 Workshop on Towards Cognitive Vehicles", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving relies on deriving understanding of objects and scenes\nthrough images. These images are often captured by sensors in the visible\nspectrum. For improved detection capabilities we propose the use of thermal\nsensors to augment the vision capabilities of an autonomous vehicle. In this\npaper, we present our investigations on the fusion of visible and thermal\nspectrum images using a publicly available dataset, and use it to analyze the\nperformance of object recognition on other known driving datasets. We present\nan comparison of object detection in night time imagery and qualitatively\ndemonstrate that thermal images significantly improve detection accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 09:29:48 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Agrawal", "Kshitij", ""], ["Subramanian", "Anbumani", ""]]}, {"id": "1909.13561", "submitter": "Yizhe Wu", "authors": "Yizhe Wu, Sudhanshu Kasewa, Oliver Groth, Sasha Salter, Li Sun, Oiwi\n  Parker Jones, Ingmar Posner", "title": "Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the richness of information captured by the latent\nspace of a vision-based generative model. The model combines unsupervised\ngenerative learning with a task-based performance predictor to learn and to\nexploit task-relevant object affordances given visual observations from a\nreaching task, involving a scenario and a stick-like tool. While the learned\nembedding of the generative model captures factors of variation in 3D tool\ngeometry (e.g. length, width, and shape), the performance predictor identifies\nsub-manifolds of the embedding that correlate with task success. Within a\nvariety of scenarios, we demonstrate that traversing the latent space via\nbackpropagation from the performance predictor allows us to imagine tools\nappropriate for the task at hand. Our results indicate that affordances-like\nthe utility for reaching-are encoded along smooth trajectories in latent space.\nAccessing these emergent affordances by considering only high-level performance\ncriteria (such as task success) enables an agent to manipulate tool geometries\nin a targeted and deliberate way.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 09:55:33 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 11:37:36 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 06:51:23 GMT"}, {"version": "v4", "created": "Wed, 7 Oct 2020 04:05:19 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Wu", "Yizhe", ""], ["Kasewa", "Sudhanshu", ""], ["Groth", "Oliver", ""], ["Salter", "Sasha", ""], ["Sun", "Li", ""], ["Jones", "Oiwi Parker", ""], ["Posner", "Ingmar", ""]]}, {"id": "1909.13579", "submitter": "Etienne Bennequin", "authors": "Etienne Bennequin", "title": "Meta-learning algorithms for Few-Shot Computer Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-Shot Learning is the challenge of training a model with only a small\namount of data. Many solutions to this problem use meta-learning algorithms,\ni.e. algorithms that learn to learn. By sampling few-shot tasks from a larger\ndataset, we can teach these algorithms to solve new, unseen tasks. This\ndocument reports my work on meta-learning algorithms for Few-Shot Computer\nVision. This work was done during my internship at Sicara, a French company\nbuilding image recognition solutions for businesses. It contains: 1. an\nextensive review of the state-of-the-art in few-shot computer vision; 2. a\nbenchmark of meta-learning algorithms for few-shot image classification; 3. the\nintroduction to a novel meta-learning algorithm for few-shot object detection,\nwhich is still in development.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 10:51:16 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Bennequin", "Etienne", ""]]}, {"id": "1909.13583", "submitter": "Dongdong Yu", "authors": "Dongdong Yu, Kai Su, Hengkai Guo, Jian Wang, Kaihui Zhou, Yuanyuan\n  Huang, Minghui Dong, Jie Shao and Changhu Wang", "title": "Towards Good Practices for Video Object Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised video object segmentation is an interesting yet challenging\ntask in machine learning. In this work, we conduct a series of refinements with\nthe propagation-based video object segmentation method and empirically evaluate\ntheir impact on the final model performance through ablation study. By taking\nall the refinements, we improve the space-time memory networks to achieve a\nOverall of 79.1 on the Youtube-VOS Challenge 2019.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 11:00:09 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Yu", "Dongdong", ""], ["Su", "Kai", ""], ["Guo", "Hengkai", ""], ["Wang", "Jian", ""], ["Zhou", "Kaihui", ""], ["Huang", "Yuanyuan", ""], ["Dong", "Minghui", ""], ["Shao", "Jie", ""], ["Wang", "Changhu", ""]]}, {"id": "1909.13584", "submitter": "Laura Rieger", "authors": "Laura Rieger, Chandan Singh, W. James Murdoch, Bin Yu", "title": "Interpretations are useful: penalizing explanations to align neural\n  networks with prior knowledge", "comments": "18 pages; published in ICML2020; Erratum: numbers in table 1 were too\n  high (now corrected) with the trend remaining the same", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an explanation of a deep learning model to be effective, it must provide\nboth insight into a model and suggest a corresponding action in order to\nachieve some objective. Too often, the litany of proposed explainable deep\nlearning methods stop at the first step, providing practitioners with insight\ninto a model, but no way to act on it. In this paper, we propose contextual\ndecomposition explanation penalization (CDEP), a method which enables\npractitioners to leverage existing explanation methods in order to increase the\npredictive accuracy of deep learning models. In particular, when shown that a\nmodel has incorrectly assigned importance to some features, CDEP enables\npractitioners to correct these errors by directly regularizing the provided\nexplanations. Using explanations provided by contextual decomposition (CD)\n(Murdoch et al., 2018), we demonstrate the ability of our method to increase\nperformance on an array of toy and real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 11:02:01 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 12:05:59 GMT"}, {"version": "v3", "created": "Sat, 1 Aug 2020 19:24:50 GMT"}, {"version": "v4", "created": "Thu, 8 Oct 2020 12:43:21 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Rieger", "Laura", ""], ["Singh", "Chandan", ""], ["Murdoch", "W. James", ""], ["Yu", "Bin", ""]]}, {"id": "1909.13589", "submitter": "Minghao Chen", "authors": "Minghao Chen, Hongyang Xue, and Deng Cai", "title": "Domain Adaptation for Semantic Segmentation with Maximum Squares Loss", "comments": "Published in IEEE International Conference on Computer Vision (ICCV)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks for semantic segmentation always require a large number\nof samples with pixel-level labels, which becomes the major difficulty in their\nreal-world applications. To reduce the labeling cost, unsupervised domain\nadaptation (UDA) approaches are proposed to transfer knowledge from labeled\nsynthesized datasets to unlabeled real-world datasets. Recently, some\nsemi-supervised learning methods have been applied to UDA and achieved\nstate-of-the-art performance. One of the most popular approaches in\nsemi-supervised learning is the entropy minimization method. However, when\napplying the entropy minimization to UDA for semantic segmentation, the\ngradient of the entropy is biased towards samples that are easy to transfer. To\nbalance the gradient of well-classified target samples, we propose the maximum\nsquares loss. Our maximum squares loss prevents the training process being\ndominated by easy-to-transfer samples in the target domain. Besides, we\nintroduce the image-wise weighting ratio to alleviate the class imbalance in\nthe unlabeled target domain. Both synthetic-to-real and cross-city adaptation\nexperiments demonstrate the effectiveness of our proposed approach. The code is\nreleased at https://github. com/ZJULearning/MaxSquareLoss.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 11:10:09 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Chen", "Minghao", ""], ["Xue", "Hongyang", ""], ["Cai", "Deng", ""]]}, {"id": "1909.13603", "submitter": "Maximilian Jaritz", "authors": "Maximilian Jaritz, Jiayuan Gu, Hao Su", "title": "Multi-view PointNet for 3D Scene Understanding", "comments": "Geometry Meets Deep Learning Workshop, ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fusion of 2D images and 3D point clouds is important because information from\ndense images can enhance sparse point clouds. However, fusion is challenging\nbecause 2D and 3D data live in different spaces. In this work, we propose\nMVPNet (Multi-View PointNet), where we aggregate 2D multi-view image features\ninto 3D point clouds, and then use a point based network to fuse the features\nin 3D canonical space to predict 3D semantic labels. To this end, we introduce\nview selection along with a 2D-3D feature aggregation module. Extensive\nexperiments show the benefit of leveraging features from dense images and\nreveal superior robustness to varying point cloud density compared to 3D-only\nmethods. On the ScanNetV2 benchmark, our MVPNet significantly outperforms prior\npoint cloud based approaches on the task of 3D Semantic Segmentation. It is\nmuch faster to train than the large networks of the sparse voxel approach. We\nprovide solid ablation studies to ease the future design of 2D-3D fusion\nmethods and their extension to other tasks, as we showcase for 3D instance\nsegmentation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 11:45:37 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Jaritz", "Maximilian", ""], ["Gu", "Jiayuan", ""], ["Su", "Hao", ""]]}, {"id": "1909.13669", "submitter": "Shivam Barwey", "authors": "Shivam Barwey, Malik Hassanaly, Venkat Raman, Adam Steinberg", "title": "Using machine learning to construct velocity fields from OH-PLIF images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work utilizes data-driven methods to morph a series of time-resolved\nexperimental OH-PLIF images into corresponding three-component planar PIV\nfields in the closed domain of a premixed swirl combustor. The task is carried\nout with a fully convolutional network, which is a type of convolutional neural\nnetwork (CNN) used in many applications in machine learning, alongside an\nexisting experimental dataset which consists of simultaneous OH-PLIF and PIV\nmeasurements in both attached and detached flame regimes. Two types of models\nare compared: 1) a global CNN which is trained using images from the entire\ndomain, and 2) a set of local CNNs, which are trained only on individual\nsections of the domain. The locally trained models show improvement in creating\nmappings in the detached regime over the global models. A comparison between\nmodel performance in attached and detached regimes shows that the CNNs are much\nmore accurate across the board in creating velocity fields for attached flames.\nInclusion of time history in the PLIF input resulted in small noticeable\nimprovement on average, which could imply a greater physical role of\ninstantaneous spatial correlations in the decoding process over temporal\ndependencies from the perspective of the CNN. Additionally, the performance of\nlocal models trained to produce mappings in one section of the domain is tested\non other, unexplored sections of the domain. Interestingly, local CNN\nperformance on unseen domain regions revealed the models' ability to utilize\nsymmetry and antisymmetry in the velocity field. Ultimately, this work shows\nthe powerful ability of the CNN to decode the three-dimensional PIV fields from\ninput OH-PLIF images, providing a potential groundwork for a very useful tool\nfor experimental configurations in which accessibility of forms of simultaneous\nmeasurements are limited.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 23:26:34 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Barwey", "Shivam", ""], ["Hassanaly", "Malik", ""], ["Raman", "Venkat", ""], ["Steinberg", "Adam", ""]]}, {"id": "1909.13690", "submitter": "Suryabhan Singh Hada", "authors": "Suryabhan Singh Hada and Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "Style Transfer by Rigid Alignment in Neural Net Feature Space", "comments": "Accepted to WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arbitrary style transfer is an important problem in computer vision that aims\nto transfer style patterns from an arbitrary style image to a given content\nimage. However, current methods either rely on slow iterative optimization or\nfast pre-determined feature transformation, but at the cost of compromised\nvisual quality of the styled image; especially, distorted content structure. In\nthis work, we present an effective and efficient approach for arbitrary style\ntransfer that seamlessly transfers style patterns as well as keep content\nstructure intact in the styled image. We achieve this by aligning style\nfeatures to content features using rigid alignment; thus modifying style\nfeatures, unlike the existing methods that do the opposite. We demonstrate the\neffectiveness of the proposed approach by generating high-quality stylized\nimages and compare the results with the current state-of-the-art techniques for\narbitrary style transfer.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 02:54:00 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 08:03:17 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Hada", "Suryabhan Singh", ""], ["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1909.13701", "submitter": "Aashish Sharma Mr", "authors": "Aashish Sharma, Lionel Heng, Loong-Fah Cheong and Robby T. Tan", "title": "Nighttime Stereo Depth Estimation using Joint Translation-Stereo\n  Learning: Light Effects and Uninformative Regions", "comments": "Accepted to 3DV 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nighttime stereo depth estimation is still challenging, as assumptions\nassociated with daytime lighting conditions do not hold any longer. Nighttime\nis not only about low-light and dense noise, but also about glow/glare, flares,\nnon-uniform distribution of light, etc. One of the possible solutions is to\ntrain a network on night stereo images in a fully supervised manner. However,\nto obtain proper disparity ground-truths that are dense, independent from\nglare/glow, and have sufficiently far depth ranges is extremely intractable. To\naddress the problem, we introduce a network joining day/night translation and\nstereo. In training the network, our method does not require ground-truth\ndisparities of the night images, or paired day/night images. We utilize a\ntranslation network that can render realistic night stereo images from day\nstereo images. We then train a stereo network on the rendered night stereo\nimages using the available disparity supervision from the corresponding day\nstereo images, and simultaneously also train the day/night translation network.\nWe handle the fake depth problem, which occurs due to the unsupervised/unpaired\ntranslation, for light effects (e.g., glow/glare) and uninformative regions\n(e.g., low-light and saturated regions), by adding structure-preservation and\nweighted-smoothness constraints. Our experiments show that our method\noutperforms the baseline methods on night images.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 13:52:36 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 01:26:10 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Sharma", "Aashish", ""], ["Heng", "Lionel", ""], ["Cheong", "Loong-Fah", ""], ["Tan", "Robby T.", ""]]}, {"id": "1909.13714", "submitter": "Eda Okur", "authors": "Eda Okur, Shachi H Kumar, Saurav Sahay, Lama Nachman", "title": "Towards Multimodal Understanding of Passenger-Vehicle Interactions in\n  Autonomous Vehicles: Intent/Slot Recognition Utilizing Audio-Visual Data", "comments": "Presented as a short-paper at the 23rd Workshop on the Semantics and\n  Pragmatics of Dialogue (SemDial 2019 - LondonLogue), Sep 4-6, 2019, London,\n  UK", "journal-ref": "Proceedings of the 23rd Workshop on the Semantics and Pragmatics\n  of Dialogue (SEMDIAL), pp. 213-215, London, United Kingdom, September 2019", "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding passenger intents from spoken interactions and car's vision\n(both inside and outside the vehicle) are important building blocks towards\ndeveloping contextual dialog systems for natural interactions in autonomous\nvehicles (AV). In this study, we continued exploring AMIE (Automated-vehicle\nMultimodal In-cabin Experience), the in-cabin agent responsible for handling\ncertain multimodal passenger-vehicle interactions. When the passengers give\ninstructions to AMIE, the agent should parse such commands properly considering\navailable three modalities (language/text, audio, video) and trigger the\nappropriate functionality of the AV system. We had collected a multimodal\nin-cabin dataset with multi-turn dialogues between the passengers and AMIE\nusing a Wizard-of-Oz scheme via realistic scavenger hunt game. In our previous\nexplorations, we experimented with various RNN-based models to detect\nutterance-level intents (set destination, change route, go faster, go slower,\nstop, park, pull over, drop off, open door, and others) along with intent\nkeywords and relevant slots (location, position/direction, object,\ngesture/gaze, time-guidance, person) associated with the action to be performed\nin our AV scenarios. In this recent work, we propose to discuss the benefits of\nmultimodal understanding of in-cabin utterances by incorporating\nverbal/language input (text and speech embeddings) together with the\nnon-verbal/acoustic and visual input from inside and outside the vehicle (i.e.,\npassenger gestures and gaze from in-cabin video stream, referred objects\noutside of the vehicle from the road view camera stream). Our experimental\nresults outperformed text-only baselines and with multimodality, we achieved\nimproved performances for utterance-level intent detection and slot filling.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 00:00:41 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Okur", "Eda", ""], ["Kumar", "Shachi H", ""], ["Sahay", "Saurav", ""], ["Nachman", "Lama", ""]]}, {"id": "1909.13719", "submitter": "Ekin Dogus Cubuk", "authors": "Ekin D. Cubuk and Barret Zoph and Jonathon Shlens and Quoc V. Le", "title": "RandAugment: Practical automated data augmentation with a reduced search\n  space", "comments": "Added ablation experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that data augmentation has the potential to\nsignificantly improve the generalization of deep learning models. Recently,\nautomated augmentation strategies have led to state-of-the-art results in image\nclassification and object detection. While these strategies were optimized for\nimproving validation accuracy, they also led to state-of-the-art results in\nsemi-supervised learning and improved robustness to common corruptions of\nimages. An obstacle to a large-scale adoption of these methods is a separate\nsearch phase which increases the training complexity and may substantially\nincrease the computational cost. Additionally, due to the separate search\nphase, these approaches are unable to adjust the regularization strength based\non model or dataset size. Automated augmentation policies are often found by\ntraining small models on small datasets and subsequently applied to train\nlarger models. In this work, we remove both of these obstacles. RandAugment has\na significantly reduced search space which allows it to be trained on the\ntarget task with no need for a separate proxy task. Furthermore, due to the\nparameterization, the regularization strength may be tailored to different\nmodel and dataset sizes. RandAugment can be used uniformly across different\ntasks and datasets and works out of the box, matching or surpassing all\nprevious automated augmentation approaches on CIFAR-10/100, SVHN, and ImageNet.\nOn the ImageNet dataset we achieve 85.0% accuracy, a 0.6% increase over the\nprevious state-of-the-art and 1.0% increase over baseline augmentation. On\nobject detection, RandAugment leads to 1.0-1.3% improvement over baseline\naugmentation, and is within 0.3% mAP of AutoAugment on COCO. Finally, due to\nits interpretable hyperparameter, RandAugment may be used to investigate the\nrole of data augmentation with varying model and dataset size. Code is\navailable online.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 14:05:14 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 04:51:03 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Cubuk", "Ekin D.", ""], ["Zoph", "Barret", ""], ["Shlens", "Jonathon", ""], ["Le", "Quoc V.", ""]]}, {"id": "1909.13726", "submitter": "Timothy Verstraeten", "authors": "Felipe Gomez Marulanda, Pieter Libin, Timothy Verstraeten, Ann Now\\'e", "title": "IPC-Net: 3D point-cloud segmentation using deep inter-point\n  convolutional layers", "comments": null, "journal-ref": "2018 IEEE 30th International Conference on Tools with Artificial\n  Intelligence (ICTAI),", "doi": "10.1109/ICTAI.2018.00054", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, the demand for better segmentation and classification\nalgorithms in 3D spaces has significantly grown due to the popularity of new 3D\nsensor technologies and advancements in the field of robotics. Point-clouds are\none of the most popular representations to store a digital description of 3D\nshapes. However, point-clouds are stored in irregular and unordered structures,\nwhich limits the direct use of segmentation algorithms such as Convolutional\nNeural Networks. The objective of our work is twofold: First, we aim to provide\na full analysis of the PointNet architecture to illustrate which features are\nbeing extracted from the point-clouds. Second, to propose a new network\narchitecture called IPC-Net to improve the state-of-the-art point cloud\narchitectures. We show that IPC-Net extracts a larger set of unique features\nallowing the model to produce more accurate segmentations compared to the\nPointNet architecture. In general, our approach outperforms PointNet on every\nfamily of 3D geometries on which the models were tested. A high generalisation\nimprovement was observed on every 3D shape, especially on the rockets dataset.\nOur experiments demonstrate that our main contribution, inter-point activation\non the network's layers, is essential to accurately segment 3D point-clouds.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 14:14:30 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Marulanda", "Felipe Gomez", ""], ["Libin", "Pieter", ""], ["Verstraeten", "Timothy", ""], ["Now\u00e9", "Ann", ""]]}, {"id": "1909.13730", "submitter": "Oleg Shipitko", "authors": "Mikhail Chekanov and Oleg Shipitko", "title": "X-ray and Visible Spectra Circular Motion Images Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the collections of images of the same rotating plastic object made\nin X-ray and visible spectra. Both parts of the dataset contain 400 images. The\nimages are maid every 0.5 degrees of the object axial rotation. The collection\nof images is designed for evaluation of the performance of circular motion\nestimation algorithms as well as for the study of X-ray nature influence on the\nimage analysis algorithms such as keypoints detection and description. The\ndataset is available at https://github.com/Visillect/xvcm-dataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 14:17:12 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 12:31:29 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Chekanov", "Mikhail", ""], ["Shipitko", "Oleg", ""]]}, {"id": "1909.13776", "submitter": "Javed Iqbal", "authors": "Javed Iqbal and Mohsen Ali", "title": "MLSL: Multi-Level Self-Supervised Learning for Domain Adaptation with\n  Spatially Independent and Semantically Consistent Labeling", "comments": "Accepted by WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most of the recent Deep Semantic Segmentation algorithms suffer from large\ngeneralization errors, even when powerful hierarchical representation models\nbased on convolutional neural networks have been employed. This could be\nattributed to limited training data and large distribution gap in train and\ntest domain datasets. In this paper, we propose a multi-level self-supervised\nlearning model for domain adaptation of semantic segmentation. Exploiting the\nidea that an object (and most of the stuff given context) should be labeled\nconsistently regardless of its location, we generate spatially independent and\nsemantically consistent (SISC) pseudo-labels by segmenting multiple sub-images\nusing base model and designing an aggregation strategy. Image level pseudo\nweak-labels, PWL, are computed to guide domain adaptation by capturing global\ncontext similarity in source and domain at latent space level. Thus helping\nlatent space learn the representation even when there are very few pixels\nbelonging to the domain category (small object for example) compared to rest of\nthe image. Our multi-level Self-supervised learning (MLSL) outperforms existing\nstate-of art (self or adversarial learning) algorithms. Specifically, keeping\nall setting similar and employing MLSL we obtain an mIoU gain of 5:1% on GTA-V\nto Cityscapes adaptation and 4:3% on SYNTHIA to Cityscapes adaptation compared\nto existing state-of-art method.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 15:16:03 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Iqbal", "Javed", ""], ["Ali", "Mohsen", ""]]}, {"id": "1909.13784", "submitter": "Reuben Tan", "authors": "Reuben Tan, Huijuan Xu, Kate Saenko, Bryan A. Plummer", "title": "LoGAN: Latent Graph Co-Attention Network for Weakly-Supervised Video\n  Moment Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of weakly-supervised video moment retrieval is to localize the video\nsegment most relevant to the given natural language query without access to\ntemporal annotations during training. Prior strongly- and weakly-supervised\napproaches often leverage co-attention mechanisms to learn visual-semantic\nrepresentations for localization. However, while such approaches tend to focus\non identifying relationships between elements of the video and language\nmodalities, there is less emphasis on modeling relational context between video\nframes given the semantic context of the query. Consequently, the\nabove-mentioned visual-semantic representations, built upon local frame\nfeatures, do not contain much contextual information. To address this\nlimitation, we propose a Latent Graph Co-Attention Network (LoGAN) that\nexploits fine-grained frame-by-word interactions to reason about\ncorrespondences between all possible pairs of frames, given the semantic\ncontext of the query. Comprehensive experiments across two datasets, DiDeMo and\nCharades-Sta, demonstrate the effectiveness of our proposed latent co-attention\nmodel where it outperforms current state-of-the-art (SOTA) weakly-supervised\napproaches by a significant margin. Notably, it even achieves a 11% improvement\nto Recall@1 accuracy over strongly-supervised SOTA methods on DiDeMo.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 16:26:30 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 18:11:37 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Tan", "Reuben", ""], ["Xu", "Huijuan", ""], ["Saenko", "Kate", ""], ["Plummer", "Bryan A.", ""]]}, {"id": "1909.13817", "submitter": "Thanh Huy Nguyen", "authors": "Thanh Huy Nguyen, Sylvie Daniel, Didier Gueriot, Christophe Sintes,\n  Jean-Marc Le Caillec", "title": "Coarse-to-Fine Registration of Airborne LiDAR Data and Optical Imagery\n  on Urban Scenes", "comments": "20 pages, 15 figures. Accepted to be published in IEEE JSTARS", "journal-ref": "IEEE Journal of Selected Topics in Applied Earth Observations and\n  Remote Sensing 2020 (Early Access)", "doi": "10.1109/JSTARS.2020.2987305", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications based on synergistic integration of optical imagery and LiDAR\ndata are receiving a growing interest from the remote sensing community.\nHowever, a misaligned integration between these datasets may fail to fully\nprofit the potential of both sensors. In this regard, an optimum fusion of\noptical imagery and LiDAR data requires an accurate registration. This is a\ncomplex problem since a versatile solution is still missing, especially when\nconsidering the context where data are collected at different times, from\ndifferent platforms, under different acquisition configurations. This paper\npresents a coarse-to-fine registration method of aerial/satellite optical\nimagery with airborne LiDAR data acquired in such context. Firstly, a coarse\nregistration involves extracting and matching of buildings from LiDAR data and\noptical imagery. Then, a Mutual Information-based fine registration is carried\nout. It involves a super-resolution approach applied to LiDAR data, and a local\napproach of transformation model estimation. The proposed method succeeds at\novercoming the challenges associated with the aforementioned difficult context.\nConsidering the experimented airborne LiDAR (2011) and orthorectified aerial\nimagery (2016) datasets, their spatial shift is reduced by 48.15% after the\nproposed coarse registration. Moreover, the incompatibility of size and spatial\nresolution is addressed by the mentioned super-resolution. Finally, a high\naccuracy of dataset alignment is also achieved, highlighted by a 40-cm error\nbased on a check-point assessment and a 64-cm error based on a check-pair-line\nassessment. These promising results enable further research for a complete\nversatile fusion methodology between airborne LiDAR and optical imagery data in\nthis challenging context.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 16:10:12 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 02:57:11 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Nguyen", "Thanh Huy", ""], ["Daniel", "Sylvie", ""], ["Gueriot", "Didier", ""], ["Sintes", "Christophe", ""], ["Caillec", "Jean-Marc Le", ""]]}, {"id": "1909.13819", "submitter": "Haitian Zheng", "authors": "Haitian Zheng, Lele Chen, Chenliang Xu, Jiebo Luo", "title": "Unsupervised Pose Flow Learning for Pose Guided Synthesis", "comments": "12 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose guided synthesis aims to generate a new image in an arbitrary target\npose while preserving the appearance details from the source image. Existing\napproaches rely on either hard-coded spatial transformations or 3D body\nmodeling. They often overlook complex non-rigid pose deformation or unmatched\noccluded regions, thus fail to effectively preserve appearance information. In\nthis paper, we propose an unsupervised pose flow learning scheme that learns to\ntransfer the appearance details from the source image. Based on such learned\npose flow, we proposed GarmentNet and SynthesisNet, both of which use\nmulti-scale feature-domain alignment for coarse-to-fine synthesis. Experiments\non the DeepFashion, MVC dataset and additional real-world datasets demonstrate\nthat our approach compares favorably with the state-of-the-art methods and\ngeneralizes to unseen poses and clothing styles.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 16:17:25 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Zheng", "Haitian", ""], ["Chen", "Lele", ""], ["Xu", "Chenliang", ""], ["Luo", "Jiebo", ""]]}, {"id": "1909.13834", "submitter": "Wen Zhang", "authors": "Wen Zhang and Yalin Wang", "title": "Geometric Brain Surface Network For Brain Cortical Parcellation", "comments": "8 pages", "journal-ref": "GLMI in Conjunction with MICCAI 2019", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A large number of surface-based analyses on brain imaging data adopt some\nspecific brain atlases to better assess structural and functional changes in\none or more brain regions. In these analyses, it is necessary to obtain an\nanatomically correct surface parcellation scheme in an individual brain by\nreferring to the given atlas. Traditional ways to accomplish this goal are\nthrough a designed surface-based registration or hand-crafted surface features,\nalthough both of them are time-consuming. A recent deep learning approach\ndepends on a regular spherical parameterization of the mesh, which is\ncomputationally prohibitive in some cases and may also demand further\npost-processing to refine the network output. Therefore, an accurate and\nfully-automatic cortical surface parcellation scheme directly working on the\noriginal brain surfaces would be highly advantageous. In this study, we propose\nan end-to-end deep brain cortical parcellation network, called \\textbf{DBPN}.\nThrough intrinsic and extrinsic graph convolution kernels, DBPN dynamically\ndeciphers neighborhood graph topology around each vertex and encodes the\ndeciphered knowledge into node features. Eventually, a non-linear mapping\nbetween the node features and parcellation labels is constructed. Our model is\na two-stage deep network which contains a coarse parcellation network with a\nU-shape structure and a refinement network to fine-tune the coarse results. We\nevaluate our model in a large public dataset and our work achieves superior\nperformance than state-of-the-art baseline methods in both accuracy and\nefficiency\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 03:36:05 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Zhang", "Wen", ""], ["Wang", "Yalin", ""]]}, {"id": "1909.13863", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Georgios Tzimiropoulos", "title": "XNOR-Net++: Improved Binary Neural Networks", "comments": "Accepted to BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an improved training algorithm for binary neural networks\nin which both weights and activations are binary numbers. A key but fairly\noverlooked feature of the current state-of-the-art method of XNOR-Net is the\nuse of analytically calculated real-valued scaling factors for re-weighting the\noutput of binary convolutions. We argue that analytic calculation of these\nfactors is sub-optimal. Instead, in this work, we make the following\ncontributions: (a) we propose to fuse the activation and weight scaling factors\ninto a single one that is learned discriminatively via backpropagation. (b)\nMore importantly, we explore several ways of constructing the shape of the\nscale factors while keeping the computational budget fixed. (c) We empirically\nmeasure the accuracy of our approximations and show that they are significantly\nmore accurate than the analytically calculated one. (d) We show that our\napproach significantly outperforms XNOR-Net within the same computational\nbudget when tested on the challenging task of ImageNet classification, offering\nup to 6\\% accuracy gain.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 17:42:09 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Bulat", "Adrian", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "1909.13868", "submitter": "Alexander  Mathis", "authors": "Mackenzie W. Mathis and Alexander Mathis", "title": "Deep learning tools for the measurement of animal behavior in\n  neuroscience", "comments": "11 pages, 3 figures, review", "journal-ref": "Current Opinion in Neurobiology Volume 60, February 2020, Pages\n  1-11", "doi": "10.1016/j.conb.2019.10.008", "report-no": null, "categories": "cs.CV q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in computer vision have made accurate, fast and robust\nmeasurement of animal behavior a reality. In the past years powerful tools\nspecifically designed to aid the measurement of behavior have come to fruition.\nHere we discuss how capturing the postures of animals - pose estimation - has\nbeen rapidly advancing with new deep learning methods. While challenges still\nremain, we envision that the fast-paced development of new deep learning tools\nwill rapidly change the landscape of realizable real-world neuroscience.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 17:50:48 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 17:40:30 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Mathis", "Mackenzie W.", ""], ["Mathis", "Alexander", ""]]}, {"id": "1909.13874", "submitter": "Rohan Chitnis", "authors": "Rohan Chitnis, Shubham Tulsiani, Saurabh Gupta, Abhinav Gupta", "title": "Efficient Bimanual Manipulation Using Learned Task Schemas", "comments": "ICRA 2020 final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of effectively composing skills to solve sparse-reward\ntasks in the real world. Given a set of parameterized skills (such as exerting\na force or doing a top grasp at a location), our goal is to learn policies that\ninvoke these skills to efficiently solve such tasks. Our insight is that for\nmany tasks, the learning process can be decomposed into learning a\nstate-independent task schema (a sequence of skills to execute) and a policy to\nchoose the parameterizations of the skills in a state-dependent manner. For\nsuch tasks, we show that explicitly modeling the schema's state-independence\ncan yield significant improvements in sample efficiency for model-free\nreinforcement learning algorithms. Furthermore, these schemas can be\ntransferred to solve related tasks, by simply re-learning the parameterizations\nwith which the skills are invoked. We find that doing so enables learning to\nsolve sparse-reward tasks on real-world robotic systems very efficiently. We\nvalidate our approach experimentally over a suite of robotic bimanual\nmanipulation tasks, both in simulation and on real hardware. See videos at\nhttp://tinyurl.com/chitnis-schema.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 17:55:09 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 16:58:56 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Chitnis", "Rohan", ""], ["Tulsiani", "Shubham", ""], ["Gupta", "Saurabh", ""], ["Gupta", "Abhinav", ""]]}]