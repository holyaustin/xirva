[{"id": "1505.00040", "submitter": "Mohammad Ehab Ragab", "authors": "Mohammad Ehab Ragab", "title": "Overlapping and Non-overlapping Camera Layouts for Robot Pose Estimation", "comments": "7 pages, 5 figures", "journal-ref": "IJCSI - March 2015 Issue (Volume 12, Issue 2)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the use of overlapping and non-overlapping camera layouts in\nestimating the ego-motion of a moving robot. To estimate the location and\norientation of the robot, we investigate using four cameras as non-overlapping\nindividuals, and as two stereo pairs. The pros and cons of the two approaches\nare elucidated. The cameras work independently and can have larger field of\nview in the non-overlapping layout. However, a scale factor ambiguity should be\ndealt with. On the other hand, stereo systems provide more accuracy but require\nestablishing feature correspondence with more computational demand. For both\napproaches, the extended Kalman filter is used as a real-time recursive\nestimator. The approaches studied are verified with synthetic and real\nexperiments alike.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 21:42:23 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Ragab", "Mohammad Ehab", ""]]}, {"id": "1505.00066", "submitter": "Shubham Tulsiani", "authors": "Shubham Tulsiani, Jo\\~ao Carreira and Jitendra Malik", "title": "Pose Induction for Novel Object Categories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of predicting pose for objects of unannotated object\ncategories from a small seed set of annotated object classes. We present a\ngeneralized classifier that can reliably induce pose given a single instance of\na novel category. In case of availability of a large collection of novel\ninstances, our approach then jointly reasons over all instances to improve the\ninitial estimates. We empirically validate the various components of our\nalgorithm and quantitatively show that our method produces reliable pose\nestimates. We also show qualitative results on a diverse set of classes and\nfurther demonstrate the applicability of our system for learning shape models\nof novel object classes.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 01:45:52 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2015 18:17:20 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Tulsiani", "Shubham", ""], ["Carreira", "Jo\u00e3o", ""], ["Malik", "Jitendra", ""]]}, {"id": "1505.00074", "submitter": "Kunal Narayan Chaudhury", "authors": "Kunal N. Chaudhury and Kollipara Rithwik", "title": "Image Denoising using Optimally Weighted Bilateral Filters: A Sure and\n  Fast Approach", "comments": "To appear in the IEEE International Conference on Image Processing\n  (ICIP 2015). Link to the Matlab code added in the revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bilateral filter is known to be quite effective in denoising images\ncorrupted with small dosages of additive Gaussian noise. The denoising\nperformance of the filter, however, is known to degrade quickly with the\nincrease in noise level. Several adaptations of the filter have been proposed\nin the literature to address this shortcoming, but often at a substantial\ncomputational overhead. In this paper, we report a simple pre-processing step\nthat can substantially improve the denoising performance of the bilateral\nfilter, at almost no additional cost. The modified filter is designed to be\nrobust at large noise levels, and often tends to perform poorly below a certain\nnoise threshold. To get the best of the original and the modified filter, we\npropose to combine them in a weighted fashion, where the weights are chosen to\nminimize (a surrogate of) the oracle mean-squared-error (MSE). The\noptimally-weighted filter is thus guaranteed to perform better than either of\nthe component filters in terms of the MSE, at all noise levels. We also provide\na fast algorithm for the weighted filtering. Visual and quantitative denoising\nresults on standard test images are reported which demonstrate that the\nimprovement over the original filter is significant both visually and in terms\nof PSNR. Moreover, the denoising performance of the optimally-weighted\nbilateral filter is competitive with the computation-intensive non-local means\nfilter.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 02:49:55 GMT"}, {"version": "v2", "created": "Mon, 25 May 2015 09:42:04 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Chaudhury", "Kunal N.", ""], ["Rithwik", "Kollipara", ""]]}, {"id": "1505.00077", "submitter": "Kunal Narayan Chaudhury", "authors": "Kunal N. Chaudhury", "title": "Fast and Accurate Bilateral Filtering using Gauss-Polynomial\n  Decomposition", "comments": "To appear in the IEEE International Conference on Image Processing\n  (ICIP 2015)", "journal-ref": null, "doi": "10.1109/ICIP.2015.7351152", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bilateral filter is a versatile non-linear filter that has found diverse\napplications in image processing, computer vision, computer graphics, and\ncomputational photography. A widely-used form of the filter is the Gaussian\nbilateral filter in which both the spatial and range kernels are Gaussian. A\ndirect implementation of this filter requires $O(\\sigma^2)$ operations per\npixel, where $\\sigma$ is the standard deviation of the spatial Gaussian. In\nthis paper, we propose an accurate approximation algorithm that can cut down\nthe computational complexity to $O(1)$ per pixel for any arbitrary $\\sigma$\n(constant-time implementation). This is based on the observation that the range\nkernel operates via the translations of a fixed Gaussian over the range space,\nand that these translated Gaussians can be accurately approximated using the\nso-called Gauss-polynomials. The overall algorithm emerging from this\napproximation involves a series of spatial Gaussian filtering, which can be\nimplemented in constant-time using separability and recursion. We present some\npreliminary results to demonstrate that the proposed algorithm compares\nfavorably with some of the existing fast algorithms in terms of speed and\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 03:01:09 GMT"}, {"version": "v2", "created": "Mon, 25 May 2015 09:50:02 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Chaudhury", "Kunal N.", ""]]}, {"id": "1505.00110", "submitter": "Peter Hall", "authors": "Hongping Cai and Qi Wu and Tadeo Corradi and Peter Hall", "title": "The Cross-Depiction Problem: Computer Vision Algorithms for Recognising\n  Objects in Artwork and in Photographs", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cross-depiction problem is that of recognising visual objects regardless\nof whether they are photographed, painted, drawn, etc. It is a potentially\nsignificant yet under-researched problem. Emulating the remarkable human\nability to recognise objects in an astonishingly wide variety of depictive\nforms is likely to advance both the foundations and the applications of\nComputer Vision.\n  In this paper we benchmark classification, domain adaptation, and deep\nlearning methods; demonstrating that none perform consistently well in the\ncross-depiction problem. Given the current interest in deep learning, the fact\nsuch methods exhibit the same behaviour as all but one other method: they show\na significant fall in performance over inhomogeneous databases compared to\ntheir peak performance, which is always over data comprising photographs only.\nRather, we find the methods that have strong models of spatial relations\nbetween parts tend to be more robust and therefore conclude that such\ninformation is important in modelling object classes regardless of appearance\ndetails.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 07:38:52 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Cai", "Hongping", ""], ["Wu", "Qi", ""], ["Corradi", "Tadeo", ""], ["Hall", "Peter", ""]]}, {"id": "1505.00145", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Ferran Cabezas, Axel Carlier, Amaia Salvador, Xavier Gir\\'o-i-Nieto\n  and Vincent Charvillat", "title": "Quality Control in Crowdsourced Object Segmentation", "comments": "Paper accepted at the IEEE International Conference on Image\n  Processing (ICIP) 2015. Quebec City, 27-30 September 2015", "journal-ref": null, "doi": "10.1109/ICIP.2015.7351606", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores processing techniques to deal with noisy data in\ncrowdsourced object segmentation tasks. We use the data collected with\n\"Click'n'Cut\", an online interactive segmentation tool, and we perform several\nexperiments towards improving the segmentation results. First, we introduce\ndifferent superpixel-based techniques to filter users' traces, and assess their\nimpact on the segmentation result. Second, we present different criteria to\ndetect and discard the traces from potential bad users, resulting in a\nremarkable increase in performance. Finally, we show a novel superpixel-based\nsegmentation algorithm which does not require any prior filtering and is based\non weighting each user's contribution according to his/her level of expertise.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 10:33:49 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Cabezas", "Ferran", ""], ["Carlier", "Axel", ""], ["Salvador", "Amaia", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""], ["Charvillat", "Vincent", ""]]}, {"id": "1505.00171", "submitter": "Ankur Handa", "authors": "Ankur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent and\n  Roberto Cipolla", "title": "SynthCam3D: Semantic Understanding With Synthetic Indoor Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We are interested in automatic scene understanding from geometric cues. To\nthis end, we aim to bring semantic segmentation in the loop of real-time\nreconstruction. Our semantic segmentation is built on a deep autoencoder stack\ntrained exclusively on synthetic depth data generated from our novel 3D scene\nlibrary, SynthCam3D. Importantly, our network is able to segment real world\nscenes without any noise modelling. We present encouraging preliminary results.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 12:55:32 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Handa", "Ankur", ""], ["Patraucean", "Viorica", ""], ["Badrinarayanan", "Vijay", ""], ["Stent", "Simon", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1505.00192", "submitter": "Sabyasachi  Mukhopadhyay", "authors": "Sabyasachi Mukhopadhyay, Soham Mandal, Sawon Pratiher, Ritwik Barman,\n  M.Venkatesh, Nirmalya Ghosh, Prasanta K. Panigrahi", "title": "Application of S-Transform on Hyper kurtosis based Modified Duo\n  Histogram Equalized DIC images for Pre-cancer Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our proposed hyper kurtosis based histogram equalized DIC images enhances the\ncontrast by preserving the brightness. The evolution and development of\nprecancerous activity among tissues are studied through S-transform (ST). The\nsignificant variations of amplitude spectra can be observed due to increased\nmedium roughness from normal tissue were observed in time-frequency domain. The\nrandomness and inhomogeneity of the tissue structures among human normal and\ndifferent grades of DIC tissues is recognized by ST based timefrequency\nanalysis. This study offers a simpler and better way to recognize the\nsubstantial changes among different stages of DIC tissues, which are reflected\nby spatial information containing within the inhomogeneity structures of\ndifferent types of tissue.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 16:32:26 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Mukhopadhyay", "Sabyasachi", ""], ["Mandal", "Soham", ""], ["Pratiher", "Sawon", ""], ["Barman", "Ritwik", ""], ["Venkatesh", "M.", ""], ["Ghosh", "Nirmalya", ""], ["Panigrahi", "Prasanta K.", ""]]}, {"id": "1505.00193", "submitter": "Heike Benninghoff", "authors": "Heike Benninghoff and Harald Garcke", "title": "Segmentation and Restoration of Images on Surfaces by Parametric Active\n  Contours with Topology Changes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.AP math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a new method for segmentation and restoration of images on\ntwo-dimensional surfaces is given. Active contour models for image segmentation\nare extended to images on surfaces. The evolving curves on the surfaces are\nmathematically described using a parametric approach. For image restoration, a\ndiffusion equation with Neumann boundary conditions is solved in a\npostprocessing step in the individual regions. Numerical schemes are presented\nwhich allow to efficiently compute segmentations and denoised versions of\nimages on surfaces. Also topology changes of the evolving curves are detected\nand performed using a fast sub-routine. Finally, several experiments are\npresented where the developed methods are applied on different artificial and\nreal images defined on different surfaces.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 14:41:23 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Benninghoff", "Heike", ""], ["Garcke", "Harald", ""]]}, {"id": "1505.00218", "submitter": "Yuri Boykov", "authors": "Yuri Boykov, Hossam Isack, Carl Olsson, Ismail Ben Ayed", "title": "Volumetric Bias in Segmentation and Reconstruction: Secrets and\n  Solutions", "comments": "9 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many standard optimization methods for segmentation and reconstruction\ncompute ML model estimates for appearance or geometry of segments, e.g.\nZhu-Yuille 1996, Torr 1998, Chan-Vese 2001, GrabCut 2004, Delong et al. 2012.\nWe observe that the standard likelihood term in these formulations corresponds\nto a generalized probabilistic K-means energy. In learning it is well known\nthat this energy has a strong bias to clusters of equal size, which can be\nexpressed as a penalty for KL divergence from a uniform distribution of\ncardinalities. However, this volumetric bias has been mostly ignored in\ncomputer vision. We demonstrate significant artifacts in standard segmentation\nand reconstruction methods due to this bias. Moreover, we propose binary and\nmulti-label optimization techniques that either (a) remove this bias or (b)\nreplace it by a KL divergence term for any given target volume distribution.\nOur general ideas apply to many continuous or discrete energy formulations in\nsegmentation, stereo, and other reconstruction problems.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 17:01:37 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Boykov", "Yuri", ""], ["Isack", "Hossam", ""], ["Olsson", "Carl", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "1505.00249", "submitter": "Aleksandar Zlateski", "authors": "Aleksandar Zlateski and H.Sebastian Seung", "title": "Image Segmentation by Size-Dependent Single Linkage Clustering of a\n  Watershed Basin Graph", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for hierarchical image segmentation that defines a\ndisaffinity graph on the image, over-segments it into watershed basins, defines\na new graph on the basins, and then merges basins with a modified,\nsize-dependent version of single linkage clustering. The quasilinear runtime of\nthe method makes it suitable for segmenting large images. We illustrate the\nmethod on the challenging problem of segmenting 3D electron microscopic brain\nimages.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 19:14:39 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Zlateski", "Aleksandar", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1505.00256", "submitter": "Chenyi Chen", "authors": "Chenyi Chen, Ari Seff, Alain Kornhauser, Jianxiong Xiao", "title": "DeepDriving: Learning Affordance for Direct Perception in Autonomous\n  Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, there are two major paradigms for vision-based autonomous driving\nsystems: mediated perception approaches that parse an entire scene to make a\ndriving decision, and behavior reflex approaches that directly map an input\nimage to a driving action by a regressor. In this paper, we propose a third\nparadigm: a direct perception approach to estimate the affordance for driving.\nWe propose to map an input image to a small number of key perception indicators\nthat directly relate to the affordance of a road/traffic state for driving. Our\nrepresentation provides a set of compact yet complete descriptions of the scene\nto enable a simple controller to drive autonomously. Falling in between the two\nextremes of mediated perception and behavior reflex, we argue that our direct\nperception representation provides the right level of abstraction. To\ndemonstrate this, we train a deep Convolutional Neural Network using recording\nfrom 12 hours of human driving in a video game and show that our model can work\nwell to drive a car in a very diverse set of virtual environments. We also\ntrain a model for car distance estimation on the KITTI dataset. Results show\nthat our direct perception approach can generalize well to real driving images.\nSource code and data are available on our project website.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 19:31:13 GMT"}, {"version": "v2", "created": "Mon, 4 May 2015 16:25:38 GMT"}, {"version": "v3", "created": "Sat, 26 Sep 2015 05:17:59 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Chen", "Chenyi", ""], ["Seff", "Ari", ""], ["Kornhauser", "Alain", ""], ["Xiao", "Jianxiong", ""]]}, {"id": "1505.00276", "submitter": "Peng Wang", "authors": "Peng Wang, Xiaohui Shen, Zhe Lin, Scott Cohen, Brian Price, Alan\n  Yuille", "title": "Joint Object and Part Segmentation using Deep Learned Potentials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting semantic objects from images and parsing them into their\nrespective semantic parts are fundamental steps towards detailed object\nunderstanding in computer vision. In this paper, we propose a joint solution\nthat tackles semantic object and part segmentation simultaneously, in which\nhigher object-level context is provided to guide part segmentation, and more\ndetailed part-level localization is utilized to refine object segmentation.\nSpecifically, we first introduce the concept of semantic compositional parts\n(SCP) in which similar semantic parts are grouped and shared among different\nobjects. A two-channel fully convolutional network (FCN) is then trained to\nprovide the SCP and object potentials at each pixel. At the same time, a\ncompact set of segments can also be obtained from the SCP predictions of the\nnetwork. Given the potentials and the generated segments, in order to explore\nlong-range context, we finally construct an efficient fully connected\nconditional random field (FCRF) to jointly predict the final object and part\nlabels. Extensive evaluation on three different datasets shows that our\napproach can mutually enhance the performance of object and part segmentation,\nand outperforms the current state-of-the-art on both tasks.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 20:35:24 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Wang", "Peng", ""], ["Shen", "Xiaohui", ""], ["Lin", "Zhe", ""], ["Cohen", "Scott", ""], ["Price", "Brian", ""], ["Yuille", "Alan", ""]]}, {"id": "1505.00295", "submitter": "Jacob Walker", "authors": "Jacob Walker, Abhinav Gupta, Martial Hebert", "title": "Dense Optical Flow Prediction from a Static Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a scene, what is going to move, and in what direction will it move?\nSuch a question could be considered a non-semantic form of action prediction.\nIn this work, we present a convolutional neural network (CNN) based approach\nfor motion prediction. Given a static image, this CNN predicts the future\nmotion of each and every pixel in the image in terms of optical flow. Our CNN\nmodel leverages the data in tens of thousands of realistic videos to train our\nmodel. Our method relies on absolutely no human labeling and is able to predict\nmotion based on the context of the scene. Because our CNN model makes no\nassumptions about the underlying scene, it can predict future optical flow on a\ndiverse set of scenarios. We outperform all previous approaches by large\nmargins.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2015 00:02:32 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2015 05:00:47 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Walker", "Jacob", ""], ["Gupta", "Abhinav", ""], ["Hebert", "Martial", ""]]}, {"id": "1505.00296", "submitter": "Limin Wang", "authors": "Limin Wang, Zhe Wang, Wenbin Du, Yu Qiao", "title": "Object-Scene Convolutional Neural Networks for Event Recognition in\n  Images", "comments": "CVPR, ChaLearn Looking at People (LAP) challenge, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event recognition from still images is of great importance for image\nunderstanding. However, compared with event recognition in videos, there are\nmuch fewer research works on event recognition in images. This paper addresses\nthe issue of event recognition from images and proposes an effective method\nwith deep neural networks. Specifically, we design a new architecture, called\nObject-Scene Convolutional Neural Network (OS-CNN). This architecture is\ndecomposed into object net and scene net, which extract useful information for\nevent understanding from the perspective of objects and scene context,\nrespectively. Meanwhile, we investigate different network architectures for\nOS-CNN design, and adapt the deep (AlexNet) and very-deep (GoogLeNet) networks\nto the task of event recognition. Furthermore, we find that the deep and\nvery-deep networks are complementary to each other. Finally, based on the\nproposed OS-CNN and comparative study of different network architectures, we\ncome up with a solution of five-stream CNN for the track of cultural event\nrecognition at the ChaLearn Looking at People (LAP) challenge 2015. Our method\nobtains the performance of 85.5% and ranks the $1^{st}$ place in this\nchallenge.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2015 01:26:42 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Wang", "Limin", ""], ["Wang", "Zhe", ""], ["Du", "Wenbin", ""], ["Qiao", "Yu", ""]]}, {"id": "1505.00308", "submitter": "Tejaswi Nimmagadda", "authors": "Tejaswi Nimmagadda and Anima Anandkumar", "title": "Multi-Object Classification and Unsupervised Scene Understanding Using\n  Deep Learning Features and Latent Tree Probabilistic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown state-of-art classification performance on datasets\nsuch as ImageNet, which contain a single object in each image. However,\nmulti-object classification is far more challenging. We present a unified\nframework which leverages the strengths of multiple machine learning methods,\nviz deep learning, probabilistic models and kernel methods to obtain\nstate-of-art performance on Microsoft COCO, consisting of non-iconic images. We\nincorporate contextual information in natural images through a conditional\nlatent tree probabilistic model (CLTM), where the object co-occurrences are\nconditioned on the extracted fc7 features from pre-trained Imagenet CNN as\ninput. We learn the CLTM tree structure using conditional pairwise\nprobabilities for object co-occurrences, estimated through kernel methods, and\nwe learn its node and edge potentials by training a new 3-layer neural network,\nwhich takes fc7 features as input. Object classification is carried out via\ninference on the learnt conditional tree model, and we obtain significant gain\nin precision-recall and F-measures on MS-COCO, especially for difficult object\ncategories. Moreover, the latent variables in the CLTM capture scene\ninformation: the images with top activations for a latent node have common\nthemes such as being a grasslands or a food scene, and on on. In addition, we\nshow that a simple k-means clustering of the inferred latent nodes alone\nsignificantly improves scene classification performance on the MIT-Indoor\ndataset, without the need for any retraining, and without using scene labels\nduring training. Thus, we present a unified framework for multi-object\nclassification and unsupervised scene understanding.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2015 03:23:46 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Nimmagadda", "Tejaswi", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1505.00315", "submitter": "Vignesh Ramanathan", "authors": "Vignesh Ramanathan, Kevin Tang, Greg Mori and Li Fei-Fei", "title": "Learning Temporal Embeddings for Complex Video Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to learn temporal embeddings of video frames for\ncomplex video analysis. Large quantities of unlabeled video data can be easily\nobtained from the Internet. These videos possess the implicit weak label that\nthey are sequences of temporally and semantically coherent images. We leverage\nthis information to learn temporal embeddings for video frames by associating\nframes with the temporal context that they appear in. To do this, we propose a\nscheme for incorporating temporal context based on past and future frames in\nvideos, and compare this to other contextual representations. In addition, we\nshow how data augmentation using multi-resolution samples and hard negatives\nhelps to significantly improve the quality of the learned embeddings. We\nevaluate various design decisions for learning temporal embeddings, and show\nthat our embeddings can improve performance for multiple video tasks such as\nretrieval, classification, and temporal order recovery in unconstrained\nInternet video.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2015 06:43:28 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Ramanathan", "Vignesh", ""], ["Tang", "Kevin", ""], ["Mori", "Greg", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1505.00353", "submitter": "Xi Yin", "authors": "Xi Yin, Xiaoming Liu, Jin Chen, David M. Kramer", "title": "Joint Multi-Leaf Segmentation, Alignment and Tracking from Fluorescence\n  Plant Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel framework for fluorescence plant video\nprocessing. The plant research community is interested in the leaf-level\nphotosynthetic analysis within a plant. A prerequisite for such analysis is to\nsegment all leaves, estimate their structures, and track them over time. We\nidentify this as a joint multi-leaf segmentation, alignment, and tracking\nproblem. First, leaf segmentation and alignment are applied on the last frame\nof a plant video to find a number of well-aligned leaf candidates. Second, leaf\ntracking is applied on the remaining frames with leaf candidate transformation\nfrom the previous frame. We form two optimization problems with shared terms in\ntheir objective functions for leaf alignment and tracking respectively. A\nquantitative evaluation framework is formulated to evaluate the performance of\nour algorithm with four metrics. Two models are learned to predict the\nalignment accuracy and detect tracking failure respectively in order to provide\nguidance for subsequent plant biology analysis. The limitation of our algorithm\nis also studied. Experimental results show the effectiveness, efficiency, and\nrobustness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2015 16:37:57 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 04:07:12 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Yin", "Xi", ""], ["Liu", "Xiaoming", ""], ["Chen", "Jin", ""], ["Kramer", "David M.", ""]]}, {"id": "1505.00389", "submitter": "Wangmeng Zuo", "authors": "Zhaoxin Li, Kuanquan Wang, Wangmeng Zuo, Deyu Meng and Lei Zhang", "title": "Detail-preserving and Content-aware Variational Multi-view Stereo\n  Reconstruction", "comments": "14 pages,16 figures. Submitted to IEEE Transaction on image\n  processing", "journal-ref": null, "doi": "10.1109/TIP.2015.2507400", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate recovery of 3D geometrical surfaces from calibrated 2D multi-view\nimages is a fundamental yet active research area in computer vision. Despite\nthe steady progress in multi-view stereo reconstruction, most existing methods\nare still limited in recovering fine-scale details and sharp features while\nsuppressing noises, and may fail in reconstructing regions with few textures.\nTo address these limitations, this paper presents a Detail-preserving and\nContent-aware Variational (DCV) multi-view stereo method, which reconstructs\nthe 3D surface by alternating between reprojection error minimization and mesh\ndenoising. In reprojection error minimization, we propose a novel inter-image\nsimilarity measure, which is effective to preserve fine-scale details of the\nreconstructed surface and builds a connection between guided image filtering\nand image registration. In mesh denoising, we propose a content-aware\n$\\ell_{p}$-minimization algorithm by adaptively estimating the $p$ value and\nregularization parameters based on the current input. It is much more promising\nin suppressing noise while preserving sharp features than conventional\nisotropic mesh smoothing. Experimental results on benchmark datasets\ndemonstrate that our DCV method is capable of recovering more surface details,\nand obtains cleaner and more accurate reconstructions than state-of-the-art\nmethods. In particular, our method achieves the best results among all\npublished methods on the Middlebury dino ring and dino sparse ring datasets in\nterms of both completeness and accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 03:03:49 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Li", "Zhaoxin", ""], ["Wang", "Kuanquan", ""], ["Zuo", "Wangmeng", ""], ["Meng", "Deyu", ""], ["Zhang", "Lei", ""]]}, {"id": "1505.00393", "submitter": "Francesco Visin", "authors": "Francesco Visin and Kyle Kastner and Kyunghyun Cho and Matteo\n  Matteucci and Aaron Courville and Yoshua Bengio", "title": "ReNet: A Recurrent Neural Network Based Alternative to Convolutional\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a deep neural network architecture for object\nrecognition based on recurrent neural networks. The proposed network, called\nReNet, replaces the ubiquitous convolution+pooling layer of the deep\nconvolutional neural network with four recurrent neural networks that sweep\nhorizontally and vertically in both directions across the image. We evaluate\nthe proposed ReNet on three widely-used benchmark datasets; MNIST, CIFAR-10 and\nSVHN. The result suggests that ReNet is a viable alternative to the deep\nconvolutional neural network, and that further investigation is needed.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 04:58:53 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2015 11:31:53 GMT"}, {"version": "v3", "created": "Thu, 23 Jul 2015 17:11:04 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Visin", "Francesco", ""], ["Kastner", "Kyle", ""], ["Cho", "Kyunghyun", ""], ["Matteucci", "Matteo", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1505.00412", "submitter": "Gonzalo Galiano", "authors": "Gonzalo Galiano and Juli\\'an Velasco", "title": "On a fast bilateral filtering formulation using functional\n  rearrangements", "comments": "29 pages, Journal of Mathematical Imaging and Vision, 2015. arXiv\n  admin note: substantial text overlap with arXiv:1406.7128", "journal-ref": null, "doi": "10.1007/s10851-015-0583-y", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an exact reformulation of a broad class of neighborhood filters,\namong which the bilateral filters, in terms of two functional rearrangements:\nthe decreasing and the relative rearrangements.\n  Independently of the image spatial dimension (one-dimensional signal, image,\nvolume of images, etc.), we reformulate these filters as integral operators\ndefined in a one-dimensional space corresponding to the level sets measures.\n  We prove the equivalence between the usual pixel-based version and the\nrearranged version of the filter. When restricted to the discrete setting, our\nreformulation of bilateral filters extends previous results for the so-called\nfast bilateral filtering. We, in addition, prove that the solution of the\ndiscrete setting, understood as constant-wise interpolators, converges to the\nsolution of the continuous setting.\n  Finally, we numerically illustrate computational aspects concerning quality\napproximation and execution time provided by the rearranged formulation.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 09:50:33 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Galiano", "Gonzalo", ""], ["Velasco", "Juli\u00e1n", ""]]}, {"id": "1505.00424", "submitter": "Piotr Plonski", "authors": "Piotr P{\\l}o\\'nski, Dorota Stefan, Robert Sulej, Krzysztof Zaremba", "title": "Electron Neutrino Classification in Liquid Argon Time Projection Chamber\n  Detector", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neutrinos are one of the least known elementary particles. The detection of\nneutrinos is an extremely difficult task since they are affected only by weak\nsub-atomic force or gravity. Therefore large detectors are constructed to\nreveal neutrino's properties. Among them the Liquid Argon Time Projection\nChamber (LAr-TPC) detectors provide excellent imaging and particle\nidentification ability for studying neutrinos. The computerized methods for\nautomatic reconstruction and identification of particles are needed to fully\nexploit the potential of the LAr-TPC technique. Herein, the novel method for\nelectron neutrino classification is presented. The method constructs a feature\ndescriptor from images of observed event. It characterizes the signal\ndistribution propagated from vertex of interest, where the particle interacts\nwith the detector medium. The classifier is learned with a constructed feature\ndescriptor to decide whether the images represent the electron neutrino or\ncascade produced by photons. The proposed approach assumes that the position of\nprimary interaction vertex is known. The method's performance in dependency to\nthe noise in a primary vertex position and deposited energy of particles is\nstudied.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 12:52:22 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["P\u0142o\u0144ski", "Piotr", ""], ["Stefan", "Dorota", ""], ["Sulej", "Robert", ""], ["Zaremba", "Krzysztof", ""]]}, {"id": "1505.00432", "submitter": "Sajib Saha", "authors": "Basura Fernando, Sezer Karaoglu, Sajib Kumar Saha", "title": "Object Class Detection and Classification using Multi Scale Gradient and\n  Corner Point based Shape Descriptors", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel multi scale gradient and a corner point based\nshape descriptors. The novel multi scale gradient based shape descriptor is\ncombined with generic Fourier descriptors to extract contour and region based\nshape information. Shape information based object class detection and\nclassification technique with a random forest classifier has been optimized.\nProposed integrated descriptor in this paper is robust to rotation, scale,\ntranslation, affine deformations, noisy contours and noisy shapes. The new\ncorner point based interpolated shape descriptor has been exploited for fast\nobject detection and classification with higher accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 14:06:08 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Fernando", "Basura", ""], ["Karaoglu", "Sezer", ""], ["Saha", "Sajib Kumar", ""]]}, {"id": "1505.00468", "submitter": "Aishwarya Agrawal", "authors": "Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C.\n  Lawrence Zitnick, Dhruv Batra, Devi Parikh", "title": "VQA: Visual Question Answering", "comments": "The first three authors contributed equally. International Conference\n  on Computer Vision (ICCV) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the task of free-form and open-ended Visual Question Answering\n(VQA). Given an image and a natural language question about the image, the task\nis to provide an accurate natural language answer. Mirroring real-world\nscenarios, such as helping the visually impaired, both the questions and\nanswers are open-ended. Visual questions selectively target different areas of\nan image, including background details and underlying context. As a result, a\nsystem that succeeds at VQA typically needs a more detailed understanding of\nthe image and complex reasoning than a system producing generic image captions.\nMoreover, VQA is amenable to automatic evaluation, since many open-ended\nanswers contain only a few words or a closed set of answers that can be\nprovided in a multiple-choice format. We provide a dataset containing ~0.25M\nimages, ~0.76M questions, and ~10M answers (www.visualqa.org), and discuss the\ninformation it provides. Numerous baselines and methods for VQA are provided\nand compared with human performance. Our VQA demo is available on CloudCV\n(http://cloudcv.org/vqa).\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 20:07:39 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 16:59:52 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2015 02:47:20 GMT"}, {"version": "v4", "created": "Wed, 18 Nov 2015 16:43:33 GMT"}, {"version": "v5", "created": "Mon, 7 Mar 2016 20:55:28 GMT"}, {"version": "v6", "created": "Wed, 20 Apr 2016 03:09:33 GMT"}, {"version": "v7", "created": "Thu, 27 Oct 2016 03:50:19 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Agrawal", "Aishwarya", ""], ["Lu", "Jiasen", ""], ["Antol", "Stanislaw", ""], ["Mitchell", "Margaret", ""], ["Zitnick", "C. Lawrence", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1505.00487", "submitter": "Subhashini Venugopalan", "authors": "Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney,\n  Trevor Darrell, Kate Saenko", "title": "Sequence to Sequence -- Video to Text", "comments": "ICCV 2015 camera-ready. Includes code, project page and LSMDC\n  challenge results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world videos often have complex dynamics; and methods for generating\nopen-domain video descriptions should be sensitive to temporal structure and\nallow both input (sequence of frames) and output (sequence of words) of\nvariable length. To approach this problem, we propose a novel end-to-end\nsequence-to-sequence model to generate captions for videos. For this we exploit\nrecurrent neural networks, specifically LSTMs, which have demonstrated\nstate-of-the-art performance in image caption generation. Our LSTM model is\ntrained on video-sentence pairs and learns to associate a sequence of video\nframes to a sequence of words in order to generate a description of the event\nin the video clip. Our model naturally is able to learn the temporal structure\nof the sequence of frames as well as the sequence model of the generated\nsentences, i.e. a language model. We evaluate several variants of our model\nthat exploit different visual features on a standard set of YouTube videos and\ntwo movie description datasets (M-VAD and MPII-MD).\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 22:32:00 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 16:08:57 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2015 18:01:06 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Venugopalan", "Subhashini", ""], ["Rohrbach", "Marcus", ""], ["Donahue", "Jeff", ""], ["Mooney", "Raymond", ""], ["Darrell", "Trevor", ""], ["Saenko", "Kate", ""]]}, {"id": "1505.00523", "submitter": "Yong Shean Chong", "authors": "Yong Shean Chong, Yong Haur Tay", "title": "Modeling Representation of Videos for Anomaly Detection using Deep\n  Learning: A Review", "comments": "arXiv admin note: text overlap with arXiv:1411.4423 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This review article surveys the current progresses made toward video-based\nanomaly detection. We address the most fundamental aspect for video anomaly\ndetection, that is, video feature representation. Much research works have been\ndone in finding the right representation to perform anomaly detection in video\nstreams accurately with an acceptable false alarm rate. However, this is very\nchallenging due to large variations in environment and human movement, and high\nspace-time complexity due to huge dimensionality of video data. The weakly\nsupervised nature of deep learning algorithms can help in learning\nrepresentations from the video data itself instead of manually designing the\nright feature for specific scenes. In this paper, we would like to review the\nexisting methods of modeling video representations using deep learning\ntechniques for the task of anomaly detection and action recognition.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 05:16:08 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Chong", "Yong Shean", ""], ["Tay", "Yong Haur", ""]]}, {"id": "1505.00529", "submitter": "Yue Wu Dr.", "authors": "Yue Wu, Stephen Rawls, Wael AbdAlmageed and Premkumar Natarajan", "title": "Learning Document Image Binarization from Data", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a fully trainable binarization solution for degraded\ndocument images. Unlike previous attempts that often used simple features with\na series of pre- and post-processing, our solution encodes all heuristics about\nwhether or not a pixel is foreground text into a high-dimensional feature\nvector and learns a more complicated decision function. In particular, we\nprepare features of three types: 1) existing features for binarization such as\nintensity [1], contrast [2], [3], and Laplacian [4], [5]; 2) reformulated\nfeatures from existing binarization decision functions such those in [6] and\n[7]; and 3) our newly developed features, namely the Logarithm Intensity\nPercentile (LIP) and the Relative Darkness Index (RDI). Our initial\nexperimental results show that using only selected samples (about 1.5% of all\navailable training data), we can achieve a binarization performance comparable\nto those fine-tuned (typically by hand), state-of-the-art methods.\nAdditionally, the trained document binarization classifier shows good\ngeneralization capabilities on out-of-domain data.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 05:57:17 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Wu", "Yue", ""], ["Rawls", "Stephen", ""], ["AbdAlmageed", "Wael", ""], ["Natarajan", "Premkumar", ""]]}, {"id": "1505.00571", "submitter": "Alexander Shekhovtsov", "authors": "Alexander Shekhovtsov", "title": "Higher Order Maximum Persistency and Comparison Theorems", "comments": "Submitted to CVIU Special Issuie on Inference in Graphical Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DM math.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We address combinatorial problems that can be formulated as minimization of a\npartially separable function of discrete variables (energy minimization in\ngraphical models, weighted constraint satisfaction, pseudo-Boolean\noptimization, 0-1 polynomial programming). For polyhedral relaxations of such\nproblems it is generally not true that variables integer in the relaxed\nsolution will retain the same values in the optimal discrete solution. Those\nwhich do are called persistent. Such persistent variables define a part of a\nglobally optimal solution. Once identified, they can be excluded from the\nproblem, reducing its size.\n  To any polyhedral relaxation we associate a sufficient condition proving\npersistency of a subset of variables. We set up a specially constructed linear\nprogram which determines the set of persistent variables maximal with respect\nto the relaxation. The condition improves as the relaxation is tightened and\npossesses all its invariances. The proposed framework explains a variety of\nexisting methods originating from different areas of research and based on\ndifferent principles. A theoretical comparison is established that relates\nthese methods to the standard linear relaxation and proves that the proposed\ntechnique identifies same or larger set of persistent variables.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 09:50:25 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Shekhovtsov", "Alexander", ""]]}, {"id": "1505.00581", "submitter": "Christian Wolf", "authors": "Eric Lombardi and Christian Wolf and Oya Celiktutan and B\\\"ulent\n  Sankur", "title": "Activity recognition from videos with parallel hypergraph matching on\n  GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method for activity recognition from videos based\non sparse local features and hypergraph matching. We benefit from special\nproperties of the temporal domain in the data to derive a sequential and fast\ngraph matching algorithm for GPUs.\n  Traditionally, graphs and hypergraphs are frequently used to recognize\ncomplex and often non-rigid patterns in computer vision, either through graph\nmatching or point-set matching with graphs. Most formulations resort to the\nminimization of a difficult discrete energy function mixing geometric or\nstructural terms with data attached terms involving appearance features.\nTraditional methods solve this minimization problem approximately, for instance\nwith spectral techniques.\n  In this work, instead of solving the problem approximatively, the exact\nsolution for the optimal assignment is calculated in parallel on GPUs. The\ngraphical structure is simplified and regularized, which allows to derive an\nefficient recursive minimization algorithm. The algorithm distributes\nsubproblems over the calculation units of a GPU, which solves them in parallel,\nallowing the system to run faster than real-time on medium-end GPUs.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 10:30:47 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Lombardi", "Eric", ""], ["Wolf", "Christian", ""], ["Celiktutan", "Oya", ""], ["Sankur", "B\u00fclent", ""]]}, {"id": "1505.00663", "submitter": "Wei-Chen Chiu", "authors": "Wei-Chen Chiu and Mario Fritz", "title": "See the Difference: Direct Pre-Image Reconstruction and Pose Estimation\n  by Differentiating HOG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Histogram of Oriented Gradient (HOG) descriptor has led to many advances\nin computer vision over the last decade and is still part of many state of the\nart approaches. We realize that the associated feature computation is piecewise\ndifferentiable and therefore many pipelines which build on HOG can be made\ndifferentiable. This lends to advanced introspection as well as opportunities\nfor end-to-end optimization. We present our implementation of $\\nabla$HOG based\non the auto-differentiation toolbox Chumpy and show applications to pre-image\nvisualization and pose estimation which extends the existing differentiable\nrenderer OpenDR pipeline. Both applications improve on the respective\nstate-of-the-art HOG approaches.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 14:50:29 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 13:16:00 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2015 19:24:51 GMT"}, {"version": "v4", "created": "Fri, 2 Oct 2015 10:00:18 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Chiu", "Wei-Chen", ""], ["Fritz", "Mario", ""]]}, {"id": "1505.00670", "submitter": "Hoo Chang Shin", "authors": "Hoo-Chang Shin, Le Lu, Lauren Kim, Ari Seff, Jianhua Yao, Ronald M.\n  Summers", "title": "Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database\n  for Automated Image Interpretation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite tremendous progress in computer vision, there has not been an attempt\nfor machine learning on very large-scale medical image databases. We present an\ninterleaved text/image deep learning system to extract and mine the semantic\ninteractions of radiology images and reports from a national research\nhospital's Picture Archiving and Communication System. With natural language\nprocessing, we mine a collection of representative ~216K two-dimensional key\nimages selected by clinicians for diagnostic reference, and match the images\nwith their descriptions in an automated manner. Our system interleaves between\nunsupervised learning and supervised learning on document- and sentence-level\ntext collections, to generate semantic labels and to predict them given an\nimage. Given an image of a patient scan, semantic topics in radiology levels\nare predicted, and associated key-words are generated. Also, a number of\nfrequent disease types are detected as present or absent, to provide more\nspecific interpretation of a patient scan. This shows the potential of\nlarge-scale learning and prediction in electronic patient records available in\nmost modern clinical institutions.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 15:05:59 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Shin", "Hoo-Chang", ""], ["Lu", "Le", ""], ["Kim", "Lauren", ""], ["Seff", "Ari", ""], ["Yao", "Jianhua", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1505.00687", "submitter": "Xiaolong Wang", "authors": "Xiaolong Wang, Abhinav Gupta", "title": "Unsupervised Learning of Visual Representations using Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is strong supervision necessary for learning a good visual representation? Do\nwe really need millions of semantically-labeled images to train a Convolutional\nNeural Network (CNN)? In this paper, we present a simple yet surprisingly\npowerful approach for unsupervised learning of CNN. Specifically, we use\nhundreds of thousands of unlabeled videos from the web to learn visual\nrepresentations. Our key idea is that visual tracking provides the supervision.\nThat is, two patches connected by a track should have similar visual\nrepresentation in deep feature space since they probably belong to the same\nobject or object part. We design a Siamese-triplet network with a ranking loss\nfunction to train this CNN representation. Without using a single image from\nImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train\nan ensemble of unsupervised networks that achieves 52% mAP (no bounding box\nregression). This performance comes tantalizingly close to its\nImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We\nalso show that our unsupervised network can perform competitively in other\ntasks such as surface-normal estimation.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 15:50:53 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 17:05:49 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Wang", "Xiaolong", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1505.00737", "submitter": "Mrinal Haloi", "authors": "Mrinal Haloi, Samarendra Dandapat, Rohit Sinha", "title": "A Gaussian Scale Space Approach For Exudates Detection, Classification\n  And Severity Prediction", "comments": "Accepted in ICIP 2015, Quebec city, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of Computer Aided Diagnosis system for diabetic retinopathy,\nwe present a novel method for detection of exudates and their classification\nfor disease severity prediction. The method is based on Gaussian scale space\nbased interest map and mathematical morphology. It makes use of support vector\nmachine for classification and location information of the optic disc and the\nmacula region for severity prediction. It can efficiently handle luminance\nvariation and it is suitable for varied sized exudates. The method has been\nprobed in publicly available DIARETDB1V2 and e-ophthaEX databases. For exudate\ndetection the proposed method achieved a sensitivity of 96.54% and prediction\nof 98.35% in DIARETDB1V2 database.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 18:05:52 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Haloi", "Mrinal", ""], ["Dandapat", "Samarendra", ""], ["Sinha", "Rohit", ""]]}, {"id": "1505.00824", "submitter": "Eva Dyer", "authors": "Eva L. Dyer, Tom A. Goldstein, Raajen Patel, Konrad P. Kording, and\n  Richard G. Baraniuk", "title": "Self-Expressive Decompositions for Matrix Approximation and Clustering", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-aware methods for dimensionality reduction and matrix decomposition aim\nto find low-dimensional structure in a collection of data. Classical approaches\ndiscover such structure by learning a basis that can efficiently express the\ncollection. Recently, \"self expression\", the idea of using a small subset of\ndata vectors to represent the full collection, has been developed as an\nalternative to learning. Here, we introduce a scalable method for computing\nsparse SElf-Expressive Decompositions (SEED). SEED is a greedy method that\nconstructs a basis by sequentially selecting incoherent vectors from the\ndataset. After forming a basis from a subset of vectors in the dataset, SEED\nthen computes a sparse representation of the dataset with respect to this\nbasis. We develop sufficient conditions under which SEED exactly represents low\nrank matrices and vectors sampled from a unions of independent subspaces. We\nshow how SEED can be used in applications ranging from matrix approximation and\ndenoising to clustering, and apply it to numerous real-world datasets. Our\nresults demonstrate that SEED is an attractive low-complexity alternative to\nother sparse matrix factorization approaches such as sparse PCA and\nself-expressive methods for clustering.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 21:56:54 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Dyer", "Eva L.", ""], ["Goldstein", "Tom A.", ""], ["Patel", "Raajen", ""], ["Kording", "Konrad P.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1505.00853", "submitter": "Bing Xu", "authors": "Bing Xu, Naiyan Wang, Tianqi Chen, Mu Li", "title": "Empirical Evaluation of Rectified Activations in Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the performance of different types of rectified\nactivation functions in convolutional neural network: standard rectified linear\nunit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified\nlinear unit (PReLU) and a new randomized leaky rectified linear units (RReLU).\nWe evaluate these activation function on standard image classification task.\nOur experiments suggest that incorporating a non-zero slope for negative part\nin rectified activation units could consistently improve the results. Thus our\nfindings are negative on the common belief that sparsity is the key of good\nperformance in ReLU. Moreover, on small scale dataset, using deterministic\nnegative slope or learning it are both prone to overfitting. They are not as\neffective as using their randomized counterpart. By using RReLU, we achieved\n75.68\\% accuracy on CIFAR-100 test set without multiple test or ensemble.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 01:16:39 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2015 06:58:14 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Xu", "Bing", ""], ["Wang", "Naiyan", ""], ["Chen", "Tianqi", ""], ["Li", "Mu", ""]]}, {"id": "1505.00855", "submitter": "Babak Saleh", "authors": "Babak Saleh and Ahmed Elgammal", "title": "Large-scale Classification of Fine-Art Paintings: Learning The Right\n  Metric on The Right Feature", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, the number of fine-art collections that are digitized\nand publicly available has been growing rapidly. With the availability of such\nlarge collections of digitized artworks comes the need to develop multimedia\nsystems to archive and retrieve this pool of data. Measuring the visual\nsimilarity between artistic items is an essential step for such multimedia\nsystems, which can benefit more high-level multimedia tasks. In order to model\nthis similarity between paintings, we should extract the appropriate visual\nfeatures for paintings and find out the best approach to learn the similarity\nmetric based on these features. We investigate a comprehensive list of visual\nfeatures and metric learning approaches to learn an optimized similarity\nmeasure between paintings. We develop a machine that is able to make\naesthetic-related semantic-level judgments, such as predicting a painting's\nstyle, genre, and artist, as well as providing similarity measures optimized\nbased on the knowledge available in the domain of art historical\ninterpretation. Our experiments show the value of using this similarity measure\nfor the aforementioned prediction tasks.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 01:25:26 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Saleh", "Babak", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1505.00866", "submitter": "Surya Prasath", "authors": "Juan C. Moreno, V. B. Surya Prasath, D. Vorotnikov, H. Proenca, K.\n  Palaniappan", "title": "Adaptive diffusion constrained total variation scheme with application\n  to `cartoon + texture + edge' image decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an image decomposition model involving a variational\n(minimization) problem and an evolutionary partial differential equation (PDE).\nWe utilize a linear inhomogenuous diffusion constrained and weighted total\nvariation (TV) scheme for image adaptive decomposition. An adaptive weight\nalong with TV regularization splits a given image into three components\nrepresenting the geometrical (cartoon), textural (small scale - microtextures),\nand edges (big scale - macrotextures). We study the wellposedness of the\ncoupled variational-PDE scheme along with an efficient numerical scheme based\non Chambolle's dual minimization method. We provide extensive experimental\nresults in cartoon-texture-edges decomposition, and denoising as well compare\nwith other related variational, coupled anisotropic diffusion PDE based\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 02:14:15 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Moreno", "Juan C.", ""], ["Prasath", "V. B. Surya", ""], ["Vorotnikov", "D.", ""], ["Proenca", "H.", ""], ["Palaniappan", "K.", ""]]}, {"id": "1505.00880", "submitter": "Hang Su", "authors": "Hang Su, Subhransu Maji, Evangelos Kalogerakis, Erik Learned-Miller", "title": "Multi-view Convolutional Neural Networks for 3D Shape Recognition", "comments": "v1: Initial version. v2: An updated ModelNet40 training/test split is\n  used; results with low-rank Mahalanobis metric learning are added. v3 (ICCV\n  2015): A second camera setup without the upright orientation assumption is\n  added; some accuracy and mAP numbers are changed slightly because a small\n  issue in mesh rendering related to specularities is fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A longstanding question in computer vision concerns the representation of 3D\nshapes for recognition: should 3D shapes be represented with descriptors\noperating on their native 3D formats, such as voxel grid or polygon mesh, or\ncan they be effectively represented with view-based descriptors? We address\nthis question in the context of learning to recognize 3D shapes from a\ncollection of their rendered views on 2D images. We first present a standard\nCNN architecture trained to recognize the shapes' rendered views independently\nof each other, and show that a 3D shape can be recognized even from a single\nview at an accuracy far higher than using state-of-the-art 3D shape\ndescriptors. Recognition rates further increase when multiple views of the\nshapes are provided. In addition, we present a novel CNN architecture that\ncombines information from multiple views of a 3D shape into a single and\ncompact shape descriptor offering even better recognition performance. The same\narchitecture can be applied to accurately recognize human hand-drawn sketches\nof shapes. We conclude that a collection of 2D views can be highly informative\nfor 3D shape recognition and is amenable to emerging CNN architectures and\ntheir derivatives.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 04:51:19 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 18:16:01 GMT"}, {"version": "v3", "created": "Sun, 27 Sep 2015 20:42:16 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Su", "Hang", ""], ["Maji", "Subhransu", ""], ["Kalogerakis", "Evangelos", ""], ["Learned-Miller", "Erik", ""]]}, {"id": "1505.00996", "submitter": "Kaiming He", "authors": "Kaiming He, Jian Sun", "title": "Fast Guided Filter", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The guided filter is a technique for edge-aware image filtering. Because of\nits nice visual quality, fast speed, and ease of implementation, the guided\nfilter has witnessed various applications in real products, such as image\nediting apps in phones and stereo reconstruction, and has been included in\nofficial MATLAB and OpenCV. In this note, we remind that the guided filter can\nbe simply sped up from O(N) time to O(N/s^2) time for a subsampling ratio s. In\na variety of applications, this leads to a speedup of >10x with almost no\nvisible degradation. We hope this acceleration will improve performance of\ncurrent applications and further popularize this filter. Code is released.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 13:10:53 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["He", "Kaiming", ""], ["Sun", "Jian", ""]]}, {"id": "1505.01065", "submitter": "Sebastian Hegenbart", "authors": "Sebastian Hegenbart, Roland Kwitt, Andreas Uhl", "title": "Proceedings of The 39th Annual Workshop of the Austrian Association for\n  Pattern Recognition (OAGM), 2015", "comments": "Index submitted before individual papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 39th annual workshop of the Austrian Association for Pattern Recognition\n(OAGM/AAPR) provides a platform for presentation and discussion of research\nprogress as well as research projects within the OAGM/AAPR community.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 10:10:16 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Hegenbart", "Sebastian", ""], ["Kwitt", "Roland", ""], ["Uhl", "Andreas", ""]]}, {"id": "1505.01085", "submitter": "David Fouhey", "authors": "David F. Fouhey and Xiaolong Wang and Abhinav Gupta", "title": "In Defense of the Direct Perception of Affordances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of functional recognition or affordance estimation from images has\nseen a revival in recent years. As originally proposed by Gibson, the\naffordances of a scene were directly perceived from the ambient light: in other\nwords, functional properties like sittable were estimated directly from\nincoming pixels. Recent work, however, has taken a mediated approach in which\naffordances are derived by first estimating semantics or geometry and then\nreasoning about the affordances. In a tribute to Gibson, this paper explores\nhis theory of affordances as originally proposed. We propose two approaches for\ndirect perception of affordances and show that they obtain good results and can\nout-perform mediated approaches. We hope this paper can rekindle discussion\naround direct perception and its implications in the long term.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 17:11:26 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Fouhey", "David F.", ""], ["Wang", "Xiaolong", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1505.01121", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Marcus Rohrbach and Mario Fritz", "title": "Ask Your Neurons: A Neural-based Approach to Answering Questions about\n  Images", "comments": "ICCV'15 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a question answering task on real-world images that is set up as a\nVisual Turing Test. By combining latest advances in image representation and\nnatural language processing, we propose Neural-Image-QA, an end-to-end\nformulation to this problem for which all parts are trained jointly. In\ncontrast to previous efforts, we are facing a multi-modal problem where the\nlanguage output (answer) is conditioned on visual and natural language input\n(image and question). Our approach Neural-Image-QA doubles the performance of\nthe previous best approach on this problem. We provide additional insights into\nthe problem by analyzing how much information is contained only in the language\npart for which we provide a new human baseline. To study human consensus, which\nis related to the ambiguities inherent in this challenging task, we propose two\nnovel metrics and collect additional answers which extends the original DAQUAR\ndataset to DAQUAR-Consensus.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 18:39:29 GMT"}, {"version": "v2", "created": "Wed, 6 May 2015 08:10:01 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2015 12:13:20 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Rohrbach", "Marcus", ""], ["Fritz", "Mario", ""]]}, {"id": "1505.01130", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Marc Bola\\~nos, Ricard Mestre, Estefan\\'ia Talavera, Xavier\n  Gir\\'o-i-Nieto and Petia Radeva", "title": "Visual Summary of Egocentric Photostreams by Representative Keyframes", "comments": "Paper accepted in the IEEE First International Workshop on Wearable\n  and Ego-vision Systems for Augmented Experience (WEsAX). Turin, Italy. July\n  3, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a visual summary from an egocentric photostream captured by a\nlifelogging wearable camera is of high interest for different applications\n(e.g. memory reinforcement). In this paper, we propose a new summarization\nmethod based on keyframes selection that uses visual features extracted by\nmeans of a convolutional neural network. Our method applies an unsupervised\nclustering for dividing the photostreams into events, and finally extracts the\nmost relevant keyframe for each event. We assess the results by applying a\nblind-taste test on a group of 20 people who assessed the quality of the\nsummaries.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 19:14:23 GMT"}, {"version": "v2", "created": "Wed, 6 May 2015 14:00:23 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Bola\u00f1os", "Marc", ""], ["Mestre", "Ricard", ""], ["Talavera", "Estefan\u00eda", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""], ["Radeva", "Petia", ""]]}, {"id": "1505.01173", "submitter": "Hui Jiang", "authors": "Hengyue Pan, Bo Wang and Hui Jiang", "title": "Deep Learning for Object Saliency Detection and Image Segmentation", "comments": "9 pages, 126 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose several novel deep learning methods for object\nsaliency detection based on the powerful convolutional neural networks. In our\napproach, we use a gradient descent method to iteratively modify an input image\nbased on the pixel-wise gradients to reduce a cost function measuring the\nclass-specific objectness of the image. The pixel-wise gradients can be\nefficiently computed using the back-propagation algorithm. The discrepancy\nbetween the modified image and the original one may be used as a saliency map\nfor the image. Moreover, we have further proposed several new training methods\nto learn saliency-specific convolutional nets for object saliency detection, in\norder to leverage the available pixel-wise segmentation information. Our\nmethods are extremely computationally efficient (processing 20-40 images per\nsecond in one GPU). In this work, we use the computed saliency maps for image\nsegmentation. Experimental results on two benchmark tasks, namely Microsoft\nCOCO and Pascal VOC 2012, have shown that our proposed methods can generate\nhigh-quality salience maps, clearly outperforming many existing methods. In\nparticular, our approaches excel in handling many difficult images, which\ncontain complex background, highly-variable salient objects, multiple objects,\nand/or very small salient objects.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 20:03:07 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Pan", "Hengyue", ""], ["Wang", "Bo", ""], ["Jiang", "Hui", ""]]}, {"id": "1505.01197", "submitter": "Georgia Gkioxari", "authors": "Georgia Gkioxari, Ross Girshick, Jitendra Malik", "title": "Contextual Action Recognition with R*CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are multiple cues in an image which reveal what action a person is\nperforming. For example, a jogger has a pose that is characteristic for\njogging, but the scene (e.g. road, trail) and the presence of other joggers can\nbe an additional source of information. In this work, we exploit the simple\nobservation that actions are accompanied by contextual cues to build a strong\naction recognition system. We adapt RCNN to use more than one region for\nclassification while still maintaining the ability to localize the action. We\ncall our system R*CNN. The action-specific models and the feature maps are\ntrained jointly, allowing for action specific representations to emerge. R*CNN\nachieves 90.2% mean AP on the PASAL VOC Action dataset, outperforming all other\napproaches in the field by a significant margin. Last, we show that R*CNN is\nnot limited to action recognition. In particular, R*CNN can also be used to\ntackle fine-grained tasks such as attribute classification. We validate this\nclaim by reporting state-of-the-art performance on the Berkeley Attributes of\nPeople dataset.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 21:56:10 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2015 20:29:26 GMT"}, {"version": "v3", "created": "Fri, 25 Mar 2016 01:06:01 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Gkioxari", "Georgia", ""], ["Girshick", "Ross", ""], ["Malik", "Jitendra", ""]]}, {"id": "1505.01214", "submitter": "Babak Saleh", "authors": "Babak Saleh and Mira Dontcheva and Aaron Hertzmann and Zhicheng Liu", "title": "Learning Style Similarity for Searching Infographics", "comments": "6 pages, to appear in the 41st annual conference on Graphics\n  Interface (GI) 2015,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.HC cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infographics are complex graphic designs integrating text, images, charts and\nsketches. Despite the increasing popularity of infographics and the rapid\ngrowth of online design portfolios, little research investigates how we can\ntake advantage of these design resources. In this paper we present a method for\nmeasuring the style similarity between infographics. Based on human perception\ndata collected from crowdsourced experiments, we use computer vision and\nmachine learning algorithms to learn a style similarity metric for infographic\ndesigns. We evaluate different visual features and learning algorithms and find\nthat a combination of color histograms and Histograms-of-Gradients (HoG)\nfeatures is most effective in characterizing the style of infographics. We\ndemonstrate our similarity metric on a preliminary image retrieval test.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 22:59:32 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Saleh", "Babak", ""], ["Dontcheva", "Mira", ""], ["Hertzmann", "Aaron", ""], ["Liu", "Zhicheng", ""]]}, {"id": "1505.01257", "submitter": "Tatiana Tommasi", "authors": "Tatiana Tommasi, Novi Patricia, Barbara Caputo, Tinne Tuytelaars", "title": "A Deeper Look at Dataset Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of a bias in each image data collection has recently attracted a\nlot of attention in the computer vision community showing the limits in\ngeneralization of any learning method trained on a specific dataset. At the\nsame time, with the rapid development of deep learning architectures, the\nactivation values of Convolutional Neural Networks (CNN) are emerging as\nreliable and robust image descriptors. In this paper we propose to verify the\npotential of the DeCAF features when facing the dataset bias problem. We\nconduct a series of analyses looking at how existing datasets differ among each\nother and verifying the performance of existing debiasing methods under\ndifferent representations. We learn important lessons on which part of the\ndataset bias problem can be considered solved and which open questions still\nneed to be tackled.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 06:19:23 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Tommasi", "Tatiana", ""], ["Patricia", "Novi", ""], ["Caputo", "Barbara", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1505.01335", "submitter": "Barbara Di Fabio", "authors": "Barbara Di Fabio and Massimo Ferri", "title": "Comparing persistence diagrams through complex vectors", "comments": "11 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": "amsacta4233", "categories": "math.AT cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The natural pseudo-distance of spaces endowed with filtering functions is\nprecious for shape classification and retrieval; its optimal estimate coming\nfrom persistence diagrams is the bottleneck distance, which unfortunately\nsuffers from combinatorial explosion. A possible algebraic representation of\npersistence diagrams is offered by complex polynomials; since far polynomials\nrepresent far persistence diagrams, a fast comparison of the coefficient\nvectors can reduce the size of the database to be classified by the bottleneck\ndistance. This article explores experimentally three transformations from\ndiagrams to polynomials and three distances between the complex vectors of\ncoefficients.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 12:13:26 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 23:02:36 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Di Fabio", "Barbara", ""], ["Ferri", "Massimo", ""]]}, {"id": "1505.01350", "submitter": "Ozgur Yilmaz", "authors": "Ozgur Yilmaz", "title": "Classification of Occluded Objects using Fast Recurrent Processing", "comments": "arXiv admin note: text overlap with arXiv:1409.8576 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks are powerful tools for handling incomplete data\nproblems in computer vision, thanks to their significant generative\ncapabilities. However, the computational demand for these algorithms is too\nhigh to work in real time, without specialized hardware or software solutions.\nIn this paper, we propose a framework for augmenting recurrent processing\ncapabilities into a feedforward network without sacrificing much from\ncomputational efficiency. We assume a mixture model and generate samples of the\nlast hidden layer according to the class decisions of the output layer, modify\nthe hidden layer activity using the samples, and propagate to lower layers. For\nvisual occlusion problem, the iterative procedure emulates feedforward-feedback\nloop, filling-in the missing hidden layer activity with meaningful\nrepresentations. The proposed algorithm is tested on a widely used dataset, and\nshown to achieve 2$\\times$ improvement in classification accuracy for occluded\nobjects. When compared to Restricted Boltzmann Machines, our algorithm shows\nsuperior performance for occluded object classification.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 12:58:58 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Yilmaz", "Ozgur", ""]]}, {"id": "1505.01429", "submitter": "Julio Cesar Ferreira", "authors": "Julio Cesar Ferreira, Elif Vural, and Christine Guillemot", "title": "Geometry-Aware Neighborhood Search for Learning Local Models for Image\n  Reconstruction", "comments": "15 pages, 10 figures and 5 tables", "journal-ref": null, "doi": "10.1109/TIP.2016.2522303", "report-no": null, "categories": "cs.CV cs.IT math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local learning of sparse image models has proven to be very effective to\nsolve inverse problems in many computer vision applications. To learn such\nmodels, the data samples are often clustered using the K-means algorithm with\nthe Euclidean distance as a dissimilarity metric. However, the Euclidean\ndistance may not always be a good dissimilarity measure for comparing data\nsamples lying on a manifold. In this paper, we propose two algorithms for\ndetermining a local subset of training samples from which a good local model\ncan be computed for reconstructing a given input test sample, where we take\ninto account the underlying geometry of the data. The first algorithm, called\nAdaptive Geometry-driven Nearest Neighbor search (AGNN), is an adaptive scheme\nwhich can be seen as an out-of-sample extension of the replicator graph\nclustering method for local model learning. The second method, called\nGeometry-driven Overlapping Clusters (GOC), is a less complex nonadaptive\nalternative for training subset selection. The proposed AGNN and GOC methods\nare evaluated in image super-resolution, deblurring and denoising applications\nand shown to outperform spectral clustering, soft clustering, and geodesic\ndistance based subset selection in most settings.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 16:46:55 GMT"}, {"version": "v2", "created": "Thu, 7 May 2015 11:46:26 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2016 13:37:40 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Ferreira", "Julio Cesar", ""], ["Vural", "Elif", ""], ["Guillemot", "Christine", ""]]}, {"id": "1505.01554", "submitter": "Xinlei Chen", "authors": "Xinlei Chen and Abhinav Gupta", "title": "Webly Supervised Learning of Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to utilize large amounts of web data for learning\nCNNs. Specifically inspired by curriculum learning, we present a two-step\napproach for CNN training. First, we use easy images to train an initial visual\nrepresentation. We then use this initial CNN and adapt it to harder, more\nrealistic images by leveraging the structure of data and categories. We\ndemonstrate that our two-stage CNN outperforms a fine-tuned CNN trained on\nImageNet on Pascal VOC 2012. We also demonstrate the strength of webly\nsupervised learning by localizing objects in web images and training a R-CNN\nstyle detector. It achieves the best performance on VOC 2007 where no VOC\ntraining data is used. Finally, we show our approach is quite robust to noise\nand performs comparably even when we use image search results from March 2013\n(pre-CNN image search era).\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 00:56:15 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2015 21:53:16 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Chen", "Xinlei", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1505.01560", "submitter": "Tam Nguyen", "authors": "Tam V. Nguyen, Canyi Lu, Jose Sepulveda, Shuicheng Yan", "title": "Adaptive Nonparametric Image Parsing", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we present an adaptive nonparametric solution to the image\nparsing task, namely annotating each image pixel with its corresponding\ncategory label. For a given test image, first, a locality-aware retrieval set\nis extracted from the training data based on super-pixel matching similarities,\nwhich are augmented with feature extraction for better differentiation of local\nsuper-pixels. Then, the category of each super-pixel is initialized by the\nmajority vote of the $k$-nearest-neighbor super-pixels in the retrieval set.\nInstead of fixing $k$ as in traditional non-parametric approaches, here we\npropose a novel adaptive nonparametric approach which determines the\nsample-specific k for each test image. In particular, $k$ is adaptively set to\nbe the number of the fewest nearest super-pixels which the images in the\nretrieval set can use to get the best category prediction. Finally, the initial\nsuper-pixel labels are further refined by contextual smoothing. Extensive\nexperiments on challenging datasets demonstrate the superiority of the new\nsolution over other state-of-the-art nonparametric solutions.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 02:28:32 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Nguyen", "Tam V.", ""], ["Lu", "Canyi", ""], ["Sepulveda", "Jose", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1505.01589", "submitter": "Li Shen", "authors": "Li Shen, Teck Wee Chua, Karianto Leman", "title": "Shadow Optimization from Structured Deep Edge Detection", "comments": "8 pages. CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Local structures of shadow boundaries as well as complex interactions of\nimage regions remain largely unexploited by previous shadow detection\napproaches. In this paper, we present a novel learning-based framework for\nshadow region recovery from a single image. We exploit the local structures of\nshadow edges by using a structured CNN learning framework. We show that using\nthe structured label information in the classification can improve the local\nconsistency of the results and avoid spurious labelling. We further propose and\nformulate a shadow/bright measure to model the complex interactions among image\nregions. The shadow and bright measures of each patch are computed from the\nshadow edges detected in the image. Using the global interaction constraints on\npatches, we formulate a least-square optimization problem for shadow recovery\nthat can be solved efficiently. Our shadow recovery method achieves\nstate-of-the-art results on the major shadow benchmark databases collected\nunder various conditions.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 05:07:11 GMT"}, {"version": "v2", "created": "Fri, 8 May 2015 06:42:48 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Shen", "Li", ""], ["Chua", "Teck Wee", ""], ["Leman", "Karianto", ""]]}, {"id": "1505.01596", "submitter": "Pulkit Agrawal", "authors": "Pulkit Agrawal, Joao Carreira, Jitendra Malik", "title": "Learning to See by Moving", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant paradigm for feature learning in computer vision relies on\ntraining neural networks for the task of object recognition using millions of\nhand labelled images. Is it possible to learn useful features for a diverse set\nof visual tasks using any other form of supervision? In biology, living\norganisms developed the ability of visual perception for the purpose of moving\nand acting in the world. Drawing inspiration from this observation, in this\nwork we investigate if the awareness of egomotion can be used as a supervisory\nsignal for feature learning. As opposed to the knowledge of class labels,\ninformation about egomotion is freely available to mobile agents. We show that\ngiven the same number of training images, features learnt using egomotion as\nsupervision compare favourably to features learnt using class-label as\nsupervision on visual tasks of scene recognition, object recognition, visual\nodometry and keypoint matching.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 06:03:01 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2015 16:59:36 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Agrawal", "Pulkit", ""], ["Carreira", "Joao", ""], ["Malik", "Jitendra", ""]]}, {"id": "1505.01599", "submitter": "Kenji Kume", "authors": "Kenji Kume and Naoko Nose-Togawa", "title": "Filter characteristics in image decomposition with singular spectrum\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singular spectrum analysis is developed as a nonparametric spectral\ndecomposition of a time series. It can be easily extended to the decomposition\nof multidimensional lattice-like data through the filtering interpretation. In\nthis viewpoint, the singular spectrum analysis can be understood as the\nadaptive and optimal generation of the filters and their two-step\npoint-symmetric operation to the original data. In this paper, we point out\nthat, when applied to the multidimensional data, the adaptively generated\nfilters exhibit symmetry properties resulting from the bisymmetric nature of\nthe lag-covariance matrices. The eigenvectors of the lag-covariance matrix are\neither symmetric or antisymmetric, and for the 2D image data, these lead to the\ndifferential-type filters with even- or odd-order derivatives. The dominant\nfilter is a smoothing filter, reflecting the dominance of low-frequency\ncomponents of the photo images. The others are the edge-enhancement or the\nnoise filters corresponding to the band-pass or the high-pass filters. The\nimplication of the decomposition to the image denoising is briefly discussed.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 06:21:15 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Kume", "Kenji", ""], ["Nose-Togawa", "Naoko", ""]]}, {"id": "1505.01631", "submitter": "Ruven Pillay", "authors": "Citlalli Gamez Serna, Ruven Pillay, Alain Tremeau", "title": "Data Fusion of Objects Using Techniques Such as Laser Scanning,\n  Structured Light and Photogrammetry for Cultural Heritage Applications", "comments": null, "journal-ref": "Computational Color Imaging, Lecture Notes in Computer Science,\n  Springer, 2015, pp. 208-224", "doi": "10.1007/978-3-319-15979-9_20", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a semi-automatic 2D-3D local registration pipeline\ncapable of coloring 3D models obtained from 3D scanners by using uncalibrated\nimages. The proposed pipeline exploits the Structure from Motion (SfM)\ntechnique in order to reconstruct a sparse representation of the 3D object and\nobtain the camera parameters from image feature matches. We then coarsely\nregister the reconstructed 3D model to the scanned one through the Scale\nIterative Closest Point (SICP) algorithm. SICP provides the global scale,\nrotation and translation parameters, using minimal manual user intervention. In\nthe final processing stage, a local registration refinement algorithm optimizes\nthe color projection of the aligned photos on the 3D object removing the\nblurring/ghosting artefacts introduced due to small inaccuracies during the\nregistration. The proposed pipeline is capable of handling real world cases\nwith a range of characteristics from objects with low level geometric features\nto complex ones.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 09:03:20 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Serna", "Citlalli Gamez", ""], ["Pillay", "Ruven", ""], ["Tremeau", "Alain", ""]]}, {"id": "1505.01728", "submitter": "Yamuna Prasad", "authors": "Yamuna Prasad, K. K. Biswas", "title": "Integrating K-means with Quadratic Programming Feature Selection", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several data mining problems are characterized by data in high dimensions.\nOne of the popular ways to reduce the dimensionality of the data is to perform\nfeature selection, i.e, select a subset of relevant and non-redundant features.\nRecently, Quadratic Programming Feature Selection (QPFS) has been proposed\nwhich formulates the feature selection problem as a quadratic program. It has\nbeen shown to outperform many of the existing feature selection methods for a\nvariety of applications. Though, better than many existing approaches, the\nrunning time complexity of QPFS is cubic in the number of features, which can\nbe quite computationally expensive even for moderately sized datasets. In this\npaper we propose a novel method for feature selection by integrating k-means\nclustering with QPFS. The basic variant of our approach runs k-means to bring\ndown the number of features which need to be passed on to QPFS. We then enhance\nthis idea, wherein we gradually refine the feature space from a very coarse\nclustering to a fine-grained one, by interleaving steps of QPFS with k-means\nclustering. Every step of QPFS helps in identifying the clusters of irrelevant\nfeatures (which can then be thrown away), whereas every step of k-means further\nrefines the clusters which are potentially relevant. We show that our iterative\nrefinement of clusters is guaranteed to converge. We provide bounds on the\nnumber of distance computations involved in the k-means algorithm. Further,\neach QPFS run is now cubic in number of clusters, which can be much smaller\nthan actual number of features. Experiments on eight publicly available\ndatasets show that our approach gives significant computational gains (both in\ntime and memory), over standard QPFS as well as other state of the art feature\nselection methods, even while improving the overall accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 14:45:11 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2015 18:06:36 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Prasad", "Yamuna", ""], ["Biswas", "K. K.", ""]]}, {"id": "1505.01740", "submitter": "Qi Wei", "authors": "Qi Wei and Jose Bioucas-Dias and Nicolas Dobigeon and Jean-Yves\n  Tourneret", "title": "Fast Spectral Unmixing based on Dykstra's Alternating Projection", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fast spectral unmixing algorithm based on Dykstra's\nalternating projection. The proposed algorithm formulates the fully constrained\nleast squares optimization problem associated with the spectral unmixing task\nas an unconstrained regression problem followed by a projection onto the\nintersection of several closed convex sets. This projection is achieved by\niteratively projecting onto each of the convex sets individually, following\nDyktra's scheme. The sequence thus obtained is guaranteed to converge to the\nsought projection. Thanks to the preliminary matrix decomposition and variable\nsubstitution, the projection is implemented intrinsically in a subspace, whose\ndimension is very often much lower than the number of bands. A benefit of this\nstrategy is that the order of the computational complexity for each projection\nis decreased from quadratic to linear time. Numerical experiments considering\ndiverse spectral unmixing scenarios provide evidence that the proposed\nalgorithm competes with the state-of-the-art, namely when the number of\nendmembers is relatively small, a circumstance often observed in real\nhyperspectral applications.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 15:22:33 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Wei", "Qi", ""], ["Bioucas-Dias", "Jose", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1505.01749", "submitter": "Spyros Gidaris", "authors": "Spyros Gidaris, Nikos Komodakis", "title": "Object detection via a multi-region & semantic segmentation-aware CNN\n  model", "comments": "Extended technical report -- short version to appear at ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an object detection system that relies on a multi-region deep\nconvolutional neural network (CNN) that also encodes semantic\nsegmentation-aware features. The resulting CNN-based representation aims at\ncapturing a diverse set of discriminative appearance factors and exhibits\nlocalization sensitivity that is essential for accurate object localization. We\nexploit the above properties of our recognition module by integrating it on an\niterative localization mechanism that alternates between scoring a box proposal\nand refining its location with a deep CNN regression model. Thanks to the\nefficient use of our modules, we detect objects with very high localization\naccuracy. On the detection challenges of PASCAL VOC2007 and PASCAL VOC2012 we\nachieve mAP of 78.2% and 73.9% correspondingly, surpassing any other published\nwork by a significant margin.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 15:42:07 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2015 16:49:44 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2015 22:24:42 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Gidaris", "Spyros", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1505.01809", "submitter": "Jacob Devlin", "authors": "Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong\n  He, Geoffrey Zweig, Margaret Mitchell", "title": "Language Models for Image Captioning: The Quirks and What Works", "comments": "See http://research.microsoft.com/en-us/projects/image_captioning for\n  project information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two recent approaches have achieved state-of-the-art results in image\ncaptioning. The first uses a pipelined process where a set of candidate words\nis generated by a convolutional neural network (CNN) trained on images, and\nthen a maximum entropy (ME) language model is used to arrange these words into\na coherent sentence. The second uses the penultimate activation layer of the\nCNN as input to a recurrent neural network (RNN) that then generates the\ncaption sequence. In this paper, we compare the merits of these different\nlanguage modeling approaches for the first time by using the same\nstate-of-the-art CNN as input. We examine issues in the different approaches,\nincluding linguistic irregularities, caption repetition, and data set overlap.\nBy combining key aspects of the ME and RNN methods, we achieve a new record\nperformance over previously published results on the benchmark COCO dataset.\nHowever, the gains we see in BLEU do not translate to human judgments.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 18:36:14 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2015 22:10:49 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2015 22:03:40 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Devlin", "Jacob", ""], ["Cheng", "Hao", ""], ["Fang", "Hao", ""], ["Gupta", "Saurabh", ""], ["Deng", "Li", ""], ["He", "Xiaodong", ""], ["Zweig", "Geoffrey", ""], ["Mitchell", "Margaret", ""]]}, {"id": "1505.01861", "submitter": "Tao Mei", "authors": "Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, Yong Rui", "title": "Jointly Modeling Embedding and Translation to Bridge Video and Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically describing video content with natural language is a fundamental\nchallenge of multimedia. Recurrent Neural Networks (RNN), which models sequence\ndynamics, has attracted increasing attention on visual interpretation. However,\nmost existing approaches generate a word locally with given previous words and\nthe visual content, while the relationship between sentence semantics and\nvisual content is not holistically exploited. As a result, the generated\nsentences may be contextually correct but the semantics (e.g., subjects, verbs\nor objects) are not true.\n  This paper presents a novel unified framework, named Long Short-Term Memory\nwith visual-semantic Embedding (LSTM-E), which can simultaneously explore the\nlearning of LSTM and visual-semantic embedding. The former aims to locally\nmaximize the probability of generating the next word given previous words and\nvisual content, while the latter is to create a visual-semantic embedding space\nfor enforcing the relationship between the semantics of the entire sentence and\nvisual content. Our proposed LSTM-E consists of three components: a 2-D and/or\n3-D deep convolutional neural networks for learning powerful video\nrepresentation, a deep RNN for generating sentences, and a joint embedding\nmodel for exploring the relationships between visual content and sentence\nsemantics. The experiments on YouTube2Text dataset show that our proposed\nLSTM-E achieves to-date the best reported performance in generating natural\nsentences: 45.3% and 31.0% in terms of BLEU@4 and METEOR, respectively. We also\ndemonstrate that LSTM-E is superior in predicting Subject-Verb-Object (SVO)\ntriplets to several state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 20:13:33 GMT"}, {"version": "v2", "created": "Sat, 30 May 2015 10:05:50 GMT"}, {"version": "v3", "created": "Thu, 4 Jun 2015 07:17:06 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Pan", "Yingwei", ""], ["Mei", "Tao", ""], ["Yao", "Ting", ""], ["Li", "Houqiang", ""], ["Rui", "Yong", ""]]}, {"id": "1505.01936", "submitter": "Avishek Chatterjee", "authors": "Avishek Chatterjee, Venu Madhav Govindu", "title": "Noise in Structured-Light Stereo Depth Cameras: Modeling and its\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth maps obtained from commercially available structured-light stereo based\ndepth cameras, such as the Kinect, are easy to use but are affected by\nsignificant amounts of noise. This paper is devoted to a study of the intrinsic\nnoise characteristics of such depth maps, i.e. the standard deviation of noise\nin estimated depth varies quadratically with the distance of the object from\nthe depth camera. We validate this theoretical model against empirical\nobservations and demonstrate the utility of this noise model in three popular\napplications: depth map denoising, volumetric scan merging for 3D modeling, and\nidentification of 3D planes in depth maps.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 06:15:42 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Chatterjee", "Avishek", ""], ["Govindu", "Venu Madhav", ""]]}, {"id": "1505.01953", "submitter": "Tuomo Valkonen", "authors": "Juan Carlos De Los Reyes, Carola-Bibiane Sch\\\"onlieb, Tuomo Valkonen", "title": "The structure of optimal parameters for image restoration problems", "comments": null, "journal-ref": null, "doi": "10.1016/j.jmaa.2015.09.023", "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the qualitative properties of optimal regularisation parameters in\nvariational models for image restoration. The parameters are solutions of\nbilevel optimisation problems with the image restoration problem as constraint.\nA general type of regulariser is considered, which encompasses total variation\n(TV), total generalized variation (TGV) and infimal-convolution total variation\n(ICTV). We prove that under certain conditions on the given data optimal\nparameters derived by bilevel optimisation problems exist. A crucial point in\nthe existence proof turns out to be the boundedness of the optimal parameters\naway from $0$ which we prove in this paper. The analysis is done on the\noriginal -- in image restoration typically non-smooth variational problem -- as\nwell as on a smoothed approximation set in Hilbert space which is the one\nconsidered in numerical computations. For the smoothed bilevel problem we also\nprove that it $\\Gamma$ converges to the original problem as the smoothing\nvanishes. All analysis is done in function spaces rather than on the\ndiscretised learning problem.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 08:26:24 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Reyes", "Juan Carlos De Los", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""], ["Valkonen", "Tuomo", ""]]}, {"id": "1505.02000", "submitter": "Matthew Lai", "authors": "Matthew Lai", "title": "Deep Learning for Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report provides an overview of the current state of the art deep\nlearning architectures and optimisation techniques, and uses the ADNI\nhippocampus MRI dataset as an example to compare the effectiveness and\nefficiency of different convolutional architectures on the task of patch-based\n3-dimensional hippocampal segmentation, which is important in the diagnosis of\nAlzheimer's Disease. We found that a slightly unconventional \"stacked 2D\"\napproach provides much better classification performance than simple 2D patches\nwithout requiring significantly more computational power. We also examined the\npopular \"tri-planar\" approach used in some recently published studies, and\nfound that it provides much better results than the 2D approaches, but also\nwith a moderate increase in computational power requirement. Finally, we\nevaluated a full 3D convolutional architecture, and found that it provides\nmarginally better results than the tri-planar approach, but at the cost of a\nvery significant increase in computational power requirement.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 11:35:53 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Lai", "Matthew", ""]]}, {"id": "1505.02074", "submitter": "Mengye Ren", "authors": "Mengye Ren, Ryan Kiros, Richard Zemel", "title": "Exploring Models and Data for Image Question Answering", "comments": "12 pages. Conference paper at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to address the problem of image-based question-answering (QA)\nwith new models and datasets. In our work, we propose to use neural networks\nand visual semantic embeddings, without intermediate stages such as object\ndetection and image segmentation, to predict answers to simple questions about\nimages. Our model performs 1.8 times better than the only published results on\nan existing image QA dataset. We also present a question generation algorithm\nthat converts image descriptions, which are widely available, into QA form. We\nused this algorithm to produce an order-of-magnitude larger dataset, with more\nevenly distributed answers. A suite of baseline results on this new dataset are\nalso presented.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 15:59:44 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 19:55:07 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2015 06:44:44 GMT"}, {"version": "v4", "created": "Sun, 29 Nov 2015 22:45:12 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Ren", "Mengye", ""], ["Kiros", "Ryan", ""], ["Zemel", "Richard", ""]]}, {"id": "1505.02108", "submitter": "Ira Kemelmacher-Shlizerman", "authors": "D. Miller, E. Brossard, S. Seitz, I. Kemelmacher-Shlizerman", "title": "MegaFace: A Million Faces for Recognition at Scale", "comments": "Please see http://megaface.cs.washington.edu/ for code and data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent face recognition experiments on the LFW benchmark show that face\nrecognition is performing stunningly well, surpassing human recognition rates.\nIn this paper, we study face recognition at scale. Specifically, we have\ncollected from Flickr a \\textbf{Million} faces and evaluated state of the art\nface recognition algorithms on this dataset. We found that the performance of\nalgorithms varies--while all perform great on LFW, once evaluated at scale\nrecognition rates drop drastically for most algorithms. Interestingly, deep\nlearning based approach by \\cite{schroff2015facenet} performs much better, but\nstill gets less robust at scale. We consider both verification and\nidentification problems, and evaluate how pose affects recognition at scale.\nMoreover, we ran an extensive human study on Mechanical Turk to evaluate human\nrecognition at scale, and report results. All the photos are creative commons\nphotos and is released at \\small{\\url{http://megaface.cs.washington.edu/}} for\nresearch and further experiments.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 17:39:23 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2015 19:45:47 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Miller", "D.", ""], ["Brossard", "E.", ""], ["Seitz", "S.", ""], ["Kemelmacher-Shlizerman", "I.", ""]]}, {"id": "1505.02120", "submitter": "Luca Calatroni", "authors": "Luca Calatroni, Cao Chung, Juan Carlos De Los Reyes, Carola-Bibiane\n  Sch\\\"onlieb, Tuomo Valkonen", "title": "Bilevel approaches for learning of variational imaging models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review some recent learning approaches in variational imaging, based on\nbilevel optimisation, and emphasize the importance of their treatment in\nfunction space. The paper covers both analytical and numerical techniques.\nAnalytically, we include results on the existence and structure of minimisers,\nas well as optimality conditions for their characterisation. Based on this\ninformation, Newton type methods are studied for the solution of the problems\nat hand, combining them with sampling techniques in case of large databases.\nThe computational verification of the developed techniques is extensively\ndocumented, covering instances with different type of regularisers, several\nnoise models, spatially dependent weights and large image databases.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 18:27:34 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Calatroni", "Luca", ""], ["Chung", "Cao", ""], ["Reyes", "Juan Carlos De Los", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""], ["Valkonen", "Tuomo", ""]]}, {"id": "1505.02146", "submitter": "Weicheng Kuo", "authors": "Weicheng Kuo, Bharath Hariharan, Jitendra Malik", "title": "DeepBox: Learning Objectness with Convolutional Networks", "comments": "ICCV 2015 Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing object proposal approaches use primarily bottom-up cues to rank\nproposals, while we believe that objectness is in fact a high level construct.\nWe argue for a data-driven, semantic approach for ranking object proposals. Our\nframework, which we call DeepBox, uses convolutional neural networks (CNNs) to\nrerank proposals from a bottom-up method. We use a novel four-layer CNN\narchitecture that is as good as much larger networks on the task of evaluating\nobjectness while being much faster. We show that DeepBox significantly improves\nover the bottom-up ranking, achieving the same recall with 500 proposals as\nachieved by bottom-up methods with 2000. This improvement generalizes to\ncategories the CNN has never seen before and leads to a 4.5-point gain in\ndetection mAP. Our implementation achieves this performance while running at\n260 ms per image.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 19:24:17 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2015 21:38:49 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Kuo", "Weicheng", ""], ["Hariharan", "Bharath", ""], ["Malik", "Jitendra", ""]]}, {"id": "1505.02206", "submitter": "Dinesh Jayaraman", "authors": "Dinesh Jayaraman and Kristen Grauman", "title": "Learning image representations tied to ego-motion", "comments": "Supplementary material appended at end. In ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how images of objects and scenes behave in response to specific\nego-motions is a crucial aspect of proper visual development, yet existing\nvisual learning methods are conspicuously disconnected from the physical source\nof their images. We propose to exploit proprioceptive motor signals to provide\nunsupervised regularization in convolutional neural networks to learn visual\nrepresentations from egocentric video. Specifically, we enforce that our\nlearned features exhibit equivariance i.e. they respond predictably to\ntransformations associated with distinct ego-motions. With three datasets, we\nshow that our unsupervised feature learning approach significantly outperforms\nprevious approaches on visual recognition and next-best-view prediction tasks.\nIn the most challenging test, we show that features learned from video captured\non an autonomous driving platform improve large-scale scene recognition in\nstatic images from a disjoint domain.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 23:15:00 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 19:30:18 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Jayaraman", "Dinesh", ""], ["Grauman", "Kristen", ""]]}, {"id": "1505.02247", "submitter": "Thomas Holzmann", "authors": "T. Holzmann, R. Prettenthaler, J. Pestana, D. Muschick, G. Graber, C.\n  Mostegel, F. Fraundorfer, H. Bischof", "title": "Performance Evaluation of Vision-Based Algorithms for MAVs", "comments": "Presented at OAGM Workshop, 2015 (arXiv:1505.01065)", "journal-ref": null, "doi": null, "report-no": "OAGM/2015/14", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important focus of current research in the field of Micro Aerial Vehicles\n(MAVs) is to increase the safety of their operation in general unstructured\nenvironments. Especially indoors, where GPS cannot be used for localization,\nreliable algorithms for localization and mapping of the environment are\nnecessary in order to keep an MAV airborne safely. In this paper, we compare\nvision-based real-time capable methods for localization and mapping and point\nout their strengths and weaknesses. Additionally, we describe algorithms for\nstate estimation, control and navigation, which use the localization and\nmapping results of our vision-based algorithms as input.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 09:07:08 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Holzmann", "T.", ""], ["Prettenthaler", "R.", ""], ["Pestana", "J.", ""], ["Muschick", "D.", ""], ["Graber", "G.", ""], ["Mostegel", "C.", ""], ["Fraundorfer", "F.", ""], ["Bischof", "H.", ""]]}, {"id": "1505.02269", "submitter": "Zongyuan Ge", "authors": "Zongyuan Ge and Christopher Mccool and Conrad Sanderson and Peter\n  Corke", "title": "Subset Feature Learning for Fine-Grained Category Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained categorisation has been a challenging problem due to small\ninter-class variation, large intra-class variation and low number of training\nimages. We propose a learning system which first clusters visually similar\nclasses and then learns deep convolutional neural network features specific to\neach subset. Experiments on the popular fine-grained Caltech-UCSD bird dataset\nshow that the proposed method outperforms recent fine-grained categorisation\nmethods under the most difficult setting: no bounding boxes are presented at\ntest time. It achieves a mean accuracy of 77.5%, compared to the previous best\nperformance of 73.2%. We also show that progressive transfer learning allows us\nto first learn domain-generic features (for bird classification) which can then\nbe adapted to specific set of bird classes, yielding improvements in accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 13:25:24 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Ge", "Zongyuan", ""], ["Mccool", "Christopher", ""], ["Sanderson", "Conrad", ""], ["Corke", "Peter", ""]]}, {"id": "1505.02438", "submitter": "Stavros Tsogkas", "authors": "S. Tsogkas, I. Kokkinos, G. Papandreou, A. Vedaldi", "title": "Deep Learning for Semantic Part Segmentation with High-Level Guidance", "comments": "11 pages (including references), 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the task of segmenting an object into its parts, or\nsemantic part segmentation. We start by adapting a state-of-the-art semantic\nsegmentation system to this task, and show that a combination of a\nfully-convolutional Deep CNN system coupled with Dense CRF labelling provides\nexcellent results for a broad range of object categories. Still, this approach\nremains agnostic to high-level constraints between object parts. We introduce\nsuch prior information by means of the Restricted Boltzmann Machine, adapted to\nour task and train our model in an discriminative fashion, as a hidden CRF,\ndemonstrating that prior information can yield additional improvements. We also\ninvestigate the performance of our approach ``in the wild'', without\ninformation concerning the objects' bounding boxes, using an object detector to\nguide a multi-scale segmentation scheme. We evaluate the performance of our\napproach on the Penn-Fudan and LFW datasets for the tasks of pedestrian parsing\nand face labelling respectively. We show superior performance with respect to\ncompetitive methods that have been extensively engineered on these benchmarks,\nas well as realistic qualitative results on part segmentation, even for\noccluded or deformable objects. We also provide quantitative and extensive\nqualitative results on three classes from the PASCAL Parts dataset. Finally, we\nshow that our multi-scale segmentation scheme can boost accuracy, recovering\nsegmentations for finer parts.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 21:12:31 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 14:22:43 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Tsogkas", "S.", ""], ["Kokkinos", "I.", ""], ["Papandreou", "G.", ""], ["Vedaldi", "A.", ""]]}, {"id": "1505.02496", "submitter": "Liwei Wang", "authors": "Liwei Wang, Chen-Yu Lee, Zhuowen Tu, Svetlana Lazebnik", "title": "Training Deeper Convolutional Networks with Deep Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most promising ways of improving the performance of deep\nconvolutional neural networks is by increasing the number of convolutional\nlayers. However, adding layers makes training more difficult and\ncomputationally expensive. In order to train deeper networks, we propose to add\nauxiliary supervision branches after certain intermediate layers during\ntraining. We formulate a simple rule of thumb to determine where these branches\nshould be added. The resulting deeply supervised structure makes the training\nmuch easier and also produces better classification results on ImageNet and the\nrecently released, larger MIT Places dataset\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 06:26:46 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Wang", "Liwei", ""], ["Lee", "Chen-Yu", ""], ["Tu", "Zhuowen", ""], ["Lazebnik", "Svetlana", ""]]}, {"id": "1505.02505", "submitter": "Lihua Guo", "authors": "Guo Lihua and Guo Chenggan", "title": "A Two-Layer Local Constrained Sparse Coding Method for Fine-Grained\n  Visual Categorization", "comments": "19 pages, 12 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Fine-grained categories are more difficulty distinguished than generic\ncategories due to the similarity of inter-class and the diversity of\nintra-class. Therefore, the fine-grained visual categorization (FGVC) is\nconsidered as one of challenge problems in computer vision recently. A new\nfeature learning framework, which is based on a two-layer local constrained\nsparse coding architecture, is proposed in this paper. The two-layer\narchitecture is introduced for learning intermediate-level features, and the\nlocal constrained term is applied to guarantee the local smooth of coding\ncoefficients. For extracting more discriminative information, local orientation\nhistograms are the input of sparse coding instead of raw pixels. Moreover, a\nquick dictionary updating process is derived to further improve the training\nspeed. Two experimental results show that our method achieves 85.29% accuracy\non the Oxford 102 flowers dataset and 67.8% accuracy on the CUB-200-2011 bird\ndataset, and the performance of our framework is highly competitive with\nexisting literatures.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 07:34:35 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Lihua", "Guo", ""], ["Chenggan", "Guo", ""]]}, {"id": "1505.02731", "submitter": "Mauricio Delbracio", "authors": "Mauricio Delbracio and Guillermo Sapiro", "title": "Removing Camera Shake via Weighted Fourier Burst Accumulation", "comments": "Errata with respect to published version: Algorithm 1, lines 9 and\n  10: w_i is replaced by w^p_i (as was correctly stated in Eq (9))", "journal-ref": "Image Processing, IEEE Transactions on, Year: 2015, Volume: 24,\n  Issue: 11, Pages: 3293 - 3307", "doi": "10.1109/TIP.2015.2442914", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous recent approaches attempt to remove image blur due to camera shake,\neither with one or multiple input images, by explicitly solving an inverse and\ninherently ill-posed deconvolution problem. If the photographer takes a burst\nof images, a modality available in virtually all modern digital cameras, we\nshow that it is possible to combine them to get a clean sharp version. This is\ndone without explicitly solving any blur estimation and subsequent inverse\nproblem. The proposed algorithm is strikingly simple: it performs a weighted\naverage in the Fourier domain, with weights depending on the Fourier spectrum\nmagnitude. The method can be seen as a generalization of the align and average\nprocedure, with a weighted average, motivated by hand-shake physiology and\ntheoretically supported, taking place in the Fourier domain. The method's\nrationale is that camera shake has a random nature and therefore each image in\nthe burst is generally blurred differently. Experiments with real camera data,\nand extensive comparisons, show that the proposed Fourier Burst Accumulation\n(FBA) algorithm achieves state-of-the-art results an order of magnitude faster,\nwith simplicity for on-board implementation on camera phones. Finally, we also\npresent experiments in real high dynamic range (HDR) scenes, showing how the\nmethod can be straightforwardly extended to HDR photography.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 19:02:35 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2015 15:10:36 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Delbracio", "Mauricio", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1505.02890", "submitter": "Benjamin Graham", "authors": "Ben Graham", "title": "Sparse 3D convolutional neural networks", "comments": "BMVC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have implemented a convolutional neural network designed for processing\nsparse three-dimensional input data. The world we live in is three dimensional\nso there are a large number of potential applications including 3D object\nrecognition and analysis of space-time objects. In the quest for efficiency, we\nexperiment with CNNs on the 2D triangular-lattice and 3D tetrahedral-lattice.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 07:30:22 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2015 15:12:37 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Graham", "Ben", ""]]}, {"id": "1505.02921", "submitter": "Gianluigi Ciocca", "authors": "Simone Bianco, Gianluigi Ciocca and Raimondo Schettini", "title": "How Far Can You Get By Combining Change Detection Algorithms?", "comments": null, "journal-ref": "Proceedings of the 19th International Conference in Image Analysis\n  and Processing - ICIAP 2017, volume 10484 of Lecture Notes in Computer\n  Science, pp. 96-107, Springer, 2017", "doi": "10.1007/978-3-319-68560-1_9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the existence of many change detection algorithms, each with its own\npeculiarities and strengths, we propose a combination strategy, that we termed\nIUTIS (In Unity There Is Strength), based on a genetic Programming framework.\nThis combination strategy is aimed at leveraging the strengths of the\nalgorithms and compensate for their weakness. In this paper we show our\nfindings in applying the proposed strategy in two different scenarios. The\nfirst scenario is purely performance-based. The second scenario performance and\nefficiency must be balanced. Results demonstrate that starting from simple\nalgorithms we can achieve comparable results with respect to more complex\nstate-of-the-art change detection algorithms, while keeping the computational\ncomplexity affordable for real-time applications.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 09:17:52 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 15:58:03 GMT"}, {"version": "v3", "created": "Wed, 4 Apr 2018 09:05:26 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Bianco", "Simone", ""], ["Ciocca", "Gianluigi", ""], ["Schettini", "Raimondo", ""]]}, {"id": "1505.02982", "submitter": "Baoguang Shi", "authors": "Baoguang Shi, Cong Yao, Chengquan Zhang, Xiaowei Guo, Feiyue Huang,\n  Xiang Bai", "title": "Automatic Script Identification in the Wild", "comments": "5 pages, 7 figures, submitted to ICDAR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid increase of transnational communication and cooperation,\npeople frequently encounter multilingual scenarios in various situations. In\nthis paper, we are concerned with a relatively new problem: script\nidentification at word or line levels in natural scenes. A large-scale dataset\nwith a great quantity of natural images and 10 types of widely used languages\nis constructed and released. In allusion to the challenges in script\nidentification in real-world scenarios, a deep learning based algorithm is\nproposed. The experiments on the proposed dataset demonstrate that our\nalgorithm achieves superior performance, compared with conventional image\nclassification methods, such as the original CNN architecture and LLC.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 12:38:30 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Shi", "Baoguang", ""], ["Yao", "Cong", ""], ["Zhang", "Chengquan", ""], ["Guo", "Xiaowei", ""], ["Huang", "Feiyue", ""], ["Bai", "Xiang", ""]]}, {"id": "1505.03046", "submitter": "Holger Roth", "authors": "Holger R. Roth, Le Lu, Jiamin Liu, Jianhua Yao, Ari Seff, Kevin\n  Cherry, Lauren Kim, Ronald M. Summers", "title": "Improving Computer-aided Detection using Convolutional Neural Networks\n  and Random View Aggregation", "comments": "2D vs 2.5D vs 3D inputs and comparison to other standard classifiers\n  such as SVM have been addressed by more experimentation and two completely\n  new sections and figures. Results and Discussions have been updated\n  accordingly", "journal-ref": "IEEE Transactions on Medical Imaging, 28 September 2015,\n  Volume:PP, Issue: 99", "doi": "10.1109/TMI.2015.2482920", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Automated computer-aided detection (CADe) in medical imaging has been an\nimportant tool in clinical practice and research. State-of-the-art methods\noften show high sensitivities but at the cost of high false-positives (FP) per\npatient rates. We design a two-tiered coarse-to-fine cascade framework that\nfirst operates a candidate generation system at sensitivities of $\\sim$100% but\nat high FP levels. By leveraging existing CAD systems, coordinates of regions\nor volumes of interest (ROI or VOI) for lesion candidates are generated in this\nstep and function as input for a second tier, which is our focus in this study.\nIn this second stage, we generate $N$ 2D (two-dimensional) or 2.5D views via\nsampling through scale transformations, random translations and rotations with\nrespect to each ROI's centroid coordinates. These random views are used to\ntrain deep convolutional neural network (ConvNet) classifiers. In testing, the\ntrained ConvNets are employed to assign class (e.g., lesion, pathology)\nprobabilities for a new set of $N$ random views that are then averaged at each\nROI to compute a final per-candidate classification probability. This second\ntier behaves as a highly selective process to reject difficult false positives\nwhile preserving high sensitivities. The methods are evaluated on three\ndifferent data sets with different numbers of patients: 59 patients for\nsclerotic metastases detection, 176 patients for lymph node detection, and\n1,186 patients for colonic polyp detection. Experimental results show the\nability of ConvNets to generalize well to different medical imaging CADe\napplications and scale elegantly to various data sets. Our proposed methods\nimprove CADe performance markedly in all cases. CADe sensitivities improved\nfrom 57% to 70%, from 43% to 77% and from 58% to 75% at 3 FPs per patient for\nsclerotic metastases, lymph nodes and colonic polyps, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 15:04:37 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 20:08:00 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Roth", "Holger R.", ""], ["Lu", "Le", ""], ["Liu", "Jiamin", ""], ["Yao", "Jianhua", ""], ["Seff", "Ari", ""], ["Cherry", "Kevin", ""], ["Kim", "Lauren", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1505.03093", "submitter": "Jose Luis Alves", "authors": "Manuel Pinheiro and J.L. Alves", "title": "A new Level-set based Protocol for Accurate Bone Segmentation from CT\n  Imaging", "comments": "11 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work it is proposed a medical image segmentation pipeline for\naccurate bone segmentation from CT imaging. It is a two-step methodology, with\na pre-segmentation step and a segmentation refinement step. First, the user\nperforms a rough segmenting of the desired region of interest. Next, a fully\nautomatic refinement step is applied to the pre-segmented data. The automatic\nsegmentation refinement is composed by several sub-stpng, namely image\ndeconvolution, image cropping and interpolation. The user-defined\npre-segmentation is then refined over the deconvolved, cropped, and up-sampled\nversion of the image. The algorithm is applied in the segmentation of CT images\nof a composite femur bone, reconstructed with different reconstruction\nprotocols. Segmentation outcomes are validated against a gold standard model\nobtained with coordinate measuring machine Nikon Metris LK V20 with a digital\nline scanner LC60-D that guarantees an accuracy of 28 $\\mu m$. High sub-pixel\naccuracy models were obtained for all tested Datasets. The algorithm is able to\nproduce high quality segmentation of the composite femur regardless of the\nsurface meshing strategy used.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 17:33:44 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Pinheiro", "Manuel", ""], ["Alves", "J. L.", ""]]}, {"id": "1505.03159", "submitter": "Ziyu Zhang", "authors": "Ziyu Zhang, Alexander G. Schwing, Sanja Fidler, Raquel Urtasun", "title": "Monocular Object Instance Segmentation and Depth Ordering with CNNs", "comments": "International Conference on Computer Vision (ICCV), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we tackle the problem of instance-level segmentation and depth\nordering from a single monocular image. Towards this goal, we take advantage of\nconvolutional neural nets and train them to directly predict instance-level\nsegmentations where the instance ID encodes the depth ordering within image\npatches. To provide a coherent single explanation of an image we develop a\nMarkov random field which takes as input the predictions of convolutional\nneural nets applied at overlapping patches of different resolutions, as well as\nthe output of a connected component algorithm. It aims to predict accurate\ninstance-level segmentation and depth ordering. We demonstrate the\neffectiveness of our approach on the challenging KITTI benchmark and show good\nperformance on both tasks.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 20:16:59 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2015 00:38:37 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Zhang", "Ziyu", ""], ["Schwing", "Alexander G.", ""], ["Fidler", "Sanja", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1505.03205", "submitter": "Kanji Tanaka", "authors": "Tsukamoto Taisho, Tanaka Kanji", "title": "Leveraging Image based Prior for Visual Place Recognition", "comments": "8 pages, 6 figures, preprint. Accepted for publication in MVA2015\n  (oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a novel scene descriptor for visual place\nrecognition. Unlike popular bag-of-words scene descriptors which rely on a\nlibrary of vector quantized visual features, our proposed descriptor is based\non a library of raw image data, such as publicly available photo collections\nfrom Google StreetView and Flickr. The library images need not to be associated\nwith spatial information regarding the viewpoint and orientation of the scene.\nAs a result, these images are cheaper than the database images; in addition,\nthey are readily available. Our proposed descriptor directly mines the image\nlibrary to discover landmarks (i.e., image patches) that suitably match an\ninput query/database image. The discovered landmarks are then compactly\ndescribed by their pose and shape (i.e., library image ID, bounding boxes) and\nused as a compact discriminative scene descriptor for the input image. We\nevaluate the effectiveness of our scene description framework by comparing its\nperformance to that of previous approaches.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 00:36:38 GMT"}, {"version": "v2", "created": "Thu, 14 May 2015 04:12:09 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Taisho", "Tsukamoto", ""], ["Kanji", "Tanaka", ""]]}, {"id": "1505.03227", "submitter": "Keze Wang", "authors": "Keze Wang and Liang Lin and Jiangbo Lu and Chenglong Li and Keyang Shi", "title": "PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance\n  Contrast Measures with Edge-Preserving Coherence", "comments": "14 pages, 14 figures, 1 table, to appear in IEEE Transactions on\n  Image Processing", "journal-ref": "IEEE Transactions on Image Processing (TIP), volume. 24, Issue.\n  10, page. 3019 - 3033, Oct. 2015", "doi": "10.1109/TIP.2015.2432712", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by recent vision and graphics applications such as image segmentation\nand object recognition, computing pixel-accurate saliency values to uniformly\nhighlight foreground objects becomes increasingly important. In this paper, we\npropose a unified framework called PISA, which stands for Pixelwise Image\nSaliency Aggregating various bottom-up cues and priors. It generates spatially\ncoherent yet detail-preserving, pixel-accurate and fine-grained saliency, and\novercomes the limitations of previous methods which use homogeneous\nsuperpixel-based and color only treatment. PISA aggregates multiple saliency\ncues in a global context such as complementary color and structure contrast\nmeasures with their spatial priors in the image domain. The saliency confidence\nis further jointly modeled with a neighborhood consistence constraint into an\nenergy minimization formulation, in which each pixel will be evaluated with\nmultiple hypothetical saliency levels. Instead of using global discrete\noptimization methods, we employ the cost-volume filtering technique to solve\nour formulation, assigning the saliency levels smoothly while preserving the\nedge-aware structure details. In addition, a faster version of PISA is\ndeveloped using a gradient-driven image sub-sampling strategy to greatly\nimprove the runtime efficiency while keeping comparable detection accuracy.\nExtensive experiments on a number of public datasets suggest that PISA\nconvincingly outperforms other state-of-the-art approaches. In addition, with\nthis work we also create a new dataset containing $800$ commodity images for\nevaluating saliency detection. The dataset and source code of PISA can be\ndownloaded at http://vision.sysu.edu.cn/project/PISA/\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 03:05:46 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Wang", "Keze", ""], ["Lin", "Liang", ""], ["Lu", "Jiangbo", ""], ["Li", "Chenglong", ""], ["Shi", "Keyang", ""]]}, {"id": "1505.03229", "submitter": "Ikuro Sato", "authors": "Ikuro Sato, Hiroki Nishimura, Kensuke Yokoi", "title": "APAC: Augmented PAttern Classification with Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been exhibiting splendid accuracies in many of\nvisual pattern classification problems. Many of the state-of-the-art methods\nemploy a technique known as data augmentation at the training stage. This paper\naddresses an issue of decision rule for classifiers trained with augmented\ndata. Our method is named as APAC: the Augmented PAttern Classification, which\nis a way of classification using the optimal decision rule for augmented data\nlearning. Discussion of methods of data augmentation is not our primary focus.\nWe show clear evidences that APAC gives far better generalization performance\nthan the traditional way of class prediction in several experiments. Our\nconvolutional neural network model with APAC achieved a state-of-the-art\naccuracy on the MNIST dataset among non-ensemble classifiers. Even our\nmultilayer perceptron model beats some of the convolutional models with\nrecently invented stochastic regularization techniques on the CIFAR-10 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 03:33:29 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Sato", "Ikuro", ""], ["Nishimura", "Hiroki", ""], ["Yokoi", "Kensuke", ""]]}, {"id": "1505.03344", "submitter": "Anjith George", "authors": "Anjith George, Anirban Dasgupta, Aurobinda Routray", "title": "A Framework for Fast Face and Eye Detection", "comments": "5 pages , 10 figures,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection is an essential step in many computer vision applications like\nsurveillance, tracking, medical analysis, facial expression analysis etc.\nSeveral approaches have been made in the direction of face detection. Among\nthem, Haar-like features based method is a robust method. In spite of the\nrobustness, Haar - like features work with some limitations. However, with some\nsimple modifications in the algorithm, its performance can be made faster and\nmore robust. The present work refers to the increase in speed of operation of\nthe original algorithm by down sampling the frames and its analysis with\ndifferent scale factors. It also discusses the detection of tilted faces using\nan affine transformation of the input image.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 11:56:01 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["George", "Anjith", ""], ["Dasgupta", "Anirban", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1505.03352", "submitter": "Anjith George", "authors": "Anirban Dasgupta, Anjith George, S. L. Happy, Aurobinda Routray", "title": "A Vision Based System for Monitoring the Loss of Attention in Automotive\n  Drivers", "comments": "14 pages, 24 figures Journal article", "journal-ref": "Intelligent Transportation Systems, IEEE Transactions on 14, no. 4\n  (2013): 1825-1838", "doi": "10.1109/TITS.2013.2271052", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On board monitoring of the alertness level of an automotive driver has been a\nchallenging research in transportation safety and management. In this paper, we\npropose a robust real time embedded platform to monitor the loss of attention\nof the driver during day as well as night driving conditions. The PERcentage of\neye CLOSure (PERCLOS) has been used as the indicator of the alertness level. In\nthis approach, the face is detected using Haar like features and tracked using\na Kalman Filter. The Eyes are detected using Principal Component Analysis (PCA)\nduring day time and the block Local Binary Pattern (LBP) features during night.\nFinally the eye state is classified as open or closed using Support Vector\nMachines(SVM). In plane and off plane rotations of the drivers face have been\ncompensated using Affine and Perspective Transformation respectively.\nCompensation in illumination variation is carried out using Bi Histogram\nEqualization (BHE). The algorithm has been cross validated using brain signals\nand finally been implemented on a Single Board Computer (SBC) having Intel Atom\nprocessor, 1 GB RAM, 1.66 GHz clock, x86 architecture, Windows Embedded XP\noperating system. The system is found to be robust under actual driving\nconditions.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 12:21:26 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Dasgupta", "Anirban", ""], ["George", "Anjith", ""], ["Happy", "S. L.", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1505.03358", "submitter": "Miriam Redi", "authors": "Rossano Schifanella, Miriam Redi, Luca Aiello", "title": "An Image is Worth More than a Thousand Favorites: Surfacing the Hidden\n  Beauty of Flickr Pictures", "comments": "ICWSM 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamics of attention in social media tend to obey power laws. Attention\nconcentrates on a relatively small number of popular items and neglecting the\nvast majority of content produced by the crowd. Although popularity can be an\nindication of the perceived value of an item within its community, previous\nresearch has hinted to the fact that popularity is distinct from intrinsic\nquality. As a result, content with low visibility but high quality lurks in the\ntail of the popularity distribution. This phenomenon can be particularly\nevident in the case of photo-sharing communities, where valuable photographers\nwho are not highly engaged in online social interactions contribute with\nhigh-quality pictures that remain unseen. We propose to use a computer vision\nmethod to surface beautiful pictures from the immense pool of\nnear-zero-popularity items, and we test it on a large dataset of\ncreative-commons photos on Flickr. By gathering a large crowdsourced ground\ntruth of aesthetics scores for Flickr images, we show that our method retrieves\nphotos whose median perceived beauty score is equal to the most popular ones,\nand whose average is lower by only 1.5%.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 12:40:24 GMT"}, {"version": "v2", "created": "Fri, 15 May 2015 10:05:34 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Schifanella", "Rossano", ""], ["Redi", "Miriam", ""], ["Aiello", "Luca", ""]]}, {"id": "1505.03365", "submitter": "Wonsik Kim", "authors": "Wonsik Kim and Kyoung Mu Lee", "title": "MRF Optimization by Graph Approximation", "comments": "CVPR2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph cuts-based algorithms have achieved great success in energy\nminimization for many computer vision applications. These algorithms provide\napproximated solutions for multi-label energy functions via move-making\napproach. This approach fuses the current solution with a proposal to generate\na lower-energy solution. Thus, generating the appropriate proposals is\nnecessary for the success of the move-making approach. However, not much\nresearch efforts has been done on the generation of \"good\" proposals,\nespecially for non-metric energy functions. In this paper, we propose an\napplication-independent and energy-based approach to generate \"good\" proposals.\nWith these proposals, we present a graph cuts-based move-making algorithm\ncalled GA-fusion (fusion with graph approximation-based proposals). Extensive\nexperiments support that our proposal generation is effective across different\nclasses of energy functions. The proposed algorithm outperforms others both on\nreal and synthetic problems.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 13:05:05 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Kim", "Wonsik", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1505.03489", "submitter": "Ajay Boyat Kumar", "authors": "Ajay Kumar Boyat (1) and Brijendra Kumar Joshi (2) ((1)Research\n  Scholar, Department of Electronics Telecomm and Computer Engineering,\n  Military College of Tele Communication Engineering, Military Head Quartar of\n  War (MHOW), Ministry of Defence, Govt. of India, India (2) Professor,\n  Department of Electronics Telecomm and Computer Engineering, Military College\n  of Tele Communication Engineering, Military Head Quartar of War (MHOW),\n  Ministry of Defence, Govt. of India, India)", "title": "A Review Paper: Noise Models in Digital Image Processing", "comments": null, "journal-ref": "Signal & Image Processing : An International Journal (SIPIJ)\n  Vol.6, No.2, April 2015", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noise is always presents in digital images during image acquisition, coding,\ntransmission, and processing steps. Noise is very difficult to remove it from\nthe digital images without the prior knowledge of noise model. That is why,\nreview of noise models are essential in the study of image denoising\ntechniques. In this paper, we express a brief overview of various noise models.\nThese noise models can be selected by analysis of their origin. In this way, we\npresent a complete and quantitative analysis of noise models available in\ndigital images.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 18:37:01 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Boyat", "Ajay Kumar", ""], ["Joshi", "Brijendra Kumar", ""]]}, {"id": "1505.03493", "submitter": "Reza Farrahi Moghaddam", "authors": "Reza Farrahi Moghaddam and Mohamed Cheriet", "title": "Modified Hausdorff Fractal Dimension (MHFD)", "comments": "15 pages, 4 figures, 2 algorithms. Working Paper WP-RFM-15-02,\n  (version: 150507)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hausdorff fractal dimension has been a fast-to-calculate method to\nestimate complexity of fractal shapes. In this work, a modified version of this\nfractal dimension is presented in order to make it more robust when applied in\nestimating complexity of non-fractal images. The modified Hausdorff fractal\ndimension stands on two features that weaken the requirement of presence of a\nshape and also reduce the impact of the noise possibly presented in the input\nimage. The new algorithm has been evaluated on a set of images of different\ncharacter with promising performance.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 18:45:07 GMT"}, {"version": "v2", "created": "Thu, 14 May 2015 13:08:15 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Moghaddam", "Reza Farrahi", ""], ["Cheriet", "Mohamed", ""]]}, {"id": "1505.03504", "submitter": "Hai-Jun Zhou", "authors": "Hai-Jun Zhou, Wei-Mou Zheng", "title": "Loop-corrected belief propagation for lattice spin models", "comments": "11 pages, minor changes with new references added. Final version as\n  published in EPJB", "journal-ref": "European Physical Journal B 88: 336 (2015)", "doi": "10.1140/epjb/e2015-60485-6", "report-no": null, "categories": "cond-mat.stat-mech cond-mat.dis-nn cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief propagation (BP) is a message-passing method for solving probabilistic\ngraphical models. It is very successful in treating disordered models (such as\nspin glasses) on random graphs. On the other hand, finite-dimensional lattice\nmodels have an abundant number of short loops, and the BP method is still far\nfrom being satisfactory in treating the complicated loop-induced correlations\nin these systems. Here we propose a loop-corrected BP method to take into\naccount the effect of short loops in lattice spin models. We demonstrate,\nthrough an application to the square-lattice Ising model, that loop-corrected\nBP improves over the naive BP method significantly. We also implement\nloop-corrected BP at the coarse-grained region graph level to further boost its\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 19:36:35 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 03:58:15 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2016 07:55:59 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Zhou", "Hai-Jun", ""], ["Zheng", "Wei-Mou", ""]]}, {"id": "1505.03505", "submitter": "Aniello Raffaele Patrone", "authors": "Aniello Raffale Patrone and Otmar Scherzer", "title": "On a spatial-temporal decomposition of the optical flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a decomposition algorithm for computation of the\nspatial-temporal optical flow of a dynamic image sequence. We consider several\napplications, such as the extraction of temporal motion features and motion\ndetection in dynamic sequences under varying illumination conditions, such as\nthey appear for instance in psychological flickering experiments. For the\nnumerical implementation we are solving an integro-differential equation by a\nfixed point iteration. For comparison purposes we use a standard time dependent\noptical flow algorithm, which in contrast to our method, constitutes in solving\na spatial-temporal differential equation.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 19:38:57 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 13:27:30 GMT"}, {"version": "v3", "created": "Sat, 28 Jan 2017 20:57:56 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Patrone", "Aniello Raffale", ""], ["Scherzer", "Otmar", ""]]}, {"id": "1505.03540", "submitter": "Mohammad Havaei", "authors": "Mohammad Havaei, Axel Davy, David Warde-Farley, Antoine Biard, Aaron\n  Courville, Yoshua Bengio, Chris Pal, Pierre-Marc Jodoin, Hugo Larochelle", "title": "Brain Tumor Segmentation with Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.media.2016.05.004", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a fully automatic brain tumor segmentation method\nbased on Deep Neural Networks (DNNs). The proposed networks are tailored to\nglioblastomas (both low and high grade) pictured in MR images. By their very\nnature, these tumors can appear anywhere in the brain and have almost any kind\nof shape, size, and contrast. These reasons motivate our exploration of a\nmachine learning solution that exploits a flexible, high capacity DNN while\nbeing extremely efficient. Here, we give a description of different model\nchoices that we've found to be necessary for obtaining competitive performance.\nWe explore in particular different architectures based on Convolutional Neural\nNetworks (CNN), i.e. DNNs specifically adapted to image data.\n  We present a novel CNN architecture which differs from those traditionally\nused in computer vision. Our CNN exploits both local features as well as more\nglobal contextual features simultaneously. Also, different from most\ntraditional uses of CNNs, our networks use a final layer that is a\nconvolutional implementation of a fully connected layer which allows a 40 fold\nspeed up. We also describe a 2-phase training procedure that allows us to\ntackle difficulties related to the imbalance of tumor labels. Finally, we\nexplore a cascade architecture in which the output of a basic CNN is treated as\nan additional source of information for a subsequent CNN. Results reported on\nthe 2013 BRATS test dataset reveal that our architecture improves over the\ncurrently published state-of-the-art while being over 30 times faster.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 20:06:21 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2015 17:37:02 GMT"}, {"version": "v3", "created": "Fri, 20 May 2016 06:30:23 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Havaei", "Mohammad", ""], ["Davy", "Axel", ""], ["Warde-Farley", "David", ""], ["Biard", "Antoine", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""], ["Pal", "Chris", ""], ["Jodoin", "Pierre-Marc", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1505.03566", "submitter": "Moein Shakeri", "authors": "Moein Shakeri, Hong Zhang", "title": "COROLA: A Sequential Solution to Moving Object Detection Using Low-rank\n  Approximation", "comments": "37 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting moving objects from a video sequence and estimating the background\nof each individual image are fundamental issues in many practical applications\nsuch as visual surveillance, intelligent vehicle navigation, and traffic\nmonitoring. Recently, some methods have been proposed to detect moving objects\nin a video via low-rank approximation and sparse outliers where the background\nis modeled with the computed low-rank component of the video and the foreground\nobjects are detected as the sparse outliers in the low-rank approximation. All\nof these existing methods work in a batch manner, preventing them from being\napplied in real time and long duration tasks. In this paper, we present an\nonline sequential framework, namely contiguous outliers representation via\nonline low-rank approximation (COROLA), to detect moving objects and learn the\nbackground model at the same time. We also show that our model can detect\nmoving objects with a moving camera. Our experimental evaluation uses simulated\ndata and real public datasets and demonstrates the superior performance of\nCOROLA in terms of both accuracy and execution time.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 22:13:20 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 21:10:35 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Shakeri", "Moein", ""], ["Zhang", "Hong", ""]]}, {"id": "1505.03578", "submitter": "Ali Borji", "authors": "Ali Borji, Mengyang Feng, Huchuan Lu", "title": "Vanishing Point Attracts Eye Movements in Scene Free-viewing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye movements are crucial in understanding complex scenes. By predicting\nwhere humans look in natural scenes, we can understand how they percieve scenes\nand priotriaze information for further high-level processing. Here, we study\nthe effect of a particular type of scene structural information known as\nvanishing point and show that human gaze is attracted to vanishing point\nregions. We then build a combined model of traditional saliency and vanishing\npoint channel that outperforms state of the art saliency models.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 00:22:35 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Borji", "Ali", ""], ["Feng", "Mengyang", ""], ["Lu", "Huchuan", ""]]}, {"id": "1505.03581", "submitter": "Ali Borji", "authors": "Ali Borji, Laurent Itti", "title": "CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency modeling has been an active research area in computer vision for\nabout two decades. Existing state of the art models perform very well in\npredicting where people look in natural scenes. There is, however, the risk\nthat these models may have been overfitting themselves to available small scale\nbiased datasets, thus trapping the progress in a local minimum. To gain a\ndeeper insight regarding current issues in saliency modeling and to better\ngauge progress, we recorded eye movements of 120 observers while they freely\nviewed a large number of naturalistic and artificial images. Our stimuli\nincludes 4000 images; 200 from each of 20 categories covering different types\nof scenes such as Cartoons, Art, Objects, Low resolution images, Indoor,\nOutdoor, Jumbled, Random, and Line drawings. We analyze some basic properties\nof this dataset and compare some successful models. We believe that our dataset\nopens new challenges for the next generation of saliency models and helps\nconduct behavioral studies on bottom-up visual attention.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 00:34:43 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Borji", "Ali", ""], ["Itti", "Laurent", ""]]}, {"id": "1505.03597", "submitter": "Eshed Ohn-Bar", "authors": "Eshed Ohn-Bar and M. M. Trivedi", "title": "Multi-scale Volumes for Deep Object Detection and Localization", "comments": "To appear in Pattern Recognition 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study aims to analyze the benefits of improved multi-scale reasoning for\nobject detection and localization with deep convolutional neural networks. To\nthat end, an efficient and general object detection framework which operates on\nscale volumes of a deep feature pyramid is proposed. In contrast to the\nproposed approach, most current state-of-the-art object detectors operate on a\nsingle-scale in training, while testing involves independent evaluation across\nscales. One benefit of the proposed approach is in better capturing of\nmulti-scale contextual information, resulting in significant gains in both\ndetection performance and localization quality of objects on the PASCAL VOC\ndataset and a multi-view highway vehicles dataset. The joint detection and\nlocalization scale-specific models are shown to especially benefit detection of\nchallenging object categories which exhibit large scale variation as well as\ndetection of small objects.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 02:07:10 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 21:15:12 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Ohn-Bar", "Eshed", ""], ["Trivedi", "M. M.", ""]]}, {"id": "1505.03703", "submitter": "Yanhai Gan", "authors": "Yanhai Gan, Jun Liu, Junyu Dong, Guoqiang Zhong", "title": "A PCA-Based Convolutional Network", "comments": "8 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel unsupervised deep learning model, called\nPCA-based Convolutional Network (PCN). The architecture of PCN is composed of\nseveral feature extraction stages and a nonlinear output stage. Particularly,\neach feature extraction stage includes two layers: a convolutional layer and a\nfeature pooling layer. In the convolutional layer, the filter banks are simply\nlearned by PCA. In the nonlinear output stage, binary hashing is applied. For\nthe higher convolutional layers, the filter banks are learned from the feature\nmaps that were obtained in the previous stage. To test PCN, we conducted\nextensive experiments on some challenging tasks, including handwritten digits\nrecognition, face recognition and texture classification. The results show that\nPCN performs competitive with or even better than state-of-the-art deep\nlearning models. More importantly, since there is no back propagation for\nsupervised finetuning, PCN is much more efficient than existing deep networks.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 12:35:19 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Gan", "Yanhai", ""], ["Liu", "Jun", ""], ["Dong", "Junyu", ""], ["Zhong", "Guoqiang", ""]]}, {"id": "1505.03795", "submitter": "Houssam Abdul-Rahman", "authors": "Houssam Abdul-Rahman and Nikolai Chernov", "title": "Fast and numerically stable circle fit", "comments": "16 pages", "journal-ref": "Journal of Mathematical Imaging and Vision June 2014, Volume 49,\n  Issue 2, pp 289-295", "doi": "10.1007/s10851-013-0461-4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new algorithm for fitting circles that does not have drawbacks\ncommonly found in existing circle fits. Our fit achieves ultimate accuracy (to\nmachine precision), avoids divergence, and is numerically stable even when\nfitting circles get arbitrary large. Lastly, our algorithm takes less than 10\niterations to converge, on average.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 16:43:07 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Abdul-Rahman", "Houssam", ""], ["Chernov", "Nikolai", ""]]}, {"id": "1505.03825", "submitter": "Suha Kwak", "authors": "Suha Kwak, Minsu Cho, Ivan Laptev, Jean Ponce, Cordelia Schmid", "title": "Unsupervised Object Discovery and Tracking in Video Collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of automatically localizing dominant objects\nas spatio-temporal tubes in a noisy collection of videos with minimal or even\nno supervision. We formulate the problem as a combination of two complementary\nprocesses: discovery and tracking. The first one establishes correspondences\nbetween prominent regions across videos, and the second one associates\nsuccessive similar object regions within the same video. Interestingly, our\nalgorithm also discovers the implicit topology of frames associated with\ninstances of the same object class across different videos, a role normally\nleft to supervisory information in the form of class labels in conventional\nimage and video understanding methods. Indeed, as demonstrated by our\nexperiments, our method can handle video collections featuring multiple object\nclasses, and substantially outperforms the state of the art in colocalization,\neven though it tackles a broader problem with much less supervision.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 18:23:24 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Kwak", "Suha", ""], ["Cho", "Minsu", ""], ["Laptev", "Ivan", ""], ["Ponce", "Jean", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1505.03832", "submitter": "Yi Hong", "authors": "Yi Hong, Nikhil Singh, Roland Kwitt, Nuno Vasconcelos and Marc\n  Niethammer", "title": "Parametric Regression on the Grassmannian", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of fitting parametric curves on the Grassmann manifold\nfor the purpose of intrinsic parametric regression. As customary in the\nliterature, we start from the energy minimization formulation of linear\nleast-squares in Euclidean spaces and generalize this concept to general\nnonflat Riemannian manifolds, following an optimal-control point of view. We\nthen specialize this idea to the Grassmann manifold and demonstrate that it\nyields a simple, extensible and easy-to-implement solution to the parametric\nregression problem. In fact, it allows us to extend the basic geodesic model to\n(1) a time-warped variant and (2) cubic splines. We demonstrate the utility of\nthe proposed solution on different vision problems, such as shape regression as\na function of age, traffic-speed estimation and crowd-counting from\nsurveillance video clips. Most notably, these problems can be conveniently\nsolved within the same framework without any specifically-tailored steps along\nthe processing pipeline.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 18:48:08 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Hong", "Yi", ""], ["Singh", "Nikhil", ""], ["Kwitt", "Roland", ""], ["Vasconcelos", "Nuno", ""], ["Niethammer", "Marc", ""]]}, {"id": "1505.03840", "submitter": "Afonso S. Bandeira", "authors": "Afonso S. Bandeira and Yutong Chen and Amit Singer", "title": "Non-unique games over compact groups and orientation estimation in\n  cryo-EM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathcal{G}$ be a compact group and let $f_{ij} \\in L^2(\\mathcal{G})$.\nWe define the Non-Unique Games (NUG) problem as finding $g_1,\\dots,g_n \\in\n\\mathcal{G}$ to minimize $\\sum_{i,j=1}^n f_{ij} \\left( g_i g_j^{-1}\\right)$. We\ndevise a relaxation of the NUG problem to a semidefinite program (SDP) by\ntaking the Fourier transform of $f_{ij}$ over $\\mathcal{G}$, which can then be\nsolved efficiently. The NUG framework can be seen as a generalization of the\nlittle Grothendieck problem over the orthogonal group and the Unique Games\nproblem and includes many practically relevant problems, such as the maximum\nlikelihood estimator} to registering bandlimited functions over the unit sphere\nin $d$-dimensions and orientation estimation in cryo-Electron Microscopy.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 19:11:27 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Bandeira", "Afonso S.", ""], ["Chen", "Yutong", ""], ["Singer", "Amit", ""]]}, {"id": "1505.03873", "submitter": "Kevin Tang", "authors": "Kevin Tang and Manohar Paluri and Li Fei-Fei and Rob Fergus and\n  Lubomir Bourdev", "title": "Improving Image Classification with Location Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the widespread availability of cellphones and cameras that have GPS\ncapabilities, it is common for images being uploaded to the Internet today to\nhave GPS coordinates associated with them. In addition to research that tries\nto predict GPS coordinates from visual features, this also opens up the door to\nproblems that are conditioned on the availability of GPS coordinates. In this\nwork, we tackle the problem of performing image classification with location\ncontext, in which we are given the GPS coordinates for images in both the train\nand test phases. We explore different ways of encoding and extracting features\nfrom the GPS coordinates, and show how to naturally incorporate these features\ninto a Convolutional Neural Network (CNN), the current state-of-the-art for\nmost image classification and recognition problems. We also show how it is\npossible to simultaneously learn the optimal pooling radii for a subset of our\nfeatures within the CNN framework. To evaluate our model and to help promote\nresearch in this area, we identify a set of location-sensitive concepts and\nannotate a subset of the Yahoo Flickr Creative Commons 100M dataset that has\nGPS coordinates with these concepts, which we make publicly available. By\nleveraging location context, we are able to achieve almost a 7% gain in mean\naverage precision.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 20:13:34 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Tang", "Kevin", ""], ["Paluri", "Manohar", ""], ["Fei-Fei", "Li", ""], ["Fergus", "Rob", ""], ["Bourdev", "Lubomir", ""]]}, {"id": "1505.03891", "submitter": "Adrian Sanchez", "authors": "Adrian A. Sanchez", "title": "Task-Based Optimization of Computed Tomography Imaging Systems", "comments": "Doctoral Dissertation - University of Chicago", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this thesis is to provide a framework for the use of task-based\nmetrics of image quality to aid in the design, implementation, and evaluation\nof CT image reconstruction algorithms and CT systems in general. We support the\nview that task-based metrics of image quality can be useful in guiding the\nalgorithm design and implementation process in order to yield images of\nobjectively superior quality and higher utility for a given task. Further, we\nbelieve that metrics such as the Hotelling observer (HO) SNR can be used as\nsummary scalar metrics of image quality for the evaluation of images produced\nby novel reconstruction algorithms. In this work, we aim to construct a concise\nand versatile formalism for image reconstruction algorithm design,\nimplementation, and assessment. The bulk of the work focuses on linear\nanalytical algorithms, specifically the ubiquitous filtered back-projection\n(FBP) algorithm. However, due to the demonstrated importance of\noptimization-based algorithms in a wide variety of CT applications, we devote\none chapter to the characterization of noise properties in TV-based iterative\nreconstruction, as the understanding of image statistics in optimization-based\nreconstruction is the limiting factor in applying HO metrics.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 21:22:00 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Sanchez", "Adrian A.", ""]]}, {"id": "1505.03932", "submitter": "Giancarlo Crocetti", "authors": "Giancarlo Crocetti, Michael Coakley, Phil Dressner, Wanda Kellum,\n  Tamba Lamin", "title": "Using Ensemble Models in the Histological Examination of Tissue\n  Abnormalities", "comments": "4 pages, 4 tables, 3 figures. Proceedings of 12th Annual Research\n  Day, 2014 - Pace University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification models for the automatic detection of abnormalities on\nhistological samples do exists, with an active debate on the cost associated\nwith false negative diagnosis (underdiagnosis) and false positive diagnosis\n(overdiagnosis). Current models tend to underdiagnose, failing to recognize a\npotentially fatal disease.\n  The objective of this study is to investigate the possibility of\nautomatically identifying abnormalities in tissue samples through the use of an\nensemble model on data generated by histological examination and to minimize\nthe number of false negative cases.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 00:59:48 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Crocetti", "Giancarlo", ""], ["Coakley", "Michael", ""], ["Dressner", "Phil", ""], ["Kellum", "Wanda", ""], ["Lamin", "Tamba", ""]]}, {"id": "1505.04026", "submitter": "S L Happy", "authors": "S L Happy and Aurobinda Routray", "title": "Automatic Facial Expression Recognition Using Features of Salient Facial\n  Patches", "comments": null, "journal-ref": "IEEE Transactions on Affective Computing, vol. 6, no. 1, pp. 1-12,\n  2015", "doi": "10.1109/TAFFC.2014.2386334", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extraction of discriminative features from salient facial patches plays a\nvital role in effective facial expression recognition. The accurate detection\nof facial landmarks improves the localization of the salient patches on face\nimages. This paper proposes a novel framework for expression recognition by\nusing appearance features of selected facial patches. A few prominent facial\npatches, depending on the position of facial landmarks, are extracted which are\nactive during emotion elicitation. These active patches are further processed\nto obtain the salient patches which contain discriminative features for\nclassification of each pair of expressions, thereby selecting different facial\npatches as salient for different pair of expression classes. One-against-one\nclassification method is adopted using these features. In addition, an\nautomated learning-free facial landmark detection technique has been proposed,\nwhich achieves similar performances as that of other state-of-art landmark\ndetection methods, yet requires significantly less execution time. The proposed\nmethod is found to perform well consistently in different resolutions, hence,\nproviding a solution for expression recognition in low resolution images.\nExperiments on CK+ and JAFFE facial expression databases show the effectiveness\nof the proposed system.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 11:07:39 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Happy", "S L", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1505.04028", "submitter": "Umut Uludag", "authors": "Mehmet Kayaoglu, Berkay Topcu, Umut Uludag", "title": "Biometric Matching and Fusion System for Fingerprints from Non-Distal\n  Phalanges", "comments": "22 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Market research indicates that fingerprints are still the most popular\nbiometric modality for personal authentication. Even with the onset of new\nmodalities (e.g. vein matching), many applications within different domains\n(e-ID, banking, border control...) and geographies rely on fingerprints\nobtained from the distal phalanges (a.k.a. sections, digits) of the human hand\nstructure. Motivated by the problem of poor quality distal fingerprint images\naffecting a non-trivial portion of the population (which decreases associated\nauthentication accuracy), we designed and tested a multifinger, multiphalanx\nfusion scheme, that combines minutiae matching scores originating from\nnon-distal (ie. middle and proximal) phalanges based on (i) simple sum fusion,\n(ii) NFIQ image-quality-based fusion, and (iii) phalanx-type-based fusion.\nUtilizing a medium-size (50 individuals, 400 unique fingers, 1600 distinct\nimages) database collected in our laboratory with a commercial optical\nfingerprint sensor, and a commercial minutiae extractor & matcher (without any\nmodification), allowed us to simulate a real-world fingerprint authentication\nsetting. Detailed analyses including ROC curves with statistical confidence\nintervals show that the proposed system can be a viable alternative for cases\nwhere (i) distal phalanx images are not usable (e.g. due to missing digits, or\nlow quality finger surface due to manual labor), and (ii) switching to a new\nbiometric modality (e.g. iris) is not possible due to economical or\ninfrastructure limits. Further, we show that when distal phalanx images are in\nfact usable, combining them with images from other phalanges increases accuracy\nas well.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 11:09:58 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 08:10:57 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Kayaoglu", "Mehmet", ""], ["Topcu", "Berkay", ""], ["Uludag", "Umut", ""]]}, {"id": "1505.04030", "submitter": "S L Happy", "authors": "S. L. Happy and Aurobinda Routray", "title": "Robust Facial Expression Classification Using Shape and Appearance\n  Features", "comments": "Proceedings of 8th International Conference of Advances in Pattern\n  Recognition, 2015", "journal-ref": null, "doi": "10.1109/ICAPR.2015.7050661", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression recognition has many potential applications which has\nattracted the attention of researchers in the last decade. Feature extraction\nis one important step in expression analysis which contributes toward fast and\naccurate expression recognition. This paper represents an approach of combining\nthe shape and appearance features to form a hybrid feature vector. We have\nextracted Pyramid of Histogram of Gradients (PHOG) as shape descriptors and\nLocal Binary Patterns (LBP) as appearance features. The proposed framework\ninvolves a novel approach of extracting hybrid features from active facial\npatches. The active facial patches are located on the face regions which\nundergo a major change during different expressions. After detection of facial\nlandmarks, the active patches are localized and hybrid features are calculated\nfrom these patches. The use of small parts of face instead of the whole face\nfor extracting features reduces the computational cost and prevents the\nover-fitting of the features for classification. By using linear discriminant\nanalysis, the dimensionality of the feature is reduced which is further\nclassified by using the support vector machine (SVM). The experimental results\non two publicly available databases show promising accuracy in recognizing all\nexpression classes.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 11:15:18 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Happy", "S. L.", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1505.04055", "submitter": "S L Happy", "authors": "S L Happy, Anirban Dasgupta, Anjith George, and Aurobinda Routray", "title": "A Video Database of Human Faces under Near Infra-Red Illumination for\n  Human Computer Interaction Aplications", "comments": null, "journal-ref": "IEEE Proceedings of 4th International Conference on Intelligent\n  Human Computer Interaction, 2012", "doi": "10.1109/IHCI.2012.6481868", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Computer Interaction (HCI) is an evolving area of research for coherent\ncommunication between computers and human beings. Some of the important\napplications of HCI as reported in literature are face detection, face pose\nestimation, face tracking and eye gaze estimation. Development of algorithms\nfor these applications is an active field of research. However, availability of\nstandard database to validate such algorithms is insufficient. This paper\ndiscusses the creation of such a database created under Near Infra-Red (NIR)\nillumination. NIR illumination has gained its popularity for night mode\napplications since prolonged exposure to Infra-Red (IR) lighting may lead to\nmany health issues. The database contains NIR videos of 60 subjects in\ndifferent head orientations and with different facial expressions, facial\nocclusions and illumination variation. This new database can be a very valuable\nresource for development and evaluation of algorithms on face detection, eye\ndetection, head tracking, eye gaze tracking etc. in NIR lighting.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 13:24:14 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Happy", "S L", ""], ["Dasgupta", "Anirban", ""], ["George", "Anjith", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1505.04058", "submitter": "S L Happy", "authors": "S. L. Happy, Anjith George and Aurobinda Routray", "title": "A Real Time Facial Expression Classification System Using Local Binary\n  Patterns", "comments": "IEEE Proceedings of 4th International Conference on Intelligent Human\n  Computer Interaction, 2012", "journal-ref": null, "doi": "10.1109/IHCI.2012.6481802", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression analysis is one of the popular fields of research in human\ncomputer interaction (HCI). It has several applications in next generation user\ninterfaces, human emotion analysis, behavior and cognitive modeling. In this\npaper, a facial expression classification algorithm is proposed which uses Haar\nclassifier for face detection purpose, Local Binary Patterns (LBP) histogram of\ndifferent block sizes of a face image as feature vectors and classifies various\nfacial expressions using Principal Component Analysis (PCA). The algorithm is\nimplemented in real time for expression classification since the computational\ncomplexity of the algorithm is small. A customizable approach is proposed for\nfacial expression analysis, since the various expressions and intensity of\nexpressions vary from person to person. The system uses grayscale frontal face\nimages of a person to classify six basic emotions namely happiness, sadness,\ndisgust, fear, surprise and anger.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 13:33:10 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Happy", "S. L.", ""], ["George", "Anjith", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1505.04117", "submitter": "Adriana Kovashka", "authors": "Adriana Kovashka and Kristen Grauman", "title": "Discovering Attribute Shades of Meaning with the Crowd", "comments": "Published in the International Journal of Computer Vision (IJCV),\n  January 2015. The final publication is available at Springer via\n  http://dx.doi.org/10.1007/s11263-014-0798-1", "journal-ref": "International Journal of Computer Vision 1573-1405 (2015,\n  Springer)", "doi": "10.1007/s11263-014-0798-1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To learn semantic attributes, existing methods typically train one\ndiscriminative model for each word in a vocabulary of nameable properties.\nHowever, this \"one model per word\" assumption is problematic: while a word\nmight have a precise linguistic definition, it need not have a precise visual\ndefinition. We propose to discover shades of attribute meaning. Given an\nattribute name, we use crowdsourced image labels to discover the latent factors\nunderlying how different annotators perceive the named concept. We show that\nstructure in those latent factors helps reveal shades, that is, interpretations\nfor the attribute shared by some group of annotators. Using these shades, we\ntrain classifiers to capture the primary (often subtle) variants of the\nattribute. The resulting models are both semantic and visually precise. By\ncatering to users' interpretations, they improve attribute prediction accuracy\non novel images. Shades also enable more successful attribute-based image\nsearch, by providing robust personalized models for retrieving multi-attribute\nquery results. They are widely applicable to tasks that involve describing\nvisual content, such as zero-shot category learning and organization of photo\ncollections.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 16:43:08 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Kovashka", "Adriana", ""], ["Grauman", "Kristen", ""]]}, {"id": "1505.04141", "submitter": "Adriana Kovashka", "authors": "Adriana Kovashka and Devi Parikh and Kristen Grauman", "title": "WhittleSearch: Interactive Image Search with Relative Attribute Feedback", "comments": "Published in the International Journal of Computer Vision (IJCV),\n  April 2015. The final publication is available at Springer via\n  http://dx.doi.org/10.1007/s11263-015-0814-0", "journal-ref": "International Journal of Computer Vision, 1573-1405 (2015,\n  Springer)", "doi": "10.1007/s11263-015-0814-0", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel mode of feedback for image search, where a user describes\nwhich properties of exemplar images should be adjusted in order to more closely\nmatch his/her mental model of the image sought. For example, perusing image\nresults for a query \"black shoes\", the user might state, \"Show me shoe images\nlike these, but sportier.\" Offline, our approach first learns a set of ranking\nfunctions, each of which predicts the relative strength of a nameable attribute\nin an image (e.g., sportiness). At query time, the system presents the user\nwith a set of exemplar images, and the user relates them to his/her target\nimage with comparative statements. Using a series of such constraints in the\nmulti-dimensional attribute space, our method iteratively updates its relevance\nfunction and re-ranks the database of images. To determine which exemplar\nimages receive feedback from the user, we present two variants of the approach:\none where the feedback is user-initiated and another where the feedback is\nactively system-initiated. In either case, our approach allows a user to\nefficiently \"whittle away\" irrelevant portions of the visual feature space,\nusing semantic language to precisely communicate her preferences to the system.\nWe demonstrate our technique for refining image search for people, products,\nand scenes, and we show that it outperforms traditional binary relevance\nfeedback in terms of search speed and accuracy. In addition, the ordinal nature\nof relative attributes helps make our active approach efficient -- both\ncomputationally for the machine when selecting the reference images, and for\nthe user by requiring less user interaction than conventional passive and\nactive methods.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 18:03:12 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 13:52:40 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Kovashka", "Adriana", ""], ["Parikh", "Devi", ""], ["Grauman", "Kristen", ""]]}, {"id": "1505.04143", "submitter": "Hilton Bristow", "authors": "Hilton Bristow, Jack Valmadre and Simon Lucey", "title": "Dense Semantic Correspondence where Every Pixel is a Classifier", "comments": "ICCV 2015 Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining dense semantic correspondences across objects and scenes is a\ndifficult problem that underpins many higher-level computer vision algorithms.\nUnlike canonical dense correspondence problems which consider images that are\nspatially or temporally adjacent, semantic correspondence is characterized by\nimages that share similar high-level structures whose exact appearance and\ngeometry may differ.\n  Motivated by object recognition literature and recent work on rapidly\nestimating linear classifiers, we treat semantic correspondence as a\nconstrained detection problem, where an exemplar LDA classifier is learned for\neach pixel. LDA classifiers have two distinct benefits: (i) they exhibit higher\naverage precision than similarity metrics typically used in correspondence\nproblems, and (ii) unlike exemplar SVM, can output globally interpretable\nposterior probabilities without calibration, whilst also being significantly\nfaster to train.\n  We pose the correspondence problem as a graphical model, where the unary\npotentials are computed via convolution with the set of exemplar classifiers,\nand the joint potentials enforce smoothly varying correspondence assignment.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 18:04:07 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Bristow", "Hilton", ""], ["Valmadre", "Jack", ""], ["Lucey", "Simon", ""]]}, {"id": "1505.04260", "submitter": "Giuseppe Boccignone", "authors": "Vittorio Cuculo, Raffaella Lanzarotti, Giuseppe Boccignone", "title": "The color of smiling: computational synaesthesia of facial expressions", "comments": "Submitted to: 18th International Conference on Image Analysis and\n  Processing (ICIAP 2015), 7-11 September 2015, Genova, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note gives a preliminary account of the transcoding or rechanneling\nproblem between different stimuli as it is of interest for the natural\ninteraction or affective computing fields. By the consideration of a simple\nexample, namely the color response of an affective lamp to a sensed facial\nexpression, we frame the problem within an information- theoretic perspective.\nA full justification in terms of the Information Bottleneck principle promotes\na latent affective space, hitherto surmised as an appealing and intuitive\nsolution, as a suitable mediator between the different stimuli.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 10:28:51 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Cuculo", "Vittorio", ""], ["Lanzarotti", "Raffaella", ""], ["Boccignone", "Giuseppe", ""]]}, {"id": "1505.04286", "submitter": "Harry Commin PhD", "authors": "Harry Commin", "title": "Robust Real-time Extraction of Fiducial Facial Feature Points using\n  Haar-like Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore methods of robustly extracting fiducial facial\nfeature points - an important process for numerous facial image processing\ntasks. We consider various methods to first detect face, then facial features\nand finally salient facial feature points. Colour-based models are analysed and\ntheir overall unsuitability for this task is summarised. The bulk of the report\nis then dedicated to proposing a learning-based method centred on the\nViola-Jones algorithm. The specific difficulties and considerations relating to\nfeature point detection are laid out in this context and a novel approach is\nestablished to address these issues. On a sequence of clear and unobstructed\nface images, our proposed system achieves average detection rates of over 90%.\nThen, using a more varied sample dataset, we identify some possible areas for\nfuture development of our system.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 15:41:21 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Commin", "Harry", ""]]}, {"id": "1505.04364", "submitter": "Kai-Fu Yang", "authors": "Kai-Fu Yang, Hui Li, Chao-Yi Li, and Yong-Jie Li", "title": "Salient Structure Detection by Context-Guided Visual Search", "comments": "13 pages, 15 figures", "journal-ref": "IEEE Transactions on Image Processing (TIP), 2016", "doi": "10.1109/TIP.2016.2572600", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define the task of salient structure (SS) detection to unify the\nsaliency-related tasks like fixation prediction, salient object detection, and\nother detection of structures of interest. In this study, we propose a unified\nframework for SS detection by modeling the two-pathway-based guided search\nstrategy of biological vision. Firstly, context-based spatial prior (CBSP) is\nextracted based on the layout of edges in the given scene along a fast visual\npathway, called non-selective pathway. This is a rough and non-selective\nestimation of the locations where the potential SSs present. Secondly, another\nflow of local feature extraction is executed in parallel along the selective\npathway. Finally, Bayesian inference is used to integrate local cues guided by\nCBSP, and to predict the exact locations of SSs in the input scene. The\nproposed model is invariant to size and features of objects. Experimental\nresults on four datasets (two fixation prediction datasets and two salient\nobject datasets) demonstrate that our system achieves competitive performance\nfor SS detection (i.e., both the tasks of fixation prediction and salient\nobject detection) comparing to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 07:15:25 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Yang", "Kai-Fu", ""], ["Li", "Hui", ""], ["Li", "Chao-Yi", ""], ["Li", "Yong-Jie", ""]]}, {"id": "1505.04366", "submitter": "Seunghoon Hong", "authors": "Hyeonwoo Noh, Seunghoon Hong, Bohyung Han", "title": "Learning Deconvolution Network for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel semantic segmentation algorithm by learning a\ndeconvolution network. We learn the network on top of the convolutional layers\nadopted from VGG 16-layer net. The deconvolution network is composed of\ndeconvolution and unpooling layers, which identify pixel-wise class labels and\npredict segmentation masks. We apply the trained network to each proposal in an\ninput image, and construct the final semantic segmentation map by combining the\nresults from all proposals in a simple manner. The proposed algorithm mitigates\nthe limitations of the existing methods based on fully convolutional networks\nby integrating deep deconvolution network and proposal-wise prediction; our\nsegmentation method typically identifies detailed structures and handles\nobjects in multiple scales naturally. Our network demonstrates outstanding\nperformance in PASCAL VOC 2012 dataset, and we achieve the best accuracy\n(72.5%) among the methods trained with no external data through ensemble with\nthe fully convolutional network.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 07:33:28 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Noh", "Hyeonwoo", ""], ["Hong", "Seunghoon", ""], ["Han", "Bohyung", ""]]}, {"id": "1505.04373", "submitter": "Lei Zhang", "authors": "Lei Zhang and David Zhang", "title": "Evolutionary Cost-sensitive Extreme Learning Machine", "comments": "This paper has been accepted for publication in IEEE Transactions on\n  Neural Networks and Learning Systems", "journal-ref": null, "doi": "10.1109/TNNLS.2016.2607757", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional extreme learning machines solve a Moore-Penrose generalized\ninverse of hidden layer activated matrix and analytically determine the output\nweights to achieve generalized performance, by assuming the same loss from\ndifferent types of misclassification. The assumption may not hold in\ncost-sensitive recognition tasks, such as face recognition based access control\nsystem, where misclassifying a stranger as a family member may result in more\nserious disaster than misclassifying a family member as a stranger. Though\nrecent cost-sensitive learning can reduce the total loss with a given cost\nmatrix that quantifies how severe one type of mistake against another, in many\nrealistic cases the cost matrix is unknown to users. Motivated by these\nconcerns, this paper proposes an evolutionary cost-sensitive extreme learning\nmachine (ECSELM), with the following merits: 1) to our best knowledge, it is\nthe first proposal of ELM in evolutionary cost-sensitive classification\nscenario; 2) it well addresses the open issue of how to define the cost matrix\nin cost-sensitive learning tasks; 3) an evolutionary backtracking search\nalgorithm is induced for adaptive cost matrix optimization. Experiments in a\nvariety of cost-sensitive tasks well demonstrate the effectiveness of the\nproposed approaches, with about 5%~10% improvements.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 09:04:47 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 09:55:31 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Zhang", "Lei", ""], ["Zhang", "David", ""]]}, {"id": "1505.04382", "submitter": "Lei Zhang", "authors": "Lei Zhang and David Zhang", "title": "Robust Visual Knowledge Transfer via EDA", "comments": "This paper has been accepted for publication in IEEE Transactions on\n  Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of visual knowledge adaptation by leveraging labeled\npatterns from source domain and a very limited number of labeled instances in\ntarget domain to learn a robust classifier for visual categorization. This\npaper proposes a new extreme learning machine based cross-domain network\nlearning framework, that is called Extreme Learning Machine (ELM) based Domain\nAdaptation (EDA). It allows us to learn a category transformation and an ELM\nclassifier with random projection by minimizing the l_(2,1)-norm of the network\noutput weights and the learning error simultaneously. The unlabeled target\ndata, as useful knowledge, is also integrated as a fidelity term to guarantee\nthe stability during cross domain learning. It minimizes the matching error\nbetween the learned classifier and a base classifier, such that many existing\nclassifiers can be readily incorporated as base classifiers. The network output\nweights cannot only be analytically determined, but also transferrable.\nAdditionally, a manifold regularization with Laplacian graph is incorporated,\nsuch that it is beneficial to semi-supervised learning. Extensively, we also\npropose a model of multiple views, referred as MvEDA. Experiments on benchmark\nvisual datasets for video event recognition and object recognition, demonstrate\nthat our EDA methods outperform existing cross-domain learning methods.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 11:23:12 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2016 07:22:34 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Zhang", "Lei", ""], ["Zhang", "David", ""]]}, {"id": "1505.04424", "submitter": "Mrinal Haloi", "authors": "Mrinal Haloi", "title": "Improved Microaneurysm Detection using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel microaneurysm (MA) detection for early\ndiabetic retinopathy screening using color fundus images. Since MA usually the\nfirst lesions to appear as an indicator of diabetic retinopathy, accurate\ndetection of MA is necessary for treatment. Each pixel of the image is\nclassified as either MA or non-MA using a deep neural network with dropout\ntraining procedure using maxout activation function. No preprocessing step or\nmanual feature extraction is required. Substantial improvements over standard\nMA detection method based on the pipeline of preprocessing, feature extraction,\nclassification followed by post processing is achieved. The presented method is\nevaluated in publicly available Retinopathy Online Challenge (ROC) and\nDiaretdb1v2 database and achieved state-of-the-art accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 17:37:14 GMT"}, {"version": "v2", "created": "Sun, 17 Jul 2016 10:07:40 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Haloi", "Mrinal", ""]]}, {"id": "1505.04427", "submitter": "Zhenzhong Lan", "authors": "Zhenzhong Lan, Dezhong Yao, Ming Lin, Shoou-I Yu, Alexander Hauptmann", "title": "The Best of Both Worlds: Combining Data-independent and Data-driven\n  Approaches for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the success of data-driven convolutional neural networks (CNNs)\nin object recognition on static images, researchers are working hard towards\ndeveloping CNN equivalents for learning video features. However, learning video\nfeatures globally has proven to be quite a challenge due to its high\ndimensionality, the lack of labelled data and the difficulty in processing\nlarge-scale video data. Therefore, we propose to leverage effective techniques\nfrom both data-driven and data-independent approaches to improve action\nrecognition system.\n  Our contribution is three-fold. First, we propose a two-stream Stacked\nConvolutional Independent Subspace Analysis (ConvISA) architecture to show that\nunsupervised learning methods can significantly boost the performance of\ntraditional local features extracted from data-independent models. Second, we\ndemonstrate that by learning on video volumes detected by Improved Dense\nTrajectory (IDT), we can seamlessly combine our novel local descriptors with\nhand-crafted descriptors. Thus we can utilize available feature enhancing\ntechniques developed for hand-crafted descriptors. Finally, similar to\nmulti-class classification framework in CNNs, we propose a training-free\nre-ranking technique that exploits the relationship among action classes to\nimprove the overall performance. Our experimental results on four benchmark\naction recognition datasets show significantly improved performance.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 17:54:38 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Lan", "Zhenzhong", ""], ["Yao", "Dezhong", ""], ["Lin", "Ming", ""], ["Yu", "Shoou-I", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1505.04467", "submitter": "C. Lawrence Zitnick", "authors": "Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C.\n  Lawrence Zitnick", "title": "Exploring Nearest Neighbor Approaches for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a variety of nearest neighbor baseline approaches for image\ncaptioning. These approaches find a set of nearest neighbor images in the\ntraining set from which a caption may be borrowed for the query image. We\nselect a caption for the query image by finding the caption that best\nrepresents the \"consensus\" of the set of candidate captions gathered from the\nnearest neighbor images. When measured by automatic evaluation metrics on the\nMS COCO caption evaluation server, these approaches perform as well as many\nrecent approaches that generate novel captions. However, human studies show\nthat a method that generates novel captions is still preferred over the nearest\nneighbor approach.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 22:14:27 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Devlin", "Jacob", ""], ["Gupta", "Saurabh", ""], ["Girshick", "Ross", ""], ["Mitchell", "Margaret", ""], ["Zitnick", "C. Lawrence", ""]]}, {"id": "1505.04474", "submitter": "Saurabh Gupta", "authors": "Saurabh Gupta and Jitendra Malik", "title": "Visual Semantic Role Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the problem of Visual Semantic Role Labeling:\ngiven an image we want to detect people doing actions and localize the objects\nof interaction. Classical approaches to action recognition either study the\ntask of action classification at the image or video clip level or at best\nproduce a bounding box around the person doing the action. We believe such an\noutput is inadequate and a complete understanding can only come when we are\nable to associate objects in the scene to the different semantic roles of the\naction. To enable progress towards this goal, we annotate a dataset of 16K\npeople instances in 10K images with actions they are doing and associate\nobjects in the scene with different semantic roles for each action. Finally, we\nprovide a set of baseline algorithms for this task and analyze error modes\nproviding directions for future work.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 23:21:35 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Gupta", "Saurabh", ""], ["Malik", "Jitendra", ""]]}, {"id": "1505.04502", "submitter": "Pierre-Luc St-Charles", "authors": "Gengjie Chen, Pierre-Luc St-Charles, Wassim Bouachir, Thomas\n  Joeisseint, Guillaume-Alexandre Bilodeau, Robert Bergevin", "title": "Reproducible Evaluation of Pan-Tilt-Zoom Tracking", "comments": "This is an extended version of the 2015 ICIP paper \"Reproducible\n  Evaluation of Pan-Tilt-Zoom Tracking\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking with a Pan-Tilt-Zoom (PTZ) camera has been a research topic in\ncomputer vision for many years. However, it is very difficult to assess the\nprogress that has been made on this topic because there is no standard\nevaluation methodology. The difficulty in evaluating PTZ tracking algorithms\narises from their dynamic nature. In contrast to other forms of tracking, PTZ\ntracking involves both locating the target in the image and controlling the\nmotors of the camera to aim it so that the target stays in its field of view.\nThis type of tracking can only be performed online. In this paper, we propose a\nnew evaluation framework based on a virtual PTZ camera. With this framework,\ntracking scenarios do not change for each experiment and we are able to\nreplicate online PTZ camera control and behavior including camera positioning\ndelays, tracker processing delays, and numerical zoom. We tested our evaluation\nframework with the Camshift tracker to show its viability and to establish\nbaseline results.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 03:52:52 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Chen", "Gengjie", ""], ["St-Charles", "Pierre-Luc", ""], ["Bouachir", "Wassim", ""], ["Joeisseint", "Thomas", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Bergevin", "Robert", ""]]}, {"id": "1505.04548", "submitter": "Michael Milford", "authors": "Michael Milford, Hanme Kim, Michael Mangan, Stefan Leutenegger, Tom\n  Stone, Barbara Webb, Andrew Davison", "title": "Place Recognition with Event-based Cameras and a Neural Implementation\n  of SeqSLAM", "comments": "Paper accepted for presentation at the \"Innovative Sensing for\n  Robotics: Focus on Neuromorphic Sensors\" workshop at the 2015 IEEE\n  International Conference on Robotics and Automation, 8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-based cameras offer much potential to the fields of robotics and\ncomputer vision, in part due to their large dynamic range and extremely high\n\"frame rates\". These attributes make them, at least in theory, particularly\nsuitable for enabling tasks like navigation and mapping on high speed robotic\nplatforms under challenging lighting conditions, a task which has been\nparticularly challenging for traditional algorithms and camera sensors. Before\nthese tasks become feasible however, progress must be made towards adapting and\ninnovating current RGB-camera-based algorithms to work with event-based\ncameras. In this paper we present ongoing research investigating two distinct\napproaches to incorporating event-based cameras for robotic navigation: the\ninvestigation of suitable place recognition / loop closure techniques, and the\ndevelopment of efficient neural implementations of place recognition techniques\nthat enable the possibility of place recognition using event-based cameras at\nvery high frame rates using neuromorphic computing hardware.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 08:33:15 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Milford", "Michael", ""], ["Kim", "Hanme", ""], ["Mangan", "Michael", ""], ["Leutenegger", "Stefan", ""], ["Stone", "Tom", ""], ["Webb", "Barbara", ""], ["Davison", "Andrew", ""]]}, {"id": "1505.04585", "submitter": "Carsten Gottschlich", "authors": "Duy Hoang Thai and Carsten Gottschlich", "title": "Global Variational Method for Fingerprint Segmentation by Three-part\n  Decomposition", "comments": null, "journal-ref": "IET Biometrics, vol. 5, no. 2, pp. 120-130, June 2016", "doi": "10.1049/iet-bmt.2015.0010", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verifying an identity claim by fingerprint recognition is a commonplace\nexperience for millions of people in their daily life, e.g. for unlocking a\ntablet computer or smartphone. The first processing step after fingerprint\nimage acquisition is segmentation, i.e. dividing a fingerprint image into a\nforeground region which contains the relevant features for the comparison\nalgorithm, and a background region. We propose a novel segmentation method by\nglobal three-part decomposition (G3PD). Based on global variational analysis,\nthe G3PD method decomposes a fingerprint image into cartoon, texture and noise\nparts. After decomposition, the foreground region is obtained from the non-zero\ncoefficients in the texture image using morphological processing. The\nsegmentation performance of the G3PD method is compared to five\nstate-of-the-art methods on a benchmark which comprises manually marked ground\ntruth segmentation for 10560 images. Performance evaluations show that the G3PD\nmethod consistently outperforms existing methods in terms of segmentation\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 10:40:38 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Thai", "Duy Hoang", ""], ["Gottschlich", "Carsten", ""]]}, {"id": "1505.04597", "submitter": "Olaf Ronneberger", "authors": "Olaf Ronneberger and Philipp Fischer and Thomas Brox", "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "comments": "conditionally accepted at MICCAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is large consent that successful training of deep networks requires\nmany thousand annotated training samples. In this paper, we present a network\nand training strategy that relies on the strong use of data augmentation to use\nthe available annotated samples more efficiently. The architecture consists of\na contracting path to capture context and a symmetric expanding path that\nenables precise localization. We show that such a network can be trained\nend-to-end from very few images and outperforms the prior best method (a\nsliding-window convolutional network) on the ISBI challenge for segmentation of\nneuronal structures in electron microscopic stacks. Using the same network\ntrained on transmitted light microscopy images (phase contrast and DIC) we won\nthe ISBI cell tracking challenge 2015 in these categories by a large margin.\nMoreover, the network is fast. Segmentation of a 512x512 image takes less than\na second on a recent GPU. The full implementation (based on Caffe) and the\ntrained networks are available at\nhttp://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 11:28:37 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Ronneberger", "Olaf", ""], ["Fischer", "Philipp", ""], ["Brox", "Thomas", ""]]}, {"id": "1505.04617", "submitter": "Liping Wang", "authors": "Liping Wang and Songcan Chen", "title": "Joint Representation Classification for Collective Face Recognition", "comments": "20 pages, 5 figures, 3 tables; 4 algorithms, 2 lemmas and 2 theorems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation based classification (SRC) is popularly used in many\napplications such as face recognition, and implemented in two steps:\nrepresentation coding and classification. For a given set of testing images,\nSRC codes every image over the base images as a sparse representation then\nclassifies it to the class with the least representation error. This scheme\nutilizes an individual representation rather than the collective one to\nclassify such a set of images, doing so obviously ignores the correlation among\nthe given images. In this paper, a joint representation classification (JRC)\nfor collective face recognition is proposed. JRC takes the correlation of\nmultiple images as well as a single representation into account. Under the\nassumption that the given face images are generally related to each other, JRC\ncodes all the testing images over the base images simultaneously to facilitate\nrecognition. To this end, the testing inputs are aligned into a matrix and the\njoint representation coding is formulated to a generalized\n$l_{2,q}-l_{2,p}$-minimization problem. To uniformly solve the induced\noptimization problems for any $q\\in[1,2]$ and $p\\in (0,2]$, an iterative\nquadratic method (IQM) is developed. IQM is proved to be a strict descent\nalgorithm with convergence to the optimal solution. Moreover, a more practical\nIQM is proposed for large-scale case. Experimental results on three public\ndatabases show that the JRC with practical IQM no only saves much computational\ncost but also achieves better performance in collective face recognition than\nthe state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 13:04:04 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Wang", "Liping", ""], ["Chen", "Songcan", ""]]}, {"id": "1505.04803", "submitter": "Yong Jae Lee", "authors": "Yong Jae Lee and Kristen Grauman", "title": "Predicting Important Objects for Egocentric Video Summarization", "comments": "Published in the International Journal of Computer Vision (IJCV),\n  January 2015", "journal-ref": null, "doi": "10.1007/s11263-014-0794-5", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a video summarization approach for egocentric or \"wearable\" camera\ndata. Given hours of video, the proposed method produces a compact storyboard\nsummary of the camera wearer's day. In contrast to traditional keyframe\nselection techniques, the resulting summary focuses on the most important\nobjects and people with which the camera wearer interacts. To accomplish this,\nwe develop region cues indicative of high-level saliency in egocentric\nvideo---such as the nearness to hands, gaze, and frequency of occurrence---and\nlearn a regressor to predict the relative importance of any new region based on\nthese cues. Using these predictions and a simple form of temporal event\ndetection, our method selects frames for the storyboard that reflect the key\nobject-driven happenings. We adjust the compactness of the final summary given\neither an importance selection criterion or a length budget; for the latter, we\ndesign an efficient dynamic programming solution that accounts for importance,\nvisual uniqueness, and temporal displacement. Critically, the approach is\nneither camera-wearer-specific nor object-specific; that means the learned\nimportance metric need not be trained for a given user or context, and it can\npredict the importance of objects and people that have never been seen\npreviously. Our results on two egocentric video datasets show the method's\npromise relative to existing techniques for saliency and summarization.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 20:07:20 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Lee", "Yong Jae", ""], ["Grauman", "Kristen", ""]]}, {"id": "1505.04845", "submitter": "Xiaowei Zhou", "authors": "Xiaowei Zhou, Menglong Zhu, Kostas Daniilidis", "title": "Multi-Image Matching via Fast Alternating Minimization", "comments": "In ICCV'2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a global optimization-based approach to jointly\nmatching a set of images. The estimated correspondences simultaneously maximize\npairwise feature affinities and cycle consistency across multiple images.\nUnlike previous convex methods relying on semidefinite programming, we\nformulate the problem as a low-rank matrix recovery problem and show that the\ndesired semidefiniteness of a solution can be spontaneously fulfilled. The\nlow-rank formulation enables us to derive a fast alternating minimization\nalgorithm in order to handle practical problems with thousands of features.\nBoth simulation and real experiments demonstrate that the proposed algorithm\ncan achieve a competitive performance with an order of magnitude speedup\ncompared to the state-of-the-art algorithm. In the end, we demonstrate the\napplicability of the proposed method to match the images of different object\ninstances and as a result the potential to reconstruct category-specific object\nmodels from those images.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 01:14:26 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 17:08:34 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Zhou", "Xiaowei", ""], ["Zhu", "Menglong", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1505.04868", "submitter": "Limin Wang", "authors": "Limin Wang, Yu Qiao, Xiaoou Tang", "title": "Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2015", "journal-ref": null, "doi": "10.1109/CVPR.2015.7299059", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual features are of vital importance for human action understanding in\nvideos. This paper presents a new video representation, called\ntrajectory-pooled deep-convolutional descriptor (TDD), which shares the merits\nof both hand-crafted features and deep-learned features. Specifically, we\nutilize deep architectures to learn discriminative convolutional feature maps,\nand conduct trajectory-constrained pooling to aggregate these convolutional\nfeatures into effective descriptors. To enhance the robustness of TDDs, we\ndesign two normalization methods to transform convolutional feature maps,\nnamely spatiotemporal normalization and channel normalization. The advantages\nof our features come from (i) TDDs are automatically learned and contain high\ndiscriminative capacity compared with those hand-crafted features; (ii) TDDs\ntake account of the intrinsic characteristics of temporal dimension and\nintroduce the strategies of trajectory-constrained sampling and pooling for\naggregating deep-learned features. We conduct experiments on two challenging\ndatasets: HMDB51 and UCF101. Experimental results show that TDDs outperform\nprevious hand-crafted features and deep-learned features. Our method also\nachieves superior performance to the state of the art on these datasets (HMDB51\n65.9%, UCF101 91.5%).\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 04:36:42 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Wang", "Limin", ""], ["Qiao", "Yu", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1505.04870", "submitter": "Bryan Plummer", "authors": "Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo,\n  Julia Hockenmaier, and Svetlana Lazebnik", "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for\n  Richer Image-to-Sentence Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Flickr30k dataset has become a standard benchmark for sentence-based\nimage description. This paper presents Flickr30k Entities, which augments the\n158k captions from Flickr30k with 244k coreference chains, linking mentions of\nthe same entities across different captions for the same image, and associating\nthem with 276k manually annotated bounding boxes. Such annotations are\nessential for continued progress in automatic image description and grounded\nlanguage understanding. They enable us to define a new benchmark for\nlocalization of textual entity mentions in an image. We present a strong\nbaseline for this task that combines an image-text embedding, detectors for\ncommon objects, a color classifier, and a bias towards selecting larger\nobjects. While our baseline rivals in accuracy more complex state-of-the-art\nmodels, we show that its gains cannot be easily parlayed into improvements on\nsuch tasks as image-sentence retrieval, thus underlining the limitations of\ncurrent methods and the need for further research.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 04:46:03 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2015 22:17:45 GMT"}, {"version": "v3", "created": "Fri, 15 Apr 2016 14:58:37 GMT"}, {"version": "v4", "created": "Mon, 19 Sep 2016 20:20:42 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Plummer", "Bryan A.", ""], ["Wang", "Liwei", ""], ["Cervantes", "Chris M.", ""], ["Caicedo", "Juan C.", ""], ["Hockenmaier", "Julia", ""], ["Lazebnik", "Svetlana", ""]]}, {"id": "1505.04873", "submitter": "Lior Talker", "authors": "Lior Talker and Yael Moses and Ilan Shimshoni", "title": "Have a Look at What I See", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for guiding a photographer to rotate her/his smartphone\ncamera to obtain an image that overlaps with another image of the same scene.\nThe other image is taken by another photographer from a different viewpoint.\nOur method is applicable even when the images do not have overlapping fields of\nview. Straightforward applications of our method include sharing attention to\nregions of interest for social purposes, or adding missing images to improve\nstructure for motion results. Our solution uses additional images of the scene,\nwhich are often available since many people use their smartphone cameras\nregularly. These images may be available online from other photographers who\nare present at the scene. Our method avoids 3D scene reconstruction; it relies\ninstead on a new representation that consists of the spatial orders of the\nscene points on two axes, x and y. This representation allows a sequence of\npoints to be chosen efficiently and projected onto the photographers images,\nusing epipolar point transfer. Overlaying these epipolar lines on the live\npreview of the camera produces a convenient interface to guide the user. The\nmethod was tested on challenging datasets of images and succeeded in guiding a\nphotographer from one view to a non-overlapping destination view.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 04:49:46 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Talker", "Lior", ""], ["Moses", "Yael", ""], ["Shimshoni", "Ilan", ""]]}, {"id": "1505.04922", "submitter": "Lianwen Jin", "authors": "Weixin Yang, Lianwen Jin, Manfei Liu", "title": "Character-level Chinese Writer Identification using Path Signature\n  Feature, DropStroke and Deep CNN", "comments": "5 pages, 4 figures, 2 tables. Manuscript is accepted to appear in\n  ICDAR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing online writer-identification systems require that the text\ncontent is supplied in advance and rely on separately designed features and\nclassifiers. The identifications are based on lines of text, entire paragraphs,\nor entire documents; however, these materials are not always available. In this\npaper, we introduce a path-signature feature to an end-to-end text-independent\nwriter-identification system with a deep convolutional neural network (DCNN).\nBecause deep models require a considerable amount of data to achieve good\nperformance, we propose a data-augmentation method named DropStroke to enrich\npersonal handwriting. Experiments were conducted on online handwritten Chinese\ncharacters from the CASIA-OLHWDB1.0 dataset, which consists of 3,866 classes\nfrom 420 writers. For each writer, we only used 200 samples for training and\nthe remaining 3,666. The results reveal that the path-signature feature is\nuseful for writer identification, and the proposed DropStroke technique\nenhances the generalization and significantly improves performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 09:25:46 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Yang", "Weixin", ""], ["Jin", "Lianwen", ""], ["Liu", "Manfei", ""]]}, {"id": "1505.04925", "submitter": "Lianwen Jin", "authors": "Zhuoyao Zhong, Lianwen Jin, Zecheng Xie", "title": "High Performance Offline Handwritten Chinese Character Recognition Using\n  GoogLeNet and Directional Feature Maps", "comments": "5 pages, 4 figures, 2 tables. Manuscript is accepted to appear in\n  ICDAR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Just like its great success in solving many computer vision problems, the\nconvolutional neural networks (CNN) provided new end-to-end approach to\nhandwritten Chinese character recognition (HCCR) with very promising results in\nrecent years. However, previous CNNs so far proposed for HCCR were neither deep\nenough nor slim enough. We show in this paper that, a deeper architecture can\nbenefit HCCR a lot to achieve higher performance, meanwhile can be designed\nwith less parameters. We also show that the traditional feature extraction\nmethods, such as Gabor or gradient feature maps, are still useful for enhancing\nthe performance of CNN. We design a streamlined version of GoogLeNet [13],\nwhich was original proposed for image classification in recent years with very\ndeep architecture, for HCCR (denoted as HCCR-GoogLeNet). The HCCR-GoogLeNet we\nused is 19 layers deep but involves with only 7.26 million parameters.\nExperiments were conducted using the ICDAR 2013 offline HCCR competition\ndataset. It has been shown that with the proper incorporation with traditional\ndirectional feature maps, the proposed single and ensemble HCCR-GoogLeNet\nmodels achieve new state of the art recognition accuracy of 96.35% and 96.74%,\nrespectively, outperforming previous best result with significant gap.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 09:32:54 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Zhong", "Zhuoyao", ""], ["Jin", "Lianwen", ""], ["Xie", "Zecheng", ""]]}, {"id": "1505.04938", "submitter": "Jos\\'e A. Iglesias", "authors": "Jos\\'e A. Iglesias, Clemens Kirisits", "title": "Convective regularization for optical flow", "comments": null, "journal-ref": "Variational Methods in Imaging and Geometric Control, Radon Series\n  on Computational and Applied Mathematics 18, pp. 184-201, 2017", "doi": "10.1515/9783110430394-005", "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that the time derivative in a fixed coordinate frame may not be the\nmost appropriate measure of time regularity of an optical flow field. Instead,\nfor a given velocity field $v$ we consider the convective acceleration $v_t +\n\\nabla v v$ which describes the acceleration of objects moving according to\n$v$. Consequently we investigate the suitability of the nonconvex functional\n$\\|v_t + \\nabla v v\\|^2_{L^2}$ as a regularization term for optical flow. We\ndemonstrate that this term acts as both a spatial and a temporal regularizer\nand has an intrinsic edge-preserving property. We incorporate it into a\ncontrast invariant and time-regularized variant of the Horn-Schunck functional,\nprove existence of minimizers and verify experimentally that it addresses some\nof the problems of basic quadratic models. For the minimization we use an\niterative scheme that approximates the original nonlinear problem with a\nsequence of linear ones. We believe that the convective acceleration may be\ngainfully introduced in a variety of optical flow models.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 10:02:53 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 11:24:36 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Iglesias", "Jos\u00e9 A.", ""], ["Kirisits", "Clemens", ""]]}, {"id": "1505.05190", "submitter": "Hiroharu Kato", "authors": "Hiroharu Kato and Tatsuya Harada", "title": "Image Reconstruction from Bag-of-Visual-Words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this work is to reconstruct an original image from\nBag-of-Visual-Words (BoVW). Image reconstruction from features can be a means\nof identifying the characteristics of features. Additionally, it enables us to\ngenerate novel images via features. Although BoVW is the de facto standard\nfeature for image recognition and retrieval, successful image reconstruction\nfrom BoVW has not been reported yet. What complicates this task is that BoVW\nlacks the spatial information for including visual words. As described in this\npaper, to estimate an original arrangement, we propose an evaluation function\nthat incorporates the naturalness of local adjacency and the global position,\nwith a method to obtain related parameters using an external image database. To\nevaluate the performance of our method, we reconstruct images of objects of 101\nkinds. Additionally, we apply our method to analyze object classifiers and to\ngenerate novel images via BoVW.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 21:12:15 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Kato", "Hiroharu", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1505.05192", "submitter": "Carl Doersch", "authors": "Carl Doersch and Abhinav Gupta and Alexei A. Efros", "title": "Unsupervised Visual Representation Learning by Context Prediction", "comments": "Oral paper at ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores the use of spatial context as a source of free and\nplentiful supervisory signal for training a rich visual representation. Given\nonly a large, unlabeled image collection, we extract random pairs of patches\nfrom each image and train a convolutional neural net to predict the position of\nthe second patch relative to the first. We argue that doing well on this task\nrequires the model to learn to recognize objects and their parts. We\ndemonstrate that the feature representation learned using this within-image\ncontext indeed captures visual similarity across images. For example, this\nrepresentation allows us to perform unsupervised visual discovery of objects\nlike cats, people, and even birds from the Pascal VOC 2011 detection dataset.\nFurthermore, we show that the learned ConvNet can be used in the R-CNN\nframework and provides a significant boost over a randomly-initialized ConvNet,\nresulting in state-of-the-art performance among algorithms which use only\nPascal-provided training set annotations.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 21:18:17 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2015 17:48:40 GMT"}, {"version": "v3", "created": "Sat, 16 Jan 2016 22:09:45 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Doersch", "Carl", ""], ["Gupta", "Abhinav", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1505.05212", "submitter": "Hamid Tizhoosh", "authors": "Hamid R. Tizhoosh", "title": "Barcode Annotations for Medical Image Retrieval: A Preliminary\n  Investigation", "comments": "To be published in proceedings of The IEEE International Conference\n  on Image Processing (ICIP 2015), September 27-30, 2015, Quebec City, Canada", "journal-ref": null, "doi": "10.1109/ICIP.2015.7350913", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to generate and to use barcodes to annotate medical\nimages and/or their regions of interest such as organs, tumors and tissue\ntypes. A multitude of efficient feature-based image retrieval methods already\nexist that can assign a query image to a certain image class. Visual\nannotations may help to increase the retrieval accuracy if combined with\nexisting feature-based classification paradigms. Whereas with annotations we\nusually mean textual descriptions, in this paper barcode annotations are\nproposed. In particular, Radon barcodes (RBC) are introduced. As well, local\nbinary patterns (LBP) and local Radon binary patterns (LRBP) are implemented as\nbarcodes. The IRMA x-ray dataset with 12,677 training images and 1,733 test\nimages is used to verify how barcodes could facilitate image retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 23:48:24 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Tizhoosh", "Hamid R.", ""]]}, {"id": "1505.05225", "submitter": "Lihua Guo", "authors": "Guo Lihua, Li Fudi", "title": "Image aesthetic evaluation using paralleled deep convolution neural\n  network", "comments": "7 pages, 6 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image aesthetic evaluation has attracted much attention in recent years.\nImage aesthetic evaluation methods heavily depend on the effective aesthetic\nfeature. Traditional meth-ods always extract hand-crafted features. However,\nthese hand-crafted features are always designed to adapt particu-lar datasets,\nand extraction of them needs special design. Rather than extracting\nhand-crafted features, an automati-cally learn of aesthetic features based on\ndeep convolutional neural network (DCNN) is first adopt in this paper. As we\nall know, when the training dataset is given, the DCNN architecture with high\ncomplexity may meet the over-fitting problem. On the other side, the DCNN\narchitecture with low complexity would not efficiently extract effective\nfeatures. For these reasons, we further propose a paralleled convolutional\nneural network (PDCNN) with multi-level structures to automatically adapt to\nthe training dataset. Experimental results show that our proposed PDCNN\narchitecture achieves better performance than other traditional methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 02:03:23 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Lihua", "Guo", ""], ["Fudi", "Li", ""]]}, {"id": "1505.05232", "submitter": "Songfan Yang", "authors": "Songfan Yang, Deva Ramanan", "title": "Multi-scale recognition with DAG-CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore multi-scale convolutional neural nets (CNNs) for image\nclassification. Contemporary approaches extract features from a single output\nlayer. By extracting features from multiple layers, one can simultaneously\nreason about high, mid, and low-level features during classification. The\nresulting multi-scale architecture can itself be seen as a feed-forward model\nthat is structured as a directed acyclic graph (DAG-CNNs). We use DAG-CNNs to\nlearn a set of multiscale features that can be effectively shared between\ncoarse and fine-grained classification tasks. While fine-tuning such models\nhelps performance, we show that even \"off-the-self\" multiscale features perform\nquite well. We present extensive analysis and demonstrate state-of-the-art\nclassification performance on three standard scene benchmarks (SUN397, MIT67,\nand Scene15). In terms of the heavily benchmarked MIT67 and Scene15 datasets,\nour results reduce the lowest previously-reported error by 23.9% and 9.5%,\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 02:52:07 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Yang", "Songfan", ""], ["Ramanan", "Deva", ""]]}, {"id": "1505.05233", "submitter": "Lei Zhang", "authors": "Lei Zhang and David Zhang", "title": "Visual Understanding via Multi-Feature Shared Learning with Global\n  Consistency", "comments": "13 pages,6 figures, this paper is accepted for publication in IEEE\n  Transactions on Multimedia", "journal-ref": null, "doi": "10.1109/TMM.2015.2510509", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image/video data is usually represented with multiple visual features. Fusion\nof multi-source information for establishing the attributes has been widely\nrecognized. Multi-feature visual recognition has recently received much\nattention in multimedia applications. This paper studies visual understanding\nvia a newly proposed l_2-norm based multi-feature shared learning framework,\nwhich can simultaneously learn a global label matrix and multiple\nsub-classifiers with the labeled multi-feature data. Additionally, a group\ngraph manifold regularizer composed of the Laplacian and Hessian graph is\nproposed for better preserving the manifold structure of each feature, such\nthat the label prediction power is much improved through the semi-supervised\nlearning with global label consistency. For convenience, we call the proposed\napproach Global-Label-Consistent Classifier (GLCC). The merits of the proposed\nmethod include: 1) the manifold structure information of each feature is\nexploited in learning, resulting in a more faithful classification owing to the\nglobal label consistency; 2) a group graph manifold regularizer based on the\nLaplacian and Hessian regularization is constructed; 3) an efficient\nalternative optimization method is introduced as a fast solver owing to the\nconvex sub-problems. Experiments on several benchmark visual datasets for\nmultimedia understanding, such as the 17-category Oxford Flower dataset, the\nchallenging 101-category Caltech dataset, the YouTube & Consumer Videos dataset\nand the large-scale NUS-WIDE dataset, demonstrate that the proposed approach\ncompares favorably with the state-of-the-art algorithms. An extensive\nexperiment on the deep convolutional activation features also show the\neffectiveness of the proposed approach. The code is available on\nhttp://www.escience.cn/people/lei/index.html\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 03:01:08 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2015 10:07:11 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Zhang", "Lei", ""], ["Zhang", "David", ""]]}, {"id": "1505.05240", "submitter": "Siddharth Srivastava", "authors": "Siddharth Srivastava, Prerana Mukherjee, Brejesh Lall", "title": "Benchmarking KAZE and MCM for Multiclass Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": "v01.0", "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach for feature generation by\nappropriately fusing KAZE and SIFT features. We then use this feature set along\nwith Minimal Complexity Machine(MCM) for object classification. We show that\nKAZE and SIFT features are complementary. Experimental results indicate that an\nelementary integration of these techniques can outperform the state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 04:09:47 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Srivastava", "Siddharth", ""], ["Mukherjee", "Prerana", ""], ["Lall", "Brejesh", ""]]}, {"id": "1505.05254", "submitter": "Yedid Hoshen", "authors": "Yedid Hoshen and Shmuel Peleg", "title": "Live Video Synopsis for Multiple Cameras", "comments": "To be presented in ICIP 2015", "journal-ref": "Proc. ICIP'15, Quebec City, Sept. 2015, pp. 212-216", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video surveillance cameras generate most of recorded video, and there is far\nmore recorded video than operators can watch. Much progress has recently been\nmade using summarization of recorded video, but such techniques do not have\nmuch impact on live video surveillance.\n  We assume a camera hierarchy where a Master camera observes the\ndecision-critical region, and one or more Slave cameras observe regions where\npast activity is important for making the current decision. We propose that\nwhen people appear in the live Master camera, the Slave cameras will display\ntheir past activities, and the operator could use past information for\nreal-time decision making.\n  The basic units of our method are action tubes, representing objects and\ntheir trajectories over time. Our object-based method has advantages over frame\nbased methods, as it can handle multiple people, multiple activities for each\nperson, and can address re-identification uncertainty.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 06:03:48 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Hoshen", "Yedid", ""], ["Peleg", "Shmuel", ""]]}, {"id": "1505.05286", "submitter": "Jean-Philippe Andreu", "authors": "Jean-Philippe Andreu, Stefan Mayer, Karlheinz Gutjahr, Harald Ganster", "title": "Measuring Visibility using Atmospheric Transmission and Digital Surface\n  Model", "comments": "Presented at OAGM Workshop, 2015 (arXiv:1505.01065)", "journal-ref": null, "doi": null, "report-no": "OAGM/2015/11", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable and exact assessment of visibility is essential for safe air\ntraffic. In order to overcome the drawbacks of the currently subjective reports\nfrom human observers, we present an approach to automatically derive visibility\nmeasures by means of image processing. It first exploits image based estimation\nof the atmospheric transmission describing the portion of the light that is not\nscattered by atmospheric phenomena (e.g., haze, fog, smoke) and reaches the\ncamera. Once the atmospheric transmission is estimated, a 3D representation of\nthe vicinity (digital surface model: DMS) is used to compute depth measurements\nfor the haze-free pixels and then derive a global visibility estimation for the\nairport. Results on foggy images demonstrate the validity of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 09:02:32 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Andreu", "Jean-Philippe", ""], ["Mayer", "Stefan", ""], ["Gutjahr", "Karlheinz", ""], ["Ganster", "Harald", ""]]}, {"id": "1505.05338", "submitter": "Poorna Dasgupta", "authors": "Poorna Banerjee Dasgupta", "title": "Algorithmic Analysis of Edge Ranking and Profiling for MTF Determination\n  of an Imaging System", "comments": "3 pages, Published with International Journal of Computer Trends and\n  Technology (IJCTT), Volume-23 Number-1, 2015", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  V23(1):46-48, May 2015", "doi": "10.14445/22312803/IJCTT-V23P110", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge detection is one of the most principal techniques for detecting\ndiscontinuities in the gray levels of image pixels. The Modulation Transfer\nFunction (MTF) is one of the main criteria for assessing imaging quality and is\na parameter frequently used for measuring the sharpness of an imaging system.\nIn order to determine the MTF, it is essential to determine the best edge from\nthe target image so that an edge profile can be developed and then the line\nspread function and hence the MTF, can be computed accordingly. For regular\nimage sizes, the human visual system is adept enough to identify suitable edges\nfrom the image. But considering huge image datasets, such as those obtained\nfrom satellites, the image size may range in few gigabytes and in such a case,\nmanual inspection of images for determination of the best suitable edge is not\nplausible and hence, edge profiling tasks have to be automated. This paper\npresents a novel, yet simple, algorithm for edge ranking and detection from\nimage data-sets for MTF computation, which is ideal for automation on\nvectorised graphical processing units.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 12:12:48 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Dasgupta", "Poorna Banerjee", ""]]}, {"id": "1505.05354", "submitter": "Lianwen Jin", "authors": "Weixin Yang, Lianwen Jin, Dacheng Tao, Zecheng Xie, Ziyong Feng", "title": "DropSample: A New Training Method to Enhance Deep Convolutional Neural\n  Networks for Large-Scale Unconstrained Handwritten Chinese Character\n  Recognition", "comments": "18 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the theory of Leitners learning box from the field of psychology,\nwe propose DropSample, a new method for training deep convolutional neural\nnetworks (DCNNs), and apply it to large-scale online handwritten Chinese\ncharacter recognition (HCCR). According to the principle of DropSample, each\ntraining sample is associated with a quota function that is dynamically\nadjusted on the basis of the classification confidence given by the DCNN\nsoftmax output. After a learning iteration, samples with low confidence will\nhave a higher probability of being selected as training data in the next\niteration; in contrast, well-trained and well-recognized samples with very high\nconfidence will have a lower probability of being involved in the next training\niteration and can be gradually eliminated. As a result, the learning process\nbecomes more efficient as it progresses. Furthermore, we investigate the use of\ndomain-specific knowledge to enhance the performance of DCNN by adding a domain\nknowledge layer before the traditional CNN. By adopting DropSample together\nwith different types of domain-specific knowledge, the accuracy of HCCR can be\nimproved efficiently. Experiments on the CASIA-OLHDWB 1.0, CASIA-OLHWDB 1.1,\nand ICDAR 2013 online HCCR competition datasets yield outstanding recognition\nrates of 97.33%, 97.06%, and 97.51% respectively, all of which are\nsignificantly better than the previous best results reported in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 13:08:57 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Yang", "Weixin", ""], ["Jin", "Lianwen", ""], ["Tao", "Dacheng", ""], ["Xie", "Zecheng", ""], ["Feng", "Ziyong", ""]]}, {"id": "1505.05459", "submitter": "Hamed Sarbolandi", "authors": "Hamed Sarbolandi, Damien Lefloch, Andreas Kolb", "title": "Kinect Range Sensing: Structured-Light versus Time-of-Flight Kinect", "comments": "58 pages, 23 figures. Accepted for publication in Computer Vision and\n  Image Understanding (CVIU)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the new Kinect One has been issued by Microsoft, providing the next\ngeneration of real-time range sensing devices based on the Time-of-Flight (ToF)\nprinciple. As the first Kinect version was using a structured light approach,\none would expect various differences in the characteristics of the range data\ndelivered by both devices. This paper presents a detailed and in-depth\ncomparison between both devices. In order to conduct the comparison, we propose\na framework of seven different experimental setups, which is a generic basis\nfor evaluating range cameras such as Kinect. The experiments have been designed\nwith the goal to capture individual effects of the Kinect devices as isolatedly\nas possible and in a way, that they can also be adopted, in order to apply them\nto any other range sensing device. The overall goal of this paper is to provide\na solid insight into the pros and cons of either device. Thus, scientists that\nare interested in using Kinect range sensing cameras in their specific\napplication scenario can directly assess the expected, specific benefits and\npotential problem of either device.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 17:33:13 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Sarbolandi", "Hamed", ""], ["Lefloch", "Damien", ""], ["Kolb", "Andreas", ""]]}, {"id": "1505.05489", "submitter": "Ibrahim Almosallam", "authors": "Ibrahim A. Almosallam, Sam N. Lindsay, Matt J. Jarvis and Stephen J.\n  Roberts", "title": "A Sparse Gaussian Process Framework for Photometric Redshift Estimation", "comments": null, "journal-ref": null, "doi": "10.1093/mnras/stv2425", "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Accurate photometric redshifts are a lynchpin for many future experiments to\npin down the cosmological model and for studies of galaxy evolution. In this\nstudy, a novel sparse regression framework for photometric redshift estimation\nis presented. Simulated and real data from SDSS DR12 were used to train and\ntest the proposed models. We show that approaches which include careful data\npreparation and model design offer a significant improvement in comparison with\nseveral competing machine learning algorithms. Standard implementations of most\nregression algorithms have as the objective the minimization of the sum of\nsquared errors. For redshift inference, however, this induces a bias in the\nposterior mean of the output distribution, which can be problematic. In this\npaper we directly target minimizing $\\Delta z = (z_\\textrm{s} -\nz_\\textrm{p})/(1+z_\\textrm{s})$ and address the bias problem via a\ndistribution-based weighting scheme, incorporated as part of the optimization\nobjective. The results are compared with other machine learning algorithms in\nthe field such as Artificial Neural Networks (ANN), Gaussian Processes (GPs)\nand sparse GPs. The proposed framework reaches a mean absolute $\\Delta z =\n0.0026(1+z_\\textrm{s})$, over the redshift range of $0 \\le z_\\textrm{s} \\le 2$\non the simulated data, and $\\Delta z = 0.0178(1+z_\\textrm{s})$ over the entire\nredshift range on the SDSS DR12 survey, outperforming the standard ANNz used in\nthe literature. We also investigate how the relative size of the training set\naffects the photometric redshift accuracy. We find that a training set of\n\\textgreater 30 per cent of total sample size, provides little additional\nconstraint on the photometric redshifts, and note that our GP formalism\nstrongly outperforms ANNz in the sparse data regime for the simulated data set.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 19:08:54 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2015 22:46:51 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2015 16:43:19 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Almosallam", "Ibrahim A.", ""], ["Lindsay", "Sam N.", ""], ["Jarvis", "Matt J.", ""], ["Roberts", "Stephen J.", ""]]}, {"id": "1505.05561", "submitter": "Devansh Arpit", "authors": "Devansh Arpit, Yingbo Zhou, Hung Ngo, Venu Govindaraju", "title": "Why Regularized Auto-Encoders learn Sparse Representation?", "comments": "8 pages of content, 1 page of reference, 4 pages of supplementary.\n  ICML 2016; bug fix in lemma 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the authors of Batch Normalization (BN) identify and address an\nimportant problem involved in training deep networks-- \\textit{Internal\nCovariate Shift}-- the current solution has certain drawbacks. For instance, BN\ndepends on batch statistics for layerwise input normalization during training\nwhich makes the estimates of mean and standard deviation of input\n(distribution) to hidden layers inaccurate due to shifting parameter values\n(especially during initial training epochs). Another fundamental problem with\nBN is that it cannot be used with batch-size $ 1 $ during training. We address\nthese drawbacks of BN by proposing a non-adaptive normalization technique for\nremoving covariate shift, that we call \\textit{Normalization Propagation}. Our\napproach does not depend on batch statistics, but rather uses a\ndata-independent parametric estimate of mean and standard-deviation in every\nlayer thus being computationally faster compared with BN. We exploit the\nobservation that the pre-activation before Rectified Linear Units follow\nGaussian distribution in deep networks, and that once the first and second\norder statistics of any given dataset are normalized, we can forward propagate\nthis normalization without the need for recalculating the approximate\nstatistics for hidden layers.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 00:10:46 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 19:22:37 GMT"}, {"version": "v3", "created": "Wed, 2 Mar 2016 15:29:29 GMT"}, {"version": "v4", "created": "Mon, 23 May 2016 23:04:21 GMT"}, {"version": "v5", "created": "Fri, 17 Jun 2016 23:01:20 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Arpit", "Devansh", ""], ["Zhou", "Yingbo", ""], ["Ngo", "Hung", ""], ["Govindaraju", "Venu", ""]]}, {"id": "1505.05601", "submitter": "S L Happy", "authors": "S L Happy, Swarnadip Chatterjee, and Debdoot Sheet", "title": "Unsupervised Segmentation of Overlapping Cervical Cell Cytoplasm", "comments": "2 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overlapping of cervical cells and poor contrast of cell cytoplasm are the\nmajor issues in accurate detection and segmentation of cervical cells. An\nunsupervised cell segmentation approach is presented here. Cell clump\nsegmentation was carried out using the extended depth of field (EDF) image\ncreated from the images of different focal planes. A modified Otsu method with\nprior class weights is proposed for accurate segmentation of nuclei from the\ncell clumps. The cell cytoplasm was further segmented from cell clump depending\nupon the number of nucleus detected in that cell clump. Level set model was\nused for cytoplasm segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 04:24:48 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Happy", "S L", ""], ["Chatterjee", "Swarnadip", ""], ["Sheet", "Debdoot", ""]]}, {"id": "1505.05612", "submitter": "Junhua Mao", "authors": "Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, Wei Xu", "title": "Are You Talking to a Machine? Dataset and Methods for Multilingual Image\n  Question Answering", "comments": "Dataset released on the project page, see\n  http://idl.baidu.com/FM-IQA.html ; NIPS 2015 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the mQA model, which is able to answer questions\nabout the content of an image. The answer can be a sentence, a phrase or a\nsingle word. Our model contains four components: a Long Short-Term Memory\n(LSTM) to extract the question representation, a Convolutional Neural Network\n(CNN) to extract the visual representation, an LSTM for storing the linguistic\ncontext in an answer, and a fusing component to combine the information from\nthe first three components and generate the answer. We construct a Freestyle\nMultilingual Image Question Answering (FM-IQA) dataset to train and evaluate\nour mQA model. It contains over 150,000 images and 310,000 freestyle Chinese\nquestion-answer pairs and their English translations. The quality of the\ngenerated answers of our mQA model on this dataset is evaluated by human judges\nthrough a Turing Test. Specifically, we mix the answers provided by humans and\nour model. The human judges need to distinguish our model from the human. They\nwill also provide a score (i.e. 0, 1, 2, the larger the better) indicating the\nquality of the answer. We propose strategies to monitor the quality of this\nevaluation process. The experiments show that in 64.7% of cases, the human\njudges cannot distinguish our model from humans. The average score is 1.454\n(1.918 for human). The details of this work, including the FM-IQA dataset, can\nbe found on the project page: http://idl.baidu.com/FM-IQA.html\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 06:09:36 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 07:45:46 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2015 21:12:15 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Gao", "Haoyuan", ""], ["Mao", "Junhua", ""], ["Zhou", "Jie", ""], ["Huang", "Zhiheng", ""], ["Wang", "Lei", ""], ["Xu", "Wei", ""]]}, {"id": "1505.05641", "submitter": "Hao Su", "authors": "Hao Su, Charles R. Qi, Yangyan Li, Leonidas Guibas", "title": "Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with\n  Rendered 3D Model Views", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object viewpoint estimation from 2D images is an essential task in computer\nvision. However, two issues hinder its progress: scarcity of training data with\nviewpoint annotations, and a lack of powerful features. Inspired by the growing\navailability of 3D models, we propose a framework to address both issues by\ncombining render-based image synthesis and CNNs. We believe that 3D models have\nthe potential in generating a large number of images of high variation, which\ncan be well exploited by deep CNN with a high learning capacity. Towards this\ngoal, we propose a scalable and overfit-resistant image synthesis pipeline,\ntogether with a novel CNN specifically tailored for the viewpoint estimation\ntask. Experimentally, we show that the viewpoint estimation from our pipeline\ncan significantly outperform state-of-the-art methods on PASCAL 3D+ benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 08:16:06 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Su", "Hao", ""], ["Qi", "Charles R.", ""], ["Li", "Yangyan", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1505.05643", "submitter": "Johann Prankl", "authors": "Aitor Aldoma, Johann Prankl, Alexander Svejda and Markus Vincze", "title": "Object Modelling with a Handheld RGB-D Camera", "comments": "Presented at OAGM Workshop, 2015 (arXiv:1505.01065)", "journal-ref": null, "doi": null, "report-no": "OAGM/2015/08", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a flexible system to reconstruct 3D models of objects\ncaptured with an RGB-D sensor. A major advantage of the method is that our\nreconstruction pipeline allows the user to acquire a full 3D model of the\nobject. This is achieved by acquiring several partial 3D models in different\nsessions that are automatically merged together to reconstruct a full model. In\naddition, the 3D models acquired by our system can be directly used by\nstate-of-the-art object instance recognition and object tracking modules,\nproviding object-perception capabilities for different applications, such as\nhuman-object interaction analysis or robot grasping. The system does not impose\nconstraints in the appearance of objects (textured, untextured) nor in the\nmodelling setup (moving camera with static object or a turn-table setup). The\nproposed reconstruction system has been used to model a large number of objects\nresulting in metrically accurate and visually appealing 3D models.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 08:24:21 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Aldoma", "Aitor", ""], ["Prankl", "Johann", ""], ["Svejda", "Alexander", ""], ["Vincze", "Markus", ""]]}, {"id": "1505.05740", "submitter": "S\\'ebastien Adam", "authors": "Julien Lerouge, Zeina Abu-Aisheh, Romain Raveaux, Pierre H\\'eroux, and\n  S\\'ebastien Adam", "title": "Graph edit distance : a new binary linear programming formulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph edit distance (GED) is a powerful and flexible graph matching paradigm\nthat can be used to address different tasks in structural pattern recognition,\nmachine learning, and data mining. In this paper, some new binary linear\nprogramming formulations for computing the exact GED between two graphs are\nproposed. A major strength of the formulations lies in their genericity since\nthe GED can be computed between directed or undirected fully attributed graphs\n(i.e. with attributes on both vertices and edges). Moreover, a relaxation of\nthe domain constraints in the formulations provides efficient lower bound\napproximations of the GED. A complete experimental study comparing the proposed\nformulations with 4 state-of-the-art algorithms for exact and approximate graph\nedit distances is provided. By considering both the quality of the proposed\nsolution and the efficiency of the algorithms as performance criteria, the\nresults show that none of the compared methods dominates the others in the\nPareto sense. As a consequence, faced to a given real-world problem, a\ntrade-off between quality and efficiency has to be chosen w.r.t. the\napplication constraints. In this context, this paper provides a guide that can\nbe used to choose the appropriate method.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 13:57:40 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Lerouge", "Julien", ""], ["Abu-Aisheh", "Zeina", ""], ["Raveaux", "Romain", ""], ["H\u00e9roux", "Pierre", ""], ["Adam", "S\u00e9bastien", ""]]}, {"id": "1505.05753", "submitter": "Mario Fritz", "authors": "Iaroslav Shcherbatyi, Andreas Bulling, Mario Fritz", "title": "GazeDPM: Early Integration of Gaze Information in Deformable Part Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of works explore collaborative human-computer systems in\nwhich human gaze is used to enhance computer vision systems. For object\ndetection these efforts were so far restricted to late integration approaches\nthat have inherent limitations, such as increased precision without increase in\nrecall. We propose an early integration approach in a deformable part model,\nwhich constitutes a joint formulation over gaze and visual data. We show that\nour GazeDPM method improves over the state-of-the-art DPM baseline by 4% and a\nrecent method for gaze-supported object detection by 3% on the public POET\ndataset. Our approach additionally provides introspection of the learnt models,\ncan reveal salient image structures, and allows us to investigate the interplay\nbetween gaze attracting and repelling areas, the importance of view-specific\nmodels, as well as viewers' personal biases in gaze patterns. We finally study\nimportant practical aspects of our approach, such as the impact of using\nsaliency maps instead of real fixations, the impact of the number of fixations,\nas well as robustness to gaze estimation error.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 14:39:51 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Shcherbatyi", "Iaroslav", ""], ["Bulling", "Andreas", ""], ["Fritz", "Mario", ""]]}, {"id": "1505.05769", "submitter": "Ishan Misra", "authors": "Ishan Misra, Abhinav Shrivastava, Martial Hebert", "title": "Watch and Learn: Semi-Supervised Learning of Object Detectors from\n  Videos", "comments": "To appear in CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a semi-supervised approach that localizes multiple unknown object\ninstances in long videos. We start with a handful of labeled boxes and\niteratively learn and label hundreds of thousands of object instances. We\npropose criteria for reliable object detection and tracking for constraining\nthe semi-supervised learning process and minimizing semantic drift. Our\napproach does not assume exhaustive labeling of each object instance in any\nsingle frame, or any explicit annotation of negative data. Working in such a\ngeneric setting allow us to tackle multiple object instances in video, many of\nwhich are static. In contrast, existing approaches either do not consider\nmultiple object instances per video, or rely heavily on the motion of the\nobjects present. The experiments demonstrate the effectiveness of our approach\nby evaluating the automatically labeled data on a variety of metrics like\nquality, coverage (recall), diversity, and relevance to training an object\ndetector.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 15:34:30 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Misra", "Ishan", ""], ["Shrivastava", "Abhinav", ""], ["Hebert", "Martial", ""]]}, {"id": "1505.05819", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu", "title": "New HSL Distance Based Colour Clustering Algorithm", "comments": "The 24th Midwest Artificial Intelligence and Cognitive Sciences\n  Conference (MAICS 2013), pp. 85-92, New Albany, Indiana. USA, April 13-14,\n  2013", "journal-ref": null, "doi": "10.13140/2.1.4990.8007", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we define a distance for the HSL colour system. Next, the\nproposed distance is used for a fuzzy colour clustering algorithm construction.\nThe presented algorithm is related to the well-known fuzzy c-means algorithm.\nFinally, the clustering algorithm is used as colour reduction method. The\nobtained experimental results are presented to demonstrate the effectiveness of\nour approach.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 13:23:58 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Patrascu", "Vasile", ""]]}, {"id": "1505.05836", "submitter": "Aroma Mahendru", "authors": "Neelima Chavali, Harsh Agrawal, Aroma Mahendru, Dhruv Batra", "title": "Object-Proposal Evaluation Protocol is 'Gameable'", "comments": "15 pages, 11 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object proposals have quickly become the de-facto pre-processing step in a\nnumber of vision pipelines (for object detection, object discovery, and other\ntasks). Their performance is usually evaluated on partially annotated datasets.\nIn this paper, we argue that the choice of using a partially annotated dataset\nfor evaluation of object proposals is problematic -- as we demonstrate via a\nthought experiment, the evaluation protocol is 'gameable', in the sense that\nprogress under this protocol does not necessarily correspond to a \"better\"\ncategory independent object proposal algorithm.\n  To alleviate this problem, we: (1) Introduce a nearly-fully annotated version\nof PASCAL VOC dataset, which serves as a test-bed to check if object proposal\ntechniques are overfitting to a particular list of categories. (2) Perform an\nexhaustive evaluation of object proposal methods on our introduced nearly-fully\nannotated PASCAL dataset and perform cross-dataset generalization experiments;\nand (3) Introduce a diagnostic experiment to detect the bias capacity in an\nobject proposal algorithm. This tool circumvents the need to collect a densely\nannotated dataset, which can be expensive and cumbersome to collect. Finally,\nwe plan to release an easy-to-use toolbox which combines various publicly\navailable implementations of object proposal algorithms which standardizes the\nproposal generation and evaluation so that new methods can be added and\nevaluated on different datasets. We hope that the results presented in the\npaper will motivate the community to test the category independence of various\nobject proposal methods by carefully choosing the evaluation protocol.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 18:53:45 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 04:23:57 GMT"}, {"version": "v3", "created": "Mon, 25 May 2015 05:14:53 GMT"}, {"version": "v4", "created": "Mon, 23 Nov 2015 07:16:16 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Chavali", "Neelima", ""], ["Agrawal", "Harsh", ""], ["Mahendru", "Aroma", ""], ["Batra", "Dhruv", ""]]}, {"id": "1505.05901", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani and George Atia", "title": "Randomized Robust Subspace Recovery for High Dimensional Data Matrices", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing ( Volume: 65, Issue: 6,\n  March15, 15 2017 )", "doi": "10.1109/TSP.2016.2645515", "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores and analyzes two randomized designs for robust Principal\nComponent Analysis (PCA) employing low-dimensional data sketching. In one\ndesign, a data sketch is constructed using random column sampling followed by\nlow dimensional embedding, while in the other, sketching is based on random\ncolumn and row sampling. Both designs are shown to bring about substantial\nsavings in complexity and memory requirements for robust subspace learning over\nconventional approaches that use the full scale data. A characterization of the\nsample and computational complexity of both designs is derived in the context\nof two distinct outlier models, namely, sparse and independent outlier models.\nThe proposed randomized approach can provably recover the correct subspace with\ncomputational and sample complexity that are almost independent of the size of\nthe data. The results of the mathematical analysis are confirmed through\nnumerical simulations using both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 21:04:33 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 22:25:06 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1505.05914", "submitter": "Huijuan Xu", "authors": "Huijuan Xu, Subhashini Venugopalan, Vasili Ramanishka, Marcus\n  Rohrbach, Kate Saenko", "title": "A Multi-scale Multiple Instance Video Description Network", "comments": "ICCV15 workshop on Closing the Loop Between Vision and Language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating natural language descriptions for in-the-wild videos is a\nchallenging task. Most state-of-the-art methods for solving this problem borrow\nexisting deep convolutional neural network (CNN) architectures (AlexNet,\nGoogLeNet) to extract a visual representation of the input video. However,\nthese deep CNN architectures are designed for single-label centered-positioned\nobject classification. While they generate strong semantic features, they have\nno inherent structure allowing them to detect multiple objects of different\nsizes and locations in the frame. Our paper tries to solve this problem by\nintegrating the base CNN into several fully convolutional neural networks\n(FCNs) to form a multi-scale network that handles multiple receptive field\nsizes in the original image. FCNs, previously applied to image segmentation,\ncan generate class heat-maps efficiently compared to sliding window mechanisms,\nand can easily handle multiple scales. To further handle the ambiguity over\nmultiple objects and locations, we incorporate the Multiple Instance Learning\nmechanism (MIL) to consider objects in different positions and at different\nscales simultaneously. We integrate our multi-scale multi-instance architecture\nwith a sequence-to-sequence recurrent neural network to generate sentence\ndescriptions based on the visual representation. Ours is the first end-to-end\ntrainable architecture that is capable of multi-scale region processing.\nEvaluation on a Youtube video dataset shows the advantage of our approach\ncompared to the original single-scale whole frame CNN model. Our flexible and\nefficient architecture can potentially be extended to support other video\nprocessing tasks.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 21:47:08 GMT"}, {"version": "v2", "created": "Mon, 25 May 2015 16:28:56 GMT"}, {"version": "v3", "created": "Sat, 19 Mar 2016 02:27:58 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Xu", "Huijuan", ""], ["Venugopalan", "Subhashini", ""], ["Ramanishka", "Vasili", ""], ["Rohrbach", "Marcus", ""], ["Saenko", "Kate", ""]]}, {"id": "1505.05916", "submitter": "Erroll Wood", "authors": "Erroll Wood, Tadas Baltrusaitis, Xucong Zhang, Yusuke Sugano, Peter\n  Robinson, and Andreas Bulling", "title": "Rendering of Eyes for Eye-Shape Registration and Gaze Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images of the eye are key in several computer vision problems, such as shape\nregistration and gaze estimation. Recent large-scale supervised methods for\nthese problems require time-consuming data collection and manual annotation,\nwhich can be unreliable. We propose synthesizing perfectly labelled\nphoto-realistic training data in a fraction of the time. We used computer\ngraphics techniques to build a collection of dynamic eye-region models from\nhead scan geometry. These were randomly posed to synthesize close-up eye images\nfor a wide range of head poses, gaze directions, and illumination conditions.\nWe used our model's controllability to verify the importance of realistic\nillumination and shape variations in eye-region training data. Finally, we\ndemonstrate the benefits of our synthesized training data (SynthesEyes) by\nout-performing state-of-the-art methods for eye-shape registration as well as\ncross-dataset appearance-based gaze estimation in the wild.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 22:12:31 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Wood", "Erroll", ""], ["Baltrusaitis", "Tadas", ""], ["Zhang", "Xucong", ""], ["Sugano", "Yusuke", ""], ["Robinson", "Peter", ""], ["Bulling", "Andreas", ""]]}, {"id": "1505.05957", "submitter": "Tianmin Shu", "authors": "Tianmin Shu, Dan Xie, Brandon Rothrock, Sinisa Todorovic, Song-Chun\n  Zhu", "title": "Joint Inference of Groups, Events and Human Roles in Aerial Videos", "comments": "CVPR 2015 Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of drones, aerial video analysis becomes increasingly\nimportant; yet, it has received scant attention in the literature. This paper\naddresses a new problem of parsing low-resolution aerial videos of large\nspatial areas, in terms of 1) grouping, 2) recognizing events and 3) assigning\nroles to people engaged in events. We propose a novel framework aimed at\nconducting joint inference of the above tasks, as reasoning about each in\nisolation typically fails in our setting. Given noisy tracklets of people and\ndetections of large objects and scene surfaces (e.g., building, grass), we use\na spatiotemporal AND-OR graph to drive our joint inference, using Markov Chain\nMonte Carlo and dynamic programming. We also introduce a new formalism of\nspatiotemporal templates characterizing latent sub-events. For evaluation, we\nhave collected and released a new aerial videos dataset using a hex-rotor\nflying over picnic areas rich with group events. Our results demonstrate that\nwe successfully address above inference tasks under challenging conditions.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 05:59:18 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Shu", "Tianmin", ""], ["Xie", "Dan", ""], ["Rothrock", "Brandon", ""], ["Todorovic", "Sinisa", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1505.06027", "submitter": "Piotr Bojanowski", "authors": "Piotr Bojanowski (WILLOW, LIENS), R\\'emi Lajugie (LIENS, SIERRA),\n  Edouard Grave (APAM), Francis Bach (LIENS, SIERRA), Ivan Laptev (WILLOW,\n  LIENS), Jean Ponce (WILLOW, LIENS), Cordelia Schmid (LEAR)", "title": "Weakly-Supervised Alignment of Video With Text", "comments": "ICCV 2015 - IEEE International Conference on Computer Vision, Dec\n  2015, Santiago, Chile", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we are given a set of videos, along with natural language\ndescriptions in the form of multiple sentences (e.g., manual annotations, movie\nscripts, sport summaries etc.), and that these sentences appear in the same\ntemporal order as their visual counterparts. We propose in this paper a method\nfor aligning the two modalities, i.e., automatically providing a time stamp for\nevery sentence. Given vectorial features for both video and text, we propose to\ncast this task as a temporal assignment problem, with an implicit linear\nmapping between the two feature modalities. We formulate this problem as an\ninteger quadratic program, and solve its continuous convex relaxation using an\nefficient conditional gradient algorithm. Several rounding procedures are\nproposed to construct the final integer solution. After demonstrating\nsignificant improvements over the state of the art on the related task of\naligning video with symbolic labels [7], we evaluate our method on a\nchallenging dataset of videos with associated textual descriptions [36], using\nboth bag-of-words and continuous representations for text.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 11:08:39 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2015 14:57:40 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Bojanowski", "Piotr", "", "WILLOW, LIENS"], ["Lajugie", "R\u00e9mi", "", "LIENS, SIERRA"], ["Grave", "Edouard", "", "APAM"], ["Bach", "Francis", "", "LIENS, SIERRA"], ["Laptev", "Ivan", "", "WILLOW,\n  LIENS"], ["Ponce", "Jean", "", "WILLOW, LIENS"], ["Schmid", "Cordelia", "", "LEAR"]]}, {"id": "1505.06072", "submitter": "Pedro Felzenszwalb", "authors": "Pedro F. Felzenszwalb, Benar F. Svaiter", "title": "Diffusion Methods for Classification with Pairwise Relationships", "comments": "To appear in the Quarterly of Applied Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define two algorithms for propagating information in classification\nproblems with pairwise relationships. The algorithms are based on contraction\nmaps and are related to non-linear diffusion and random walks on graphs. The\napproach is also related to message passing algorithms, including belief\npropagation and mean field methods. The algorithms we describe are guaranteed\nto converge on graphs with arbitrary topology. Moreover they always converge to\na unique fixed point, independent of initialization. We prove that the fixed\npoints of the algorithms under consideration define lower-bounds on the energy\nfunction and the max-marginals of a Markov random field. The theoretical\nresults also illustrate a relationship between message passing algorithms and\nvalue iteration for an infinite horizon Markov decision process. We illustrate\nthe practical application of the algorithms under study with numerical\nexperiments in image restoration, stereo depth estimation and binary\nclassification on a grid.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 13:36:58 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2015 14:37:37 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2015 14:10:08 GMT"}, {"version": "v4", "created": "Tue, 14 May 2019 18:10:26 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Felzenszwalb", "Pedro F.", ""], ["Svaiter", "Benar F.", ""]]}, {"id": "1505.06079", "submitter": "Andrea Fusiello", "authors": "Federica Arrigoni, Andrea Fusiello, Beatrice Rossi, Pasqualina\n  Fragneto", "title": "Robust Rotation Synchronization via Low-rank and Sparse Matrix\n  Decomposition", "comments": "The material contained in this paper is part of a manuscript\n  submitted to CVIU", "journal-ref": "In Computer Vision and Image Understanding, 174: 95-113, 2018", "doi": "10.1016/j.cviu.2018.08.001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the rotation synchronization problem, which arises in\nglobal registration of 3D point-sets and in structure from motion. The problem\nis formulated in an unprecedented way as a \"low-rank and sparse\" matrix\ndecomposition that handles both outliers and missing data. A minimization\nstrategy, dubbed R-GoDec, is also proposed and evaluated experimentally against\nstate-of-the-art algorithms on simulated and real data. The results show that\nR-GoDec is the fastest among the robust algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 13:48:10 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 13:58:54 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Arrigoni", "Federica", ""], ["Fusiello", "Andrea", ""], ["Rossi", "Beatrice", ""], ["Fragneto", "Pasqualina", ""]]}, {"id": "1505.06162", "submitter": "Anjith George", "authors": "Anjith George, Aurobinda Routray", "title": "Design and Implementation of Real-time Algorithms for Eye Tracking and\n  PERCLOS Measurement for on board Estimation of Alertness of Drivers", "comments": "Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alertness level of drivers can be estimated with the use of computer\nvision based methods. The level of fatigue can be found from the value of\nPERCLOS. It is the ratio of closed eye frames to the total frames processed.\nThe main objective of the thesis is the design and implementation of real-time\nalgorithms for measurement of PERCLOS. In this work we have developed a\nreal-time system which is able to process the video onboard and to alarm the\ndriver in case the driver is in alert. For accurate estimation of PERCLOS the\nframe rate should be greater than 4 and accuracy should be greater than 90%.\nFor eye detection we have used mainly two approaches Haar classifier based\nmethod and Principal Component Analysis (PCA) based method for day time. During\nnight time active Near Infra Red (NIR) illumination is used. Local Binary\nPattern (LBP) histogram based method is used for the detection of eyes at night\ntime. The accuracy rate of the algorithms was found to be more than 90% at\nframe rates more than 5 fps which was suitable for the application.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 17:51:53 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["George", "Anjith", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1505.06163", "submitter": "Yong Chul Ju", "authors": "Yong Chul Ju, Daniel Maurer, Michael Breu\\ss, Andr\\'es Bruhn", "title": "Direct Variational Perspective Shape from Shading with Cartesian Depth\n  Parametrisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of today's state-of-the-art methods for perspective shape from shading\nare modelled in terms of partial differential equations (PDEs) of\nHamilton-Jacobi type. To improve the robustness of such methods w.r.t. noise\nand missing data, first approaches have recently been proposed that seek to\nembed the underlying PDE into a variational framework with data and smoothness\nterm. So far, however, such methods either make use of a radial depth\nparametrisation that makes the regularisation hard to interpret from a\ngeometrical viewpoint or they consider indirect smoothness terms that require\nadditional consistency constraints to provide valid solutions. Moreover the\nminimisation of such frameworks is an intricate task, since the underlying\nenergy is typically non-convex. In our paper we address all three of the\naforementioned issues. First, we propose a novel variational model that\noperates directly on the Cartesian depth. In this context, we also point out a\ncommon mistake in the derivation of the surface normal. Moreover, we employ a\ndirect second-order regulariser with edge-preservation property. This direct\nregulariser yields by construction valid solutions without requiring additional\nconsistency constraints. Finally, we also propose a novel coarse-to-fine\nminimisation framework based on an alternating explicit scheme. This framework\nallows us to avoid local minima during the minimisation and thus to improve the\naccuracy of the reconstruction. Experiments show the good quality of our model\nas well as the usefulness of the proposed numerical scheme.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 17:52:11 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2015 11:03:25 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Ju", "Yong Chul", ""], ["Maurer", "Daniel", ""], ["Breu\u00df", "Michael", ""], ["Bruhn", "Andr\u00e9s", ""]]}, {"id": "1505.06219", "submitter": "Sabyasachi  Mukhopadhyay", "authors": "Sabyasachi Mukhopadhyay, Soham Mandal, Sawon Pratiher, Satyasaran\n  Changdar, Ritwik Burman, Nirmalya Ghosh, Prasanta K. Panigrahi", "title": "A comparative study between proposed Hyper Kurtosis based Modified\n  Duo-Histogram Equalization (HKMDHE) and Contrast Limited Adaptive Histogram\n  Equalization (CLAHE) for Contrast Enhancement Purpose of Low Contrast Human\n  Brain CT scan images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a comparative study between proposed hyper kurtosis based\nmodified duo-histogram equalization (HKMDHE) algorithm and contrast limited\nadaptive histogram enhancement (CLAHE) has been presented for the\nimplementation of contrast enhancement and brightness preservation of low\ncontrast human brain CT scan images. In HKMDHE algorithm, contrast enhancement\nis done on the hyper-kurtosis based application. The results are very promising\nof proposed HKMDHE technique with improved PSNR values and lesser AMMBE values\nthan CLAHE technique.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 01:26:06 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Mukhopadhyay", "Sabyasachi", ""], ["Mandal", "Soham", ""], ["Pratiher", "Sawon", ""], ["Changdar", "Satyasaran", ""], ["Burman", "Ritwik", ""], ["Ghosh", "Nirmalya", ""], ["Panigrahi", "Prasanta K.", ""]]}, {"id": "1505.06236", "submitter": "Le Lu", "authors": "Amal Farag, Le Lu, Holger R. Roth, Jiamin Liu, Evrim Turkbey, Ronald\n  M. Summers", "title": "A Bottom-up Approach for Pancreas Segmentation using Cascaded\n  Superpixels and (Deep) Image Patch Labeling", "comments": "14 pages, 14 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Robust automated organ segmentation is a prerequisite for computer-aided\ndiagnosis (CAD), quantitative imaging analysis and surgical assistance. For\nhigh-variability organs such as the pancreas, previous approaches report\nundesirably low accuracies. We present a bottom-up approach for pancreas\nsegmentation in abdominal CT scans that is based on a hierarchy of information\npropagation by classifying image patches at different resolutions; and\ncascading superpixels. There are four stages: 1) decomposing CT slice images as\na set of disjoint boundary-preserving superpixels; 2) computing pancreas class\nprobability maps via dense patch labeling; 3) classifying superpixels by\npooling both intensity and probability features to form empirical statistics in\ncascaded random forest frameworks; and 4) simple connectivity based\npost-processing. The dense image patch labeling are conducted by: efficient\nrandom forest classifier on image histogram, location and texture features; and\nmore expensive (but with better specificity) deep convolutional neural network\nclassification on larger image windows (with more spatial contexts). Evaluation\nof the approach is performed on a database of 80 manually segmented CT volumes\nin six-fold cross-validation (CV). Our achieved results are comparable, or\nbetter than the state-of-the-art methods (evaluated by\n\"leave-one-patient-out\"), with Dice 70.7% and Jaccard 57.9%. The computational\nefficiency has been drastically improved in the order of 6~8 minutes, comparing\nwith others of ~10 hours per case. Finally, we implement a multi-atlas label\nfusion (MALF) approach for pancreas segmentation using the same datasets. Under\nsix-fold CV, our bottom-up segmentation method significantly outperforms its\nMALF counterpart: (70.7 +/- 13.0%) versus (52.5 +/- 20.8%) in Dice. Deep CNN\npatch labeling confidences offer more numerical stability, reflected by smaller\nstandard deviations.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 21:59:45 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 18:24:43 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Farag", "Amal", ""], ["Lu", "Le", ""], ["Roth", "Holger R.", ""], ["Liu", "Jiamin", ""], ["Turkbey", "Evrim", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1505.06237", "submitter": "Gerhard Paar", "authors": "Arnold Bauer, Karlheinz Gutjahr, Gerhard Paar, Heiner Kontrus and\n  Robert Glatzl", "title": "Tunnel Surface 3D Reconstruction from Unoriented Image Sequences", "comments": "Presented at OAGM Workshop, 2015 (arXiv:1505.01065)", "journal-ref": null, "doi": null, "report-no": "OAGM/2015/10", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3D documentation of the tunnel surface during construction requires fast\nand robust measurement systems. In the solution proposed in this paper, during\ntunnel advance a single camera is taking pictures of the tunnel surface from\nseveral positions. The recorded images are automatically processed to gain a 3D\ntunnel surface model. Image acquisition is realized by the\ntunneling/advance/driving personnel close to the tunnel face (= the front end\nof the advance). Based on the following fully automatic analysis/evaluation, a\ndecision on the quality of the outbreak can be made within a few minutes. This\npaper describes the image recording system and conditions as well as the\nstereo-photogrammetry based workflow for the continuously merged dense 3D\nreconstruction of the entire advance region. Geo-reference is realized by means\nof signalized targets that are automatically detected in the images. We report\non the results of recent testing under real construction conditions, and\nconclude with prospects for further development in terms of on-site\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 22:00:55 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Bauer", "Arnold", ""], ["Gutjahr", "Karlheinz", ""], ["Paar", "Gerhard", ""], ["Kontrus", "Heiner", ""], ["Glatzl", "Robert", ""]]}, {"id": "1505.06250", "submitter": "George Toderici", "authors": "Balakrishnan Varadarajan and George Toderici and Sudheendra\n  Vijayanarasimhan and Apostol Natsev", "title": "Efficient Large Scale Video Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video classification has advanced tremendously over the recent years. A large\npart of the improvements in video classification had to do with the work done\nby the image classification community and the use of deep convolutional\nnetworks (CNNs) which produce competitive results with hand- crafted motion\nfeatures. These networks were adapted to use video frames in various ways and\nhave yielded state of the art classification results. We present two methods\nthat build on this work, and scale it up to work with millions of videos and\nhundreds of thousands of classes while maintaining a low computational cost. In\nthe context of large scale video processing, training CNNs on video frames is\nextremely time consuming, due to the large number of frames involved. We\npropose to avoid this problem by training CNNs on either YouTube thumbnails or\nFlickr images, and then using these networks' outputs as features for other\nhigher level classifiers. We discuss the challenges of achieving this and\npropose two models for frame-level and video-level classification. The first is\na highly efficient mixture of experts while the latter is based on long short\nterm memory neural networks. We present results on the Sports-1M video dataset\n(1 million videos, 487 classes) and on a new dataset which has 12 million\nvideos and 150,000 labels.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 23:45:32 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Varadarajan", "Balakrishnan", ""], ["Toderici", "George", ""], ["Vijayanarasimhan", "Sudheendra", ""], ["Natsev", "Apostol", ""]]}, {"id": "1505.06319", "submitter": "Samuel De Sousa", "authors": "Samuel de Sousa and Walter G. Kropatsch", "title": "The Minimum Spanning Tree of Maximum Entropy", "comments": "Presented at OAGM Workshop, 2015 (arXiv:1505.01065)", "journal-ref": null, "doi": null, "report-no": "OAGM/2015/15", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision, we have the problem of creating graphs out of\nunstructured point-sets, i.e. the data graph. A common approach for this\nproblem consists of building a triangulation which might not always lead to the\nbest solution. Small changes in the location of the points might generate\ngraphs with unstable configurations and the topology of the graph could change\nsignificantly. After building the data-graph, one could apply Graph Matching\ntechniques to register the original point-sets. In this paper, we propose a\ndata graph technique based on the Minimum Spanning Tree of Maximum Entropty\n(MSTME). We aim at a data graph construction which could be more stable than\nthe Delaunay triangulation with respect to small variations in the neighborhood\nof points. Our technique aims at creating data graphs which could help the\npoint-set registration process. We propose an algorithm with a single free\nparameter that weighs the importance between the total weight cost and the\nentropy of the current spanning tree. We compare our algorithm on a number of\ndifferent databases with the Delaunay triangulation.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 12:49:30 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["de Sousa", "Samuel", ""], ["Kropatsch", "Walter G.", ""]]}, {"id": "1505.06389", "submitter": "Ting Liu", "authors": "Ting Liu, Mojtaba Seyedhosseini, Tolga Tasdizen", "title": "Image Segmentation Using Hierarchical Merge Tree", "comments": null, "journal-ref": "IEEE.Trans.Image.Processing 25 (2016) 4596-4607", "doi": "10.1109/TIP.2016.2592704", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates one of the most fundamental computer vision problems:\nimage segmentation. We propose a supervised hierarchical approach to\nobject-independent image segmentation. Starting with over-segmenting\nsuperpixels, we use a tree structure to represent the hierarchy of region\nmerging, by which we reduce the problem of segmenting image regions to finding\na set of label assignment to tree nodes. We formulate the tree structure as a\nconstrained conditional model to associate region merging with likelihoods\npredicted using an ensemble boundary classifier. Final segmentations can then\nbe inferred by finding globally optimal solutions to the model efficiently. We\nalso present an iterative training and testing algorithm that generates various\ntree structures and combines them to emphasize accurate boundaries by\nsegmentation accumulation. Experiment results and comparisons with other very\nrecent methods on six public data sets demonstrate that our approach achieves\nthe state-of-the-art region accuracy and is very competitive in image\nsegmentation without semantic priors.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2015 00:22:09 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2016 16:00:20 GMT"}, {"version": "v3", "created": "Sun, 31 Jul 2016 22:14:45 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Liu", "Ting", ""], ["Seyedhosseini", "Mojtaba", ""], ["Tasdizen", "Tolga", ""]]}, {"id": "1505.06531", "submitter": "Tsu-Wei Chen", "authors": "Tsu-Wei Chen, Meena Abdelmaseeh, Daniel Stashuk", "title": "Affine and Regional Dynamic Time Warpng", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pointwise matches between two time series are of great importance in time\nseries analysis, and dynamic time warping (DTW) is known to provide generally\nreasonable matches. There are situations where time series alignment should be\ninvariant to scaling and offset in amplitude or where local regions of the\nconsidered time series should be strongly reflected in pointwise matches. Two\ndifferent variants of DTW, affine DTW (ADTW) and regional DTW (RDTW), are\nproposed to handle scaling and offset in amplitude and provide regional\nemphasis respectively. Furthermore, ADTW and RDTW can be combined in two\ndifferent ways to generate alignments that incorporate advantages from both\nmethods, where the affine model can be applied either globally to the entire\ntime series or locally to each region. The proposed alignment methods\noutperform DTW on specific simulated datasets, and one-nearest-neighbor\nclassifiers using their associated difference measures are competitive with the\ndifference measures associated with state-of-the-art alignment methods on real\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 03:23:31 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Chen", "Tsu-Wei", ""], ["Abdelmaseeh", "Meena", ""], ["Stashuk", "Daniel", ""]]}, {"id": "1505.06578", "submitter": "Kunal Narayan Chaudhury", "authors": "Kollipara Rithwik and Kunal Narayan Chaudhury", "title": "A Simple Yet Effective Improvement to the Bilateral Filter for Image\n  Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bilateral filter has diverse applications in image processing, computer\nvision, and computational photography. In particular, this non-linear filter is\nquite effective in denoising images corrupted with additive Gaussian noise. The\nfilter, however, is known to perform poorly at large noise levels. Several\nadaptations of the filter have been proposed in the literature to address this\nshortcoming, but often at an added computational cost. In this paper, we report\na simple yet effective modification that improves the denoising performance of\nthe bilateral filter at almost no additional cost. We provide visual and\nquantitative results on standard test images which show that this improvement\nis significant both visually and in terms of PSNR and SSIM (often as large as 5\ndB). We also demonstrate how the proposed filtering can be implemented at\nreduced complexity by adapting a recent idea for fast bilateral filtering.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 09:28:46 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Rithwik", "Kollipara", ""], ["Chaudhury", "Kunal Narayan", ""]]}, {"id": "1505.06600", "submitter": "Nati Ofir", "authors": "Nati Ofir, Meirav Galun, Boaz Nadler, Ronen Basri", "title": "Fast Detection of Curved Edges at Low SNR", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting edges is a fundamental problem in computer vision with many\napplications, some involving very noisy images. While most edge detection\nmethods are fast, they perform well only on relatively clean images. Indeed,\nedges in such images can be reliably detected using only local filters.\nDetecting faint edges under high levels of noise cannot be done locally at the\nindividual pixel level, and requires more sophisticated global processing.\nUnfortunately, existing methods that achieve this goal are quite slow. In this\npaper we develop a novel multiscale method to detect curved edges in noisy\nimages. While our algorithm searches for edges over a huge set of candidate\ncurves, it does so in a practical runtime, nearly linear in the total number of\nimage pixels. As we demonstrate experimentally, our algorithm is orders of\nmagnitude faster than previous methods designed to deal with high noise levels.\nNevertheless, it obtains comparable, if not better, edge detection quality on a\nvariety of challenging noisy images.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 11:47:37 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Ofir", "Nati", ""], ["Galun", "Meirav", ""], ["Nadler", "Boaz", ""], ["Basri", "Ronen", ""]]}, {"id": "1505.06605", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Ravi Kiran Sarvadevabhatla, R. Venkatesh Babu", "title": "Expresso : A user-friendly GUI for Designing, Training and Exploring\n  Convolutional Neural Networks", "comments": "Project page : http://val.serc.iisc.ernet.in/expresso/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a view to provide a user-friendly interface for designing, training and\ndeveloping deep learning frameworks, we have developed Expresso, a GUI tool\nwritten in Python. Expresso is built atop Caffe, the open-source, prize-winning\nframework popularly used to develop Convolutional Neural Networks. Expresso\nprovides a convenient wizard-like graphical interface which guides the user\nthrough various common scenarios -- data import, construction and training of\ndeep networks, performing various experiments, analyzing and visualizing the\nresults of these experiments. The multi-threaded nature of Expresso enables\nconcurrent execution and notification of events related to the aforementioned\nscenarios. The GUI sub-components and inter-component interfaces in Expresso\nhave been designed with extensibility in mind. We believe Expresso's\nflexibility and ease of use will come in handy to researchers, newcomers and\nseasoned alike, in their explorations related to deep learning.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 12:12:30 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2015 07:06:35 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Sarvadevabhatla", "Ravi Kiran", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1505.06606", "submitter": "Vasileios Belagiannis", "authors": "Vasileios Belagiannis, Christian Rupprecht, Gustavo Carneiro, Nassir\n  Navab", "title": "Robust Optimization for Deep Regression", "comments": "Accepted for publication at the International Conference on Computer\n  Vision (ICCV) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (ConvNets) have successfully contributed to\nimprove the accuracy of regression-based methods for computer vision tasks such\nas human pose estimation, landmark localization, and object detection. The\nnetwork optimization has been usually performed with L2 loss and without\nconsidering the impact of outliers on the training process, where an outlier in\nthis context is defined by a sample estimation that lies at an abnormal\ndistance from the other training sample estimations in the objective space. In\nthis work, we propose a regression model with ConvNets that achieves robustness\nto such outliers by minimizing Tukey's biweight function, an M-estimator robust\nto outliers, as the loss function for the ConvNet. In addition to the robust\nloss, we introduce a coarse-to-fine model, which processes input images of\nprogressively higher resolutions for improving the accuracy of the regressed\nvalues. In our experiments, we demonstrate faster convergence and better\ngeneralization of our robust loss function for the tasks of human pose\nestimation and age estimation from face images. We also show that the\ncombination of the robust loss function with the coarse-to-fine model produces\ncomparable or better results than current state-of-the-art approaches in four\npublicly available human pose estimation datasets.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 12:25:19 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2015 15:24:58 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Belagiannis", "Vasileios", ""], ["Rupprecht", "Christian", ""], ["Carneiro", "Gustavo", ""], ["Navab", "Nassir", ""]]}, {"id": "1505.06611", "submitter": "Tatsuya Yokota", "authors": "Tatsuya Yokota and Qibin Zhao and Andrzej Cichocki", "title": "Smooth PARAFAC Decomposition for Tensor Completion", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TSP.2016.2586759", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, low-rank based tensor completion, which is a higher-order\nextension of matrix completion, has received considerable attention. However,\nthe low-rank assumption is not sufficient for the recovery of visual data, such\nas color and 3D images, where the ratio of missing data is extremely high. In\nthis paper, we consider \"smoothness\" constraints as well as low-rank\napproximations, and propose an efficient algorithm for performing tensor\ncompletion that is particularly powerful regarding visual data. The proposed\nmethod admits significant advantages, owing to the integration of smooth\nPARAFAC decomposition for incomplete tensors and the efficient selection of\nmodels in order to minimize the tensor rank. Thus, our proposed method is\ntermed as \"smooth PARAFAC tensor completion (SPC).\" In order to impose the\nsmoothness constraints, we employ two strategies, total variation (SPC-TV) and\nquadratic variation (SPC-QV), and invoke the corresponding algorithms for model\nlearning. Extensive experimental evaluations on both synthetic and real-world\nvisual data illustrate the significant improvements of our method, in terms of\nboth prediction performance and efficiency, compared with many state-of-the-art\ntensor completion methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 12:47:34 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2015 05:29:10 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2016 08:20:01 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Yokota", "Tatsuya", ""], ["Zhao", "Qibin", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "1505.06621", "submitter": "Giuseppe Riccio", "authors": "Giuseppe Riccio, Stefano Cavuoti, Eugenio Schisano, Massimo Brescia,\n  Amata Mercurio, Davide Elia, Milena Benedettini, Stefano Pezzuto, Sergio\n  Molinari and Anna Maria Di Giorgio", "title": "Machine learning based data mining for Milky Way filamentary structures\n  reconstruction", "comments": "Proceeding of WIRN 2015 Conference, May 20-22, Vietri sul Mare,\n  Salerno, Italy. Published in Smart Innovation, Systems and Technology,\n  Springer, ISSN 2190-3018, 9 pages, 4 figures", "journal-ref": null, "doi": "10.1007/978-3-319-33747-0_3", "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an innovative method called FilExSeC (Filaments Extraction,\nSelection and Classification), a data mining tool developed to investigate the\npossibility to refine and optimize the shape reconstruction of filamentary\nstructures detected with a consolidated method based on the flux derivative\nanalysis, through the column-density maps computed from Herschel infrared\nGalactic Plane Survey (Hi-GAL) observations of the Galactic plane. The present\nmethodology is based on a feature extraction module followed by a machine\nlearning model (Random Forest) dedicated to select features and to classify the\npixels of the input images. From tests on both simulations and real\nobservations the method appears reliable and robust with respect to the\nvariability of shape and distribution of filaments. In the cases of highly\ndefined filament structures, the presented method is able to bridge the gaps\namong the detected fragments, thus improving their shape reconstruction. From a\npreliminary \"a posteriori\" analysis of derived filament physical parameters,\nthe method appears potentially able to add a sufficient contribution to\ncomplete and refine the filament reconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 13:28:31 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 11:31:43 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Riccio", "Giuseppe", ""], ["Cavuoti", "Stefano", ""], ["Schisano", "Eugenio", ""], ["Brescia", "Massimo", ""], ["Mercurio", "Amata", ""], ["Elia", "Davide", ""], ["Benedettini", "Milena", ""], ["Pezzuto", "Stefano", ""], ["Molinari", "Sergio", ""], ["Di Giorgio", "Anna Maria", ""]]}, {"id": "1505.06623", "submitter": "Lianwen Jin", "authors": "Meijun He, Shuye Zhang, Huiyun Mao, Lianwen Jin", "title": "Recognition Confidence Analysis of Handwritten Chinese Character with\n  CNN", "comments": "5 pages, 8 figures, 4 tables. Accepted to appear at ICDAR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an effective method to analyze the recognition\nconfidence of handwritten Chinese character, based on the softmax regression\nscore of a high performance convolutional neural networks (CNN). Through\ncareful and thorough statistics of 827,685 testing samples that randomly\nselected from total 8836 different classes of Chinese characters, we find that\nthe confidence measurement based on CNN is an useful metric to know how\nreliable the recognition results are. Furthermore, we find by experiments that\nthe recognition confidence can be used to find out similar and confusable\ncharacter-pairs, to check wrongly or cursively written samples, and even to\ndiscover and correct mis-labelled samples. Many interesting observations and\nstatistics are given and analyzed in this study.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 13:32:58 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["He", "Meijun", ""], ["Zhang", "Shuye", ""], ["Mao", "Huiyun", ""], ["Jin", "Lianwen", ""]]}, {"id": "1505.06702", "submitter": "Philipp Kniefacz", "authors": "Philipp Kniefacz and Walter Kropatsch", "title": "Smooth and iteratively Restore: A simple and fast edge-preserving\n  smoothing model", "comments": "Presented at OAGM Workshop, 2015 (arXiv:1505.01065)", "journal-ref": null, "doi": null, "report-no": "OAGM/2015/16", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image processing, it can be a useful pre-processing step to smooth away\nsmall structures, such as noise or unimportant details, while retaining the\noverall structure of the image by keeping edges, which separate objects, sharp.\nTypically this edge-preserving smoothing process is achieved using edge-aware\nfilters. However such filters may preserve unwanted small structures as well if\nthey contain edges. In this work we present a novel framework for\nedge-preserving smoothing which separates the process into two different steps:\nFirst the image is smoothed using a blurring filter and in the second step the\nimportant edges are restored using a guided edge-aware filter. The presented\nmethod proves to deliver very good results, compared to state-of-the-art\nedge-preserving smoothing filters, especially at removing unwanted small\nstructures. Furthermore it is very versatile and can easily be adapted to\ndifferent fields of applications while at the same time being very fast to\ncompute and therefore well-suited for real time applications.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 17:50:22 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Kniefacz", "Philipp", ""], ["Kropatsch", "Walter", ""]]}, {"id": "1505.06769", "submitter": "Alexander Gruschina", "authors": "Alexander Gruschina", "title": "VeinPLUS: A Transillumination and Reflection-based Hand Vein Database", "comments": "Presented at OAGM Workshop, 2015 (arXiv:1505.01065)", "journal-ref": null, "doi": null, "report-no": "OAGM/2015/03", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives a short summary of work related to the creation of a\ndepartment-hosted hand vein database. After the introducing section, special\nproperties of the hand vein acquisition are explained, followed by a comparison\ntable, which shows key differences to existing well-known hand vein databases.\nAt the end, the ROI extraction process is described and sample images and ROIs\nare presented.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 22:18:36 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Gruschina", "Alexander", ""]]}, {"id": "1505.06795", "submitter": "Nikolaos Karianakis", "authors": "Nikolaos Karianakis, Jingming Dong and Stefano Soatto", "title": "An Empirical Evaluation of Current Convolutional Architectures' Ability\n  to Manage Nuisance Location and Scale Variability", "comments": "10 pages, 5 figures, 3 tables -- CVPR 2016, camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct an empirical study to test the ability of Convolutional Neural\nNetworks (CNNs) to reduce the effects of nuisance transformations of the input\ndata, such as location, scale and aspect ratio. We isolate factors by adopting\na common convolutional architecture either deployed globally on the image to\ncompute class posterior distributions, or restricted locally to compute class\nconditional distributions given location, scale and aspect ratios of bounding\nboxes determined by proposal heuristics. In theory, averaging the latter should\nyield inferior performance compared to proper marginalization. Yet empirical\nevidence suggests the converse, leading us to conclude that - at the current\nlevel of complexity of convolutional architectures and scale of the data sets\nused to train them - CNNs are not very effective at marginalizing nuisance\nvariability. We also quantify the effects of context on the overall\nclassification task and its impact on the performance of CNNs, and propose\nimproved sampling techniques for heuristic proposal schemes that improve\nend-to-end performance to state-of-the-art levels. We test our hypothesis on a\nclassification task using the ImageNet Challenge benchmark and on a\nwide-baseline matching task using the Oxford and Fischer's datasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 03:11:11 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 05:20:40 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Karianakis", "Nikolaos", ""], ["Dong", "Jingming", ""], ["Soatto", "Stefano", ""]]}, {"id": "1505.06798", "submitter": "Kaiming He", "authors": "Xiangyu Zhang, Jianhua Zou, Kaiming He, Jian Sun", "title": "Accelerating Very Deep Convolutional Networks for Classification and\n  Detection", "comments": "TPAMI, accepted. arXiv admin note: substantial text overlap with\n  arXiv:1411.4229", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to accelerate the test-time computation of convolutional\nneural networks (CNNs), especially very deep CNNs that have substantially\nimpacted the computer vision community. Unlike previous methods that are\ndesigned for approximating linear filters or linear responses, our method takes\nthe nonlinear units into account. We develop an effective solution to the\nresulting nonlinear optimization problem without the need of stochastic\ngradient descent (SGD). More importantly, while previous methods mainly focus\non optimizing one or two layers, our nonlinear method enables an asymmetric\nreconstruction that reduces the rapidly accumulated error when multiple (e.g.,\n>=10) layers are approximated. For the widely used very deep VGG-16 model, our\nmethod achieves a whole-model speedup of 4x with merely a 0.3% increase of\ntop-5 error in ImageNet classification. Our 4x accelerated VGG-16 model also\nshows a graceful accuracy degradation for object detection when plugged into\nthe Fast R-CNN detector.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 03:30:59 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 06:16:59 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Zhang", "Xiangyu", ""], ["Zou", "Jianhua", ""], ["He", "Kaiming", ""], ["Sun", "Jian", ""]]}, {"id": "1505.06800", "submitter": "Baochang Zhang", "authors": "Lei Wang, Baochang Zhang", "title": "Boosting-like Deep Learning For Pedestrian Detection", "comments": "9 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper proposes boosting-like deep learning (BDL) framework for\npedestrian detection. Due to overtraining on the limited training samples,\noverfitting is a major problem of deep learning. We incorporate a boosting-like\ntechnique into deep learning to weigh the training samples, and thus prevent\novertraining in the iterative process. We theoretically give the details of\nderivation of our algorithm, and report the experimental results on open data\nsets showing that BDL achieves a better stable performance than the\nstate-of-the-arts. Our approach achieves 15.85% and 3.81% reduction in the\naverage miss rate compared with ACF and JointDeep on the largest Caltech\nbenchmark dataset, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 03:52:52 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Wang", "Lei", ""], ["Zhang", "Baochang", ""]]}, {"id": "1505.06814", "submitter": "Francesco  Palmieri A. N.", "authors": "Francesco A. N. Palmieri and Amedeo Buonanno", "title": "Discrete Independent Component Analysis (DICA) with Belief Propagation", "comments": "Sumbitted for publication (May 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply belief propagation to a Bayesian bipartite graph composed of\ndiscrete independent hidden variables and discrete visible variables. The\nnetwork is the Discrete counterpart of Independent Component Analysis (DICA)\nand it is manipulated in a factor graph form for inference and learning. A full\nset of simulations is reported for character images from the MNIST dataset. The\nresults show that the factorial code implemented by the sources contributes to\nbuild a good generative model for the data that can be used in various\ninference modes.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 06:02:05 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Palmieri", "Francesco A. N.", ""], ["Buonanno", "Amedeo", ""]]}, {"id": "1505.06821", "submitter": "Shi-Zhe Chen", "authors": "Shi-Zhe Chen, Chun-Chao Guo, Jian-Huang Lai", "title": "Deep Ranking for Person Re-identification via Joint Representation\n  Learning", "comments": "15 pages, 15 figures, IEEE Transactions on Image Processing (TIP),\n  2016", "journal-ref": null, "doi": "10.1109/TIP.2016.2545929", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach to person re-identification, a\nfundamental task in distributed multi-camera surveillance systems. Although a\nvariety of powerful algorithms have been presented in the past few years, most\nof them usually focus on designing hand-crafted features and learning metrics\neither individually or sequentially. Different from previous works, we\nformulate a unified deep ranking framework that jointly tackles both of these\nkey components to maximize their strengths. We start from the principle that\nthe correct match of the probe image should be positioned in the top rank\nwithin the whole gallery set. An effective learning-to-rank algorithm is\nproposed to minimize the cost corresponding to the ranking disorders of the\ngallery. The ranking model is solved with a deep convolutional neural network\n(CNN) that builds the relation between input image pairs and their similarity\nscores through joint representation learning directly from raw image pixels.\nThe proposed framework allows us to get rid of feature engineering and does not\nrely on any assumption. An extensive comparative evaluation is given,\ndemonstrating that our approach significantly outperforms all state-of-the-art\napproaches, including both traditional and CNN-based methods on the challenging\nVIPeR, CUHK-01 and CAVIAR4REID datasets. Additionally, our approach has better\nability to generalize across datasets without fine-tuning.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 06:35:46 GMT"}, {"version": "v2", "created": "Wed, 23 Mar 2016 03:37:36 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Chen", "Shi-Zhe", ""], ["Guo", "Chun-Chao", ""], ["Lai", "Jian-Huang", ""]]}, {"id": "1505.06907", "submitter": "Andreas Gr\\\"unauer", "authors": "Andreas Gr\\\"unauer and Markus Vincze", "title": "Using Dimension Reduction to Improve the Classification of\n  High-dimensional Data", "comments": "Presented at OAGM Workshop, 2015 (arXiv:1505.01065)", "journal-ref": null, "doi": null, "report-no": "OAGM/2015/09", "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we show that the classification performance of high-dimensional\nstructural MRI data with only a small set of training examples is improved by\nthe usage of dimension reduction methods. We assessed two different dimension\nreduction variants: feature selection by ANOVA F-test and feature\ntransformation by PCA. On the reduced datasets, we applied common learning\nalgorithms using 5-fold cross-validation. Training, tuning of the\nhyperparameters, as well as the performance evaluation of the classifiers was\nconducted using two different performance measures: Accuracy, and Receiver\nOperating Characteristic curve (AUC). Our hypothesis is supported by\nexperimental results.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 11:33:04 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Gr\u00fcnauer", "Andreas", ""], ["Vincze", "Markus", ""]]}, {"id": "1505.06957", "submitter": "Nicolas Gillis", "authors": "Gabriella Casalino, Nicolas Gillis", "title": "Sequential Dimensionality Reduction for Extracting Localized Features", "comments": "24 pages, 12 figures. New numerical experiments on synthetic data\n  sets, discussion about the convergence", "journal-ref": "Pattern Recoginition 63, pp. 15-29, 2017", "doi": "10.1016/j.patcog.2016.09.006", "report-no": null, "categories": "cs.CV cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear dimensionality reduction techniques are powerful tools for image\nanalysis as they allow the identification of important features in a data set.\nIn particular, nonnegative matrix factorization (NMF) has become very popular\nas it is able to extract sparse, localized and easily interpretable features by\nimposing an additive combination of nonnegative basis elements. Nonnegative\nmatrix underapproximation (NMU) is a closely related technique that has the\nadvantage to identify features sequentially. In this paper, we propose a\nvariant of NMU that is particularly well suited for image analysis as it\nincorporates the spatial information, that is, it takes into account the fact\nthat neighboring pixels are more likely to be contained in the same features,\nand favors the extraction of localized features by looking for sparse basis\nelements. We show that our new approach competes favorably with comparable\nstate-of-the-art techniques on synthetic, facial and hyperspectral image data\nsets.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 14:06:16 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 06:44:58 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Casalino", "Gabriella", ""], ["Gillis", "Nicolas", ""]]}, {"id": "1505.06973", "submitter": "Bjoern Andres", "authors": "Margret Keuper, Evgeny Levinkov, Nicolas Bonneel, Guillaume Lavou\\'e,\n  Thomas Brox, Bjoern Andres", "title": "Efficient Decomposition of Image and Mesh Graphs by Lifted Multicuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formulations of the Image Decomposition Problem as a Multicut Problem (MP)\nw.r.t. a superpixel graph have received considerable attention. In contrast,\ninstances of the MP w.r.t. a pixel grid graph have received little attention,\nfirstly, because the MP is NP-hard and instances w.r.t. a pixel grid graph are\nhard to solve in practice, and, secondly, due to the lack of long-range terms\nin the objective function of the MP. We propose a generalization of the MP with\nlong-range terms (LMP). We design and implement two efficient algorithms\n(primal feasible heuristics) for the MP and LMP which allow us to study\ninstances of both problems w.r.t. the pixel grid graphs of the images in the\nBSDS-500 benchmark. The decompositions we obtain do not differ significantly\nfrom the state of the art, suggesting that the LMP is a competitive formulation\nof the Image Decomposition Problem. To demonstrate the generality of the LMP,\nwe apply it also to the Mesh Decomposition Problem posed by the Princeton\nbenchmark, obtaining state-of-the-art decompositions.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 14:46:27 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 14:16:25 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Keuper", "Margret", ""], ["Levinkov", "Evgeny", ""], ["Bonneel", "Nicolas", ""], ["Lavou\u00e9", "Guillaume", ""], ["Brox", "Thomas", ""], ["Andres", "Bjoern", ""]]}, {"id": "1505.07192", "submitter": "Hongyang Li", "authors": "Hongyang Li, Huchuan Lu, Zhe Lin, Xiaohui Shen, Brian Price", "title": "Inner and Inter Label Propagation: Salient Object Detection in the Wild", "comments": "The full version of the TIP 2015 publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we propose a novel label propagation based method for saliency\ndetection. A key observation is that saliency in an image can be estimated by\npropagating the labels extracted from the most certain background and object\nregions. For most natural images, some boundary superpixels serve as the\nbackground labels and the saliency of other superpixels are determined by\nranking their similarities to the boundary labels based on an inner propagation\nscheme. For images of complex scenes, we further deploy a 3-cue-center-biased\nobjectness measure to pick out and propagate foreground labels. A\nco-transduction algorithm is devised to fuse both boundary and objectness\nlabels based on an inter propagation scheme. The compactness criterion decides\nwhether the incorporation of objectness labels is necessary, thus greatly\nenhancing computational efficiency. Results on five benchmark datasets with\npixel-wise accurate annotations show that the proposed method achieves superior\nperformance compared with the newest state-of-the-arts in terms of different\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 05:24:03 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Li", "Hongyang", ""], ["Lu", "Huchuan", ""], ["Lin", "Zhe", ""], ["Shen", "Xiaohui", ""], ["Price", "Brian", ""]]}, {"id": "1505.07203", "submitter": "Jean Cousty", "authors": "Jean Cousty (LIGM), Laurent Najman (LIGM), Yukiko Kenmochi (LIGM),\n  Silvio Guimar\\~A{\\pounds}es (VIPLAB, LIGM)", "title": "New characterizations of minimum spanning trees and of saliency maps\n  based on quasi-flat zones", "comments": null, "journal-ref": "12th International Symposium on Mathematical Morphology (ISMM),\n  May 2015, Reykjavik, Iceland. Lecture Notes in Computer Science (LNCS), 9082,\n  pp.205-216, Mathematical Morphology and Its Applications to Signal and Image\n  Processing", "doi": "10.1007/978-3-319-18720-4_18", "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study three representations of hierarchies of partitions: dendrograms\n(direct representations), saliency maps, and minimum spanning trees. We provide\na new bijection between saliency maps and hierarchies based on quasi-flat zones\nas used in image processing and characterize saliency maps and minimum spanning\ntrees as solutions to constrained minimization problems where the constraint is\nquasi-flat zones preservation. In practice, these results form a toolkit for\nnew hierarchical methods where one can choose the most convenient\nrepresentation. They also invite us to process non-image data with\nmorphological hierarchies.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 06:36:10 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Cousty", "Jean", "", "LIGM"], ["Najman", "Laurent", "", "LIGM"], ["Kenmochi", "Yukiko", "", "LIGM"], ["Guimar\u00c3\u00a3es", "Silvio", "", "VIPLAB, LIGM"]]}, {"id": "1505.07293", "submitter": "Vijay Badrinarayanan", "authors": "Vijay Badrinarayanan, Ankur Handa, Roberto Cipolla", "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust\n  Semantic Pixel-Wise Labelling", "comments": "This version was first submitted to CVPR' 15 on November 14, 2014\n  with paper Id 1468. A similar architecture was proposed more recently on May\n  17, 2015, see http://arxiv.org/pdf/1505.04366.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep architecture, SegNet, for semantic pixel wise image\nlabelling. SegNet has several attractive properties; (i) it only requires\nforward evaluation of a fully learnt function to obtain smooth label\npredictions, (ii) with increasing depth, a larger context is considered for\npixel labelling which improves accuracy, and (iii) it is easy to visualise the\neffect of feature activation(s) in the pixel label space at any depth. SegNet\nis composed of a stack of encoders followed by a corresponding decoder stack\nwhich feeds into a soft-max classification layer. The decoders help map low\nresolution feature maps at the output of the encoder stack to full input image\nsize feature maps. This addresses an important drawback of recent deep learning\napproaches which have adopted networks designed for object categorization for\npixel wise labelling. These methods lack a mechanism to map deep layer feature\nmaps to input dimensions. They resort to ad hoc methods to upsample features,\ne.g. by replication. This results in noisy predictions and also restricts the\nnumber of pooling layers in order to avoid too much upsampling and thus reduces\nspatial context. SegNet overcomes these problems by learning to map encoder\noutputs to image pixel labels. We test the performance of SegNet on outdoor RGB\nscenes from CamVid, KITTI and indoor scenes from the NYU dataset. Our results\nshow that SegNet achieves state-of-the-art performance even without use of\nadditional cues such as depth, video frames or post-processing with CRF models.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 12:54:17 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Badrinarayanan", "Vijay", ""], ["Handa", "Ankur", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1505.07376", "submitter": "Leon Gatys", "authors": "Leon A. Gatys, Alexander S. Ecker and Matthias Bethge", "title": "Texture Synthesis Using Convolutional Neural Networks", "comments": "Revision for NIPS 2015 Camera Ready. In line with reviewer's comments\n  we now focus on the texture model and texture synthesis performance. We limit\n  the relationship of our texture model to the ventral stream and its potential\n  use for neuroscience to the discussion of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we introduce a new model of natural textures based on the feature spaces\nof convolutional neural networks optimised for object recognition. Samples from\nthe model are of high perceptual quality demonstrating the generative power of\nneural networks trained in a purely discriminative fashion. Within the model,\ntextures are represented by the correlations between feature maps in several\nlayers of the network. We show that across layers the texture representations\nincreasingly capture the statistical properties of natural images while making\nobject information more and more explicit. The model provides a new tool to\ngenerate stimuli for neuroscience and might offer insights into the deep\nrepresentations learned by convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 15:29:52 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2015 23:10:38 GMT"}, {"version": "v3", "created": "Fri, 6 Nov 2015 13:55:09 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Gatys", "Leon A.", ""], ["Ecker", "Alexander S.", ""], ["Bethge", "Matthias", ""]]}, {"id": "1505.07409", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Carles Ventura, Xavier Gir\\'o-i-Nieto, Ver\\'onica Vilaplana, Kevin\n  McGuinness, Ferran Marqu\\'es and Noel E. O'Connor", "title": "Improving Spatial Codification in Semantic Segmentation", "comments": "Paper accepted at the IEEE International Conference on Image\n  Processing, ICIP 2015. Quebec City, 27-30 September. Project page:\n  https://imatge.upc.edu/web/publications/improving-spatial-codification-semantic-segmentation", "journal-ref": null, "doi": "10.1109/ICIP.2015.7351476", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores novel approaches for improving the spatial codification\nfor the pooling of local descriptors to solve the semantic segmentation\nproblem. We propose to partition the image into three regions for each object\nto be described: Figure, Border and Ground. This partition aims at minimizing\nthe influence of the image context on the object description and vice versa by\nintroducing an intermediate zone around the object contour. Furthermore, we\nalso propose a richer visual descriptor of the object by applying a Spatial\nPyramid over the Figure region. Two novel Spatial Pyramid configurations are\nexplored: Cartesian-based and crown-based Spatial Pyramids. We test these\napproaches with state-of-the-art techniques and show that they improve the\nFigure-Ground based pooling in the Pascal VOC 2011 and 2012 semantic\nsegmentation challenges.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 17:26:39 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Ventura", "Carles", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""], ["Vilaplana", "Ver\u00f3nica", ""], ["McGuinness", "Kevin", ""], ["Marqu\u00e9s", "Ferran", ""], ["O'Connor", "Noel E.", ""]]}, {"id": "1505.07427", "submitter": "Alex Kendall", "authors": "Alex Kendall, Matthew Grimes and Roberto Cipolla", "title": "PoseNet: A Convolutional Network for Real-Time 6-DOF Camera\n  Relocalization", "comments": "9 pages, 13 figures; Corrected numerical error in orientation results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust and real-time monocular six degree of freedom\nrelocalization system. Our system trains a convolutional neural network to\nregress the 6-DOF camera pose from a single RGB image in an end-to-end manner\nwith no need of additional engineering or graph optimisation. The algorithm can\noperate indoors and outdoors in real time, taking 5ms per frame to compute. It\nobtains approximately 2m and 6 degree accuracy for large scale outdoor scenes\nand 0.5m and 10 degree accuracy indoors. This is achieved using an efficient 23\nlayer deep convnet, demonstrating that convnets can be used to solve\ncomplicated out of image plane regression problems. This was made possible by\nleveraging transfer learning from large scale classification data. We show the\nconvnet localizes from high level features and is robust to difficult lighting,\nmotion blur and different camera intrinsics where point based SIFT registration\nfails. Furthermore we show how the pose feature that is produced generalizes to\nother scenes allowing us to regress pose with only a few dozen training\nexamples. PoseNet code, dataset and an online demonstration is available on our\nproject webpage, at http://mi.eng.cam.ac.uk/projects/relocalisation/\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 18:18:42 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 11:52:30 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 10:10:01 GMT"}, {"version": "v4", "created": "Thu, 18 Feb 2016 13:52:18 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Kendall", "Alex", ""], ["Grimes", "Matthew", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1505.07428", "submitter": "Manuel L\\'opez-Antequera", "authors": "Ruben Gomez-Ojeda, Manuel Lopez-Antequera, Nicolai Petkov, Javier\n  Gonzalez-Jimenez", "title": "Training a Convolutional Neural Network for Appearance-Invariant Place\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Place recognition is one of the most challenging problems in computer vision,\nand has become a key part in mobile robotics and autonomous driving\napplications for performing loop closure in visual SLAM systems. Moreover, the\ndifficulty of recognizing a revisited location increases with appearance\nchanges caused, for instance, by weather or illumination variations, which\nhinders the long-term application of such algorithms in real environments. In\nthis paper we present a convolutional neural network (CNN), trained for the\nfirst time with the purpose of recognizing revisited locations under severe\nappearance changes, which maps images to a low dimensional space where\nEuclidean distances represent place dissimilarity. In order for the network to\nlearn the desired invariances, we train it with triplets of images selected\nfrom datasets which present a challenging variability in visual appearance. The\ntriplets are selected in such way that two samples are from the same location\nand the third one is taken from a different place. We validate our system\nthrough extensive experimentation, where we demonstrate better performance than\nstate-of-art algorithms in a number of popular datasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 18:21:54 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Gomez-Ojeda", "Ruben", ""], ["Lopez-Antequera", "Manuel", ""], ["Petkov", "Nicolai", ""], ["Gonzalez-Jimenez", "Javier", ""]]}, {"id": "1505.07522", "submitter": "Daniele Quercia", "authors": "Miriam Redi, Daniele Quercia, Lindsay T. Graham, Samuel D. Gosling", "title": "Like Partying? Your Face Says It All. Predicting the Ambiance of Places\n  with Profile Pictures", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To choose restaurants and coffee shops, people are increasingly relying on\nsocial-networking sites. In a popular site such as Foursquare or Yelp, a place\ncomes with descriptions and reviews, and with profile pictures of people who\nfrequent them. Descriptions and reviews have been widely explored in the\nresearch area of data mining. By contrast, profile pictures have received\nlittle attention. Previous work showed that people are able to partly guess a\nplace's ambiance, clientele, and activities not only by observing the place\nitself but also by observing the profile pictures of its visitors. Here we\nfurther that work by determining which visual cues people may have relied upon\nto make their guesses; showing that a state-of-the-art algorithm could make\npredictions more accurately than humans at times; and demonstrating that the\nvisual cues people relied upon partly differ from those of the algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 01:23:05 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Redi", "Miriam", ""], ["Quercia", "Daniele", ""], ["Graham", "Lindsay T.", ""], ["Gosling", "Samuel D.", ""]]}, {"id": "1505.07647", "submitter": "Yushi Jing", "authors": "Yushi Jing and David Liu and Dmitry Kislyuk and Andrew Zhai and\n  Jiajing Xu and Jeff Donahue and Sarah Tavel", "title": "Visual Search at Pinterest", "comments": "in Proceedings of the 21th ACM SIGKDD International Conference on\n  Knowledge and Discovery and Data Mining, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that, with the availability of distributed computation\nplatforms such as Amazon Web Services and open-source tools, it is possible for\na small engineering team to build, launch and maintain a cost-effective,\nlarge-scale visual search system with widely available tools. We also\ndemonstrate, through a comprehensive set of live experiments at Pinterest, that\ncontent recommendation powered by visual search improve user engagement. By\nsharing our implementation details and the experiences learned from launching a\ncommercial visual search engines from scratch, we hope visual search are more\nwidely incorporated into today's commercial applications.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 11:17:39 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 23:17:40 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 05:02:00 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Jing", "Yushi", ""], ["Liu", "David", ""], ["Kislyuk", "Dmitry", ""], ["Zhai", "Andrew", ""], ["Xu", "Jiajing", ""], ["Donahue", "Jeff", ""], ["Tavel", "Sarah", ""]]}, {"id": "1505.07672", "submitter": "Debapriya Das", "authors": "Niklas Ludtke, Debapriya Das, Lucas Theis, Matthias Bethge", "title": "A Generative Model of Natural Texture Surrogates", "comments": "34 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural images can be viewed as patchworks of different textures, where the\nlocal image statistics is roughly stationary within a small neighborhood but\notherwise varies from region to region. In order to model this variability, we\nfirst applied the parametric texture algorithm of Portilla and Simoncelli to\nimage patches of 64X64 pixels in a large database of natural images such that\neach image patch is then described by 655 texture parameters which specify\ncertain statistics, such as variances and covariances of wavelet coefficients\nor coefficient magnitudes within that patch.\n  To model the statistics of these texture parameters, we then developed\nsuitable nonlinear transformations of the parameters that allowed us to fit\ntheir joint statistics with a multivariate Gaussian distribution. We find that\nthe first 200 principal components contain more than 99% of the variance and\nare sufficient to generate textures that are perceptually extremely close to\nthose generated with all 655 components. We demonstrate the usefulness of the\nmodel in several ways: (1) We sample ensembles of texture patches that can be\ndirectly compared to samples of patches from the natural image database and can\nto a high degree reproduce their perceptual appearance. (2) We further\ndeveloped an image compression algorithm which generates surprisingly accurate\nimages at bit rates as low as 0.14 bits/pixel. Finally, (3) We demonstrate how\nour approach can be used for an efficient and objective evaluation of samples\ngenerated with probabilistic models of natural images.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 12:37:15 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Ludtke", "Niklas", ""], ["Das", "Debapriya", ""], ["Theis", "Lucas", ""], ["Bethge", "Matthias", ""]]}, {"id": "1505.07675", "submitter": "Lianwen Jin", "authors": "Weixin Yang, Lianwen Jin, Zecheng Xie, Ziyong Feng", "title": "Improved Deep Convolutional Neural Network For Online Handwritten\n  Chinese Character Recognition using Domain-Specific Knowledge", "comments": "5 pages, 4 figures, 3 tables. Accepted to appear at ICDAR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (DCNNs) have achieved great success in\nvarious computer vision and pattern recognition applications, including those\nfor handwritten Chinese character recognition (HCCR). However, most current\nDCNN-based HCCR approaches treat the handwritten sample simply as an image\nbitmap, ignoring some vital domain-specific information that may be useful but\nthat cannot be learnt by traditional networks. In this paper, we propose an\nenhancement of the DCNN approach to online HCCR by incorporating a variety of\ndomain-specific knowledge, including deformation, non-linear normalization,\nimaginary strokes, path signature, and 8-directional features. Our contribution\nis twofold. First, these domain-specific technologies are investigated and\nintegrated with a DCNN to form a composite network to achieve improved\nperformance. Second, the resulting DCNNs with diversity in their domain\nknowledge are combined using a hybrid serial-parallel (HSP) strategy.\nConsequently, we achieve a promising accuracy of 97.20% and 96.87% on\nCASIA-OLHWDB1.0 and CASIA-OLHWDB1.1, respectively, outperforming the best\nresults previously reported in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 12:43:22 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Yang", "Weixin", ""], ["Jin", "Lianwen", ""], ["Xie", "Zecheng", ""], ["Feng", "Ziyong", ""]]}, {"id": "1505.07690", "submitter": "Remco Duits", "authors": "Michiel Janssen, Remco Duits, Marcel Breeuwer", "title": "Invertible Orientation Scores of 3D Images", "comments": "ssvm 2015 published version in LNCS contains a mistake (a switch\n  notation spherical angles) that is corrected in this arxiv version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The enhancement and detection of elongated structures in noisy image data is\nrelevant for many biomedical applications. To handle complex crossing\nstructures in 2D images, 2D orientation scores were introduced, which already\nshowed their use in a variety of applications. Here we extend this work to 3D\norientation scores. First, we construct the orientation score from a given\ndataset, which is achieved by an invertible coherent state type of transform.\nFor this transformation we introduce 3D versions of the 2D cake-wavelets, which\nare complex wavelets that can simultaneously detect oriented structures and\noriented edges. For efficient implementation of the different steps in the\nwavelet creation we use a spherical harmonic transform. Finally, we show some\nfirst results of practical applications of 3D orientation scores.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 13:52:41 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Janssen", "Michiel", ""], ["Duits", "Remco", ""], ["Breeuwer", "Marcel", ""]]}, {"id": "1505.07778", "submitter": "Suman Ghosh", "authors": "Suman K. Ghosh and Ernest Valveny", "title": "Query by String word spotting based on character bi-gram indexing", "comments": "To be published in ICDAR2015", "journal-ref": null, "doi": "10.1109/ICDAR.2015.7333888", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a segmentation-free query by string word spotting\nmethod. Both the documents and query strings are encoded using a recently\nproposed word representa- tion that projects images and strings into a common\natribute space based on a pyramidal histogram of characters(PHOC). These\nattribute models are learned using linear SVMs over the Fisher Vector\nrepresentation of the images along with the PHOC labels of the corresponding\nstrings. In order to search through the whole page, document regions are\nindexed per character bi- gram using a similar attribute representation. On top\nof that, we propose an integral image representation of the document using a\nsimplified version of the attribute model for efficient computation. Finally we\nintroduce a re-ranking step in order to boost retrieval performance. We show\nstate-of-the-art results for segmentation-free query by string word spotting in\nsingle-writer and multi-writer standard datasets\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 17:59:24 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Ghosh", "Suman K.", ""], ["Valveny", "Ernest", ""]]}, {"id": "1505.07922", "submitter": "Junshi Huang", "authors": "Junshi Huang, Rogerio S. Feris, Qiang Chen, Shuicheng Yan", "title": "Cross-domain Image Retrieval with a Dual Attribute-aware Ranking Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of cross-domain image retrieval, considering the\nfollowing practical application: given a user photo depicting a clothing image,\nour goal is to retrieve the same or attribute-similar clothing items from\nonline shopping stores. This is a challenging problem due to the large\ndiscrepancy between online shopping images, usually taken in ideal\nlighting/pose/background conditions, and user photos captured in uncontrolled\nconditions. To address this problem, we propose a Dual Attribute-aware Ranking\nNetwork (DARN) for retrieval feature learning. More specifically, DARN consists\nof two sub-networks, one for each domain, whose retrieval feature\nrepresentations are driven by semantic attribute learning. We show that this\nattribute-guided learning is a key factor for retrieval accuracy improvement.\nIn addition, to further align with the nature of the retrieval problem, we\nimpose a triplet visual similarity constraint for learning to rank across the\ntwo sub-networks. Another contribution of our work is a large-scale dataset\nwhich makes the network learning feasible. We exploit customer review websites\nto crawl a large set of online shopping images and corresponding offline user\nphotos with fine-grained clothing attributes, i.e., around 450,000 online\nshopping images and about 90,000 exact offline counterpart images of those\nonline ones. All these images are collected from real-world consumer websites\nreflecting the diversity of the data modality, which makes this dataset unique\nand rare in the academic community. We extensively evaluate the retrieval\nperformance of networks in different configurations. The top-20 retrieval\naccuracy is doubled when using the proposed DARN other than the current popular\nsolution using pre-trained CNN features only (0.570 vs. 0.268).\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 04:46:37 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Huang", "Junshi", ""], ["Feris", "Rogerio S.", ""], ["Chen", "Qiang", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1505.07923", "submitter": "Anirban Dasgupta", "authors": "Anirban Dasgupta, Aurobinda Routray", "title": "Fast Computation of PERCLOS and Saccadic Ratio", "comments": "MS Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis describes the development of fast algorithms for the computation\nof PERcentage CLOSure of eyes (PERCLOS) and Saccadic Ratio (SR). PERCLOS and SR\nare two ocular parameters reported to be measures of alertness levels in human\nbeings. PERCLOS is the percentage of time in which at least 80% of the eyelid\nremains closed over the pupil. Saccades are fast and simultaneous movement of\nboth the eyes in the same direction. SR is the ratio of peak saccadic velocity\nto the saccadic duration. This thesis addresses the issues of image based\nestimation of PERCLOS and SR, prevailing in the literature such as illumination\nvariation, poor illumination conditions, head rotations etc. In this work,\nalgorithms for real-time PERCLOS computation has been developed and implemented\non an embedded platform. The platform has been used as a case study for\nassessment of loss of attention in automotive drivers. The SR estimation has\nbeen carried out offline as real-time implementation requires high frame rates\nof processing which is difficult to achieve due to hardware limitations. The\naccuracy in estimation of the loss of attention using PERCLOS and SR has been\nvalidated using brain signals, which are reported to be an authentic cue for\nestimating the state of alertness in human beings. The major contributions of\nthis thesis include database creation, design and implementation of fast\nalgorithms for estimating PERCLOS and SR on embedded computing platforms.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 05:04:30 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Dasgupta", "Anirban", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1505.07930", "submitter": "Tam Nguyen", "authors": "Tam V. Nguyen, Jose Sepulveda", "title": "Salient Object Detection via Augmented Hypotheses", "comments": "IJCAI 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we propose using \\textit{augmented hypotheses} which consider\nobjectness, foreground and compactness for salient object detection. Our\nalgorithm consists of four basic steps. First, our method generates the\nobjectness map via objectness hypotheses. Based on the objectness map, we\nestimate the foreground margin and compute the corresponding foreground map\nwhich prefers the foreground objects. From the objectness map and the\nforeground map, the compactness map is formed to favor the compact objects. We\nthen derive a saliency measure that produces a pixel-accurate saliency map\nwhich uniformly covers the objects of interest and consistently separates fore-\nand background. We finally evaluate the proposed framework on two challenging\ndatasets, MSRA-1000 and iCoSeg. Our extensive experimental results show that\nour method outperforms state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 06:03:57 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Nguyen", "Tam V.", ""], ["Sepulveda", "Jose", ""]]}, {"id": "1505.07934", "submitter": "Martin Lukac", "authors": "Martin Lukac and Kamila Abdiyeva and Michitaka Kameyama", "title": "Symbolic Segmentation Using Algorithm Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an alternative approach to symbolic segmentation;\ninstead of implementing a new method we approach symbolic segmentation as an\nalgorithm selection problem. That is, let there be $n$ available algorithms for\nsymbolic segmentation, a selection mechanism forms a set of input features and\nimage attributes and selects on a case by case basis the best algorithm. The\nselection mechanism is demonstrated from within an algorithm framework where\nthe selection is done in a set of various algorithm networks. Two sets of\nexperiments are performed and in both cases we demonstrate that the algorithm\nselection allows to increase the result of the symbolic segmentation by a\nconsiderable amount.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 06:18:31 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Lukac", "Martin", ""], ["Abdiyeva", "Kamila", ""], ["Kameyama", "Michitaka", ""]]}, {"id": "1505.08019", "submitter": "Fei Fei Shen", "authors": "Feifei Shen, Zhenjian Song, Congrui Wu, Jiaqi Geng, Qingyun Wang", "title": "Research on the fast Fourier transform of image based on GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Study of general purpose computation by GPU (Graphics Processing Unit) can\nimprove the image processing capability of micro-computer system. This paper\nstudies the parallelism of the different stages of decimation in time radix 2\nFFT algorithm, designs the butterfly and scramble kernels and implements 2D FFT\non GPU. The experiment result demonstrates the validity and advantage over\ngeneral CPU, especially in the condition of large input size. The approach can\nalso be generalized to other transforms alike.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 12:33:52 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Shen", "Feifei", ""], ["Song", "Zhenjian", ""], ["Wu", "Congrui", ""], ["Geng", "Jiaqi", ""], ["Wang", "Qingyun", ""]]}, {"id": "1505.08070", "submitter": "Yirmeyahu Kaminski Ph.D.", "authors": "Yirmeyahu Kaminski, Mike Werman", "title": "General Deformations of Point Configurations Viewed By a Pinhole Model\n  Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a theoretical study of the following Non-Rigid Structure from\nMotion problem. What can be computed from a monocular view of a parametrically\ndeforming set of points? We treat various variations of this problem for affine\nand polynomial deformations with calibrated and uncalibrated cameras. We show\nthat in general at least three images with quasi-identical two deformations are\nneeded in order to have a finite set of solutions of the points' structure and\ncalculate some simple examples.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 14:51:18 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Kaminski", "Yirmeyahu", ""], ["Werman", "Mike", ""]]}, {"id": "1505.08071", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain", "title": "Geometry of Graph Edit Distance Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the geometry of graph spaces endowed with a special\nclass of graph edit distances. The focus is on geometrical results useful for\nstatistical pattern recognition. The main result is the Graph Representation\nTheorem. It states that a graph is a point in some geometrical space, called\norbit space. Orbit spaces are well investigated and easier to explore than the\noriginal graph space. We derive a number of geometrical results from the orbit\nspace representation, translate them to the graph space, and indicate their\nsignificance and usefulness in statistical pattern recognition.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 14:51:23 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Jain", "Brijnesh J.", ""]]}, {"id": "1505.08082", "submitter": "Santi Segui Dr.", "authors": "Santi Segu\\'i, Oriol Pujol, Jordi Vitri\\`a", "title": "Learning to count with deep object features", "comments": "This paper has been accepted at Deep Vision Workshop at CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to count is a learning strategy that has been recently proposed in\nthe literature for dealing with problems where estimating the number of object\ninstances in a scene is the final objective. In this framework, the task of\nlearning to detect and localize individual object instances is seen as a harder\ntask that can be evaded by casting the problem as that of computing a\nregression value from hand-crafted image features. In this paper we explore the\nfeatures that are learned when training a counting convolutional neural network\nin order to understand their underlying representation. To this end we define a\ncounting problem for MNIST data and show that the internal representation of\nthe network is able to classify digits in spite of the fact that no direct\nsupervision was provided for them during training. We also present preliminary\nresults about a deep network that is able to count the number of pedestrians in\na scene.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 15:06:15 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Segu\u00ed", "Santi", ""], ["Pujol", "Oriol", ""], ["Vitri\u00e0", "Jordi", ""]]}, {"id": "1505.08098", "submitter": "Gianluigi Ciocca", "authors": "Simone Bianco, Gianluigi Ciocca, Claudio Cusano", "title": "CURL: Co-trained Unsupervised Representation Learning for Image\n  Classification", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a strategy for semi-supervised image classification\nthat leverages unsupervised representation learning and co-training. The\nstrategy, that is called CURL from Co-trained Unsupervised Representation\nLearning, iteratively builds two classifiers on two different views of the\ndata. The two views correspond to different representations learned from both\nlabeled and unlabeled data and differ in the fusion scheme used to combine the\nimage features. To assess the performance of our proposal, we conducted several\nexperiments on widely used data sets for scene and object recognition. We\nconsidered three scenarios (inductive, transductive and self-taught learning)\nthat differ in the strategy followed to exploit the unlabeled data. As image\nfeatures we considered a combination of GIST, PHOG, and LBP as well as features\nextracted from a Convolutional Neural Network. Moreover, two embodiments of\nCURL are investigated: one using Ensemble Projection as unsupervised\nrepresentation learning coupled with Logistic Regression, and one based on\nLapSVM. The results show that CURL clearly outperforms other supervised and\nsemi-supervised learning methods in the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 15:57:40 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 12:21:20 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Bianco", "Simone", ""], ["Ciocca", "Gianluigi", ""], ["Cusano", "Claudio", ""]]}, {"id": "1505.08153", "submitter": "Mohammad Sabokrou", "authors": "Mohsen Fayyaz, Mohammad Hajizadeh_Saffar, Mohammad Sabokrou and\n  Mahmood Fathy", "title": "Feature Representation for Online Signature Verification", "comments": "10 pages, 10 figures, Submitted to IEEE Transactions on Information\n  Forensics and Security", "journal-ref": null, "doi": "10.1109/AISP.2015.7123528", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometrics systems have been used in a wide range of applications and have\nimproved people authentication. Signature verification is one of the most\ncommon biometric methods with techniques that employ various specifications of\na signature. Recently, deep learning has achieved great success in many fields,\nsuch as image, sounds and text processing. In this paper, deep learning method\nhas been used for feature extraction and feature selection.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 19:09:02 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Fayyaz", "Mohsen", ""], ["Hajizadeh_Saffar", "Mohammad", ""], ["Sabokrou", "Mohammad", ""], ["Fathy", "Mahmood", ""]]}]