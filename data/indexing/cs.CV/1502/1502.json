[{"id": "1502.00030", "submitter": "Sravanthi Bondugula", "authors": "Sravanthi Bondugula, Varun Manjunatha, Larry S. Davis, David Doermann", "title": "SHOE: Supervised Hashing with Output Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a supervised binary encoding scheme for image retrieval that\nlearns projections by taking into account similarity between classes obtained\nfrom output embeddings. Our motivation is that binary hash codes learned in\nthis way improve both the visual quality of retrieval results and existing\nsupervised hashing schemes. We employ a sequential greedy optimization that\nlearns relationship aware projections by minimizing the difference between\ninner products of binary codes and output embedding vectors. We develop a joint\noptimization framework to learn projections which improve the accuracy of\nsupervised hashing over the current state of the art with respect to standard\nand sibling evaluation metrics. We further boost performance by applying the\nsupervised dimensionality reduction technique on kernelized input CNN features.\nExperiments are performed on three datasets: CUB-2011, SUN-Attribute and\nImageNet ILSVRC 2010. As a by-product of our method, we show that using a\nsimple k-nn pooling classifier with our discriminative codes improves over the\ncomplex classification models on fine grained datasets like CUB and offer an\nimpressive compression ratio of 1024 on CNN features.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 22:04:12 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Bondugula", "Sravanthi", ""], ["Manjunatha", "Varun", ""], ["Davis", "Larry S.", ""], ["Doermann", "David", ""]]}, {"id": "1502.00046", "submitter": "Davis King", "authors": "Davis E. King", "title": "Max-Margin Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most object detection methods operate by applying a binary classifier to\nsub-windows of an image, followed by a non-maximum suppression step where\ndetections on overlapping sub-windows are removed. Since the number of possible\nsub-windows in even moderately sized image datasets is extremely large, the\nclassifier is typically learned from only a subset of the windows. This avoids\nthe computational difficulty of dealing with the entire set of sub-windows,\nhowever, as we will show in this paper, it leads to sub-optimal detector\nperformance.\n  In particular, the main contribution of this paper is the introduction of a\nnew method, Max-Margin Object Detection (MMOD), for learning to detect objects\nin images. This method does not perform any sub-sampling, but instead optimizes\nover all sub-windows. MMOD can be used to improve any object detection method\nwhich is linear in the learned parameters, such as HOG or bag-of-visual-word\nmodels. Using this approach we show substantial performance gains on three\npublicly available datasets. Strikingly, we show that a single rigid HOG filter\ncan outperform a state-of-the-art deformable part model on the Face Detection\nData Set and Benchmark when the HOG filter is learned via MMOD.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 00:32:34 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["King", "Davis E.", ""]]}, {"id": "1502.00082", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Ravi Kiran Sarvadevabhatla and R. Venkatesh Babu", "title": "Category-Epitomes : Discriminatively Minimalist Representations for\n  Object Categories", "comments": "Submitted to ICIP-2015, 5 pages, sketch, object category recognition,\n  temporal, sparse", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Freehand line sketches are an interesting and unique form of visual\nrepresentation. Typically, such sketches are studied and utilized as an end\nproduct of the sketching process. However, we have found it instructive to\nstudy the sketches as sequentially accumulated composition of drawing strokes\nadded over time. Studying sketches in this manner has enabled us to create\nnovel sparse yet discriminative sketch-based representations for object\ncategories which we term category-epitomes. Our procedure for obtaining these\nepitomes concurrently provides a natural measure for quantifying the sparseness\nunderlying the original sketch, which we term epitome-score. We construct and\nanalyze category-epitomes and epitome-scores for freehand sketches belonging to\nvarious object categories. Our analysis provides a novel viewpoint for studying\nthe semantic nature of object categories.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 08:32:35 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Sarvadevabhatla", "Ravi Kiran", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1502.00115", "submitter": "Canyi Lu", "authors": "Can-Yi Lu, De-Shuang Huang", "title": "Optimized Projection for Sparse Representation Based Classification", "comments": "Neurocomputing 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction (DR) methods have been commonly used as a principled\nway to understand the high-dimensional data such as facial images. In this\npaper, we propose a new supervised DR method called Optimized Projection for\nSparse Representation based Classification (OP-SRC), which is based on the\nrecent face recognition method, Sparse Representation based Classification\n(SRC). SRC seeks a sparse linear combination on all the training data for a\ngiven query image, and make the decision by the minimal reconstruction\nresidual. OP-SRC is designed on the decision rule of SRC, it aims to reduce the\nwithin-class reconstruction residual and simultaneously increase the\nbetween-class reconstruction residual on the training data. The projections are\noptimized and match well with the mechanism of SRC. Therefore, SRC performs\nwell in the OP-SRC transformed space. The feasibility and effectiveness of the\nproposed method is verified on the Yale, ORL and UMIST databases with promising\nresults.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 14:44:05 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Lu", "Can-Yi", ""], ["Huang", "De-Shuang", ""]]}, {"id": "1502.00192", "submitter": "Menglong Zhu", "authors": "Menglong Zhu, Xiaowei Zhou, Kostas Daniilidis", "title": "Pose and Shape Estimation with Discriminatively Learned Parts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach for estimating the 3D pose and the 3D shape of an\nobject from a single image. Given a training set of view exemplars, we learn\nand select appearance-based discriminative parts which are mapped onto the 3D\nmodel from the training set through a facil- ity location optimization. The\ntraining set of 3D models is summarized into a sparse set of shapes from which\nwe can generalize by linear combination. Given a test picture, we detect\nhypotheses for each part. The main challenge is to select from these hypotheses\nand compute the 3D pose and shape coefficients at the same time. To achieve\nthis, we optimize a function that minimizes simultaneously the geometric\nreprojection error as well as the appearance matching of the parts. We apply\nthe alternating direction method of multipliers (ADMM) to minimize the\nresulting convex function. We evaluate our approach on the Fine Grained 3D Car\ndataset with superior performance in shape and pose errors. Our main and novel\ncontribution is the simultaneous solution for part localization, 3D pose and\nshape by maximizing both geometric and appearance compatibility.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 04:09:23 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Zhu", "Menglong", ""], ["Zhou", "Xiaowei", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1502.00250", "submitter": "C\\'eline Craye", "authors": "C\\'eline Craye, Fakhri Karray", "title": "Driver distraction detection and recognition using RGB-D sensor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driver inattention assessment has become a very active field in intelligent\ntransportation systems. Based on active sensor Kinect and computer vision\ntools, we have built an efficient module for detecting driver distraction and\nrecognizing the type of distraction. Based on color and depth map data from the\nKinect, our system is composed of four sub-modules. We call them eye behavior\n(detecting gaze and blinking), arm position (is the right arm up, down, right\nof forward), head orientation, and facial expressions. Each module produces\nrelevant information for assessing driver inattention. They are merged together\nlater on using two different classification strategies: AdaBoost classifier and\nHidden Markov Model. Evaluation is done using a driving simulator and 8 drivers\nof different gender, age and nationality for a total of more than 8 hours of\nrecording. Qualitative and quantitative results show strong and accurate\ndetection and recognition capacity (85% accuracy for the type of distraction\nand 90% for distraction detection). Moreover, each module is obtained\nindependently and could be used for other types of inference, such as fatigue\ndetection, and could be implemented for real cars systems.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 13:24:49 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Craye", "C\u00e9line", ""], ["Karray", "Fakhri", ""]]}, {"id": "1502.00254", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Ravi Kiran Sarvadevabhatla and R. Venkatesh Babu", "title": "Freehand Sketch Recognition Using Deep Features", "comments": "Submitted to ICIP-2015, 5 pages. Removed an erroneous claim regarding\n  a work cited in the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Freehand sketches often contain sparse visual detail. In spite of the\nsparsity, they are easily and consistently recognized by humans across\ncultures, languages and age groups. Therefore, analyzing such sparse sketches\ncan aid our understanding of the neuro-cognitive processes involved in visual\nrepresentation and recognition. In the recent past, Convolutional Neural\nNetworks (CNNs) have emerged as a powerful framework for feature representation\nand recognition for a variety of image domains. However, the domain of sketch\nimages has not been explored. This paper introduces a freehand sketch\nrecognition framework based on \"deep\" features extracted from CNNs. We use two\npopular CNNs for our experiments -- Imagenet CNN and a modified version of\nLeNet CNN. We evaluate our recognition framework on a publicly available\nbenchmark database containing thousands of freehand sketches depicting everyday\nobjects. Our results are an improvement over the existing state-of-the-art\naccuracies by 3% - 11%. The effectiveness and relative compactness of our deep\nfeatures also make them an ideal candidate for related problems such as\nsketch-based image retrieval. In addition, we provide a preliminary glimpse of\nhow such features can help identify crucial attributes (e.g. object-parts) of\nthe sketched objects.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 13:35:44 GMT"}, {"version": "v2", "created": "Wed, 4 Feb 2015 14:41:15 GMT"}], "update_date": "2015-02-05", "authors_parsed": [["Sarvadevabhatla", "Ravi Kiran", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1502.00256", "submitter": "Liang Lin", "authors": "Yuanlu Xu, Liang Lin, Wei-Shi Zheng, Xiaobai Liu", "title": "Human Re-identification by Matching Compositional Template with Cluster\n  Sampling", "comments": "This manuscript has 8 pages with 7 figures, and a preliminary version\n  was published in ICCV 2013", "journal-ref": null, "doi": "10.1109/ICCV.2013.391", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at a newly raising task in visual surveillance:\nre-identifying people at a distance by matching body information, given several\nreference examples. Most of existing works solve this task by matching a\nreference template with the target individual, but often suffer from large\nhuman appearance variability (e.g. different poses/views, illumination) and\nhigh false positives in matching caused by conjunctions, occlusions or\nsurrounding clutters. Addressing these problems, we construct a simple yet\nexpressive template from a few reference images of a certain individual, which\nrepresents the body as an articulated assembly of compositional and alternative\nparts, and propose an effective matching algorithm with cluster sampling. This\nalgorithm is designed within a candidacy graph whose vertices are matching\ncandidates (i.e. a pair of source and target body parts), and iterates in two\nsteps for convergence. (i) It generates possible partial matches based on\ncompatible and competitive relations among body parts. (ii) It confirms the\npartial matches to generate a new matching solution, which is accepted by the\nMarkov Chain Monte Carlo (MCMC) mechanism. In the experiments, we demonstrate\nthe superior performance of our approach on three public databases compared to\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 13:47:49 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Xu", "Yuanlu", ""], ["Lin", "Liang", ""], ["Zheng", "Wei-Shi", ""], ["Liu", "Xiaobai", ""]]}, {"id": "1502.00258", "submitter": "Liang Lin", "authors": "Xiaodan Liang, Liang Lin, Liangliang Cao", "title": "Learning Latent Spatio-Temporal Compositional Model for Human Action\n  Recognition", "comments": "This manuscript has 10 pages with 7 figures, and a preliminary\n  version was published in ACM MM'13", "journal-ref": null, "doi": "10.1145/2502081.2502089", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition is an important problem in multimedia understanding. This\npaper addresses this problem by building an expressive compositional action\nmodel. We model one action instance in the video with an ensemble of\nspatio-temporal compositions: a number of discrete temporal anchor frames, each\nof which is further decomposed to a layout of deformable parts. In this way,\nour model can identify a Spatio-Temporal And-Or Graph (STAOG) to represent the\nlatent structure of actions e.g. triple jumping, swinging and high jumping. The\nSTAOG model comprises four layers: (i) a batch of leaf-nodes in bottom for\ndetecting various action parts within video patches; (ii) the or-nodes over\nbottom, i.e. switch variables to activate their children leaf-nodes for\nstructural variability; (iii) the and-nodes within an anchor frame for\nverifying spatial composition; and (iv) the root-node at top for aggregating\nscores over temporal anchor frames. Moreover, the contextual interactions are\ndefined between leaf-nodes in both spatial and temporal domains. For model\ntraining, we develop a novel weakly supervised learning algorithm which\niteratively determines the structural configuration (e.g. the production of\nleaf-nodes associated with the or-nodes) along with the optimization of\nmulti-layer parameters. By fully exploiting spatio-temporal compositions and\ninteractions, our approach handles well large intra-class action variance (e.g.\ndifferent views, individual appearances, spatio-temporal structures). The\nexperimental results on the challenging databases demonstrate superior\nperformance of our approach over other competing methods.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 13:49:31 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Liang", "Xiaodan", ""], ["Lin", "Liang", ""], ["Cao", "Liangliang", ""]]}, {"id": "1502.00303", "submitter": "Xianbiao Qi", "authors": "Xianbiao Qi, Chun-Guang Li, Guoying Zhao, Xiaopeng Hong, Matti\n  Pietik\\\"ainen", "title": "Dynamic texture and scene classification by transferring deep image\n  features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic texture and scene classification are two fundamental problems in\nunderstanding natural video content. Extracting robust and effective features\nis a crucial step towards solving these problems. However the existing\napproaches suffer from the sensitivity to either varying illumination, or\nviewpoint changing, or even camera motion, and/or the lack of spatial\ninformation. Inspired by the success of deep structures in image\nclassification, we attempt to leverage a deep structure to extract feature for\ndynamic texture and scene classification. To tackle with the challenges in\ntraining a deep structure, we propose to transfer some prior knowledge from\nimage domain to video domain. To be specific, we propose to apply a\nwell-trained Convolutional Neural Network (ConvNet) as a mid-level feature\nextractor to extract features from each frame, and then form a representation\nof a video by concatenating the first and the second order statistics over the\nmid-level features. We term this two-level feature extraction scheme as a\nTransferred ConvNet Feature (TCoF). Moreover we explore two different\nimplementations of the TCoF scheme, i.e., the \\textit{spatial} TCoF and the\n\\textit{temporal} TCoF, in which the mean-removed frames and the difference\nbetween two adjacent frames are used as the inputs of the ConvNet,\nrespectively. We evaluate systematically the proposed spatial TCoF and the\ntemporal TCoF schemes on three benchmark data sets, including DynTex, YUPENN,\nand Maryland, and demonstrate that the proposed approach yields superior\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 20:22:00 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Qi", "Xianbiao", ""], ["Li", "Chun-Guang", ""], ["Zhao", "Guoying", ""], ["Hong", "Xiaopeng", ""], ["Pietik\u00e4inen", "Matti", ""]]}, {"id": "1502.00319", "submitter": "Mahdi Salarian mr", "authors": "Mahdi Salarian, Rashid Ansari", "title": "Efficient refinement of GPS-based localization in urban areas using\n  visual information and sensor parameter", "comments": "this paper has been withdrawn bt the authors due to couple of errors\n  in exprimental part", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient method is proposed for refining GPS-acquired location\ncoordinates in urban areas using camera images, Google Street View (GSV) and\nsensor parameters. The main goal is to compensate for GPS location imprecision\nin dense area of cities due to proximity to walls and buildings. Avail-able\nmethods for better localization often use visual information by using query\nimages acquired with camera-equipped mobile devices and applying image\nretrieval techniques to find the closest match in a GPS-referenced image data\nset. The search areas required for reliable search are about 1-2 sq. Km and the\naccuracy is typically 25-100 meters. Here we describe a method based on image\nretrieval where a reliable search can be confined to areas of 0.01 sq. Km and\nthe accuracy in our experiments is less than 10 meters. To test our procedure\nwe created a database by acquiring all Google Street View images close to what\nis seen by a pedestrian in a large region of downtown Chicago and saved all\ncoordinates and orientation data to be used for confining our search region.\nPrior knowledge from approximate position of query image is leveraged to\naddress complexity and accuracy issues of our search in a large scale\ngeo-tagged data set. One key aspect that differentiates our work is that it\nutilizes the sensor information of GPS SOS and the camera orientation in\nimproving localization. Finally we demonstrate retrieval-based technique are\nless accurate in sparse open areas compared with purely GPS measurement. The\neffectiveness of our approach is discussed in detail and experimental results\nshow improved performance when compared with regular approaches.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 21:56:39 GMT"}, {"version": "v2", "created": "Sun, 15 Feb 2015 06:52:09 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2015 18:08:37 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Salarian", "Mahdi", ""], ["Ansari", "Rashid", ""]]}, {"id": "1502.00324", "submitter": "Mahdi Salarian mr", "authors": "M.Salarian, H. Miar Naimi", "title": "Modified Fast Fractal Image Compression Algorithm in spatial domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new fractal image compression algorithm is proposed in which\nthe time of encoding process is considerably reduced. The algorithm exploits a\ndomain pool reduction approach, along with using innovative predefined values\nfor contrast scaling factor, S, instead of searching it across [0,1]. Only the\ndomain blocks with entropy greater than a threshold are considered as domain\npool. As a novel point, it is assumed that in each step of the encoding\nprocess, the domain block with small enough distance shall be found only for\nthe range blocks with low activity (equivalently low entropy). This novel point\nis used to find reasonable estimations of S, and use them in the encoding\nprocess as predefined values, mentioned above, the remaining range blocks are\nsplit into four new smaller range blocks and the algorithm must be iterated for\nthem, considered as the other step of encoding process. The algorithm has been\nexamined for some of the well-known images and the results have been compared\nwith the state-of-the-art algorithms. The experiments show that our proposed\nalgorithm has considerably lower encoding time than the other where the encoded\nimages are approximately the same in quality.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 22:49:52 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Salarian", "M.", ""], ["Naimi", "H. Miar", ""]]}, {"id": "1502.00341", "submitter": "Liang Lin", "authors": "Liang Lin, Xiaolong Wang, Wei Yang, Jian-Huang Lai", "title": "Discriminatively Trained And-Or Graph Models for Object Shape Detection", "comments": "15 pages, 14 figures, TPAMI 2014", "journal-ref": "Pattern Analysis and Machine Intelligence, IEEE Transactions on ,\n  vol.PP, no.99, pp.1,1, 2014", "doi": "10.1109/TPAMI.2014.2359888", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a novel reconfigurable part-based model, namely\nAnd-Or graph model, to recognize object shapes in images. Our proposed model\nconsists of four layers: leaf-nodes at the bottom are local classifiers for\ndetecting contour fragments; or-nodes above the leaf-nodes function as the\nswitches to activate their child leaf-nodes, making the model reconfigurable\nduring inference; and-nodes in a higher layer capture holistic shape\ndeformations; one root-node on the top, which is also an or-node, activates one\nof its child and-nodes to deal with large global variations (e.g. different\nposes and views). We propose a novel structural optimization algorithm to\ndiscriminatively train the And-Or model from weakly annotated data. This\nalgorithm iteratively determines the model structures (e.g. the nodes and their\nlayouts) along with the parameter learning. On several challenging datasets,\nour model demonstrates the effectiveness to perform robust shape-based object\ndetection against background clutter and outperforms the other state-of-the-art\napproaches. We also release a new shape database with annotations, which\nincludes more than 1500 challenging shape instances, for recognition and\ndetection.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 02:04:01 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Lin", "Liang", ""], ["Wang", "Xiaolong", ""], ["Yang", "Wei", ""], ["Lai", "Jian-Huang", ""]]}, {"id": "1502.00344", "submitter": "Liang Lin", "authors": "Liang Lin, Yuanlu Xu, Xiaodan Liang, Jianhuang Lai", "title": "Complex Background Subtraction by Pursuing Dynamic Spatio-Temporal\n  Models", "comments": "12 pages, 7 figures", "journal-ref": "Image Processing, IEEE Transactions on , vol.23, no.7,\n  pp.3191,3202, July 2014", "doi": "10.1109/TIP.2014.2326776", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although it has been widely discussed in video surveillance, background\nsubtraction is still an open problem in the context of complex scenarios, e.g.,\ndynamic backgrounds, illumination variations, and indistinct foreground\nobjects. To address these challenges, we propose an effective background\nsubtraction method by learning and maintaining an array of dynamic texture\nmodels within the spatio-temporal representations. At any location of the\nscene, we extract a sequence of regular video bricks, i.e. video volumes\nspanning over both spatial and temporal domain. The background modeling is thus\nposed as pursuing subspaces within the video bricks while adapting the scene\nvariations. For each sequence of video bricks, we pursue the subspace by\nemploying the ARMA (Auto Regressive Moving Average) Model that jointly\ncharacterizes the appearance consistency and temporal coherence of the\nobservations. During online processing, we incrementally update the subspaces\nto cope with disturbances from foreground objects and scene changes. In the\nexperiments, we validate the proposed method in several complex scenarios, and\nshow superior performances over other state-of-the-art approaches of background\nsubtraction. The empirical studies of parameter setting and component analysis\nare presented as well.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 03:04:01 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Lin", "Liang", ""], ["Xu", "Yuanlu", ""], ["Liang", "Xiaodan", ""], ["Lai", "Jianhuang", ""]]}, {"id": "1502.00363", "submitter": "Wangmeng Zuo", "authors": "Wangmeng Zuo, Faqiang Wang, David Zhang, Liang Lin, Yuchi Huang, Deyu\n  Meng, Lei Zhang", "title": "Iterated Support Vector Machines for Distance Metric Learning", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance metric learning aims to learn from the given training data a valid\ndistance metric, with which the similarity between data samples can be more\neffectively evaluated for classification. Metric learning is often formulated\nas a convex or nonconvex optimization problem, while many existing metric\nlearning algorithms become inefficient for large scale problems. In this paper,\nwe formulate metric learning as a kernel classification problem, and solve it\nby iterated training of support vector machines (SVM). The new formulation is\neasy to implement, efficient in training, and tractable for large-scale\nproblems. Two novel metric learning models, namely Positive-semidefinite\nConstrained Metric Learning (PCML) and Nonnegative-coefficient Constrained\nMetric Learning (NCML), are developed. Both PCML and NCML can guarantee the\nglobal optimality of their solutions. Experimental results on UCI dataset\nclassification, handwritten digit recognition, face verification and person\nre-identification demonstrate that the proposed metric learning methods achieve\nhigher classification accuracy than state-of-the-art methods and they are\nsignificantly more efficient in training.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 05:30:44 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Zuo", "Wangmeng", ""], ["Wang", "Faqiang", ""], ["Zhang", "David", ""], ["Lin", "Liang", ""], ["Huang", "Yuchi", ""], ["Meng", "Deyu", ""], ["Zhang", "Lei", ""]]}, {"id": "1502.00374", "submitter": "Liang Lin", "authors": "Liang Lin, Ruimao Zhang, Xiaohua Duan", "title": "Adaptive Scene Category Discovery with Generative Learning and\n  Compositional Sampling", "comments": "11 pages, 7 figures", "journal-ref": "Circuits and Systems for Video Technology, IEEE Transactions on ,\n  vol.PP, no.99, pp.1,1, 2014", "doi": "10.1109/TCSVT.2014.2313897", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a general framework to discover categories of\nunlabeled scene images according to their appearances (i.e., textures and\nstructures). We jointly solve the two coupled tasks in an unsupervised manner:\n(i) classifying images without pre-determining the number of categories, and\n(ii) pursuing generative model for each category. In our method, each image is\nrepresented by two types of image descriptors that are effective to capture\nimage appearances from different aspects. By treating each image as a graph\nvertex, we build up an graph, and pose the image categorization as a graph\npartition process. Specifically, a partitioned sub-graph can be regarded as a\ncategory of scenes, and we define the probabilistic model of graph partition by\naccumulating the generative models of all separated categories. For efficient\ninference with the graph, we employ a stochastic cluster sampling algorithm,\nwhich is designed based on the Metropolis-Hasting mechanism. During the\niterations of inference, the model of each category is analytically updated by\na generative learning algorithm. In the experiments, our approach is validated\non several challenging databases, and it outperforms other popular\nstate-of-the-art methods. The implementation details and empirical analysis are\npresented as well.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 06:33:51 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Lin", "Liang", ""], ["Zhang", "Ruimao", ""], ["Duan", "Xiaohua", ""]]}, {"id": "1502.00377", "submitter": "Liang Lin", "authors": "Liang Lin, Yongyi Lu, Yan Pan, Xiaowu Chen", "title": "Integrating Graph Partitioning and Matching for Trajectory Analysis in\n  Video Surveillance", "comments": "10 pages, 12 figures", "journal-ref": "Image Processing, IEEE Transactions on , vol.21, no.12,\n  pp.4844-4857, 2012", "doi": "10.1109/TIP.2012.2211373", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to track the moving objects in long range against occlusion,\ninterruption, and background clutter, this paper proposes a unified approach\nfor global trajectory analysis. Instead of the traditional frame-by-frame\ntracking, our method recovers target trajectories based on a short sequence of\nvideo frames, e.g. $15$ frames. We initially calculate a foreground map at each\nframe, as obtained from a state-of-the-art background model. An attribute graph\nis then extracted from the foreground map, where the graph vertices are image\nprimitives represented by the composite features. With this graph\nrepresentation, we pose trajectory analysis as a joint task of spatial graph\npartitioning and temporal graph matching. The task can be formulated by\nmaximizing a posteriori under the Bayesian framework, in which we integrate the\nspatio-temporal contexts and the appearance models. The probabilistic inference\nis achieved by a data-driven Markov Chain Monte Carlo (MCMC) algorithm. Given a\nperoid of observed frames, the algorithm simulates a ergodic and aperiodic\nMarkov Chain, and it visits a sequence of solution states in the joint space of\nspatial graph partitioning and temporal graph matching. In the experiments, our\nmethod is tested on several challenging videos from the public datasets of\nvisual surveillance, and it outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 06:52:47 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Lin", "Liang", ""], ["Lu", "Yongyi", ""], ["Pan", "Yan", ""], ["Chen", "Xiaowu", ""]]}, {"id": "1502.00416", "submitter": "Liang Lin", "authors": "Bo Jiang, Yongyi Lu, Xiying Li, Liang Lin", "title": "Towards a solid solution of real-time fire and flame detection", "comments": "14 pages, 6 figures", "journal-ref": "Multimedia Tools and Applications, pp. 1380-7501, 2014", "doi": "10.1007/s11042-014-2106-z", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the object detection and recognition has received growing attention\nfor decades, a robust fire and flame detection method is rarely explored. This\npaper presents an empirical study, towards a general and solid approach to fast\ndetect fire and flame in videos, with the applications in video surveillance\nand event retrieval. Our system consists of three cascaded steps: (1) candidate\nregions proposing by a background model, (2) fire region classifying with\ncolor-texture features and a dictionary of visual words, and (3) temporal\nverifying. The experimental evaluation and analysis are done for each step. We\nbelieve that it is a useful service to both academic research and real-world\napplication. In addition, we release the software of the proposed system with\nthe source code, as well as a public benchmark and data set, including 64 video\nclips covered both indoor and outdoor scenes under different conditions. We\nachieve an 82% Recall with 93% Precision on the data set, and greatly improve\nthe performance by state-of-the-arts methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 09:34:01 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Jiang", "Bo", ""], ["Lu", "Yongyi", ""], ["Li", "Xiying", ""], ["Lin", "Liang", ""]]}, {"id": "1502.00478", "submitter": "Weiyang Liu", "authors": "Yandong Wen, Weiyang Liu, Meng Yang, Yuli Fu, Youjun Xiang, Rui Hu", "title": "Structured Occlusion Coding for Robust Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusion in face recognition is a common yet challenging problem. While\nsparse representation based classification (SRC) has been shown promising\nperformance in laboratory conditions (i.e. noiseless or random pixel\ncorrupted), it performs much worse in practical scenarios. In this paper, we\nconsider the practical face recognition problem, where the occlusions are\npredictable and available for sampling. We propose the structured occlusion\ncoding (SOC) to address occlusion problems. The structured coding here lies in\ntwo folds. On one hand, we employ a structured dictionary for recognition. On\nthe other hand, we propose to use the structured sparsity in this formulation.\nSpecifically, SOC simultaneously separates the occlusion and classifies the\nimage. In this way, the problem of recognizing an occluded image is turned into\nseeking a structured sparse solution on occlusion-appended dictionary. In order\nto construct a well-performing occlusion dictionary, we propose an occlusion\nmask estimating technique via locality constrained dictionary (LCD), showing\nstriking improvement in occlusion sample. On a category-specific occlusion\ndictionary, we replace norm sparsity with the structured sparsity which is\nshown more robust, further enhancing the robustness of our approach. Moreover,\nSOC achieves significant improvement in handling large occlusion in real world.\nExtensive experiments are conducted on public data sets to validate the\nsuperiority of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 13:48:46 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2015 06:24:26 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Wen", "Yandong", ""], ["Liu", "Weiyang", ""], ["Yang", "Meng", ""], ["Fu", "Yuli", ""], ["Xiang", "Youjun", ""], ["Hu", "Rui", ""]]}, {"id": "1502.00500", "submitter": "Miguel Heredia Conde", "authors": "Miguel Heredia, Felix Endres, Wolfram Burgard and Rafael Sanz", "title": "Fast and Robust Feature Matching for RGB-D Based Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel approach to global localization using an\nRGB-D camera in maps of visual features. For large maps, the performance of\npure image matching techniques decays in terms of robustness and computational\ncost. Particularly, repeated occurrences of similar features due to repeating\nstructure in the world (e.g., doorways, chairs, etc.) or missing associations\nbetween observations pose critical challenges to visual localization. We\naddress these challenges using a two-step approach. We first estimate a\ncandidate pose using few correspondences between features of the current camera\nframe and the feature map. The initial set of correspondences is established by\nproximity in feature space. The initial pose estimate is used in the second\nstep to guide spatial matching of features in 3D, i.e., searching for\nassociations where the image features are expected to be found in the map. A\nRANSAC algorithm is used to compute a fine estimation of the pose from the\ncorrespondences. Our approach clearly outperforms localization based on feature\nmatching exclusively in feature space, both in terms of estimation accuracy and\nrobustness to failure and allows for global localization in real time (30Hz).\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 15:02:10 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Heredia", "Miguel", ""], ["Endres", "Felix", ""], ["Burgard", "Wolfram", ""], ["Sanz", "Rafael", ""]]}, {"id": "1502.00501", "submitter": "Liang Lin", "authors": "Zhujin Liang, Xiaolong Wang, Rui Huang, Liang Lin", "title": "An Expressive Deep Model for Human Action Parsing from A Single Image", "comments": "6 pages, 8 figures, ICME 2014", "journal-ref": null, "doi": "10.1109/ICME.2014.6890158", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at one newly raising task in vision and multimedia research:\nrecognizing human actions from still images. Its main challenges lie in the\nlarge variations in human poses and appearances, as well as the lack of\ntemporal motion information. Addressing these problems, we propose to develop\nan expressive deep model to naturally integrate human layout and surrounding\ncontexts for higher level action understanding from still images. In\nparticular, a Deep Belief Net is trained to fuse information from different\nnoisy sources such as body part detection and object detection. To bridge the\nsemantic gap, we used manually labeled data to greatly improve the\neffectiveness and efficiency of the pre-training and fine-tuning stages of the\nDBN training. The resulting framework is shown to be robust to sometimes\nunreliable inputs (e.g., imprecise detections of human parts and objects), and\noutperforms the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 15:03:25 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Liang", "Zhujin", ""], ["Wang", "Xiaolong", ""], ["Huang", "Rui", ""], ["Lin", "Liang", ""]]}, {"id": "1502.00555", "submitter": "Renato J Cintra", "authors": "P. A. M. Oliveira, R. J. Cintra, F. M. Bayer, S. Kulasekera, A.\n  Madanayake", "title": "A Discrete Tchebichef Transform Approximation for Image and Video Coding", "comments": "13 pages, 5 figures, 2 tables", "journal-ref": "IEEE Signal Processing Letters, vol. 22, issue 8, pp. 1137-1141,\n  2015", "doi": "10.1109/LSP.2015.2389899", "report-no": null, "categories": "stat.ME cs.CV cs.MM cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a low-complexity approximation for the discrete\nTchebichef transform (DTT). The proposed forward and inverse transforms are\nmultiplication-free and require a reduced number of additions and bit-shifting\noperations. Numerical compression simulations demonstrate the efficiency of the\nproposed transform for image and video coding. Furthermore, Xilinx Virtex-6\nFPGA based hardware realization shows 44.9% reduction in dynamic power\nconsumption and 64.7% lower area when compared to the literature.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 14:07:44 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Oliveira", "P. A. M.", ""], ["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["Kulasekera", "S.", ""], ["Madanayake", "A.", ""]]}, {"id": "1502.00558", "submitter": "Marcelo Cicconet", "authors": "Marcelo Cicconet, Davi Geiger, and Michael Werman", "title": "Complex-Valued Hough Transforms for Circles", "comments": "The paper has been withdrawn since the authors concluded a more\n  comprehensive study on the choice of parameters needs to be performed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper advocates the use of complex variables to represent votes in the\nHough transform for circle detection. Replacing the positive numbers\nclassically used in the parameter space of the Hough transforms by complex\nnumbers allows cancellation effects when adding up the votes. Cancellation and\nthe computation of shape likelihood via a complex number's magnitude square\nlead to more robust solutions than the \"classic\" algorithms, as shown by\ncomputational experiments on synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 17:22:26 GMT"}, {"version": "v2", "created": "Mon, 9 Feb 2015 19:38:58 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Cicconet", "Marcelo", ""], ["Geiger", "Davi", ""], ["Werman", "Michael", ""]]}, {"id": "1502.00561", "submitter": "Marcelo Cicconet", "authors": "Marcelo Cicconet, Davi Geiger, and Michael Werman", "title": "Quantum Pairwise Symmetry: Applications in 2D Shape Analysis", "comments": "The paper has been withdrawn since the authors concluded a more\n  comprehensive study on the choice of parameters needs to be performed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pair of rooted tangents -- defining a quantum triangle -- with an\nassociated quantum wave of spin 1/2 is proposed as the primitive to represent\nand compute symmetry. Measures of the spin characterize how \"isosceles\" or how\n\"degenerate\" these triangles are -- which corresponds to their mirror or\nparallel symmetry. We also introduce a complex-valued kernel to model\nprobability errors in the parameter space, which is more robust to noise and\nclutter than the classical model.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 17:33:55 GMT"}, {"version": "v2", "created": "Mon, 9 Feb 2015 19:38:39 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Cicconet", "Marcelo", ""], ["Geiger", "Davi", ""], ["Werman", "Michael", ""]]}, {"id": "1502.00592", "submitter": "Renato J Cintra", "authors": "C. J. Tablada, F. M. Bayer, R. J. Cintra", "title": "A Class of DCT Approximations Based on the Feig-Winograd Algorithm", "comments": "26 pages, 4 figures, 5 tables, fixed arithmetic complexity in Table\n  IV", "journal-ref": "Signal Processing, vol. 113, pp. 38-51, August 2015", "doi": "10.1016/j.sigpro.2015.01.011", "report-no": null, "categories": "stat.ME cs.CV cs.MM cs.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of matrices based on a parametrization of the Feig-Winograd\nfactorization of 8-point DCT is proposed. Such parametrization induces a matrix\nsubspace, which unifies a number of existing methods for DCT approximation. By\nsolving a comprehensive multicriteria optimization problem, we identified\nseveral new DCT approximations. Obtained solutions were sought to possess the\nfollowing properties: (i) low multiplierless computational complexity, (ii)\northogonality or near orthogonality, (iii) low complexity invertibility, and\n(iv) close proximity and performance to the exact DCT. Proposed approximations\nwere submitted to assessment in terms of proximity to the DCT, coding\nperformance, and suitability for image compression. Considering Pareto\nefficiency, particular new proposed approximations could outperform various\nexisting methods archived in literature.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 19:39:46 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 21:25:18 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Tablada", "C. J.", ""], ["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""]]}, {"id": "1502.00652", "submitter": "Lubor Ladicky", "authors": "\\v{L}ubor Ladick\\'y, Christian H\\\"ane and Marc Pollefeys", "title": "Learning the Matching Function", "comments": "rejected from ACCV 2014 and probably from CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The matching function for the problem of stereo reconstruction or optical\nflow has been traditionally designed as a function of the distance between the\nfeatures describing matched pixels. This approach works under assumption, that\nthe appearance of pixels in two stereo cameras or in two consecutive video\nframes does not change dramatically. However, this might not be the case, if we\ntry to match pixels over a large interval of time.\n  In this paper we propose a method, which learns the matching function, that\nautomatically finds the space of allowed changes in visual appearance, such as\ndue to the motion blur, chromatic distortions, different colour calibration or\nseasonal changes. Furthermore, it automatically learns the importance of\nmatching scores of contextual features at different relative locations and\nscales. Proposed classifier gives reliable estimations of pixel disparities\nalready without any form of regularization.\n  We evaluated our method on two standard problems - stereo matching on KITTI\noutdoor dataset, optical flow on Sintel data set, and on newly introduced\nTimeLapse change detection dataset. Our algorithm obtained very promising\nresults comparable to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 21:20:43 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Ladick\u00fd", "\u013dubor", ""], ["H\u00e4ne", "Christian", ""], ["Pollefeys", "Marc", ""]]}, {"id": "1502.00705", "submitter": "Greg Ongie", "authors": "Greg Ongie and Mathews Jacob", "title": "Recovery of Piecewise Smooth Images from Few Fourier Samples", "comments": "5 pages. Submitted to SampTA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Prony-like method to recover a continuous domain 2-D piecewise\nsmooth image from few of its Fourier samples. Assuming the discontinuity set of\nthe image is localized to the zero level-set of a trigonometric polynomial, we\nshow the Fourier transform coefficients of partial derivatives of the signal\nsatisfy an annihilation relation. We present necessary and sufficient\nconditions for unique recovery of piecewise constant images using the above\nannihilation relation. We pose the recovery of the Fourier coefficients of the\nsignal from the measurements as a convex matrix completion algorithm, which\nrelies on the lifting of the Fourier data to a structured low-rank matrix; this\napproach jointly estimates the signal and the annihilating filter. Finally, we\ndemonstrate our algorithm on the recovery of MRI phantoms from few\nlow-resolution Fourier samples.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 01:48:27 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Ongie", "Greg", ""], ["Jacob", "Mathews", ""]]}, {"id": "1502.00712", "submitter": "Liang Lin", "authors": "Zhanglin Peng, Liang Lin, Ruimao Zhang, Jing Xu", "title": "Deep Boosting: Layered Feature Mining for General Image Classification", "comments": "6 pages, 4 figures, ICME 2014", "journal-ref": "Multimedia and Expo (ICME), 2014 IEEE International Conference on\n  , vol., no., pp.1,6, 14-18 July 2014", "doi": "10.1109/ICME.2014.6890323", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing effective representations is a critical but challenging problem\nin multimedia understanding. The traditional handcraft features often rely on\ndomain knowledge, limiting the performances of exiting methods. This paper\ndiscusses a novel computational architecture for general image feature mining,\nwhich assembles the primitive filters (i.e. Gabor wavelets) into compositional\nfeatures in a layer-wise manner. In each layer, we produce a number of base\nclassifiers (i.e. regression stumps) associated with the generated features,\nand discover informative compositions by using the boosting algorithm. The\noutput compositional features of each layer are treated as the base components\nto build up the next layer. Our framework is able to generate expressive image\nrepresentations while inducing very discriminate functions for image\nclassification. The experiments are conducted on several public datasets, and\nwe demonstrate superior performances over state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 02:44:10 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Peng", "Zhanglin", ""], ["Lin", "Liang", ""], ["Zhang", "Ruimao", ""], ["Xu", "Jing", ""]]}, {"id": "1502.00717", "submitter": "Hongyuan Zhu", "authors": "Hongyuan Zhu, Fanman Meng, Jianfei Cai, Shijian Lu", "title": "Beyond Pixels: A Comprehensive Survey from Bottom-up to Semantic Image\n  Segmentation and Cosegmentation", "comments": "submitted to Elsevier Journal of Visual Communications and Image\n  Representation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Image segmentation refers to the process to divide an image into\nnonoverlapping meaningful regions according to human perception, which has\nbecome a classic topic since the early ages of computer vision. A lot of\nresearch has been conducted and has resulted in many applications. However,\nwhile many segmentation algorithms exist, yet there are only a few sparse and\noutdated summarizations available, an overview of the recent achievements and\nissues is lacking. We aim to provide a comprehensive review of the recent\nprogress in this field. Covering 180 publications, we give an overview of broad\nareas of segmentation topics including not only the classic bottom-up\napproaches, but also the recent development in superpixel, interactive methods,\nobject proposals, semantic image parsing and image cosegmentation. In addition,\nwe also review the existing influential datasets and evaluation metrics.\nFinally, we suggest some design flavors and research directions for future\nresearch in image segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 03:00:52 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Zhu", "Hongyuan", ""], ["Meng", "Fanman", ""], ["Cai", "Jianfei", ""], ["Lu", "Shijian", ""]]}, {"id": "1502.00723", "submitter": "Liang Lin", "authors": "Liang Lin, Xiaolong Wang, Wei Yang, Jianhuang Lai", "title": "Learning Contour-Fragment-based Shape Model with And-Or Tree\n  Representation", "comments": "8 pages, 7 figures, CVPR 2012", "journal-ref": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE\n  Conference on , vol., no., pp.135,142, 16-21 June 2012", "doi": "10.1109/CVPR.2012.6247668", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a simple yet effective method to learn the hierarchical\nobject shape model consisting of local contour fragments, which represents a\ncategory of shapes in the form of an And-Or tree. This model extends the\ntraditional hierarchical tree structures by introducing the \"switch\" variables\n(i.e. the or-nodes) that explicitly specify production rules to capture shape\nvariations. We thus define the model with three layers: the leaf-nodes for\ndetecting local contour fragments, the or-nodes specifying selection of\nleaf-nodes, and the root-node encoding the holistic distortion. In the training\nstage, for optimization of the And-Or tree learning, we extend the\nconcave-convex procedure (CCCP) by embedding the structural clustering during\nthe iterative learning steps. The inference of shape detection is consistent\nwith the model optimization, which integrates the local testings via the\nleaf-nodes and or-nodes with the global verification via the root-node. The\nadvantages of our approach are validated on the challenging shape databases\n(i.e., ETHZ and INRIA Horse) and summarized as follows. (1) The proposed method\nis able to accurately localize shape contours against unreliable edge detection\nand edge tracing. (2) The And-Or tree model enables us to well capture the\nintraclass variance.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 03:42:10 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Lin", "Liang", ""], ["Wang", "Xiaolong", ""], ["Yang", "Wei", ""], ["Lai", "Jianhuang", ""]]}, {"id": "1502.00739", "submitter": "Liang Lin", "authors": "Wei Yang, Ping Luo, Liang Lin", "title": "Clothing Co-Parsing by Joint Image Segmentation and Labeling", "comments": "8 pages, 5 figures, CVPR 2014", "journal-ref": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE\n  Conference on , vol., no., pp.3182,3189, 23-28 June 2014", "doi": "10.1109/CVPR.2014.407", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at developing an integrated system of clothing co-parsing, in\norder to jointly parse a set of clothing images (unsegmented but annotated with\ntags) into semantic configurations. We propose a data-driven framework\nconsisting of two phases of inference. The first phase, referred as \"image\nco-segmentation\", iterates to extract consistent regions on images and jointly\nrefines the regions over all images by employing the exemplar-SVM (E-SVM)\ntechnique [23]. In the second phase (i.e. \"region co-labeling\"), we construct a\nmulti-image graphical model by taking the segmented regions as vertices, and\nincorporate several contexts of clothing configuration (e.g., item location and\nmutual interactions). The joint label assignment can be solved using the\nefficient Graph Cuts algorithm. In addition to evaluate our framework on the\nFashionista dataset [30], we construct a dataset called CCP consisting of 2098\nhigh-resolution street fashion photos to demonstrate the performance of our\nsystem. We achieve 90.29% / 88.23% segmentation accuracy and 65.52% / 63.89%\nrecognition rate on the Fashionista and the CCP datasets, respectively, which\nare superior compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 04:59:41 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Yang", "Wei", ""], ["Luo", "Ping", ""], ["Lin", "Liang", ""]]}, {"id": "1502.00741", "submitter": "Liang Lin", "authors": "Xiaolong Wang, Liang Lin", "title": "Dynamical And-Or Graph Learning for Object Shape Modeling and Detection", "comments": "9 pages, 4 figures, NIPS 2012", "journal-ref": "Advances in Neural Information Processing Systems (pp. 242-250),\n  2014", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a novel discriminative part-based model to represent and\nrecognize object shapes with an \"And-Or graph\". We define this model consisting\nof three layers: the leaf-nodes with collaborative edges for localizing local\nparts, the or-nodes specifying the switch of leaf-nodes, and the root-node\nencoding the global verification. A discriminative learning algorithm, extended\nfrom the CCCP [23], is proposed to train the model in a dynamical manner: the\nmodel structure (e.g., the configuration of the leaf-nodes associated with the\nor-nodes) is automatically determined with optimizing the multi-layer\nparameters during the iteration. The advantages of our method are two-fold. (i)\nThe And-Or graph model enables us to handle well large intra-class variance and\nbackground clutters for object shape detection from images. (ii) The proposed\nlearning algorithm is able to obtain the And-Or graph representation without\nrequiring elaborate supervision and initialization. We validate the proposed\nmethod on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and\nUIUC-People), and it outperforms the state-of-the-arts approaches.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 05:14:40 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Wang", "Xiaolong", ""], ["Lin", "Liang", ""]]}, {"id": "1502.00743", "submitter": "Liang Lin", "authors": "Xiaolong Wang, Liliang Zhang, Liang Lin, Zhujin Liang, Wangmeng Zuo", "title": "Deep Joint Task Learning for Generic Object Extraction", "comments": "9 pages, 4 figures, NIPS 2014", "journal-ref": "Advances in Neural Information Processing Systems (pp. 523-531),\n  2014", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates how to extract objects-of-interest without relying on\nhand-craft features and sliding windows approaches, that aims to jointly solve\ntwo sub-tasks: (i) rapidly localizing salient objects from images, and (ii)\naccurately segmenting the objects based on the localizations. We present a\ngeneral joint task learning framework, in which each task (either object\nlocalization or object segmentation) is tackled via a multi-layer convolutional\nneural network, and the two networks work collaboratively to boost performance.\nIn particular, we propose to incorporate latent variables bridging the two\nnetworks in a joint optimization manner. The first network directly predicts\nthe positions and scales of salient objects from raw images, and the latent\nvariables adjust the object localizations to feed the second network that\nproduces pixelwise object masks. An EM-type method is presented for the\noptimization, iterating with two steps: (i) by using the two networks, it\nestimates the latent variables by employing an MCMC-based sampling method; (ii)\nit optimizes the parameters of the two networks unitedly via back propagation,\nwith the fixed latent variables. Extensive experiments suggest that our\nframework significantly outperforms other state-of-the-art approaches in both\naccuracy and efficiency (e.g. 1000 times faster than competing approaches).\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 05:35:09 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Wang", "Xiaolong", ""], ["Zhang", "Liliang", ""], ["Lin", "Liang", ""], ["Liang", "Zhujin", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "1502.00744", "submitter": "Liang Lin", "authors": "Xiaolong Wang, Liang Lin, Lichao Huang, Shuicheng Yan", "title": "Incorporating Structural Alternatives and Sharing into Hierarchy for\n  Multiclass Object Recognition and Detection", "comments": "8 pages, 6 figures, CVPR 2013", "journal-ref": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE\n  Conference on , vol., no., pp.3334,3341, 23-28 June 2013", "doi": "10.1109/CVPR.2013.428", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a reconfigurable model to recognize and detect multiclass\n(or multiview) objects with large variation in appearance. Compared with well\nacknowledged hierarchical models, we study two advanced capabilities in\nhierarchy for object modeling: (i) \"switch\" variables(i.e. or-nodes) for\nspecifying alternative compositions, and (ii) making local classifiers (i.e.\nleaf-nodes) shared among different classes. These capabilities enable us to\naccount well for structural variabilities while preserving the model compact.\nOur model, in the form of an And-Or Graph, comprises four layers: a batch of\nleaf-nodes with collaborative edges in bottom for localizing object parts; the\nor-nodes over bottom to activate their children leaf-nodes; the and-nodes to\nclassify objects as a whole; one root-node on the top for switching multiclass\nclassification, which is also an or-node. For model training, we present an\nEM-type algorithm, namely dynamical structural optimization (DSO), to\niteratively determine the structural configuration, (e.g., leaf-node generation\nassociated with their parent or-nodes and shared across other classes), along\nwith optimizing multi-layer parameters. The proposed method is valid on\nchallenging databases, e.g., PASCAL VOC 2007 and UIUC-People, and it achieves\nstate-of-the-arts performance.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 05:45:56 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Wang", "Xiaolong", ""], ["Lin", "Liang", ""], ["Huang", "Lichao", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1502.00749", "submitter": "Liang Lin", "authors": "Xionghao Liu, Wei Yang, Liang Lin, Qing Wang, Zhaoquan Cai, Jianhuang\n  Lai", "title": "Data-Driven Scene Understanding with Adaptively Retrieved Exemplars", "comments": "8 pages, 5 figures", "journal-ref": "MultiMedia, IEEE , vol.PP, no.99, pp.1,1, 2015", "doi": "10.1109/MMUL.2015.22", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article investigates a data-driven approach for semantically scene\nunderstanding, without pixelwise annotation and classifier training. Our\nframework parses a target image with two steps: (i) retrieving its exemplars\n(i.e. references) from an image database, where all images are unsegmented but\nannotated with tags; (ii) recovering its pixel labels by propagating semantics\nfrom the references. We present a novel framework making the two steps mutually\nconditional and bootstrapped under the probabilistic Expectation-Maximization\n(EM) formulation. In the first step, the references are selected by jointly\nmatching their appearances with the target as well as the semantics (i.e. the\nassigned labels of the target and the references). We process the second step\nvia a combinatorial graphical representation, in which the vertices are\nsuperpixels extracted from the target and its selected references. Then we\nderive the potentials of assigning labels to one vertex of the target, which\ndepend upon the graph edges that connect the vertex to its spatial neighbors of\nthe target and to its similar vertices of the references. Besides, the proposed\nframework can be naturally applied to perform image annotation on new test\nimages. In the experiments, we validate our approach on two public databases,\nand demonstrate superior performances over the state-of-the-art methods in both\nsemantic segmentation and image annotation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 06:04:14 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Liu", "Xionghao", ""], ["Yang", "Wei", ""], ["Lin", "Liang", ""], ["Wang", "Qing", ""], ["Cai", "Zhaoquan", ""], ["Lai", "Jianhuang", ""]]}, {"id": "1502.00750", "submitter": "Liang Lin", "authors": "Xiaodan Liang, Qingxing Cao, Rui Huang, Liang Lin", "title": "Recognizing Focal Liver Lesions in Contrast-Enhanced Ultrasound with\n  Discriminatively Trained Spatio-Temporal Model", "comments": "5 pages, 1 figures", "journal-ref": "Biomedical Imaging (ISBI), 2014 IEEE 11th International Symposium\n  on , vol., no., pp.1184-1187, April 2014", "doi": "10.1109/ISBI.2014.6868087", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this study is to provide an automatic computational framework to\nassist clinicians in diagnosing Focal Liver Lesions (FLLs) in\nContrast-Enhancement Ultrasound (CEUS). We represent FLLs in a CEUS video clip\nas an ensemble of Region-of-Interests (ROIs), whose locations are modeled as\nlatent variables in a discriminative model. Different types of FLLs are\ncharacterized by both spatial and temporal enhancement patterns of the ROIs.\nThe model is learned by iteratively inferring the optimal ROI locations and\noptimizing the model parameters. To efficiently search the optimal spatial and\ntemporal locations of the ROIs, we propose a data-driven inference algorithm by\ncombining effective spatial and temporal pruning. The experiments show that our\nmethod achieves promising results on the largest dataset in the literature (to\nthe best of our knowledge), which we have made publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 06:14:30 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Liang", "Xiaodan", ""], ["Cao", "Qingxing", ""], ["Huang", "Rui", ""], ["Lin", "Liang", ""]]}, {"id": "1502.00756", "submitter": "Shonal Chaudhry", "authors": "Shonal Chaudhry and Rohitash Chandra", "title": "Design of a Mobile Face Recognition System for Visually Impaired Persons", "comments": "Added author names in sections 1 and 2. Certain details in sections 3\n  and 4 are now clearer. Removed external camera from implementation, results\n  unaffected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is estimated that 285 million people globally are visually impaired. A\nmajority of these people live in developing countries and are among the elderly\npopulation. One of the most difficult tasks faced by the visually impaired is\nidentification of people. While naturally, voice recognition is a common method\nof identification, it is an intuitive and difficult process. The rise of\ncomputation capability of mobile devices gives motivation to develop\napplications that can assist visually impaired persons. With the availability\nof mobile devices, these people can be assisted by an additional method of\nidentification through intelligent software based on computer vision\ntechniques. In this paper, we present the design and implementation of a face\ndetection and recognition system for the visually impaired through the use of\nmobile computing. This mobile system is assisted by a server-based support\nsystem. The system was tested on a custom video database. Experiment results\nshow high face detection accuracy and promising face recognition accuracy in\nsuitable conditions. The challenges of the system lie in better recognition\ntechniques for difficult situations in terms of lighting and weather.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 06:38:22 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 09:58:48 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Chaudhry", "Shonal", ""], ["Chandra", "Rohitash", ""]]}, {"id": "1502.00836", "submitter": "Xiaoxia Sun", "authors": "Xiaoxia Sun, Nasser M. Nasrabadi, and Trac D. Tran", "title": "Task-Driven Dictionary Learning for Hyperspectral Image Classification\n  with Structured Sparsity Constraints", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2015.2399978", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation models a signal as a linear combination of a small\nnumber of dictionary atoms. As a generative model, it requires the dictionary\nto be highly redundant in order to ensure both a stable high sparsity level and\na low reconstruction error for the signal. However, in practice, this\nrequirement is usually impaired by the lack of labelled training samples.\nFortunately, previous research has shown that the requirement for a redundant\ndictionary can be less rigorous if simultaneous sparse approximation is\nemployed, which can be carried out by enforcing various structured sparsity\nconstraints on the sparse codes of the neighboring pixels. In addition,\nnumerous works have shown that applying a variety of dictionary learning\nmethods for the sparse representation model can also improve the classification\nperformance. In this paper, we highlight the task-driven dictionary learning\nalgorithm, which is a general framework for the supervised dictionary learning\nmethod. We propose to enforce structured sparsity priors on the task-driven\ndictionary learning method in order to improve the performance of the\nhyperspectral classification. Our approach is able to benefit from both the\nadvantages of the simultaneous sparse representation and those of the\nsupervised dictionary learning. We enforce two different structured sparsity\npriors, the joint and Laplacian sparsity, on the task-driven dictionary\nlearning method and provide the details of the corresponding optimization\nalgorithms. Experiments on numerous popular hyperspectral images demonstrate\nthat the classification performance of our approach is superior to sparse\nrepresentation classifier with structured priors or the task-driven dictionary\nlearning method.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 12:24:40 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Sun", "Xiaoxia", ""], ["Nasrabadi", "Nasser M.", ""], ["Tran", "Trac D.", ""]]}, {"id": "1502.00852", "submitter": "Christos Sagonas", "authors": "Christos Sagonas, Yannis Panagakis, Stefanos Zafeiriou and Maja Pantic", "title": "Face frontalization for Alignment and Recognition", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it was shown that excellent results can be achieved in both face\nlandmark localization and pose-invariant face recognition. These breakthroughs\nare attributed to the efforts of the community to manually annotate facial\nimages in many different poses and to collect 3D faces data. In this paper, we\npropose a novel method for joint face landmark localization and frontal face\nreconstruction (pose correction) using a small set of frontal images only. By\nobserving that the frontal facial image is the one with the minimum rank from\nall different poses we formulate an appropriate model which is able to jointly\nrecover the facial landmarks as well as the frontalized version of the face. To\nthis end, a suitable optimization problem, involving the minimization of the\nnuclear norm and the matrix $\\ell_1$ norm, is solved. The proposed method is\nassessed in frontal face reconstruction (pose correction), face landmark\nlocalization, and pose-invariant face recognition and verification by\nconducting experiments on $6$ facial images databases. The experimental results\ndemonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 13:05:34 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Sagonas", "Christos", ""], ["Panagakis", "Yannis", ""], ["Zafeiriou", "Stefanos", ""], ["Pantic", "Maja", ""]]}, {"id": "1502.00873", "submitter": "Yi Sun", "authors": "Yi Sun, Ding Liang, Xiaogang Wang, Xiaoou Tang", "title": "DeepID3: Face Recognition with Very Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art of face recognition has been significantly advanced by\nthe emergence of deep learning. Very deep neural networks recently achieved\ngreat success on general object recognition because of their superb learning\ncapacity. This motivates us to investigate their effectiveness on face\nrecognition. This paper proposes two very deep neural network architectures,\nreferred to as DeepID3, for face recognition. These two architectures are\nrebuilt from stacked convolution and inception layers proposed in VGG net and\nGoogLeNet to make them suitable to face recognition. Joint face\nidentification-verification supervisory signals are added to both intermediate\nand final feature extraction layers during training. An ensemble of the\nproposed two architectures achieves 99.53% LFW face verification accuracy and\n96.0% LFW rank-1 face identification accuracy, respectively. A further\ndiscussion of LFW face verification result is given in the end.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 14:28:55 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Sun", "Yi", ""], ["Liang", "Ding", ""], ["Wang", "Xiaogang", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1502.00946", "submitter": "Sofya Chepushtanova", "authors": "Sofya Chepushtanova and Michael Kirby", "title": "Classification of Hyperspectral Imagery on Embedded Grassmannians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for capturing the signal variability in hyperspectral\nimagery using the framework of the Grassmann manifold. Labeled points from each\nclass are sampled and used to form abstract points on the Grassmannian. The\nresulting points on the Grassmannian have representations as orthonormal\nmatrices and as such do not reside in Euclidean space in the usual sense. There\nare a variety of metrics which allow us to determine a distance matrices that\ncan be used to realize the Grassmannian as an embedding in Euclidean space. We\nillustrate that we can achieve an approximately isometric embedding of the\nGrassmann manifold using the chordal metric while this is not the case with\ngeodesic distances. However, non-isometric embeddings generated by using a\npseudometric on the Grassmannian lead to the best classification results. We\nobserve that as the dimension of the Grassmannian grows, the accuracy of the\nclassification grows to 100% on two illustrative examples. We also observe a\ndecrease in classification rates if the dimension of the points on the\nGrassmannian is too large for the dimension of the Euclidean space. We use\nsparse support vector machines to perform additional model reduction. The\nresulting classifier selects a subset of dimensions of the embedding without\nloss in classification performance.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 18:17:13 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Chepushtanova", "Sofya", ""], ["Kirby", "Michael", ""]]}, {"id": "1502.00956", "submitter": "Raul Mur-Artal", "authors": "Raul Mur-Artal, J. M. M. Montiel and Juan D. Tardos", "title": "ORB-SLAM: a Versatile and Accurate Monocular SLAM System", "comments": "17 pages. 13 figures. IEEE Transactions on Robotics, 2015. Project\n  webpage (videos, code): http://webdiis.unizar.es/~raulmur/orbslam/", "journal-ref": null, "doi": "10.1109/TRO.2015.2463671", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents ORB-SLAM, a feature-based monocular SLAM system that\noperates in real time, in small and large, indoor and outdoor environments. The\nsystem is robust to severe motion clutter, allows wide baseline loop closing\nand relocalization, and includes full automatic initialization. Building on\nexcellent algorithms of recent years, we designed from scratch a novel system\nthat uses the same features for all SLAM tasks: tracking, mapping,\nrelocalization, and loop closing. A survival of the fittest strategy that\nselects the points and keyframes of the reconstruction leads to excellent\nrobustness and generates a compact and trackable map that only grows if the\nscene content changes, allowing lifelong operation. We present an exhaustive\nevaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves\nunprecedented performance with respect to other state-of-the-art monocular SLAM\napproaches. For the benefit of the community, we make the source code public.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 18:52:23 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 09:50:11 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Mur-Artal", "Raul", ""], ["Montiel", "J. M. M.", ""], ["Tardos", "Juan D.", ""]]}, {"id": "1502.01032", "submitter": "Tiep Vu", "authors": "Tiep H. Vu, Hojjat S. Mousavi, Vishal Monga, UK Arvind Rao, Ganesh Rao", "title": "DFDL: Discriminative Feature-oriented Dictionary Learning for\n  Histopathological Image Classification", "comments": "Accepted to IEEE International Symposium on Biomedical Imaging\n  (ISBI), 2015", "journal-ref": null, "doi": "10.1109/ISBI.2015.7164037", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In histopathological image analysis, feature extraction for classification is\na challenging task due to the diversity of histology features suitable for each\nproblem as well as presence of rich geometrical structure. In this paper, we\npropose an automatic feature discovery framework for extracting discriminative\nclass-specific features and present a low-complexity method for classification\nand disease grading in histopathology. Essentially, our Discriminative\nFeature-oriented Dictionary Learning (DFDL) method learns class-specific\nfeatures which are suitable for representing samples from the same class while\nare poorly capable of representing samples from other classes. Experiments on\nthree challenging real-world image databases: 1) histopathological images of\nintraductal breast lesions, 2) mammalian lung images provided by the Animal\nDiagnostics Lab (ADL) at Pennsylvania State University, and 3) brain tumor\nimages from The Cancer Genome Atlas (TCGA) database, show the significance of\nDFDL model in a variety problems over state-of-the-art methods\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 21:05:23 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Vu", "Tiep H.", ""], ["Mousavi", "Hojjat S.", ""], ["Monga", "Vishal", ""], ["Rao", "UK Arvind", ""], ["Rao", "Ganesh", ""]]}, {"id": "1502.01094", "submitter": "Soheil Bahrampour", "authors": "Soheil Bahrampour, Nasser M. Nasrabadi, Asok Ray, W. Kenneth Jenkins", "title": "Multimodal Task-Driven Dictionary Learning for Image Classification", "comments": "To appear at IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2015.2496275", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning algorithms have been successfully used for both\nreconstructive and discriminative tasks, where an input signal is represented\nwith a sparse linear combination of dictionary atoms. While these methods are\nmostly developed for single-modality scenarios, recent studies have\ndemonstrated the advantages of feature-level fusion based on the joint sparse\nrepresentation of the multimodal inputs. In this paper, we propose a multimodal\ntask-driven dictionary learning algorithm under the joint sparsity constraint\n(prior) to enforce collaborations among multiple homogeneous/heterogeneous\nsources of information. In this task-driven formulation, the multimodal\ndictionaries are learned simultaneously with their corresponding classifiers.\nThe resulting multimodal dictionaries can generate discriminative latent\nfeatures (sparse codes) from the data that are optimized for a given task such\nas binary or multiclass classification. Moreover, we present an extension of\nthe proposed formulation using a mixed joint and independent sparsity prior\nwhich facilitates more flexible fusion of the modalities at feature level. The\nefficacy of the proposed algorithms for multimodal classification is\nillustrated on four different applications -- multimodal face recognition,\nmulti-view face recognition, multi-view action recognition, and multimodal\nbiometric recognition. It is also shown that, compared to the counterpart\nreconstructive-based dictionary learning algorithms, the task-driven\nformulations are more computationally efficient in the sense that they can be\nequipped with more compact dictionaries and still achieve superior performance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 05:17:50 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 07:26:59 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Bahrampour", "Soheil", ""], ["Nasrabadi", "Nasser M.", ""], ["Ray", "Asok", ""], ["Jenkins", "W. Kenneth", ""]]}, {"id": "1502.01097", "submitter": "Gui-Song Xia", "authors": "Jingwen Hu, Gui-Song Xia, Fan Hu, Liangpei Zhang", "title": "Dense v.s. Sparse: A Comparative Study of Sampling Analysis in Scene\n  Classification of High-Resolution Remote Sensing Imagery", "comments": "This paper has been withdrawn by the author due to the submission\n  requirement of a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene classification is a key problem in the interpretation of\nhigh-resolution remote sensing imagery. Many state-of-the-art methods, e.g.\nbag-of-visual-words model and its variants, the topic models as well as deep\nlearning-based approaches, share similar procedures: patch sampling, feature\ndescription/learning and classification. Patch sampling is the first and a key\nprocedure which has a great influence on the results. In the literature, many\ndifferent sampling strategies have been used, {e.g. dense sampling, random\nsampling, keypoint-based sampling and saliency-based sampling, etc. However, it\nis still not clear which sampling strategy is suitable for the scene\nclassification of high-resolution remote sensing images. In this paper, we\ncomparatively study the effects of different sampling strategies under the\nscenario of scene classification of high-resolution remote sensing images. We\ndivide the existing sampling methods into two types: dense sampling and sparse\nsampling, the later of which includes random sampling, keypoint-based sampling\nand various saliency-based sampling proposed recently. In order to compare\ntheir performances, we rely on a standard bag-of-visual-words model to\nconstruct our testing scheme, owing to their simplicity, robustness and\nefficiency. The experimental results on two commonly used datasets show that\ndense sampling has the best performance among all the strategies but with high\nspatial and computational complexity, random sampling gives better or\ncomparable results than other sparse sampling methods, like the sophisticated\nmulti-scale key-point operators and the saliency-based methods which are\nintensively studied and commonly used recently.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 05:34:31 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2015 07:02:30 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Hu", "Jingwen", ""], ["Xia", "Gui-Song", ""], ["Hu", "Fan", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1502.01199", "submitter": "Reza Farrahi Moghaddam", "authors": "Reza Farrahi Moghaddam and Mohamed Cheriet", "title": "A Multiple-Expert Binarization Framework for Multispectral Images", "comments": "12 pages, 8 figures, 6 tables. Presented at ICDAR'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a multiple-expert binarization framework for multispectral\nimages is proposed. The framework is based on a constrained subspace selection\nlimited to the spectral bands combined with state-of-the-art gray-level\nbinarization methods. The framework uses a binarization wrapper to enhance the\nperformance of the gray-level binarization. Nonlinear preprocessing of the\nindividual spectral bands is used to enhance the textual information. An\nevolutionary optimizer is considered to obtain the optimal and some suboptimal\n3-band subspaces from which an ensemble of experts is then formed. The\nframework is applied to a ground truth multispectral dataset with promising\nresults. In addition, a generalization to the cross-validation approach is\ndeveloped that not only evaluates generalizability of the framework, it also\nprovides a practical instance of the selected experts that could be then\napplied to unseen inputs despite the small size of the given ground truth\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 14:01:38 GMT"}, {"version": "v2", "created": "Thu, 5 Feb 2015 18:56:40 GMT"}, {"version": "v3", "created": "Mon, 9 Feb 2015 17:42:37 GMT"}, {"version": "v4", "created": "Wed, 11 Feb 2015 18:04:21 GMT"}, {"version": "v5", "created": "Mon, 13 Apr 2015 14:49:16 GMT"}, {"version": "v6", "created": "Wed, 26 Aug 2015 13:27:54 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Moghaddam", "Reza Farrahi", ""], ["Cheriet", "Mohamed", ""]]}, {"id": "1502.01228", "submitter": "Moustafa Meshry", "authors": "Moustafa Meshry, Mohamed E. Hussein, Marwan Torki", "title": "Linear-time Online Action Detection From 3D Skeletal Data Using Bags of\n  Gesturelets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliding window is one direct way to extend a successful recognition system to\nhandle the more challenging detection problem. While action recognition decides\nonly whether or not an action is present in a pre-segmented video sequence,\naction detection identifies the time interval where the action occurred in an\nunsegmented video stream. Sliding window approaches for action detection can\nhowever be slow as they maximize a classifier score over all possible\nsub-intervals. Even though new schemes utilize dynamic programming to speed up\nthe search for the optimal sub-interval, they require offline processing on the\nwhole video sequence. In this paper, we propose a novel approach for online\naction detection based on 3D skeleton sequences extracted from depth data. It\nidentifies the sub-interval with the maximum classifier score in linear time.\nFurthermore, it is invariant to temporal scale variations and is suitable for\nreal-time applications with low latency.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 15:13:04 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 15:48:51 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2015 06:49:03 GMT"}, {"version": "v4", "created": "Tue, 14 Apr 2015 08:58:23 GMT"}, {"version": "v5", "created": "Tue, 22 Dec 2015 09:23:05 GMT"}, {"version": "v6", "created": "Mon, 28 Dec 2015 07:40:11 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Meshry", "Moustafa", ""], ["Hussein", "Mohamed E.", ""], ["Torki", "Marwan", ""]]}, {"id": "1502.01241", "submitter": "Reza Ebrahimpour", "authors": "Amirhossein Farzmahdi, Karim Rajaei, Masoud Ghodrati, Reza\n  Ebrahimpour, Seyed-Mahdi Khaligh-Razavi", "title": "A specialized face-processing network consistent with the\n  representational geometry of monkey face patches", "comments": "41 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ample evidence suggests that face processing in human and non-human primates\nis performed differently compared with other objects. Converging reports, both\nphysiologically and psychophysically, indicate that faces are processed in\nspecialized neural networks in the brain -i.e. face patches in monkeys and the\nfusiform face area (FFA) in humans. We are all expert face-processing agents,\nand able to identify very subtle differences within the category of faces,\ndespite substantial visual and featural similarities. Identification is\nperformed rapidly and accurately after viewing a whole face, while\nsignificantly drops if some of the face configurations (e.g. inversion,\nmisalignment) are manipulated or if partial views of faces are shown due to\nocclusion. This refers to a hotly-debated, yet highly-supported concept, known\nas holistic face processing. We built a hierarchical computational model of\nface-processing based on evidence from recent neuronal and behavioural studies\non faces processing in primates. Representational geometries of the last three\nlayers of the model have characteristics similar to those observed in monkey\nface patches (posterior, middle and anterior patches). Furthermore, several\nface-processing-related phenomena reported in the literature automatically\nemerge as properties of this model. The representations are evolved through\nseveral computational layers, using biologically plausible learning rules. The\nmodel satisfies face inversion effect, composite face effect, other race\neffect, view and identity selectivity, and canonical face views. To our\nknowledge, no models have so far been proposed with this performance and\nagreement with biological data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 15:50:11 GMT"}, {"version": "v2", "created": "Thu, 5 Feb 2015 12:55:58 GMT"}, {"version": "v3", "created": "Sun, 30 Oct 2016 15:47:46 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Farzmahdi", "Amirhossein", ""], ["Rajaei", "Karim", ""], ["Ghodrati", "Masoud", ""], ["Ebrahimpour", "Reza", ""], ["Khaligh-Razavi", "Seyed-Mahdi", ""]]}, {"id": "1502.01400", "submitter": "Marcelo Pereyra", "authors": "Marcelo Pereyra and Steve McLaughlin", "title": "Fast unsupervised Bayesian image segmentation with adaptive spatial\n  regularisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new Bayesian estimation technique for hidden\nPotts-Markov random fields with unknown regularisation parameters, with\napplication to fast unsupervised K-class image segmentation. The technique is\nderived by first removing the regularisation parameter from the Bayesian model\nby marginalisation, followed by a small-variance-asymptotic (SVA) analysis in\nwhich the spatial regularisation and the integer-constrained terms of the Potts\nmodel are decoupled. The evaluation of this SVA Bayesian estimator is then\nrelaxed into a problem that can be computed efficiently by iteratively solving\na convex total-variation denoising problem and a least-squares clustering\n(K-means) problem, both of which can be solved straightforwardly, even in\nhigh-dimensions, and with parallel computing techniques. This leads to a fast\nfully unsupervised Bayesian image segmentation methodology in which the\nstrength of the spatial regularisation is adapted automatically to the observed\nimage during the inference procedure, and that can be easily applied in large\n2D and 3D scenarios or in applications requiring low computing times.\nExperimental results on real images, as well as extensive comparisons with\nstate-of-the-art algorithms, confirm that the proposed methodology offer\nextremely fast convergence and produces accurate segmentation results, with the\nimportant additional advantage of self-adjusting regularisation parameters.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 00:35:08 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2016 14:34:34 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2016 08:53:06 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Pereyra", "Marcelo", ""], ["McLaughlin", "Steve", ""]]}, {"id": "1502.01423", "submitter": "Chen Fang", "authors": "Chen Fang, Hailin Jin, Jianchao Yang, Zhe Lin", "title": "Collaborative Feature Learning from Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image feature representation plays an essential role in image recognition and\nrelated tasks. The current state-of-the-art feature learning paradigm is\nsupervised learning from labeled data. However, this paradigm requires\nlarge-scale category labels, which limits its applicability to domains where\nlabels are hard to obtain. In this paper, we propose a new data-driven feature\nlearning paradigm which does not rely on category labels. Instead, we learn\nfrom user behavior data collected on social media. Concretely, we use the image\nrelationship discovered in the latent space from the user behavior data to\nguide the image feature learning. We collect a large-scale image and user\nbehavior dataset from Behance.net. The dataset consists of 1.9 million images\nand over 300 million view records from 1.9 million users. We validate our\nfeature learning paradigm on this dataset and find that the learned feature\nsignificantly outperforms the state-of-the-art image features in learning\nbetter image similarities. We also show that the learned feature performs\ncompetitively on various recognition benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 03:32:19 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2015 19:27:33 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2015 18:36:54 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Fang", "Chen", ""], ["Jin", "Hailin", ""], ["Yang", "Jianchao", ""], ["Lin", "Zhe", ""]]}, {"id": "1502.01475", "submitter": "Han Peng", "authors": "Peng Han", "title": "Fast Constraint Propagation for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel selective constraint propagation method for\nconstrained image segmentation. In the literature, many pairwise constraint\npropagation methods have been developed to exploit pairwise constraints for\ncluster analysis. However, since most of these methods have a polynomial time\ncomplexity, they are not much suitable for segmentation of images even with a\nmoderate size, which is actually equivalent to cluster analysis with a large\ndata size. Considering the local homogeneousness of a natural image, we choose\nto perform pairwise constraint propagation only over a selected subset of\npixels, but not over the whole image. Such a selective constraint propagation\nproblem is then solved by an efficient graph-based learning algorithm. To\nfurther speed up our selective constraint propagation, we also discard those\nless important propagated constraints during graph-based learning. Finally, the\nselectively propagated constraints are exploited based on $L_1$-minimization\nfor normalized cuts over the whole image. The experimental results demonstrate\nthe promising performance of the proposed method for segmentation with\nselectively propagated constraints.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 09:41:28 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Han", "Peng", ""]]}, {"id": "1502.01480", "submitter": "Pierre Paleo", "authors": "Pierre Paleo and Alessandro Mirone", "title": "Ring artifacts correction in compressed sensing tomographic\n  reconstruction", "comments": "IUCR template, preprint mode, 35 figures", "journal-ref": null, "doi": "10.1107/S1600577515010176", "report-no": null, "categories": "physics.comp-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to handle ring artifacts correction in compressed\nsensing tomographic reconstruction. The correction is part of the\nreconstruction process, which differs from classical sinogram pre-processing\nand image post-processing techniques. The principle of compressed sensing\ntomographic reconstruction is presented. Then, we show that the ring artifacts\ncorrection can be integrated in the reconstruction problem formalism. We\nprovide numerical results for both simulated and real data. This technique is\nincluded in the PyHST2 code which is used at the European Synchrotron Radiation\nFacility for tomographic reconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 09:51:59 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Paleo", "Pierre", ""], ["Mirone", "Alessandro", ""]]}, {"id": "1502.01526", "submitter": "Jing Wang", "authors": "Jing Wang, Jie Shen, Ping Li", "title": "Object Proposal with Kernelized Partial Ranking", "comments": null, "journal-ref": "Pattern Recognition, 2017", "doi": "10.1016/j.patcog.2017.03.022", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object proposals are an ensemble of bounding boxes with high potential to\ncontain objects. In order to determine a small set of proposals with a high\nrecall, a common scheme is extracting multiple features followed by a ranking\nalgorithm which however, incurs two major challenges: {\\bf 1)} The ranking\nmodel often imposes pairwise constraints between each proposal, rendering the\nproblem away from an efficient training/testing phase; {\\bf 2)} Linear kernels\nare utilized due to the computational and memory bottleneck of training a\nkernelized model.\n  In this paper, we remedy these two issues by suggesting a {\\em kernelized\npartial ranking model}. In particular, we demonstrate that {\\bf i)} our partial\nranking model reduces the number of constraints from $O(n^2)$ to $O(nk)$ where\n$n$ is the number of all potential proposals for an image but we are only\ninterested in top-$k$ of them that has the largest overlap with the ground\ntruth; {\\bf ii)} we permit non-linear kernels in our model which is often\nsuperior to the linear classifier in terms of accuracy. For the sake of\nmitigating the computational and memory issues, we introduce a consistent\nweighted sampling~(CWS) paradigm that approximates the non-linear kernel as\nwell as facilitates an efficient learning. In fact, as we will show, training a\nlinear CWS model amounts to learning a kernelized model. Extensive experiments\ndemonstrate that equipped with the non-linear kernel and the partial ranking\nalgorithm, recall at top-$k$ proposals can be substantially improved.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 12:58:29 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 15:06:59 GMT"}, {"version": "v3", "created": "Thu, 18 May 2017 15:57:36 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Wang", "Jing", ""], ["Shen", "Jie", ""], ["Li", "Ping", ""]]}, {"id": "1502.01540", "submitter": "Xun Xu", "authors": "Xun Xu, Timothy Hospedales, Shaogang Gong", "title": "Semantic Embedding Space for Zero-Shot Action Recognition", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of categories for action recognition is growing rapidly. It is\nthus becoming increasingly hard to collect sufficient training data to learn\nconventional models for each category. This issue may be ameliorated by the\nincreasingly popular 'zero-shot learning' (ZSL) paradigm. In this framework a\nmapping is constructed between visual features and a human interpretable\nsemantic description of each category, allowing categories to be recognised in\nthe absence of any training data. Existing ZSL studies focus primarily on image\ndata, and attribute-based semantic representations. In this paper, we address\nzero-shot recognition in contemporary video action recognition tasks, using\nsemantic word vector space as the common space to embed videos and category\nlabels. This is more challenging because the mapping between the semantic space\nand space-time features of videos containing complex actions is more complex\nand harder to learn. We demonstrate that a simple self-training and data\naugmentation strategy can significantly improve the efficacy of this mapping.\nExperiments on human action datasets including HMDB51 and UCF101 demonstrate\nthat our approach achieves the state-of-the-art zero-shot action recognition\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 13:34:48 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Xu", "Xun", ""], ["Hospedales", "Timothy", ""], ["Gong", "Shaogang", ""]]}, {"id": "1502.01643", "submitter": "Letizia Mariotti", "authors": "Letizia Mariotti and Nicholas Devaney", "title": "Performance Analysis of Cone Detection Algorithms", "comments": "13 pages, 7 figures, 2 tables", "journal-ref": null, "doi": "10.1364/JOSAA.32.000497", "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms have been proposed to help clinicians evaluate cone density\nand spacing, as these may be related to the onset of retinal diseases. However,\nthere has been no rigorous comparison of the performance of these algorithms.\nIn addition, the performance of such algorithms is typically determined by\ncomparison with human observers. Here we propose a technique to simulate\nrealistic images of the cone mosaic. We use the simulated images to test the\nperformance of two popular cone detection algorithms and we introduce an\nalgorithm which is used by astronomers to detect stars in astronomical images.\nWe use Free Response Operating Characteristic (FROC) curves to evaluate and\ncompare the performance of the three algorithms. This allows us to optimize the\nperformance of each algorithm. We observe that performance is significantly\nenhanced by up-sampling the images. We investigate the effect of noise and\nimage quality on cone mosaic parameters estimated using the different\nalgorithms, finding that the estimated regularity is the most sensitive\nparameter.\n  This paper was published in JOSA A and is made available as an electronic\nreprint with the permission of OSA. The paper can be found at the following URL\non the OSA website: http://www.opticsinfobase.org/abstract.cfm?msid=224577.\nSystematic or multiple reproduction or distribution to multiple locations via\nelectronic or other means is prohibited and is subject to penalties under law.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 17:11:40 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Mariotti", "Letizia", ""], ["Devaney", "Nicholas", ""]]}, {"id": "1502.01659", "submitter": "Sudeep Pillai", "authors": "Sudeep Pillai, Matthew R. Walter, Seth Teller", "title": "Learning Articulated Motions From Visual Demonstration", "comments": "Published in Robotics: Science and Systems X, Berkeley, CA. ISBN:\n  978-0-9923747-0-9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many functional elements of human homes and workplaces consist of rigid\ncomponents which are connected through one or more sliding or rotating\nlinkages. Examples include doors and drawers of cabinets and appliances;\nlaptops; and swivel office chairs. A robotic mobile manipulator would benefit\nfrom the ability to acquire kinematic models of such objects from observation.\nThis paper describes a method by which a robot can acquire an object model by\ncapturing depth imagery of the object as a human moves it through its range of\nmotion. We envision that in future, a machine newly introduced to an\nenvironment could be shown by its human user the articulated objects particular\nto that environment, inferring from these \"visual demonstrations\" enough\ninformation to actuate each object independently of the user.\n  Our method employs sparse (markerless) feature tracking, motion segmentation,\ncomponent pose estimation, and articulation learning; it does not require prior\nobject models. Using the method, a robot can observe an object being exercised,\ninfer a kinematic model incorporating rigid, prismatic and revolute joints,\nthen use the model to predict the object's motion from a novel vantage point.\nWe evaluate the method's performance, and compare it to that of a previously\npublished technique, for a variety of household objects.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 17:59:07 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Pillai", "Sudeep", ""], ["Walter", "Matthew R.", ""], ["Teller", "Seth", ""]]}, {"id": "1502.01761", "submitter": "Tom Lee", "authors": "Tom Lee, Sanja Fidler, Alex Levinshtein, Cristian Sminchisescu, and\n  Sven Dickinson", "title": "A Framework for Symmetric Part Detection in Cluttered Scenes", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role of symmetry in computer vision has waxed and waned in importance\nduring the evolution of the field from its earliest days. At first figuring\nprominently in support of bottom-up indexing, it fell out of favor as shape\ngave way to appearance and recognition gave way to detection. With a strong\nprior in the form of a target object, the role of the weaker priors offered by\nperceptual grouping was greatly diminished. However, as the field returns to\nthe problem of recognition from a large database, the bottom-up recovery of the\nparts that make up the objects in a cluttered scene is critical for their\nrecognition. The medial axis community has long exploited the ubiquitous\nregularity of symmetry as a basis for the decomposition of a closed contour\ninto medial parts. However, today's recognition systems are faced with\ncluttered scenes, and the assumption that a closed contour exists, i.e. that\nfigure-ground segmentation has been solved, renders much of the medial axis\ncommunity's work inapplicable. In this article, we review a computational\nframework, previously reported in Lee et al. (2013), Levinshtein et al. (2009,\n2013), that bridges the representation power of the medial axis and the need to\nrecover and group an object's parts in a cluttered scene. Our framework is\nrooted in the idea that a maximally inscribed disc, the building block of a\nmedial axis, can be modeled as a compact superpixel in the image. We evaluate\nthe method on images of cluttered scenes.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 23:51:16 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Lee", "Tom", ""], ["Fidler", "Sanja", ""], ["Levinshtein", "Alex", ""], ["Sminchisescu", "Cristian", ""], ["Dickinson", "Sven", ""]]}, {"id": "1502.01782", "submitter": "Conrad Sanderson", "authors": "Johanna Carvajal, Conrad Sanderson, Chris McCool, Brian C. Lovell", "title": "Multi-Action Recognition via Stochastic Modelling of Optical Flow and\n  Gradients", "comments": null, "journal-ref": "Workshop on Machine Learning for Sensory Data Analysis (MLSDA),\n  pp. 19-24, 2014", "doi": "10.1145/2689746.2689748", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel approach to multi-action recognition that\nperforms joint segmentation and classification. This approach models each\naction using a Gaussian mixture using robust low-dimensional action features.\nSegmentation is achieved by performing classification on overlapping temporal\nwindows, which are then merged to produce the final result. This approach is\nconsiderably less complicated than previous methods which use dynamic\nprogramming or computationally expensive hidden Markov models (HMMs). Initial\nexperiments on a stitched version of the KTH dataset show that the proposed\napproach achieves an accuracy of 78.3%, outperforming a recent HMM-based\napproach which obtained 71.2%.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 03:30:10 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Carvajal", "Johanna", ""], ["Sanderson", "Conrad", ""], ["McCool", "Chris", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1502.01812", "submitter": "Teng Li Dr.", "authors": "Teng Li, Huan Chang, Meng Wang, Bingbing Ni, Richang Hong and\n  Shuicheng Yan", "title": "Crowded Scene Analysis: A Survey", "comments": "20 pages in IEEE Transactions on Circuits and Systems for Video\n  Technology, 2015", "journal-ref": null, "doi": "10.1109/TCSVT.2014.2358029", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated scene analysis has been a topic of great interest in computer\nvision and cognitive science. Recently, with the growth of crowd phenomena in\nthe real world, crowded scene analysis has attracted much attention. However,\nthe visual occlusions and ambiguities in crowded scenes, as well as the complex\nbehaviors and scene semantics, make the analysis a challenging task. In the\npast few years, an increasing number of works on crowded scene analysis have\nbeen reported, covering different aspects including crowd motion pattern\nlearning, crowd behavior and activity analysis, and anomaly detection in\ncrowds. This paper surveys the state-of-the-art techniques on this topic. We\nfirst provide the background knowledge and the available features related to\ncrowded scenes. Then, existing models, popular algorithms, evaluation\nprotocols, as well as system performance are provided corresponding to\ndifferent aspects of crowded scene analysis. We also outline the available\ndatasets for performance evaluation. Finally, some research problems and\npromising future directions are presented with discussions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 06:36:12 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Li", "Teng", ""], ["Chang", "Huan", ""], ["Wang", "Meng", ""], ["Ni", "Bingbing", ""], ["Hong", "Richang", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1502.01823", "submitter": "Anurag Kumar", "authors": "Anurag Kumar, Bhiksha Raj", "title": "Unsupervised Fusion Weight Learning in Multiple Classifier Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an unsupervised method to learn the weights with\nwhich the scores of multiple classifiers must be combined in classifier fusion\nsettings. We also introduce a novel metric for ranking instances based on an\nindex which depends upon the rank of weighted scores of test points among the\nweighted scores of training points. We show that the optimized index can be\nused for computing measures such as average precision. Unlike most classifier\nfusion methods where a single weight is learned to weigh all examples our\nmethod learns instance-specific weights. The problem is formulated as learning\nthe weight which maximizes a clarity index; subsequently the index itself and\nthe learned weights both are used separately to rank all the test points. Our\nmethod gives an unsupervised method of optimizing performance on actual test\ndata, unlike the well known stacking-based methods where optimization is done\nover a labeled training set. Moreover, we show that our method is tolerant to\nnoisy classifiers and can be used for selecting N-best classifiers.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 08:28:57 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Kumar", "Anurag", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1502.01827", "submitter": "Guang-Tong Zhou", "authors": "Guang-Tong Zhou, Sung Ju Hwang, Mark Schmidt, Leonid Sigal and Greg\n  Mori", "title": "Hierarchical Maximum-Margin Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hierarchical maximum-margin clustering method for unsupervised\ndata analysis. Our method extends beyond flat maximum-margin clustering, and\nperforms clustering recursively in a top-down manner. We propose an effective\ngreedy splitting criteria for selecting which cluster to split next, and employ\nregularizers that enforce feature sharing/competition for capturing data\nsemantics. Experimental results obtained on four standard datasets show that\nour method outperforms flat and hierarchical clustering baselines, while\nforming clean and semantically meaningful cluster hierarchies.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 08:37:55 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Zhou", "Guang-Tong", ""], ["Hwang", "Sung Ju", ""], ["Schmidt", "Mark", ""], ["Sigal", "Leonid", ""], ["Mori", "Greg", ""]]}, {"id": "1502.01852", "submitter": "Kaiming He", "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun", "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on\n  ImageNet Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rectified activation units (rectifiers) are essential for state-of-the-art\nneural networks. In this work, we study rectifier neural networks for image\nclassification from two aspects. First, we propose a Parametric Rectified\nLinear Unit (PReLU) that generalizes the traditional rectified unit. PReLU\nimproves model fitting with nearly zero extra computational cost and little\noverfitting risk. Second, we derive a robust initialization method that\nparticularly considers the rectifier nonlinearities. This method enables us to\ntrain extremely deep rectified models directly from scratch and to investigate\ndeeper or wider network architectures. Based on our PReLU networks\n(PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012\nclassification dataset. This is a 26% relative improvement over the ILSVRC 2014\nwinner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass\nhuman-level performance (5.1%, Russakovsky et al.) on this visual recognition\nchallenge.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 10:44:00 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["He", "Kaiming", ""], ["Zhang", "Xiangyu", ""], ["Ren", "Shaoqing", ""], ["Sun", "Jian", ""]]}, {"id": "1502.01853", "submitter": "Laurent Jacques", "authors": "K. Degraux, V. Cambareri, L. Jacques, B. Geelen, C. Blanch and G.\n  Lafruit", "title": "Generalized Inpainting Method for Hyperspectral Image Acquisition", "comments": "Keywords: Hyperspectral, inpainting, iterative hard thresholding,\n  sparse models, CMOS, Fabry-P\\'erot", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recently designed hyperspectral imaging device enables multiplexed\nacquisition of an entire data volume in a single snapshot thanks to\nmonolithically-integrated spectral filters. Such an agile imaging technique\ncomes at the cost of a reduced spatial resolution and the need for a\ndemosaicing procedure on its interleaved data. In this work, we address both\nissues and propose an approach inspired by recent developments in compressed\nsensing and analysis sparse models. We formulate our superresolution and\ndemosaicing task as a 3-D generalized inpainting problem. Interestingly, the\ntarget spatial resolution can be adjusted for mitigating the compression level\nof our sensing. The reconstruction procedure uses a fast greedy method called\nPseudo-inverse IHT. We also show on simulations that a random arrangement of\nthe spectral filters on the sensor is preferable to regular mosaic layout as it\nimproves the quality of the reconstruction. The efficiency of our technique is\ndemonstrated through numerical experiments on both synthetic and real data as\nacquired by the snapshot imager.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 10:45:36 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Degraux", "K.", ""], ["Cambareri", "V.", ""], ["Jacques", "L.", ""], ["Geelen", "B.", ""], ["Blanch", "C.", ""], ["Lafruit", "G.", ""]]}, {"id": "1502.01880", "submitter": "Helio M. de Oliveira", "authors": "E.F. Melo and H.M. de Oliveira", "title": "A Fingerprint-based Access Control using Principal Component Analysis\n  and Edge Detection", "comments": "5 pages, 9 figures. SBrT/IEEE International Telecommunication\n  Symposium, ITS 2010, Manaus, AM, Brazil", "journal-ref": null, "doi": "10.14209/SBRT.2010.63", "report-no": null, "categories": "cs.CV cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for deciding on the appropriateness or\nnot of an acquired fingerprint image into a given database. The process begins\nwith the assembly of a training base in an image space constructed by combining\nPrincipal Component Analysis (PCA) and edge detection. Then, the parameter H, a\nnew feature that helps in the decision making about the relevance of a\nfingerprint image in databases, is derived from a relationship between\nEuclidean and Mahalanobian distances. This procedure ends with the lifting of\nthe curve of the Receiver Operating Characteristic (ROC), where the thresholds\ndefined on the parameter H are chosen according to the acceptable rates of\nfalse positives and false negatives.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 13:31:54 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Melo", "E. F.", ""], ["de Oliveira", "H. M.", ""]]}, {"id": "1502.02063", "submitter": "Hossein Hajimirsadeghi", "authors": "Hossein Hajimirsadeghi, Wang Yan, Arash Vahdat, Greg Mori", "title": "Visual Recognition by Counting Instances: A Multi-Instance Cardinality\n  Potential Kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many visual recognition problems can be approached by counting instances. To\ndetermine whether an event is present in a long internet video, one could count\nhow many frames seem to contain the activity. Classifying the activity of a\ngroup of people can be done by counting the actions of individual people.\nEncoding these cardinality relationships can reduce sensitivity to clutter, in\nthe form of irrelevant frames or individuals not involved in a group activity.\nLearned parameters can encode how many instances tend to occur in a class of\ninterest. To this end, this paper develops a powerful and flexible framework to\ninfer any cardinality relation between latent labels in a multi-instance model.\nHard or soft cardinality relations can be encoded to tackle diverse levels of\nambiguity. Experiments on tasks such as human activity recognition, video event\ndetection, and video summarization demonstrate the effectiveness of using\ncardinality relations for improving recognition results.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 21:57:55 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 22:41:49 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Hajimirsadeghi", "Hossein", ""], ["Yan", "Wang", ""], ["Vahdat", "Arash", ""], ["Mori", "Greg", ""]]}, {"id": "1502.02077", "submitter": "Matthew Hirn", "authors": "Matthew Hirn and Nicolas Poilvert and St\\'ephane Mallat", "title": "Quantum Energy Regression using Scattering Transforms", "comments": "9 pages, 2 figures, 1 table. v2: Correction to Section 4.3. v3:\n  Replaced by arXiv:1605.04654", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.chem-ph physics.comp-ph quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to the regression of quantum mechanical energies\nbased on a scattering transform of an intermediate electron density\nrepresentation. A scattering transform is a deep convolution network computed\nwith a cascade of multiscale wavelet transforms. It possesses appropriate\ninvariant and stability properties for quantum energy regression. This new\nframework removes fundamental limitations of Coulomb matrix based energy\nregressions, and numerical experiments give state-of-the-art accuracy over\nplanar molecules.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 23:55:13 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2015 19:21:29 GMT"}, {"version": "v3", "created": "Fri, 20 May 2016 14:02:49 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Hirn", "Matthew", ""], ["Poilvert", "Nicolas", ""], ["Mallat", "St\u00e9phane", ""]]}, {"id": "1502.02092", "submitter": "Hang Zhang", "authors": "Hang Zhang, Kristin Dana and Ko Nishino", "title": "Reflectance Hashing for Material Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel method for using reflectance to identify materials.\nReflectance offers a unique signature of the material but is challenging to\nmeasure and use for recognizing materials due to its high-dimensionality. In\nthis work, one-shot reflectance is captured using a unique optical camera\nmeasuring {\\it reflectance disks} where the pixel coordinates correspond to\nsurface viewing angles. The reflectance has class-specific stucture and angular\ngradients computed in this reflectance space reveal the material class.\n  These reflectance disks encode discriminative information for efficient and\naccurate material recognition. We introduce a framework called reflectance\nhashing that models the reflectance disks with dictionary learning and binary\nhashing. We demonstrate the effectiveness of reflectance hashing for material\nrecognition with a number of real-world materials.\n", "versions": [{"version": "v1", "created": "Sat, 7 Feb 2015 03:29:10 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Zhang", "Hang", ""], ["Dana", "Kristin", ""], ["Nishino", "Ko", ""]]}, {"id": "1502.02160", "submitter": "Mohammad Ehab Ragab", "authors": "Allam Shehata Hassanein, Sherien Mohammad, Mohamed Sameer, and\n  Mohammad Ehab Ragab", "title": "A Survey on Hough Transform, Theory, Techniques and Applications", "comments": "18 pages, and 11 figures", "journal-ref": "IJCSI Volume 12, Issue 1, January 2015", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For more than half a century, the Hough transform is ever-expanding for new\nfrontiers. Thousands of research papers and numerous applications have evolved\nover the decades. Carrying out an all-inclusive survey is hardly possible and\nenormously space-demanding. What we care about here is emphasizing some of the\nmost crucial milestones of the transform. We describe its variations\nelaborating on the basic ones such as the line and circle Hough transforms. The\nhigh demand for storage and computation time is clarified with different\nsolution approaches. Since most uses of the transform take place on binary\nimages, we have been concerned with the work done directly on gray or color\nimages. The myriad applications of the standard transform and its variations\nhave been classified highlighting the up-to-date and the unconventional ones.\nDue to its merits such as noise-immunity and expandability, the transform has\nan excellent history, and a bright future as well.\n", "versions": [{"version": "v1", "created": "Sat, 7 Feb 2015 16:40:17 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Hassanein", "Allam Shehata", ""], ["Mohammad", "Sherien", ""], ["Sameer", "Mohamed", ""], ["Ragab", "Mohammad Ehab", ""]]}, {"id": "1502.02171", "submitter": "Liang Zheng", "authors": "Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jiahao Bu, Qi Tian", "title": "Person Re-identification Meets Image Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For long time, person re-identification and image search are two separately\nstudied tasks. However, for person re-identification, the effectiveness of\nlocal features and the \"query-search\" mode make it well posed for image search\ntechniques.\n  In the light of recent advances in image search, this paper proposes to treat\nperson re-identification as an image search problem. Specifically, this paper\nclaims two major contributions. 1) By designing an unsupervised Bag-of-Words\nrepresentation, we are devoted to bridging the gap between the two tasks by\nintegrating techniques from image search in person re-identification. We show\nthat our system sets up an effective yet efficient baseline that is amenable to\nfurther supervised/unsupervised improvements. 2) We contribute a new high\nquality dataset which uses DPM detector and includes a number of distractor\nimages. Our dataset reaches closer to realistic settings, and new perspectives\nare provided.\n  Compared with approaches that rely on feature-feature match, our method is\nfaster by over two orders of magnitude. Moreover, on three datasets, we report\ncompetitive results compared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 7 Feb 2015 18:56:35 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Zheng", "Liang", ""], ["Shen", "Liyue", ""], ["Tian", "Lu", ""], ["Wang", "Shengjin", ""], ["Bu", "Jiahao", ""], ["Tian", "Qi", ""]]}, {"id": "1502.02182", "submitter": "Jelena Badnjar", "authors": "Jelena Badnjar", "title": "Comparison of Algorithms for Compressed Sensing of Magnetic Resonance\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) is an essential medical tool with inherently\nslow data acquisition process. Slow acquisition process requires patient to be\nlong time exposed to scanning apparatus. In recent years significant efforts\nare made towards the applying Compressive Sensing technique to the acquisition\nprocess of MRI and biomedical images. Compressive Sensing is an emerging theory\nin signal processing. It aims to reduce the amount of acquired data required\nfor successful signal reconstruction. Reducing the amount of acquired image\ncoefficients leads to lower acquisition time, i.e. time of exposition to the\nMRI apparatus. Using optimization algorithms, satisfactory image quality can be\nobtained from the small set of acquired samples. A number of optimization\nalgorithms for the reconstruction of the biomedical images is proposed in the\nliterature. In this paper, three commonly used optimization algorithms are\ncompared and results are presented on the several MRI images.\n", "versions": [{"version": "v1", "created": "Sat, 7 Feb 2015 20:24:10 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2015 10:23:15 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2015 22:08:54 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Badnjar", "Jelena", ""]]}, {"id": "1502.02330", "submitter": "Dacheng Tao", "authors": "Yong Luo, Dacheng Tao, Yonggang Wen, Kotagiri Ramamohanarao, Chao Xu", "title": "Tensor Canonical Correlation Analysis for Multi-view Dimension Reduction", "comments": "20 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) has proven an effective tool for\ntwo-view dimension reduction due to its profound theoretical foundation and\nsuccess in practical applications. In respect of multi-view learning, however,\nit is limited by its capability of only handling data represented by two-view\nfeatures, while in many real-world applications, the number of views is\nfrequently many more. Although the ad hoc way of simultaneously exploring all\npossible pairs of features can numerically deal with multi-view data, it\nignores the high order statistics (correlation information) which can only be\ndiscovered by simultaneously exploring all features.\n  Therefore, in this work, we develop tensor CCA (TCCA) which straightforwardly\nyet naturally generalizes CCA to handle the data of an arbitrary number of\nviews by analyzing the covariance tensor of the different views. TCCA aims to\ndirectly maximize the canonical correlation of multiple (more than two) views.\nCrucially, we prove that the multi-view canonical correlation maximization\nproblem is equivalent to finding the best rank-1 approximation of the data\ncovariance tensor, which can be solved efficiently using the well-known\nalternating least squares (ALS) algorithm. As a consequence, the high order\ncorrelation information contained in the different views is explored and thus a\nmore reliable common subspace shared by all features can be obtained. In\naddition, a non-linear extension of TCCA is presented. Experiments on various\nchallenge tasks, including large scale biometric structure prediction, internet\nadvertisement classification and web image annotation, demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 01:58:27 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Luo", "Yong", ""], ["Tao", "Dacheng", ""], ["Wen", "Yonggang", ""], ["Ramamohanarao", "Kotagiri", ""], ["Xu", "Chao", ""]]}, {"id": "1502.02410", "submitter": "Elif Vural", "authors": "Elif Vural and Christine Guillemot", "title": "Out-of-sample generalizations for supervised manifold learning for\n  classification", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2520368", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised manifold learning methods for data classification map data samples\nresiding in a high-dimensional ambient space to a lower-dimensional domain in a\nstructure-preserving way, while enhancing the separation between different\nclasses in the learned embedding. Most nonlinear supervised manifold learning\nmethods compute the embedding of the manifolds only at the initially available\ntraining points, while the generalization of the embedding to novel points,\nknown as the out-of-sample extension problem in manifold learning, becomes\nespecially important in classification applications. In this work, we propose a\nsemi-supervised method for building an interpolation function that provides an\nout-of-sample extension for general supervised manifold learning algorithms\nstudied in the context of classification. The proposed algorithm computes a\nradial basis function (RBF) interpolator that minimizes an objective function\nconsisting of the total embedding error of unlabeled test samples, defined as\ntheir distance to the embeddings of the manifolds of their own class, as well\nas a regularization term that controls the smoothness of the interpolation\nfunction in a direction-dependent way. The class labels of test data and the\ninterpolation function parameters are estimated jointly with a progressive\nprocedure. Experimental results on face and object images demonstrate the\npotential of the proposed out-of-sample extension algorithm for the\nclassification of manifold-modeled data sets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 09:56:57 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Vural", "Elif", ""], ["Guillemot", "Christine", ""]]}, {"id": "1502.02445", "submitter": "Giovanni Montana", "authors": "Alexandre de Brebisson, Giovanni Montana", "title": "Deep Neural Networks for Anatomical Brain Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to automatically segment magnetic resonance (MR)\nimages of the human brain into anatomical regions. Our methodology is based on\na deep artificial neural network that assigns each voxel in an MR image of the\nbrain to its corresponding anatomical region. The inputs of the network capture\ninformation at different scales around the voxel of interest: 3D and orthogonal\n2D intensity patches capture the local spatial context while large, compressed\n2D orthogonal patches and distances to the regional centroids enforce global\nspatial consistency. Contrary to commonly used segmentation methods, our\ntechnique does not require any non-linear registration of the MR images. To\nbenchmark our model, we used the dataset provided for the MICCAI 2012 challenge\non multi-atlas labelling, which consists of 35 manually segmented MR images of\nthe brain. We obtained competitive results (mean dice coefficient 0.725, error\nrate 0.163) showing the potential of our approach. To our knowledge, our\ntechnique is the first to tackle the anatomical segmentation of the whole brain\nusing deep neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 11:48:42 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 16:19:44 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["de Brebisson", "Alexandre", ""], ["Montana", "Giovanni", ""]]}, {"id": "1502.02478", "submitter": "Benjamin Graham", "authors": "Ben Graham, Jeremy Reizenstein, Leigh Robinson", "title": "Efficient batchwise dropout training using submatrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout is a popular technique for regularizing artificial neural networks.\nDropout networks are generally trained by minibatch gradient descent with a\ndropout mask turning off some of the units---a different pattern of dropout is\napplied to every sample in the minibatch. We explore a very simple alternative\nto the dropout mask. Instead of masking dropped out units by setting them to\nzero, we perform matrix multiplication using a submatrix of the weight\nmatrix---unneeded hidden units are never calculated. Performing dropout\nbatchwise, so that one pattern of dropout is used for each sample in a\nminibatch, we can substantially reduce training times. Batchwise dropout can be\nused with fully-connected and convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 13:29:48 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Graham", "Ben", ""], ["Reizenstein", "Jeremy", ""], ["Robinson", "Leigh", ""]]}, {"id": "1502.02506", "submitter": "Giovanni Montana", "authors": "Adrien Payan, Giovanni Montana", "title": "Predicting Alzheimer's disease: a neuroimaging study with 3D\n  convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern recognition methods using neuroimaging data for the diagnosis of\nAlzheimer's disease have been the subject of extensive research in recent\nyears. In this paper, we use deep learning methods, and in particular sparse\nautoencoders and 3D convolutional neural networks, to build an algorithm that\ncan predict the disease status of a patient, based on an MRI scan of the brain.\nWe report on experiments using the ADNI data set involving 2,265 historical\nscans. We demonstrate that 3D convolutional neural networks outperform several\nother classifiers reported in the literature and produce state-of-art results.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 14:46:40 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Payan", "Adrien", ""], ["Montana", "Giovanni", ""]]}, {"id": "1502.02590", "submitter": "Alhussein Fawzi", "authors": "Alhussein Fawzi, Omar Fawzi, Pascal Frossard", "title": "Analysis of classifiers' robustness to adversarial perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to analyze an intriguing phenomenon recently\ndiscovered in deep networks, namely their instability to adversarial\nperturbations (Szegedy et. al., 2014). We provide a theoretical framework for\nanalyzing the robustness of classifiers to adversarial perturbations, and show\nfundamental upper bounds on the robustness of classifiers. Specifically, we\nestablish a general upper bound on the robustness of classifiers to adversarial\nperturbations, and then illustrate the obtained upper bound on the families of\nlinear and quadratic classifiers. In both cases, our upper bound depends on a\ndistinguishability measure that captures the notion of difficulty of the\nclassification task. Our results for both classes imply that in tasks involving\nsmall distinguishability, no classifier in the considered set will be robust to\nadversarial perturbations, even if a good accuracy is achieved. Our theoretical\nframework moreover suggests that the phenomenon of adversarial instability is\ndue to the low flexibility of classifiers, compared to the difficulty of the\nclassification task (captured by the distinguishability). Moreover, we show the\nexistence of a clear distinction between the robustness of a classifier to\nrandom noise and its robustness to adversarial perturbations. Specifically, the\nformer is shown to be larger than the latter by a factor that is proportional\nto \\sqrt{d} (with d being the signal dimension) for linear classifiers. This\nresult gives a theoretical explanation for the discrepancy between the two\nrobustness properties in high dimensional problems, which was empirically\nobserved in the context of neural networks. To the best of our knowledge, our\nresults provide the first theoretical work that addresses the phenomenon of\nadversarial instability recently observed for deep networks. Our analysis is\ncomplemented by experimental results on controlled and real-world data.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 18:20:00 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 20:57:10 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 18:55:41 GMT"}, {"version": "v4", "created": "Mon, 28 Mar 2016 22:50:52 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Fawzi", "Omar", ""], ["Frossard", "Pascal", ""]]}, {"id": "1502.02734", "submitter": "Liang-Chieh Chen", "authors": "George Papandreou and Liang-Chieh Chen and Kevin Murphy and Alan L.\n  Yuille", "title": "Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image\n  Segmentation", "comments": "Accepted to ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (DCNNs) trained on a large number of\nimages with strong pixel-level annotations have recently significantly pushed\nthe state-of-art in semantic image segmentation. We study the more challenging\nproblem of learning DCNNs for semantic image segmentation from either (1)\nweakly annotated training data such as bounding boxes or image-level labels or\n(2) a combination of few strongly labeled and many weakly labeled images,\nsourced from one or multiple datasets. We develop Expectation-Maximization (EM)\nmethods for semantic image segmentation model training under these weakly\nsupervised and semi-supervised settings. Extensive experimental evaluation\nshows that the proposed techniques can learn models delivering competitive\nresults on the challenging PASCAL VOC 2012 image segmentation benchmark, while\nrequiring significantly less annotation effort. We share source code\nimplementing the proposed system at\nhttps://bitbucket.org/deeplab/deeplab-public.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 23:38:45 GMT"}, {"version": "v2", "created": "Fri, 8 May 2015 17:49:00 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2015 23:29:28 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Papandreou", "George", ""], ["Chen", "Liang-Chieh", ""], ["Murphy", "Kevin", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1502.02766", "submitter": "Sachin Sudhakar Farfade", "authors": "Sachin Sudhakar Farfade, Mohammad Saberian, Li-Jia Li", "title": "Multi-view Face Detection Using Deep Convolutional Neural Networks", "comments": "in International Conference on Multimedia Retrieval 2015 (ICMR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper we consider the problem of multi-view face detection. While\nthere has been significant research on this problem, current state-of-the-art\napproaches for this task require annotation of facial landmarks, e.g. TSM [25],\nor annotation of face poses [28, 22]. They also require training dozens of\nmodels to fully capture faces in all orientations, e.g. 22 models in HeadHunter\nmethod [22]. In this paper we propose Deep Dense Face Detector (DDFD), a method\nthat does not require pose/landmark annotation and is able to detect faces in a\nwide range of orientations using a single model based on deep convolutional\nneural networks. The proposed method has minimal complexity; unlike other\nrecent deep learning object detection methods [9], it does not require\nadditional components such as segmentation, bounding-box regression, or SVM\nclassifiers. Furthermore, we analyzed scores of the proposed face detector for\nfaces in different orientations and found that 1) the proposed method is able\nto detect faces from different angles and can handle occlusion to some extent,\n2) there seems to be a correlation between dis- tribution of positive examples\nin the training set and scores of the proposed face detector. The latter\nsuggests that the proposed methods performance can be further improved by using\nbetter sampling strategies and more sophisticated data augmentation techniques.\nEvaluations on popular face detection benchmark datasets show that our\nsingle-model face detector algorithm has similar or better performance compared\nto the previous methods, which are more complex and require annotations of\neither different poses or facial landmarks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 03:15:21 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 10:07:20 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2015 20:18:57 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Farfade", "Sachin Sudhakar", ""], ["Saberian", "Mohammad", ""], ["Li", "Li-Jia", ""]]}, {"id": "1502.02772", "submitter": "Kean Lau Hong", "authors": "Kean Hong Lau, Yong Haur Tay, Fook Loong Lo", "title": "A HMAX with LLC for visual recognition", "comments": "10 pages, 3 figures, 2 tables, 23 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's high performance deep artificial neural networks (ANNs) rely heavily\non parameter optimization, which is sequential in nature and even with a\npowerful GPU, would have taken weeks to train them up for solving challenging\ntasks [22]. HMAX [17] has demonstrated that a simple high performing network\ncould be obtained without heavy optimization. In this paper, we had improved on\nthe existing best HMAX neural network [12] in terms of structural simplicity\nand performance. Our design replaces the L1 minimization sparse coding (SC)\nwith a locality-constrained linear coding (LLC) [20] which has a lower\ncomputational demand. We also put the simple orientation filter bank back into\nthe front layer of the network replacing PCA. Our system's performance has\nimproved over the existing architecture and reached 79.0% on the challenging\nCaltech-101 [7] dataset, which is state-of-the-art for ANNs (without transfer\nlearning). From our empirical data, the main contributors to our system's\nperformance include an introduction of partial signal whitening, a spot\ndetector, and a spatial pyramid matching (SPM) [14] layer.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 04:01:43 GMT"}, {"version": "v2", "created": "Wed, 11 Feb 2015 04:40:20 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Lau", "Kean Hong", ""], ["Tay", "Yong Haur", ""], ["Lo", "Fook Loong", ""]]}, {"id": "1502.02871", "submitter": "Edward Aboufadel", "authors": "Edward Aboufadel, Sylvanna V. Krawczyk, and Melissa Sherman-Bennett", "title": "Talk to the Hand: Generating a 3D Print from Photographs", "comments": "12 pages, 13 figures, final report from the 2013 REU program at Grand\n  Valley State University", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.HO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript presents a linear algebra-based technique that only requires\ntwo unique photographs from a digital camera to mathematically construct a 3D\nsurface representation which can then be 3D printed. Basic computer vision\ntheory and manufacturing principles are also briefly discussed.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 12:00:35 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Aboufadel", "Edward", ""], ["Krawczyk", "Sylvanna V.", ""], ["Sherman-Bennett", "Melissa", ""]]}, {"id": "1502.02905", "submitter": "Chaitannya Supe Mr.", "authors": "Chaitannya Supe", "title": "Real Time Implementation of Spatial Filtering On FPGA", "comments": "8 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Field Programmable Gate Array (FPGA) technology has gained vital importance\nmainly because of its parallel processing hardware which makes it ideal for\nimage and video processing. In this paper, a step by step approach to apply a\nlinear spatial filter on real time video frame sent by Omnivision OV7670 camera\nusing Zynq Evaluation and Development board based on Xilinx XC7Z020 has been\ndiscussed. Face detection application was chosen to explain above procedure.\nThis procedure is applicable to most of the complex image processing algorithms\nwhich needs to be implemented using FPGA.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 13:37:49 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Supe", "Chaitannya", ""]]}, {"id": "1502.02965", "submitter": "Zhi Han", "authors": "Zhi Han and Zongben Xu and Song-Chun Zhu", "title": "Video Primal Sketch: A Unified Middle-Level Representation for Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a middle-level video representation named Video Primal\nSketch (VPS), which integrates two regimes of models: i) sparse coding model\nusing static or moving primitives to explicitly represent moving corners,\nlines, feature points, etc., ii) FRAME /MRF model reproducing feature\nstatistics extracted from input video to implicitly represent textured motion,\nsuch as water and fire. The feature statistics include histograms of\nspatio-temporal filters and velocity distributions. This paper makes three\ncontributions to the literature: i) Learning a dictionary of video primitives\nusing parametric generative models; ii) Proposing the Spatio-Temporal FRAME\n(ST-FRAME) and Motion-Appearance FRAME (MA-FRAME) models for modeling and\nsynthesizing textured motion; and iii) Developing a parsimonious hybrid model\nfor generic video representation. Given an input video, VPS selects the proper\nmodels automatically for different motion patterns and is compatible with\nhigh-level action representations. In the experiments, we synthesize a number\nof textured motion; reconstruct real videos using the VPS; report a series of\nhuman perception experiments to verify the quality of reconstructed videos;\ndemonstrate how the VPS changes over the scale transition in videos; and\npresent the close connection between VPS and high-level action models.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 16:13:01 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Han", "Zhi", ""], ["Xu", "Zongben", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1502.03044", "submitter": "Kelvin Xu", "authors": "Kelvin Xu and Jimmy Ba and Ryan Kiros and Kyunghyun Cho and Aaron\n  Courville and Ruslan Salakhutdinov and Richard Zemel and Yoshua Bengio", "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual\n  Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent work in machine translation and object detection, we\nintroduce an attention based model that automatically learns to describe the\ncontent of images. We describe how we can train this model in a deterministic\nmanner using standard backpropagation techniques and stochastically by\nmaximizing a variational lower bound. We also show through visualization how\nthe model is able to automatically learn to fix its gaze on salient objects\nwhile generating the corresponding words in the output sequence. We validate\nthe use of attention with state-of-the-art performance on three benchmark\ndatasets: Flickr8k, Flickr30k and MS COCO.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 19:18:29 GMT"}, {"version": "v2", "created": "Wed, 11 Feb 2015 02:58:54 GMT"}, {"version": "v3", "created": "Tue, 19 Apr 2016 16:43:09 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Xu", "Kelvin", ""], ["Ba", "Jimmy", ""], ["Kiros", "Ryan", ""], ["Cho", "Kyunghyun", ""], ["Courville", "Aaron", ""], ["Salakhutdinov", "Ruslan", ""], ["Zemel", "Richard", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1502.03121", "submitter": "Qi Wei", "authors": "Qi Wei, Nicolas Dobigeon, and Jean-Yves Tourneret", "title": "Fast Fusion of Multi-Band Images Based on Solving a Sylvester Equation", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2015.2458572", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a fast multi-band image fusion algorithm, which combines\na high-spatial low-spectral resolution image and a low-spatial high-spectral\nresolution image. The well admitted forward model is explored to form the\nlikelihoods of the observations. Maximizing the likelihoods leads to solving a\nSylvester equation. By exploiting the properties of the circulant and\ndownsampling matrices associated with the fusion problem, a closed-form\nsolution for the corresponding Sylvester equation is obtained explicitly,\ngetting rid of any iterative update step. Coupled with the alternating\ndirection method of multipliers and the block coordinate descent method, the\nproposed algorithm can be easily generalized to incorporate prior information\nfor the fusion problem, allowing a Bayesian estimator. Simulation results show\nthat the proposed algorithm achieves the same performance as existing\nalgorithms with the advantage of significantly decreasing the computational\ncomplexity of these algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 21:18:54 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Wei", "Qi", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1502.03126", "submitter": "Soheil Bahrampour", "authors": "Soheil Bahrampour and Nasser M. Nasrabadi and Asok Ray and Kenneth W.\n  Jenkins", "title": "Kernel Task-Driven Dictionary Learning for Hyperspectral Image\n  Classification", "comments": "5 pages, IEEE International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning algorithms have been successfully used in both\nreconstructive and discriminative tasks, where the input signal is represented\nby a linear combination of a few dictionary atoms. While these methods are\nusually developed under $\\ell_1$ sparsity constrain (prior) in the input\ndomain, recent studies have demonstrated the advantages of sparse\nrepresentation using structured sparsity priors in the kernel domain. In this\npaper, we propose a supervised dictionary learning algorithm in the kernel\ndomain for hyperspectral image classification. In the proposed formulation, the\ndictionary and classifier are obtained jointly for optimal classification\nperformance. The supervised formulation is task-driven and provides learned\nfeatures from the hyperspectral data that are well suited for the\nclassification task. Moreover, the proposed algorithm uses a joint\n($\\ell_{12}$) sparsity prior to enforce collaboration among the neighboring\npixels. The simulation results illustrate the efficiency of the proposed\ndictionary learning algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 21:27:27 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Bahrampour", "Soheil", ""], ["Nasrabadi", "Nasser M.", ""], ["Ray", "Asok", ""], ["Jenkins", "Kenneth W.", ""]]}, {"id": "1502.03215", "submitter": "Pratyaydipta Rudra", "authors": "Smarajit Bose, Amita Pal, Jhimli Mallick, Sunil Kumar and Pratyaydipta\n  Rudra", "title": "A Hybrid Approach for Improved Content-based Image Retrieval using\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of Content-Based Image Retrieval (CBIR) methods is essentially\nto extract, from large (image) databases, a specified number of images similar\nin visual and semantic content to a so-called query image. To bridge the\nsemantic gap that exists between the representation of an image by low-level\nfeatures (namely, colour, shape, texture) and its high-level semantic content\nas perceived by humans, CBIR systems typically make use of the relevance\nfeedback (RF) mechanism. RF iteratively incorporates user-given inputs\nregarding the relevance of retrieved images, to improve retrieval efficiency.\nOne approach is to vary the weights of the features dynamically via feature\nreweighting. In this work, an attempt has been made to improve retrieval\naccuracy by enhancing a CBIR system based on color features alone, through\nimplicit incorporation of shape information obtained through prior segmentation\nof the images. Novel schemes for feature reweighting as well as for\ninitialization of the relevant set for improved relevance feedback, have also\nbeen proposed for boosting performance of RF- based CBIR. At the same time, new\nmeasures for evaluation of retrieval accuracy have been suggested, to overcome\nthe limitations of existing measures in the RF context. Results of extensive\nexperiments have been presented to illustrate the effectiveness of the proposed\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 08:23:05 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Bose", "Smarajit", ""], ["Pal", "Amita", ""], ["Mallick", "Jhimli", ""], ["Kumar", "Sunil", ""], ["Rudra", "Pratyaydipta", ""]]}, {"id": "1502.03240", "submitter": "Shuai Zheng", "authors": "Shuai Zheng and Sadeep Jayasumana and Bernardino Romera-Paredes and\n  Vibhav Vineet and Zhizhong Su and Dalong Du and Chang Huang and Philip H. S.\n  Torr", "title": "Conditional Random Fields as Recurrent Neural Networks", "comments": "This paper is published in IEEE ICCV 2015", "journal-ref": null, "doi": "10.1109/ICCV.2015.179", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pixel-level labelling tasks, such as semantic segmentation, play a central\nrole in image understanding. Recent approaches have attempted to harness the\ncapabilities of deep learning techniques for image recognition to tackle\npixel-level labelling tasks. One central issue in this methodology is the\nlimited capacity of deep learning techniques to delineate visual objects. To\nsolve this problem, we introduce a new form of convolutional neural network\nthat combines the strengths of Convolutional Neural Networks (CNNs) and\nConditional Random Fields (CRFs)-based probabilistic graphical modelling. To\nthis end, we formulate mean-field approximate inference for the Conditional\nRandom Fields with Gaussian pairwise potentials as Recurrent Neural Networks.\nThis network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a\ndeep network that has desirable properties of both CNNs and CRFs. Importantly,\nour system fully integrates CRF modelling with CNNs, making it possible to\ntrain the whole deep network end-to-end with the usual back-propagation\nalgorithm, avoiding offline post-processing methods for object delineation. We\napply the proposed method to the problem of semantic image segmentation,\nobtaining top results on the challenging Pascal VOC 2012 segmentation\nbenchmark.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 10:02:50 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2015 19:15:31 GMT"}, {"version": "v3", "created": "Wed, 13 Apr 2016 23:26:45 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Zheng", "Shuai", ""], ["Jayasumana", "Sadeep", ""], ["Romera-Paredes", "Bernardino", ""], ["Vineet", "Vibhav", ""], ["Su", "Zhizhong", ""], ["Du", "Dalong", ""], ["Huang", "Chang", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1502.03273", "submitter": "Dai-Qiang Chen", "authors": "Dai-Qiang Chen", "title": "Image denoising based on improved data-driven sparse representation", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation of images under certain transform domain has been\nplaying a fundamental role in image restoration tasks. One such representative\nmethod is the widely used wavelet tight frame systems. Instead of adopting\nfixed filters for constructing a tight frame to sparsely model any input image,\na data-driven tight frame was proposed for the sparse representation of images,\nand shown to be very efficient for image denoising very recently. However, in\nthis method the number of framelet filters used for constructing a tight frame\nis the same as the length of filters. In fact, through further investigation it\nis found that part of these filters are unnecessary and even harmful to the\nrecovery effect due to the influence of noise. Therefore, an improved\ndata-driven sparse representation systems constructed with much less number of\nfilters are proposed. Numerical results on denoising experiments demonstrate\nthat the proposed algorithm overall outperforms the original data-driven tight\nframe construction scheme on both the recovery quality and computational time.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 11:57:53 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 13:09:36 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Chen", "Dai-Qiang", ""]]}, {"id": "1502.03409", "submitter": "Karl Ni", "authors": "Karl Ni, Roger Pearce, Kofi Boakye, Brian Van Essen, Damian Borth,\n  Barry Chen, Eric Wang", "title": "Large-Scale Deep Learning on the YFCC100M Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We present a work-in-progress snapshot of learning with a 15 billion\nparameter deep learning network on HPC architectures applied to the largest\npublicly available natural image and video dataset released to-date. Recent\nadvancements in unsupervised deep neural networks suggest that scaling up such\nnetworks in both model and training dataset size can yield significant\nimprovements in the learning of concepts at the highest layers. We train our\nthree-layer deep neural network on the Yahoo! Flickr Creative Commons 100M\ndataset. The dataset comprises approximately 99.2 million images and 800,000\nuser-created videos from Yahoo's Flickr image and video sharing platform.\nTraining of our network takes eight days on 98 GPU nodes at the High\nPerformance Computing Center at Lawrence Livermore National Laboratory.\nEncouraging preliminary results and future research directions are presented\nand discussed.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 19:24:36 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Ni", "Karl", ""], ["Pearce", "Roger", ""], ["Boakye", "Kofi", ""], ["Van Essen", "Brian", ""], ["Borth", "Damian", ""], ["Chen", "Barry", ""], ["Wang", "Eric", ""]]}, {"id": "1502.03436", "submitter": "Felix X. Yu", "authors": "Yu Cheng, Felix X. Yu, Rogerio S. Feris, Sanjiv Kumar, Alok Choudhary,\n  Shih-Fu Chang", "title": "An exploration of parameter redundancy in deep networks with circulant\n  projections", "comments": "International Conference on Computer Vision (ICCV) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the redundancy of parameters in deep neural networks by replacing\nthe conventional linear projection in fully-connected layers with the circulant\nprojection. The circulant structure substantially reduces memory footprint and\nenables the use of the Fast Fourier Transform to speed up the computation.\nConsidering a fully-connected neural network layer with d input nodes, and d\noutput nodes, this method improves the time complexity from O(d^2) to O(dlogd)\nand space complexity from O(d^2) to O(d). The space savings are particularly\nimportant for modern deep convolutional neural network architectures, where\nfully-connected layers typically contain more than 90% of the network\nparameters. We further show that the gradient computation and optimization of\nthe circulant projections can be performed very efficiently. Our experiments on\nthree standard datasets show that the proposed approach achieves this\nsignificant gain in storage and efficiency with minimal increase in error rate\ncompared to neural networks with unstructured projections.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 20:56:02 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 06:45:51 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Cheng", "Yu", ""], ["Yu", "Felix X.", ""], ["Feris", "Rogerio S.", ""], ["Kumar", "Sanjiv", ""], ["Choudhary", "Alok", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1502.03532", "submitter": "Weihua Chen", "authors": "Weihua Chen, Lijun Cao, Xiaotang Chen, Kaiqi Huang", "title": "An equalised global graphical model-based approach for multi-camera\n  object tracking", "comments": "13 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-overlapping multi-camera visual object tracking typically consists of two\nsteps: single camera object tracking and inter-camera object tracking. Most of\ntracking methods focus on single camera object tracking, which happens in the\nsame scene, while for real surveillance scenes, inter-camera object tracking is\nneeded and single camera tracking methods can not work effectively. In this\npaper, we try to improve the overall multi-camera object tracking performance\nby a global graph model with an improved similarity metric. Our method treats\nthe similarities of single camera tracking and inter-camera tracking\ndifferently and obtains the optimization in a global graph model. The results\nshow that our method can work better even in the condition of poor single\ncamera object tracking.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 04:10:56 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 01:45:53 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Chen", "Weihua", ""], ["Cao", "Lijun", ""], ["Chen", "Xiaotang", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1502.03537", "submitter": "Vamsi Ithapu", "authors": "Vamsi K Ithapu, Sathya Ravi, Vikas Singh", "title": "Convergence of gradient based pre-training in Denoising autoencoders", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep architectures is at least in part attributed to the\nlayer-by-layer unsupervised pre-training that initializes the network. Various\npapers have reported extensive empirical analysis focusing on the design and\nimplementation of good pre-training procedures. However, an understanding\npertaining to the consistency of parameter estimates, the convergence of\nlearning procedures and the sample size estimates is still unavailable in the\nliterature. In this work, we study pre-training in classical and distributed\ndenoising autoencoders with these goals in mind. We show that the gradient\nconverges at the rate of $\\frac{1}{\\sqrt{N}}$ and has a sub-linear dependence\non the size of the autoencoder network. In a distributed setting where disjoint\nsections of the whole network are pre-trained synchronously, we show that the\nconvergence improves by at least $\\tau^{3/4}$, where $\\tau$ corresponds to the\nsize of the sections. We provide a broad set of experiments to empirically\nevaluate the suggested behavior.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 04:31:36 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Ithapu", "Vamsi K", ""], ["Ravi", "Sathya", ""], ["Singh", "Vikas", ""]]}, {"id": "1502.03596", "submitter": "Fredrik Sandin", "authors": "Sergio Martin-del-Campo, Fredrik Sandin", "title": "Towards zero-configuration condition monitoring based on dictionary\n  learning", "comments": "5 pages, 3 figures", "journal-ref": "2015 23rd European Signal Processing Conference (EUSIPCO)", "doi": "10.1109/EUSIPCO.2015.7362595", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Condition-based predictive maintenance can significantly improve overall\nequipment effectiveness provided that appropriate monitoring methods are used.\nOnline condition monitoring systems are customized to each type of machine and\nneed to be reconfigured when conditions change, which is costly and requires\nexpert knowledge. Basic feature extraction methods limited to signal\ndistribution functions and spectra are commonly used, making it difficult to\nautomatically analyze and compare machine conditions. In this paper, we\ninvestigate the possibility to automate the condition monitoring process by\ncontinuously learning a dictionary of optimized shift-invariant feature vectors\nusing a well-known sparse approximation method. We study how the feature\nvectors learned from a vibration signal evolve over time when a fault develops\nwithin a ball bearing of a rotating machine. We quantify the adaptation rate of\nlearned features and find that this quantity changes significantly in the\ntransitions between normal and faulty states of operation of the ball bearing.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 10:49:12 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Martin-del-Campo", "Sergio", ""], ["Sandin", "Fredrik", ""]]}, {"id": "1502.03723", "submitter": "Helio M. de Oliveira", "authors": "H.M. de Oliveira, J. Ranhel, R.B.A. Alves", "title": "Simulation of Color Blindness and a Proposal for Using Google Glass as\n  Color-correcting Tool", "comments": "4 pages, 6 figures, XXIV Congresso Brasileiro de Engenharia\n  Biomedica, Uberlandia, MG, Brazil, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human visual color response is driven by specialized cells called cones,\nwhich exist in three types, viz. R, G, and B. Software is developed to simulate\nhow color images are displayed for different types of color blindness.\nSpecified the default color deficiency associated with a user, it generates a\npreview of the rainbow (in the visible range, from red to violet) and shows up,\nside by side with a colorful image provided as input, the display correspondent\ncolorblind. The idea is to provide an image processing after image acquisition\nto enable a better perception ofcolors by the color blind. Examples of\npseudo-correction are shown for the case of Protanopia (red blindness). The\nsystem is adapted into a screen of an i-pad or a cellphone in which the\ncolorblind observe the camera, the image processed with color detail previously\nimperceptible by his naked eye. As prospecting, wearable computer glasses could\nbe manufactured to provide a corrected image playback. The approach can also\nprovide augmented reality for human vision by adding the UV or IR responses as\na new feature of Google Glass.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 16:36:55 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["de Oliveira", "H. M.", ""], ["Ranhel", "J.", ""], ["Alves", "R. B. A.", ""]]}, {"id": "1502.03851", "submitter": "Arash Vahdat", "authors": "Mehran Khodabandeh, Arash Vahdat, Guang-Tong Zhou, Hossein\n  Hajimirsadeghi, Mehrsan Javan Roshtkhari, Greg Mori, Stephen Se", "title": "Discovering Human Interactions in Videos with Limited Data Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for discovering human interactions in videos.\nActivity understanding techniques usually require a large number of labeled\nexamples, which are not available in many practical cases. Here, we focus on\nrecovering semantically meaningful clusters of human-human and human-object\ninteraction in an unsupervised fashion. A new iterative solution is introduced\nbased on Maximum Margin Clustering (MMC), which also accepts user feedback to\nrefine clusters. This is achieved by formulating the whole process as a unified\nconstrained latent max-margin clustering problem. Extensive experiments have\nbeen carried out over three challenging datasets, Collective Activity, VIRAT,\nand UT-interaction. Empirical results demonstrate that the proposed algorithm\ncan efficiently discover perfect semantic clusters of human interactions with\nonly a small amount of labeling effort.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 22:38:28 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Khodabandeh", "Mehran", ""], ["Vahdat", "Arash", ""], ["Zhou", "Guang-Tong", ""], ["Hajimirsadeghi", "Hossein", ""], ["Roshtkhari", "Mehrsan Javan", ""], ["Mori", "Greg", ""], ["Se", "Stephen", ""]]}, {"id": "1502.03879", "submitter": "Weiya Ren", "authors": "Weiya Ren", "title": "Semi-supervised Data Representation via Affinity Graph Learning", "comments": "10 pages,2 Tables. Written in Aug,2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the general problem of utilizing both labeled and unlabeled data\nto improve data representation performance. A new semi-supervised learning\nframework is proposed by combing manifold regularization and data\nrepresentation methods such as Non negative matrix factorization and sparse\ncoding. We adopt unsupervised data representation methods as the learning\nmachines because they do not depend on the labeled data, which can improve\nmachine's generation ability as much as possible. The proposed framework forms\nthe Laplacian regularizer through learning the affinity graph. We incorporate\nthe new Laplacian regularizer into the unsupervised data representation to\nsmooth the low dimensional representation of data and make use of label\ninformation. Experimental results on several real benchmark datasets indicate\nthat our semi-supervised learning framework achieves encouraging results\ncompared with state-of-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 03:35:15 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Ren", "Weiya", ""]]}, {"id": "1502.03913", "submitter": "Smitha M.L.", "authors": "B.H. Shekar, Smitha M.L", "title": "Skeleton Matching based approach for Text Localization in Scene Images", "comments": "10 pages, 8 figures, Eighth International Conference on Image and\n  Signal Processing,Elsevier Publications,pp: 145-153, held at UVCE, Bangalore\n  in July 2014. ISBN: 9789351072522", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a skeleton matching based approach which aids in\ntext localization in scene images. The input image is preprocessed and\nsegmented into blocks using connected component analysis. We obtain the\nskeleton of the segmented block using morphology based approach. The\nskeletonized images are compared with the trained templates in the database to\ncategorize into text and non-text blocks. Further, the newly designed\ngeometrical rules and morphological operations are employed on the detected\ntext blocks for scene text localization. The experimental results obtained on\npublicly available standard datasets illustrate that the proposed method can\ndetect and localize the texts of various sizes, fonts and colors.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 08:42:00 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Shekar", "B. H.", ""], ["L", "Smitha M.", ""]]}, {"id": "1502.03918", "submitter": "Smitha M.L.", "authors": "B.H. Shekar, Smitha M.L", "title": "Gradient Difference based approach for Text Localization in Compressed\n  domain", "comments": "11 pages, Second International Conference on Emerging Research in\n  Computing, Information, Communications and Applications, Elsevier\n  Publications, ISBN: 9789351072638, vol. III, pp: 299-308, held at NMIT,\n  Bangalore August 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a gradient difference based approach to text\nlocalization in videos and scene images. The input video frame/ image is first\ncompressed using multilevel 2-D wavelet transform. The edge information of the\nreconstructed image is found which is further used for finding the maximum\ngradient difference between the pixels and then the boundaries of the detected\ntext blocks are computed using zero crossing technique. We perform logical AND\noperation of the text blocks obtained by gradient difference and the zero\ncrossing technique followed by connected component analysis to eliminate the\nfalse positives. Finally, the morphological dilation operation is employed on\nthe detected text blocks for scene text localization. The experimental results\nobtained on publicly available standard datasets illustrate that the proposed\nmethod can detect and localize the texts of various sizes, fonts and colors.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 09:08:35 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Shekar", "B. H.", ""], ["L", "Smitha M.", ""]]}, {"id": "1502.04110", "submitter": "Pascal Fua", "authors": "Pascal Fua and Graham Knott", "title": "Modeling Brain Circuitry over a Wide Range of Scales", "comments": null, "journal-ref": null, "doi": "10.3389/fnana.2015.00042", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If we are ever to unravel the mysteries of brain function at its most\nfundamental level, we will need a precise understanding of how its component\nneurons connect to each other. Electron Microscopes (EM) can now provide the\nnanometer resolution that is needed to image synapses, and therefore\nconnections, while Light Microscopes (LM) see at the micrometer resolution\nrequired to model the 3D structure of the dendritic network. Since both the\ntopology and the connection strength are integral parts of the brain's wiring\ndiagram, being able to combine these two modalities is critically important.\n  In fact, these microscopes now routinely produce high-resolution imagery in\nsuch large quantities that the bottleneck becomes automated processing and\ninterpretation, which is needed for such data to be exploited to its full\npotential. In this paper, we briefly review the Computer Vision techniques we\nhave developed at EPFL to address this need. They include delineating dendritic\narbors from LM imagery, segmenting organelles from EM, and combining the two\ninto a consistent representation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 20:41:07 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2015 07:23:47 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Fua", "Pascal", ""], ["Knott", "Graham", ""]]}, {"id": "1502.04132", "submitter": "Zhenzhong Lan", "authors": "Zhenzhong Lan, Xuanchong Li, Ming Lin, Alexander G. Hauptmann", "title": "Long-short Term Motion Feature for Action Classification and Retrieval", "comments": "arXiv admin note: text overlap with arXiv:1411.6660", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for representing motion information for video\nclassification and retrieval. We improve upon local descriptor based methods\nthat have been among the most popular and successful models for representing\nvideos. The desired local descriptors need to satisfy two requirements: 1) to\nbe representative, 2) to be discriminative. Therefore, they need to occur\nfrequently enough in the videos and to be be able to tell the difference among\ndifferent types of motions. To generate such local descriptors, the video\nblocks they are based on must contain just the right amount of motion\ninformation. However, current state-of-the-art local descriptor methods use\nvideo blocks with a single fixed size, which is insufficient for covering\nactions with varying speeds. In this paper, we introduce a long-short term\nmotion feature that generates descriptors from video blocks with multiple\nlengths, thus covering motions with large speed variance. Experimental results\nshow that, albeit simple, our model achieves state-of-the-arts results on\nseveral benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 21:15:57 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Lan", "Zhenzhong", ""], ["Li", "Xuanchong", ""], ["Lin", "Ming", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1502.04204", "submitter": "Amelia Carolina Sparavigna", "authors": "Amelia Carolina Sparavigna", "title": "Gray-Level Image Transitions Driven by Tsallis Entropic Index", "comments": "Tsallis Entropy, Image Processing, Image Segmentation, Image\n  Thresholding, Texture Transitions, Medical Image Processing, Typos emended", "journal-ref": "International Journal of Sciences 4(2), 16-25, 2015", "doi": "10.18483/ijSci.621", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum entropy principle is largely used in thresholding and\nsegmentation of images. Among the several formulations of this principle, the\nmost effectively applied is that based on Tsallis non-extensive entropy. Here,\nwe discuss the role of its entropic index in determining the thresholds. When\nthis index is spanning the interval (0,1), for some images, the values of\nthresholds can have large leaps. In this manner, we observe abrupt transitions\nin the appearance of corresponding bi-level or multi-level images. These\ngray-level image transitions are analogous to order or texture transitions\nobserved in physical systems, transitions which are driven by the temperature\nor by other physical quantities.\n", "versions": [{"version": "v1", "created": "Sat, 14 Feb 2015 12:35:12 GMT"}, {"version": "v2", "created": "Thu, 19 Feb 2015 18:12:30 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Sparavigna", "Amelia Carolina", ""]]}, {"id": "1502.04252", "submitter": "Tizita Nesibu Shewaye Mrs", "authors": "Tizita Nesibu Shewaye", "title": "Cardiac MR Image Segmentation Techniques: an overview", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Broadly speaking, the objective in cardiac image segmentation is to delineate\nthe outer and inner walls of the heart to segment out either the entire or\nparts of the organ boundaries. This paper will focus on MR images as they are\nthe most widely used in cardiac segmentation -- as a result of the accurate\nmorphological information and better soft tissue contrast they provide. This\ncardiac segmentation information is very useful as it eases physical\nmeasurements that provides useful metrics for cardiac diagnosis such as\ninfracted volumes, ventricular volumes, ejection fraction, myocardial mass,\ncardiac movement, and the like. But, this task is difficult due to the\nintensity and texture similarities amongst the different cardiac and background\nstructures on top of some noisy artifacts present in MR images. Thus far,\nvarious researchers have proposed different techniques to solve some of the\npressing issues. This seminar paper presents an overview of representative\nmedical image segmentation techniques. The paper also highlights preferred\napproaches for segmentation of the four cardiac chambers: the left ventricle\n(LV), right ventricle (RV), left atrium (LA) and right atrium (RA), on short\naxis image planes.\n", "versions": [{"version": "v1", "created": "Sat, 14 Feb 2015 22:15:39 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Shewaye", "Tizita Nesibu", ""]]}, {"id": "1502.04272", "submitter": "Alex James Dr", "authors": "Joshin John Mathew and Alex Pappachen James", "title": "Spatial Stimuli Gradient Sketch Model", "comments": "accepted for publication in IEEE Signal Processing Letters, 2015", "journal-ref": "Volume: 22 Issue: 9 On page(s): 1336-1339, 2015", "doi": "10.1109/LSP.2015.2404827", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The inability of automated edge detection methods inspired from primal sketch\nmodels to accurately calculate object edges under the influence of pixel noise\nis an open problem. Extending the principles of image perception i.e.\nWeber-Fechner law, and Sheperd similarity law, we propose a new edge detection\nmethod and formulation that use perceived brightness and neighbourhood\nsimilarity calculations in the determination of robust object edges. The\nrobustness of the detected edges is benchmark against Sobel, SIS, Kirsch, and\nPrewitt edge detection methods in an example face recognition problem showing\nstatistically significant improvement in recognition accuracy and pixel noise\ntolerance.\n", "versions": [{"version": "v1", "created": "Sun, 15 Feb 2015 02:42:56 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Mathew", "Joshin John", ""], ["James", "Alex Pappachen", ""]]}, {"id": "1502.04275", "submitter": "Yukun Zhu", "authors": "Yukun Zhu, Raquel Urtasun, Ruslan Salakhutdinov, Sanja Fidler", "title": "segDeepM: Exploiting Segmentation and Context in Deep Neural Networks\n  for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an approach that exploits object segmentation in\norder to improve the accuracy of object detection. We frame the problem as\ninference in a Markov Random Field, in which each detection hypothesis scores\nobject appearance as well as contextual information using Convolutional Neural\nNetworks, and allows the hypothesis to choose and score a segment out of a\nlarge pool of accurate object segmentation proposals. This enables the detector\nto incorporate additional evidence when it is available and thus results in\nmore accurate detections. Our experiments show an improvement of 4.1% in mAP\nover the R-CNN baseline on PASCAL VOC 2010, and 3.4% over the current\nstate-of-the-art, demonstrating the power of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 15 Feb 2015 02:53:56 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Zhu", "Yukun", ""], ["Urtasun", "Raquel", ""], ["Salakhutdinov", "Ruslan", ""], ["Fidler", "Sanja", ""]]}, {"id": "1502.04383", "submitter": "Dacheng Tao", "authors": "Changxing Ding, Dacheng Tao", "title": "A Comprehensive Survey on Pose-Invariant Face Recognition", "comments": "final version, ACM Transactions on Intelligent Systems and\n  Technology, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capacity to recognize faces under varied poses is a fundamental human\nability that presents a unique challenge for computer vision systems. Compared\nto frontal face recognition, which has been intensively studied and has\ngradually matured in the past few decades, pose-invariant face recognition\n(PIFR) remains a largely unsolved problem. However, PIFR is crucial to\nrealizing the full potential of face recognition for real-world applications,\nsince face recognition is intrinsically a passive biometric technology for\nrecognizing uncooperative subjects. In this paper, we discuss the inherent\ndifficulties in PIFR and present a comprehensive review of established\ntechniques. Existing PIFR methods can be grouped into four categories, i.e.,\npose-robust feature extraction approaches, multi-view subspace learning\napproaches, face synthesis approaches, and hybrid approaches. The motivations,\nstrategies, pros/cons, and performance of representative approaches are\ndescribed and compared. Moreover, promising directions for future research are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Sun, 15 Feb 2015 23:25:32 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2015 09:42:50 GMT"}, {"version": "v3", "created": "Sat, 16 Jul 2016 06:40:18 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Ding", "Changxing", ""], ["Tao", "Dacheng", ""]]}, {"id": "1502.04492", "submitter": "Francesco  Palmieri A. N.", "authors": "Amedeo Buonanno and Francesco A.N. Palmieri", "title": "Towards Building Deep Networks with Bayesian Factor Graphs", "comments": "Submitted for journal publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Multi-Layer Network based on the Bayesian framework of the\nFactor Graphs in Reduced Normal Form (FGrn) applied to a two-dimensional\nlattice. The Latent Variable Model (LVM) is the basic building block of a\nquadtree hierarchy built on top of a bottom layer of random variables that\nrepresent pixels of an image, a feature map, or more generally a collection of\nspatially distributed discrete variables. The multi-layer architecture\nimplements a hierarchical data representation that, via belief propagation, can\nbe used for learning and inference. Typical uses are pattern completion,\ncorrection and classification. The FGrn paradigm provides great flexibility and\nmodularity and appears as a promising candidate for building deep networks: the\nsystem can be easily extended by introducing new and different (in cardinality\nand in type) variables. Prior knowledge, or supervised information, can be\nintroduced at different scales. The FGrn paradigm provides a handy way for\nbuilding all kinds of architectures by interconnecting only three types of\nunits: Single Input Single Output (SISO) blocks, Sources and Replicators. The\nnetwork is designed like a circuit diagram and the belief messages flow\nbidirectionally in the whole system. The learning algorithms operate only\nlocally within each block. The framework is demonstrated in this paper in a\nthree-layer structure applied to images extracted from a standard data set.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 11:01:25 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Buonanno", "Amedeo", ""], ["Palmieri", "Francesco A. N.", ""]]}, {"id": "1502.04499", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu", "title": "Color Image Enhancement Using the lrgb Coordinates in the Context of\n  Support Fuzzification", "comments": null, "journal-ref": "Romanian Academy Journal, Fuzzy systems and A. I., Reports and\n  Letters, F.S.A.I., Vol.10, Nos. 1-2, pp. 29-40, 2004", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image enhancement is an important stage in the image-processing domain. The\nmost known image enhancement method is the histogram equalization. This method\nis an automated one, and realizes a simultaneous modification for brightness\nand contrast in the case of monochrome images and for brightness, contrast,\nsaturation and hue in the case of color images. Simple and efficient methods\ncan be obtained if affine transforms within logarithmic models are used. A very\nimportant thing in the affine transform determination for color images is the\ncoordinate system that is used for color space representation. Thus, the using\nof the RGB coordinates leads to a simultaneous modification of luminosity and\nsaturation. In this paper using the lrgb perceptual coordinates one can define\naffine transforms, which allow a separated modification of luminosity l and\nsaturation s (saturation being calculated with the component rgb in the\nchromatic plane). Better results can be obtained if partitions are defined on\nthe image support and then the pixels are separately processed in each window\nbelonging to the defined partition. Classical partitions frequently lead to the\nappearance of some discontinuities at the boundaries between these windows. In\norder to avoid all these drawbacks the classical partitions may be replaced by\nfuzzy partitions. Their elements will be fuzzy windows and in each of them\nthere will be defined an affine transform induced by parameters using the fuzzy\nmean, fuzzy variance and fuzzy saturation computed for the pixels that belong\nto the analyzed window. The final image is obtained by summing up in a weight\nway the images of every fuzzy window.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 11:32:00 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Patrascu", "Vasile", ""]]}, {"id": "1502.04500", "submitter": "Amelia Carolina Sparavigna", "authors": "Amelia Carolina Sparavigna", "title": "Bi-Level Image Thresholding obtained by means of Kaniadakis Entropy", "comments": "Kaniadakis Entropy, Image Processing, Image Segmentation, Image\n  Thresholding, Texture Transitions, added reference and revised typos This\n  paper has been withdrawn by the author due to a crucial sign error in Figures\n  3 and 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are proposing the use of Kaniadakis entropy in the bi-level\nthresholding of images, in the framework of a maximum entropy principle. We\ndiscuss the role of its entropic index in determining the threshold and in\ndriving an \"image transition\", that is, an abrupt transition in the appearance\nof the corresponding bi-level image. Some examples are proposed to illustrate\nthe method and for comparing it to the approach which is using the Tsallis\nentropy.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 11:32:34 GMT"}, {"version": "v2", "created": "Thu, 19 Feb 2015 12:04:57 GMT"}, {"version": "v3", "created": "Sun, 22 Feb 2015 12:24:51 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Sparavigna", "Amelia Carolina", ""]]}, {"id": "1502.04502", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "Clustering by Descending to the Nearest Neighbor in the Delaunay Graph\n  Space", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In our previous works, we proposed a physically-inspired rule to organize the\ndata points into an in-tree (IT) structure, in which some undesired edges are\nallowed to occur. By removing those undesired or redundant edges, this IT\nstructure is divided into several separate parts, each representing one\ncluster. In this work, we seek to prevent the undesired edges from arising at\nthe source. Before using the physically-inspired rule, data points are at first\norganized into a proximity graph which restricts each point to select the\noptimal directed neighbor just among its neighbors. Consequently, separated\nin-trees or clusters automatically arise, without redundant edges requiring to\nbe removed.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 11:50:42 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1502.04569", "submitter": "Mainak Jas", "authors": "Mainak Jas and Devi Parikh", "title": "Image Specificity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For some images, descriptions written by multiple people are consistent with\neach other. But for other images, descriptions across people vary considerably.\nIn other words, some images are specific $-$ they elicit consistent\ndescriptions from different people $-$ while other images are ambiguous.\nApplications involving images and text can benefit from an understanding of\nwhich images are specific and which ones are ambiguous. For instance, consider\ntext-based image retrieval. If a query description is moderately similar to the\ncaption (or reference description) of an ambiguous image, that query may be\nconsidered a decent match to the image. But if the image is very specific, a\nmoderate similarity between the query and the reference description may not be\nsufficient to retrieve the image.\n  In this paper, we introduce the notion of image specificity. We present two\nmechanisms to measure specificity given multiple descriptions of an image: an\nautomated measure and a measure that relies on human judgement. We analyze\nimage specificity with respect to image content and properties to better\nunderstand what makes an image specific. We then train models to automatically\npredict the specificity of an image from image features alone without requiring\ntextual descriptions of the image. Finally, we show that modeling image\nspecificity leads to improvements in a text-based image retrieval application.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 15:16:25 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2015 13:13:46 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Jas", "Mainak", ""], ["Parikh", "Devi", ""]]}, {"id": "1502.04623", "submitter": "Ivo Danihelka", "authors": "Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan\n  Wierstra", "title": "DRAW: A Recurrent Neural Network For Image Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural\nnetwork architecture for image generation. DRAW networks combine a novel\nspatial attention mechanism that mimics the foveation of the human eye, with a\nsequential variational auto-encoding framework that allows for the iterative\nconstruction of complex images. The system substantially improves on the state\nof the art for generative models on MNIST, and, when trained on the Street View\nHouse Numbers dataset, it generates images that cannot be distinguished from\nreal data with the naked eye.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 16:48:56 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 15:29:42 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Gregor", "Karol", ""], ["Danihelka", "Ivo", ""], ["Graves", "Alex", ""], ["Rezende", "Danilo Jimenez", ""], ["Wierstra", "Daan", ""]]}, {"id": "1502.04652", "submitter": "Saurabh Gupta", "authors": "Saurabh Gupta, Pablo Arbel\\'aez, Ross Girshick, Jitendra Malik", "title": "Inferring 3D Object Pose in RGB-D Images", "comments": "13 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to replace objects in an RGB-D scene with\ncorresponding 3D models from a library. We approach this problem by first\ndetecting and segmenting object instances in the scene using the approach from\nGupta et al. [13]. We use a convolutional neural network (CNN) to predict the\npose of the object. This CNN is trained using pixel normals in images\ncontaining rendered synthetic objects. When tested on real data, it outperforms\nalternative algorithms trained on real data. We then use this coarse pose\nestimate along with the inferred pixel support to align a small number of\nprototypical models to the data, and place the model that fits the best into\nthe scene. We observe a 48% relative improvement in performance at the task of\n3D detection over the current state-of-the-art [33], while being an order of\nmagnitude faster at the same time.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 18:15:54 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Gupta", "Saurabh", ""], ["Arbel\u00e1ez", "Pablo", ""], ["Girshick", "Ross", ""], ["Malik", "Jitendra", ""]]}, {"id": "1502.04658", "submitter": "Xianbiao Qi", "authors": "Xianbiao Qi, Guoying Zhao, Chun-Guang Li, Jun Guo, Matti Pietik\\\"ainen", "title": "HEp-2 Cell Classification via Fusing Texture and Shape Information", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indirect Immunofluorescence (IIF) HEp-2 cell image is an effective evidence\nfor diagnosis of autoimmune diseases. Recently computer-aided diagnosis of\nautoimmune diseases by IIF HEp-2 cell classification has attracted great\nattention. However the HEp-2 cell classification task is quite challenging due\nto large intra-class variation and small between-class variation. In this paper\nwe propose an effective and efficient approach for the automatic classification\nof IIF HEp-2 cell image by fusing multi-resolution texture information and\nricher shape information. To be specific, we propose to: a) capture the\nmulti-resolution texture information by a novel Pairwise Rotation Invariant\nCo-occurrence of Local Gabor Binary Pattern (PRICoLGBP) descriptor, b) depict\nthe richer shape information by using an Improved Fisher Vector (IFV) model\nwith RootSIFT features which are sampled from large image patches in multiple\nscales, and c) combine them properly. We evaluate systematically the proposed\napproach on the IEEE International Conference on Pattern Recognition (ICPR)\n2012, IEEE International Conference on Image Processing (ICIP) 2013 and ICPR\n2014 contest data sets. The experimental results for the proposed methods\nsignificantly outperform the winners of ICPR 2012 and ICIP 2013 contest, and\nachieve comparable performance with the winner of the newly released ICPR 2014\ncontest.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 18:36:31 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Qi", "Xianbiao", ""], ["Zhao", "Guoying", ""], ["Li", "Chun-Guang", ""], ["Guo", "Jun", ""], ["Pietik\u00e4inen", "Matti", ""]]}, {"id": "1502.04681", "submitter": "Nitish Srivastava", "authors": "Nitish Srivastava, Elman Mansimov and Ruslan Salakhutdinov", "title": "Unsupervised Learning of Video Representations using LSTMs", "comments": "Added link to code on github", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use multilayer Long Short Term Memory (LSTM) networks to learn\nrepresentations of video sequences. Our model uses an encoder LSTM to map an\ninput sequence into a fixed length representation. This representation is\ndecoded using single or multiple decoder LSTMs to perform different tasks, such\nas reconstructing the input sequence, or predicting the future sequence. We\nexperiment with two kinds of input sequences - patches of image pixels and\nhigh-level representations (\"percepts\") of video frames extracted using a\npretrained convolutional net. We explore different design choices such as\nwhether the decoder LSTMs should condition on the generated output. We analyze\nthe outputs of the model qualitatively to see how well the model can\nextrapolate the learned video representation into the future and into the past.\nWe try to visualize and interpret the learned features. We stress test the\nmodel by running it on longer time scales and on out-of-domain data. We further\nevaluate the representations by finetuning them for a supervised learning\nproblem - human action recognition on the UCF-101 and HMDB-51 datasets. We show\nthat the representations help improve classification accuracy, especially when\nthere are only a few training examples. Even models pretrained on unrelated\ndatasets (300 hours of YouTube videos) can help action recognition performance.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 20:00:07 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 23:45:59 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2016 00:42:07 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Srivastava", "Nitish", ""], ["Mansimov", "Elman", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1502.04726", "submitter": "Hojjat Seyed Mousavi", "authors": "Hojjat S. Mousavi, Vishal Monga, Trac D. Tran", "title": "ICR: Iterative Convex Refinement for Sparse Signal Recovery Using Spike\n  and Slab Priors", "comments": "Submitted to IEEE Signal Processing Letters, Feb 2015", "journal-ref": null, "doi": "10.1109/LSP.2015.2438255", "report-no": null, "categories": "stat.ML cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we address sparse signal recovery using spike and slab\npriors. In particular, we focus on a Bayesian framework where sparsity is\nenforced on reconstruction coefficients via probabilistic priors. The\noptimization resulting from spike and slab prior maximization is known to be a\nhard non-convex problem, and existing solutions involve simplifying assumptions\nand/or relaxations. We propose an approach called Iterative Convex Refinement\n(ICR) that aims to solve the aforementioned optimization problem directly\nallowing for greater generality in the sparse structure. Essentially, ICR\nsolves a sequence of convex optimization problems such that sequence of\nsolutions converges to a sub-optimal solution of the original hard optimization\nproblem. We propose two versions of our algorithm: a.) an unconstrained\nversion, and b.) with a non-negativity constraint on sparse coefficients, which\nmay be required in some real-world problems. Experimental validation is\nperformed on both synthetic data and for a real-world image recovery problem,\nwhich illustrates merits of ICR over state of the art alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 21:17:52 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Mousavi", "Hojjat S.", ""], ["Monga", "Vishal", ""], ["Tran", "Trac D.", ""]]}, {"id": "1502.04754", "submitter": "Cosimo Rubino", "authors": "Cosimo Rubino and Marco Crocco and Alessandro Perina and Vittorio\n  Murino and Alessio Del Bue", "title": "3D Pose from Detections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method to infer, in closed-form, a general 3D spatial\noccupancy and orientation of a collection of rigid objects given 2D image\ndetections from a sequence of images. In particular, starting from 2D ellipses\nfitted to bounding boxes, this novel multi-view problem can be reformulated as\nthe estimation of a quadric (ellipsoid) in 3D. We show that an efficient\nsolution exists in the dual-space using a minimum of three views while a\nsolution with two views is possible through the use of regularization. However,\nthis algebraic solution can be negatively affected in the presence of gross\ninaccuracies in the bounding boxes estimation. To this end, we also propose a\nrobust ellipse fitting algorithm able to improve performance in the presence of\nerrors in the detected objects. Results on synthetic tests and on different\nreal datasets, involving real challenging scenarios, demonstrate the\napplicability and potential of our method.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 00:11:41 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2015 23:40:57 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2015 18:27:38 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Rubino", "Cosimo", ""], ["Crocco", "Marco", ""], ["Perina", "Alessandro", ""], ["Murino", "Vittorio", ""], ["Del Bue", "Alessio", ""]]}, {"id": "1502.04824", "submitter": "Aviv Rotbart", "authors": "Aviv Rotbart, Gil Shabat, Yaniv Shmueli and Amir Averbuch", "title": "Randomized LU decomposition: An Algorithm for Dictionaries Construction", "comments": "Errors in several parts of the paper. No replacement is currently\n  available or being written", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, distinctive-dictionary construction has gained importance\ndue to his usefulness in data processing. Usually, one or more dictionaries are\nconstructed from a training data and then they are used to classify signals\nthat did not participate in the training process. A new dictionary construction\nalgorithm is introduced. It is based on a low-rank matrix factorization being\nachieved by the application of the randomized LU decomposition to a training\ndata. This method is fast, scalable, parallelizable, consumes low memory,\noutperforms SVD in these categories and works also extremely well on large\nsparse matrices. In contrast to existing methods, the randomized LU\ndecomposition constructs an under-complete dictionary, which simplifies both\nthe construction and the classification processes of newly arrived signals. The\ndictionary construction is generic and general that fits different\napplications. We demonstrate the capabilities of this algorithm for file type\nidentification, which is a fundamental task in digital security arena,\nperformed nowadays for example by sandboxing mechanism, deep packet inspection,\nfirewalls and anti-virus systems. We propose a content-based method that\ndetects file types that neither depend on file extension nor on metadata. Such\napproach is harder to deceive and we show that only a few file fragments from a\nwhole file are needed for a successful classification. Based on the constructed\ndictionaries, we show that the proposed method can effectively identify\nexecution code fragments in PDF files.\n  $\\textbf{Keywords.}$ Dictionary construction, classification, LU\ndecomposition, randomized LU decomposition, content-based file detection,\ncomputer security.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 08:18:00 GMT"}, {"version": "v2", "created": "Sat, 27 Jan 2018 05:59:28 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Rotbart", "Aviv", ""], ["Shabat", "Gil", ""], ["Shmueli", "Yaniv", ""], ["Averbuch", "Amir", ""]]}, {"id": "1502.04837", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "Nonparametric Nearest Neighbor Descent Clustering based on Delaunay\n  Triangulation", "comments": "7 pages; 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In our physically inspired in-tree (IT) based clustering algorithm and the\nseries after it, there is only one free parameter involved in computing the\npotential value of each point. In this work, based on the Delaunay\nTriangulation or its dual Voronoi tessellation, we propose a nonparametric\nprocess to compute potential values by the local information. This computation,\nthough nonparametric, is relatively very rough, and consequently, many local\nextreme points will be generated. However, unlike those gradient-based methods,\nour IT-based methods are generally insensitive to those local extremes. This\npositively demonstrates the superiority of these parametric (previous) and\nnonparametric (in this work) IT-based methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 09:27:03 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 12:17:46 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1502.04981", "submitter": "Mete Ozay", "authors": "Mete Ozay", "title": "Semi-supervised Segmentation Fusion of Multi-spectral and Aerial Images", "comments": "A version of the manuscript was published in ICPR 2014", "journal-ref": "Proc. 22nd International Conference on Pattern Recognition, pp.\n  3839-3844, Stockholm, 2014", "doi": "10.1109/ICPR.2014.659", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Semi-supervised Segmentation Fusion algorithm is proposed using consensus\nand distributed learning. The aim of Unsupervised Segmentation Fusion (USF) is\nto achieve a consensus among different segmentation outputs obtained from\ndifferent segmentation algorithms by computing an approximate solution to the\nNP problem with less computational complexity. Semi-supervision is incorporated\nin USF using a new algorithm called Semi-supervised Segmentation Fusion (SSSF).\nIn SSSF, side information about the co-occurrence of pixels in the same or\ndifferent segments is formulated as the constraints of a convex optimization\nproblem. The results of the experiments employed on artificial and real-world\nbenchmark multi-spectral and aerial images show that the proposed algorithms\nperform better than the individual state-of-the art segmentation algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 18:01:13 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 00:11:44 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Ozay", "Mete", ""]]}, {"id": "1502.04983", "submitter": "Thanapong Intharah", "authors": "Thanapong Intharah and Gabriel J. Brostow", "title": "Context Tricks for Cheap Semantic Segmentation", "comments": "Supplementary material can be found at\n  http://www0.cs.ucl.ac.uk/staff/T.Intharah/research.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate semantic labeling of image pixels is difficult because intra-class\nvariability is often greater than inter-class variability. In turn, fast\nsemantic segmentation is hard because accurate models are usually too\ncomplicated to also run quickly at test-time. Our experience with building and\nrunning semantic segmentation systems has also shown a reasonably obvious\nbottleneck on model complexity, imposed by small training datasets. We\ntherefore propose two simple complementary strategies that leverage context to\ngive better semantic segmentation, while scaling up or down to train on\ndifferent-sized datasets.\n  As easy modifications for existing semantic segmentation algorithms, we\nintroduce Decorrelated Semantic Texton Forests, and the Context Sensitive Image\nLevel Prior. The proposed modifications are tested using a Semantic Texton\nForest (STF) system, and the modifications are validated on two standard\nbenchmark datasets, MSRC-21 and PascalVOC-2010. In Python based comparisons,\nour system is insignificantly slower than STF at test-time, yet produces\nsuperior semantic segmentations overall, with just push-button training.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 18:08:53 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Intharah", "Thanapong", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "1502.05082", "submitter": "Jan Hosang", "authors": "Jan Hosang and Rodrigo Benenson and Piotr Doll\\'ar and Bernt Schiele", "title": "What makes for effective detection proposals?", "comments": "TPAMI final version, duplicate proposals removed in experiments", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2465908", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current top performing object detectors employ detection proposals to guide\nthe search for objects, thereby avoiding exhaustive sliding window search\nacross images. Despite the popularity and widespread use of detection\nproposals, it is unclear which trade-offs are made when using them during\nobject detection. We provide an in-depth analysis of twelve proposal methods\nalong with four baselines regarding proposal repeatability, ground truth\nannotation recall on PASCAL, ImageNet, and MS COCO, and their impact on DPM,\nR-CNN, and Fast R-CNN detection performance. Our analysis shows that for object\ndetection improving proposal localisation accuracy is as important as improving\nrecall. We introduce a novel metric, the average recall (AR), which rewards\nboth high recall and good localisation and correlates surprisingly well with\ndetection performance. Our findings show common strengths and weaknesses of\nexisting methods, and provide insights and metrics for selecting and tuning\nproposal methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 22:45:14 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2015 09:42:58 GMT"}, {"version": "v3", "created": "Sat, 1 Aug 2015 16:33:25 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Hosang", "Jan", ""], ["Benenson", "Rodrigo", ""], ["Doll\u00e1r", "Piotr", ""], ["Schiele", "Bernt", ""]]}, {"id": "1502.05137", "submitter": "Andreas Bulling", "authors": "Hosnieh Sattar, Sabine M\\\"uller, Mario Fritz, Andreas Bulling", "title": "Prediction of Search Targets From Fixations in Open-World Settings", "comments": null, "journal-ref": null, "doi": "10.1109/CVPR.2015.7298700", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work on predicting the target of visual search from human fixations\nonly considered closed-world settings in which training labels are available\nand predictions are performed for a known set of potential targets. In this\nwork we go beyond the state of the art by studying search target prediction in\nan open-world setting in which we no longer assume that we have fixation data\nto train for the search targets. We present a dataset containing fixation data\nof 18 users searching for natural images from three image categories within\nsynthesised image collages of about 80 images. In a closed-world baseline\nexperiment we show that we can predict the correct target image out of a\ncandidate set of five images. We then present a new problem formulation for\nsearch target prediction in the open-world setting that is based on learning\ncompatibilities between fixations and potential targets.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 07:04:04 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 09:26:03 GMT"}, {"version": "v3", "created": "Sat, 11 Apr 2015 14:56:51 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Sattar", "Hosnieh", ""], ["M\u00fcller", "Sabine", ""], ["Fritz", "Mario", ""], ["Bulling", "Andreas", ""]]}, {"id": "1502.05197", "submitter": "Silvia Tozza", "authors": "Silvia Tozza and Maurizio Falcone", "title": "Analysis and approximation of some Shape-from-Shading models for\n  non-Lambertian surfaces", "comments": "Accepted version to Journal of Mathematical Imaging and Vision, 57\n  pages", "journal-ref": null, "doi": null, "report-no": "Roma01.Math.NA", "categories": "math.NA cs.CV cs.NA math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reconstruction of a 3D object or a scene is a classical inverse problem\nin Computer Vision. In the case of a single image this is called the\nShape-from-Shading (SfS) problem and it is known to be ill-posed even in a\nsimplified version like the vertical light source case. A huge number of works\ndeals with the orthographic SfS problem based on the Lambertian reflectance\nmodel, the most common and simplest model which leads to an eikonal type\nequation when the light source is on the vertical axis. In this paper we want\nto study non-Lambertian models since they are more realistic and suitable\nwhenever one has to deal with different kind of surfaces, rough or specular. We\nwill present a unified mathematical formulation of some popular orthographic\nnon-Lambertian models, considering vertical and oblique light directions as\nwell as different viewer positions. These models lead to more complex\nstationary nonlinear partial differential equations of Hamilton-Jacobi type\nwhich can be regarded as the generalization of the classical eikonal equation\ncorresponding to the Lambertian case. However, all the equations corresponding\nto the models considered here (Oren-Nayar and Phong) have a similar structure\nso we can look for weak solutions to this class in the viscosity solution\nframework. Via this unified approach, we are able to develop a semi-Lagrangian\napproximation scheme for the Oren-Nayar and the Phong model and to prove a\ngeneral convergence result. Numerical simulations on synthetic and real images\nwill illustrate the effectiveness of this approach and the main features of the\nscheme, also comparing the results with previous results in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 12:24:41 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 18:49:24 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Tozza", "Silvia", ""], ["Falcone", "Maurizio", ""]]}, {"id": "1502.05212", "submitter": "Gianluigi Cioca", "authors": "Gianluigi Ciocca, Paolo Napoletano, Raimondo Schettini", "title": "IAT - Image Annotation Tool: Manual", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The annotation of image and video data of large datasets is a fundamental\ntask in multimedia information retrieval and computer vision applications. In\norder to support the users during the image and video annotation process,\nseveral software tools have been developed to provide them with a graphical\nenvironment which helps drawing object contours, handling tracking information\nand specifying object metadata. Here we introduce a preliminary version of the\nimage annotation tools developed at the Imaging and Vision Laboratory.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 13:11:46 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Ciocca", "Gianluigi", ""], ["Napoletano", "Paolo", ""], ["Schettini", "Raimondo", ""]]}, {"id": "1502.05224", "submitter": "Yun Gu", "authors": "Yun Gu, Haoyang Xue, Jie Yang", "title": "Cross-Modality Hashing with Partial Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a hashing function for cross-media search is very desirable due to\nits low storage cost and fast query speed. However, the data crawled from\nInternet cannot always guarantee good correspondence among different modalities\nwhich affects the learning for hashing function. In this paper, we focus on\ncross-modal hashing with partially corresponded data. The data without full\ncorrespondence are made in use to enhance the hashing performance. The\nexperiments on Wiki and NUS-WIDE datasets demonstrates that the proposed method\noutperforms some state-of-the-art hashing approaches with fewer correspondence\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 13:41:23 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2015 12:13:47 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Gu", "Yun", ""], ["Xue", "Haoyang", ""], ["Yang", "Jie", ""]]}, {"id": "1502.05241", "submitter": "Michael Dirnberger", "authors": "Michael Dirnberger and Adrian Neumann and Tim Kehl", "title": "NEFI: Network Extraction From Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks and network-like structures are amongst the central building blocks\nof many technological and biological systems. Given a mathematical graph\nrepresentation of a network, methods from graph theory enable a precise\ninvestigation of its properties. Software for the analysis of graphs is widely\navailable and has been applied to graphs describing large scale networks such\nas social networks, protein-interaction networks, etc. In these applications,\ngraph acquisition, i.e., the extraction of a mathematical graph from a network,\nis relatively simple. However, for many network-like structures, e.g. leaf\nvenations, slime molds and mud cracks, data collection relies on images where\ngraph extraction requires domain-specific solutions or even manual. Here we\nintroduce Network Extraction From Images, NEFI, a software tool that\nautomatically extracts accurate graphs from images of a wide range of networks\noriginating in various domains. While there is previous work on graph\nextraction from images, theoretical results are fully accessible only to an\nexpert audience and ready-to-use implementations for non-experts are rarely\navailable or insufficiently documented. NEFI provides a novel platform allowing\npractitioners from many disciplines to easily extract graph representations\nfrom images by supplying flexible tools from image processing, computer vision\nand graph theory bundled in a convenient package. Thus, NEFI constitutes a\nscalable alternative to tedious and error-prone manual graph extraction and\nspecial purpose tools. We anticipate NEFI to enable the collection of larger\ndatasets by reducing the time spent on graph extraction. The analysis of these\nnew datasets may open up the possibility to gain new insights into the\nstructure and function of various types of networks. NEFI is open source and\navailable http://nefi.mpi-inf.mpg.de.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 14:20:25 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Dirnberger", "Michael", ""], ["Neumann", "Adrian", ""], ["Kehl", "Tim", ""]]}, {"id": "1502.05243", "submitter": "Shanmuganathan Raman", "authors": "Aalok Gangopadhyay, Shivam Mani Tripathi, Ishan Jindal, Shanmuganathan\n  Raman", "title": "SA-CNN: Dynamic Scene Classification using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of classifying videos of natural dynamic scenes into appropriate\nclasses has gained lot of attention in recent years. The problem especially\nbecomes challenging when the camera used to capture the video is dynamic. In\nthis paper, we analyse the performance of statistical aggregation (SA)\ntechniques on various pre-trained convolutional neural network(CNN) models to\naddress this problem. The proposed approach works by extracting CNN activation\nfeatures for a number of frames in a video and then uses an aggregation scheme\nin order to obtain a robust feature descriptor for the video. We show through\nresults that the proposed approach performs better than the-state-of-the arts\nfor the Maryland and YUPenn dataset. The final descriptor obtained is powerful\nenough to distinguish among dynamic scenes and is even capable of addressing\nthe scenario where the camera motion is dominant and the scene dynamics are\ncomplex. Further, this paper shows an extensive study on the performance of\nvarious aggregation methods and their combinations. We compare the proposed\napproach with other dynamic scene classification algorithms on two publicly\navailable datasets - Maryland and YUPenn to demonstrate the superior\nperformance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 12:25:27 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2015 06:01:02 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Gangopadhyay", "Aalok", ""], ["Tripathi", "Shivam Mani", ""], ["Jindal", "Ishan", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1502.05435", "submitter": "Mete Ozay", "authors": "Mete Ozay, Fatos T. Yarman Vural, Sanjeev R. Kulkarni, H. Vincent Poor", "title": "Fusion of Image Segmentation Algorithms using Consensus Clustering", "comments": "A version of the manuscript was published in ICIP 2013", "journal-ref": "20th IEEE International Conference on Image Processing (ICIP), pp.\n  4049-4053, Melbourne, VIC, 15-18 Sept. 2013", "doi": "10.1109/ICIP.2013.6738834", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new segmentation fusion method is proposed that ensembles the output of\nseveral segmentation algorithms applied on a remotely sensed image. The\ncandidate segmentation sets are processed to achieve a consensus segmentation\nusing a stochastic optimization algorithm based on the Filtered Stochastic BOEM\n(Best One Element Move) method. For this purpose, Filtered Stochastic BOEM is\nreformulated as a segmentation fusion problem by designing a new distance\nlearning approach. The proposed algorithm also embeds the computation of the\noptimum number of clusters into the segmentation fusion problem.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 22:53:22 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Ozay", "Mete", ""], ["Vural", "Fatos T. Yarman", ""], ["Kulkarni", "Sanjeev R.", ""], ["Poor", "H. Vincent", ""]]}, {"id": "1502.05461", "submitter": "Carl Vondrick", "authors": "Carl Vondrick, Aditya Khosla, Hamed Pirsiavash, Tomasz Malisiewicz,\n  Antonio Torralba", "title": "Visualizing Object Detection Features", "comments": "In submission to IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce algorithms to visualize feature spaces used by object detectors.\nOur method works by inverting a visual feature back to multiple natural images.\nWe found that these visualizations allow us to analyze object detection systems\nin new ways and gain new insight into the detector's failures. For example,\nwhen we visualize the features for high scoring false alarms, we discovered\nthat, although they are clearly wrong in image space, they do look deceptively\nsimilar to true positives in feature space. This result suggests that many of\nthese false alarms are caused by our choice of feature space, and supports that\ncreating a better learning algorithm or building bigger datasets is unlikely to\ncorrect these errors. By visualizing feature spaces, we can gain a more\nintuitive understanding of recognition systems.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 04:11:14 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Vondrick", "Carl", ""], ["Khosla", "Aditya", ""], ["Pirsiavash", "Hamed", ""], ["Malisiewicz", "Tomasz", ""], ["Torralba", "Antonio", ""]]}, {"id": "1502.05565", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu", "title": "Multi-valued Color Representation Based on Frank t-norm Properties", "comments": "12th International Conference Information Processing and Management\n  of Uncertainty for Knowledge-Based Systems, IPMU'2008, pp. 1215-1222, June\n  22-27, 2008, Malaga, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper two knowledge representation models are proposed, FP4 and FP6.\nBoth combine ideas from fuzzy sets and four-valued and hexa-valued logics. Both\nrepresent imprecise properties whose accomplished degree is unknown or\ncontradictory for some objects. A possible application in the color analysis\nand color image processing is discussed.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 13:35:06 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Patrascu", "Vasile", ""]]}, {"id": "1502.05678", "submitter": "Clint Solomon Mathialagan", "authors": "Clint Solomon Mathialagan, Andrew C. Gallagher and Dhruv Batra", "title": "VIP: Finding Important People in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People preserve memories of events such as birthdays, weddings, or vacations\nby capturing photos, often depicting groups of people. Invariably, some\nindividuals in the image are more important than others given the context of\nthe event. This paper analyzes the concept of the importance of individuals in\ngroup photographs. We address two specific questions -- Given an image, who are\nthe most important individuals in it? Given multiple images of a person, which\nimage depicts the person in the most important role? We introduce a measure of\nimportance of people in images and investigate the correlation between\nimportance and visual saliency. We find that not only can we automatically\npredict the importance of people from purely visual cues, incorporating this\npredicted importance results in significant improvement in applications such as\nim2text (generating sentences that describe images of groups of people).\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 19:47:27 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2015 02:49:11 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Mathialagan", "Clint Solomon", ""], ["Gallagher", "Andrew C.", ""], ["Batra", "Dhruv", ""]]}, {"id": "1502.05689", "submitter": "Ming-Yu Liu", "authors": "Ming-Yu Liu, Arun Mallya, Oncel C. Tuzel, Xi Chen", "title": "Unsupervised Network Pretraining via Encoding Human Design", "comments": "9 pages, 11 figures, WACV 2016: IEEE Conference on Applications of\n  Computer Vision", "journal-ref": null, "doi": "10.1109/WACV.2016.7477698", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, computer vision researchers have spent an immense amount of\neffort on designing image features for the visual object recognition task. We\npropose to incorporate this valuable experience to guide the task of training\ndeep neural networks. Our idea is to pretrain the network through the task of\nreplicating the process of hand-designed feature extraction. By learning to\nreplicate the process, the neural network integrates previous research\nknowledge and learns to model visual objects in a way similar to the\nhand-designed features. In the succeeding finetuning step, it further learns\nobject-specific representations from labeled data and this boosts its\nclassification power. We pretrain two convolutional neural networks where one\nreplicates the process of histogram of oriented gradients feature extraction,\nand the other replicates the process of region covariance feature extraction.\nAfter finetuning, we achieve substantially better performance than the baseline\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 20:26:35 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2016 23:49:28 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Liu", "Ming-Yu", ""], ["Mallya", "Arun", ""], ["Tuzel", "Oncel C.", ""], ["Chen", "Xi", ""]]}, {"id": "1502.05742", "submitter": "Ahmadreza Baghaie", "authors": "Ahmadreza Baghaie, Roshan M. D'souza, Zeyun Yu", "title": "Application of Independent Component Analysis Techniques in Speckle\n  Noise Reduction of Retinal OCT Images", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijleo.2016.03.078", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Coherence Tomography (OCT) is an emerging technique in the field of\nbiomedical imaging, with applications in ophthalmology, dermatology, coronary\nimaging etc. OCT images usually suffer from a granular pattern, called speckle\nnoise, which restricts the process of interpretation. Therefore the need for\nspeckle noise reduction techniques is of high importance. To the best of our\nknowledge, use of Independent Component Analysis (ICA) techniques has never\nbeen explored for speckle reduction of OCT images. Here, a comparative study of\nseveral ICA techniques (InfoMax, JADE, FastICA and SOBI) is provided for noise\nreduction of retinal OCT images. Having multiple B-scans of the same location,\nthe eye movements are compensated using a rigid registration technique. Then,\ndifferent ICA techniques are applied to the aggregated set of B-scans for\nextracting the noise-free image. Signal-to-Noise-Ratio (SNR),\nContrast-to-Noise-Ratio (CNR) and Equivalent-Number-of-Looks (ENL), as well as\nanalysis on the computational complexity of the methods, are considered as\nmetrics for comparison. The results show that use of ICA can be beneficial,\nespecially in case of having fewer number of B-scans.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 22:49:37 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 00:33:56 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2015 15:31:04 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Baghaie", "Ahmadreza", ""], ["D'souza", "Roshan M.", ""], ["Yu", "Zeyun", ""]]}, {"id": "1502.05752", "submitter": "Zhiwu Lu", "authors": "Zhenyong Fu and Zhiwu Lu", "title": "Pairwise Constraint Propagation: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the most important types of (weaker) supervised information in\nmachine learning and pattern recognition, pairwise constraint, which specifies\nwhether a pair of data points occur together, has recently received significant\nattention, especially the problem of pairwise constraint propagation. At least\ntwo reasons account for this trend: the first is that compared to the data\nlabel, pairwise constraints are more general and easily to collect, and the\nsecond is that since the available pairwise constraints are usually limited,\nthe constraint propagation problem is thus important.\n  This paper provides an up-to-date critical survey of pairwise constraint\npropagation research. There are two underlying motivations for us to write this\nsurvey paper: the first is to provide an up-to-date review of the existing\nliterature, and the second is to offer some insights into the studies of\npairwise constraint propagation. To provide a comprehensive survey, we not only\ncategorize existing propagation techniques but also present detailed\ndescriptions of representative methods within each category.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 23:59:48 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Fu", "Zhenyong", ""], ["Lu", "Zhiwu", ""]]}, {"id": "1502.05803", "submitter": "Luka \\v{C}ehovin", "authors": "Luka \\v{C}ehovin, Ale\\v{s} Leonardis, Matej Kristan", "title": "Visual object tracking performance measures revisited", "comments": null, "journal-ref": "IEEE Transactions on Image Processing (March 2016), 1261 - 1274", "doi": "10.1109/TIP.2016.2520370", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of visual tracking evaluation is sporting a large variety of\nperformance measures, and largely suffers from lack of consensus about which\nmeasures should be used in experiments. This makes the cross-paper tracker\ncomparison difficult. Furthermore, as some measures may be less effective than\nothers, the tracking results may be skewed or biased towards particular\ntracking aspects. In this paper we revisit the popular performance measures and\ntracker performance visualizations and analyze them theoretically and\nexperimentally. We show that several measures are equivalent from the point of\ninformation they provide for tracker comparison and, crucially, that some are\nmore brittle than the others. Based on our analysis we narrow down the set of\npotential measures to only two complementary ones, describing accuracy and\nrobustness, thus pushing towards homogenization of the tracker evaluation\nmethodology. These two measures can be intuitively interpreted and visualized\nand have been employed by the recent Visual Object Tracking (VOT) challenges as\nthe foundation for the evaluation methodology.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 08:57:31 GMT"}, {"version": "v2", "created": "Fri, 15 May 2015 12:33:50 GMT"}, {"version": "v3", "created": "Mon, 7 Mar 2016 14:29:56 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["\u010cehovin", "Luka", ""], ["Leonardis", "Ale\u0161", ""], ["Kristan", "Matej", ""]]}, {"id": "1502.05840", "submitter": "Junchi Yan", "authors": "Junchi Yan, Minsu Cho, Hongyuan Zha, Xiaokang Yang, Stephen Chu", "title": "A General Multi-Graph Matching Approach via Graduated\n  Consistency-regularized Boosting", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  38(6) 2016, page 1228 - 1242", "doi": "10.1109/TPAMI.2015.2477832", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of matching $N$ weighted graphs referring to\nan identical object or category. More specifically, matching the common node\ncorrespondences among graphs. This multi-graph matching problem involves two\ningredients affecting the overall accuracy: i) the local pairwise matching\naffinity score among graphs; ii) the global matching consistency that measures\nthe uniqueness of the pairwise matching results by different chaining orders.\nPrevious studies typically either enforce the matching consistency constraints\nin the beginning of iterative optimization, which may propagate matching error\nboth over iterations and across graph pairs; or separate affinity optimizing\nand consistency regularization in two steps. This paper is motivated by the\nobservation that matching consistency can serve as a regularizer in the\naffinity objective function when the function is biased due to noises or\ninappropriate modeling. We propose multi-graph matching methods to incorporate\nthe two aspects by boosting the affinity score, meanwhile gradually infusing\nthe consistency as a regularizer. Furthermore, we propose a node-wise\nconsistency/affinity-driven mechanism to elicit the common inlier nodes out of\nthe irrelevant outliers. Extensive results on both synthetic and public image\ndatasets demonstrate the competency of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 11:45:25 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Yan", "Junchi", ""], ["Cho", "Minsu", ""], ["Zha", "Hongyuan", ""], ["Yang", "Xiaokang", ""], ["Chu", "Stephen", ""]]}, {"id": "1502.05908", "submitter": "Paul Wohlhart", "authors": "Paul Wohlhart, Vincent Lepetit", "title": "Learning Descriptors for Object Recognition and 3D Pose Estimation", "comments": "CVPR 2015", "journal-ref": null, "doi": "10.1109/CVPR.2015.7298930", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting poorly textured objects and estimating their 3D pose reliably is\nstill a very challenging problem. We introduce a simple but powerful approach\nto computing descriptors for object views that efficiently capture both the\nobject identity and 3D pose. By contrast with previous manifold-based\napproaches, we can rely on the Euclidean distance to evaluate the similarity\nbetween descriptors, and therefore use scalable Nearest Neighbor search methods\nto efficiently handle a large number of objects under a large range of poses.\nTo achieve this, we train a Convolutional Neural Network to compute these\ndescriptors by enforcing simple similarity and dissimilarity constraints\nbetween the descriptors. We show that our constraints nicely untangle the\nimages from different objects and different views into clusters that are not\nonly well-separated but also structured as the corresponding sets of poses: The\nEuclidean distance between descriptors is large when the descriptors are from\ndifferent objects, and directly related to the distance between the poses when\nthe descriptors are from the same object. These important properties allow us\nto outperform state-of-the-art object views representations on challenging RGB\nand RGB-D data.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 15:39:42 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 13:53:07 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Wohlhart", "Paul", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1502.05928", "submitter": "Mehrdad Gangeh", "authors": "Mehrdad J. Gangeh, Ahmed K. Farahat, Ali Ghodsi, Mohamed S. Kamel", "title": "Supervised Dictionary Learning and Sparse Representation-A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning and sparse representation (DLSR) is a recent and\nsuccessful mathematical model for data representation that achieves\nstate-of-the-art performance in various fields such as pattern recognition,\nmachine learning, computer vision, and medical imaging. The original\nformulation for DLSR is based on the minimization of the reconstruction error\nbetween the original signal and its sparse representation in the space of the\nlearned dictionary. Although this formulation is optimal for solving problems\nsuch as denoising, inpainting, and coding, it may not lead to optimal solution\nin classification tasks, where the ultimate goal is to make the learned\ndictionary and corresponding sparse representation as discriminative as\npossible. This motivated the emergence of a new category of techniques, which\nis appropriately called supervised dictionary learning and sparse\nrepresentation (S-DLSR), leading to more optimal dictionary and sparse\nrepresentation in classification tasks. Despite many research efforts for\nS-DLSR, the literature lacks a comprehensive view of these techniques, their\nconnections, advantages and shortcomings. In this paper, we address this gap\nand provide a review of the recently proposed algorithms for S-DLSR. We first\npresent a taxonomy of these algorithms into six categories based on the\napproach taken to include label information into the learning of the dictionary\nand/or sparse representation. For each category, we draw connections between\nthe algorithms in this category and present a unified framework for them. We\nthen provide guidelines for applied researchers on how to represent and learn\nthe building blocks of an S-DLSR solution based on the problem at hand. This\nreview provides a broad, yet deep, view of the state-of-the-art methods for\nS-DLSR and allows for the advancement of research and development in this\nemerging area of research.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 16:51:19 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Gangeh", "Mehrdad J.", ""], ["Farahat", "Ahmed K.", ""], ["Ghodsi", "Ali", ""], ["Kamel", "Mohamed S.", ""]]}, {"id": "1502.05957", "submitter": "Paul Vitanyi", "authors": "Andrew R. Cohen (Dept Electri. Comput. Eng., Drexel Univ.), Paul M.B.\n  Vitanyi (CWI and University of Amsterdam)", "title": "Web Similarity in Sets of Search Terms using Database Queries", "comments": "LaTeX 18 pages, 3 tables. A precursor is arXiv:1308.3177", "journal-ref": "SN COMPUT. SCI. 1, 161(2020)", "doi": "10.1007/s42979-020-00148-5", "report-no": null, "categories": "cs.IR cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalized web distance (NWD) is a similarity or normalized semantic distance\nbased on the World Wide Web or another large electronic database, for instance\nWikipedia, and a search engine that returns reliable aggregate page counts. For\nsets of search terms the NWD gives a common similarity (common semantics) on a\nscale from 0 (identical) to 1 (completely different). The NWD approximates the\nsimilarity of members of a set according to all (upper semi)computable\nproperties. We develop the theory and give applications of classifying using\nAmazon, Wikipedia, and the NCBI website from the National Institutes of Health.\nThe last gives new correlations between health hazards. A restriction of the\nNWD to a set of two yields the earlier normalized google distance (NGD) but no\ncombination of the NGD's of pairs in a set can extract the information the NWD\nextracts from the set. The NWD enables a new contextual (different databases)\nlearning approachbased on Kolmogorov complexity theory that incorporates\nknowledge from these databases.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 17:55:58 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 16:27:48 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Cohen", "Andrew R.", "", "Dept Electri. Comput. Eng., Drexel Univ."], ["Vitanyi", "Paul M. B.", "", "CWI and University of Amsterdam"]]}, {"id": "1502.06073", "submitter": "Zengxi Huang", "authors": "Zengxi Huang, Yiguang Liu, Xiaoming Wang, Jinrong Hu", "title": "Study on Sparse Representation based Classification for Biometric\n  Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a multimodal verification system integrating face\nand ear based on sparse representation based classification (SRC). The face and\near query samples are first encoded separately to derive sparsity-based match\nscores, and which are then combined with sum-rule fusion for verification.\nApart from validating the encouraging performance of SRC-based multimodal\nverification, this paper also dedicates to provide a clear understanding about\nthe characteristics of SRC-based biometric verification. To this end, two\nsparsity-based metrics, i.e. spare coding error (SCE) and sparse contribution\nrate (SCR), are involved, together with face and ear unimodal SRC-based\nverification. As for the issue that SRC-based biometric verification may suffer\nfrom heavy computational burden and verification accuracy degradation with\nincrease of enrolled subjects, we argue that it could be properly resolved by\nexploiting small random dictionary for sparsity-based score computation, which\nconsists of training samples from a limited number of randomly selected\nsubjects. Experimental results demonstrate the superiority of SRC-based\nmultimodal verification compared to the state-of-the-art multimodal methods\nlike likelihood ratio (LLR), support vector machine (SVM), and the sum-rule\nfusion methods using cosine similarity, meanwhile the idea of using small\nrandom dictionary is feasible in both effectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 07:00:01 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 11:50:33 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Huang", "Zengxi", ""], ["Liu", "Yiguang", ""], ["Wang", "Xiaoming", ""], ["Hu", "Jinrong", ""]]}, {"id": "1502.06075", "submitter": "Weiyao Lin", "authors": "Weiyao Lin, Yuanzhe Chen, Jianxin Wu, Hanli Wang, Bin Sheng, Hongxiang\n  Li", "title": "A new network-based algorithm for human activity recognition in video", "comments": "This manuscript is the accepted version for TCSVT (IEEE Transactions\n  on Circuits and Systems for Video Technology)", "journal-ref": "IEEE Trans. Circuits and Systems for Video Technology, vol. 24,\n  no. 5, pp. 826-841, 2014", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new network-transmission-based (NTB) algorithm is proposed\nfor human activity recognition in videos. The proposed NTB algorithm models the\nentire scene as an error-free network. In this network, each node corresponds\nto a patch of the scene and each edge represents the activity correlation\nbetween the corresponding patches. Based on this network, we further model\npeople in the scene as packages while human activities can be modeled as the\nprocess of package transmission in the network. By analyzing these specific\n\"package transmission\" processes, various activities can be effectively\ndetected. The implementation of our NTB algorithm into abnormal activity\ndetection and group activity recognition are described in detail in the paper.\nExperimental results demonstrate the effectiveness of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 07:10:02 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Lin", "Weiyao", ""], ["Chen", "Yuanzhe", ""], ["Wu", "Jianxin", ""], ["Wang", "Hanli", ""], ["Sheng", "Bin", ""], ["Li", "Hongxiang", ""]]}, {"id": "1502.06076", "submitter": "Weiyao Lin", "authors": "Weiyao Lin, Hang Chu, Jianxin Wu, Bin Sheng, Zhenzhong Chen", "title": "A Heat-Map-based Algorithm for Recognizing Group Activities in Videos", "comments": "This manuscript is the accepted version for TCSVT(IEEE Transactions\n  on Circuits and Systems for Video Technology)", "journal-ref": "IEEE Trans. Circuits and Systems for Video Technology, vol. 23,\n  no. 11, pp. 1980-1992, 2013", "doi": "10.1109/TCSVT.2013.2269780", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new heat-map-based (HMB) algorithm is proposed for group\nactivity recognition. The proposed algorithm first models human trajectories as\nseries of \"heat sources\" and then applies a thermal diffusion process to create\na heat map (HM) for representing the group activities. Based on this heat map,\na new key-point based (KPB) method is used for handling the alignments among\nheat maps with different scales and rotations. And a surface-fitting (SF)\nmethod is also proposed for recognizing group activities. Our proposed HM\nfeature can efficiently embed the temporal motion information of the group\nactivities while the proposed KPB and SF methods can effectively utilize the\ncharacteristics of the heat map for activity recognition. Experimental results\ndemonstrate the effectiveness of our proposed algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 07:22:07 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Lin", "Weiyao", ""], ["Chu", "Hang", ""], ["Wu", "Jianxin", ""], ["Sheng", "Bin", ""], ["Chen", "Zhenzhong", ""]]}, {"id": "1502.06080", "submitter": "Weiyao Lin", "authors": "Yuanzhe Chen, Weiyao Lin, Chongyang Zhang, Zhenzhong Chen, Ning Xu,\n  Jun Xie", "title": "Intra-and-Inter-Constraint-based Video Enhancement based on Piecewise\n  Tone Mapping", "comments": "This manuscript is the accepted version for TCSVT (IEEE Transactions\n  on Circuits and Systems for Video Technology)", "journal-ref": "IEEE Trans. Circuits and Systems for Video Technology, vol. 23,\n  no. 1, pp. 74-82, 2013", "doi": "10.1109/TCSVT.2012.2203198", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video enhancement plays an important role in various video applications. In\nthis paper, we propose a new intra-and-inter-constraint-based video enhancement\napproach aiming to 1) achieve high intra-frame quality of the entire picture\nwhere multiple region-of-interests (ROIs) can be adaptively and simultaneously\nenhanced, and 2) guarantee the inter-frame quality consistencies among video\nframes. We first analyze features from different ROIs and create a piecewise\ntone mapping curve for the entire frame such that the intra-frame quality of a\nframe can be enhanced. We further introduce new inter-frame constraints to\nimprove the temporal quality consistency. Experimental results show that the\nproposed algorithm obviously outperforms the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 07:36:26 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Chen", "Yuanzhe", ""], ["Lin", "Weiyao", ""], ["Zhang", "Chongyang", ""], ["Chen", "Zhenzhong", ""], ["Xu", "Ning", ""], ["Xie", "Jun", ""]]}, {"id": "1502.06081", "submitter": "Radu Arsinte", "authors": "Radu Arsinte", "title": "Study of a Robust Algorithm Applied in the Optimal Position Tuning for\n  the Camera Lens in Automated Visual Inspection Systems", "comments": "5 pages, 2 figures", "journal-ref": "Proceedings of Fifth International Conference on Pattern\n  Recognition and Information Processing - PRIP'99 - May 18-20, 1999 Minsk,\n  Belarus - pag.237-242 - ISBN 83-87362-16-6", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper present the mathematical fundaments and experimental study of an\nalgorithm used to find the optimal position for the camera lens to obtain a\nmaximum of details. This information can be further applied to a appropriate\nsystem to automatically correct this position. The algorithm is based on the\nevaluation of a so called resolution function who calculates the maximum of\ngradient in a certain zone of the image. The paper also presents alternative\nforms of the function, results of measurements and set up a set of practical\nrules for the right application of the algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 07:40:18 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Arsinte", "Radu", ""]]}, {"id": "1502.06105", "submitter": "Taehoon Lee", "authors": "Taehoon Lee, Taesup Moon, Seung Jean Kim, Sungroh Yoon", "title": "Regularization and Kernelization of the Maximin Correlation Approach", "comments": "Submitted to IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2016.2551727", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust classification becomes challenging when each class consists of\nmultiple subclasses. Examples include multi-font optical character recognition\nand automated protein function prediction. In correlation-based\nnearest-neighbor classification, the maximin correlation approach (MCA)\nprovides the worst-case optimal solution by minimizing the maximum\nmisclassification risk through an iterative procedure. Despite the optimality,\nthe original MCA has drawbacks that have limited its wide applicability in\npractice. That is, the MCA tends to be sensitive to outliers, cannot\neffectively handle nonlinearities in datasets, and suffers from having high\ncomputational complexity. To address these limitations, we propose an improved\nsolution, named regularized maximin correlation approach (R-MCA). We first\nreformulate MCA as a quadratically constrained linear programming (QCLP)\nproblem, incorporate regularization by introducing slack variables in the\nprimal problem of the QCLP, and derive the corresponding Lagrangian dual. The\ndual formulation enables us to apply the kernel trick to R-MCA so that it can\nbetter handle nonlinearities. Our experimental results demonstrate that the\nregularization and kernelization make the proposed R-MCA more robust and\naccurate for various classification tasks than the original MCA. Furthermore,\nwhen the data size or dimensionality grows, R-MCA runs substantially faster by\nsolving either the primal or dual (whichever has a smaller variable dimension)\nof the QCLP.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 14:37:44 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 04:42:12 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Lee", "Taehoon", ""], ["Moon", "Taesup", ""], ["Kim", "Seung Jean", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1502.06108", "submitter": "Xiao Lin", "authors": "Xiao Lin, Devi Parikh", "title": "Don't Just Listen, Use Your Imagination: Leveraging Visual Common Sense\n  for Non-Visual Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial agents today can answer factual questions. But they fall short on\nquestions that require common sense reasoning. Perhaps this is because most\nexisting common sense databases rely on text to learn and represent knowledge.\nBut much of common sense knowledge is unwritten - partly because it tends not\nto be interesting enough to talk about, and partly because some common sense is\nunnatural to articulate in text. While unwritten, it is not unseen. In this\npaper we leverage semantic common sense knowledge learned from images - i.e.\nvisual common sense - in two textual tasks: fill-in-the-blank and visual\nparaphrasing. We propose to \"imagine\" the scene behind the text, and leverage\nvisual cues from the \"imagined\" scenes in addition to textual cues while\nanswering these questions. We imagine the scenes as a visual abstraction. Our\napproach outperforms a strong text-only baseline on these tasks. Our proposed\ntasks can serve as benchmarks to quantitatively evaluate progress in solving\ntasks that go \"beyond recognition\". Our code and datasets are publicly\navailable.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 15:25:40 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 18:54:05 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2015 03:04:19 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Lin", "Xiao", ""], ["Parikh", "Devi", ""]]}, {"id": "1502.06219", "submitter": "Smitha M.L.", "authors": "B.H. Shekar, Smitha M.L.", "title": "Video Text Localization with an emphasis on Edge Features", "comments": "8 pages, Eighth International Conference on Image and Signal\n  Processing, Elsevier Publications, ISBN: 9789351072522, pp: 324-330, held at\n  UVCE, Bangalore in July 2014. arXiv admin note: text overlap with\n  arXiv:1502.03913", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The text detection and localization plays a major role in video analysis and\nunderstanding. The scene text embedded in video consist of high-level semantics\nand hence contributes significantly to visual content analysis and retrieval.\nThis paper proposes a novel method to robustly localize the texts in natural\nscene images and videos based on sobel edge emphasizing approach. The input\nimage is preprocessed and edge emphasis is done to detect the text clusters.\nFurther, a set of rules have been devised using morphological operators for\nfalse positive elimination and connected component analysis is performed to\ndetect the text regions and hence text localization is performed. The\nexperimental results obtained on publicly available standard datasets\nillustrate that the proposed method can detect and localize the texts of\nvarious sizes, fonts and colors.\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 12:32:18 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Shekar", "B. H.", ""], ["L.", "Smitha M.", ""]]}, {"id": "1502.06220", "submitter": "Yaniv Romano", "authors": "Yaniv Romano and Michael Elad", "title": "Boosting of Image Denoising Algorithms", "comments": "33 pages, 9 figures, 3 tables, submitted to SIAM Journal on Imaging\n  Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a generic recursive algorithm for improving image\ndenoising methods. Given the initial denoised image, we suggest repeating the\nfollowing \"SOS\" procedure: (i) (S)trengthen the signal by adding the previous\ndenoised image to the degraded input image, (ii) (O)perate the denoising method\non the strengthened image, and (iii) (S)ubtract the previous denoised image\nfrom the restored signal-strengthened outcome. The convergence of this process\nis studied for the K-SVD image denoising and related algorithms. Still in the\ncontext of K-SVD image denoising, we introduce an interesting interpretation of\nthe SOS algorithm as a technique for closing the gap between the local\npatch-modeling and the global restoration task, thereby leading to improved\nperformance. In a quest for the theoretical origin of the SOS algorithm, we\nprovide a graph-based interpretation of our method, where the SOS recursive\nupdate effectively minimizes a penalty function that aims to denoise the image,\nwhile being regularized by the graph Laplacian. We demonstrate the SOS boosting\nalgorithm for several leading denoising methods (K-SVD, NLM, BM3D, and EPLL),\nshowing tendency to further improve denoising performance.\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 12:40:48 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2015 13:49:42 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Romano", "Yaniv", ""], ["Elad", "Michael", ""]]}, {"id": "1502.06235", "submitter": "Borislav Antic", "authors": "Borislav Anti\\'c and Bj\\\"orn Ommer", "title": "Spatio-temporal Video Parsing for Abnormality Detection", "comments": "15 pages, 12 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abnormality detection in video poses particular challenges due to the\ninfinite size of the class of all irregular objects and behaviors. Thus no (or\nby far not enough) abnormal training samples are available and we need to find\nabnormalities in test data without actually knowing what they are.\nNevertheless, the prevailing concept of the field is to directly search for\nindividual abnormal local patches or image regions independent of another. To\naddress this problem, we propose a method for joint detection of abnormalities\nin videos by spatio-temporal video parsing. The goal of video parsing is to\nfind a set of indispensable normal spatio-temporal object hypotheses that\njointly explain all the foreground of a video, while, at the same time, being\nsupported by normal training samples. Consequently, we avoid a direct detection\nof abnormalities and discover them indirectly as those hypotheses which are\nneeded for covering the foreground without finding an explanation for\nthemselves by normal samples. Abnormalities are localized by MAP inference in a\ngraphical model and we solve it efficiently by formulating it as a convex\noptimization problem. We experimentally evaluate our approach on several\nchallenging benchmark sets, improving over the state-of-the-art on all standard\nbenchmarks both in terms of abnormality classification and localization.\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 14:54:02 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Anti\u0107", "Borislav", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "1502.06236", "submitter": "P. Christopher Staecker", "authors": "P. Christopher Staecker", "title": "Some enumerations of binary digital images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CV math.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topology of digital images has been studied much in recent years, but no\nattempt has been made to exhaustively catalog the structure of binary images of\nsmall numbers of points. We produce enumerations of several classes of digital\nimages up to isomorphism and decide which among them are homotopy equivalent to\none another. Noting some patterns in the results, we make some conjectures\nabout digital images which are irreducible but not rigid.\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 15:15:51 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Staecker", "P. Christopher", ""]]}, {"id": "1502.06260", "submitter": "Xin Yuan", "authors": "Xin Yuan, Tsung-Han Tsai, Ruoyu Zhu, Patrick Llull, David Brady,\n  Lawrence Carin", "title": "Compressive Hyperspectral Imaging with Side Information", "comments": "20 pages, 21 figures. To appear in the IEEE Journal of Selected\n  Topics Signal Processing", "journal-ref": null, "doi": "10.1109/JSTSP.2015.2411575", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A blind compressive sensing algorithm is proposed to reconstruct\nhyperspectral images from spectrally-compressed measurements.The\nwavelength-dependent data are coded and then superposed, mapping the\nthree-dimensional hyperspectral datacube to a two-dimensional image. The\ninversion algorithm learns a dictionary {\\em in situ} from the measurements via\nglobal-local shrinkage priors. By using RGB images as side information of the\ncompressive sensing system, the proposed approach is extended to learn a\ncoupled dictionary from the joint dataset of the compressed measurements and\nthe corresponding RGB images, to improve reconstruction quality. A prototype\ncamera is built using a liquid-crystal-on-silicon modulator. Experimental\nreconstructions of hyperspectral datacubes from both simulated and real\ncompressed measurements demonstrate the efficacy of the proposed inversion\nalgorithm, the feasibility of the camera and the benefit of side information.\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 19:10:31 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Yuan", "Xin", ""], ["Tsai", "Tsung-Han", ""], ["Zhu", "Ruoyu", ""], ["Llull", "Patrick", ""], ["Brady", "David", ""], ["Carin", "Lawrence", ""]]}, {"id": "1502.06344", "submitter": "Erik Rodner", "authors": "Clemens-Alexander Brust, Sven Sickert, Marcel Simon, Erik Rodner,\n  Joachim Denzler", "title": "Convolutional Patch Networks with Spatial Prior for Road Detection and\n  Urban Scene Understanding", "comments": "VISAPP 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying single image patches is important in many different applications,\nsuch as road detection or scene understanding. In this paper, we present\nconvolutional patch networks, which are convolutional networks learned to\ndistinguish different image patches and which can be used for pixel-wise\nlabeling. We also show how to incorporate spatial information of the patch as\nan input to the network, which allows for learning spatial priors for certain\ncategories jointly with an appearance model. In particular, we focus on road\ndetection and urban scene understanding, two application areas where we are\nable to achieve state-of-the-art results on the KITTI as well as on the\nLabelMeFacade dataset.\n  Furthermore, our paper offers a guideline for people working in the area and\ndesperately wandering through all the painstaking details that render training\nCNs on image patches extremely difficult.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 08:47:37 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Brust", "Clemens-Alexander", ""], ["Sickert", "Sven", ""], ["Simon", "Marcel", ""], ["Rodner", "Erik", ""], ["Denzler", "Joachim", ""]]}, {"id": "1502.06464", "submitter": "Djork-Arn\\'e Clevert", "authors": "Djork-Arn\\'e Clevert, Andreas Mayr, Thomas Unterthiner, Sepp\n  Hochreiter", "title": "Rectified Factor Networks", "comments": "9 pages + 49 pages supplement", "journal-ref": "Advances in Neural Information Processing Systems 28 (NIPS 2015)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose rectified factor networks (RFNs) to efficiently construct very\nsparse, non-linear, high-dimensional representations of the input. RFN models\nidentify rare and small events in the input, have a low interference between\ncode units, have a small reconstruction error, and explain the data covariance\nstructure. RFN learning is a generalized alternating minimization algorithm\nderived from the posterior regularization method which enforces non-negative\nand normalized posterior means. We proof convergence and correctness of the RFN\nlearning algorithm. On benchmarks, RFNs are compared to other unsupervised\nmethods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to\nprevious sparse coding methods, RFNs yield sparser codes, capture the data's\ncovariance structure more precisely, and have a significantly smaller\nreconstruction error. We test RFNs as pretraining technique for deep networks\non different vision datasets, where RFNs were superior to RBMs and\nautoencoders. On gene expression data from two pharmaceutical drug discovery\nstudies, RFNs detected small and rare gene modules that revealed highly\nrelevant new biological insights which were so far missed by other unsupervised\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 15:44:37 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2015 21:27:53 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Clevert", "Djork-Arn\u00e9", ""], ["Mayr", "Andreas", ""], ["Unterthiner", "Thomas", ""], ["Hochreiter", "Sepp", ""]]}, {"id": "1502.06556", "submitter": "Amelia Carolina Sparavigna", "authors": "Amelia Carolina Sparavigna", "title": "Shannon, Tsallis and Kaniadakis entropies in bi-level image thresholding", "comments": "Keywords: Kaniadakis Entropy, Image Processing, Image Segmentation,\n  Image Thresholding, Texture Transitions", "journal-ref": "International Journal of Sciences 4(2), 35-43, 2015", "doi": "10.18483/ijSci.626", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum entropy principle is often used for bi-level or multi-level\nthresholding of images. For this purpose, some methods are available based on\nShannon and Tsallis entropies. In this paper, we discuss them and propose a\nmethod based on Kaniadakis entropy.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 19:11:12 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Sparavigna", "Amelia Carolina", ""]]}, {"id": "1502.06648", "submitter": "Marcus Rohrbach", "authors": "Marcus Rohrbach and Anna Rohrbach and Michaela Regneri and Sikandar\n  Amin and Mykhaylo Andriluka and Manfred Pinkal and Bernt Schiele", "title": "Recognizing Fine-Grained and Composite Activities using Hand-Centric\n  Features and Script Data", "comments": "in International Journal of Computer Vision (IJCV) 2015", "journal-ref": null, "doi": "10.1007/s11263-015-0851-8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity recognition has shown impressive progress in recent years. However,\nthe challenges of detecting fine-grained activities and understanding how they\nare combined into composite activities have been largely overlooked. In this\nwork we approach both tasks and present a dataset which provides detailed\nannotations to address them. The first challenge is to detect fine-grained\nactivities, which are defined by low inter-class variability and are typically\ncharacterized by fine-grained body motions. We explore how human pose and hands\ncan help to approach this challenge by comparing two pose-based and two\nhand-centric features with state-of-the-art holistic features. To attack the\nsecond challenge, recognizing composite activities, we leverage the fact that\nthese activities are compositional and that the essential components of the\nactivities can be obtained from textual descriptions or scripts. We show the\nbenefits of our hand-centric approach for fine-grained activity classification\nand detection. For composite activity recognition we find that decomposition\ninto attributes allows sharing information across composites and is essential\nto attack this hard task. Using script data we can recognize novel composites\nwithout having training data for them.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 22:48:17 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2015 16:02:19 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Rohrbach", "Marcus", ""], ["Rohrbach", "Anna", ""], ["Regneri", "Michaela", ""], ["Amin", "Sikandar", ""], ["Andriluka", "Mykhaylo", ""], ["Pinkal", "Manfred", ""], ["Schiele", "Bernt", ""]]}, {"id": "1502.06703", "submitter": "Smitha M.L.", "authors": "B.H. Shekar, Smitha M.L., P. Shivakumara", "title": "Discrete Wavelet Transform and Gradient Difference based approach for\n  text localization in videos", "comments": "Fifth International Conference on Signals and Image Processing, IEEE,\n  DOI 10.1109/ICSIP.2014.50, pp. 280-284, held at BNMIT, Bangalore in January\n  2014", "journal-ref": null, "doi": "10.1109/ICSIP.2014.50", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The text detection and localization is important for video analysis and\nunderstanding. The scene text in video contains semantic information and thus\ncan contribute significantly to video retrieval and understanding. However,\nmost of the approaches detect scene text in still images or single video frame.\nVideos differ from images in temporal redundancy. This paper proposes a novel\nhybrid method to robustly localize the texts in natural scene images and videos\nbased on fusion of discrete wavelet transform and gradient difference. A set of\nrules and geometric properties have been devised to localize the actual text\nregions. Then, morphological operation is performed to generate the text\nregions and finally the connected component analysis is employed to localize\nthe text in a video frame. The experimental results obtained on publicly\navailable standard ICDAR 2003 and Hua dataset illustrate that the proposed\nmethod can accurately detect and localize texts of various sizes, fonts and\ncolors. The experimentation on huge collection of video databases reveal the\nsuitability of the proposed method to video databases.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 07:46:34 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Shekar", "B. H.", ""], ["L.", "Smitha M.", ""], ["Shivakumara", "P.", ""]]}, {"id": "1502.06796", "submitter": "Seunghoon Hong", "authors": "Seunghoon Hong, Tackgeun You, Suha Kwak, Bohyung Han", "title": "Online Tracking by Learning Discriminative Saliency Map with\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an online visual tracking algorithm by learning discriminative\nsaliency map using Convolutional Neural Network (CNN). Given a CNN pre-trained\non a large-scale image repository in offline, our algorithm takes outputs from\nhidden layers of the network as feature descriptors since they show excellent\nrepresentation performance in various general visual recognition problems. The\nfeatures are used to learn discriminative target appearance models using an\nonline Support Vector Machine (SVM). In addition, we construct target-specific\nsaliency map by backpropagating CNN features with guidance of the SVM, and\nobtain the final tracking result in each frame based on the appearance model\ngeneratively constructed with the saliency map. Since the saliency map\nvisualizes spatial configuration of target effectively, it improves target\nlocalization accuracy and enable us to achieve pixel-level target segmentation.\nWe verify the effectiveness of our tracking algorithm through extensive\nexperiment on a challenging benchmark, where our method illustrates outstanding\nperformance compared to the state-of-the-art tracking algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 13:10:32 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Hong", "Seunghoon", ""], ["You", "Tackgeun", ""], ["Kwak", "Suha", ""], ["Han", "Bohyung", ""]]}, {"id": "1502.06807", "submitter": "Markus Oberweger", "authors": "Markus Oberweger, Paul Wohlhart, Vincent Lepetit", "title": "Hands Deep in Deep Learning for Hand Pose Estimation", "comments": "added link to source https://github.com/moberweger/deep-prior", "journal-ref": "In Proceedings of 20th Computer Vision Winter Workshop (CVWW)\n  2015, pp. 21-30", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and evaluate several architectures for Convolutional Neural\nNetworks to predict the 3D joint locations of a hand given a depth map. We\nfirst show that a prior on the 3D pose can be easily introduced and\nsignificantly improves the accuracy and reliability of the predictions. We also\nshow how to use context efficiently to deal with ambiguities between fingers.\nThese two contributions allow us to significantly outperform the\nstate-of-the-art on several challenging benchmarks, both in terms of accuracy\nand computation times.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 13:39:55 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 15:41:25 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Oberweger", "Markus", ""], ["Wohlhart", "Paul", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1502.07019", "submitter": "Shreyansh Daftry", "authors": "Shreyansh Daftry, Christof Hoppe and Horst Bischof", "title": "Building with Drones: Accurate 3D Facade Reconstruction using MAVs", "comments": "8 Pages, 2015 IEEE International Conference on Robotics and\n  Automation (ICRA '15), Seattle, WA, USA", "journal-ref": null, "doi": "10.1109/ICRA.2015.7139681", "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic reconstruction of 3D models from images using multi-view\nStructure-from-Motion methods has been one of the most fruitful outcomes of\ncomputer vision. These advances combined with the growing popularity of Micro\nAerial Vehicles as an autonomous imaging platform, have made 3D vision tools\nubiquitous for large number of Architecture, Engineering and Construction\napplications among audiences, mostly unskilled in computer vision. However, to\nobtain high-resolution and accurate reconstructions from a large-scale object\nusing SfM, there are many critical constraints on the quality of image data,\nwhich often become sources of inaccuracy as the current 3D reconstruction\npipelines do not facilitate the users to determine the fidelity of input data\nduring the image acquisition. In this paper, we present and advocate a\nclosed-loop interactive approach that performs incremental reconstruction in\nreal-time and gives users an online feedback about the quality parameters like\nGround Sampling Distance (GSD), image redundancy, etc on a surface mesh. We\nalso propose a novel multi-scale camera network design to prevent scene drift\ncaused by incremental map building, and release the first multi-scale image\nsequence dataset as a benchmark. Further, we evaluate our system on real\noutdoor scenes, and show that our interactive pipeline combined with a\nmulti-scale camera network approach provides compelling accuracy in multi-view\nreconstruction tasks when compared against the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 00:52:11 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Daftry", "Shreyansh", ""], ["Hoppe", "Christof", ""], ["Bischof", "Horst", ""]]}, {"id": "1502.07041", "submitter": "Jamil Ahmad", "authors": "Jamil Ahmad, Muhammad Sajjad, Irfan Mehmood, Seungmin Rho and Sung\n  Wook Baik", "title": "Describing Colors, Textures and Shapes for Content Based Image Retrieval\n  - A Survey", "comments": null, "journal-ref": "(2014), Journal of Platform Technology 2(4): 34-48", "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Visual media has always been the most enjoyed way of communication. From the\nadvent of television to the modern day hand held computers, we have witnessed\nthe exponential growth of images around us. Undoubtedly it's a fact that they\ncarry a lot of information in them which needs be utilized in an effective\nmanner. Hence intense need has been felt to efficiently index and store large\nimage collections for effective and on- demand retrieval. For this purpose\nlow-level features extracted from the image contents like color, texture and\nshape has been used. Content based image retrieval systems employing these\nfeatures has proven very successful. Image retrieval has promising applications\nin numerous fields and hence has motivated researchers all over the world. New\nand improved ways to represent visual content are being developed each day.\nTremendous amount of research has been carried out in the last decade. In this\npaper we will present a detailed overview of some of the powerful color,\ntexture and shape descriptors for content based image retrieval. A comparative\nanalysis will also be carried out for providing an insight into outstanding\nchallenges in this field.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 03:51:43 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Ahmad", "Jamil", ""], ["Sajjad", "Muhammad", ""], ["Mehmood", "Irfan", ""], ["Rho", "Seungmin", ""], ["Baik", "Sung Wook", ""]]}, {"id": "1502.07058", "submitter": "Adam Harley", "authors": "Adam W. Harley, Alex Ufkes, and Konstantinos G. Derpanis", "title": "Evaluation of Deep Convolutional Nets for Document Image Classification\n  and Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new state-of-the-art for document image classification\nand retrieval, using features learned by deep convolutional neural networks\n(CNNs). In object and scene analysis, deep neural nets are capable of learning\na hierarchical chain of abstraction from pixel inputs to concise and\ndescriptive representations. The current work explores this capacity in the\nrealm of document analysis, and confirms that this representation strategy is\nsuperior to a variety of popular hand-crafted alternatives. Experiments also\nshow that (i) features extracted from CNNs are robust to compression, (ii) CNNs\ntrained on non-document images transfer well to document analysis tasks, and\n(iii) enforcing region-specific feature-learning is unnecessary given\nsufficient training data. This work also makes available a new labelled subset\nof the IIT-CDIP collection, containing 400,000 document images across 16\ncategories, useful for training new CNNs for document analysis.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 05:58:43 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Harley", "Adam W.", ""], ["Ufkes", "Alex", ""], ["Derpanis", "Konstantinos G.", ""]]}, {"id": "1502.07209", "submitter": "Zuxuan Wu", "authors": "Yu-Gang Jiang, Zuxuan Wu, Jun Wang, Xiangyang Xue, Shih-Fu Chang", "title": "Exploiting Feature and Class Relationships in Video Categorization with\n  Regularized Deep Neural Networks", "comments": "Please cite the officially published IEEE TPAMI version if you find\n  this work helpful", "journal-ref": "IEEE TPAMI 40.2 (2018): 352-364", "doi": "10.1109/TPAMI.2017.2670560", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the challenging problem of categorizing videos\naccording to high-level semantics such as the existence of a particular human\naction or a complex event. Although extensive efforts have been devoted in\nrecent years, most existing works combined multiple video features using simple\nfusion strategies and neglected the utilization of inter-class semantic\nrelationships. This paper proposes a novel unified framework that jointly\nexploits the feature relationships and the class relationships for improved\ncategorization performance. Specifically, these two types of relationships are\nestimated and utilized by rigorously imposing regularizations in the learning\nprocess of a deep neural network (DNN). Such a regularized DNN (rDNN) can be\nefficiently realized using a GPU-based implementation with an affordable\ntraining cost. Through arming the DNN with better capability of harnessing both\nthe feature and the class relationships, the proposed rDNN is more suitable for\nmodeling video semantics. With extensive experimental evaluations, we show that\nrDNN produces superior performance over several state-of-the-art approaches. On\nthe well-known Hollywood2 and Columbia Consumer Video benchmarks, we obtain\nvery competitive results: 66.9\\% and 73.5\\% respectively in terms of mean\naverage precision. In addition, to substantially evaluate our rDNN and\nstimulate future research on large scale video categorization, we collect and\nrelease a new benchmark dataset, called FCVID, which contains 91,223 Internet\nvideos and 239 manually annotated categories.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 15:41:48 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 20:37:34 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Jiang", "Yu-Gang", ""], ["Wu", "Zuxuan", ""], ["Wang", "Jun", ""], ["Xue", "Xiangyang", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1502.07241", "submitter": "Frank Hannig", "authors": "Frank Hannig, Dietmar Fey, Anton Lokhmotov", "title": "Proceedings of the DATE Friday Workshop on Heterogeneous Architectures\n  and Design Methods for Embedded Image Systems (HIS 2015)", "comments": "Website of the workshop: https://www12.cs.fau.de/ws/his2015/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the papers accepted at the DATE Friday Workshop on\nHeterogeneous Architectures and Design Methods for Embedded Image Systems (HIS\n2015), held in Grenoble, France, March 13, 2015. HIS 2015 was co-located with\nthe Conference on Design, Automation and Test in Europe (DATE).\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 16:52:56 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 06:00:09 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Hannig", "Frank", ""], ["Fey", "Dietmar", ""], ["Lokhmotov", "Anton", ""]]}, {"id": "1502.07243", "submitter": "Afdel Karim", "authors": "Bousaaid Mourad, Ayaou Tarik, Afdel Karim, Estraillier Pascal", "title": "Real-Time System of Hand Detection And Gesture Recognition In Cyber\n  Presence Interactive System For E-Learning", "comments": "5 pages. arXiv admin note: substantial text overlap with\n  arXiv:1502.06641", "journal-ref": "Journal of Engineering Research and Applications Vol. 4, Issue 9\n  (Version 1), September 2014, pp.1-5", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The development of technologies of multimedia, linked to that of Internet and\ndemocratization of high outflow, has made henceforth E-learning possible for\nlearners being in virtual classes and geographically distributed. The quality\nand quantity of asynchronous and synchronous communications are the key\nelements for E-learning success. It is important to have a propitious\nsupervision to reduce the feeling of isolation in E-learning. This feeling of\nisolation is among the main causes of loss and high rates of stalling in\nE-learning. The researches to be conducted in this domain aim to bring\nsolutions of convergence coming from real time image for the capture and\nrecognition of hand gestures. These gestures will be analyzed by the system and\ntransformed as indicator of participation. This latter is displayed in the\ntable of performance of the tutor as a curve according to the time. In case of\nisolation of learner, the indicator of participation will become red and the\ntutor will be informed of learners with difficulties to participate during\nlearning session.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 17:44:30 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Mourad", "Bousaaid", ""], ["Tarik", "Ayaou", ""], ["Karim", "Afdel", ""], ["Pascal", "Estraillier", ""]]}, {"id": "1502.07331", "submitter": "Roman Chertovskih", "authors": "Ugo Boscain, Roman Chertovskih, Jean-Paul Gauthier, Dario Prandi,\n  Alexey Remizov", "title": "Highly corrupted image inpainting through hypoelliptic diffusion", "comments": "15 pages, 10 figures", "journal-ref": "Journal of Mathematical Imaging and Vision, 2018.\n  https://rdcu.be/KFww", "doi": "10.1007/s10851-018-0810-4", "report-no": null, "categories": "cs.CV math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new image inpainting algorithm, the Averaging and Hypoelliptic\nEvolution (AHE) algorithm, inspired by the one presented in [SIAM J. Imaging\nSci., vol. 7, no. 2, pp. 669--695, 2014] and based upon a semi-discrete\nvariation of the Citti-Petitot-Sarti model of the primary visual cortex V1. The\nAHE algorithm is based on a suitable combination of sub-Riemannian hypoelliptic\ndiffusion and ad-hoc local averaging techniques. In particular, we focus on\nreconstructing highly corrupted images (i.e. where more than the 80% of the\nimage is missing), for which we obtain reconstructions comparable with the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 20:36:08 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 16:01:38 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 12:59:30 GMT"}, {"version": "v4", "created": "Thu, 5 Apr 2018 16:52:34 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Boscain", "Ugo", ""], ["Chertovskih", "Roman", ""], ["Gauthier", "Jean-Paul", ""], ["Prandi", "Dario", ""], ["Remizov", "Alexey", ""]]}, {"id": "1502.07411", "submitter": "Chunhua Shen", "authors": "Fayao Liu, Chunhua Shen, Guosheng Lin, Ian Reid", "title": "Learning Depth from Single Monocular Images Using Deep Convolutional\n  Neural Fields", "comments": "Appearing in IEEE T. Pattern Analysis and Machine Intelligence.\n  Journal version of arXiv:1411.6387 . Test code is available at\n  https://bitbucket.org/fayao/dcnf-fcsp", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2505283", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we tackle the problem of depth estimation from single\nmonocular images. Compared with depth estimation using multiple images such as\nstereo depth perception, depth from monocular images is much more challenging.\nPrior work typically focuses on exploiting geometric priors or additional\nsources of information, most using hand-crafted features. Recently, there is\nmounting evidence that features from deep convolutional neural networks (CNN)\nset new records for various vision applications. On the other hand, considering\nthe continuous characteristic of the depth values, depth estimations can be\nnaturally formulated as a continuous conditional random field (CRF) learning\nproblem. Therefore, here we present a deep convolutional neural field model for\nestimating depths from single monocular images, aiming to jointly explore the\ncapacity of deep CNN and continuous CRF. In particular, we propose a deep\nstructured learning scheme which learns the unary and pairwise potentials of\ncontinuous CRF in a unified deep CNN framework. We then further propose an\nequally effective model based on fully convolutional networks and a novel\nsuperpixel pooling method, which is $\\sim 10$ times faster, to speedup the\npatch-wise convolutions in the deep model. With this more efficient model, we\nare able to design deeper networks to pursue better performance. Experiments on\nboth indoor and outdoor scene datasets demonstrate that the proposed method\noutperforms state-of-the-art depth estimation approaches.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 01:26:22 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2015 03:31:44 GMT"}, {"version": "v3", "created": "Sat, 18 Apr 2015 10:13:39 GMT"}, {"version": "v4", "created": "Wed, 30 Sep 2015 14:19:19 GMT"}, {"version": "v5", "created": "Thu, 8 Oct 2015 06:02:00 GMT"}, {"version": "v6", "created": "Wed, 25 Nov 2015 00:03:31 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Liu", "Fayao", ""], ["Shen", "Chunhua", ""], ["Lin", "Guosheng", ""], ["Reid", "Ian", ""]]}, {"id": "1502.07423", "submitter": "Xi Peng", "authors": "Xi Peng, Canyi Lu, Zhang Yi, Huajin Tang", "title": "Connections Between Nuclear Norm and Frobenius Norm Based\n  Representations", "comments": "IEEE Trans. on Neural Networks and Learning Systems, 2016", "journal-ref": "IEEE Trans. on Neural Networks and Learning Systems, 2016", "doi": "10.1109/TNNLS.2016.2608834", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of works have shown that frobenius-norm based representation (FNR) is\ncompetitive to sparse representation and nuclear-norm based representation\n(NNR) in numerous tasks such as subspace clustering. Despite the success of FNR\nin experimental studies, less theoretical analysis is provided to understand\nits working mechanism. In this paper, we fill this gap by building the\ntheoretical connections between FNR and NNR. More specially, we prove that: 1)\nwhen the dictionary can provide enough representative capacity, FNR is exactly\nNNR even though the data set contains the Gaussian noise, Laplacian noise, or\nsample-specified corruption, 2) otherwise, FNR and NNR are two solutions on the\ncolumn space of the dictionary.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 03:59:36 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 03:17:01 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Peng", "Xi", ""], ["Lu", "Canyi", ""], ["Yi", "Zhang", ""], ["Tang", "Huajin", ""]]}, {"id": "1502.07432", "submitter": "Yu-Hui Chen", "authors": "Yu-Hui Chen, Dennis Wei, Gregory Newstadt, Jeffrey Simmons and Alfred\n  Hero", "title": "Coercive Region-level Registration for Multi-modal Images", "comments": "This work has been accepted to International Conference on Image\n  Processing (ICIP) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a coercive approach to simultaneously register and segment\nmulti-modal images which share similar spatial structure. Registration is done\nat the region level to facilitate data fusion while avoiding the need for\ninterpolation. The algorithm performs alternating minimization of an objective\nfunction informed by statistical models for pixel values in different\nmodalities. Hypothesis tests are developed to determine whether to refine\nsegmentations by splitting regions. We demonstrate that our approach has\nsignificantly better performance than the state-of-the-art registration and\nsegmentation methods on microscopy images.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 04:57:52 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 16:08:35 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2015 03:24:57 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Chen", "Yu-Hui", ""], ["Wei", "Dennis", ""], ["Newstadt", "Gregory", ""], ["Simmons", "Jeffrey", ""], ["Hero", "Alfred", ""]]}, {"id": "1502.07436", "submitter": "Yu-Hui Chen", "authors": "Yu-Hui Chen, Se Un Park, Dennis Wei, Gregory Newstadt, Michael\n  Jackson, Jeff P. Simmons, Marc De Graef and Alfred O. Hero", "title": "A Dictionary Approach to EBSD Indexing", "comments": "This paper is in press in the Journal of Microscopy and\n  Microanalysis, Cambridge University Press, Feb. 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for indexing of grain and sub-grain structures in\nelectron backscatter diffraction (EBSD) images of polycrystalline materials.\nThe framework is based on a previously introduced physics-based forward model\nby Callahan and De Graef (2013) relating measured patterns to grain\norientations (Euler angle). The forward model is tuned to the microscope and\nthe sample symmetry group. We discretize the domain of the forward model onto a\ndense grid of Euler angles and for each measured pattern we identify the most\nsimilar patterns in the dictionary. These patterns are used to identify\nboundaries, detect anomalies, and index crystal orientations. The statistical\ndistribution of these closest matches is used in an unsupervised binary\ndecision tree (DT) classifier to identify grain boundaries and anomalous\nregions. The DT classifies a pattern as an anomaly if it has an abnormally low\nsimilarity to any pattern in the dictionary. It classifies a pixel as being\nnear a grain boundary if the highly ranked patterns in the dictionary differ\nsignificantly over the pixels 3x3 neighborhood. Indexing is accomplished by\ncomputing the mean orientation of the closest dictionary matches to each\npattern. The mean orientation is estimated using a maximum likelihood approach\nthat models the orientation distribution as a mixture of Von Mises-Fisher\ndistributions over the quaternionic 3-sphere. The proposed dictionary matching\napproach permits segmentation, anomaly detection, and indexing to be performed\nin a unified manner with the additional benefit of uncertainty quantification.\nWe demonstrate the proposed dictionary-based approach on a Ni-base IN100 alloy.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 05:03:49 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 20:29:28 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Chen", "Yu-Hui", ""], ["Park", "Se Un", ""], ["Wei", "Dennis", ""], ["Newstadt", "Gregory", ""], ["Jackson", "Michael", ""], ["Simmons", "Jeff P.", ""], ["De Graef", "Marc", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1502.07446", "submitter": "V\\'itor Schwambach", "authors": "V\\'itor Schwambach, S\\'ebastien Cleyet-Merle, Alain Issard, St\\'ephane\n  Mancini", "title": "Estimating the Potential Speedup of Computer Vision Applications on\n  Embedded Multiprocessors", "comments": "Presented at DATE Friday Workshop on Heterogeneous Architectures and\n  Design Methods for Embedded Image Systems (HIS 2015) (arXiv:1502.07241)", "journal-ref": null, "doi": null, "report-no": "DATEHIS/2015/01", "categories": "cs.CV cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision applications constitute one of the key drivers for embedded\nmulticore architectures. Although the number of available cores is increasing\nin new architectures, designing an application to maximize the utilization of\nthe platform is still a challenge. In this sense, parallel performance\nprediction tools can aid developers in understanding the characteristics of an\napplication and finding the most adequate parallelization strategy. In this\nwork, we present a method for early parallel performance estimation on embedded\nmultiprocessors from sequential application traces. We describe its\nimplementation in Parana, a fast trace-driven simulator targeting OpenMP\napplications on the STMicroelectronics' STxP70 Application-Specific\nMultiprocessor (ASMP). Results for the FAST key point detector application show\nan error margin of less than 10% compared to the reference cycle-approximate\nsimulator, with lower modeling effort and up to 20x faster execution time.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 06:13:47 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Schwambach", "V\u00edtor", ""], ["Cleyet-Merle", "S\u00e9bastien", ""], ["Issard", "Alain", ""], ["Mancini", "St\u00e9phane", ""]]}, {"id": "1502.07448", "submitter": "Frank Hannig", "authors": "Oliver Reiche, Konrad H\\\"aublein, Marc Reichenbach, Frank Hannig,\n  J\\\"urgen Teich, Dietmar Fey", "title": "Automatic Optimization of Hardware Accelerators for Image Processing", "comments": "Presented at DATE Friday Workshop on Heterogeneous Architectures and\n  Design Methods for Embedded Image Systems (HIS 2015) (arXiv:1502.07241)", "journal-ref": null, "doi": null, "report-no": "DATEHIS/2015/03", "categories": "cs.PL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the domain of image processing, often real-time constraints are required.\nIn particular, in safety-critical applications, such as X-ray computed\ntomography in medical imaging or advanced driver assistance systems in the\nautomotive domain, timing is of utmost importance. A common approach to\nmaintain real-time capabilities of compute-intensive applications is to offload\nthose computations to dedicated accelerator hardware, such as Field\nProgrammable Gate Arrays (FPGAs). Programming such architectures is a\nchallenging task, with respect to the typical FPGA-specific design criteria:\nAchievable overall algorithm latency and resource usage of FPGA primitives\n(BRAM, FF, LUT, and DSP). High-Level Synthesis (HLS) dramatically simplifies\nthis task by enabling the description of algorithms in well-known higher\nlanguages (C/C++) and its automatic synthesis that can be accomplished by HLS\ntools. However, algorithm developers still need expert knowledge about the\ntarget architecture, in order to achieve satisfying results. Therefore, in\nprevious work, we have shown that elevating the description of image algorithms\nto an even higher abstraction level, by using a Domain-Specific Language (DSL),\ncan significantly cut down the complexity for designing such algorithms for\nFPGAs. To give the developer even more control over the common trade-off,\nlatency vs. resource usage, we will present an automatic optimization process\nwhere these criteria are analyzed and fed back to the DSL compiler, in order to\ngenerate code that is closer to the desired design specifications. Finally, we\ngenerate code for stereo block matching algorithms and compare it with\nhandwritten implementations to quantify the quality of our results.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 06:16:51 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Reiche", "Oliver", ""], ["H\u00e4ublein", "Konrad", ""], ["Reichenbach", "Marc", ""], ["Hannig", "Frank", ""], ["Teich", "J\u00fcrgen", ""], ["Fey", "Dietmar", ""]]}, {"id": "1502.07449", "submitter": "Lan Shi", "authors": "Lan Shi, Christopher Soell, Andreas Baenisch, Robert Weigel, J\\\"urgen\n  Seiler, Thomas Ussmueller", "title": "Concept for a CMOS Image Sensor Suited for Analog Image Pre-Processing", "comments": "Presented at DATE Friday Workshop on Heterogeneous Architectures and\n  Design Methods for Embedded Image Systems (HIS 2015) (arXiv:1502.07241)", "journal-ref": null, "doi": null, "report-no": "DATEHIS/2015/04", "categories": "cs.ET cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A concept for a novel CMOS image sensor suited for analog image\npre-processing is presented in this paper. As an example, an image restoration\nalgorithm for reducing image noise is applied as image pre-processing in the\nanalog domain. To supply low-latency data input for analog image preprocessing,\nthe proposed concept for a CMOS image sensor offers a new sensor signal\nacquisition method in 2D. In comparison to image pre-processing in the digital\ndomain, the proposed analog image pre-processing promises an improved image\nquality. Furthermore, the image noise at the stage of analog sensor signal\nacquisition can be used to select the most effective restoration algorithm\napplied to the analog circuit due to image processing prior to the A/D\nconverter.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 06:18:04 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Shi", "Lan", ""], ["Soell", "Christopher", ""], ["Baenisch", "Andreas", ""], ["Weigel", "Robert", ""], ["Seiler", "J\u00fcrgen", ""], ["Ussmueller", "Thomas", ""]]}, {"id": "1502.07453", "submitter": "Christian Hartmann", "authors": "Christian Hartmann, Anna Yupatova, Marc Reichenbach, Dietmar Fey,\n  Reinhard German", "title": "A Holistic Approach for Modeling and Synthesis of Image Processing\n  Applications for Heterogeneous Computing Architectures", "comments": "Presented at DATE Friday Workshop on Heterogeneous Architectures and\n  Design Methods for Embedded Image Systems (HIS 2015) (arXiv:1502.07241)", "journal-ref": null, "doi": null, "report-no": "DATEHIS/2015/06", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image processing applications are common in every field of our daily life.\nHowever, most of them are very complex and contain several tasks with different\ncomplexities which result in varying requirements for computing architectures.\nNevertheless, a general processing scheme in every image processing application\nhas a similar structure, called image processing pipeline: (1) capturing an\nimage, (2) pre-processing using local operators, (3) processing with global\noperators and (4) post-processing using complex operations. Therefore,\napplication-specialized hardware solutions based on heterogeneous architectures\nare used for image processing. Unfortunately the development of applications\nfor heterogeneous hardware architectures is challenging due to the distribution\nof computational tasks among processors and programmable logic units. Nowadays,\nimage processing systems are started from scratch which is time-consuming,\nerror-prone and inflexible. A new methodology for modeling and implementing is\nneeded in order to reduce the development time of heterogenous image processing\nsystems. This paper introduces a new holistic top down approach for image\nprocessing systems. Two challenges have to be investigated. First, designers\nought to be able to model their complete image processing pipeline on an\nabstract layer using UML. Second, we want to close the gap between the abstract\nsystem and the system architecture.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 06:20:44 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Hartmann", "Christian", ""], ["Yupatova", "Anna", ""], ["Reichenbach", "Marc", ""], ["Fey", "Dietmar", ""], ["German", "Reinhard", ""]]}, {"id": "1502.07540", "submitter": "Anupama Ray Ms", "authors": "Anupama Ray, Sai Rajeswar, and Santanu Chaudhury", "title": "A hypothesize-and-verify framework for Text Recognition using Deep\n  Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep LSTM is an ideal candidate for text recognition. However text\nrecognition involves some initial image processing steps like segmentation of\nlines and words which can induce error to the recognition system. Without\nsegmentation, learning very long range context is difficult and becomes\ncomputationally intractable. Therefore, alternative soft decisions are needed\nat the pre-processing level. This paper proposes a hybrid text recognizer using\na deep recurrent neural network with multiple layers of abstraction and long\nrange context along with a language model to verify the performance of the deep\nneural network. In this paper we construct a multi-hypotheses tree architecture\nwith candidate segments of line sequences from different segmentation\nalgorithms at its different branches. The deep neural network is trained on\nperfectly segmented data and tests each of the candidate segments, generating\nunicode sequences. In the verification step, these unicode sequences are\nvalidated using a sub-string match with the language model and best first\nsearch is used to find the best possible combination of alternative hypothesis\nfrom the tree structure. Thus the verification framework using language models\neliminates wrong segmentation outputs and filters recognition errors.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 13:18:09 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Ray", "Anupama", ""], ["Rajeswar", "Sai", ""], ["Chaudhury", "Santanu", ""]]}, {"id": "1502.07643", "submitter": "Ryan Robinson", "authors": "Ryan Robinson", "title": "Dynamic Belief Fusion for Object Detection", "comments": "The paper has been withdrawn and an updated paper has been uploaded\n  by a co-author: http://arxiv.org/pdf/1511.03183.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach for the fusion of detection scores from disparate object\ndetection methods is proposed. In order to effectively integrate the outputs of\nmultiple detectors, the level of ambiguity in each individual detection score\n(called \"uncertainty\") is estimated using the precision/recall relationship of\nthe corresponding detector. The proposed fusion method, called Dynamic Belief\nFusion (DBF), dynamically assigns basic probabilities to propositions (target,\nnon-target, uncertain) based on confidence levels in the detection results of\nindividual approaches. A joint basic probability assignment, containing\ninformation from all detectors, is determined using Dempster's combination\nrule, and is easily reduced to a single fused detection score. Experiments on\nARL and PASCAL VOC 07 datasets demonstrate that the detection accuracy of DBF\nis considerably greater than conventional fusion approaches as well as\nstate-of-the-art individual detectors.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 17:31:15 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2015 15:40:11 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2015 04:19:40 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Robinson", "Ryan", ""]]}, {"id": "1502.07666", "submitter": "Markus Grasmair", "authors": "Martin Bauer, Markus Eslitzbichler, Markus Grasmair", "title": "Landmark-Guided Elastic Shape Analysis of Human Character Motions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motions of virtual characters in movies or video games are typically\ngenerated by recording actors using motion capturing methods. Animations\ngenerated this way often need postprocessing, such as improving the periodicity\nof cyclic animations or generating entirely new motions by interpolation of\nexisting ones. Furthermore, search and classification of recorded motions\nbecomes more and more important as the amount of recorded motion data grows.\n  In this paper, we will apply methods from shape analysis to the processing of\nanimations. More precisely, we will use the by now classical elastic metric\nmodel used in shape matching, and extend it by incorporating additional inexact\nfeature point information, which leads to an improved temporal alignment of\ndifferent animations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 08:45:48 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Bauer", "Martin", ""], ["Eslitzbichler", "Markus", ""], ["Grasmair", "Markus", ""]]}, {"id": "1502.07743", "submitter": "Kevin Judd", "authors": "Kevin Judd", "title": "Tracking an Object with Unknown Accelerations using a Shadowing Filter", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A commonly encountered problem is the tracking of a physical object, like a\nmaneuvering ship, aircraft, land vehicle, spacecraft or animate creature\ncarrying a wireless device. The sensor data is often limited and inaccurate\nobservations of range or bearing. This problem is more difficult than tracking\na ballistic trajectory, because an operative affects unknown and arbitrarily\nchanging accelerations. Although stochastic methods of filtering or state\nestimation (Kalman filters and particle filters) are widely used, out of vogue\nvariational methods are more appropriate in this tracking context, because the\nobjects do not typically display any significant random motions at the length\nand time scales of interest. This leads us to propose a rather elegant approach\nbased on a \\emph{shadowing filter}. The resulting filter is efficient (reduces\nto the solution of linear equations) and robust (uneffected by missing data and\nsingular correlations that would cause catastrophic failure of Bayesian\nfilters.) The tracking is so robust, that in some common situations it actually\nperforms better by ignoring error correlations that are so vital to Kalman\nfilters.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 06:30:51 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Judd", "Kevin", ""]]}, {"id": "1502.07770", "submitter": "Vladimir Kolmogorov", "authors": "Vladimir Kolmogorov, Thomas Pock, Michal Rolinek", "title": "Total variation on a tree", "comments": "accepted to SIAM Journal on Imaging Sciences (SIIMS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing the continuous valued total variation\nsubject to different unary terms on trees and propose fast direct algorithms\nbased on dynamic programming to solve these problems. We treat both the convex\nand the non-convex case and derive worst case complexities that are equal or\nbetter than existing methods. We show applications to total variation based 2D\nimage processing and computer vision problems based on a Lagrangian\ndecomposition approach. The resulting algorithms are very efficient, offer a\nhigh degree of parallelism and come along with memory requirements which are\nonly in the order of the number of image pixels.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 21:36:07 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2015 07:33:57 GMT"}, {"version": "v3", "created": "Mon, 25 Apr 2016 19:41:23 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Kolmogorov", "Vladimir", ""], ["Pock", "Thomas", ""], ["Rolinek", "Michal", ""]]}, {"id": "1502.07781", "submitter": "Yuriy Bunyak", "authors": "Yuriy A. Bunyak, Roman N. Kvetnyy and Olga Yu. Sofina", "title": "The conjugated null space method of blind PSF estimation and\n  deconvolution optimization", "comments": "arXiv admin note: text overlap with arXiv:1206.3594", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have shown that the vector of the point spread function (PSF)\nlexicographical presentation belongs to the left side conjugated null space\n(NS) of the autoregression (AR) matrix operator on condition the AR parameters\nare common for original and blurred images. The method of the PSF and inverse\nPSF (IPSF) evaluation in the basis of the NS eigenfunctions is offered. The\noptimization of the PSF and IPSF shape with the aim of fluctuation elimination\nis considered in NS spectral domain and image space domain. The function of\nsurface area was used as the regularization functional. Two methods of original\nimage estimate optimization were designed basing on maximum entropy\ngeneralization of sought and blurred images conditional probability density and\nregularization. The first method uses balanced variations of convolutions with\nthe PSF and IPSF to obtaining iterative schema of image optimization. The\nvariations balance is providing by dynamic regularization basing on condition\nof the iteration process convergence. The regularization has dynamic character\nbecause depends on current and previous image estimate variations. The second\nmethod implements the regularization of the deconvolution optimization in\ncurved space with metric defined on image estimate surface. The given iterative\nschemas have fast convergence and therefore can be used for reconstruction of\nhigh resolution images series in real time. The NS can be used for design of\ndenoising bilateral linear filter which does not introduce image smoothing.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 22:37:01 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Bunyak", "Yuriy A.", ""], ["Kvetnyy", "Roman N.", ""], ["Sofina", "Olga Yu.", ""]]}, {"id": "1502.07802", "submitter": "Zongyuan Ge", "authors": "ZongYuan Ge, Chris McCool, Conrad Sanderson, Peter Corke", "title": "Modelling Local Deep Convolutional Neural Network Features to Improve\n  Fine-Grained Image Classification", "comments": "5 pages, three figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a local modelling approach using deep convolutional neural\nnetworks (CNNs) for fine-grained image classification. Recently, deep CNNs\ntrained from large datasets have considerably improved the performance of\nobject recognition. However, to date there has been limited work using these\ndeep CNNs as local feature extractors. This partly stems from CNNs having\ninternal representations which are high dimensional, thereby making such\nrepresentations difficult to model using stochastic models. To overcome this\nissue, we propose to reduce the dimensionality of one of the internal fully\nconnected layers, in conjunction with layer-restricted retraining to avoid\nretraining the entire network. The distribution of low-dimensional features\nobtained from the modified layer is then modelled using a Gaussian mixture\nmodel. Comparative experiments show that considerable performance improvements\ncan be achieved on the challenging Fish and UEC FOOD-100 datasets.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 02:04:57 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Ge", "ZongYuan", ""], ["McCool", "Chris", ""], ["Sanderson", "Conrad", ""], ["Corke", "Peter", ""]]}, {"id": "1502.07816", "submitter": "Joshua Glaser", "authors": "Joshua I. Glaser, Bradley M. Zamft, George M. Church, Konrad P.\n  Kording", "title": "Puzzle Imaging: Using Large-scale Dimensionality Reduction Algorithms\n  for Localization", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0131593", "report-no": null, "categories": "q-bio.NC cs.CE cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current high-resolution imaging techniques require an intact sample that\npreserves spatial relationships. We here present a novel approach, \"puzzle\nimaging,\" that allows imaging a spatially scrambled sample. This technique\ntakes many spatially disordered samples, and then pieces them back together\nusing local properties embedded within the sample. We show that puzzle imaging\ncan efficiently produce high-resolution images using dimensionality reduction\nalgorithms. We demonstrate the theoretical capabilities of puzzle imaging in\nthree biological scenarios, showing that (1) relatively precise 3-dimensional\nbrain imaging is possible; (2) the physical structure of a neural network can\noften be recovered based only on the neural connectivity matrix; and (3) a\nchemical map could be reproduced using bacteria with chemosensitive DNA and\nconjugative transfer. The ability to reconstruct scrambled images promises to\nenable imaging based on DNA sequencing of homogenized tissue samples.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 04:55:54 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2015 07:16:17 GMT"}, {"version": "v3", "created": "Sun, 21 Jun 2015 19:17:03 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Glaser", "Joshua I.", ""], ["Zamft", "Bradley M.", ""], ["Church", "George M.", ""], ["Kording", "Konrad P.", ""]]}, {"id": "1502.07828", "submitter": "Luca Baroffio", "authors": "Luca Baroffio, Matteo Cesana, Alessandro Redondi, Marco Tagliasacchi,\n  Stefano Tubaro", "title": "Hybrid coding of visual content and local image features", "comments": "submitted to IEEE International Conference on Image Processing", "journal-ref": null, "doi": "10.1109/ICIP.2015.7351258", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed visual analysis applications, such as mobile visual search or\nVisual Sensor Networks (VSNs) require the transmission of visual content on a\nbandwidth-limited network, from a peripheral node to a processing unit.\nTraditionally, a Compress-Then-Analyze approach has been pursued, in which\nsensing nodes acquire and encode the pixel-level representation of the visual\ncontent, that is subsequently transmitted to a sink node in order to be\nprocessed. This approach might not represent the most effective solution, since\nseveral analysis applications leverage a compact representation of the content,\nthus resulting in an inefficient usage of network resources. Furthermore,\ncoding artifacts might significantly impact the accuracy of the visual task at\nhand. To tackle such limitations, an orthogonal approach named\nAnalyze-Then-Compress has been proposed. According to such a paradigm, sensing\nnodes are responsible for the extraction of visual features, that are encoded\nand transmitted to a sink node for further processing. In spite of improved\ntask efficiency, such paradigm implies the central processing node not being\nable to reconstruct a pixel-level representation of the visual content. In this\npaper we propose an effective compromise between the two paradigms, namely\nHybrid-Analyze-Then-Compress (HATC) that aims at jointly encoding visual\ncontent and local image features. Furthermore, we show how a target tradeoff\nbetween image quality and task accuracy might be achieved by accurately\nallocating the bitrate to either visual content or local features.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 08:44:21 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Baroffio", "Luca", ""], ["Cesana", "Matteo", ""], ["Redondi", "Alessandro", ""], ["Tagliasacchi", "Marco", ""], ["Tubaro", "Stefano", ""]]}, {"id": "1502.07939", "submitter": "Luca Baroffio", "authors": "Luca Baroffio, Antonio Canclini, Matteo Cesana, Alessandro Redondi,\n  Marco Tagliasacchi, Stefano Tubaro", "title": "Coding local and global binary visual features extracted from video\n  sequences", "comments": "submitted to IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2015.2445294", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary local features represent an effective alternative to real-valued\ndescriptors, leading to comparable results for many visual analysis tasks,\nwhile being characterized by significantly lower computational complexity and\nmemory requirements. When dealing with large collections, a more compact\nrepresentation based on global features is often preferred, which can be\nobtained from local features by means of, e.g., the Bag-of-Visual-Word (BoVW)\nmodel. Several applications, including for example visual sensor networks and\nmobile augmented reality, require visual features to be transmitted over a\nbandwidth-limited network, thus calling for coding techniques that aim at\nreducing the required bit budget, while attaining a target level of efficiency.\nIn this paper we investigate a coding scheme tailored to both local and global\nbinary features, which aims at exploiting both spatial and temporal redundancy\nby means of intra- and inter-frame coding. In this respect, the proposed coding\nscheme can be conveniently adopted to support the Analyze-Then-Compress (ATC)\nparadigm. That is, visual features are extracted from the acquired content,\nencoded at remote nodes, and finally transmitted to a central controller that\nperforms visual analysis. This is in contrast with the traditional approach, in\nwhich visual content is acquired at a node, compressed and then sent to a\ncentral unit for further processing, according to the Compress-Then-Analyze\n(CTA) paradigm. In this paper we experimentally compare ATC and CTA by means of\nrate-efficiency curves in the context of two different visual analysis tasks:\nhomography estimation and content-based retrieval. Our results show that the\nnovel ATC paradigm based on the proposed coding primitives can be competitive\nwith CTA, especially in bandwidth limited scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 14:23:39 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Baroffio", "Luca", ""], ["Canclini", "Antonio", ""], ["Cesana", "Matteo", ""], ["Redondi", "Alessandro", ""], ["Tagliasacchi", "Marco", ""], ["Tubaro", "Stefano", ""]]}, {"id": "1502.07976", "submitter": "Miguel Angel Bautista Martin", "authors": "Miguel Angel Bautista, Oriol Pujol, Fernando de la Torre and Sergio\n  Escalera", "title": "Error-Correcting Factorization", "comments": "Under review at TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error Correcting Output Codes (ECOC) is a successful technique in multi-class\nclassification, which is a core problem in Pattern Recognition and Machine\nLearning. A major advantage of ECOC over other methods is that the multi- class\nproblem is decoupled into a set of binary problems that are solved\nindependently. However, literature defines a general error-correcting\ncapability for ECOCs without analyzing how it distributes among classes,\nhindering a deeper analysis of pair-wise error-correction. To address these\nlimitations this paper proposes an Error-Correcting Factorization (ECF) method,\nour contribution is three fold: (I) We propose a novel representation of the\nerror-correction capability, called the design matrix, that enables us to build\nan ECOC on the basis of allocating correction to pairs of classes. (II) We\nderive the optimal code length of an ECOC using rank properties of the design\nmatrix. (III) ECF is formulated as a discrete optimization problem, and a\nrelaxed solution is found using an efficient constrained block coordinate\ndescent approach. (IV) Enabled by the flexibility introduced with the design\nmatrix we propose to allocate the error-correction on classes that are prone to\nconfusion. Experimental results in several databases show that when allocating\nthe error-correction to confusable classes ECF outperforms state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 17:22:53 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 17:49:16 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Bautista", "Miguel Angel", ""], ["Pujol", "Oriol", ""], ["de la Torre", "Fernando", ""], ["Escalera", "Sergio", ""]]}, {"id": "1502.08029", "submitter": "Li Yao", "authors": "Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal,\n  Hugo Larochelle, Aaron Courville", "title": "Describing Videos by Exploiting Temporal Structure", "comments": "Accepted to ICCV15. This version comes with code release and\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in using recurrent neural networks (RNNs) for image\ndescription has motivated the exploration of their application for video\ndescription. However, while images are static, working with videos requires\nmodeling their dynamic temporal structure and then properly integrating that\ninformation into a natural language description. In this context, we propose an\napproach that successfully takes into account both the local and global\ntemporal structure of videos to produce descriptions. First, our approach\nincorporates a spatial temporal 3-D convolutional neural network (3-D CNN)\nrepresentation of the short temporal dynamics. The 3-D CNN representation is\ntrained on video action recognition tasks, so as to produce a representation\nthat is tuned to human motion and behavior. Second we propose a temporal\nattention mechanism that allows to go beyond local temporal modeling and learns\nto automatically select the most relevant temporal segments given the\ntext-generating RNN. Our approach exceeds the current state-of-art for both\nBLEU and METEOR metrics on the Youtube2Text dataset. We also present results on\na new, larger and more challenging dataset of paired video and natural language\ndescriptions.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 19:30:40 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 17:24:47 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2015 15:27:08 GMT"}, {"version": "v4", "created": "Sat, 25 Apr 2015 20:32:27 GMT"}, {"version": "v5", "created": "Thu, 1 Oct 2015 00:12:46 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Yao", "Li", ""], ["Torabi", "Atousa", ""], ["Cho", "Kyunghyun", ""], ["Ballas", "Nicolas", ""], ["Pal", "Christopher", ""], ["Larochelle", "Hugo", ""], ["Courville", "Aaron", ""]]}, {"id": "1502.08039", "submitter": "Jihun Hamm", "authors": "Jihun Hamm, Mikhail Belkin", "title": "Probabilistic Zero-shot Classification with Semantic Rankings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a non-metric ranking-based representation of\nsemantic similarity that allows natural aggregation of semantic information\nfrom multiple heterogeneous sources. We apply the ranking-based representation\nto zero-shot learning problems, and present deterministic and probabilistic\nzero-shot classifiers which can be built from pre-trained classifiers without\nretraining. We demonstrate their the advantages on two large real-world image\ndatasets. In particular, we show that aggregating different sources of semantic\ninformation, including crowd-sourcing, leads to more accurate classification.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 20:00:53 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Hamm", "Jihun", ""], ["Belkin", "Mikhail", ""]]}, {"id": "1502.08040", "submitter": "Mayank Kumar", "authors": "Mayank Kumar, Ashok Veeraraghavan, Ashutosh Sabharval", "title": "DistancePPG: Robust non-contact vital signs monitoring using a camera", "comments": "24 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vital signs such as pulse rate and breathing rate are currently measured\nusing contact probes. But, non-contact methods for measuring vital signs are\ndesirable both in hospital settings (e.g. in NICU) and for ubiquitous in-situ\nhealth tracking (e.g. on mobile phone and computers with webcams). Recently,\ncamera-based non-contact vital sign monitoring have been shown to be feasible.\nHowever, camera-based vital sign monitoring is challenging for people with\ndarker skin tone, under low lighting conditions, and/or during movement of an\nindividual in front of the camera. In this paper, we propose distancePPG, a new\ncamera-based vital sign estimation algorithm which addresses these challenges.\nDistancePPG proposes a new method of combining skin-color change signals from\ndifferent tracked regions of the face using a weighted average, where the\nweights depend on the blood perfusion and incident light intensity in the\nregion, to improve the signal-to-noise ratio (SNR) of camera-based estimate.\nOne of our key contributions is a new automatic method for determining the\nweights based only on the video recording of the subject. The gains in SNR of\ncamera-based PPG estimated using distancePPG translate into reduction of the\nerror in vital sign estimation, and thus expand the scope of camera-based vital\nsign monitoring to potentially challenging scenarios. Further, a dataset will\nbe released, comprising of synchronized video recordings of face and pulse\noximeter based ground truth recordings from the earlobe for people with\ndifferent skin tones, under different lighting conditions and for various\nmotion scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 20:03:06 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2015 02:31:18 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Kumar", "Mayank", ""], ["Veeraraghavan", "Ashok", ""], ["Sabharval", "Ashutosh", ""]]}, {"id": "1502.08046", "submitter": "Piotr Plonski", "authors": "Piotr P{\\l}o\\'nski, Dorota Stefan, Robert Sulej, Krzysztof Zaremba", "title": "Image Segmentation in Liquid Argon Time Projection Chamber Detector", "comments": "10 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Liquid Argon Time Projection Chamber (LAr-TPC) detectors provide\nexcellent imaging and particle identification ability for studying neutrinos.\nAn efficient and automatic reconstruction procedures are required to exploit\npotential of this imaging technology. Herein, a novel method for segmentation\nof images from LAr-TPC detectors is presented. The proposed approach computes a\nfeature descriptor for each pixel in the image, which characterizes amplitude\ndistribution in pixel and its neighbourhood. The supervised classifier is\nemployed to distinguish between pixels representing particle's track and noise.\nThe classifier is trained and evaluated on the hand-labeled dataset. The\nproposed approach can be a preprocessing step for reconstructing algorithms\nworking directly on detector images.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 20:32:35 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["P\u0142o\u0144ski", "Piotr", ""], ["Stefan", "Dorota", ""], ["Sulej", "Robert", ""], ["Zaremba", "Krzysztof", ""]]}]